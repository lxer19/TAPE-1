URL: http://robotics.Stanford.EDU/~sbenson/AML.ps
Refering-URL: http://robotics.Stanford.EDU/~sbenson/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: sbenson@cs.stanford.edu  
Phone: Tel: 415-725-8787  
Title: Action Model Learning and Action Execution in a Reactive Agent  
Author: Scott Benson 
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Note: SUBMITTED TO IJCAI-95  This work was partially supported by the National Science Foundation, Grant #IRI 9116399.  
Abstract: We present a reactive agent that successfully learns action models in a continuous and dynamic environment. The TRAIL agent uses teleo-reactive trees to integrate planning, reactive execution, and performance improvement through action model learning. This paper discusses the difficulties of action model learning in the face of irrelevant features, durative actions, and stochastic action effects, and presents TRAIL's solutions to these problems. We focus on two particular aspects of TRAIL: the identification of action successes and failures during the execution of a teleo-reactive tree, and the analysis of execution failures through the use of experimentation to distinguish among possible causes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bates, J. </author> <year> (1992), </year> <title> "Virtual reality, art, and entertainment", PRESENCE: </title> <booktitle> Teleoperators and Virtual Environments 1(1), </booktitle> <pages> 133-138. </pages>
Reference-contexts: As possible extended domains, we are considering a HERO robot simulator, the SGI Flight Simulator, and the Woggleworld system <ref> (Bates 1992) </ref>. Because of insurmountable combinatorial problems, both planning and learning eventually require hierarchies of actions. We imagine that TRAIL will first construct low level TOPs and then gradually learn the effects of higher level actions.
Reference: <author> Benson, S. </author> <year> (1995), </year> <title> Inductive learning of reactive action models, </title> <note> Submitted to ML-95. </note>
Reference: <author> Benson, S. & Nilsson, N. </author> <year> (1994), </year> <title> Reacting, planning, and learning in an autonomous agent, </title> <editor> in K. Furukawa, D. Michie & S. Muggleton, eds, </editor> <booktitle> "Machine Intelligence 14", </booktitle> <publisher> Oxford: the Clarendon Press. in press. </publisher>
Reference: <author> Christiansen, A. D. </author> <year> (1991), </year> <title> Manipulation planning from empirical backprojection, </title> <booktitle> in "Proceedings. 1991 IEEE International Conference on Robotics and Automation", </booktitle> <pages> pp. 762-768. </pages>
Reference: <author> Connell, J. H. & Mahadevan, S., </author> <title> eds (1993), Robot Learning, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: 1 Introduction Autonomous agents acting in complex environments must sometimes resort to learning, both to avoid the need for exhaustive preprogramming and to adapt to unanticipated or changing situations. Most such work has focused on learning control policies directly <ref> (Connell & Mahadevan 1993, Maes 1991, Sutton 1988) </ref>. We focus instead on learning models of actions in the domain, from which control policies, in the form of reactive plans, can be computed.
Reference: <author> DeJong, G. F. </author> <year> (1994), </year> <title> "Learning to plan in continuous domains", </title> <journal> Artificial Intelligence 65, </journal> <volume> 71 - 141. </volume>
Reference: <author> Drescher, G. </author> <year> (1991), </year> <title> Made Up Minds: A Constructivist Approach to Artificial Intelligence, </title> <publisher> MIT Press. </publisher>
Reference: <author> Dzeroski, S. & Lavrac, N. </author> <year> (1994), </year> <title> Inductive Logic Programming, </title> <address> Chichester, Eng-land: </address> <publisher> Ellis Horwood. </publisher>
Reference-contexts: The teleo-operator model is used to describe durative actions in a compact manner and to focus on predicting relevant features of the world. Exploration serves to distinguish the causes of failures. Finally, the teleo-operator formalism allows TRAIL to use the inductive logic programming framework <ref> (Dzeroski & Lavrac 1994, Muggleton 1992) </ref> to learn despite the presence of noisy examples.
Reference: <author> Fikes, R. E. & Nilsson, N. J. </author> <year> (1971), </year> <title> "Strips: A new approach to the application of theorem proving to problem solving", </title> <booktitle> Artificial Intelligence 2, </booktitle> <pages> 189-208. </pages>
Reference-contexts: Since most features of an environment are irrelevant to any particular task, such prediction is expensive and un necessary. * Actions can be durative, such as "walk" or "turn," as well as discrete. Durative actions cannot be described well using the state-action-next state model of traditional STRIPS operators <ref> (Fikes & Nilsson 1971) </ref>.
Reference: <author> Gil, Y. </author> <year> (1992), </year> <title> Acquiring Domain Knowledge for Planning by Experimentation, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Lozano-Perez, T., Mason, M. T. & Taylor, R. </author> <year> (1984), </year> <title> "Automatic synthesis of fine-motion strategies for robots", </title> <journal> International Journal of Robotics Research 3(1), </journal> <volume> 3 - 24. </volume>
Reference: <author> Maes, P. </author> <year> (1991), </year> <title> Adaptive action selection, </title> <booktitle> in "Proceedings of the Thirteenth Annual Converence of the Cognitive Science Society". </booktitle>
Reference: <author> Mahadevan, S. </author> <year> (1992), </year> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions, </title> <editor> in D. Sleeman & P. Edwards, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Ninth International Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 290 - 299. </pages>
Reference: <author> Muggleton, S., ed. </author> <year> (1992), </year> <title> Inductive Logic Programming, </title> <publisher> Academic Press. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> (1994), </year> <title> "Teleo-reactive programs for agent control", </title> <journal> Journal of Artificial Intelligence Research 1, </journal> <volume> 139 - 158. </volume>
Reference-contexts: We have designed an autonomous agent that successfully learns in settings having all the above properties. Our agent, which we call TRAIL, 1 is based on the teleo-reactive agent described by Benson & Nilsson (1994). Teleo-reactive trees <ref> (Nilsson 1994) </ref> form a framework for integrating the processes 1 Teleo-Reactive Agent with Inductive Learning 1 of planning, execution, and learning. The teleo-operator model is used to describe durative actions in a compact manner and to focus on predicting relevant features of the world.
Reference: <author> Sablon, G. & Bruynooghe, M. </author> <year> (1994), </year> <title> Using the event calculus to integrate planning and learning in an intelligent autonomous agent, </title> <editor> in C. Backstrom & E. Sandewall, eds, </editor> <booktitle> "Current Trends in AI Planning", </booktitle> <publisher> IOS Press, </publisher> <pages> pp. 254 - 265. </pages>
Reference: <author> Shen, W.-M. </author> <year> (1989), </year> <title> Learning from the Environment Based on Actions and Percepts, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Shen, W.-M. </author> <year> (1994), </year> <title> Autonomous Learning from the Environment, </title> <publisher> Computer Science Press, W.H. Freeman and Company. </publisher>
Reference-contexts: Throughout this paper, we denote the TOP defined by action a j and literal i by a j 4 The Cycle of Execution, Learning, and Planning The overall organization of TRAIL is similar to that of LIVE <ref> (Shen 1994) </ref> and Wang and Carbonell's (1994) learner. Its behavior arises from the interaction of three distinct processes: a planner, a reactive executor, and an action model learner, connected in a plan-execute-learn cycle.
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> "Learning to predict by methods of temporal difference", </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference: <author> Teo, P. </author> <year> (1992), </year> <note> Botworld, unpublished manual. </note>
Reference-contexts: both cases as negative examples and relies on the learner to produce a correct preimage. 7 Preliminary Experimental Results We have investigated and implemented these ideas in a robotic agent that operates in a variety of simulated worlds, currently including an office delivery environment and a simulated construction domain, Botworld <ref> (Teo 1992) </ref>. Botworld is a continuous domain in which there are bars that can be picked up, moved around, and welded together. It includes durative actions, action failures of all of the types discussed in Section 6, and noise in the form of perceptual aliasing.
Reference: <author> Vere, S. A. </author> <year> (1980), </year> <title> "Multilevel counterfactuals for generalizations of relational concepts and productions", </title> <booktitle> Artificial Intelligence 14(2), </booktitle> <pages> 139-164. </pages>
Reference: <author> Wang, X. & Carbonell, J. </author> <year> (1994), </year> <title> "Learning by observation and practice: Towards real applications of planning systems", </title> <booktitle> AAAI Fall Symposium on Planning and Learning: On to Real Applications. </booktitle>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active perception and reinforcement learning, </title> <booktitle> in "Machine Learning: Proceedings of the Seventh International Conference", </booktitle> <address> Austin, TX., </address> <pages> pp. 179-188. 14 </pages>
Reference-contexts: complicated environments, predicting each world state that will be visited during an action execution is computationally intractable. * The causes of an action failure are often ambiguous, an issue which is not discussed in most other learning work. * Actions may not always produce consistent effects, due to perceptual aliasing <ref> (Whitehead & Ballard 1990) </ref>, run-time execution failures, or the actions of other agents. All three produce noise in the form of mislabeled examples of execution success or failure. We have designed an autonomous agent that successfully learns in settings having all the above properties.
References-found: 23

