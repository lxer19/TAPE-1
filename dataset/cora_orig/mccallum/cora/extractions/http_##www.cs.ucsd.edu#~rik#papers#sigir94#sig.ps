URL: http://www.cs.ucsd.edu/~rik/papers/sigir94/sig.ps
Refering-URL: http://www.cs.ucsd.edu/~rik/bibliography3_5.html
Root-URL: http://www.cs.ucsd.edu
Email: bbartell@eb.com  gary@cs.ucsd.edu  rik@cs.ucsd.edu  
Title: Automatic Combination of Multiple Ranked Retrieval Systems  
Author: Brian T. Bartelly Garrison W. Cottrellz Richard K. Belewz 
Address: 3252 Holiday Court, Ste. 208 La Jolla, California 92037  La Jolla, California 92093-0114  
Affiliation: yAdvanced Technology Group Encylopdia Britannica, Inc.  zDepartment of Computer Science Engineering-0114 University of California, San Diego  
Abstract: Retrieval performance can often be improved significantly by using a number of different retrieval algorithms and combining the results, in contrast to using just a single retrieval algorithm. This is because different retrieval algorithms, or retrieval experts, often emphasize different document and query features when determining relevance and therefore retrieve different sets of documents. However, it is unclear how the different experts are to be combined, in general, to yield a superior overall estimate. We propose a method by which the relevance estimates made by different experts can be automatically combined to result in superior retrieval performance. We apply the method to two expert combination tasks. The applications demonstrate that the method can identify high performance combinations of experts and also is a novel means for determining the combined effectiveness of experts.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Brian T. Bartell. </author> <title> Optimizing Ranking Functions: A Connectionist Approach to Adaptive Information Retrieval. </title> <type> PhD thesis, </type> <institution> Department of Computer Science & Engineering, The University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: In the next section, we outline our method for automatically combining experts. The approach is based on the general rank optimization method developed in previous work <ref> [1] </ref> [2]. Sections 3 and 4 present the results of two applications, Section 5 compares the method to an alternative, and Section 6 discusses the method's strengths and limitations. 2 Expert Combination Algorithm A number of methods for combining retrieval experts have been proposed. <p> The goal of our method is to automatically determine values for these parameters so that the overall estimates result in the best ranking of documents possible. Though we emphasize the linear model here, non-linear models (neural networks) have yielded positive results <ref> [1] </ref>. 2.2 Optimizing Ranking Performance To find values for the parameters, we use a criterion which measures how well the model is ranking documents for a set of training queries. We numerically optimize the criterion in order to heuristically search for parameter values which result in superior ranking performance. <p> Criterion J is not differentiable everywhere (e.g., J is singular if R fi;q (d) = R fi;q (d 0 ), for d q d 0 ) and therefore might be resistant to gradient-based optimization. However, other work has demonstrated that the singularities do not generally cause difficulties <ref> [1] </ref>, and Conjugate Gradient has been quite effective throughout our experiments. The goal of the optimization is to find parameter values such that the system ranks document d higher than document d 0 whenever d is preferred by the user to d 0 . <p> Restricting optimization to only the top-ranked 15 documents effectively ignores these problematic documents. This behavior appears to be a characteristic of the linear model other work has demonstrated that some non-linear neural network models do not suffer from this sensitivity to problematic documents <ref> [1] </ref>. 5 Comparison to a Supervised Learning Algorithm There are alternatives to the particular rank order criterion we have used in this work.
Reference: 2. <author> Brian T. Bartell, Garrison W. Cottrell, and Richard K. Belew. </author> <title> Learning the optimal parameters in a ranked retrieval system using multi-query relevance feedback. </title> <booktitle> In Proceedings of the Symposium on Document Analysis and Information Retrieval, </booktitle> <address> Las Vegas, </address> <year> 1994. </year> <note> in press. </note>
Reference-contexts: In the next section, we outline our method for automatically combining experts. The approach is based on the general rank optimization method developed in previous work [1] <ref> [2] </ref>. Sections 3 and 4 present the results of two applications, Section 5 compares the method to an alternative, and Section 6 discusses the method's strengths and limitations. 2 Expert Combination Algorithm A number of methods for combining retrieval experts have been proposed. <p> We have selected this variation of Guttman's Point Alienation because it is amenable to gradient-based optimization, it is general enough to handle arbitrary user preference relations, and because the measure has been shown to correspond well empirically to average precision, a more standard measure of retrieval effectiveness in Information Retrieval <ref> [2] </ref>. We have chosen not to explicitly optimize average precision because it is a discrete measure and is therefore not amenable to gradient-based optimization. However, methods do exist for optimizing such non-differentiable criteria, and some methods have been applied to tasks in Information Retrieval [7].
Reference: 3. <author> Nicholas J. Belkin, C. Cool, W. Bruce Croft, and James P. Callan. </author> <title> Effect of multiple query representations on information retrieval system performance. </title> <booktitle> In Proc. SIGIR 1993, </booktitle> <pages> pages 339-346, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, Saracevic & Kantor [13] demonstrated that the odds of a document being relevant to a query increase monotonically with the number of experts that retrieve the document. A possible explanation is offered by a recent study by Belkin et al. <ref> [3] </ref>. This study demonstrated that the retrieval results based on different query formulations can be combined to result in performance that is better than any single formulation. Belkin et al. suggest that each expert's relevance estimates may be interpreted as sources of evidence of the true relevance. <p> We will demonstrate that this automatically derived combination of experts performs much better than any of the individual experts when applied to two different problems. 1 In addition, we demonstrate that the method is a useful and novel analytic tool. Belkin et al. <ref> [3] </ref> made the important observation that a different query formulation (or expert) can contribute to improved performance in the combined system even when it performs poorly on its own. <p> The system designer evaluates one or more candidate combination strategies using a set of queries with relevance-tagged documents, and the strategy resulting in the best performance is selected. Typical of this approach are: Belkin et al.'s <ref> [3] </ref> analysis of alternative query formulations on the TREC database, Lewis & Croft's [11] comparison of term, phrase, and concept clustering approaches, and Turtle & Croft's [16] inference network framework, in which manual search is used to find the best weighted average of experts. <p> Typical approaches for evaluating a retrieval method's performance evaluate the method in isolation. This allows for comparison between methods, but does give the system designer the information needed to decide whether a particular method should be included in the system. Echoing Belkin et al.'s <ref> [3] </ref> findings, we have demonstrated that experts which perform poorly in isolation can still contribute positively to a combined solution. Nevertheless, the issue of analyzing combinations of experts is far from resolved.
Reference: 4. <author> I. Borg and J. Lingoes. </author> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The criterion J defined in equation (2) is a particular mathematical formalization of this goal. There are numerous alternatives, however, to the particular criterion we have used. For example, other criteria have been proposed to solve similar rank-order problems in the field of Multidimensional Scaling (MDS) <ref> [4] </ref>. <p> We report average precision because it is a standard measure for comparison in Information Retrieval and because it can be difficult to draw conclusions about absolute system performance from the criterion scores (cf. <ref> [4, p. 47] </ref>).
Reference: 5. <author> Edward A. Fox, M. Prabhakar Koushik, Joseph Shaw, Russell Modlin, and Durgesh Rao. </author> <title> Combining evidence from multiple searches. </title> <editor> In Donna K. Harman, editor, </editor> <booktitle> The First Text REtrieval Conference (TREC-1), </booktitle> <pages> pages 319-328, </pages> <month> March </month> <year> 1993. </year> <note> NIST Special Publication 500-207. </note>
Reference-contexts: A limitation of manual search is that typically only a small number of strategies are examined. Alternatives to search include Fox et al.'s <ref> [5] </ref> Similarity Merge approach, in which a document's overall relevance score is the maximum of all expert's relevance estimates, and Thompson's [15] evaluation method in which each expert is weighted based on its individual utility. In this second approach, each expert is evaluated independently to determine its relative performance level.
Reference: 6. <author> Norbert Fuhr and Chris Buckley. </author> <title> A probabilistic learning approach for document indexing. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 223-248, </pages> <year> 1991. </year>
Reference-contexts: Minimizing this criterion will make the system's relevance estimates more like the true probabilities of relevance. A similar squared error criterion has been used recently by Fuhr & Buckley <ref> [6] </ref>, though they use the criterion to optimize relevance estimates based on single terms rather than to optimize the overall retrieval performance as we do here.
Reference: 7. <author> Michael Gordon. </author> <title> Probabilistic and genetic algorithms in document retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 31(10), </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: We have chosen not to explicitly optimize average precision because it is a discrete measure and is therefore not amenable to gradient-based optimization. However, methods do exist for optimizing such non-differentiable criteria, and some methods have been applied to tasks in Information Retrieval <ref> [7] </ref>. This may be a fruitful direction for further research. 3 Learning Weights on Phrases and Terms Our first application of the method involves combining two experts.
Reference: 8. <author> L. Guttman. </author> <title> What is not what in statistics. </title> <journal> The Statistician, </journal> <volume> 26 </volume> <pages> 81-107, </pages> <year> 1978. </year>
Reference-contexts: We numerically optimize the criterion in order to heuristically search for parameter values which result in superior ranking performance. The criterion we use is a variation on Guttman's Point Alienation <ref> [8] </ref>, a statistical measure of rank correlation.
Reference: 9. <author> Donna Harman. </author> <title> Overview of the first Text REtrieval Conference. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 36-48, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Part of the benefit may be due to the increased recall resulting from a combination of experts. It has been observed that different experts often retrieve different documents; e.g., Katzer et al. [10] found that very different sets of documents were retrieved using different document representation methods, and Harman <ref> [9] </ref> observed that the systems in the 1993 Text REtrieval Conference (TREC-1) retrieved substantially different sets of documents, even though most systems performed at the same approximate level.
Reference: 10. <author> J. Katzer, M. J. McGill, J. A. Tessier, W. Frakes, and P. DasGupta. </author> <title> A study of the overlap among document representations. </title> <journal> Information Technology: Research and Development, </journal> <volume> 1(4) </volume> <pages> 261-274, </pages> <month> Oct </month> <year> 1982. </year>
Reference-contexts: Part of the benefit may be due to the increased recall resulting from a combination of experts. It has been observed that different experts often retrieve different documents; e.g., Katzer et al. <ref> [10] </ref> found that very different sets of documents were retrieved using different document representation methods, and Harman [9] observed that the systems in the 1993 Text REtrieval Conference (TREC-1) retrieved substantially different sets of documents, even though most systems performed at the same approximate level.
Reference: 11. <author> David D. Lewis and W. Bruce Croft. </author> <title> Term clustering of syntactic phrases. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <address> Brussels, </address> <month> Sept </month> <year> 1990. </year>
Reference-contexts: The system designer evaluates one or more candidate combination strategies using a set of queries with relevance-tagged documents, and the strategy resulting in the best performance is selected. Typical of this approach are: Belkin et al.'s [3] analysis of alternative query formulations on the TREC database, Lewis & Croft's <ref> [11] </ref> comparison of term, phrase, and concept clustering approaches, and Turtle & Croft's [16] inference network framework, in which manual search is used to find the best weighted average of experts. A limitation of manual search is that typically only a small number of strategies are examined.
Reference: 12. <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: We use Conjugate Gradient, a gradient-based numerical optimization technique, to minimize J. Conjugate Gradient is preferred over other gradient methods because it is parameter-free for most applications and source code is readily available (see <ref> [12] </ref>). Criterion J is not differentiable everywhere (e.g., J is singular if R fi;q (d) = R fi;q (d 0 ), for d q d 0 ) and therefore might be resistant to gradient-based optimization.
Reference: 13. <author> T. Saracevic and P. Kantor. </author> <title> A study of information seeking and retrieving. III. Searchers, searches, overlap. </title> <journal> Journal of the ASIS, </journal> <volume> 39(3) </volume> <pages> 197-216, </pages> <year> 1988. </year>
Reference-contexts: Since different experts can retrieve different documents, the perceived performance of the combined system may be higher because of the increased levels of recall that are attainable. Not all of the performance benefit can be attributed to enhanced recall in the combined system. For example, Saracevic & Kantor <ref> [13] </ref> demonstrated that the odds of a document being relevant to a query increase monotonically with the number of experts that retrieve the document. A possible explanation is offered by a recent study by Belkin et al. [3].
Reference: 14. <author> Paul Thompson. </author> <title> A combination of expert opinion approach to probabilistic information retrieval, part 1: The conceptual model. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 26(3) </volume> <pages> 371-382, </pages> <year> 1990. </year>
Reference-contexts: Weighting experts based on their individual performance does not clearly address how they will perform together. Indeed, Thompson found that the combined experts, weighted by performance level, performed no better than a combination using uniform weights. Another alternative is Thompson's Combination of Expert Opinion (CEO) model <ref> [14] </ref>. CEO is a Bayesian model of the combination of evidence from a set of experts. Experts are combined to yield a more reliable overall estimate of a document's relevance to a query. <p> The method is an alternative to a number of techniques for combining experts that have been proposed recently, in particular the manual search method, weighting by individual performance, and Thompson's CEO method <ref> [14] </ref>. Each of these alternatives has certain strengths, such as their foundation in probability theory for combining evidence and their potential intuitive appeal. However, the proposed method has certain alternative advantages. Foremost, the method is applicable to a wide range of possible text retrieval approaches.
Reference: 15. <author> Paul Thompson. </author> <title> Description of the PRC CEO algorithm for TREC. </title> <editor> In Donna K. Harman, editor, </editor> <booktitle> The First Text REtrieval Conference (TREC-1), </booktitle> <pages> pages 337-342, </pages> <month> March </month> <year> 1993. </year> <note> NIST Special Publication 500-207. </note>
Reference-contexts: A limitation of manual search is that typically only a small number of strategies are examined. Alternatives to search include Fox et al.'s [5] Similarity Merge approach, in which a document's overall relevance score is the maximum of all expert's relevance estimates, and Thompson's <ref> [15] </ref> evaluation method in which each expert is weighted based on its individual utility. In this second approach, each expert is evaluated independently to determine its relative performance level. During retrieval, the experts' relevance estimates are combined in proportion to their performance level. <p> Experts are combined to yield a more reliable overall estimate of a document's relevance to a query. The method assumes that the experts are statistically independent, though work continues to make this model more generally applicable <ref> [15] </ref>.
Reference: 16. <author> Howard Turtle and W. Bruce Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 187-222, </pages> <month> july </month> <year> 1991. </year>
Reference-contexts: Typical of this approach are: Belkin et al.'s [3] analysis of alternative query formulations on the TREC database, Lewis & Croft's [11] comparison of term, phrase, and concept clustering approaches, and Turtle & Croft's <ref> [16] </ref> inference network framework, in which manual search is used to find the best weighted average of experts. A limitation of manual search is that typically only a small number of strategies are examined.
Reference: 17. <author> S. K. M. Wong, Y. J. Cai, and Y. Y. Yao. </author> <title> Computation of term associations by a neural network. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: It is assumed that we either know the relevance of all retrieved documents for the training queries, or we restrict the optimization to those documents that are known. This knowledge is represented by the binary relation q (a notation adopted from Wong & Yao <ref> [17] </ref>). q is a preference relation over document pairs, interpreted as: d q d 0 () the user prefers d to d 0 (3) This preference relation is an alternative to the more standard two-valued relevance in which a document is either relevant or irrelevant to a query.
References-found: 17

