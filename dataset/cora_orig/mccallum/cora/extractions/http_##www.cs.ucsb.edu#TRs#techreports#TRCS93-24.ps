URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS93-24.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Maya: A Simulation Platform for Parallel Architectures and Distributed Shared Memories  
Author: Divyakant Agrawal Manhoi Choy Hong Va Leong Ambuj K. Singh 
Keyword: modeling of parallel architectures, parallel and distributed simulation, parallel programming, weak memories  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California at Santa Barbara  
Abstract: Maya is a simulation platform for evaluating the performance of parallel programs on parallel architectures with different memory coherence protocols. It uses the communication library PVM to ensure portability. Rapid prototyping of different memory protocols of varying degrees of coherence is possible and the impact of these protocols on the performance of application programs can be studied. We describe the design of Maya and the simulation mechanism briefly. Some of the performance results on architectural simulation with different memory coherence protocols are presented. Parallel discrete event simulation techniques are adopted for the execution-driven simulation of parallel architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [ABHN91] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <pages> pages 9-30. </pages> <publisher> LNCS, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent [Lam79]. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs <ref> [ABHN91, AH90, DSB86, GLL + 90, LS88] </ref>. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows. <p> These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs [ABHN91, AH90, DSB86, GLL + 90, LS88]. Maya is capable of simulating the execution of a number of such memories including causal memory <ref> [ABHN91] </ref>, and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows. An overview of the design of Maya is presented in Section 2. <p> Several application programs including the Gaussian elimination and matrix inversion (Gaussian inverse), all pairs shortest path, the traveling salespersons problem, the Jocabi iterative synchronous linear equation solver <ref> [ABHN91] </ref>, and the Cholesky factorization from the SPLASH benchmark [SWG91] have been used. Preliminary results of the performance of some of these coherence protocols and application programs when used in the native mode are obtained from a network of Sun workstations and from an Intel Paragon [Maya93].
Reference: [AH90] <author> S.V. Adve and M.D. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent [Lam79]. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs <ref> [ABHN91, AH90, DSB86, GLL + 90, LS88] </ref>. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows.
Reference: [AHH88] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache performance of operating system and multiprocessing workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Trace-driven simulation systems <ref> [AHH88, BKW90, EKKL90] </ref> have been used for a long time. They are fast but suffer from a low accuracy. Functional simulators such as the ASIM simulation sys tem [CLN90] provide a high degree of accuracy of a simulated execution with respect to the real-time execution.
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Execution-driven simulators, on the other hand, represent a compromise between the two extremes. These simulators sacrifice the speed of trace-driven simulations for greater accuracy. The representatives of this class are the Tango system from Stanford [DGH91], the Proteus system from MIT <ref> [BDCW91] </ref>, and the Wisconsin Wind Tunnel [RHL + 93]. These simulation systems provide a reasonable level of accuracy between simulated and real-time executions with a moderate slowdown of about one to two orders of magnitude. <p> The simulator then selects the access or simulation event with the smallest simulated time and carries out the relevant computation or communication. The original implementation of Tango is only available on SGI and DEC workstations and is not portable. Proteus <ref> [BDCW91] </ref> uses lightweight threads to reduce the overhead of context switching and data copying. All data of user processes are encapsulated into a single large address space and shared accesses are replaced by function calls to the simulator during the pre-processing phase. <p> The results obtained from the simulation mode are compared with the results obtained from the native mode. Finally, the performance of Maya is evaluated by comparing it with another simulation tool Proteus <ref> [BDCW91] </ref> and by varying the degree of concurrency. 4.1 Results of Architectural Simulation The input to the Gaussian inverse problem is a matrix of size N fi N and the output is the inverse of the input matrix.
Reference: [BKW90] <author> A. Borg, R.E. Kessler, and D.W. Wall. </author> <title> Generation and analysis of very long address traces. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 270-279, </pages> <year> 1990. </year>
Reference-contexts: Trace-driven simulation systems <ref> [AHH88, BKW90, EKKL90] </ref> have been used for a long time. They are fast but suffer from a low accuracy. Functional simulators such as the ASIM simulation sys tem [CLN90] provide a high degree of accuracy of a simulated execution with respect to the real-time execution.
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In 14 Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <year> 1993. </year>
Reference-contexts: M A C H I N E C O M M U N I C A T I O N N E T W O R K Target Machine Network Simulation Module Event Scheduler Message Delay Model Communication 3.1 Modeling Parallel Architectures Several models exist for parallel architectures and computations <ref> [FW78, Gib89, Val90, CKP + 93] </ref>. The traditional PRAM models [FW78, Gib89] are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps. <p> If it has, the system proceeds to the next superstep. Otherwise, the next interval is allocated to the unfinished superstep. This model is simple to implement, but it is only appropriate for parallel programs that conform to the model. The LogP model <ref> [CKP + 93] </ref> characterizes a parallel architecture by the communication delay L, the communication overhead o, the communication bandwidth g, and the number of processors P . Various aspects of the architecture are approximated by these four parameters. Our architectural simulation model is similar to that of the LogP model.
Reference: [CLN90] <author> D. Chaiken, B.H. Lim, and D. Nussbaum. </author> <title> ASIM User Manual. </title> <journal> ALEWIFE Systems Memo#13, </journal> <month> August </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Trace-driven simulation systems [AHH88, BKW90, EKKL90] have been used for a long time. They are fast but suffer from a low accuracy. Functional simulators such as the ASIM simulation sys tem <ref> [CLN90] </ref> provide a high degree of accuracy of a simulated execution with respect to the real-time execution. However, these simulators are slow with a slowdown generally in the range of two to three orders of magnitude. Execution-driven simulators, on the other hand, represent a compromise between the two extremes.
Reference: [CM79] <author> K.M. Chandy and J. Misra. </author> <title> Distributed simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(9) </volume> <pages> 440-452, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: This can be guaranteed by observing the input waiting rule and the 7 output waiting rule as in the conservative approach for parallel discrete event simulation <ref> [CM79] </ref>. In Maya, the input waiting rule is enforced by requiring the manager to wait until it receives a message from each memory subsystem. <p> Conservative approaches in parallel discrete event simulation may lead to deadlocks. One way to avoid deadlocks is to flush all the communication channels periodically with null messages <ref> [CM79] </ref>. In Maya, the deadlock resolution scheme is attached to the centralized manager. The deadlock resolution scheme is a combination of deadlock avoidance and deadlock detection. The manager keeps track of the number of outstanding messages of each memory subsystem.
Reference: [DGH91] <author> H. Davis, S.R. Goldschmidt, and J. Hennessy. </author> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Execution-driven simulators, on the other hand, represent a compromise between the two extremes. These simulators sacrifice the speed of trace-driven simulations for greater accuracy. The representatives of this class are the Tango system from Stanford <ref> [DGH91] </ref>, the Proteus system from MIT [BDCW91], and the Wisconsin Wind Tunnel [RHL + 93]. These simulation systems provide a reasonable level of accuracy between simulated and real-time executions with a moderate slowdown of about one to two orders of magnitude. <p> Only accesses that are of interest to the simulation are considered and their effects are integrated into the state of the simulation. Complete program dependencies and program output are reproduced. Tango <ref> [DGH91] </ref>, an example of such a simulator, models each simulated user process by a Unix process and uses semaphores for process synchronization. Shared accesses to memory are replaced with function calls to the simulator, which maintains all the timing and scheduling information.
Reference: [DSB86] <author> Michael Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent [Lam79]. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs <ref> [ABHN91, AH90, DSB86, GLL + 90, LS88] </ref>. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows.
Reference: [EKKL90] <author> S.J. Eggers, D.R. Keppel, E.J. Koldinger, and H.M. Levy. </author> <title> Technique for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-47, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Trace-driven simulation systems <ref> [AHH88, BKW90, EKKL90] </ref> have been used for a long time. They are fast but suffer from a low accuracy. Functional simulators such as the ASIM simulation sys tem [CLN90] provide a high degree of accuracy of a simulated execution with respect to the real-time execution.
Reference: [Fuj90] <author> R. Fujimoto. </author> <title> Parallel distributed discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Different scheduling schemes require different time mapping functions. For the applications we are simulating, there is only a single user process residing at each node. Consequently, function time mapping simply models the overheads and delays of a memory subsystem. Event scheduling follows the framework of parallel discrete event simulation <ref> [Fuj90] </ref>. Owing to the asynchrony of the simulated system, one has to make sure that events are scheduled in the same order as in the actual execution.
Reference: [FW78] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: M A C H I N E C O M M U N I C A T I O N N E T W O R K Target Machine Network Simulation Module Event Scheduler Message Delay Model Communication 3.1 Modeling Parallel Architectures Several models exist for parallel architectures and computations <ref> [FW78, Gib89, Val90, CKP + 93] </ref>. The traditional PRAM models [FW78, Gib89] are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps. <p> The traditional PRAM models <ref> [FW78, Gib89] </ref> are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps.
Reference: [Gib89] <author> P.B. Gibbons. </author> <title> A more practical PRAM model. </title> <booktitle> In Proceedings of the 1st Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <year> 1989. </year>
Reference-contexts: M A C H I N E C O M M U N I C A T I O N N E T W O R K Target Machine Network Simulation Module Event Scheduler Message Delay Model Communication 3.1 Modeling Parallel Architectures Several models exist for parallel architectures and computations <ref> [FW78, Gib89, Val90, CKP + 93] </ref>. The traditional PRAM models [FW78, Gib89] are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps. <p> The traditional PRAM models <ref> [FW78, Gib89] </ref> are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J.L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent [Lam79]. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs <ref> [ABHN91, AH90, DSB86, GLL + 90, LS88] </ref>. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows.
Reference: [Lam78] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The target machines being simulated are the same network of Sun workstations and an Intel Paragon. The application program is run with n user processes. In a network of Sun workstations, an Ethernet 2 This causal order reflects the happen-before relations <ref> [Lam78] </ref> between the read and write operations to shared memory locations. 10 is used to connect a set of Sparc LX workstations. Each user process and the memory subsystem it communicates with are assigned to one workstation so that there are a total of n workstations.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multi-process programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent <ref> [Lam79] </ref>. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs [ABHN91, AH90, DSB86, GLL + 90, LS88]. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88].
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The trace contains all the required accesses to the shared memory by the execution threads of the parallel program. This trace is used as an input to the simulator which in turn simulates the target architecture or a protocol (e.g., a distributed shared memory protocol <ref> [LH89] </ref>) running on the target architecture. The input traces can also be generated statistically from some benchmarks or some measured distributions. These simulators do not actually execute the parallel program or the protocol. The input program trace determines the runtime dependency of the simulated program statically. <p> Currently the interface between the users and the memory subsystem is based on messages. The memory subsystem executes the underlying distributed shared memory protocol by communicating with the memory subsystems on other nodes. The distributed static manager scheme <ref> [LH89] </ref> is implemented as the base case memory protocol (referred to as "atomic memory" protocol in this paper) for both comparison and testing purposes. The memory subsystem in Maya is extensible. <p> In the case of atomic memory, all reads and writes are blocking, i.e., the user processes issuing these operations are blocked until the operations are successfully performed. Copies of the memory may migrate dynamically and are kept track of by a distributed directory <ref> [LH89] </ref>. In the case of causal memory, each memory subsystem keeps a copy of the shared memory. Reads to the shared memory are served locally.
Reference: [LL90] <author> Y.B. Lin and E.D. Lazowska. </author> <title> Exploiting lookahead in parallel simulation. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(4) </volume> <pages> 457-469, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Furthermore, if deadlock does occur, the number of outstanding messages among the memory subsystems will eventually become zero. Subsequently, the manager will be able to detect the deadlock and resolve it by sending another value of T . To compute the largest possible value of T , lookahead techniques <ref> [LL90] </ref> are adopted. In particular, suppose the smallest send timestamp of all the messages the manager has received is t and the minimum message delay on the target architecture is d.
Reference: [LS88] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Thus a major thrust of Maya has been the examination of different protocols for shared memories, especially the implementations of memories that are not sequentially consistent [Lam79]. These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs <ref> [ABHN91, AH90, DSB86, GLL + 90, LS88] </ref>. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows. <p> These weaker memories have a lower latency and when used correctly, can enhance the performance of parallel programs [ABHN91, AH90, DSB86, GLL + 90, LS88]. Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory <ref> [LS88] </ref>. Preliminary evaluation results for a number of user applications appear in [Maya93]. This paper is organized as follows. An overview of the design of Maya is presented in Section 2. In Section 3, the simulation environment and the modeling of parallel architectures are discussed.
Reference: [Maya93] <institution> Evaluating weak memories with Maya. </institution> <type> Technical report, </type> <year> 1993. </year>
Reference-contexts: Maya is capable of simulating the execution of a number of such memories including causal memory [ABHN91], and pipelined random access memory [LS88]. Preliminary evaluation results for a number of user applications appear in <ref> [Maya93] </ref>. This paper is organized as follows. An overview of the design of Maya is presented in Section 2. In Section 3, the simulation environment and the modeling of parallel architectures are discussed. Section 4 contains some architectural simulation results and a performance comparison with Proteus. <p> The memory subsystem in Maya is extensible. Currently, we have developed a 4 library to support distributed shared memory protocols based on sequentially consistent memory, causal memory, and pipelined random access memory <ref> [Maya93] </ref>. The primary responsibility of the communication subsystem is to facilitate communication among the memory subsystems, which cooperate with one another to implement distributed shared memory. The communication subsystem in Maya is based on the message passing library PVM [Sun90]. <p> Preliminary results of the performance of some of these coherence protocols and application programs when used in the native mode are obtained from a network of Sun workstations and from an Intel Paragon <ref> [Maya93] </ref>. In this section, results from the simulation mode are presented. These results are based on one user application: a Gaussian inverse algorithm, two kinds of coherence protocol: atomic memory and causal memory, and two different architectures: a network of Sun workstations and an Intel Paragon.
Reference: [RHL + 93] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Recent interest in high performance computing has led to a greater need for adequate simulation tools and models for evaluating parallel architectures and parallel programs on these architectures. Several simulation systems have been developed <ref> [AHH88, CLN90, EKKL90, BDCW91, DGH91, RHL + 93] </ref> and based on the granularity of the simulator, the simulation techniques used in these simulators can be broadly classified into three categories: trace-driven or statistical simulation, functional or instruction level simulation, and execution-driven simulation with direct execution. <p> Execution-driven simulators, on the other hand, represent a compromise between the two extremes. These simulators sacrifice the speed of trace-driven simulations for greater accuracy. The representatives of this class are the Tango system from Stanford [DGH91], the Proteus system from MIT [BDCW91], and the Wisconsin Wind Tunnel <ref> [RHL + 93] </ref>. These simulation systems provide a reasonable level of accuracy between simulated and real-time executions with a moderate slowdown of about one to two orders of magnitude. In this paper, we describe an execution-driven simulation system, Maya 1 , that is being developed. <p> All data of user processes are encapsulated into a single large address space and shared accesses are replaced by function calls to the simulator during the pre-processing phase. Both Tango and Proteus implementations are intended for sequential or shared memory architectures. The Wisconsin Wind Tunnel (WWT) <ref> [RHL + 93] </ref> is a solitary example of an execution-driven simulation system that can be executed on a distributed memory machine. The system distinguishes between local and shared memory accesses by exploiting the ECC hardware of CM-5.
Reference: [Sun90] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Maya is a parallel programming system which supports execution-driven simulation on distributed memory architectures. Inspired by the success of WWT in exploiting multiple processors to speed up the simulation, Maya is intended to work primarily on a distributed memory platform. It is based on the PVM communication library <ref> [Sun90] </ref>, and can be ported to any environment that supports Unix and PVM. We have so far ported Maya to a network of Sun workstations and the Intel Paragon. Maya is intended to radically change the process of parallel program development on MIMD architectures. <p> The primary responsibility of the communication subsystem is to facilitate communication among the memory subsystems, which cooperate with one another to implement distributed shared memory. The communication subsystem in Maya is based on the message passing library PVM <ref> [Sun90] </ref>. Since PVM is available on most distributed memory architectures, Maya can be supported on a variety of hardware platforms. Another advantage of using PVM is that it can be used in a network of heterogeneous Unix compatible machines since PVM uses an external message passing standard.
Reference: [SWG91] <author> J.P. Singh, W.D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <type> Technical report, </type> <institution> Stanford, Computer Systems Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Several application programs including the Gaussian elimination and matrix inversion (Gaussian inverse), all pairs shortest path, the traveling salespersons problem, the Jocabi iterative synchronous linear equation solver [ABHN91], and the Cholesky factorization from the SPLASH benchmark <ref> [SWG91] </ref> have been used. Preliminary results of the performance of some of these coherence protocols and application programs when used in the native mode are obtained from a network of Sun workstations and from an Intel Paragon [Maya93]. In this section, results from the simulation mode are presented.
Reference: [Val90] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: M A C H I N E C O M M U N I C A T I O N N E T W O R K Target Machine Network Simulation Module Event Scheduler Message Delay Model Communication 3.1 Modeling Parallel Architectures Several models exist for parallel architectures and computations <ref> [FW78, Gib89, Val90, CKP + 93] </ref>. The traditional PRAM models [FW78, Gib89] are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model [Val90] views a computation as a sequence of supersteps. <p> The traditional PRAM models [FW78, Gib89] are not accurate enough for useful modeling since many important architectural aspects such as communication overheads are missing. The bulk-synchronous parallel computer model <ref> [Val90] </ref> views a computation as a sequence of supersteps. In each superstep, each component of the system is allocated a task consisting of local computation, message transmission, and message reception.
References-found: 25

