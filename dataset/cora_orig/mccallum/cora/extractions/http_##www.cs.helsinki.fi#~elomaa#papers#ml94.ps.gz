URL: http://www.cs.helsinki.fi/~elomaa/papers/ml94.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~elomaa/
Root-URL: 
Email: elomaa@cs.helsinki.fi  
Title: In Defense of C4.5: Notes on Learning One-Level Decision Trees  
Author: Tapio Elomaa P. O. Box (Teollisuuskatu ) 
Address: Helsinki, Finland  
Affiliation: Department of Computer Science  University of  
Pubnum: FIN-00014  
Abstract: We discuss the implications of Holte's recently-published article, which demonstrated that on the most commonly used data very simple classification rules are almost as accurate as decision trees produced by Quinlan's C4.5. We consider, in particular, what is the significance of Holte's results for the future of top-down induction of decision trees. To an extent, Holte questioned the sense of further research on multilevel decision tree learning. We go in detail through all the parts of Holte's study. We try to put the results into perspective. We argue that the (in absolute terms) small difference in accuracy between 1R and C4.5 that was witnessed by Holte is still significant. We claim that C4.5 possesses additional accuracy-related advantages over 1R. In addition we discuss the representativeness of the databases used by Holte. We compare empirically the optimal accuracies of multilevel and one-level decision trees and observe some significant differences. We point out several deficien cies of limited-complexity classifiers.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R., Ghosh, S., Imielinski, T., Iyer, B., and Swami, A. </author> <year> (1992). </year> <title> An interval classifier for database mining applications. </title> <editor> In L.-Y. Yuan (ed.), </editor> <booktitle> Proc. Eighteenth International Conference on Very Large Data Bases (pp. </booktitle> <pages> 560-573). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Applicability of machine learning approaches is currently restricted more by the lack of imagination than inherent limitations of the methodology. Decision tree learning has recently found success in completely new areas that differ from those areas that are covered in the UCI repository. In database mining <ref> (e.g., Agrawal et al., 1992) </ref> the data is organized by attributes and thus appears similar as in traditional machine learning data sets, but the database contents is typically different from that of traditional test data.
Reference: <author> Arbab, B. and Michie, D. </author> <year> (1985). </year> <title> Generating rules from examples. </title> <booktitle> In Proc. Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 631-633). </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Arikawa, S., Miyano, S., Shinohara, A., Kuhara, S., Muk-ouchi, Y., and Shinohara, T. </author> <year> (1993). </year> <title> A machine discovery from amino acid sequences by decision trees over regular patterns. </title> <journal> New Gener. Comput. </journal> <volume> 11: </volume> <pages> 361-375. </pages>
Reference-contexts: Manipulating text databases (e.g., Sakakibara, Misue, and Koshiba, 1993) and string learning <ref> (e.g., Arikawa et al., 1993) </ref> are two related applications that will undoubtedly become more and more important. In these applications there are no naturally arising attributes at all. Table 1: The baseline accuracies of the datasets (0Rw) and the accuracies of 1R and C4.5 when trained on the whole data.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Overpruning large decision trees. </title> <booktitle> In Proc. Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 764-769). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning3: </booktitle> <pages> 261-283. </pages>
Reference-contexts: Thus, their expressive power is of different magnitude than that of one-level decision trees. At same time the hypothesis space also grows substantially and the complexity of navigating in it increases considerably. In practical decision list learning there are also other sources of hypothesis space complexity <ref> (see e.g., Clark and Niblett, 1989) </ref>. Another possibility is that Holte intends maximizing the linearity of the learned decision trees as studied early on by Arbab and Michie (1985). In their approach the decision tree that has maximum degree of linearity is sought for from among the candidate hypotheses.
Reference: <author> Craven, M. and Shavlik, J. </author> <year> (1993). </year> <title> Learning to represent codons: a challenge problem for constructive induction. </title> <booktitle> In Proc. Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1319-1324). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They are domains that would require constructing new features from the attributes that are used to express the data. Even the naturally adaptive connectionist approaches lack adequate capabilities in this respect <ref> (Craven and Shavlik, 1993) </ref>. We leave this aspect of concept learning beyond our current discussion. 3 TEST DOMAINS Holte believes that the practical significance of his study is largely determined by whether or not the test data is naturally arising.
Reference: <author> Ehrenfeucht, A. and Haussler, D. </author> <year> (1989). </year> <title> Learning decision trees from random examples. </title> <booktitle> Information and Computation82: </booktitle> <pages> 231-246. </pages>
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used data sets. </title> <booktitle> Machine Learning11: </booktitle> <pages> 63-91. </pages>
Reference: <author> Iba, W. and Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <editor> In D. Sleeman and P. Edwards (eds.), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop (pp. </booktitle> <pages> 233-240). </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Iba, W., Wogulis, J., and Langley, P. </author> <year> (1988). </year> <title> Trading off simplicity and coverage in incremental concept learning. </title> <editor> In J. Laird (ed.), </editor> <booktitle> Proc. Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 73-79). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, because of the inherent tradeoff between simplicity and accuracy <ref> (Iba, Wogulis, and Langley, 1988) </ref> one is doomed to balance between these desired properties. Extreme simplicity, 1-level decision trees, for example, does not constitute a good basis for comprehensive representations.
Reference: <author> Karlsson, F. </author> <year> (1985). </year> <title> Automatic hyphenation of Finnish. </title> <note> In Computational Morphosyntax (pp. 93-136). Publication No. 13. </note> <institution> Department of General Linguistics, University of Helsinki. </institution>
Reference-contexts: In order to be able to find the remaining 5% of the hyphenation points the rule set has to be expanded by four more simple rules and a list of c. 250 exceptions <ref> (Karlsson, 1985) </ref>. The case of the preceding example is archetype of an artificial intelligence problem. High baseline success is achieved by simple means, but further advances require much more effort. It is these hard-earned final strides that validate the term artificial intelligence.
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1992). </year> <title> UCI Repository of Machine Learning Databases [machine-readable data repository]. </title> <institution> Department of Information and Computer Science, University of California at Irvine. </institution>
Reference-contexts: The test domains included a large portion of domains which have, over the years, been used as standard test cases for new inductive learning programs. The domains can nowadays be found at the University of California at Irvine repository of machine learning data sets <ref> (Murphy and Aha, 1992) </ref>. Holte's experiments revealed that the average classification accuracy achieved by C4.5 over 16 commonly used data sets is not much higher than that achieved by his learning algorithm 1R, which produces one-level decision trees.
Reference: <author> Murphy, P. and Pazzani, M. </author> <year> (1994). </year> <title> Exploring the decision forest: an empirical investigation of Occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research1: </journal> <pages> 257-275. </pages>
Reference: <author> Nunez, M. </author> <year> (1988). </year> <title> Economic induction: a case study. </title>
Reference: <editor> In D. Sleeman (ed.), </editor> <booktitle> Proc. Third European Working Session on Learning (pp. </booktitle> <pages> 139-145). </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference: <author> Quinlan, R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning1: </booktitle> <pages> 81-106. </pages>
Reference-contexts: Surely, it has been known that the latter in many cases is not a good reference point. In more recent experiments, one can consider that C4.5's ancestor ID3 <ref> (Quinlan, 1986) </ref> has often acted in the role of a benchmark accuracy/tree complexity measure. <p> The simplicity first methodology is not exactly new, not even in decision tree learning. For instance, ID3 aims to work by a kind of a simplicity first methodology. Recall that ID3 originally pruned trees in construction time, not in a separate pruning phase <ref> (Quinlan, 1986) </ref>. Thus, ID3 in principle only considers as complex trees as are required to fulfill the confidence determined by its parameter (s).
Reference: <author> Quinlan, R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> Int. J. Man-Mach. Stud. </journal> <volume> 27: </volume> <pages> 221-234. </pages>
Reference-contexts: Very large decision trees undoubtedly are hard for humans to grasp, but C4.5 provides an excellent remedy to this: convert the tree into a set of rules (typically with the cost of a slight loss of accuracy) <ref> (Quinlan, 1987) </ref>. The independent rules are much smaller entities and easily comprehended by humans. Based on the positive experiences on 1R Holte turns to advocate the simplicity first methodology as a new research paradigm for machine learning.
Reference: <author> Quinlan, R. </author> <year> (1988). </year> <title> An empirical comparison of genetic and decision-tree classifiers. </title> <editor> In J. Laird (ed.), </editor> <booktitle> Proc. Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 135-141). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In a mathematical model familiar functions tend to be used. In the simplest form familiar functions are Boolean functions. They have been used in showing that certain functions are guaranteed to lead TDIDT to produce suboptimal decision trees <ref> (e.g., Quinlan, 1988) </ref>. Similar functions serve to prove the converse for 1R: it cannot improve in accuracy from the baseline accuracy for a family of Boolean functions.
Reference: <author> Quinlan, R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 INTRODUCTION In his recent article Holte (1993) examined empirically the classification accuracy of one-level decision trees, i.e., concept classifiers that decide an instance's class on the basis of the value of a single attribute. He compared their accuracy to that of the state-of-the-art decision tree learner C4.5 <ref> (Quinlan, 1993) </ref>. The test domains included a large portion of domains which have, over the years, been used as standard test cases for new inductive learning programs. The domains can nowadays be found at the University of California at Irvine repository of machine learning data sets (Murphy and Aha, 1992).
Reference: <author> Rivest, R. </author> <year> (1987). </year> <title> Learning decision lists. </title> <booktitle> Machine Learning2: </booktitle> <pages> 229-246. </pages>
Reference-contexts: Holte does not define exactly what he means by linear decision trees. If he intends trees where each node has at most one nonterminal son, then in the binary case these correspond to decision lists <ref> (Rivest, 1987) </ref>. Decision lists do not have a static complexity bound, i.e., their complexity can be increased dynamically as the complexity of the problem increases. Thus, their expressive power is of different magnitude than that of one-level decision trees.
Reference: <author> Sakakibara, Y., Misue, K., and Koshiba, T. </author> <year> (1993). </year> <title> Text classification and automatic extraction of keywords by learning decision trees. </title> <booktitle> In Proc. Ninth Conference on Artificial Intelligence for Applications (p. 466). </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In database mining (e.g., Agrawal et al., 1992) the data is organized by attributes and thus appears similar as in traditional machine learning data sets, but the database contents is typically different from that of traditional test data. Manipulating text databases <ref> (e.g., Sakakibara, Misue, and Koshiba, 1993) </ref> and string learning (e.g., Arikawa et al., 1993) are two related applications that will undoubtedly become more and more important. In these applications there are no naturally arising attributes at all.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning10: </booktitle> <pages> 153-178. </pages>
Reference: <author> Tan, M. and Schlimmer, J. </author> <year> (1990). </year> <title> Two case studies in cost-sensitive concept acquisition. </title> <booktitle> In Proc. Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 854-860). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Thrun, S. et al. </author> <year> (1991). </year> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Report CMU-CS-91-197. </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: We consider concept representation further in Section 5. To be honest, there are domains where C4.5 is clearly outperformed, for example, by neural nets <ref> (see e.g., Thrun et al., 1991) </ref>. They are domains that would require constructing new features from the attributes that are used to express the data. Even the naturally adaptive connectionist approaches lack adequate capabilities in this respect (Craven and Shavlik, 1993).
Reference: <author> Vapnik, V. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
References-found: 26

