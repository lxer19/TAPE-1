URL: http://www.cs.rpi.edu/~bubnak/iccv.ps
Refering-URL: http://www.cs.rpi.edu/~bubnak/resume.html
Root-URL: http://www.cs.rpi.edu
Email: fbubnak, stewartg@cs.rpi.edu  
Title: Model Selection and Surface Merging in Reconstruction Algorithms  
Author: Kishore Bubna Charles V. Stewart 
Address: Troy, NY 12180-3590  
Affiliation: Department of Computer Science, Rensselaer Polytechnic Institute  
Abstract: The problem of model selection is relevant to many areas of computer vision. Model selection criteria have been used in the vision literature and many more have been proposed in statistics, but the relative strengths of these criteria have not been analyzed in vision. More importantly, suitable extensions to these criteria must be made to solve problems unique to computer vision. Using the problem of surface reconstruction as our context, we analyze existing criteria using simulations and sensor data, introduce new criteria from statistics, develop novel criteria capable of handling unknown error distributions and outliers, and extend model selection criteria to apply to the surface merging problem. The new surface merging rules improve upon previous results, and work well even at small step heights (h = 3oe) and crease discontinuities. Our results show that a Bayesian criteria and its bootstrapped variant perform the best, although for time-sensitive applications, a variant of the Akaike criterion may be a better choice. Unfortunately, none of the criteria work reliably for small region sizes, implying that model selection and surface merging should be avoided unless the region size is sufficiently large. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Information theory and an extension of the maximum likelihood principle. </title> <editor> In B. N. Petrov and F. Csaki, editors, </editor> <booktitle> 2nd International Symposium of Information Theory, </booktitle> <pages> pages 267-281. </pages> <publisher> Akademiai Kiado, </publisher> <year> 1973. </year>
Reference-contexts: Different criteria based on K-L distance result from different bias adjustments to 2 log L ( ^ m ). Criteria based on Bayes rule choose the model that maximizes the probability of the data, D, given the model m and prior information I . This Name Criteria AIC <ref> [1] </ref> d ( ^ m ; fl ) 2 log L ( ^ m ) + 2d m CAIC d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1) [7, 8] d ( ^ m ; fl ) 2 log
Reference: [2] <author> R. H. Bartels and J. J. Jezioranski. </author> <title> Least-squares fitting using orthogonal multinomials. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11(3) </volume> <pages> 201-217, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: We call the former 2D range images and the latter 3D range images. Candidate models and parameter estimation: We fit orthogonal polynomials <ref> [2, 3] </ref> to the data. This gives well-conditioned matrices, and is efficient because the fit to higher order models builds on fits to lower order models. The second advantage, however, is lost when using robust estimation techniques.
Reference: [3] <author> P. J. Besl. </author> <title> Surfaces in Range Image Understanding. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: These results, therefore, can be used to decide among different model selection criteria and merging criteria for different types of data and applications. We study both information-theoretic model selection criteria, as well as criteria based on Chi-square test [37], F-test [36, page 96], and runs test <ref> [3, 9] </ref>, and formulate merging rules based on them. However, this paper only presents the information-theoretic model selection and merging criteria, and simply summarizes the performance of the rest. <p> We call the former 2D range images and the latter 3D range images. Candidate models and parameter estimation: We fit orthogonal polynomials <ref> [2, 3] </ref> to the data. This gives well-conditioned matrices, and is efficient because the fit to higher order models builds on fits to lower order models. The second advantage, however, is lost when using robust estimation techniques.
Reference: [4] <author> P. J. Besl and R. C. Jain. </author> <title> Segmentation through variable-order surface fitting. </title> <journal> IEEE PAMI, </journal> <volume> 10 </volume> <pages> 167-192, </pages> <year> 1988. </year>
Reference-contexts: Yet without good solutions to these problems, the estimated parameters have little meaning. The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds <ref> [4, 6, 29, 34] </ref>, others, especially recent ones, are applications of information theoretic criteria [5, 7, 12, 21, 22, 23, 33, 37, 38]. <p> Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. Merging techniques, with the exception of [20], are generally based on empirical heuristics <ref> [4, 11, 35, 29] </ref>. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model [4, 11, 20, 35], potentially limiting the effectiveness of merging. <p> Merging techniques, with the exception of [20], are generally based on empirical heuristics [4, 11, 35, 29]. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model <ref> [4, 11, 20, 35] </ref>, potentially limiting the effectiveness of merging. Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers [7, 12] and unknown noise distributions. <p> Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing <ref> [4, 7, 22, 34] </ref>), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries <ref> [4, 11, 29, 35] </ref>. In the absence of a priori information, model selection forms an important part of each step. For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model.
Reference: [5] <author> R. M. Bolle and D. B. Cooper. </author> <title> Bayesian recognition of local 3-D shape by approximating image intensity functions with quadric polynomials. </title> <journal> IEEE PAMI, </journal> <volume> 6(4) </volume> <pages> 418-429, </pages> <year> 1984. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models.
Reference: [6] <author> R. C. Bolles and M. A. Fischler. </author> <title> A RANSAC-based approach to model fitting and its applications to finding cylinders in range data. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 637-643, </pages> <year> 1981. </year>
Reference-contexts: The problem of model selection is implicit in merging. regions into larger image regions (fig. 1 (b)). While the problem of estimating the model parameters is well studied <ref> [6, 7, 25, 26, 32] </ref>, the associated issues of model selection and merging have received much less attention in computer vision. Yet without good solutions to these problems, the estimated parameters have little meaning. <p> Yet without good solutions to these problems, the estimated parameters have little meaning. The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds <ref> [4, 6, 29, 34] </ref>, others, especially recent ones, are applications of information theoretic criteria [5, 7, 12, 21, 22, 23, 33, 37, 38].
Reference: [7] <author> K. L. Boyer, M. J. Mirza, and G. Ganguly. </author> <title> The robust sequential estimator: A general approach and its application to surface organization in range data. </title> <journal> IEEE PAMI, </journal> <volume> 16(10) </volume> <pages> 987-1001, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The problem of model selection is implicit in merging. regions into larger image regions (fig. 1 (b)). While the problem of estimating the model parameters is well studied <ref> [6, 7, 25, 26, 32] </ref>, the associated issues of model selection and merging have received much less attention in computer vision. Yet without good solutions to these problems, the estimated parameters have little meaning. <p> The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. <p> Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers <ref> [7, 12] </ref> and unknown noise distributions. One computer vision problem where model selection and merging techniques are crucial is surface reconstruc-tion. Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. <p> Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing <ref> [4, 7, 22, 34] </ref>), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches <ref> [7, 22] </ref>, or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model. When pruning redundant fits a model selection criteria may be used explicitly <ref> [7] </ref> or combined with greedy search techniques [22]. When merging adjacent surfaces, a criterion must be used to determine if the data should be represented by a single fit or by two or more different fits. <p> Note that oe may or may not be known a priori. Information-theoretic criteria use the loglikelihood of estimated parameters for model selection, hence, maximum likelihood estimators (MLEs) must be used for parameter estimation. We use ordinary least-squares for data with Gaussian errors, and following <ref> [7] </ref>, we use iteratively reweighted least squares (IRLS) [16] with an M-estimator based on t-distribution for data with outliers. In the latter case, IRLS is initialized using least median of squares (LMS) [25]. <p> This Name Criteria AIC [1] d ( ^ m ; fl ) 2 log L ( ^ m ) + 2d m CAIC d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1) <ref> [7, 8] </ref> d ( ^ m ; fl ) 2 log L ( ^ m ; ^oe m ) + d m (log n + 1) BAYES P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 [10, 19] P (Djm; G) = <p> For testing performance of merging criteria, we attempt to merge each ground-truth segment with its adjacent segments. In another experiment, we divide a ground-truth segment into small regions and attempt to merge them using these criteria. As mentioned before, we assume errors are t-distributed (following <ref> [7] </ref>, f = 1:5), and oe is unknown. Note, that the current version of BMSC-BAYES cannot be used in the presence of outliers. 7.1 Model selection This section compares model selection criteria on both large and small segments. For most large segments, the different criteria correctly select a planar model.
Reference: [8] <author> H. Bozdogan. </author> <title> Model selection and Akaike's information criterion (AIC): The general theory and its analytical extensions. </title> <journal> Psychome-trika, </journal> <volume> 52 </volume> <pages> 345-370, </pages> <year> 1987. </year>
Reference-contexts: This Name Criteria AIC [1] d ( ^ m ; fl ) 2 log L ( ^ m ) + 2d m CAIC d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1) <ref> [7, 8] </ref> d ( ^ m ; fl ) 2 log L ( ^ m ; ^oe m ) + d m (log n + 1) BAYES P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 [10, 19] P (Djm; G) =
Reference: [9] <author> K. A. Brownlee. </author> <title> Statistical Theory and Methodology in Science and Engineering. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1960. </year>
Reference-contexts: These results, therefore, can be used to decide among different model selection criteria and merging criteria for different types of data and applications. We study both information-theoretic model selection criteria, as well as criteria based on Chi-square test [37], F-test [36, page 96], and runs test <ref> [3, 9] </ref>, and formulate merging rules based on them. However, this paper only presents the information-theoretic model selection and merging criteria, and simply summarizes the performance of the rest.
Reference: [10] <author> K. Bubna and C. V. Stewart. </author> <title> Model selection and surface merging in reconstruction algorithms. </title> <type> Technical Report 4, CS, </type> <institution> RPI, </institution> <year> 1997. </year> <note> Available at http://www.cs.rpi.edu/tr/. </note>
Reference-contexts: However, this paper only presents the information-theoretic model selection and merging criteria, and simply summarizes the performance of the rest. For these criteria and detailed results, refer <ref> [10] </ref>. 2 Definitions Range image: A range image is characterized by a point p i = [x i z i ] T at any pixel i in the image. <p> We denote the likelihood for model m by L ( m ) and the MLE of m by ^ m . When oe is unknown, the likelihood is written as log L ( m ; oe), and ^oe m is the MLE of oe for model m. See <ref> [10] </ref> for details. 3 Model selection This section summarizes model selection criteria already used in reconstruction algorithms and introduces promising criteria from the statistics literature. Most Information-theoretic criteria are based on Kullback-Leibler (K-L) distances, Bayesian probabilities, and Minimum Description Length (MDL). <p> m (log n + 1) [7, 8] d ( ^ m ; fl ) 2 log L ( ^ m ; ^oe m ) + d m (log n + 1) BAYES P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 <ref> [10, 19] </ref> P (Djm; G) = q 2 (d m +2) n jX T m X m jRSS (nd m ) m q BIC [31] P (Djm; I) L ( ^ m ) n d m =2 RISS [28] len m log 2 L ( ^ m ) + log 2 <p> While the first expressions corresponding to each criteria is used when oe is known a priori, the second expression is used when oe is unknown. For unknown oe, BAYES maximizes P (Djm; G) for a Gaussian distribution, and P (Djm; t) for the t-distribution (see <ref> [10] </ref>). <p> RISS on the other hand is based on MDL. The MDL criteria in [27] is exactly the same as BIC. Interestingly, the MDL criteria used in a quadratic optimization function in [22] can be shown to be similar in form to AIC <ref> [10] </ref>. 4 New selection criteria using bootstrap The criteria presented in Section 3, implicitly assume the error distribution is known a priori. <p> In this paper, we derive a bootstrap version of the BAYES criteria given by P (Djm; I) in table 1 (see <ref> [10] </ref> for criteria based on RISS). Like any other criteria, it is a penalized likelihood, balancing between accuracy and complexity of the model given the data. In this criteria, L ( ^ m ) measures accuracy and H ( ^ m ) measures complexity or model stability. <p> the Bayesian rule, and is given by maxf (BMSC-BAYES A + BMSC-BAYES B ); BMSC-BAYES m 0 ; : : : g 2 It can be shown that the optimization function used in [22] can be used for merging surfaces, and is similar to the merging rule based on AIC <ref> [10] </ref> 3 In surface reconstruction, a similar Bayesian merging approach has been used in [20]. However, this approach only merges surfaces corresponding to the same model. Besides, it also restricts the parameter space such that jj m jj = 1. <p> The results <ref> [10] </ref> show that all criteria find it difficult to choose the correct model when a 1 and a 2 are relatively small in magnitude (refer Figure 2). Among these, RISS is the most affected. <p> For example, simulations at a pixel size of 0.0032 cm (as opposed to 0.0016 cm in Section 6) show substantially better results. Criteria based on Chi-square, F, and runs test, detailed in <ref> [10] </ref>, also perform poorly at small region sizes. These criteria perform relatively worse than most information-theoretic model selection criteria, reaching a success rate of only 90% to 95% in the experiments described in section 6.1.
Reference: [11] <author> F. S. Cohen and R. D. Rimey. </author> <title> A maximum likelihood approach to segmenting range data. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 1696-1701, </pages> <year> 1988. </year>
Reference-contexts: Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. Merging techniques, with the exception of [20], are generally based on empirical heuristics <ref> [4, 11, 35, 29] </ref>. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model [4, 11, 20, 35], potentially limiting the effectiveness of merging. <p> Merging techniques, with the exception of [20], are generally based on empirical heuristics [4, 11, 35, 29]. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model <ref> [4, 11, 20, 35] </ref>, potentially limiting the effectiveness of merging. Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers [7, 12] and unknown noise distributions. <p> One computer vision problem where model selection and merging techniques are crucial is surface reconstruc-tion. Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques <ref> [11, 35] </ref>, clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries <ref> [4, 11, 29, 35] </ref>. In the absence of a priori information, model selection forms an important part of each step. For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model.
Reference: [12] <author> T. Darrell and A. Pentland. </author> <title> Cooperative robust estimation using layers of support. </title> <journal> IEEE PAMI, </journal> <volume> 17(5) </volume> <pages> 474-487, </pages> <year> 1995. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. <p> Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers <ref> [7, 12] </ref> and unknown noise distributions. One computer vision problem where model selection and merging techniques are crucial is surface reconstruc-tion. Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy.
Reference: [13] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley Publications, </publisher> <year> 1973. </year>
Reference-contexts: In these expressions, RSS m denotes the residual sum of squares for model m, H () stands for the Hessian of the log-likelihood, V d m is the volume of the d m -dimensional unit hyper-sphere <ref> [13, page 24] </ref>, and log 2 fl (t) = log 2 t + log 2 log 2 t + : : : , including only its positive terms. probability is denoted by P (Djm; I).
Reference: [14] <author> B. Efron and R. J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: In computer vision problems, however, error distributions are often unknown and difficult to model accurately, making it crucial to develop model selection criteria that depend on only weak assumptions about error distributions. We address this problem in this section by deriving bootstrap <ref> [14] </ref> versions of criteria in Section 3. The bootstrap is a method for estimating an unknown distribution from available data. <p> The plug-in bootstrap principle <ref> [14, chapter 4] </ref> substitutes P with ^ P m and this is used to generate R bootstrap error vectors, e fl Rm . Sampling from ^ P m is the same as sampling from the set fe 1 ; : : : ; e n g with replacement. <p> Our sensor has a oe of about 0.02 cm at a depth of 100 cm. In our experiments we vary oe from 0.02 to 0.1 cm. Results are based on 500 simulations, and for bootstrap criteria, the number of bootstrap replications, R, is set to 200 <ref> [14] </ref>. In the first set of experiments, a 0 = 100 and a 1 = 1 (for both the models), and a 2 = 0:1 for the quadratic model.
Reference: [15] <author> R. Hoffman and A. Jain. </author> <title> Segmentation and classification of range images. </title> <journal> IEEE PAMI, </journal> <volume> 9 </volume> <pages> 608-620, </pages> <year> 1987. </year>
Reference-contexts: Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods <ref> [15, 29] </ref>, or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step.
Reference: [16] <author> P. W. Holland and R. E. Welsch. </author> <title> Robust regression using iteratively reweighted least-squares. </title> <journal> Commun. Statist.-Theor. Meth., </journal> <volume> A6:813-827, </volume> <year> 1977. </year>
Reference-contexts: Information-theoretic criteria use the loglikelihood of estimated parameters for model selection, hence, maximum likelihood estimators (MLEs) must be used for parameter estimation. We use ordinary least-squares for data with Gaussian errors, and following [7], we use iteratively reweighted least squares (IRLS) <ref> [16] </ref> with an M-estimator based on t-distribution for data with outliers. In the latter case, IRLS is initialized using least median of squares (LMS) [25]. We denote the likelihood for model m by L ( m ) and the MLE of m by ^ m .
Reference: [17] <author> A. Hoover, G. Jean-Baptiste, X. Jiang, P. Flynn, H. Bunke, D. Gold-gof, K. Bowyer, D. Eggert, A. Fitzgibbon, and R. Fisher. </author> <title> An experimental comparison of range image segmentation algorithms. </title> <journal> IEEE PAMI, </journal> <volume> 18 </volume> <pages> 673-689, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: AIC AIC Table 2: Overall performance of model selection and merging criteria using data with Gaussian errors. small segments, labels are marked outside the segment. performances. 7 Results using sensor data This section compares different criteria using one of the Perceptron test data sets from the USF Segmentation Comparison Project <ref> [17] </ref> (see Figure 6 (a)). This allows testing different criteria in the presence of small-scale random noise and outliers. To test performance of model selection, we apply each criteria on the ground-truth segments provided with the image, as well as on regions of different sizes within certain segments.
Reference: [18] <author> E. T. Jaynes. </author> <booktitle> Probability Theory the Logic of Science. Physics, </booktitle> <address> Washington University, St. Louis, MO 63130, USA, http://omega.albany.edu:8008/JaynesBook.html, 1994. </address>
Reference-contexts: To obtain a distribution-free measure of H ( ^ m ), observe that H ( ^ m ) [V ( ^ m )] 1 , the covariance matrix of ^ m <ref> [18, chapter 24] </ref>.
Reference: [19] <author> R. E. Kass and A. E. Raftery. </author> <title> Bayes factors. </title> <journal> American Statistical Association, </journal> <volume> 90(430) </volume> <pages> 773-795, </pages> <year> 1995. </year>
Reference-contexts: m (log n + 1) [7, 8] d ( ^ m ; fl ) 2 log L ( ^ m ; ^oe m ) + d m (log n + 1) BAYES P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 <ref> [10, 19] </ref> P (Djm; G) = q 2 (d m +2) n jX T m X m jRSS (nd m ) m q BIC [31] P (Djm; I) L ( ^ m ) n d m =2 RISS [28] len m log 2 L ( ^ m ) + log 2
Reference: [20] <author> S. M. LaValle and S. A. Hutchinson. </author> <title> A Bayesian segmentation methodology for parametric image models. </title> <journal> IEEE PAMI, </journal> <volume> 17(2) </volume> <pages> 211-217, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. Merging techniques, with the exception of <ref> [20] </ref>, are generally based on empirical heuristics [4, 11, 35, 29]. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model [4, 11, 20, 35], potentially limiting the effectiveness of merging. <p> Merging techniques, with the exception of [20], are generally based on empirical heuristics [4, 11, 35, 29]. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model <ref> [4, 11, 20, 35] </ref>, potentially limiting the effectiveness of merging. Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers [7, 12] and unknown noise distributions. <p> BMSC-BAYES m 0 ; : : : g 2 It can be shown that the optimization function used in [22] can be used for merging surfaces, and is similar to the merging rule based on AIC [10] 3 In surface reconstruction, a similar Bayesian merging approach has been used in <ref> [20] </ref>. However, this approach only merges surfaces corresponding to the same model. Besides, it also restricts the parameter space such that jj m jj = 1.
Reference: [21] <author> Y. G. Leclerc. </author> <title> Constructing simple stable descriptions for image partitioning. </title> <journal> IJCV, </journal> <volume> 3 </volume> <pages> 73-102, </pages> <year> 1989. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models.
Reference: [22] <author> A. Leonardis, A. Gupta, and R. </author> <title> Bajcsy. Segmentation of range images as the search for geometric parametric models. </title> <journal> IJCV, </journal> <volume> 14 </volume> <pages> 253-277, </pages> <year> 1995. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. <p> Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing <ref> [4, 7, 22, 34] </ref>), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches <ref> [7, 22] </ref>, or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model. When pruning redundant fits a model selection criteria may be used explicitly [7] or combined with greedy search techniques <ref> [22] </ref>. When merging adjacent surfaces, a criterion must be used to determine if the data should be represented by a single fit or by two or more different fits. Observe how the problem of model selection is implicit in the problem of surface merging. <p> RISS on the other hand is based on MDL. The MDL criteria in [27] is exactly the same as BIC. Interestingly, the MDL criteria used in a quadratic optimization function in <ref> [22] </ref> can be shown to be similar in form to AIC [10]. 4 New selection criteria using bootstrap The criteria presented in Section 3, implicitly assume the error distribution is known a priori. <p> As such, the correspond ing merging rules is similar to the Bayesian rule, and is given by maxf (BMSC-BAYES A + BMSC-BAYES B ); BMSC-BAYES m 0 ; : : : g 2 It can be shown that the optimization function used in <ref> [22] </ref> can be used for merging surfaces, and is similar to the merging rule based on AIC [10] 3 In surface reconstruction, a similar Bayesian merging approach has been used in [20]. However, this approach only merges surfaces corresponding to the same model.
Reference: [23] <author> M. Li. </author> <title> Minimum description length based 2D shape description. </title> <booktitle> In ICCV, </booktitle> <pages> pages 512-517, </pages> <year> 1993. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models.
Reference: [24] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby. </author> <title> Multivariate Analysis. </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: More sophisticated estimators are unnecessary because our weak assumptions on sensor noise are sufficient to yield unbiased, minimum variance estimates with least-squares <ref> [24, page 172] </ref>. The bootstrap estimate of oe, oe fl , is calculated by finding the average standard deviation of Z fl 1m ; : : : ; Z fl Rm . Unfortunately, this gives four different oe fl s corresponding to the four models in M .
Reference: [25] <author> P. Meer, D. Mintz, A. Rosenfeld, and D. Y. Kim. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> IJCV, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: The problem of model selection is implicit in merging. regions into larger image regions (fig. 1 (b)). While the problem of estimating the model parameters is well studied <ref> [6, 7, 25, 26, 32] </ref>, the associated issues of model selection and merging have received much less attention in computer vision. Yet without good solutions to these problems, the estimated parameters have little meaning. <p> We use ordinary least-squares for data with Gaussian errors, and following [7], we use iteratively reweighted least squares (IRLS) [16] with an M-estimator based on t-distribution for data with outliers. In the latter case, IRLS is initialized using least median of squares (LMS) <ref> [25] </ref>. We denote the likelihood for model m by L ( m ) and the MLE of m by ^ m . When oe is unknown, the likelihood is written as log L ( m ; oe), and ^oe m is the MLE of oe for model m.
Reference: [26] <author> J. V. Miller and C. V. Stewart. </author> <title> MUSE: Robust surface fitting using unbiased scale estimates. </title> <booktitle> In CVPR, </booktitle> <pages> pages 300-306, </pages> <year> 1996. </year>
Reference-contexts: The problem of model selection is implicit in merging. regions into larger image regions (fig. 1 (b)). While the problem of estimating the model parameters is well studied <ref> [6, 7, 25, 26, 32] </ref>, the associated issues of model selection and merging have received much less attention in computer vision. Yet without good solutions to these problems, the estimated parameters have little meaning.
Reference: [27] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 468-471, </pages> <year> 1978. </year>
Reference-contexts: Table 1 shows the various criteria studied in this paper. While AIC, and CAIC are based on the K-L distances, BAYES and BIC are derived from Bayes rule using non-informative priors. RISS on the other hand is based on MDL. The MDL criteria in <ref> [27] </ref> is exactly the same as BIC.
Reference: [28] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: =2 L ( ^ m )[j H ( ^ m )j] 1=2 [10, 19] P (Djm; G) = q 2 (d m +2) n jX T m X m jRSS (nd m ) m q BIC [31] P (Djm; I) L ( ^ m ) n d m =2 RISS <ref> [28] </ref> len m log 2 L ( ^ m ) + log 2 flV d m + 2 log 2 fl ^ m (H ( ^ m )) ^ m len m log 2 L ( ^ m ; ^oe) + log 2 flV d m + 2 log 2 fl
Reference: [29] <author> B. Sabata, F. Arman, and J. K. Aggarwal. </author> <title> Segmentation of 3D range images using pyramidal data structures. </title> <journal> CVGIP:IU, </journal> <volume> 57 </volume> <pages> 373-387, </pages> <year> 1993. </year>
Reference-contexts: Yet without good solutions to these problems, the estimated parameters have little meaning. The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds <ref> [4, 6, 29, 34] </ref>, others, especially recent ones, are applications of information theoretic criteria [5, 7, 12, 21, 22, 23, 33, 37, 38]. <p> Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. Merging techniques, with the exception of [20], are generally based on empirical heuristics <ref> [4, 11, 35, 29] </ref>. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model [4, 11, 20, 35], potentially limiting the effectiveness of merging. <p> Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods <ref> [15, 29] </ref>, or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries <ref> [4, 11, 29, 35] </ref>. In the absence of a priori information, model selection forms an important part of each step. For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model.
Reference: [30] <author> K. Sato and S. </author> <title> Inokuchi. Range-imaging system utilizing nematic liquid crystal mask. </title> <booktitle> In ICCV, </booktitle> <pages> pages 657-661, </pages> <year> 1987. </year>
Reference-contexts: The data contains Gaussian errors and are generated using focal length=1.77 cm and pixel size=0.0016 cm, the calibration parameters of our range sensor <ref> [30] </ref>.
Reference: [31] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: + d m (log n + 1) BAYES P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 [10, 19] P (Djm; G) = q 2 (d m +2) n jX T m X m jRSS (nd m ) m q BIC <ref> [31] </ref> P (Djm; I) L ( ^ m ) n d m =2 RISS [28] len m log 2 L ( ^ m ) + log 2 flV d m + 2 log 2 fl ^ m (H ( ^ m )) ^ m len m log 2 L ( ^
Reference: [32] <author> C. V. Stewart. MINPRAN: </author> <title> A new robust estimator for computer vision. </title> <journal> IEEE PAMI, </journal> <volume> 17(10) </volume> <pages> 925-938, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: The problem of model selection is implicit in merging. regions into larger image regions (fig. 1 (b)). While the problem of estimating the model parameters is well studied <ref> [6, 7, 25, 26, 32] </ref>, the associated issues of model selection and merging have received much less attention in computer vision. Yet without good solutions to these problems, the estimated parameters have little meaning.
Reference: [33] <author> J. Subrahmonia, D. B. Cooper, and D. Keren. </author> <title> Practical reliable Bayesian recognition of 2D and 3D objects using implicit polynomials and algebraic invariants. </title> <journal> IEEE PAMI, </journal> <volume> 18(5) </volume> <pages> 505-519, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models.
Reference: [34] <author> G. Taubin. </author> <title> Estimation of planar curves, surfaces, and nonplanar space curves defined by implicit equations with applications to edge and range segmentation. </title> <journal> IEEE PAMI, </journal> <volume> 13(11) </volume> <pages> 1115-1138, </pages> <year> 1991. </year>
Reference-contexts: Yet without good solutions to these problems, the estimated parameters have little meaning. The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds <ref> [4, 6, 29, 34] </ref>, others, especially recent ones, are applications of information theoretic criteria [5, 7, 12, 21, 22, 23, 33, 37, 38]. <p> Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing <ref> [4, 7, 22, 34] </ref>), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step.
Reference: [35] <author> R. Taylor, M. Savini, and A. Reeves. </author> <title> Fast Segmentation of Range Imagery Into Planar Regions. </title> <journal> CVGIP, </journal> <volume> 45 </volume> <pages> 42-60, </pages> <year> 1989. </year>
Reference-contexts: Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. Merging techniques, with the exception of [20], are generally based on empirical heuristics <ref> [4, 11, 35, 29] </ref>. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model [4, 11, 20, 35], potentially limiting the effectiveness of merging. <p> Merging techniques, with the exception of [20], are generally based on empirical heuristics [4, 11, 35, 29]. Further, in an attempt to avoid the model selection problem, many merging techniques only join fits to the same model <ref> [4, 11, 20, 35] </ref>, potentially limiting the effectiveness of merging. Hence, mathematical criteria to merge regions and to simultaneously decide the correct model for a merged region must be formulated. Finally, both model selection and merging criteria in vision must tolerate outliers [7, 12] and unknown noise distributions. <p> One computer vision problem where model selection and merging techniques are crucial is surface reconstruc-tion. Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach involves estimating initial surface patches (using grid techniques <ref> [11, 35] </ref>, clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries [4, 11, 29, 35]. In the absence of a priori information, model selection forms an important part of each step. <p> This approach involves estimating initial surface patches (using grid techniques [11, 35], clustering methods [15, 29], or by region growing [4, 7, 22, 34]), and later pruning redundant surface patches [7, 22], or merging artificial surface boundaries <ref> [4, 11, 29, 35] </ref>. In the absence of a priori information, model selection forms an important part of each step. For example, when expanding seed regions, at each iteration it must be decided whether to continue growing using the same model or to switch to a different model.
Reference: [36] <author> S. Weisberg. </author> <title> Applied Linear Regression. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: These results, therefore, can be used to decide among different model selection criteria and merging criteria for different types of data and applications. We study both information-theoretic model selection criteria, as well as criteria based on Chi-square test [37], F-test <ref> [36, page 96] </ref>, and runs test [3, 9], and formulate merging rules based on them. However, this paper only presents the information-theoretic model selection and merging criteria, and simply summarizes the performance of the rest.
Reference: [37] <author> P. Whaite and F. P. Ferrie. </author> <title> Active exploration: knowing when we're wrong. </title> <booktitle> In ICCV, </booktitle> <pages> pages 41-48, </pages> <year> 1993. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models. <p> These results, therefore, can be used to decide among different model selection criteria and merging criteria for different types of data and applications. We study both information-theoretic model selection criteria, as well as criteria based on Chi-square test <ref> [37] </ref>, F-test [36, page 96], and runs test [3, 9], and formulate merging rules based on them. However, this paper only presents the information-theoretic model selection and merging criteria, and simply summarizes the performance of the rest.
Reference: [38] <author> S. C. Zhu and A. Yuille. </author> <title> Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation. </title> <journal> IEEE PAMI, </journal> <volume> 18(9) </volume> <pages> 884-900, </pages> <year> 1996. </year>
Reference-contexts: The relative strengths of model selection techniques and merging techniques used in vision algorithms are not yet well understood. Some model selection criteria in vision rely on heuristics and user-defined thresholds [4, 6, 29, 34], others, especially recent ones, are applications of information theoretic criteria <ref> [5, 7, 12, 21, 22, 23, 33, 37, 38] </ref>. Most of these criteria do not work well for small region sizes, most make errors near small magnitude discontinu-ities, and some are biased towards higher or lower order models.
References-found: 38

