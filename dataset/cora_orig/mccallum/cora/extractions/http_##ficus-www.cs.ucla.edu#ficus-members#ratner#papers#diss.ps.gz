URL: http://ficus-www.cs.ucla.edu/ficus-members/ratner/papers/diss.ps.gz
Refering-URL: http://ficus-www.cs.ucla.edu/ficus-members/ratner/papers.html
Root-URL: http://www.cs.ucla.edu
Title: Roam: A Scalable Replication System for Mobile and Distributed Computing  
Author: David Howard Ratner Gerald J. Popek, co-chair W. W. Chu, co-chair Eli Gafni Mario Gerla Donald Morisky 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science  Thesis committee:  
Date: January, 1998  
Address: Los Angeles  
Affiliation: University of California,  UCLA Computer Science Department  
Pubnum: Technical Report UCLA-CSD-970044  
Abstract-found: 0
Intro-found: 1
Reference: [Aguilera et al. 1997] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. "Heartbeat: </author> <title> A Timeout-Free Failure Detector for Quiescent Reliable Communication." </title> <type> Technical Report TR97-1631, </type> <institution> Cornell University, Computer Science, </institution> <month> May 30, </month> <year> 1997. </year>
Reference-contexts: We could use a similar procedure to replicate ward masters, creating more replicas at higher levels where redundancy is more important. As mentioned in Chapter 4, the Ward Model fully supports multiple ward masters. A second idea uses a variant of network heartbeats <ref> [Andrews 1991, Aguilera et al. 1997] </ref>. If the ward members maintained better contact with the ward master, they could more quickly detect ward master 131 132 CHAPTER 11. FUTURE WORK failures (or the creation of network partitions that separate them from the ward master).
Reference: [Alexandrov et al. 1997] <author> Albert D. Alexandrov, Maximilian Ibel, Klaus E. Schauser, and Chris J. Scheiman. </author> <title> "Extending the Operating System at the User-Level: the Ufo Global File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 77-90, </pages> <address> Anaheim, California, </address> <month> January </month> <year> 1997. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: While the "hooks" exist in Roam to locate remote replicas storing the objects, the mechanism to actually redirect access to a remote replica is missing. Selective replication should not break name transparency. A lightweight, user-level mechanism similar to that implemented in the UFO file system <ref> [Alexandrov et al. 1997] </ref> should be included. Roam's functionality would be greatly improved. 11.4 Real-time Update Propagation Another feature of Ficus that should be implemented in Roam is a best-effort, real-time propagation of updates.
Reference: [Alonso et al. 1989] <author> Rafael Alonso, Daniel Barbara, and Luis L. Cova. </author> <title> "A File Storage Implementation for Very Large Distributed Systems." </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1989. </year>
Reference: [Anderson et al. 1995] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> "Serverless Network File Systems." </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 109-126, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou [Terry et al. 1995], Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project <ref> [Anderson et al. 1995] </ref>. The peer model provides a very rich and robust communication framework. However, it has typically suffered from scaling problems.
Reference: [Andrews 1991] <author> Gregory R. Andrews. </author> <title> "Paradigms for Process Interaction in Distributed Programs." </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1) </volume> <pages> 49-90, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: We could use a similar procedure to replicate ward masters, creating more replicas at higher levels where redundancy is more important. As mentioned in Chapter 4, the Ward Model fully supports multiple ward masters. A second idea uses a variant of network heartbeats <ref> [Andrews 1991, Aguilera et al. 1997] </ref>. If the ward members maintained better contact with the ward master, they could more quickly detect ward master 131 132 CHAPTER 11. FUTURE WORK failures (or the creation of network partitions that separate them from the ward master).
Reference: [Awerbuch 1987] <author> Baruch Awerbuch. </author> <title> "Optimal Distributed Algorithms for Minimum Weight Spanning Tree, Counting, Leader Election and Related Problems (Detailed Summary)." </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 230-240, </pages> <address> New York City, </address> <month> 25-27 May </month> <year> 1987. </year>
Reference: [Bagrodia et al. 1995] <author> Rajive Bagrodia, Wesley W. Chu, Leonard Kleinrock, and Gerald Popek. </author> <title> "Vision, Issues, and Architecture for Nomadic Computing." </title> <journal> IEEE Personal Communications Magazine, </journal> <volume> 2(6) </volume> <pages> 14-27, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: We do not believe that communication is or will ever be free, and therefore see a continuing need for data replication. 3.5. DEVELOPMENT ENVIRONMENT 23 3.5 Development Environment Roam was developed as part of the Travler project <ref> [Bagrodia et al. 1995] </ref> at the University of California, Los Angeles. It was developed in C ++ with occasional Perl scripts. Roam is a system in real use; in fact, Roam's source code and executables are replicated using Roam.
Reference: [Bastani et al. 1987] <author> F. B. Bastani and I-Ling Yen. </author> <title> "A Fault Tolerant Replicated Storage System." </title> <booktitle> In Proceedings of the Third International Conference on Data Engineering, </booktitle> <pages> pp. 449-454. </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1987. </year>
Reference-contexts: Systems based on conservative replication strategies [Prusker et al. 1990, Liskov et al. 1991, Brereton 1986, Davcev et al. 1985, Herlihy 1986, Thomas 1979, Guerraoui et al. 1996] are not suitable for replication in mobile environments. Conservative or pessimistic strategies such as primary-site [Stonebraker 1979], majority-vote <ref> [P^aris 1989, Renesse et al. 1988, Bastani et al. 1987] </ref> or token-based techniques do not allow multiple writers when the writers cannot directly communicate to serialize the order of operations. The conservative strategies provide very high consistency but decreased availability in terms of how often users can generate updates.
Reference: [Blaustein et al. 1985] <author> Barbara T. Blaustein and Charles W. Kaufman. </author> <title> "Updating Replicated Data during Communications Failures." </title> <booktitle> In Proceedings of the Eleventh International Conference on Very Large Data Bases, </booktitle> <pages> pp. 49-58, </pages> <month> August </month> <year> 1985. </year>
Reference: [Brereton 1986] <author> O. P. </author> <title> Brereton. "Management of Replicated Files in a Unix Environment." </title> <journal> Software| Practice and Experience, </journal> <volume> 16(8) </volume> <pages> 771-780, </pages> <month> August </month> <year> 1986. </year>
Reference: [Ceri et al. 1992] <author> Stefano Ceri, Maurice A. W. Houtsma, Arthur M. Keller, and Pierangela Samarati. </author> <title> "The Case for Independent Updates." </title> <booktitle> In Proceedings of the Second Workshop on Management of Replicated Data, </booktitle> <pages> pp. 17-19. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1992. </year> <note> 139 140 REFERENCES </note>
Reference: [Chandra et al. 1991] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> "Unreliable Failure Detectors for Asynchronous Systems." </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 325-340. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1991. </year>
Reference: [Cooper 1992] <author> M.A. Cooper. </author> <title> "Overhauling Rdist for the '90s." </title> <booktitle> In Proceedings of the Sixth USENIX Systems Administration Conference (LISA VI), </booktitle> <pages> pp. 175-188. </pages> <address> Berkeley, California, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The model is very simple, yielding a simple implementation, and has been used in many replication packages such as laplink [Nance 1995] and 1 Rumor can be downloaded from http://fmg-www.cs.ucla.edu/rumor. 3.3. REPLICATION DEFINITIONS AND TERMS 19 rdist <ref> [Cooper 1992] </ref>. However, with the simple model comes limited functionality: the slave is essentially read-only. Most master-slave services ignore all updates or modifications performed at the slave, and "undo" the update during synchronization, making the slave identical to the master. <p> Lotus Notes provides a simple and useful replication service. However, it is completely separate from any file system and requires a great commitment in infrastructure. It works well with documents that lend themselves to append-only usage, such as logs, but its universality is quite limited. 10.1.11 rdist rdist <ref> [Cooper 1992] </ref> is a relatively simple replication package based on the master-slave model. One replica is designated the master and all others are slaves. The synchronization protocol makes the slaves identical to the master, regardless of the file system activity that has occurred at the slaves.
Reference: [Corporation 1989] <author> Lotus Development Corporation. </author> <title> "Lotus Notes: Essential Software for Group Communications." </title> <booktitle> Lotus Notes Technical Series Vol. </booktitle> <volume> 1, </volume> <month> December 6 </month> <year> 1989. </year>
Reference-contexts: While they do cluster replicas and allow a client to change its cluster, the algorithms to do so are more complex. Additionally, their system does nothing to solve the client-to-client interaction problem, as clients still cannot directly synchronize with other clients. 10.1.10 Lotus Notes Lotus Notes <ref> [Corporation 1989] </ref> provides a database-oriented filing environment with a built-in replication service. Files are stored as forms in databases, and entire databases can be replicated. Notes databases provide a revision control service that is closely related to replication.
Reference: [Davcev et al. 1985] <author> Danco Davcev and Walter A. Burkhard. </author> <title> "Consistency and Recovery Control for Replicated Files." </title> <booktitle> In Proceedings of the Tenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 87-96. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference: [Davidson 1984] <author> Susan B. Davidson. </author> <title> "Optimism and Consistency in Partitioned Distributed Database Systems." </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(3) </volume> <pages> 456-481, </pages> <month> September </month> <year> 1984. </year>
Reference: [Davidson et al. 1985] <author> Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen. </author> <title> "Consistency in Partitioned Networks." </title> <journal> ACM Computing Surveys, </journal> <volume> 17(3) </volume> <pages> 341-370, </pages> <month> September </month> <year> 1985. </year>
Reference: [Demers et al. 1994] <author> Alan Demers, Karin Petersen, Mike Spreitzer, Douglas Terry, Marvin Theimer, and Brent Welch. </author> <title> "The Bayou Architecture: Support for Data Sharing among Mobile Users." </title> <booktitle> In Proceedings of the Workshop on Mobile Computing Systems and Applications1994, </booktitle> <address> Santa Cruz, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: The goal is therefore to design a peer-based model that scales well. Replication services based on the traditional peer model, such as Ficus [Guy et al. 1990b], Rumor [Reiher et al. 1996], and Bayou <ref> [Demers et al. 1994] </ref>, all suffer from scaling problems. In response, we have designed the Ward Model.
Reference: [Dwork et al. 1988] <author> Cynthia Dwork, Nancy Lynch, and L. Stockmeyer. </author> <title> "Consensus in the Presence of Partial Synchrony." </title> <journal> Journal of the ACM, </journal> <volume> 35(2), </volume> <month> April </month> <year> 1988. </year>
Reference: [Ebling 1997] <author> Maria Ebling, </author> <year> 1997. </year> <title> Personal communication with Geoff Kuenning and Maria Ebling, </title> <month> 11 July. </month>
Reference-contexts: Each directory with an overwritten entry in the log is marked in update/update conflict and must be manually resolved, requiring user involvement and temporarily barring access to the objects underneath the directory. Coda has not yet experienced many problems with its log wrap-around approach <ref> [Ebling 1997] </ref>; however, its servers are tightly integrated and typically well-connected. In fact, the only time Coda has experienced problems with log wrap-around is when a server died and it took a few days to begin functioning again [Ebling 1997]. <p> Coda has not yet experienced many problems with its log wrap-around approach <ref> [Ebling 1997] </ref>; however, its servers are tightly integrated and typically well-connected. In fact, the only time Coda has experienced problems with log wrap-around is when a server died and it took a few days to begin functioning again [Ebling 1997]. Should the same strategy be applied in a mobile environment where disconnections are commonplace, it seems that the log wrap-around strategy would result in a large number of unnecessary conflicts, causing unneeded problems for mobile users.
Reference: [Fischer 1983] <author> Michael J. Fischer. </author> <title> "The Consensus Problem in Unreliable Distributed Systems (a brief survey)." </title> <type> Technical Report YALEU/DSC/RR-273, </type> <institution> Yale University, </institution> <month> June </month> <year> 1983. </year>
Reference: [Fischer et al. 1982] <author> Michael J. Fischer and Alan Michael. </author> <title> "Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network." </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Database Systems, </booktitle> <month> March </month> <year> 1982. </year>
Reference-contexts: We will discuss the more complicated case of deletion first, followed by that of addition. There are three basic deletion issues: the potential loss of updates, the resolution of the create/delete ambiguity <ref> [Fischer et al. 1982] </ref> and the removal of full backstoring information. All of these are examined below. Potential loss of updates Replica deletions should only affect storage locations, not the file's existence or physical data. Note that a replica deletion is different than physically removing a file. <p> In such environments, where distributed actions are performed without global consent, the potential for create/delete ambiguities arises <ref> [Fischer et al. 1982] </ref>. The general description of a create/delete ambiguity 5 "File" in this context refers to all file system objects, including directories and symbolic links. 66 CHAPTER 6. <p> Any relation, either by name or by data, is merely semantic and does not imply a causal relationship from the point of view of garbage collection of the original object. Additionally, garbage collection must resolve the create-delete ambiguity <ref> [Fischer et al. 1982, Guy et al. 1993] </ref>. When an object exists at one replica and not at the other, it cannot be determined without additional information whether the object should be created at the second replica or removed at the first. Problems occur if the ambiguity is not resolved.
Reference: [Floyd 1986a] <author> Rick Floyd. </author> <title> "Directory Reference Patterns in a UNIX Environment." </title> <type> Technical Report TR-179, </type> <institution> University of Rochester, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Remove/update conflicts caused by interactions with dynamic naming are called pseudo, because they are not true remove/update conflicts|the object was never globally inaccessible. However, dynamic naming rarely occurs <ref> [Floyd 1986a] </ref>, meaning that the percentage of pseudo conflicts is expected to be low. Additionally, the inherent value in detecting a remove/update conflict is in recovering the data for the user instead of requiring it to be reconstructed manually.
Reference: [Floyd 1986b] <author> Rick Floyd. </author> <title> "Short-Term File Reference Patterns in a UNIX Environment." </title> <type> Technical Report TR-177, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: File system analyses and simulation data seems to support the hot-spot notion. Analyses of file usage in existing systems suggest that both concurrent read and write sharing is quite rare <ref> [Floyd 1986b] </ref>.
Reference: [Gafni 1985] <author> Eli Gafni. </author> <title> "Improvements in the time complexity of two message-optimal election algorithms." </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Distributed Computing. ACM, </booktitle> <month> August </month> <year> 1985. </year>
Reference: [Garcia-Molina 1982] <author> H. Garcia-Molina. </author> <title> "Elections in Distributed Computer Systems." </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31:48-59, </volume> <year> 1982. </year>
Reference: [George et al. 1981] <author> A. George and W.H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year> <booktitle> Prentice-Hall Series in Computational Mathematics. </booktitle>
Reference-contexts: Therefore, we must preserve the ability for each replica to generate updates while taking advantage of the fact that many replicas will generate no updates. Sparse-matrix approaches <ref> [George et al. 1981] </ref> could potentially be used to compress the zero elements. However, the non-zero elements could be anywhere in the vector. The writing replicas are not guaranteed to be in one contiguous list, minimizing the benefit of sparse matrix compression techniques.
Reference: [Goel 1996] <author> Ashvin Goel. </author> <title> "View Consistency for Optimistic Replication." </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> February </month> <year> 1996. </year> <note> Available as UCLA technical report CSD-960011. REFERENCES 141 </note>
Reference-contexts: Algorithm analysis can be found in Sections 7.6 and 7.7. Algorithm details and pseudo code are provided at the end of this chapter (Section 7.9). 7.5.1 New semantics The algorithm provides what we call time moves forward semantics, or simply TMF. Similar to Goel's work with optimistic consistency models <ref> [Goel 1996] </ref>, we guarantee that time as seen by file names always moves forward. Each name for an object always refers to data versions that increase with time. <p> Bayou provides session guarantees [Terry et al. 1994] to improve the perceived consistency by users. Additionally, Bayou establishes strong guarantees about its data|writes can be classified either as committed or tentative. Similar research has been done in optimistic file systems <ref> [Goel 1996] </ref> and could be applied to Roam to offer many of the same types of guarantees. Bayou does not provide any form of selective replication. The databases must be entirely replicated at all storage sites. <p> Roam's garbage collection algorithms are related in spirit to Goel's work on view consistency for optimistic replication systems <ref> [Goel 1996] </ref>. Goel provided higher-level constraints on the replica selection mechanism to guarantee that users never accessed older data than they had previously seen at some other replica, which is similar to the guarantees Roam provides on the garbage collection of objects.
Reference: [Gray 1978] <author> J. N. Gray. </author> <booktitle> "Notes on Database Operating Systems." In Operating Systems: An Advanced Course, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: allowing user access to it via a special name. 7.4 Garbage Collection in Ficus and Rumor Ficus and Rumor provide the no lost updates semantics, using a two-phase, coordinator-free, distributed solution [Guy et al. 1993, Ratner et al. 1996a] that is essentially a distributed implementation of the two-phase commit protocol <ref> [Gray 1978, Lampson et al. 1979] </ref>. The algorithm verifies that no participant completes before all participants are knowledgeable that completion is imminent. <p> Our version vector compression algorithm forms consensus on the value of a given element and atomically removes it from the vector. As a consensus algorithm, it is closely related to Guy's garbage collection algorithm [Guy et al. 1993] and two-phase commit protocols <ref> [Gray 1978, Lampson et al. 1979] </ref>. However, unlike the earlier algorithms, our algorithm can afford to be one-phase, rather than requiring two.
Reference: [Gray et al. 1996] <author> Jim Gray, Pat Helland, Patrick O'Neil, and Dennis Shasha. </author> <title> "The Dangers of Replication and a Solution." </title> <booktitle> In Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. 173-182. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1996. </year>
Reference-contexts: Scaling has not typically been an issue with replication services, because most scenarios have not required large numbers (more than a dozen) of writable replicas. Some have, in fact, argued that peer solutions by their nature simply cannot scale to large numbers <ref> [Satyanarayanan 1992, Gray et al. 1996] </ref>. However, while mobile environments seem to require a peer-based solution (as described above), they also seem to negate the assumption that a handful of replicas is enough. <p> Similarly, strategies that provide upgradable read-mostly replicas (upgradable on demand from read-only to read-write) are yet another form of class-based replication, and therefore equally not viable in the mobile arena. Gray et al. <ref> [Gray et al. 1996] </ref> have devised analytical models of replication which they claim demonstrate that optimistic, peer replication cannot scale. However, their models are based on transactional databases, 12 CHAPTER 2. SOLUTION REQUIREMENTS and therefore are substantially different from the target of this dissertation (the file system).
Reference: [Guerraoui et al. 1996] <author> Rachid Guerraoui, Rui Oliveira, and Andre Schiper. </author> <title> "Atomic Updates of Replicated Data." </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 1150 </volume> <pages> 365-381, </pages> <month> October </month> <year> 1996. </year>
Reference: [Guy 1987] <author> Richard G. Guy. </author> <title> "A Replicated Filesystem Design for a Distributed UNIX System." </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1987. </year>
Reference-contexts: The framework is currently being modified to use a real two-way synchronization protocol; however, at this time the effort has not been completed. This chapter describes the details of the pairwise, pull-only process, including the detection of updates 1 The term was first used by Guy in Ficus <ref> [Guy 1987] </ref>. 45 46 CHAPTER 5. CONSISTENCY MAINTENANCE processes as rectangles within them. Communication is indicated as arrows between processes. and the fetching and installation of data. We will first provide an overview of the reconciliation architecture and then describe its major pieces.
Reference: [Guy 1991] <author> Richard G. Guy. Ficus: </author> <title> A Very Large Scale Reliable Distributed File System. </title> <type> Ph.D. dissertation, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> June </month> <year> 1991. </year> <note> Also available as UCLA technical report CSD-910018. </note>
Reference-contexts: We conclude with a discussion of a new algorithm and semantics, as well as an analysis of that algorithm, a proof of correctness, and algorithm pseudo-code. 7.1 File System Model We adopt the same model as Guy <ref> [Guy 1991] </ref>. Briefly, the model provides users with a persistent storage service for a collection of files or objects. Each object has a logical name or set of names by which users access file data. Names can have different replication factors from each other and from the underlying object itself. <p> Both of these invariants are properties of the two-phase algorithm for reclamation of the object record, not the fast reclamation of object data. Guy already demonstrated that the two-phase algorithm is correct under the above conditions in <ref> [Guy 1991] </ref>; his proof applies here equally well. 7.9 Algorithm Details We outline the new garbage collection algorithm in pseudo code. <p> First we define the various data structures used, and then present the algorithm. 7.9.1 Algorithm data structures The following is a list of the data structures used by the algorithm when garbage collecting a single object, many of which are adopted from Guy <ref> [Guy 1991] </ref>. The only replicated data structure among the list is 7.9.
Reference: [Guy et al. 1990a] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> "Implementation of the Ficus Replicated File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 63-71. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: When conflicts do occur, many can be resolved transparently and automatically without user involvement [Kumar et al. 1993, Reiher et al. 1994]. Roam is based on an optimistic approach, and is the logical outgrowth of two earlier optimistic replication systems, Ficus and Rumor. 3.1.1 Ficus Ficus <ref> [Guy et al. 1990a, Guy et al. 1993] </ref> is an optimistic file system that provides data replication as well as transparent replica selection and remote access to non-local data. Based on the traditional peer-to-peer model, it allows any two replicas to directly synchronize with each other. <p> Any replica can synchronize with any other replica, and any file system modification or update can be applied at any accessible replica. The peer model has been implemented in systems such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou [Terry et al. 1995], Ficus <ref> [Guy et al. 1990a, Page et al. 1991] </ref>, and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. The peer model provides a very rich and robust communication framework. <p> While Locus allows concurrent updates across partitions, the mechanism used to merge partitions is complex and does not scale, because it always attempts to maintain a perfect picture of partition membership. Locus can not, therefore, support mobility. 10.1.6 Ficus Ficus <ref> [Guy et al. 1990a, Page et al. 1991] </ref> is one of the intellectual and physical predecessors of Roam, and Roam therefore shares many of the same characteristics as Ficus.
Reference: [Guy et al. 1990b] <author> Richard G. Guy, Thomas W. Page, Jr., John S. Heidemann, and Gerald J. Popek. </author> <title> "Name Transparency in Very Large Scale Distributed File Systems." </title> <booktitle> In Second IEEE Workshop on Experimental Distributed Systems, </booktitle> <pages> pp. 20-25. </pages> <institution> University of California, Los Angeles, IEEE, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Building upon the peer model seems an esthetically cleaner approach, and ought to better preserve the pureness of the underlying framework. The goal is therefore to design a peer-based model that scales well. Replication services based on the traditional peer model, such as Ficus <ref> [Guy et al. 1990b] </ref>, Rumor [Reiher et al. 1996], and Bayou [Demers et al. 1994], all suffer from scaling problems. In response, we have designed the Ward Model.
Reference: [Guy et al. 1990c] <author> Richard G. Guy and Gerald J. Popek. </author> <title> "Reconciling Partially Replicated Name Spaces." </title> <type> Technical Report CSD-900010, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: New names can be distinguished from old ones that have been deleted at one of the two replicas by a process known as create/delete disambiguation (discussed in depth in Chapter 7). Full details on the decision-making process can be found elsewhere <ref> [Guy et al. 1990c, Guy et al. 1993, Ratner 1995] </ref>. When all decisions have been made, and all requests given to the server, the recon process waits until the server has fulfilled all data requests and performed all system modifications. Upon notification from the server, the recon process terminates.
Reference: [Guy et al. 1993] <author> Richard G. Guy, Gerald J. Popek, and Thomas W. Page, Jr. </author> <title> "Consistency Algorithms for Optimistic Replication." </title> <booktitle> In Proceedings of the First International Conference on Network Protocols. IEEE, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: When conflicts do occur, many can be resolved transparently and automatically without user involvement [Kumar et al. 1993, Reiher et al. 1994]. Roam is based on an optimistic approach, and is the logical outgrowth of two earlier optimistic replication systems, Ficus and Rumor. 3.1.1 Ficus Ficus <ref> [Guy et al. 1990a, Guy et al. 1993] </ref> is an optimistic file system that provides data replication as well as transparent replica selection and remote access to non-local data. Based on the traditional peer-to-peer model, it allows any two replicas to directly synchronize with each other. <p> Ficus and Rumor use a fully distributed, two-phase, coordinator-free algorithm to ensure that all replicas are knowledgeable about the garbage collection process and eventually complete it, although any given set of participants may never be present simultaneously <ref> [Guy et al. 1993] </ref>. We build on this algorithm and improve its efficiency correctness, and mobile-friendliness, as discussed in Chapter 7. 3.3.6 Synchronization We call the synchronization process reconciliation. It is a pairwise process between two replicas. <p> New names can be distinguished from old ones that have been deleted at one of the two replicas by a process known as create/delete disambiguation (discussed in depth in Chapter 7). Full details on the decision-making process can be found elsewhere <ref> [Guy et al. 1990c, Guy et al. 1993, Ratner 1995] </ref>. When all decisions have been made, and all requests given to the server, the recon process waits until the server has fulfilled all data requests and performed all system modifications. Upon notification from the server, the recon process terminates. <p> The inability to distinguish the two cases is the essence of the create/delete ambiguity. Replicas R and S cannot decide if the file should be added by R or was at some point in the past deleted by R. Guy <ref> [Guy et al. 1993] </ref> describes a solution for resolving the create/delete ambiguity in the context of garbage collection of distributed file system objects; we adopt his solution here. <p> Time in the table flows downward. data will be reclaimed. For instance, consider the Ficus and Rumor no lost updates semantics <ref> [Guy et al. 1993] </ref>. These semantics guarantee that data will be preserved as long as the file is globally accessible, and that no replica will remove data until it becomes globally inaccessible. <p> Any relation, either by name or by data, is merely semantic and does not imply a causal relationship from the point of view of garbage collection of the original object. Additionally, garbage collection must resolve the create-delete ambiguity <ref> [Fischer et al. 1982, Guy et al. 1993] </ref>. When an object exists at one replica and not at the other, it cannot be determined without additional information whether the object should be created at the second replica or removed at the first. Problems occur if the ambiguity is not resolved. <p> We resolve remove/update conflicts by preserving the updated version of the data and allowing user access to it via a special name. 7.4 Garbage Collection in Ficus and Rumor Ficus and Rumor provide the no lost updates semantics, using a two-phase, coordinator-free, distributed solution <ref> [Guy et al. 1993, Ratner et al. 1996a] </ref> that is essentially a distributed implementation of the two-phase commit protocol [Gray 1978, Lampson et al. 1979]. The algorithm verifies that no participant completes before all participants are knowledgeable that completion is imminent. <p> Any site with a locally inaccessible object initiates garbage collection on that object; the algorithm either stops when a new name is discovered, or consensus is reached that the object is globally inaccessible and garbage collection completes. Algorithm details can be found in <ref> [Guy et al. 1993] </ref> and [Ratner et al. 1996a]. The algorithm performs adequately in Ficus and Rumor; however, its semantics often cause disks to become full of garbage waiting to be collected, since data is preserved at all replicas until algorithm completion. <p> The two phases guarantee that each replica knows both that everyone else is participating in garbage collection (phase 1) and that everyone else knows that it itself is participating (phase 2) <ref> [Guy et al. 1993] </ref>. Once any replica has completed the two-phase protocol, it may remove the record itself, completing the garbage collection process. If the object is not globally inaccessible, the two-phase algorithm will detect the new name and terminate. <p> In this case, each replica preserves the object record and eventually learns of the new name and data for the object. Guy has previously demonstrated that we require two phases to correctly remove the record and resolve the create/delete ambiguity <ref> [Guy et al. 1993] </ref>. 7.5.4 Garbage collection and wards As described in Chapter 4, garbage collection operates semi-independently between the wards, with the ward masters managing the cooperation. That is, garbage collection occurs on a ward-by-ward basis. Each ward completes when all of its members are ready to complete. <p> X 1 * at time t k from X k to X k+1 for 1 k &lt; n and t 0 &lt; t k &lt; t k+1 * at time t n from X n to S, t n1 &lt; t n Guy calls this set of conditions time connectivity <ref> [Guy et al. 1993] </ref>. 80 CHAPTER 7. GARBAGE COLLECTION 7.6 Algorithm Analysis Assume there are N replicas of a specific file, labeled R 1 , R 2 , R 3 ,. . . ,R N . <p> We use a one-phase bit vector to determine when consensus has occurred. Each participant indicates knowledge and agreement by setting its corresponding bit in the bit vector. In general, one phase is not enough to determine consensus <ref> [Guy et al. 1993] </ref>. However, in our case we only require one phase. Unlike the scenario with general consensus problems, the replica whose element is being compressed, called the generating replica, cannot, by design, change its "vote" (i.e. b-value) once it is set. <p> Ficus and Rumor both use a garbage collection algorithm similar to Roam's; in fact, Roam's algorithm is the intellectual descendent of the one developed for Ficus <ref> [Guy et al. 1993] </ref>. Ficus and Rumor provide the stronger no lost updates semantics, while Roam provides the weaker time moves forward semantics. <p> In fact, the term time moves forward originated in Goel's work. The two-phase algorithm for record reclamation and create/delete disambiguation is adopted from Guy <ref> [Guy et al. 1993] </ref> and is related to the general topic of forming consensus in distributed environments. Heddaya et al. [Heddaya et al. 1989] use a two-phase gossip protocol to manage distributed event histories of updates to object replicas. <p> Our version vector compression algorithm forms consensus on the value of a given element and atomically removes it from the vector. As a consensus algorithm, it is closely related to Guy's garbage collection algorithm <ref> [Guy et al. 1993] </ref> and two-phase commit protocols [Gray 1978, Lampson et al. 1979]. However, unlike the earlier algorithms, our algorithm can afford to be one-phase, rather than requiring two.
Reference: [Heddaya et al. 1989] <author> Abdelsalam Heddaya, Meichun Hsu, and William Weihl. </author> <title> "Two Phase Gossip: Managing Distributed Event Histories." </title> <journal> Information Sciences, </journal> <volume> 49 </volume> <pages> 35-57, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In fact, the term time moves forward originated in Goel's work. The two-phase algorithm for record reclamation and create/delete disambiguation is adopted from Guy [Guy et al. 1993] and is related to the general topic of forming consensus in distributed environments. Heddaya et al. <ref> [Heddaya et al. 1989] </ref> use a two-phase gossip protocol to manage distributed event histories of updates to object replicas. However, their solution does not address the problem of completely forgetting that a history exists; it only addresses forgetting items in the history.
Reference: [Hedetniemi et al. 1988] <author> Sandra M. Hedetniemi, Stephen T. Hedetniemi, and Arthur L. Liestman. </author> <title> "A Survey of Gossiping and Broadcasting in Communication Networks." </title> <journal> NETWORKS, </journal> <volume> 18 </volume> <pages> 319-349, </pages> <year> 1988. </year>
Reference-contexts: Finally, the issue of garbage collection is related to the general "gossip" problem, in which each node in a graph must communicate a unique datum to every other node in the graph. A variety of researchers have done work in this area <ref> [Hedetniemi et al. 1988] </ref>. 10.3 Version Vector Management Version vectors are basically an implementation of Lamport clocks [Lamport 1978]. Since Parker et al.'s original work [Parker et al. 1983], very little research has been done on the version vector.
Reference: [Heidemann et al. 1992] <author> John S. Heidemann, Thomas W. Page, Jr., Richard G. Guy, and Gerald J. Popek. </author> <title> "Primarily Disconnected Operation: Experiences with Ficus." </title> <booktitle> In Proceedings of the Second Workshop on Management of Replicated Data, </booktitle> <pages> pp. 2-5. </pages> <institution> University of California, Los Angeles, IEEE, </institution> <month> November </month> <year> 1992. </year>
Reference: [Heidemann et al. 1994] <author> John S. Heidemann and Gerald J. Popek. </author> <title> "File-System Development with Stackable Layers." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(1) </volume> <pages> 58-89, </pages> <year> 1994. </year> <note> Preliminary version available as UCLA technical report CSD-930019. </note>
Reference-contexts: A second process called rec 17 18 CHAPTER 3. BACKGROUND onciliation runs periodically to guarantee consistency in the face of lost or undeliverable update notification messages. Ficus is implemented in SunOS 4.1.1 and is designed using stackable layers <ref> [Heidemann et al. 1994] </ref>. As such, it is tightly integrated in the kernel, making it difficult to distribute and port to other operating systems.
Reference: [Herlihy 1986] <author> Maurice Herlihy. </author> <title> "A Quorum-Consensus Replication Method for Abstract Data Types." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(1) </volume> <pages> 32-53, </pages> <month> February </month> <year> 1986. </year>
Reference: [Herring 1996] <author> Thomas A. Herring. </author> <title> "The Global Positioning System." </title> <journal> Scientific American, </journal> <volume> 274(2) </volume> <pages> 44-50, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Clock assumptions: We assume that the clocks are unsynchronized. Given the commonality of network partitions, the length of time that partitions may exist, and the number of system participants, it is simply infeasible to ensure clock synchronization. Systems like the Global Positioning System (GPS) <ref> [Herring 1996] </ref> may change this assumption in the future, but designing a solution for unsynchronized clocks forces the solution to be more robust in the face of clock-synchronization failures.
Reference: [Hisgen et al. 1989] <author> Andy Hisgen, Andrew Birrell, Timothy Mann, Michael Schroeder, and Garret Swart. </author> <title> "Availability and Consistency Tradeoffs in the Echo Distributed File System." </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1989. </year> <note> 142 REFERENCES </note>
Reference-contexts: However, building such control within the client-server model is significantly easier and provides less functionality than Roam's selective replication in the peer model. Alternatively, some have argued specifically against file-level granularity in the replication service, as in the case of the Echo distributed file system <ref> [Hisgen et al. 1989, Hisgen et al. 1990] </ref>. The authors argue the necessity of a large-granularity grouping mechanism for performance reasons, and specifically argue against file-level granularity. However, Echo utilizes quorum-based replication techniques with a primary 10.5.
Reference: [Hisgen et al. 1990] <author> Andy Hisgen, Andrew Birrell, Chuck Jerian, Timothy Mann, Michael Schroeder, and Garret Swart. </author> <title> "Granularity and Semantic Level of Replication in the Echo Distributed File System." </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data, </booktitle> <pages> pp. 2-4. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: However, building such control within the client-server model is significantly easier and provides less functionality than Roam's selective replication in the peer model. Alternatively, some have argued specifically against file-level granularity in the replication service, as in the case of the Echo distributed file system <ref> [Hisgen et al. 1989, Hisgen et al. 1990] </ref>. The authors argue the necessity of a large-granularity grouping mechanism for performance reasons, and specifically argue against file-level granularity. However, Echo utilizes quorum-based replication techniques with a primary 10.5.
Reference: [Honeyman et al. 1992] <author> Peter Honeyman, Larry Huston, Jim Rees, and Dave Bachmann. </author> <title> "The Little Work Project." </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <pages> pp. 11-14. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: However, optimistic replication is only one facet of the solution, as there are many different optimistic replication models. Systems based on the client-server model <ref> [Satyanarayanan et al. 1990, Honeyman et al. 1992] </ref> make a class-based distinction between "clients" and "servers." The client-server model by definition prohibits direct communication and synchronization among clients (i.e., the mobile machines), and servers are defined as expressly being non-mobile. <p> communicate with a local partner rather than a remote one, mobile users want the ability to directly communicate and synchronize with whomever is "nearby." Consistency can of course be correctly maintained even if two machines cannot directly synchronize with each other, as demonstrated by systems based on the client-server model <ref> [Honeyman et al. 1992, Satyanarayanan et al. 1990] </ref>, but local synchronization increases usability and the level of functionality while decreasing the inherent synchronization cost. The issue is usability, expected functionality, and inherent cost. <p> All types of data modifications and updates can be generated at the client. The client-server model has been successfully implemented in replication systems such as Coda [Satyanarayanan et al. 1990, Kistler et al. 1991] and Little Work <ref> [Honeyman et al. 1992] </ref>. The major drawback from the mobility vantage point is the communication restrictions placed on the client. Clients cannot intercommunicate or synchronize with each other, limiting their functionality when mobile. <p> Some of the same ideas could be applied in Roam; however, more research is required in optimizing communication protocols for weak connectivity in peer models. 123 124 CHAPTER 10. RELATED WORK 10.1.2 Little Work The Little Work project <ref> [Honeyman et al. 1992] </ref> is similar to Coda, but modifies only the clients, leaving the AFS [Howard et al. 1988] servers unaltered. Congestion caused by client's slow links is reduced in a variety of ways, including client-side modifications of AFS, its underlying RPC, and other congestion avoidance and control methods.
Reference: [Howard et al. 1988] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satya-narayanan, Robert Sidebotham, and Michael West. </author> <title> "Scale and Performance in a Distributed File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Multiple replicas may exist on the same machine. 3.3.2 Volumes The volume [Satyanarayanan et al. 1985] is a file system construct originally developed for the Andrew File System <ref> [Howard et al. 1988] </ref> but later used by replication systems such as Coda and Ficus. It is defined to be smaller than a disk partition but larger than a single directory; however, since it is a loose definition, there is typically no enforced maximum or minimum size. <p> RELATED WORK 10.1.2 Little Work The Little Work project [Honeyman et al. 1992] is similar to Coda, but modifies only the clients, leaving the AFS <ref> [Howard et al. 1988] </ref> servers unaltered. Congestion caused by client's slow links is reduced in a variety of ways, including client-side modifications of AFS, its underlying RPC, and other congestion avoidance and control methods. However, again clients cannot directly communicate, hindering the usability of the system in dynamic, mobile environments.
Reference: [Itai et al. 1990] <author> Alon Itai, Shay Kutten, Yaron Wolfstahl, and Shmuel Zaks. </author> <title> "Optimal Distributed t-Resilient Election in Complete Networks." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 415-420, </pages> <month> April </month> <year> 1990. </year>
Reference: [Jain 1991] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: Additionally, given the performance studies over both Ethernet and WaveLAN, we can compare the relative performance differences of Roam and Rumor. A statistical analysis of the difference between Roam and Rumor using the formulas for unpaired observations <ref> [Jain 1991] </ref> indicates that at the 95% confidence level, there is no significant difference in the relative performance of Roam and Rumor between the Ethernet and WaveLAN studies. In other words, the difference between Roam and Rumor remains constant, independent of the data transfer mechanism or speed.
Reference: [Kistler et al. 1991] <author> James J. Kistler and Mahadev Satyanarayanan. </author> <title> "Disconnected Operation in the Coda File System." </title> <type> Technical Report CMU-CS-91-166, </type> <institution> Carnegie-Mellon University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Regardless of the type of container, the systems force the container to always be fully-replicated |either the entire container is replicated locally or else none of it can be replicated locally. Coda <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref>, while permitting more flexible sharing arrangements at its clients, also forces full replication between its peer servers. The full-replication policy makes the replication algorithms easier to develop and implement, but causes headaches for mobile users. <p> However, the functionality of the clients is greatly improved, and multiple inter-communicating servers are permitted. All types of data modifications and updates can be generated at the client. The client-server model has been successfully implemented in replication systems such as Coda <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref> and Little Work [Honeyman et al. 1992]. The major drawback from the mobility vantage point is the communication restrictions placed on the client. Clients cannot intercommunicate or synchronize with each other, limiting their functionality when mobile. <p> Additionally, there are commercial and share-ware replication systems, such as Lotus Notes TM and rdist. Each system will be discussed briefly. 10.1.1 Coda The Coda file system <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref> is an optimistically replicated file system constructed on a client-server model, as opposed to the peer architecture proposed by Roam. Coda provides replication flexibility akin to selective replication at the clients, but not at the replicated servers, which are traditional peers.
Reference: [Kistler et al. 1992] <author> James J. Kistler and Mahadev Satyanarayanan. </author> <title> "Disconnected Operation in the Coda File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 3-25, </pages> <year> 1992. </year>
Reference-contexts: One area in which Coda is clearly superior is the low-bandwidth scenario. Coda has greatly optimized the communications and synchronization between client and server, especially in environments with weak connectivity <ref> [Kistler et al. 1992, Mummert et al. 1995] </ref>. Some of the same ideas could be applied in Roam; however, more research is required in optimizing communication protocols for weak connectivity in peer models. 123 124 CHAPTER 10. <p> However, again clients cannot directly communicate, hindering the usability of the system in dynamic, mobile environments. Additionally, the method used to populate the client with the necessary data is sub-optimal compared to other strategies <ref> [Kistler et al. 1992, Kuenning 1994] </ref>. 10.1.3 Bayou The Bayou system [Terry et al. 1995] is another replicated storage system, replicating databases rather than file system objects. Like Roam, it is based on the peer-to-peer model and provides support for application-dependent resolution of conflicts.
Reference: [Kleinrock 1997a] <author> Leonard Kleinrock. "Nomadicity." </author> <title> Presentation at the GloMo PI Meeting (February 4) at the University of California, </title> <address> Los Angeles, </address> <year> 1997. </year>
Reference-contexts: This section describes our definition of mobility, mobile machines, and mobile users. We then discuss the requirements that these definitions place on the replication system. 2.2.1 Definition of mobility Kleinrock <ref> [Kleinrock 1997a] </ref> defines four types of mobility, and we adopt his definitions here: Social: moving from one context or mind-set to another Physical: moving from one physical location to another Appliance: moving from device to device or system to system Application: moving from one task to another This dissertation is primarily
Reference: [Kleinrock 1997b] <author> Leonard Kleinrock. </author> <booktitle> "Progress in Nomadic Computing." </booktitle> <institution> Presentation at the UCLA Computer Science Department Research Review (April 10) at the University of California, </institution> <address> Los Angeles, </address> <year> 1997. </year>
Reference-contexts: SOLUTION REQUIREMENTS Similarly, users who only travel short distances when mobile nevertheless require much the same functionality as those that travel long distances <ref> [Kleinrock 1997b] </ref>. Regardless of the distance, any change in location should still be considered an act of mobility because of the changes in the environment, including but not limited to location, communication medium and accessible devices. <p> Finally, Kleinrock (among others) has recently observed that the term "client-server" is technically a misnomer, as the more appropriate description is client-network -server <ref> [Kleinrock 1997b] </ref>. The network is an integral part of the client-server model. Since nomadicity exacerbates network connectivity, the network effectively shrinks and often becomes non-existent.
Reference: [Kuenning 1994] <author> Geoffrey H. Kuenning. </author> <title> "The Design of the SEER Predictive Caching System." </title> <booktitle> In Proceedings of the Workshop on Mobile Computing Systems and Applications, </booktitle> <address> Santa Cruz, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: However, again clients cannot directly communicate, hindering the usability of the system in dynamic, mobile environments. Additionally, the method used to populate the client with the necessary data is sub-optimal compared to other strategies <ref> [Kistler et al. 1992, Kuenning 1994] </ref>. 10.1.3 Bayou The Bayou system [Terry et al. 1995] is another replicated storage system, replicating databases rather than file system objects. Like Roam, it is based on the peer-to-peer model and provides support for application-dependent resolution of conflicts.
Reference: [Kuenning 1997] <author> Geoffrey Houston Kuenning. Seer: </author> <title> Predictive File Hoarding for Disconnected Mobile Operation. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Los Angeles, Los Angeles, CA, </address> <month> May </month> <year> 1997. </year> <note> Also available as UCLA CSD Technical Report UCLA-CSD-970015. </note>
Reference-contexts: Unimportant data occupies otherwise usable disk space, which therefore cannot be used for more critical objects. In the mobile context, important data that cannot be locally stored causes problems ranging from minor inconveniences to complete stoppages of work and productivity, as described by Kuenning <ref> [Kuenning 1997] </ref>. Kuenning's studies of user behavior indicate that the set of required data can in fact be completely stored locally, but only if the underlying replication service provides the appropriate flexibility to individually select objects for replication. <p> First, additional data occupies what would otherwise be usable disk space. If the user does not require specific objects, they might as well not reside on the local disk, allowing that disk space to be used for more important data. Kuenning's studies with file system trace data <ref> [Kuenning 1997] </ref> indicate that the user's set of currently "required" data objects, called the working set, is small enough to fit completely on the portable computer. However, if disk space is occupied by unimportant data not in the user's current working-set, then the entire working set may no longer fit. <p> However, if disk space is occupied by unimportant data not in the user's current working-set, then the entire working set may no longer fit. When the complete working set cannot be locally stored, the mobile user encounters problems ranging from minor inconveniences to complete stoppages of work and productivity <ref> [Kuenning 1997] </ref>. Selective replication therefore allows for better disk space utilization and, in cases where the entire working set could not otherwise be locally stored, is required for correctness. Second, locally replicating unimportant data requires that additional time be spent to download updates and maintain consistency on the unimportant data. <p> Since the pathname is not available to the user through the file system, alternative methods must be employed to determine the correct name of the object before it can be added. However, this is not a major problem for three reasons. First, automated tools such as Seer <ref> [Kuenning 1997] </ref> will often be the primary clients of selective replication. Seer performs predictive hoarding of data for mobile computers; it watches the user's file system activity and ensures that the "important" objects get replicated on the user's portable prior to disconnection.
Reference: [Kuenning et al. 1994] <author> Geoffrey H. Kuenning, Gerald J. Popek, and Peter Reiher. </author> <title> "An Analysis of Trace Data for Predictive File Caching in Mobile Computing." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 291-306. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: A more recent study looked at traces taken in three different commercial environments. In studies of productivity environment data taken at Locus Computing <ref> [Kuenning et al. 1994] </ref>, Wang found that replicated objects tend to have only a small set of common writers at any one time [Wang et al. 1997].
Reference: [Kuenning et al. 1997] <author> Geoffrey H. Kuenning and Gerald J. Popek. </author> <title> "Automated Hoarding for Mobile Computers." </title> <booktitle> In Proceedings of the 16th Symposium on Operating Systems Principles, </booktitle> <address> St. Malo, France, </address> <month> October </month> <year> 1997. </year> <note> ACM. </note>
Reference-contexts: Both control and data flow are indicated. Only one source replica is indicated, though multiple source replicas may be queried to reconcile all of the data stored at the target. 72 CHAPTER 6. SELECTIVE REPLICATION Furthermore, the success of file hoarding systems like Seer <ref> [Kuenning et al. 1997] </ref> in peer contexts absolutely requires the ability of the replication substrate to make selective decisions. Seer watches file accesses and asks a replication substrate to hoard the necessary files on a laptop prior to network disconnection, to ensure correct operation during the disconnection period. <p> With Roam's new garbage-collection semantics, the volume actually resides within approximately 200MB, and once we switched replication substrates to Roam, we no longer experienced any disk space problems. Multiple people within the UCLA community are actively using Roam for their replication requirements. Additionally, a Seer <ref> [Kuenning et al. 1997] </ref> interface is being developed, so that Seer could use Roam as its replication substrate for predictive hoarding of files on portable computers. <p> Reconciliation could, for example, prioritize the user's files, and only transmit a subset of all available updates depending on the connectivity. Automated tools such as Seer <ref> [Kuenning et al. 1997] </ref>, which have knowledge of the files in active use, could automate the assignment of priorities. 11.8 Reconciliation Performance Improvements Chapter 5 mentioned a list of techniques that would increase the performance of reconciliation.
Reference: [Kumar 1994] <author> Puneet Kumar. </author> <title> Mitigating the Effects of Optimistic Replication in a Distributed File System. </title> <type> Ph.D. dissertation, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1994. </year> <note> Available as technical report CMU-CS-94-215. </note>
Reference-contexts: However, their algorithms are not appropriate in mobile environments. Deceit's solution is not guaranteed to operate correctly in environments with common network partitions [Siegel 1992]. Coda's solution (at the servers) uses logging of all directory operations and a log wrap-around technique when the log becomes full <ref> [Kumar 1994] </ref>. The log assists synchronization by removing the create/delete ambiguity: since the log contains complete update histories, the logs can be directly compared, and the correct actions can be determined.
Reference: [Kumar et al. 1993] <author> Puneet Kumar and Mahadev Satyanarayanan. </author> <title> "Supporting Application-Specific Resolution in an Optimistically Replicated File System." </title> <booktitle> In Proceedings of the Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pp. 66-70, </pages> <address> Napa, California, </address> <month> October </month> <year> 1993. </year> <title> IEEE. [Kure 1988] ivind Kure. "Optimization of File Migration in Distributed Systems." </title> <type> Technical Report UCB/CSD 88/413, </type> <institution> University of California, Berkeley, </institution> <month> April </month> <year> 1988. </year> <note> REFERENCES 143 </note>
Reference-contexts: Optimistic 3 strategies provide high availability by allowing independent updates to all replicas, detecting and resolving the possible conflicts created by concurrent updates some time later <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. However, optimistic replication is only one facet of the solution, as there are many different optimistic replication models. <p> When conflicts do occur, many can be resolved transparently and automatically without user involvement <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. <p> That is, data and the associated version vector are copied from the dominating replica to the dominated replica and installed verbatim. When version vectors are found to conflict, their respective file replicas are said to be "in conflict." Special mechanisms must be invoked to resolve the conflict <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. As replicas communicate and the pairwise version-vector comparison continues throughout all replicas, the most recent data propagates, and all replicas eventually converge to a common global state. 22 CHAPTER 3. <p> In the above example, the configuration file could be used to indicate that specific replicas do not store files matching the typical pattern for compiled objects, such as f*.og. Similar to update/update conflict-resolution files <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>, these configuration files would be specified on a per-directory or per-volume basis. A per-directory specification seems to make more sense, as the mapping remains with the directory whenever the directory is moved, and the regular expressions are generally shorter. <p> In this case, replica 3's element (value 2) is removed from all vectors, leaving the dominance relation the same. Version vectors are indicated as tuples of freplica identifier, valueg. 8.2 Dynamic Vector Compression Studies of replicated file systems <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref> illustrate that conflict rates are generally quite low, meaning that concurrent writing of the same object (within the synchronization time-window) rarely occurs. Experience with replication systems like Ficus and Rumor seems to expand upon this notion.
Reference: [Lamport 1978] <author> Leslie Lamport. </author> <title> "Time, Clocks, and the Ordering of Events in a Distributed System." </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: A variety of researchers have done work in this area [Hedetniemi et al. 1988]. 10.3 Version Vector Management Version vectors are basically an implementation of Lamport clocks <ref> [Lamport 1978] </ref>. Since Parker et al.'s original work [Parker et al. 1983], very little research has been done on the version vector.
Reference: [Lamport 1987] <author> Leslie Lamport, </author> <year> 1987. </year> <note> Email was sent by Leslie Lamport at 12:23:29 PDT on May 28, 1987, message ID 8705281923.AA09105@jumbo.dec.com. </note>
Reference-contexts: This variable ranges from zero (no mobility) to infinity (a mobile user need never return to his or her original "home"). Reliability Leslie Lamport has said "A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable" <ref> [Lamport 1987] </ref>. His comment was not entirely in jest. A distributed system has many inter-related machines, not unlike a large machine with many inter-connecting parts.
Reference: [Lamport 1989] <author> Leslie Lamport. </author> <title> "The Part-Time Parliament." </title> <type> Technical Report 49, </type> <institution> DEC Systems Research Center, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: Nevertheless, there are some similarities with the above, more robust algorithms. Specifically, the idea of algorithm restarts and multiple "rounds" is similar to the Paxos algorithm <ref> [Lamport 1989] </ref>. 10.4 Selective Replication Many replication systems based on the client-server model provide file granularity control over which objects should be physically stored at the clients. Coda and Little Work are two such examples. They recognize the need for fine-grain control at the file level.
Reference: [Lampson et al. 1979] <author> Butler W. Lampson and Howard E. Sturgis. </author> <title> "Crash recovery in a distributed data storage system." </title> <type> Technical report, </type> <institution> XEROX Palo Alto Research Center, </institution> <month> April </month> <year> 1979. </year>
Reference-contexts: allowing user access to it via a special name. 7.4 Garbage Collection in Ficus and Rumor Ficus and Rumor provide the no lost updates semantics, using a two-phase, coordinator-free, distributed solution [Guy et al. 1993, Ratner et al. 1996a] that is essentially a distributed implementation of the two-phase commit protocol <ref> [Gray 1978, Lampson et al. 1979] </ref>. The algorithm verifies that no participant completes before all participants are knowledgeable that completion is imminent. <p> Our version vector compression algorithm forms consensus on the value of a given element and atomically removes it from the vector. As a consensus algorithm, it is closely related to Guy's garbage collection algorithm [Guy et al. 1993] and two-phase commit protocols <ref> [Gray 1978, Lampson et al. 1979] </ref>. However, unlike the earlier algorithms, our algorithm can afford to be one-phase, rather than requiring two.
Reference: [Liskov et al. 1991] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> "Replication in the Harp File System." </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 226-238. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Additionally, its platform-independent design [Salomone 1998] makes porting Rumor to other operating systems straightforward and easy. From one vantage point, Roam could be seen as the next generation of Rumor, both in its implementation and its underlying algorithms. 126 CHAPTER 10. RELATED WORK 10.1.8 Harp The Harp file system <ref> [Liskov et al. 1991] </ref> implements replication for a Unix client-server environment using a primary-copy concurrency control mechanism. Harp achieves high performance and reliability by combining write-behind logging techniques with an uninterruptible power supply that allows logs to be forced to non-volatile storage after a power failure.
Reference: [Lynch 1996] <author> Nancy Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgane Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference: [Marchetti-Spaccamela 1987] <author> A. Marchetti-Spaccamela. </author> <title> "New protocols for the election of a leader in a ring." </title> <journal> Theoretical Computer Science, </journal> <volume> 54(1) </volume> <pages> 53-64, </pages> <month> September </month> <year> 1987. </year>
Reference: [Mummert et al. 1994] <author> Lily B. Mummert and Mahadev Satyanarayanan. </author> <title> "Large Granularity Cache Coherence for Intermittent Connectivity." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 279-289. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: The network is an integral part of the client-server model. Since nomadicity exacerbates network connectivity, the network effectively shrinks and often becomes non-existent. Coda has demonstrated that the client-server model can be optimized in several ways to handle the shrinking network problem <ref> [Mummert et al. 1994, Mummert et al. 1995] </ref>; however, an overall cleaner approach may result from utilizing a peer model, and therefore not relying on the network's presence or the need for optimizations. Larger replication factors Most replication systems only provide for a handful of replicas of any given object.
Reference: [Mummert et al. 1995] <author> Lily B. Mummert, Maria R. Ebling, and Mahadev Satyanarayanan. </author> <title> "Exploiting Weak Connectivity for Mobile File Access." </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 143-155, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: The network is an integral part of the client-server model. Since nomadicity exacerbates network connectivity, the network effectively shrinks and often becomes non-existent. Coda has demonstrated that the client-server model can be optimized in several ways to handle the shrinking network problem <ref> [Mummert et al. 1994, Mummert et al. 1995] </ref>; however, an overall cleaner approach may result from utilizing a peer model, and therefore not relying on the network's presence or the need for optimizations. Larger replication factors Most replication systems only provide for a handful of replicas of any given object. <p> One area in which Coda is clearly superior is the low-bandwidth scenario. Coda has greatly optimized the communications and synchronization between client and server, especially in environments with weak connectivity <ref> [Kistler et al. 1992, Mummert et al. 1995] </ref>. Some of the same ideas could be applied in Roam; however, more research is required in optimizing communication protocols for weak connectivity in peer models. 123 124 CHAPTER 10. <p> Roam does not take into account the connection quality when reconciling, unlike Coda which has implemented various optimizations for weak-connectivity environments <ref> [Mummert et al. 1995] </ref>. Reconciliation could, for example, prioritize the user's files, and only transmit a subset of all available updates depending on the connectivity.
Reference: [Nance 1995] <author> B. Nance. </author> <title> "File transfer on steroids." </title> <journal> BYTE Magazine, </journal> <month> February </month> <year> 1995. </year>
Reference-contexts: The replication paradigm is that slaves should always be identical to the master. The model is very simple, yielding a simple implementation, and has been used in many replication packages such as laplink <ref> [Nance 1995] </ref> and 1 Rumor can be downloaded from http://fmg-www.cs.ucla.edu/rumor. 3.3. REPLICATION DEFINITIONS AND TERMS 19 rdist [Cooper 1992]. However, with the simple model comes limited functionality: the slave is essentially read-only.
Reference: [Oki et al. 1988] <author> Brian M. Oki and Barbara Liskov. </author> <title> "Viewstamped Replication: A General Primary Copy Method to Support Highly-Available Distributed Systems." </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 8-17, </pages> <month> August </month> <year> 1988. </year>
Reference: [Ousterhout et al. 1989] <author> John Ousterhout and Fred Douglis. </author> <title> "Beating the I/O Bottleneck: A Case for Log-Structured File Systems." </title> <journal> Operating Systems Review, </journal> <volume> 23(1) </volume> <pages> 11-28, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: However, it is unclear what "replicating" such an object really implies, given that each generation of the object is potentially different. 2.3. SOLUTION CHARACTERISTICS 15 Update rate: We assume that the load presented to the file service is that of "engineering/office applications" <ref> [Ousterhout et al. 1989] </ref>, which consists of many applications using many small files, with a large degree of sequential read-write sharing but little concurrent sharing. This is considered the "standard" workload by most research file system designs. <p> This is considered the "standard" workload by most research file system designs. Update distribution: Again, we assume that the file system load is typical of "engineering/office applications" <ref> [Ousterhout et al. 1989] </ref>. The update distribution therefore contains considerable read-write sharing, but little simultaneous sharing. We expect the updates to be spread across a number of different replicas, rather than having all updates applied to a single replica. Replication granularity: Replication flexibility is extremely important to the mobile user.
Reference: [Page et al. 1991] <author> Thomas W. Page, Jr., Richard G. Guy, Gerald J. Popek, John S. Heidemann, Wai Mak, and Dieter Rothmeier. </author> <title> "Management of Replicated Volume Location Data in the Ficus Replicated File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 17-29. </pages> <institution> University of California, Los Angeles, USENIX, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Any replica can synchronize with any other replica, and any file system modification or update can be applied at any accessible replica. The peer model has been implemented in systems such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou [Terry et al. 1995], Ficus <ref> [Guy et al. 1990a, Page et al. 1991] </ref>, and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. The peer model provides a very rich and robust communication framework. <p> While Locus allows concurrent updates across partitions, the mechanism used to merge partitions is complex and does not scale, because it always attempts to maintain a perfect picture of partition membership. Locus can not, therefore, support mobility. 10.1.6 Ficus Ficus <ref> [Guy et al. 1990a, Page et al. 1991] </ref> is one of the intellectual and physical predecessors of Roam, and Roam therefore shares many of the same characteristics as Ficus.
Reference: [P^aris 1989] <author> Jehan-Fran~cois P^aris. </author> <title> "Voting with Bystanders." </title> <booktitle> In Proceedings of the Ninth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 394-401, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Systems based on conservative replication strategies [Prusker et al. 1990, Liskov et al. 1991, Brereton 1986, Davcev et al. 1985, Herlihy 1986, Thomas 1979, Guerraoui et al. 1996] are not suitable for replication in mobile environments. Conservative or pessimistic strategies such as primary-site [Stonebraker 1979], majority-vote <ref> [P^aris 1989, Renesse et al. 1988, Bastani et al. 1987] </ref> or token-based techniques do not allow multiple writers when the writers cannot directly communicate to serialize the order of operations. The conservative strategies provide very high consistency but decreased availability in terms of how often users can generate updates.
Reference: [Parker et al. 1983] <author> D. Stott Parker, Jr., Gerald Popek, Gerard Rudisin, Allen Stoughton, Bruce J. Walker, Evelyn Walton, Johanna M. Chow, David Edwards, Stephen Kiser, and Charles Kline. </author> <title> "Detection of Mutual Inconsistency in Distributed Systems." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(3) </volume> <pages> 240-247, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: In each case, the result of comparing the two vectors is indicated. Note that the result is achieved without actually analyzing physical file data. 3.3.4 Version vectors Version vectors <ref> [Parker et al. 1983] </ref> are a method for tracking updates in a distributed system. They are utilized in Ficus, Rumor, Bayou, and Roam; a variant is implemented in Coda. <p> Finally, system administration duties can easily be made from one site, instead of having to independently and serially access each site, some of which will be unavailable. The conflict is resolved by applying a version vector <ref> [Parker et al. 1983] </ref> approach to the status vector. Each element is implemented as a monotonically increasing counter, like the version vector. The low-order bits are used to indicate the actual status; the high-order bits implement the counter. <p> GARBAGE COLLECTION Chapter 8 Version Vector Maintenance Version vectors <ref> [Parker et al. 1983] </ref> are the mechanism used by Roam to track updates and compare data versions. To recap from Chapter 3, the version vector is an array of length equal to the number of replicas. <p> Eventually, when all replicas are consistent, all replicas will have equivalent version vectors. Table 3.1 illustrates a simple example. Version vectors are well known and proven correct <ref> [Parker et al. 1983] </ref>. However, they do not scale well in large systems, given that each replica maintains its own, dedicated position in the vector. <p> A variety of researchers have done work in this area [Hedetniemi et al. 1988]. 10.3 Version Vector Management Version vectors are basically an implementation of Lamport clocks [Lamport 1978]. Since Parker et al.'s original work <ref> [Parker et al. 1983] </ref>, very little research has been done on the version vector. Ramarao attempted to disprove the correctness of the version vector [Ramarao 1987], but his refutation stemmed from a misunderstanding, and the version vector has gained global acceptance as the staple behind optimistic replication.
Reference: [Popek 1997] <author> Gerald J. Popek, </author> <year> 1997. </year> <title> Personal communication by email, </title> <month> 26 February. </month>
Reference-contexts: Machines emerged that were truly portable and as powerful as their stationary cousins. Making use of this new type of machine with its attendant portability, users became increasingly mobile. In 1996, approximately one third of the computers sold were mobile-enabled: that is, portable form-factor with communications capability <ref> [Popek 1997] </ref>. With their new ability to compute while mobile, users unfortunately found themselves vastly under-equipped from a software architecture point of view. The hardware allowed them to be mobile, but the antiquated system software still assumed a relatively static world. <p> Additionally, the machine should not require extensive setup or tear-down time to make it mobile. Over one-third of form-factor machines sold today fit this description, and the market percentage is constantly growing <ref> [Popek 1997] </ref>. A mobile user is, therefore, someone who uses a mobile machine and occasionally travels with the machine. Regardless of how often the traveling occurs or how far the distance traveled, we include all such examples in our definition of a mobile user.
Reference: [Popek et al. 1981] <author> Gerald Popek, Bruce Walker, Johanna Chow, David Edwards, Charles Kline, Gerald Rudisin, and Greg Thiel. </author> <title> "LOCUS: A Network Transparent, High Reliability Distributed System." </title> <booktitle> In Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 169-177. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1981. </year> <note> 144 REFERENCES </note>
Reference-contexts: The peer model is not class based: all replicas are equals, or peers. Any replica can synchronize with any other replica, and any file system modification or update can be applied at any accessible replica. The peer model has been implemented in systems such as Locus <ref> [Walker et al. 1983, Popek et al. 1981] </ref>, Bayou [Terry et al. 1995], Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. <p> Furthermore, the in-memory tree can potentially absorb much of main memory. 10.1. REPLICATION SYSTEMS 125 10.1.5 Locus The university Locus operating system <ref> [Popek et al. 1981, Walker et al. 1983] </ref> provides volumes and allows selective replication within the volume. However, the approach taken toward replication is not strictly optimistic.
Reference: [Prusker et al. 1990] <author> Francis J. Prusker and Edward P. Wobber. </author> <title> "The Siphon: Managing Distant Replicated Repositories." </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data, </booktitle> <pages> pp. 44-47. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1990. </year>
Reference: [Queinnec et al. 1993] <author> Philippe Queinnec and Gerard Padiou. </author> <title> "Flight Plan Management in Distributed Air Traffic Control System." </title> <booktitle> In Proceedings of the International Symposium on Autonomous Decentralized Systems, </booktitle> <address> Kawasaki, Japan, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Finally, some have argued the need for larger replication factors independent of the mobile scenario, such as in the case of air traffic control <ref> [Queinnec et al. 1993] </ref>. Other examples of environments requiring larger replication factors include stock exchanges, network routing tables and mechanisms, large database systems like airline reservation systems, and military command and control. Read-only strategies and other class-based techniques cannot adequately solve the scaling problem, at least in the mobile scenario.
Reference: [Ramarao 1987] <author> K. V. S. Ramarao. </author> <title> "Comments on "Detection of Mutual Inconsistency in Distributed Systems"." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(6) </volume> <pages> 759-760, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Since Parker et al.'s original work [Parker et al. 1983], very little research has been done on the version vector. Ramarao attempted to disprove the correctness of the version vector <ref> [Ramarao 1987] </ref>, but his refutation stemmed from a misunderstanding, and the version vector has gained global acceptance as the staple behind optimistic replication. Some have commented on the poor scalability of the version vector, but no one, until now, has improved upon it.
Reference: [Ratner 1995] <author> David Howard Ratner. </author> <title> "Selective Replication: Fine-Grain Control of Replicated Files." </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> March </month> <year> 1995. </year> <note> Available as UCLA technical report CSD-950007. </note>
Reference-contexts: INTRODUCTION Model [Ratner et al. 1996c, Ratner et al. 1996b], it enables true mobility by providing: * The ability to optimistically replicate data. * Algorithms for the selective replication <ref> [Ratner et al. 1996a, Ratner 1995] </ref> of data; that is, data can be replicated independently of the logical "container" in which it resides. * A peer-based model so that any replica can directly communicate and synchronize with any other replica, regardless of its origin or original location in the system. * <p> We believe the file is the appropriate replication granularity in this environment. There are clearly other choices in the file system environment, such as directories, but the rationale behind namespace and semantic groupings often differs from that controlling replication requirements <ref> [Ratner 1995] </ref>. There is little necessity to replicate at any granularity smaller than a file, as the file object is typically treated as a unit, and a piece of a file is almost worthless to most applications. <p> Based on the traditional peer-to-peer model, it allows any two replicas to directly synchronize with each other. It was originally designed using the volume as its unit of replication, although selective replication was later added <ref> [Ratner 1995] </ref>. Ficus maintains data consistency with two separate mechanisms. At update time, update notification messages are sent to all accessible replicas in a "one-time, best-effort" manner. A second process called rec 17 18 CHAPTER 3. <p> Selective replication <ref> [Ratner et al. 1996a, Ratner 1995] </ref> allows objects from the volume or other large container to be individually selected for replication purposes. Selective replication provides a much-improved user model, both in terms of functionality and performance. The details regarding selective replication are discussed in Chapter 6. 3.3. <p> Having to individually name all objects is not a ward-specific phenomenon. Any volume-granularity replication facility can name a set of data by naming the volume; a system providing selective replication, such as Ficus and Rumor before Roam, requires each replica to individually list the objects it locally stores <ref> [Ratner 1995, Ratner et al. 1996a] </ref>. Individually naming the elements of the ward set is simply the higher-level equivalent of individually naming the elements of a replica set. <p> New names can be distinguished from old ones that have been deleted at one of the two replicas by a process known as create/delete disambiguation (discussed in depth in Chapter 7). Full details on the decision-making process can be found elsewhere <ref> [Guy et al. 1990c, Guy et al. 1993, Ratner 1995] </ref>. When all decisions have been made, and all requests given to the server, the recon process waits until the server has fulfilled all data requests and performed all system modifications. Upon notification from the server, the recon process terminates. <p> SELECTIVE REPLICATION files wherever they are stored. When users are not connected to other replicas, a special error code should be returned to the requesting process, indicating that the file exists but that no replica is accessible. Ficus <ref> [Ratner 1995] </ref> provides this level of implementation. However, Roam (and the underlying Rumor) is at the application level, not embedded in the operating system like Ficus. It cannot therefore easily represent non-local files differently to the user or return special access codes to requesting processes. <p> Consistency is periodically maintained on these attributes-only replicas; however, since consistency is maintained optimistically, there are still scenarios where the attributes-only replicas are inconsistent. We therefore treat the replicas like a cache, and resort to a multi-level polling and cache-replacement solution if the cache is invalid <ref> [Ratner 1995] </ref>. Treating the attributes-only replicas like a cache means that there is zero overhead or performance impact during the normal reconciliation case. An occasional, special reconciliation maintains periodic consistency. The directory-entry invariant basically requires that directories are always entirely replicated. <p> The inconsistencies in the status vector must also be resolved (case 3). The selective replication implementation in Ficus dynamically requests the attributes of the remote object and simply compares the two status vectors <ref> [Ratner 1995] </ref>. The dynamic request is possible and feasible because Ficus is a distributed file system and as such has access to the remote site's file system. Roam (and Rumor) do not have this capability. <p> While the number of wards may increase, the synchronization performance within each ward remains constant within statistical error. 9.2.5 Impact of selective replication We previously demonstrated that selective control did not create any additional performance costs for users during normal operation in Ficus, as measured by a series of micro-benchmarks <ref> [Ratner et al. 1996a, Ratner 1995] </ref>. However, it is probably more important to characterize the actual performance of synchronization under different selective replication patterns and workloads. We performed our experiments with the same two portable machines described in Section 9.2 and in the same environment.
Reference: [Ratner 1997] <author> David Ratner. </author> <title> "Selective Replication Design and Implementation in Rumor." Rumor internal design document, </title> <month> February </month> <year> 1997. </year>
Reference-contexts: Real time update propagation is much easier to enable in Roam than Rumor, and will most likely be enabled at some future point, as discussed in Chapter 11. Rumor, like Ficus, originally provided replication at the volume granularity. Selective replication was added later <ref> [Ratner 1997] </ref>, as one stage in the Roam redesign. Rumor is a real system in actual use, currently running on two different operating systems|Linux and Windows r 95. Where possible, Roam was constructed using existing Rumor code, to ease and speed development.
Reference: [Ratner et al. 1996a] <author> David Ratner, Gerald J. Popek, and Peter Reiher. </author> <title> "Peer Replication with Selective Control." </title> <type> Technical Report CSD-960031, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: INTRODUCTION Model [Ratner et al. 1996c, Ratner et al. 1996b], it enables true mobility by providing: * The ability to optimistically replicate data. * Algorithms for the selective replication <ref> [Ratner et al. 1996a, Ratner 1995] </ref> of data; that is, data can be replicated independently of the logical "container" in which it resides. * A peer-based model so that any replica can directly communicate and synchronize with any other replica, regardless of its origin or original location in the system. * <p> Naming: A collection of logically-connected files can easily be identified and acted upon as a single unit. 3.3.3 Selective replication For a variety of reasons discussed more fully in Chapter 2, replication control needs to be independent of the layout of the namespace <ref> [Ratner et al. 1996a] </ref>. The motivations and rationale behind replication are often different from those that govern namespace issues. <p> Selective replication <ref> [Ratner et al. 1996a, Ratner 1995] </ref> allows objects from the volume or other large container to be individually selected for replication purposes. Selective replication provides a much-improved user model, both in terms of functionality and performance. The details regarding selective replication are discussed in Chapter 6. 3.3. <p> Having to individually name all objects is not a ward-specific phenomenon. Any volume-granularity replication facility can name a set of data by naming the volume; a system providing selective replication, such as Ficus and Rumor before Roam, requires each replica to individually list the objects it locally stores <ref> [Ratner 1995, Ratner et al. 1996a] </ref>. Individually naming the elements of the ward set is simply the higher-level equivalent of individually naming the elements of a replica set. <p> The solution uses an adaptive ring for each file object, rather than one for the whole volume, and then coalesces multiple per-file rings into a single ring based on the intersection between replica sets <ref> [Ratner et al. 1996a] </ref>. The approach has previously been implemented and used for selective replication synchronization in both Ficus and Rumor. The full details are described in Chapter 6; we provide only a brief overview here. <p> We resolve remove/update conflicts by preserving the updated version of the data and allowing user access to it via a special name. 7.4 Garbage Collection in Ficus and Rumor Ficus and Rumor provide the no lost updates semantics, using a two-phase, coordinator-free, distributed solution <ref> [Guy et al. 1993, Ratner et al. 1996a] </ref> that is essentially a distributed implementation of the two-phase commit protocol [Gray 1978, Lampson et al. 1979]. The algorithm verifies that no participant completes before all participants are knowledgeable that completion is imminent. <p> Any site with a locally inaccessible object initiates garbage collection on that object; the algorithm either stops when a new name is discovered, or consensus is reached that the object is globally inaccessible and garbage collection completes. Algorithm details can be found in [Guy et al. 1993] and <ref> [Ratner et al. 1996a] </ref>. The algorithm performs adequately in Ficus and Rumor; however, its semantics often cause disks to become full of garbage waiting to be collected, since data is preserved at all replicas until algorithm completion. <p> While the number of wards may increase, the synchronization performance within each ward remains constant within statistical error. 9.2.5 Impact of selective replication We previously demonstrated that selective control did not create any additional performance costs for users during normal operation in Ficus, as measured by a series of micro-benchmarks <ref> [Ratner et al. 1996a, Ratner 1995] </ref>. However, it is probably more important to characterize the actual performance of synchronization under different selective replication patterns and workloads. We performed our experiments with the same two portable machines described in Section 9.2 and in the same environment.
Reference: [Ratner et al. 1996b] <author> David Ratner, Gerald J. Popek, and Peter Reiher. </author> <title> "The Ward Model: A Replication Architecture for Mobile Environments." </title> <type> Technical Report CSD-960045, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: INTRODUCTION Model <ref> [Ratner et al. 1996c, Ratner et al. 1996b] </ref>, it enables true mobility by providing: * The ability to optimistically replicate data. * Algorithms for the selective replication [Ratner et al. 1996a, Ratner 1995] of data; that is, data can be replicated independently of the logical "container" in which it resides. *
Reference: [Ratner et al. 1996c] <author> David Ratner, Gerald J. Popek, and Peter Reiher. </author> <title> "The Ward Model: A Scalable Replication Architecture for Mobility." </title> <booktitle> In Workshop on Object Replication and Mobile Computing, </booktitle> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: INTRODUCTION Model <ref> [Ratner et al. 1996c, Ratner et al. 1996b] </ref>, it enables true mobility by providing: * The ability to optimistically replicate data. * Algorithms for the selective replication [Ratner et al. 1996a, Ratner 1995] of data; that is, data can be replicated independently of the logical "container" in which it resides. *
Reference: [Reiher et al. 1993a] <author> P. Reiher, S. Crocker, J. Cook, T. Page, and G. Popek. </author> <title> "Tru*es|Secure File Sharing With Minimal System Administrator Intervention." </title> <booktitle> In Proceedings of the 1993 World Conference On Tools and Techniques for System Administration, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Clearly, for the Ward Model to truly support direct, efficient, any-to-any communication, the interaction with mobile-IP solutions must be analyzed and dealt with. 11.6 Integration With Tru*es The Ward Model could be better integrated with a security model like Truffles <ref> [Reiher et al. 1993a] </ref>. Since the wardd and the in-out server control all accesses into and off of the machine, we have the ability 11.7. PRIORITIZED RECONCILIATION 133 the underlying mobile-IP communication protocol.
Reference: [Reiher et al. 1993b] <author> P. Reiher, T. Page, S. Crocker, J. Cook, and G. Popek. </author> <title> "Tru*es|A Secure Service for Widespread File Sharing." </title> <booktitle> In Proceedings of the The Privacy and Security Research Group Workshop on Network and Distributed System Security, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: The argument is based on the assumption that security features must be encapsulated within the peer model, and thereby the unauthorized access of any peer thwarts all security barriers and mechanisms. However, projects such as Truffles <ref> [Reiher et al. 1993b] </ref> have demonstrated that security policies can be 10 CHAPTER 2. SOLUTION REQUIREMENTS modularized and logically situated around a peer replication framework while still remaining independent of the replication system. <p> Our solution does not provide read-only replicas. Read-only replication is fundamentally different from read-write replication in terms of control structure and system impact. Read-only replication could achieved with much simpler mechanisms than those proposed by this solution, and could be easily added on as an incremental improvement. Security policies <ref> [Reiher et al. 1993b] </ref> can be erected if the sole goal of the read-only replication is data protection and privacy. Number of system participants: A major goal of this dissertation is to enable widespread mobile computing. <p> The dissertation therefore assumes a high level of trust between the actual replicas. It is assumed that the replicas are neither malevolent nor Byzantine; solutions such as Truffles <ref> [Reiher et al. 1993b] </ref> can be used in conjunction to provide better security. This dissertation clearly acknowledges that data transport must always be protected, and provides for the encryption and authentication of all data traveling into and out of each replica, but leaves the higher-level security concerns to other modules. <p> Furthermore, the division of labor means that the server process controls the transport of all information into and out of the replica. The server process becomes the single point for security authentication and encapsulation of various security policies, enabling an easy incorporation of the Truffles <ref> [Reiher et al. 1993b] </ref> security manager. Furthermore, the single point allows a transport-independent design, so the reconciliation processes can be designed and execute independently of the physical transport mechanism being used between machines. Performance data on the architectural design is discussed later in Chapter 9. 54 CHAPTER 5. <p> Both selective replication and the Ward Model are not mechanisms for information hiding or security/privacy features; these should be handled in other ways, such as creating additional volumes for secure information and using security-enhancements like Truffles <ref> [Reiher et al. 1993b] </ref>. If the user wants different replication behavior based on physical geographic location, this is best handled with tools that query the Ward Model for information and supply the replication system with specific decisions, rather than building a more rigid structure into the Ward 6.3.
Reference: [Reiher et al. 1994] <author> Peter Reiher, John S. Heidemann, David Ratner, Gregory Skinner, and Gerald J. Popek. </author> <title> "Resolving File Conflicts in the Ficus File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 183-195. </pages> <institution> University of California, Los Angeles, USENIX, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Optimistic 3 strategies provide high availability by allowing independent updates to all replicas, detecting and resolving the possible conflicts created by concurrent updates some time later <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. However, optimistic replication is only one facet of the solution, as there are many different optimistic replication models. <p> When conflicts do occur, many can be resolved transparently and automatically without user involvement <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. <p> That is, data and the associated version vector are copied from the dominating replica to the dominated replica and installed verbatim. When version vectors are found to conflict, their respective file replicas are said to be "in conflict." Special mechanisms must be invoked to resolve the conflict <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>. As replicas communicate and the pairwise version-vector comparison continues throughout all replicas, the most recent data propagates, and all replicas eventually converge to a common global state. 22 CHAPTER 3. <p> CONSISTENCY MAINTENANCE new data from the remote replica and install it, along with its accompanying version vector. If the remote version conflicts with the local one, an update/update conflict has been detected and is handled by the conflict handling mechanism <ref> [Reiher et al. 1994] </ref>. Conflicts are handled in the same manner as in Ficus and Rumor: remote data is requested so that it can be accessed locally for either automatic or user-involved conflict resolution. <p> Having the name not appear in the local namespace suffers in that potentially more name conflicts can occur: that is, creations of multiple objects in the same directory with the same name. While typically rare 6.2. DESIGN AND IMPLEMENTATION 57 in normal operation <ref> [Reiher et al. 1994] </ref>, these conflicts could become more common when the names do not appear in the namespace. <p> In the above example, the configuration file could be used to indicate that specific replicas do not store files matching the typical pattern for compiled objects, such as f*.og. Similar to update/update conflict-resolution files <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref>, these configuration files would be specified on a per-directory or per-volume basis. A per-directory specification seems to make more sense, as the mapping remains with the directory whenever the directory is moved, and the regular expressions are generally shorter. <p> If the remote replica dominated the local replica, the data is installed in place of the existing, out-dated version. If the two replicas conflict, the data is preserved in a hidden place for use by the conflict resolution tools <ref> [Reiher et al. 1994] </ref>. In the cases where the data is the same and only the attributes need updating, we perform the local modification without requesting data or any additional information from the remote site. 6.4. <p> In this case, replica 3's element (value 2) is removed from all vectors, leaving the dominance relation the same. Version vectors are indicated as tuples of freplica identifier, valueg. 8.2 Dynamic Vector Compression Studies of replicated file systems <ref> [Kumar et al. 1993, Reiher et al. 1994] </ref> illustrate that conflict rates are generally quite low, meaning that concurrent writing of the same object (within the synchronization time-window) rarely occurs. Experience with replication systems like Ficus and Rumor seems to expand upon this notion.
Reference: [Reiher et al. 1996] <author> Peter Reiher, Jerry Popek, Michial Gunter, John Salomone, and David Ratner. </author> <title> "Peer-to-peer Reconciliation Based Replication for Mobile Computers." </title> <booktitle> In Proceedings of the ECOOP Workshop on Mobility and Replication, </booktitle> <month> July </month> <year> 1996. </year>
Reference-contexts: Gray assumes that transactions cannot be coalesced into a single transaction or update; additionally, he assumes that the entire transaction must be transmitted and replayed at every replica. Both assumptions are incorrect for typical file systems. Multiple updates can be viewed as a single large update; in fact, Rumor <ref> [Reiher et al. 1996] </ref> on Unix TM coalesces all updates that occur between synchronization intervals into a single update and only increments the associated version vector once. <p> Nevertheless, Ficus is a real system with over 256 man-months of practical use and experience. Many of Roam's distributed algorithms are either descendents or direct adaptations of the Ficus algorithms. 3.1.2 Rumor Rumor <ref> [Reiher et al. 1996, Salomone 1998] </ref> is the logical outgrowth of Ficus. It is the embodiment of the same optimistic algorithms and control structures in a user-level, file-system independent package. <p> The code base is separated into file-system independent and dependent portions. Porting Rumor to a new operating system involves only rewriting the file-system dependent portion. Rumor maintains consistency entirely with a periodic synchronization process known as reconciliation <ref> [Reiher et al. 1996] </ref>. Rather than performing real-time update propagation like Ficus, Rumor only maintains consistency at specific reconciliation intervals, or more generally whenever the user or the system initiates a reconciliation process. The tradeoff is one of system performance versus consistency. <p> The peer model has been implemented in systems such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou [Terry et al. 1995], Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor <ref> [Reiher et al. 1996, Salomone 1998] </ref> and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. The peer model provides a very rich and robust communication framework. However, it has typically suffered from scaling problems. <p> Building upon the peer model seems an esthetically cleaner approach, and ought to better preserve the pureness of the underlying framework. The goal is therefore to design a peer-based model that scales well. Replication services based on the traditional peer model, such as Ficus [Guy et al. 1990b], Rumor <ref> [Reiher et al. 1996] </ref>, and Bayou [Demers et al. 1994], all suffer from scaling problems. In response, we have designed the Ward Model. <p> Since these are orthogonal issues, it makes sense to have them logically separated in different processes. Additionally, the complexity of each is reduced by limiting its responsibilities; a process responsible for both would constantly be switching between data installation and object reconciliation duties. Rumor operates in such a mode <ref> [Reiher et al. 1996] </ref>. <p> A more detailed discussion can be found elsewhere <ref> [Reiher et al. 1996] </ref>. The file-system scanner infers the user's file system modifications that have been performed since the last scan of the file system and updates Roam's data structures accordingly. <p> More information regarding how Rumor (and Roam) formulate version vectors can be found in <ref> [Reiher et al. 1996, Salomone 1998] </ref>. Note that the hot versus cold distinction is on a per-object, per-replica basis. A given replica will typically be in the hot-spot for some objects and not for others. 8.2. DYNAMIC VECTOR COMPRESSION 91 The situation is therefore ripe for dynamic version vector compression. <p> Transparent remote access is an area of important future work for Roam. Ficus offers it by being tightly integrated in the kernel, which provides a functionality versus performance tradeoff. Ideally, Roam would provide the same functionality, but in a lightweight manner. 10.1.7 Rumor Rumor <ref> [Reiher et al. 1996] </ref> is the direct predecessor of Roam; in fact, much of Roam's implementation is directly based on modified Rumor code. Rumor was designed to be a direct adaptation of Ficus, only at the user level instead of integrated in the kernel.
Reference: [Renesse et al. 1988] <author> Robbert van Renesse and Andrew S. Tanenbaum. </author> <title> "Voting with Ghosts." </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 456-461. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Systems based on conservative replication strategies [Prusker et al. 1990, Liskov et al. 1991, Brereton 1986, Davcev et al. 1985, Herlihy 1986, Thomas 1979, Guerraoui et al. 1996] are not suitable for replication in mobile environments. Conservative or pessimistic strategies such as primary-site [Stonebraker 1979], majority-vote <ref> [P^aris 1989, Renesse et al. 1988, Bastani et al. 1987] </ref> or token-based techniques do not allow multiple writers when the writers cannot directly communicate to serialize the order of operations. The conservative strategies provide very high consistency but decreased availability in terms of how often users can generate updates.
Reference: [Salomone 1998] <author> John Raymond Salomone. </author> <title> "Portable Rumor: A Platform Independent File Replication Package." </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> June </month> <year> 1998. </year> <note> To be finished. Expected completion: Spring 1998. </note>
Reference-contexts: Nevertheless, Ficus is a real system with over 256 man-months of practical use and experience. Many of Roam's distributed algorithms are either descendents or direct adaptations of the Ficus algorithms. 3.1.2 Rumor Rumor <ref> [Reiher et al. 1996, Salomone 1998] </ref> is the logical outgrowth of Ficus. It is the embodiment of the same optimistic algorithms and control structures in a user-level, file-system independent package. <p> Rumor's functionality is more limited than Ficus's, because it does not currently support either update propagation (real-time distribution of updates) or transparent remote access. Nevertheless, it does support the optimistic replication of data in a manner that is largely file-system independent <ref> [Salomone 1998] </ref>. The code base is separated into file-system independent and dependent portions. Porting Rumor to a new operating system involves only rewriting the file-system dependent portion. Rumor maintains consistency entirely with a periodic synchronization process known as reconciliation [Reiher et al. 1996]. <p> The peer model has been implemented in systems such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou [Terry et al. 1995], Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor <ref> [Reiher et al. 1996, Salomone 1998] </ref> and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. The peer model provides a very rich and robust communication framework. However, it has typically suffered from scaling problems. <p> A complete list of the replicated attributes and their specific purpose is discussed in <ref> [Salomone 1998] </ref>. Since the filelist contains only attributes and no data, it is small and relatively inexpensive to transfer. 5.2.2 Decision module Each scanning process generates a set of replication meta-information about the state of the replicated data on that machine. <p> If the remote version vector dominates the local one, then remote data is requested. The server will asynchronously receive the 2 For portability between operating systems, the replicated attributes also contain attributes native to other file systems when replicas exist on multiple operating system platforms <ref> [Salomone 1998] </ref>. 48 CHAPTER 5. CONSISTENCY MAINTENANCE new data from the remote replica and install it, along with its accompanying version vector. If the remote version conflicts with the local one, an update/update conflict has been detected and is handled by the conflict handling mechanism [Reiher et al. 1994]. <p> More information regarding how Rumor (and Roam) formulate version vectors can be found in <ref> [Reiher et al. 1996, Salomone 1998] </ref>. Note that the hot versus cold distinction is on a per-object, per-replica basis. A given replica will typically be in the hot-spot for some objects and not for others. 8.2. DYNAMIC VECTOR COMPRESSION 91 The situation is therefore ripe for dynamic version vector compression. <p> Ficus is tightly integrated in the file system, and as such porting between systems is not straightforward. In contrast, Roam is entirely at the user level, and therefore trivial to port between different Unix versions. Additionally, since Roam is designed in a platform-independent manner <ref> [Salomone 1998] </ref>, it should not be difficult to port it to non-Unix systems. Ficus also provides transparent remote access, a feature that Roam does not currently offer. <p> However, Rumor was the first optimistic peer replication system to operate completely at the user level, and partly for that reason pioneered peer replication for mobile use. It is a portable system that can easily be installed on different Unix platforms. Additionally, its platform-independent design <ref> [Salomone 1998] </ref> makes porting Rumor to other operating systems straightforward and easy. From one vantage point, Roam could be seen as the next generation of Rumor, both in its implementation and its underlying algorithms. 126 CHAPTER 10.
Reference: [Sandberg et al. 1985] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> "Design and Implementation of the Sun Network File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 119-130. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1985. </year> <note> REFERENCES 145 </note>
Reference-contexts: For example, given two replicas of a spreadsheet in an office environment and twenty mobile users who require access to it, the replication factor of the object increases tenfold. Replication factors can often be minimized in office environments due to LAN-style sharing and remote-access capabilities like NFS TM <ref> [Sandberg et al. 1985] </ref>: one machine stores a replica that every machine in the office can access quickly, cheaply and easily. However, network-based file sharing cannot be utilized in mobile environments due to the frequency of network partitions and the wide range of available bandwidth and transfer latency.
Reference: [Satyanarayanan 1992] <author> Mahadev Satyanarayanan. </author> <title> "The Influence of Scale on Distributed File System Design." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-18(1):1-8, </volume> <month> January </month> <year> 1992. </year>
Reference-contexts: The argument is that since mobile computers are physically less secure, they should be "second-class" citizens with respect to the highly secure servers located behind locked doors <ref> [Satyanarayanan 1992] </ref>. The class-based distinction supposedly provides improved security by limiting the potential security breach to only a second-class object. <p> Scaling has not typically been an issue with replication services, because most scenarios have not required large numbers (more than a dozen) of writable replicas. Some have, in fact, argued that peer solutions by their nature simply cannot scale to large numbers <ref> [Satyanarayanan 1992, Gray et al. 1996] </ref>. However, while mobile environments seem to require a peer-based solution (as described above), they also seem to negate the assumption that a handful of replicas is enough.
Reference: [Satyanarayanan et al. 1985] <author> Mahadev Satyanarayanan, John H. Howard, David A. Nichols, Robert N. Sidebotham, Alfred Z. Spector, and Michael J. West. </author> <title> "The ITC Distributed File System: </title> <booktitle> Principles and Design." In Proceedings of the Tenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 35-50. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: Roam is a replication solution redesigned especially for mobile computing. Built using the Ward 1 A volume is smaller than a file system but larger than a directory <ref> [Satyanarayanan et al. 1985] </ref>. <p> A given data object may only have one replica, in which case it is the only copy in existence, or it may have N replicas, with N ranging into the low hundreds. Multiple replicas may exist on the same machine. 3.3.2 Volumes The volume <ref> [Satyanarayanan et al. 1985] </ref> is a file system construct originally developed for the Andrew File System [Howard et al. 1988] but later used by replication systems such as Coda and Ficus.
Reference: [Satyanarayanan et al. 1990] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasaki, Ellen H. Siegel, and David C. Steere. "Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment." </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 447-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: However, optimistic replication is only one facet of the solution, as there are many different optimistic replication models. Systems based on the client-server model <ref> [Satyanarayanan et al. 1990, Honeyman et al. 1992] </ref> make a class-based distinction between "clients" and "servers." The client-server model by definition prohibits direct communication and synchronization among clients (i.e., the mobile machines), and servers are defined as expressly being non-mobile. <p> Regardless of the type of container, the systems force the container to always be fully-replicated |either the entire container is replicated locally or else none of it can be replicated locally. Coda <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref>, while permitting more flexible sharing arrangements at its clients, also forces full replication between its peer servers. The full-replication policy makes the replication algorithms easier to develop and implement, but causes headaches for mobile users. <p> communicate with a local partner rather than a remote one, mobile users want the ability to directly communicate and synchronize with whomever is "nearby." Consistency can of course be correctly maintained even if two machines cannot directly synchronize with each other, as demonstrated by systems based on the client-server model <ref> [Honeyman et al. 1992, Satyanarayanan et al. 1990] </ref>, but local synchronization increases usability and the level of functionality while decreasing the inherent synchronization cost. The issue is usability, expected functionality, and inherent cost. <p> However, the functionality of the clients is greatly improved, and multiple inter-communicating servers are permitted. All types of data modifications and updates can be generated at the client. The client-server model has been successfully implemented in replication systems such as Coda <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref> and Little Work [Honeyman et al. 1992]. The major drawback from the mobility vantage point is the communication restrictions placed on the client. Clients cannot intercommunicate or synchronize with each other, limiting their functionality when mobile. <p> Additionally, there are commercial and share-ware replication systems, such as Lotus Notes TM and rdist. Each system will be discussed briefly. 10.1.1 Coda The Coda file system <ref> [Satyanarayanan et al. 1990, Kistler et al. 1991] </ref> is an optimistically replicated file system constructed on a client-server model, as opposed to the peer architecture proposed by Roam. Coda provides replication flexibility akin to selective replication at the clients, but not at the replicated servers, which are traditional peers.
Reference: [Satyanarayanan et al. 1993] <author> Mahadev Satyanarayanan, James J. Kistler, Lily B. Mummert, Maria R. Ebling, Puneet Kumar, and Qi Lu. </author> <title> "Experience with Disconnected Operation in a Mobile Computing Environment." </title> <booktitle> In Proceedings of the USENIX Symposium on Mobile and Location-Independent Computing, </booktitle> <pages> pp. 11-28, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1993. </year> <booktitle> USENIX. </booktitle>
Reference: [Siegel 1992] <author> Alexander Siegel. </author> <title> Performance in Flexible Distributed File Systems. </title> <type> Ph.D. dissertation, </type> <institution> Cor-nell, </institution> <month> February </month> <year> 1992. </year> <note> Also available as Cornell technical report TR 92-1266. </note>
Reference-contexts: Bayou supports mobility in the same way as all systems based on the traditional peer-to-peer model. They allow direct any-to-any communication, but suffer the same scaling problems as Rumor and Ficus when faced with mobile environments. 10.1.4 Deceit The ISIS environment's Deceit file system <ref> [Siegel et al. 1990, Siegel 1992] </ref> places all files into one "volume" and allows each individual file to be replicated independently with varying numbers of replicas. In this sense it provides selective replication. <p> Roam's solution is not the only one, however. Coda and Deceit both have much simpler garbage collection algorithms. However, their algorithms are not appropriate in mobile environments. Deceit's solution is not guaranteed to operate correctly in environments with common network partitions <ref> [Siegel 1992] </ref>. Coda's solution (at the servers) uses logging of all directory operations and a log wrap-around technique when the log becomes full [Kumar 1994].
Reference: [Siegel et al. 1990] <author> Alex Siegel, Kenneth Birman, and Keith Marzullo. "Deceit: </author> <title> A Flexible Distributed File System." </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pp. 51-61. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Bayou supports mobility in the same way as all systems based on the traditional peer-to-peer model. They allow direct any-to-any communication, but suffer the same scaling problems as Rumor and Ficus when faced with mobile environments. 10.1.4 Deceit The ISIS environment's Deceit file system <ref> [Siegel et al. 1990, Siegel 1992] </ref> places all files into one "volume" and allows each individual file to be replicated independently with varying numbers of replicas. In this sense it provides selective replication. <p> In this sense it provides selective replication. However, Deceit employs a conservative approach to replication, namely a writer-token mechanism, and therefore cannot provide the high availability offered by optimistic mechanisms. In addition, Deceit cannot tolerate long-term network partitions; instead, only a related failure known as a virtual partition <ref> [Siegel et al. 1990] </ref>, which eventually corrects itself, is tolerated. These factors make the Deceit system unsuitable for the environments discussed in Chapter 2. Nevertheless, the Deceit system has several good features, among them simple user controls for specifying replication factors.
Reference: [Singh 1994] <author> Gurdip Singh. </author> <title> "Leader Election in the Presence of Link Failures." </title> <booktitle> In Symposium on Principles of Distributed Computing (PODC '94), </booktitle> <pages> pp. 375-375, </pages> <address> New York, USA, </address> <month> August </month> <year> 1994. </year> <note> ACM Press. </note>
Reference: [Skeen 1981] <author> Dale Skeen. </author> <title> "Nonblocking Commit Protocols." </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. 133-142, </pages> <month> May </month> <year> 1981. </year>
Reference: [Smith 1981] <author> Alan J. Smith. </author> <title> "Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 7(4), </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Update sharing was even rarer. Just 2.4% of all files written (and less than 1% of user-owned files) were updated by multiple writers. Similar results were found for directories. Analyses of commercial systems suggests similar conclusions. The re-analysis of trace data collected in the 1970s <ref> [Smith 1981] </ref> from three commercial IBM timesharing environments found that from 3% to 12% of the files accessed are updated by multiple users [Kure 1988]. A more recent study looked at traces taken in three different commercial environments.
Reference: [Stonebraker 1979] <author> Michael Stonebraker. </author> <title> "Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRES." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(3), </volume> <month> May </month> <year> 1979. </year>
Reference-contexts: Systems based on conservative replication strategies [Prusker et al. 1990, Liskov et al. 1991, Brereton 1986, Davcev et al. 1985, Herlihy 1986, Thomas 1979, Guerraoui et al. 1996] are not suitable for replication in mobile environments. Conservative or pessimistic strategies such as primary-site <ref> [Stonebraker 1979] </ref>, majority-vote [P^aris 1989, Renesse et al. 1988, Bastani et al. 1987] or token-based techniques do not allow multiple writers when the writers cannot directly communicate to serialize the order of operations.
Reference: [Tait et al. 1991] <author> Carl D. Tait and Dan Duchamp. </author> <title> "Service Interface and Replica Consistency Algorithm for Mobile File System Clients." </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pp. 190-197, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Furthermore, the notion of using uninterruptible power supplies is completely unsuitable for mobile computers that are often untethered. 10.1.9 Tait and Duchamp Tait and Duchamp designed an algorithm for replica management in mobile file systems <ref> [Tait et al. 1991] </ref> based on the client-server model. They allow the servers to be geographically separated, and allow clients to dynamic change their primary server. Their "dynamic client-server" model is therefore similar to the Ward Model, but with some substantial differences.
Reference: [Terry et al. 1994] <author> D.B. Terry, A.J. Demers, K. Petersen, M.J. Spreitzer, M.M. Theimer, and B.B. Welch. </author> <title> "Session Guarantees for Weakly Consistent Replicated Data." </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pp. 140-149, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: However, unlike Roam, Bayou does not attempt to provide transparent conflict detection. Applications must specify a condition that determines when a conflicting access has been made, and must specify the particular resolution process. Bayou provides session guarantees <ref> [Terry et al. 1994] </ref> to improve the perceived consistency by users. Additionally, Bayou establishes strong guarantees about its data|writes can be classified either as committed or tentative.
Reference: [Terry et al. 1995] <author> Douglas B. Terry, Marvin M. Theimer, Karin Petersen, Alan J. Demers, Mike J. Spre-itzer, and Carl H. Hauser. </author> <title> "Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System." </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 172-183, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Additionally, most peer models utilize a replication granularity that users find inefficient in synchronization time and disk space usage. Both Ficus and Rumor initially provided replication at the volume granularity; 1;2 Bayou <ref> [Terry et al. 1995] </ref> replicates entire databases. Regardless of the type of container, the systems force the container to always be fully-replicated |either the entire container is replicated locally or else none of it can be replicated locally. <p> Palmtop computers are becoming more common, and there is even a wristwatch that can download calendar data from another computer. Researchers <ref> [Terry et al. 1995, Want et al. 1995] </ref> have built systems that allow laptop and palmtop machines to share data dynamically and opportunistically. <p> Any replica can synchronize with any other replica, and any file system modification or update can be applied at any accessible replica. The peer model has been implemented in systems such as Locus [Walker et al. 1983, Popek et al. 1981], Bayou <ref> [Terry et al. 1995] </ref>, Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. <p> However, again clients cannot directly communicate, hindering the usability of the system in dynamic, mobile environments. Additionally, the method used to populate the client with the necessary data is sub-optimal compared to other strategies [Kistler et al. 1992, Kuenning 1994]. 10.1.3 Bayou The Bayou system <ref> [Terry et al. 1995] </ref> is another replicated storage system, replicating databases rather than file system objects. Like Roam, it is based on the peer-to-peer model and provides support for application-dependent resolution of conflicts. However, unlike Roam, Bayou does not attempt to provide transparent conflict detection.
Reference: [Thomas 1979] <author> Robert H. Thomas. </author> <title> "A Majority Consensus Approach to Concurrency Control for Multiple Copy Databases." </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <month> June </month> <year> 1979. </year>
Reference: [Walker et al. 1983] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. </author> <title> "The LOCUS Distributed Operating System." </title> <booktitle> In Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 49-70. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year> <note> 146 REFERENCES </note>
Reference-contexts: The peer model is not class based: all replicas are equals, or peers. Any replica can synchronize with any other replica, and any file system modification or update can be applied at any accessible replica. The peer model has been implemented in systems such as Locus <ref> [Walker et al. 1983, Popek et al. 1981] </ref>, Bayou [Terry et al. 1995], Ficus [Guy et al. 1990a, Page et al. 1991], and Rumor [Reiher et al. 1996, Salomone 1998] and has more generally been espoused in other distributed environments such as xFS in the NOW project [Anderson et al. 1995]. <p> Furthermore, the in-memory tree can potentially absorb much of main memory. 10.1. REPLICATION SYSTEMS 125 10.1.5 Locus The university Locus operating system <ref> [Popek et al. 1981, Walker et al. 1983] </ref> provides volumes and allows selective replication within the volume. However, the approach taken toward replication is not strictly optimistic.
Reference: [Wang et al. 1997] <author> An-I A. Wang, Peter L. Reiher, and Rajive Bagrodia. </author> <title> "A Simulation Framework for Evaluating Replicated Filing Environments." </title> <type> Technical Report CSD-970018, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: However, they do not scale well in large systems, given that each replica maintains its own, dedicated position in the vector. Various researchers have commented on the size of the version vector as a primary scaling problem in Ficus, and a simulation of Rumor demonstrated the scaling problems concretely <ref> [Wang et al. 1997] </ref>. The data structures simply become too large as the number of replicas increases. In an effort to remedy the scaling problem, we noted two key observations: 1. Updates typically occur in a few isolated "hot-spots," as opposed to concurrently at all replicas. 2. <p> A more recent study looked at traces taken in three different commercial environments. In studies of productivity environment data taken at Locus Computing [Kuenning et al. 1994], Wang found that replicated objects tend to have only a small set of common writers at any one time <ref> [Wang et al. 1997] </ref>. In short, while we must provide the ability for everyone to generate updates, it seems rare that a significant percentage of the replicas are trying to do so at the same time|a hypothesis we call the hot-spot hypothesis. <p> These techniques included building the filelists in parallel and building the filelists entirely "off-line." 11.9 Reconciliation Topologies Concurrently with Roam's development, work is ongoing with respect to the effects of different reconciliation topologies on processor utilization, overall performance, message complexity, conflict rates, and other parameters <ref> [Wang et al. 1997] </ref>. When complete, those results should be incorporated, possibly altering Roam's reconciliation topologies. 134 CHAPTER 11.
Reference: [Want et al. 1995] <author> Roy Want, Bill N. Schilit, Norman I. Adams, Rich Gold, Karin Petersen, David Goldberg, John R. Ellis, and Mark Weiser. </author> <title> "An Overview of the ParcTab Ubiquitous Computing Experiment." </title> <journal> IEEE Personal Communications Magazine, </journal> <volume> 2(6) </volume> <pages> 28-43, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Palmtop computers are becoming more common, and there is even a wristwatch that can download calendar data from another computer. Researchers <ref> [Terry et al. 1995, Want et al. 1995] </ref> have built systems that allow laptop and palmtop machines to share data dynamically and opportunistically.
Reference: [Welch et al. 1986] <author> Brent Welch and John Ousterhout. </author> <title> "Prefix Tables: A Simple Mechanism for Locating Files in a Distributed System." </title> <booktitle> Sixth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 184-189, </pages> <month> May 19-23, </month> <year> 1986. </year>
Reference-contexts: However, there is no need for any files under Src, Misc, or Old, so these subtrees are not stored locally. A common alternate solution employs prefix pointers <ref> [Welch et al. 1986] </ref>, a second directory structure used to connect the user's disconnected namespace. While prefix-pointer solutions avoid storing intermediate directories, they must maintain an independent directory structure and integrate it into the user's namespace. <p> Our selective replication implementation solved the local availability problem by enforcing the system invariant of full backstoring (Chapter 6). Deceit instead implements the directory structure using an in-memory binary tree of all hard links, backed up to non-volatile storage. Similar to the "prefix-pointer" mechanism <ref> [Welch et al. 1986] </ref>, Deceit's solution has the advantage that it saves the system from locally storing the intermediate directories, but has the disadvantage of added complexity, both to maintain the list and to garbage collect from it. Furthermore, the in-memory tree can potentially absorb much of main memory. 10.1.
Reference: [Wiseman 1988] <author> Simon R. Wiseman. </author> <title> Garbage Collection in Distributed Systems. </title> <type> Ph.D. dissertation, </type> <institution> University of Newcastle Upon Tyne, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Heddaya et al. [Heddaya et al. 1989] use a two-phase gossip protocol to manage distributed event histories of updates to object replicas. However, their solution does not address the problem of completely forgetting that a history exists; it only addresses forgetting items in the history. Wiseman's survey <ref> [Wiseman 1988] </ref> of distributed garbage collection methods includes several techniques 128 CHAPTER 10. RELATED WORK based on reference counting, but none are designed for use on replicated objects, and none are directly applicable to imperfectly connected networks.
References-found: 110

