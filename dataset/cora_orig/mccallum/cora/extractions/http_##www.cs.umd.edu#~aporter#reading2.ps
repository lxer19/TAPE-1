URL: http://www.cs.umd.edu/~aporter/reading2.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/references.html
Root-URL: 
Email: aporter@cs.umd.edu votta@research.bell-labs.com  
Title: Comparing Detection Methods For Software Requirements Inspections: A Replication Using Professional Subjects  
Author: Adam Porter Lawrence Votta 
Keyword: Inspection, Controlled Experiment, Replication.  
Note: This work is supported in part by a National Science Foundation Faculty Early Career Development Award CCR-9501354.  
Address: College Park, Maryland 20742 Naperville, Illinois 60566  
Affiliation: Computer Science Department Software Production Research Department University of Maryland Lucent Technologies  
Abstract: Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. In previous work we evaluated this hypothesis using 48 graduate students in computer science as subjects. We now have replicated this experiment using 18 professional developers from Lucent Technologies as subjects. Our goals were to (1) extend the external credibility of our results by studying professional developers, and to (2) compare the performances of professionals with that of the graduate students to better understand how generalizable the results of the less expensive student experiments were. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detection rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate). For both the professionals and the students the experimental results are that (1) the Scenario method had a higher fault detection rate than either Ad Hoc or Checklist methods, (2) Checklist reviewers were no more effective than Ad Hoc reviewers, (3) Collection meetings produced no net improvement in the fault, and detection rate - meeting gains were offset by meeting losses, Finally, although specific measures differed between the professional and student populations, the outcomes of almost all statistical tests were identical. This suggests that the graduate students provided an adequate model of the professional population and that the much greater expense of conducting studies with professionals may not always be required. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> IEEE Guide to Software Requirements Specifications. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1984. </year> <note> IEEE Std 830-1984. </note>
Reference-contexts: The references for these lectures were Fagan [7], Parnas [15], and the IEEE Guide to Software Requirements Specifications <ref> [1] </ref>. The participants were then assembled into three-person teams see Section 2.1.3 for details.
Reference: [2] <author> Mark A. Ardis. </author> <title> Lessons from using basic lotos. </title> <booktitle> In Proceedings of the Sixteenth International Conference on Software Engineering, </booktitle> <pages> pages 5-14, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation. (See Section 2.2.) Although several groups at AT&T and elsewhere are experimenting with formal notations <ref> [2, 8] </ref>, it is not the industry's standard practice. Secondly, the specifications are considerably smaller than industrial ones. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [3] <author> V. R. Basili and D. M. Weiss. </author> <title> Evaluation of a software requirements document by analysis of change data. </title> <booktitle> In Proceedings of the Fifth International Conference on Software Engineering, </booktitle> <pages> pages 314-323, </pages> <address> San Diego, CA, </address> <month> March </month> <year> 1981. </year>
Reference-contexts: As a result, Scenario responsibilities are distinct subsets of Checklist and Ad Hoc responsibilities. The relationship between the three methods is depicted in Figure 2. The taxonomy is a composite of two schemes developed by Schneider, et al. [17] and Basili and Weiss <ref> [3] </ref>. Faults are divided into two broad types: omission in which important information is left unstated and commission - in which incorrect, redundant, or ambiguous information is put into the SRS by the author.
Reference: [4] <author> Barry W. Boehm. </author> <title> Software Engineering Economics. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Consequently, we will discuss only work directly related to our current efforts. Fagan [7] defined the basic software inspection process. While most writers have endorsed his approach <ref> [4, 11] </ref>, Parnas and Weiss are more critical [15]. In part, they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack systematic techniques for meeting those responsibilities.
Reference: [5] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see <ref> [5] </ref>, pp. 165ff and 210ff or [9]).
Reference: [6] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Scott A. Vander Wiel, and Lawrence G. Votta. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Secondly, the specifications are considerably smaller than industrial ones. 3. The inspection process in our experimental design may not be representative of software development practice. We have modeled our experiment's inspection process after the one used in several development organizations within AT&T <ref> [6] </ref>. Although this process is similar to a Fagan-style inspection, there are some differences. One difference is that reviewers use the fault detection activity to to find faults, not just to prepare for the inspection meeting.
Reference: [7] <author> M. E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: Consequently, we will discuss only work directly related to our current efforts. Fagan <ref> [7] </ref> defined the basic software inspection process. While most writers have endorsed his approach [4, 11], Parnas and Weiss are more critical [15]. In part, they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack systematic techniques for meeting those responsibilities. <p> Faults number 10 and 11, found by reviewer 12 of team C for the WLMS specification are shown. requirements notation, inspection procedures, the fault classification scheme, and the filling out of data collection forms. The references for these lectures were Fagan <ref> [7] </ref>, Parnas [15], and the IEEE Guide to Software Requirements Specifications [1]. The participants were then assembled into three-person teams see Section 2.1.3 for details.
Reference: [8] <author> S. Gerhart, D. Craigen, and T. Ralston. </author> <title> Experience with formal methods in critical systems. </title> <journal> IEEE Software, </journal> <volume> 11(1) </volume> <pages> 21-28, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation. (See Section 2.2.) Although several groups at AT&T and elsewhere are experimenting with formal notations <ref> [2, 8] </ref>, it is not the industry's standard practice. Secondly, the specifications are considerably smaller than industrial ones. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [9] <author> R. M. </author> <title> Heiberger. Computation for the Analysis of Designed Experiments. </title> <publisher> Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1989. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see [5], pp. 165ff and 210ff or <ref> [9] </ref>).
Reference: [10] <author> Kathryn L. Heninger. </author> <title> Specifying Software Requirements for Complex Systems: New Techniques and their Application. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(1):2-13, </volume> <month> January </month> <year> 1980. </year>
Reference-contexts: Each specification has four sections: Overview, Specific Functional Requirements, External Interfaces, and a Glossary. The overview is written in natural language, while the other three sections are specified using the SCR tabular requirements notation <ref> [10] </ref>. For this experiment, all three documents were adapted to adhere to the IEEE suggested format [12]. All faults present in these SRS appear in the original documents or were generated during the adaptation process; no faults were intentionally seeded into the document.
Reference: [11] <author> Watts S. Humphery. </author> <title> Managing the Software Process. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> 1989. Reading, Massachusetts. </address>
Reference-contexts: 1 Introduction One way of validating a software requirements specification (SRS) is to submit it to an inspection by a team of reviewers. Many organizations use a three-step inspection procedure for eliminating faults : detection, collection, and repair 1 . <ref> [11, 19] </ref> A team of reviewers reads the SRS, identifying as many faults as possible. Newly identified faults are collected, usually at a team meeting, and then sent to the document's authors for repair. <p> Initially we use students rather than professional because cost considerations severely limit our opportunities to conduct studies with professional 1 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see <ref> [11] </ref> pp. 171ff and [12]. 1 developers. Therefore we prefer to refine our experimental designs and measurement strategies in the university before using them in industry. This approach also allows us to do a kind of bulk screening of our research hypotheses. <p> Below we describe the relevant literature, several alternative fault detection methods which motivated our study, our research hypothesis, and our experimental observations, analysis and conclusions. 1.2 Inspection Literature A summary of the origins and the current practice of inspections may be found in Humphrey <ref> [11] </ref>. Consequently, we will discuss only work directly related to our current efforts. Fagan [7] defined the basic software inspection process. While most writers have endorsed his approach [4, 11], Parnas and Weiss are more critical [15]. <p> Consequently, we will discuss only work directly related to our current efforts. Fagan [7] defined the basic software inspection process. While most writers have endorsed his approach <ref> [4, 11] </ref>, Parnas and Weiss are more critical [15]. In part, they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack systematic techniques for meeting those responsibilities.
Reference: [12] <institution> IEEE Standard for software reviews and audits. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1989. </year> <note> IEEE Std 1028-1988. </note>
Reference-contexts: Initially we use students rather than professional because cost considerations severely limit our opportunities to conduct studies with professional 1 Depending on the exact form of the inspection, they are sometimes called reviews or walkthroughs. For a more thorough description of the taxonomy see [11] pp. 171ff and <ref> [12] </ref>. 1 developers. Therefore we prefer to refine our experimental designs and measurement strategies in the university before using them in industry. This approach also allows us to do a kind of bulk screening of our research hypotheses. <p> The overview is written in natural language, while the other three sections are specified using the SCR tabular requirements notation [10]. For this experiment, all three documents were adapted to adhere to the IEEE suggested format <ref> [12] </ref>. All faults present in these SRS appear in the original documents or were generated during the adaptation process; no faults were intentionally seeded into the document. The authors discovered 42 faults in the WLMS SRS; and 26 in the CRUISE SRS.
Reference: [13] <author> Charles M. Judd, Eliot R. Smith, and Louise H. Kidder. </author> <title> Research Methods in Social Relations. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <address> Fort Worth, TX, sixth edition, </address> <year> 1991. </year>
Reference-contexts: (Ad Hoc, Checklist, or Scenario); 2. the specification to be inspected (two are used during the experiment); 3. the inspection round (each reviewer participates in two inspections during the experiment); 4. the order in which the specifications are inspected (either specification can be inspected first). 2 See Judd, et al. <ref> [13] </ref>, chapter 4 for an excellent discussion of randomized social experimental designs. 4 The detection method is our treatment variable. The other variables allow us to assess several potential threats to the experiment's internal validity.
Reference: [14] <author> J. Kirby. </author> <title> Example NRL/SCR software requirements for an automobile cruise control and monitoring system. </title> <type> Technical Report TR-87-07, </type> <institution> Wang Institute of Graduate Studies, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Water Level Monitoring System (WLMS) [18] describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages). 8 Automobile Cruise Control System (CRUISE) <ref> [14] </ref> describes the functional and performance require-ments for an automobile cruise control system (31 pages). 2.2.2 Fault Detection Methods To make a fair assessment of the three detection methods (Ad Hoc, Checklist, and Scenario) each method should search for a well-defined population of faults.
Reference: [15] <author> Dave L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: When they are not coordinated, all reviewers have identical responsibilities. In contrast, the reviewers in coordinated teams may have separate and distinct responsibilities. In practice, reviewers often use Ad Hoc or Checklist detection techniques to discharge identical, general responsibilities. Some authors, notably Parnas and Weiss <ref> [15] </ref>, have argued that inspections would be more effective if each reviewer used a different set of systematic detection techniques to discharge different, specific responsibilities. 1.1 Preliminary Research In earlier work [16] we conducted an experiment to compare alternative detection methods for software inspections. <p> Consequently, we will discuss only work directly related to our current efforts. Fagan [7] defined the basic software inspection process. While most writers have endorsed his approach [4, 11], Parnas and Weiss are more critical <ref> [15] </ref>. In part, they argue that effectiveness suffers because individual reviewers are not assigned specific responsibilities and because they lack systematic techniques for meeting those responsibilities. Some might argue that Checklists are systematic because they help define each reviewer's responsibilities and suggest ways to identify faults. <p> Faults number 10 and 11, found by reviewer 12 of team C for the WLMS specification are shown. requirements notation, inspection procedures, the fault classification scheme, and the filling out of data collection forms. The references for these lectures were Fagan [7], Parnas <ref> [15] </ref>, and the IEEE Guide to Software Requirements Specifications [1]. The participants were then assembled into three-person teams see Section 2.1.3 for details.
Reference: [16] <author> Adam Porter, Lawrence G. Votta, and Victor Basili. </author> <title> Comparing detection methods for software requirement inspections: A replicated experim ent. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Some authors, notably Parnas and Weiss [15], have argued that inspections would be more effective if each reviewer used a different set of systematic detection techniques to discharge different, specific responsibilities. 1.1 Preliminary Research In earlier work <ref> [16] </ref> we conducted an experiment to compare alternative detection methods for software inspections. Our results suggest that the choice of fault detection method significantly affects inspection performance. Our subjects for that study were 48 graduate students in computer science.
Reference: [17] <author> G. Michael Schneider, Johnny Martin, and W. T. Tsai. </author> <title> An experimental study of fault detection in user requirements. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(2) </volume> <pages> 188-204, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: As a result, Scenario responsibilities are distinct subsets of Checklist and Ad Hoc responsibilities. The relationship between the three methods is depicted in Figure 2. The taxonomy is a composite of two schemes developed by Schneider, et al. <ref> [17] </ref> and Basili and Weiss [3]. Faults are divided into two broad types: omission in which important information is left unstated and commission - in which incorrect, redundant, or ambiguous information is put into the SRS by the author.
Reference: [18] <author> J. vanSchouwen. </author> <title> The A-7 requirements model: Re-examination for real-time systems and an application to monitoring systems. </title> <type> Technical Report TR-90-276, </type> <institution> Queen's University, Kingston, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. Elevator Control System (ELEVATOR) [20] describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages). Water Level Monitoring System (WLMS) <ref> [18] </ref> describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages). 8 Automobile Cruise Control System (CRUISE) [14] describes the functional and performance require-ments for an automobile cruise control system (31 pages). 2.2.2 Fault Detection Methods To make a fair assessment
Reference: [19] <author> Lawrence G. Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering. Association for Computing Machinery, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction One way of validating a software requirements specification (SRS) is to submit it to an inspection by a team of reviewers. Many organizations use a three-step inspection procedure for eliminating faults : detection, collection, and repair 1 . <ref> [11, 19] </ref> A team of reviewers reads the SRS, identifying as many faults as possible. Newly identified faults are collected, usually at a team meeting, and then sent to the document's authors for repair. <p> The rates are not significantly different between different populations or different specifications. It is interesting to note that these results are consistent with an earlier industrial case study by Votta <ref> [19] </ref>. 16 inspection, i.e., the number of faults first identified at a collection meeting divided by the total number of faults in the specification. Each rate is marked with symbol indicating the inspection method used.
Reference: [20] <author> William G. Wood. </author> <title> Temporal logic case study. </title> <type> Technical Report CMU/SEI-89-TR-24, </type> <institution> Software Engineering Institute, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1989. </year> <month> 21 </month>
Reference-contexts: The authors discovered 42 faults in the WLMS SRS; and 26 in the CRUISE SRS. The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. Elevator Control System (ELEVATOR) <ref> [20] </ref> describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages).
References-found: 20

