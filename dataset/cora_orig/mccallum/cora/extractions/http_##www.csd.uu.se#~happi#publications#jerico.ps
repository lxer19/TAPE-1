URL: http://www.csd.uu.se/~happi/publications/jerico.ps
Refering-URL: http://www.csd.uu.se/~happi/
Root-URL: 
Title: Uppsala Master's Theses in  Native Code Compilation for Erlang  
Author: Erik Johansson Christer Jonsson Examiner: H-akan Millroth 
Address: Box 311 S-751 05 Uppsala Sweden  
Affiliation: Computing Science Department Uppsala University  Passed:  
Date: October 2, 1996 ISSN 1100-1836  
Pubnum: Computing Science 100 Examensarbete DV3  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aho, V. A., Sethi, R. and Ullman, J. D., </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley 1986. </publisher>
Reference-contexts: Data-flow information is collected by setting up and solving systems of equations <ref> [1, ch. 10] </ref>. 3.2.1 Constant propagation If an instruction assigns a constant value to a register, this value is propagated through the code, and used instead of the register in all instructions between this instruction and the first instruction that writes to this register. t1 := 1 t1 := HTOP [0] <p> Graph coloring is NP-complete so a heuristic method is required. Our implementation of the algorithm consists of 4 phases: 1. Calculate liveness. We have to know which registers are alive at a specific point in the program. This is done by global dataflow analysis <ref> [1] </ref>. 2. The liveness information is used to build an interference graph where the nodes are registers and an edge between two nodes indicates that they are simultaneously live. 3. There are K available machine registers, this graph has to be colored with K colors.
Reference: [2] <author> A t-Kaci, H., </author> <title> Warren's Abstract Machine, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [3] <author> Armstrong, J. L., D acker, B. O., Virding, S. R. and Williams, M. C., </author> <title> Implementing a Functional Language for Highly Parallel Real Time Applications. SETSS 92, 30th March to 1st April 1992, </title> <address> Florence. </address>
Reference-contexts: Bear in mind that these results are for sequential code, and that for typical Erlang programs, utilizing processes and message passing, the overall performance increase might not be as large. Chapter 2 JAM We have built our compiler on top of JAM <ref> [3] </ref>, which is a stack based byte code emulator for Erlang. Erlang is a dynamically typed functional programming language, with built in support for processes and communication. No destructive updates of variables and data structures are allowed, and memory is managed automatically.
Reference: [4] <author> Armstrong, J. L., Virding, S. R., Wikstr om, C., and Williams, M. C., </author> <title> Concurrent Programming in Erlang. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1996. </year>
Reference-contexts: Introduction In this paper we will present a straightforward implementation of a native code compiler for Erlang <ref> [4] </ref>. Erlang is a functional programming language intended for use in telecommunication switches with high demands on availability. This intention has led to the demand that Erlang object code should be small, and that it should be replaceable at runtime (hot code loading).
Reference: [5] <author> Briggs, P., Cooper, K. D., and Torczon, L., </author> <title> Improvements to graph coloring register allocation. </title> <journal> ACM transactions on programming languages and systems 16, </journal> <month> 3 (May </month> <year> 1994), </year> <pages> pp. 428-455. </pages>
Reference-contexts: We do not do a state-of-the-art branch instruction scheduling, but search the basic block preceding the branch for a suitable instruction [13]. 3.4 Register Allocation The registers in our intermediate code must be assigned to machine registers. We use pessimistic graph coloring register allocation <ref> [5, 7, 8] </ref>. This is an abstraction of the register allocation problem to a graph coloring problem. Graph coloring is NP-complete so a heuristic method is required. Our implementation of the algorithm consists of 4 phases: 1. Calculate liveness.
Reference: [6] <author> Calder, B., Gr unwald, D., </author> <title> Reducing Branch Costs via Branch Alignment. </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <month> April </month> <year> 1991. </year> <pages> pp. 242-251. </pages>
Reference-contexts: Tuning branch instructions. A similar optimization on a lower level is to profile each branch instruction. Then the code could be rearranged 21 Chapter 6: Conclusion & Future Work 22 in order to make branch prediction easier for the processor <ref> [6] </ref>. Storing often executed basic blocks together. The profiling information generated (at runtime) by the above mentioned techniques could also be used to find the most frequently executed basic blocks. These could then be stored together to improve instruction cache behav ior [17].
Reference: [7] <author> Chaitin, G. J., Auslander, M. A., Chandra, A. K., Cocke, J., Hopkins, M.E., and Markstein, P. W., </author> <title> Register allocation via coloring. </title> <booktitle> Computer Languages 6, </booktitle> <month> (January </month> <year> 1981), </year> <pages> pp. 47-57. </pages>
Reference-contexts: The process of deciding which register to keep a value in is called register allocation. It is desirable that as many values as possible are held in registers for their whole lifetime. Graph coloring register allocation as described by Chaitin et al. <ref> [7, 8] </ref> is used to achieve this. Assembly. <p> We do not do a state-of-the-art branch instruction scheduling, but search the basic block preceding the branch for a suitable instruction [13]. 3.4 Register Allocation The registers in our intermediate code must be assigned to machine registers. We use pessimistic graph coloring register allocation <ref> [5, 7, 8] </ref>. This is an abstraction of the register allocation problem to a graph coloring problem. Graph coloring is NP-complete so a heuristic method is required. Our implementation of the algorithm consists of 4 phases: 1. Calculate liveness.
Reference: [8] <author> Chaitin, G. J., </author> <title> Register allocation and spilling via graph coloring. </title> <journal> SIGPLAN Notices 17, </journal> <month> 6 (June </month> <year> 1982), </year> <pages> pp. 98-105. </pages> <booktitle> Proceeding of the ACM SIGPLAN'82 Symposium on Compiler Construction. </booktitle>
Reference-contexts: The process of deciding which register to keep a value in is called register allocation. It is desirable that as many values as possible are held in registers for their whole lifetime. Graph coloring register allocation as described by Chaitin et al. <ref> [7, 8] </ref> is used to achieve this. Assembly. <p> We do not do a state-of-the-art branch instruction scheduling, but search the basic block preceding the branch for a suitable instruction [13]. 3.4 Register Allocation The registers in our intermediate code must be assigned to machine registers. We use pessimistic graph coloring register allocation <ref> [5, 7, 8] </ref>. This is an abstraction of the register allocation problem to a graph coloring problem. Graph coloring is NP-complete so a heuristic method is required. Our implementation of the algorithm consists of 4 phases: 1. Calculate liveness.
Reference: [9] <author> Chambers, C., </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <month> March </month> <year> 1992. </year> <note> Tech Report STAN-CS-92-1420. </note>
Reference-contexts: Inspiration to our work comes from many sources: * Ertl [10] has inspired us with his work on compiling stack machines. * Haygood [11] has described native compilation for SICStus Prolog. * Runtime compilation has been examined by several researchers, for example Chambers <ref> [9] </ref>, and Leone and Lee [16, 15]. Results On the average, our compiler is 55% faster than Erlang compiled via C, and more than four times faster than emulated Erlang. Our generated code is about 60% smaller than Erlang compiled via C.
Reference: [10] <author> Ertl, M. A., </author> <title> A New Approach to Forth Native Code Generation. </title> <booktitle> EuroForth '92, </booktitle> <pages> pp. 73-78. 23 </pages>
Reference-contexts: Related Work Compilation of Erlang to C has been implemented by Hausman in his BEAM compiler [12], and we have compared our results with this compiler. Inspiration to our work comes from many sources: * Ertl <ref> [10] </ref> has inspired us with his work on compiling stack machines. * Haygood [11] has described native compilation for SICStus Prolog. * Runtime compilation has been examined by several researchers, for example Chambers [9], and Leone and Lee [16, 15]. <p> The JAM code is easier to compile than ordinary Erlang code. The compilation consists of these stages: Translation. In this stage, the JAM code is translated into intermediate-code. We use techniques worked out by Ertl <ref> [10] </ref> to translate stack based JAM code to register code. Optimization. After the translation the compiler make some global (in the sense of reaching over the whole function) transformations, performing some standard optimizations, such as constant propagation and dead code elimination. Machine specific transformations. <p> There are several possible solutions to this problem. One approach would be to keep the stack as it is and by means of analysis, keep the most used stack positions in registers. We have chosen another approach. Inspired by Ertl's <ref> [10] </ref> work on Forth, we wanted to transform all stack references to register references in the translation stage. A virtual stack is maintained during translation, corresponding to what the real stack would look like at runtime.
Reference: [11] <author> Haygood, R. C., </author> <title> Native Code Compilation in SICStus Prolog. </title> <booktitle> In--ternational Conference on Logic Programming 1994, </booktitle> <pages> pp. 191-204. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Related Work Compilation of Erlang to C has been implemented by Hausman in his BEAM compiler [12], and we have compared our results with this compiler. Inspiration to our work comes from many sources: * Ertl [10] has inspired us with his work on compiling stack machines. * Haygood <ref> [11] </ref> has described native compilation for SICStus Prolog. * Runtime compilation has been examined by several researchers, for example Chambers [9], and Leone and Lee [16, 15]. Results On the average, our compiler is 55% faster than Erlang compiled via C, and more than four times faster than emulated Erlang.
Reference: [12] <author> Hausman, B., </author> <title> Turbo Erlang: Approaching the Speed of C. </title> <booktitle> Implementations of Logic Programming Systems, </booktitle> <pages> pp. 119-135, </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Therefore we have not made any changes to the rest of the runtime system. To clearly see the effect of native compilation, we have concentrated our work on sequential Erlang code. Related Work Compilation of Erlang to C has been implemented by Hausman in his BEAM compiler <ref> [12] </ref>, and we have compared our results with this compiler. <p> We will discuss this further in chapter 6. Chapter 5 Performance Evaluation We have compared our implementation with emulated JAM 4.3.1, BEAM/T 4.3 and BEAM/C 4.3. BEAM <ref> [12] </ref> is a register machine originally designed to be compiled to C. The current implementation supports a direct threaded emulator (BEAM/T) and compilation via C (BEAM/C). We have used nine sequential benchmarks to evaluate our compiler.
Reference: [13] <author> Hennessy, J., L., Patterson, D., A., </author> <title> Computer Architecture, a Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: There are considerable performance gains to make if these delay slots can be filled with useful instructions. We do not do a state-of-the-art branch instruction scheduling, but search the basic block preceding the branch for a suitable instruction <ref> [13] </ref>. 3.4 Register Allocation The registers in our intermediate code must be assigned to machine registers. We use pessimistic graph coloring register allocation [5, 7, 8]. This is an abstraction of the register allocation problem to a graph coloring problem. Graph coloring is NP-complete so a heuristic method is required.
Reference: [14] <author> H olzle, U., </author> <title> Adaptive Optimization for Self: Reconciling High Performance with Exploratory Programming. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Stanford University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: We call this process adaptive compilation <ref> [14] </ref>. Adaptive compilation is achieved by counting the number of calls to each function. When the counter reaches a limit (C ), the function is compiled. Over time, those functions that are called most will be compiled. <p> Chapter 4: System Integration 16 By varying the C and T parameters the same compiler can work in different environments (space critical or time critical). We have not investigated this further, but much work in this direction has been done on SELF by Holzle <ref> [14] </ref>. There are a number of analyses that a runtime compiler can do, that are hard for a static compiler, but we have not explored this field yet. We will discuss this further in chapter 6. <p> Examples of possible optimizations are: Specializing functions by their actual arguments. By looking at the actual values that functions are called with, these functions could be specialized with respect to the types or maybe even the values them selves <ref> [14, 16, 15] </ref>. Ordering function clauses after use. The function clause that is taken most often should be tried first. Most Erlang programmers knows this and places, for example, the test for the empty list last. But it is not always trivial to see beforehand which clause to place first.
Reference: [15] <author> Lee, P. and Leone, M., </author> <title> Optimizing ML with Run-Time Code Generation. </title> <booktitle> ACM SIGPLAN '96 Conference on Programming Langauge Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: Inspiration to our work comes from many sources: * Ertl [10] has inspired us with his work on compiling stack machines. * Haygood [11] has described native compilation for SICStus Prolog. * Runtime compilation has been examined by several researchers, for example Chambers [9], and Leone and Lee <ref> [16, 15] </ref>. Results On the average, our compiler is 55% faster than Erlang compiled via C, and more than four times faster than emulated Erlang. Our generated code is about 60% smaller than Erlang compiled via C. <p> Examples of possible optimizations are: Specializing functions by their actual arguments. By looking at the actual values that functions are called with, these functions could be specialized with respect to the types or maybe even the values them selves <ref> [14, 16, 15] </ref>. Ordering function clauses after use. The function clause that is taken most often should be tried first. Most Erlang programmers knows this and places, for example, the test for the empty list last. But it is not always trivial to see beforehand which clause to place first.
Reference: [16] <author> Leone, M. and Lee, P., </author> <title> Lightweight Run-Time Code Generation. </title> <booktitle> Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pp. 97-106, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Inspiration to our work comes from many sources: * Ertl [10] has inspired us with his work on compiling stack machines. * Haygood [11] has described native compilation for SICStus Prolog. * Runtime compilation has been examined by several researchers, for example Chambers [9], and Leone and Lee <ref> [16, 15] </ref>. Results On the average, our compiler is 55% faster than Erlang compiled via C, and more than four times faster than emulated Erlang. Our generated code is about 60% smaller than Erlang compiled via C. <p> Examples of possible optimizations are: Specializing functions by their actual arguments. By looking at the actual values that functions are called with, these functions could be specialized with respect to the types or maybe even the values them selves <ref> [14, 16, 15] </ref>. Ordering function clauses after use. The function clause that is taken most often should be tried first. Most Erlang programmers knows this and places, for example, the test for the empty list last. But it is not always trivial to see beforehand which clause to place first.
Reference: [17] <author> McFarling, S., </author> <title> Procedure Merging with Instruction Caches. </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Langauge Design and Implementation, </booktitle> <year> 1991. </year> <pages> pp. 71-79. </pages>
Reference-contexts: Storing often executed basic blocks together. The profiling information generated (at runtime) by the above mentioned techniques could also be used to find the most frequently executed basic blocks. These could then be stored together to improve instruction cache behav ior <ref> [17] </ref>. We plan to examine these possibilities, and at the same time try to find other possible techniques.
Reference: [18] <author> Smith, T. F., and Waterman, M. S., </author> <title> Identification of common molecular subsequences. </title> <journal> Journal of Molecular Biology 147, </journal> <year> 1981, </year> <pages> pp. 195-197. </pages>
Reference-contexts: Each benchmark runs for approximately 1-2 minutes and was run 3 times giving a total running time of well over 30 minutes. The benchmarks used were: Huffman. A huffman encoder. Compresses and uncompresses a text file of 32 kilobytes. 138 lines. Smith-Waterman. The Smith-Waterman DNA sequence matching algorithm <ref> [18] </ref>. Matches a sequence of length 32 to 600 other sequences of length 32. 68 lines. Barnes-Hut. Simulates gravitational forces between 1000 bodies. 156 lines. Raytracer. A ray tracer. Traces a picture with spheres, planes and texture mapping. Approximately 1000 lines. Quicksort. Ordinary quicksort.
Reference: [19] <author> The SPARC Architecture Manual, </author> <title> Version 8, </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year> <month> 24 </month>
References-found: 19

