URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-302.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fpicard, tpminkag@media.mit.edu  
Title: Texture for Annotation  
Author: R. W. Picard and T. P. Minka 
Address: 20 Ames St; Cambridge, MA 02139  
Affiliation: Vision and Modeling Group MIT Media Laboratory  
Note: Vision  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 302 Multimedia Systems: Special Issue on Content-based Retrieval Abstract This paper demonstrates a new application of computer vision to digital libraries the use of texture for annotation, the description of content. Vision-based annotation assists the user in attaching descriptions to large sets of images and video. If a user labels a piece of an image as "water," a texture model can be used to propagate this label to other "visually similar" regions. However, a serious problem is that no single model has been found to be good enough to reliably match human perception of similarity in pictures. Rather than using one model, the system described here knows several texture models, and is equipped with the ability to choose the one which "best explains" the regions selected by the user for annotating. If none of these models suffices, then it creates new explanations by combining models. Examples are given of annotations propagated by the system on natural scenes. The system provides an average gain of four to one in label prediction over a set of 98 images.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, P. Yanker, C. Faloutsos, 8 and G. Taubin, </author> <title> "The QBIC project: Querying im-ages by content using color, texture, and shape," in Proc. SPIE Storage and Retrieval for Image and Video Databases (W. Niblack, </title> <editor> ed.), </editor> <address> (San Jose, CA), </address> <pages> pp. 173-181, </pages> <publisher> SPIE-The Society for Optical Engineers, </publisher> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Solving requests like these is the difficult quest of new research in pattern recognition, computer vision and acoustic scene analysis. Progress is rapidly being made in the ability to search for similar scenes based on shape, color, and texture. Examples are the QBIC system of IBM <ref> [1] </ref>, SWIM system of ISS [2], and Photobook system of MIT [3] [4]. Even "purely visual" databases (e.g. a collection of paintings) will still tend to have text associated with them (title, artist, subject, etc.). <p> The problem of finding visually similar regions to which to propagate the label is very close to the problem that arises in retrieval of similar pictures <ref> [1] </ref> [14]. In both cases, features need to be identified which are important for measuring similarity. Determining which model gives the best features for measuring similarity is complicated by the fact that users are fickle. People are nonlinear time-varying systems when it comes to predicting their behavior.
Reference: [2] <author> S. W. Smoliar and H. Zhang, </author> <title> "Content-based video indexing and retrieval," </title> <booktitle> IEEE Multimedia, </booktitle> <pages> pp. 62-72, </pages> <month> Summer </month> <year> 1994. </year>
Reference-contexts: Progress is rapidly being made in the ability to search for similar scenes based on shape, color, and texture. Examples are the QBIC system of IBM [1], SWIM system of ISS <ref> [2] </ref>, and Photobook system of MIT [3] [4]. Even "purely visual" databases (e.g. a collection of paintings) will still tend to have text associated with them (title, artist, subject, etc.).
Reference: [3] <author> R. W. Picard and T. Kabir, </author> <title> "Finding similar patterns in large image databases," </title> <booktitle> in Proc. ICASSP, (Min-neapolis, MN), </booktitle> <pages> pp. </pages> <address> V-161-V-164, </address> <year> 1993. </year>
Reference-contexts: Progress is rapidly being made in the ability to search for similar scenes based on shape, color, and texture. Examples are the QBIC system of IBM [1], SWIM system of ISS [2], and Photobook system of MIT <ref> [3] </ref> [4]. Even "purely visual" databases (e.g. a collection of paintings) will still tend to have text associated with them (title, artist, subject, etc.). Annotation is both important for preparing multimedia digital libraries for query and retrieval, and useful for adding personal notes to the user's online collection. <p> This data will be helpful for learning how to deal effectively with context, as well as for other applications. Future improvements to the present system are planned for incorporating context. 3 New Photobook with annotation We began with the MIT Photobook image retrieval system <ref> [3] </ref> [4], and have augmented it to perform interactive annotation. The evolved "interactive annotation extension to Photobook" is still called "Photobook" for short.
Reference: [4] <author> A. Pentland, R. W. Picard, and S. Sclaroff, "Photo-book: </author> <title> Tools for content-base manipulation of image databases," </title> <booktitle> in SPIE Storage and Retrieval of Image & Video Databases II, </booktitle> <address> (San Jose, CA), </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Progress is rapidly being made in the ability to search for similar scenes based on shape, color, and texture. Examples are the QBIC system of IBM [1], SWIM system of ISS [2], and Photobook system of MIT [3] <ref> [4] </ref>. Even "purely visual" databases (e.g. a collection of paintings) will still tend to have text associated with them (title, artist, subject, etc.). Annotation is both important for preparing multimedia digital libraries for query and retrieval, and useful for adding personal notes to the user's online collection. <p> This data will be helpful for learning how to deal effectively with context, as well as for other applications. Future improvements to the present system are planned for incorporating context. 3 New Photobook with annotation We began with the MIT Photobook image retrieval system [3] <ref> [4] </ref>, and have augmented it to perform interactive annotation. The evolved "interactive annotation extension to Photobook" is still called "Photobook" for short.
Reference: [5] <author> H. Tamura, S. Mori, and T. Yamawaki, </author> <title> "Textural features corresponding to visual perception," </title> <journal> IEEE T. Sys., Man and Cyber., </journal> <volume> vol. SMC-8, no. 6, </volume> <pages> pp. 460-473, </pages> <year> 1978. </year>
Reference-contexts: This research focuses on the use of collective visual properties, or "vision texture" for annotation. Texture models extract "stuff" features such as directionality, periodicity, randomness, roughness, regularity, coarseness, color distribution, contrast, and complexity, which are hypothesized to be important for human perception and attention <ref> [5] </ref> [6] [7]. Low-level color texture features [8] have also been shown to be useful for "selection" of regions of interest preceding actual recognition. Low-level texture features may also play a significant role in high-level tasks such as recognition.
Reference: [6] <author> A. Treisman and G. Gelade, </author> <title> "A feature-integration theory of attention," </title> <journal> Cognitive Psychology, </journal> <volume> vol. 12, </volume> <pages> pp. 97-136, </pages> <year> 1980. </year>
Reference-contexts: This research focuses on the use of collective visual properties, or "vision texture" for annotation. Texture models extract "stuff" features such as directionality, periodicity, randomness, roughness, regularity, coarseness, color distribution, contrast, and complexity, which are hypothesized to be important for human perception and attention [5] <ref> [6] </ref> [7]. Low-level color texture features [8] have also been shown to be useful for "selection" of regions of interest preceding actual recognition. Low-level texture features may also play a significant role in high-level tasks such as recognition.
Reference: [7] <author> A. R. Rao and G. L. Lohse, </author> <title> "Towards a texture naming system: Identifying relevant dimensions of texture," </title> <booktitle> in IEEE Conf. on Visualization, </booktitle> <address> (San Jose), </address> <year> 1993. </year>
Reference-contexts: This research focuses on the use of collective visual properties, or "vision texture" for annotation. Texture models extract "stuff" features such as directionality, periodicity, randomness, roughness, regularity, coarseness, color distribution, contrast, and complexity, which are hypothesized to be important for human perception and attention [5] [6] <ref> [7] </ref>. Low-level color texture features [8] have also been shown to be useful for "selection" of regions of interest preceding actual recognition. Low-level texture features may also play a significant role in high-level tasks such as recognition.
Reference: [8] <author> T. Syeda-Mahmood, </author> <title> "Model-driven selection using texture," </title> <booktitle> in Proc. 4th British Machine Vision Conference (J. </booktitle> <institution> Illingworth, ed.), (Univ. of Surrey, </institution> <type> Guildford), </type> <pages> pp. 65-74, </pages> <publisher> BMVA Press, </publisher> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Texture models extract "stuff" features such as directionality, periodicity, randomness, roughness, regularity, coarseness, color distribution, contrast, and complexity, which are hypothesized to be important for human perception and attention [5] [6] [7]. Low-level color texture features <ref> [8] </ref> have also been shown to be useful for "selection" of regions of interest preceding actual recognition. Low-level texture features may also play a significant role in high-level tasks such as recognition.
Reference: [9] <author> R. J. Herrnstein, D. H. Loveland, and C. </author> <title> Cable, "Natural concepts in pigeons," </title> <journal> J. of Exp. Psych: Anim. Beh. Procs., </journal> <volume> vol. 2, </volume> <pages> pp. 285-302, </pages> <year> 1976. </year>
Reference-contexts: Low-level color texture features [8] have also been shown to be useful for "selection" of regions of interest preceding actual recognition. Low-level texture features may also play a significant role in high-level tasks such as recognition. Studies with pigeons <ref> [9] </ref> indicate they can recognize categories such as "water" and "tree;" such studies support the hypothesis for a simple 3 mechanism, that perhaps looks at collective low-level features for making comparatively high-level quick classifications.
Reference: [10] <author> M. M. Gorkani and R. W. </author> <title> Picard, "Texture orientation for sorting photos at a glance," </title> <booktitle> in Proc. Int. Conf. Pat. Rec., </booktitle> <address> (Jerusalem, Israel), </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Studies with pigeons [9] indicate they can recognize categories such as "water" and "tree;" such studies support the hypothesis for a simple 3 mechanism, that perhaps looks at collective low-level features for making comparatively high-level quick classifications. A recent study <ref> [10] </ref> demonstrated that features based on texture orientation closely matched human high-level classifications on 91 out of 98 digitized vacation photos (two of which are shown in Fig. 1). <p> does the system dynamically choose models and respond to user feedback? 7 3.2.1 Pre-processing: configuring model trees In the examples shown in this paper, the test data consists of ten natural 512 fi 512 color scenes taken from a set of 98 digitized vacation photos (the same set used in <ref> [10] </ref>). Each image in the database is split into square subimages or "patches" (64 per image for the current system; e.g. see Fig. 6).
Reference: [11] <author> M. J. Swain and D. H. Ballard, </author> <title> "Indexing via color histograms," </title> <booktitle> in Image Understanding Workshop, </booktitle> <address> (Pitts-burgh, PA), </address> <pages> pp. 623-630, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Simple color histograms have also been shown to be useful in the recognition task of matching known models to unknown image data <ref> [11] </ref>. The belief that low-level vision is limited to only low-level tasks is too restrictive. 2.1 Annotation by fickle users Scene annotation is an ill-posed problem in general. <p> If the "car stuff" is sufficiently unique compared to the other content in the database, this low-level approach meets with success. The idea that simple color histograms could provide useful invariants for object recognition was demonstrated in <ref> [11] </ref>. Thus, vision texture is sometimes able to find objects like cars, even though cars are not usually considered texture. The true performance of a labeling system is ultimately judged by the user.
Reference: [12] <author> G. Miller, </author> <title> "Wordnet: An on-line lexical database," </title> <journal> Int. Journal of Lexicography, </journal> <volume> vol. 3, no. 4, </volume> <year> 1990. </year>
Reference-contexts: In fact, measuring similarity is significantly more complicated than these examples reveal, encompassing arguments that feature spaces for measuring similarity may be non-metric [16] and non-symmetric [17]. We briefly identify the following influences: Visual features Regions may look similar at a quick 4 WordNet <ref> [12] </ref> for example, contains synonyms and semantic relations of English words and concepts.
Reference: [13] <author> A. S. Chakravarthy, </author> <title> "Toward semantic retrieval of pictures and video," </title> <booktitle> in RIAO'94, Intelligent Multimedia Information Retrieval Systems and Management, </booktitle> <address> (New York), </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We briefly identify the following influences: Visual features Regions may look similar at a quick 4 WordNet [12] for example, contains synonyms and semantic relations of English words and concepts. It can be used to retrieve "semantically close" words, e.g. "operating room" retrieves "hospital" with the semantic relation "part-of" <ref> [13] </ref>. 5 After interacting with many users, however, the system might begin to recognize some "universal" preferences and adopt those for automatic annotation. 2 brick straw matting oriental rattan brick It can also be influenced by culture, context, or user preference.
Reference: [14] <author> R. W. Picard and F. Liu, </author> <title> "A new Wold ordering for image similarity," </title> <booktitle> in Proc. IEEE Conf. on Acoustics, Speech, and Signal Proc., </booktitle> <address> (Adelaide, Australia), </address> <pages> pp. </pages> <address> V-129-V-132, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The problem of finding visually similar regions to which to propagate the label is very close to the problem that arises in retrieval of similar pictures [1] <ref> [14] </ref>. In both cases, features need to be identified which are important for measuring similarity. Determining which model gives the best features for measuring similarity is complicated by the fact that users are fickle. People are nonlinear time-varying systems when it comes to predicting their behavior.
Reference: [15] <author> P. Brodatz, </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <address> New York: </address> <publisher> Dover, </publisher> <year> 1966. </year>
Reference-contexts: People, too, will arrive at different decisions, depending on which set of features best meets their goals. The four images shown here are taken from <ref> [15] </ref>. glance; you could swap one for the other and proba bly not notice, e.g. dense leafy treetops and grass. Viewpoint Images may be of the same scene, but differ in camera viewpoint or lighting. <p> The fifteen features are computed on the NTSC gray component. A Mahalanobis distance was used for discrimination. This model has been found to work better than eigenvectors for retrieval of images in the Brodatz <ref> [15] </ref> texture database [25]. The TSW features are also based only on the NTSC gray component.
Reference: [16] <author> W. Richards and J. K. Koenderink, </author> <title> "Trajectory mapping ("TM"): A new non-metric scaling technique," </title> <institution> Center for Cognitive Science 48, MIT, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Others would say the two brick images are most similar because they are both brick. In fact, measuring similarity is significantly more complicated than these examples reveal, encompassing arguments that feature spaces for measuring similarity may be non-metric <ref> [16] </ref> and non-symmetric [17]. We briefly identify the following influences: Visual features Regions may look similar at a quick 4 WordNet [12] for example, contains synonyms and semantic relations of English words and concepts.
Reference: [17] <author> A. Tversky, </author> <title> "Features of similarity," </title> <journal> Psychological Review, </journal> <volume> vol. 84, </volume> <pages> pp. 327-352, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Others would say the two brick images are most similar because they are both brick. In fact, measuring similarity is significantly more complicated than these examples reveal, encompassing arguments that feature spaces for measuring similarity may be non-metric [16] and non-symmetric <ref> [17] </ref>. We briefly identify the following influences: Visual features Regions may look similar at a quick 4 WordNet [12] for example, contains synonyms and semantic relations of English words and concepts.
Reference: [18] <author> M. Minsky, </author> <booktitle> The Society of Mind, </booktitle> <address> p. 45. New York: </address> <publisher> Simon & Schuster, </publisher> <year> 1985. </year>
Reference-contexts: We also assume that models will tend to specialize, and that they can work alone or together to model regions in the images. This second assumption is in the same spirit as the "Society of Mind" <ref> [18] </ref>, whereby specialized agents, or models in this case, interact to make sense of what they see. In this case we have a "society of models" which interact to explain "stuff" in pictures.
Reference: [19] <author> T. S. C. Tan and J. Kittler, </author> <title> "Colour texture classification using features from colour histogram," </title> <booktitle> in SCIA Conference on Image Analysis, </booktitle> <volume> vol. 2, </volume> <year> 1993. </year>
Reference-contexts: To evaluate which texture model does "best" for this unknown 3 Table 1: Models known by the system Model Description Ref. HIST-D Color histogram differences HIST-EE Color histogram energy and entropy <ref> [19] </ref> HIST-I Color histogram invariant features [20] EV Eigenvectors of RGB covariance [21] MSAR Multiscale simultaneous auto-regressive [22] TSW Tree-structured wavelet transform [23] data set is not doable in a meaningful way; it is arguable what measure would provide a universal measure of "best," given the four influences discussed above, and <p> We know of no studies which evaluate this model. The HIST-EE model consists of the energy and entropy features of the three Ohta color channels. These have been shown <ref> [19] </ref> to improve discrimination (in comparison to filter energy features used alone) among an extended set of images formed from twelve color granite images. The HIST-I model computes illumination invariant features of RGB histograms.
Reference: [20] <author> G. Healey and D. Slater, </author> <title> "Using illumination invariant color histogram descriptors for recognition," </title> <booktitle> in Proceedings of CVPR, </booktitle> <address> (Seattle, WA), </address> <pages> pp. 355-360, </pages> <publisher> IEEE, Braun-Brumfield, Inc., </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: To evaluate which texture model does "best" for this unknown 3 Table 1: Models known by the system Model Description Ref. HIST-D Color histogram differences HIST-EE Color histogram energy and entropy [19] HIST-I Color histogram invariant features <ref> [20] </ref> EV Eigenvectors of RGB covariance [21] MSAR Multiscale simultaneous auto-regressive [22] TSW Tree-structured wavelet transform [23] data set is not doable in a meaningful way; it is arguable what measure would provide a universal measure of "best," given the four influences discussed above, and their own data dependency. <p> The HIST-I model computes illumination invariant features of RGB histograms. These features were shown to outperform two comparable sets of color features on recognition of twenty-seven images, consisting of nine different scenes, each shot under three different conditions of illumination <ref> [20] </ref>. For the EV model, three separate covariances are computed on the R, G, and B color components, and the values corresponding to the twenty-one largest eigenvectors are kept as features for each component. Euclidean distance is used for comparison.
Reference: [21] <author> C. W. Therrien, </author> <title> Decision Estimation and Classification. </title> <address> New York: </address> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1989. </year>
Reference-contexts: To evaluate which texture model does "best" for this unknown 3 Table 1: Models known by the system Model Description Ref. HIST-D Color histogram differences HIST-EE Color histogram energy and entropy [19] HIST-I Color histogram invariant features [20] EV Eigenvectors of RGB covariance <ref> [21] </ref> MSAR Multiscale simultaneous auto-regressive [22] TSW Tree-structured wavelet transform [23] data set is not doable in a meaningful way; it is arguable what measure would provide a universal measure of "best," given the four influences discussed above, and their own data dependency. <p> Euclidean distance is used for comparison. We have not seen any published studies of the use of this model for discrimination or recognition of color images, although eigenvectors are commonly used for a variety of pattern recognition problems <ref> [21] </ref>. The MSAR model computes parameters for a second-order simultaneous autoregressive model over three scales, for a total of fifteen features [22]. The fifteen features are computed on the NTSC gray component. A Mahalanobis distance was used for discrimination.
Reference: [22] <author> J. Mao and A. K. Jain, </author> <title> "Texture classification and segmentation using multiresolution simultaneous autore-gressive models," </title> <booktitle> Patt. Rec., </booktitle> <volume> vol. 25, no. 2, </volume> <pages> pp. 173-188, </pages> <year> 1992. </year>
Reference-contexts: To evaluate which texture model does "best" for this unknown 3 Table 1: Models known by the system Model Description Ref. HIST-D Color histogram differences HIST-EE Color histogram energy and entropy [19] HIST-I Color histogram invariant features [20] EV Eigenvectors of RGB covariance [21] MSAR Multiscale simultaneous auto-regressive <ref> [22] </ref> TSW Tree-structured wavelet transform [23] data set is not doable in a meaningful way; it is arguable what measure would provide a universal measure of "best," given the four influences discussed above, and their own data dependency. <p> The MSAR model computes parameters for a second-order simultaneous autoregressive model over three scales, for a total of fifteen features <ref> [22] </ref>. The fifteen features are computed on the NTSC gray component. A Mahalanobis distance was used for discrimination. This model has been found to work better than eigenvectors for retrieval of images in the Brodatz [15] texture database [25].
Reference: [23] <author> T. Chang and C.-C. J. Kuo, </author> <title> "Texture analysis and classification with tree-structured wavelet transform," </title> <editor> IEEE T. </editor> <booktitle> Image Proc., </booktitle> <volume> vol. 2, </volume> <pages> pp. 429-441, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: HIST-D Color histogram differences HIST-EE Color histogram energy and entropy [19] HIST-I Color histogram invariant features [20] EV Eigenvectors of RGB covariance [21] MSAR Multiscale simultaneous auto-regressive [22] TSW Tree-structured wavelet transform <ref> [23] </ref> data set is not doable in a meaningful way; it is arguable what measure would provide a universal measure of "best," given the four influences discussed above, and their own data dependency. <p> A Mahalanobis distance was used for discrimination. This model has been found to work better than eigenvectors for retrieval of images in the Brodatz [15] texture database [25]. The TSW features are also based only on the NTSC gray component. Unlike in <ref> [23] </ref> where their trees were averaged across several samples from the same Brodatz image, here only one tree of features is generated per selected image region because the regions are not big enough to support several samples. The wavelet trees were expanded down four levels, as in that paper. <p> The wavelet trees were expanded down four levels, as in that paper. This method has been shown to outperform similar transform methods on a test set of 30 Brodatz textures <ref> [23] </ref>. 2.4 Annotation to help learn context A patch from the side of a building and a patch from a street can have identical pixel values. Perceptually indistinguishable texture patches can be extracted from a grass lawn and from mandril fur.
Reference: [24] <author> Y.-I. Ohta, T. Kanade, and T. Sakai, </author> <title> "Color information for region segmentation," Comp. Graph. </title> <journal> and Img. Proc., </journal> <volume> vol. 13, </volume> <pages> pp. 222-241, </pages> <year> 1980. </year>
Reference-contexts: The annotation system helps "discover" these relations, but a lot more data is needed before general predictions can be inferred empirically. A few comments are given below on each of the six models used presently. The HIST-D model works as follows: Concatenate the histograms of the Ohta <ref> [24] </ref> color components I 1 , I 2 , I 3 into a feature vector, then compare two feature vectors with the Euclidean distance. We know of no studies which evaluate this model. The HIST-EE model consists of the energy and entropy features of the three Ohta color channels.
Reference: [25] <author> R. W. Picard, T. Kabir, and F. Liu, </author> <title> "Real-time recognition with the entire brodatz texture database," </title> <booktitle> in Proc. IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <address> (New York), </address> <pages> pp. 638-639, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The fifteen features are computed on the NTSC gray component. A Mahalanobis distance was used for discrimination. This model has been found to work better than eigenvectors for retrieval of images in the Brodatz [15] texture database <ref> [25] </ref>. The TSW features are also based only on the NTSC gray component.
Reference: [26] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Note that the number of features and the dynamic range of the feature values are both dependent on m. 2. Cluster the patches based on their feature vectors. Here we used the common-neighbor clustering of Jarvis and Patrick described in <ref> [26] </ref> since it is independent of the definition of feature distance. Two images are put in the same cluster if they have k t = 4 of their k nearest neighbors in common and are k-nearest neighbors of each other.
Reference: [27] <author> A. S. Sherstinsky, M-Lattice: </author> <title> A System for Signal Synthesis and Processing Based on Reaction-Diffusion. </title> <type> ScD thesis, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: Texture is useful not only for recognizing such sounds, but also for describing concepts such as timbre and rhythm. Many of the same models used for vision texture appear to work well for sound texture <ref> [27] </ref> [28]. To adapt the method presented here, one may substitute sound features for image features. One can also easily substitute new models for the six here.
Reference: [28] <author> N. Saint-Arnaud, </author> <month> Spring </month> <year> 1994. </year> <title> Class project for MIT Pattern Recognition and Analysis. </title> <note> 9 10 11 labels in this image 12 in sixteen satisfactory labels 13 </note>
Reference-contexts: Texture is useful not only for recognizing such sounds, but also for describing concepts such as timbre and rhythm. Many of the same models used for vision texture appear to work well for sound texture [27] <ref> [28] </ref>. To adapt the method presented here, one may substitute sound features for image features. One can also easily substitute new models for the six here.
References-found: 28

