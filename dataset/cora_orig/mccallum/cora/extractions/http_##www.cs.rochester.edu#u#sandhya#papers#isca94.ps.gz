URL: http://www.cs.rochester.edu/u/sandhya/papers/isca94.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Title: Software Versus Hardware Shared-Memory Implementation: A Case Study cation overhead improve the performance of the
Author: Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel 
Note: This research was supported in part by the National Science Foundation under Grants CCR-9116343, CCR-9211004, CDA-9222911, and CDA-9310073, and by the Texas Advanced Technology Program under Grants 003604014 and 003604012.  
Affiliation: Department of Computer Science Rice University  
Abstract: We compare the performance of software-supported shared memory on a general-purpose network to hardware-supported shared memory on a dedicated interconnect. Up to eight processors, our results are based on the execution of a set of application programs on a SGI 4D/480 multiprocessor and on TreadMarks, a distributed shared memory system that runs on a Fore ATM LAN of DECstation-5000/240s. Since the DEC-station and the 4D/480 use the same processor, primary cache, and compiler, the shared-memory implementation is the principal difference between the systems. Our results show that TreadMarks performs comparably to the 4D/480 for applications with moderate amounts of synchronization, but the difference in performance grows as the synchronization frequency increases. For applications that require a large amount of memory bandwidth, TreadMarks can perform better than the SGI 4D/480. Beyond eight processors, our results are based on execution-driven simulation. Specifically, we compare a software implementation on a general-purpose network of uniprocessor nodes, a hardware implementation using a directory-based protocol on a dedicated interconnect, and a combined implementation using software to provide shared memory between multiprocessor nodes with hardware implementing shared memory within a node. For the modest size of the problems that we can simulate, the hardware implementation scales well and the software implementation scales poorly. The combined approach delivers performance close to that of the hardware implementation for applications with small to moderate synchronization rates and good locality. Reductions in communi 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The LRC algorithm used by TreadMarks delays the propagation of modifications to a processor until that processor executes an acquire. To do so, LRC uses the happened-before-1 partial order <ref> [1] </ref>. The happened-before-1 partial order is the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs. Vector timestamps are used to represent the partial order [14].
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Over the last decade, considerable effort has been spent on software implementations of shared memory on general-purpose networks, e.g., <ref> [2, 4, 18] </ref>. We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., [10, 17].
Reference: [3] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Fel-ten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <type> Technical Report CS-TR-487-93, </type> <institution> Department of Computer Science, Prince-ton University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: to the AS architecture, its performance does not match AH because the number of synchronization messages and the wait time to acquire the locks remain high (see Figure 12). 3.2.4 Reduced Software Overhead By optimizing the software structure, as in Pere-grine [13], or a user-level hardware interface, as in SHRIMP <ref> [3] </ref>, lower software overheads can be achieved. In this section, we examine the effect of reducing both the fixed and per word overheads.
Reference: [4] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Over the last decade, considerable effort has been spent on software implementations of shared memory on general-purpose networks, e.g., <ref> [2, 4, 18] </ref>. We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., [10, 17]. <p> The effect of larger applications remains to be investigated. 4 Related Work TreadMarks implements shared memory entirely in software. Both data movement and memory coherence are performed by software using the message passing and virtual memory management hardware. Previous evaluations of such systems, for example, Carter et al. <ref> [4] </ref> have compared their performance to hand-coded message passing. Other related studies have examined software versus hardware cache coherence. In these studies, the hardware is responsible for performing the data movement. Upon access, the hardware automatically loads invalid cache lines from memory.
Reference: [5] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It does not require the programmer or compiler to insert cache flush in structions. Using trace-driven simulation, she compared the performance of her software scheme on a shared-bus to snoopy cache hardware. A few implementations using both hardware and software have been proposed. Both Chaiken et al. <ref> [5] </ref> and Hill et al. [12] describe shared memory implementations that handle the most common cache coherence operations in hardware and the most unusual operations in software, thereby reducing the complexity of the hardware without significantly impacting the performance. 5 Conclusions In this paper we have assessed the performance tradeoffs between
Reference: [6] <author> H. Cheong and A.V. Veidenbaum. </author> <title> A cache coherence scheme with fast selective invalidation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-145, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Upon access, the hardware automatically loads invalid cache lines from memory. To maintain coherency, these schemes require the placement of cache flush/invalidation instructions by the compiler or the programmer at the end of critical sections. Cytron et al. [8] and Cheong and Veidenbaum <ref> [6] </ref> describe algorithms for compiler-based software cache coherence. Owicki and Agarwal compare analytically the performance of such a scheme to snoopy cache coherence hardware [20]. Petersen, on the other hand, describes a software cache coherence scheme using the virtual memory management hardware [22]. This scheme is transparent to the programmer.
Reference: [7] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, S. Madala, and J. B. Sinclair. </author> <title> The efficient simulation of parallel computer systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Furthermore, the cost of the interconnection hardware is reduced by roughly a factor of N . Compared to the AH approach, commodity parts can be used, reducing the cost and complexity of the design. 3.1 Simulation Models We simulated the three architectures using an execution-driven simulator <ref> [7] </ref>. Instead of the DECstation-5000/240 and SGI 4D/480, we base our models on leading-edge technology. All of the architectural models use RISC processors with a 150 Mhz clock, 64 Kbyte direct-mapped caches with a block size of 32 bytes, and main memory sufficient to hold the simulated problem without paging.
Reference: [8] <author> R. Cytron, S. Karlovsky, </author> <title> and K.P. McAuliffe. Automatic management of programmable caches. </title> <booktitle> In 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Upon access, the hardware automatically loads invalid cache lines from memory. To maintain coherency, these schemes require the placement of cache flush/invalidation instructions by the compiler or the programmer at the end of critical sections. Cytron et al. <ref> [8] </ref> and Cheong and Veidenbaum [6] describe algorithms for compiler-based software cache coherence. Owicki and Agarwal compare analytically the performance of such a scheme to snoopy cache coherence hardware [20]. Petersen, on the other hand, describes a software cache coherence scheme using the virtual memory management hardware [22].
Reference: [9] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <note> To appear in Human Heredity, </note> <year> 1993. </year>
Reference-contexts: ILINK <ref> [9] </ref> is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. We ran ILINK with two different inputs, CLP and BAD, both corresponding to real data sets used in disease gene location. <p> ILINK achieves less than linear speedup on both the 4D/480 and TreadMarks because of a load balancing problem inherent to the nature of the algorithm <ref> [9] </ref>. It is not possible to predict in advance whether the set of iterations distributed to the processors will result in the same amount of work on each processor, without significant computation and communication. The 4D/480 outperforms TreadMarks because of the large amount of communication.
Reference: [10] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical report, </type> <institution> Silicon Graphics Computer Systems, </institution> <year> 1993. </year>
Reference-contexts: We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., <ref> [10, 17] </ref>. Several studies have compared software to hardware cache coherence mechanisms [20, 22], but these systems still rely on hardware initiated data movement and a dedicated interconnect.
Reference: [11] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Section 3 presents simulation results comparing the AS, AH, and HS architectures for a larger number of processors. Section 4 examines related work. Section 5 presents our conclusions. 2 SGI 4D/480 versus TreadMarks 2.1 TreadMarks In this section we briefly describe the release consistency (RC) model <ref> [11] </ref> and the lazy release consistency (LRC) implementation [14] used by TreadMarks. Further details on TreadMarks may be found in Keleher et al. [15]. RC is a relaxed memory consistency model. <p> The difference is slightly larger for the 18-city problem because of the increased synchronization and communication rates (see Table 2). The performance on TreadMarks suffers from the fact that TSP is not a properly labeled <ref> [11] </ref> program. Although updates to the current minimum tour length are synchronized, read accesses are not. Since Tread-Marks updates cached values only on an acquire, a processor may read an old value of the current minimum.
Reference: [12] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative shared memory: Software and hardware support for scaleable multiprocessors. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <month> Octo-ber </month> <year> 1992. </year>
Reference-contexts: Using trace-driven simulation, she compared the performance of her software scheme on a shared-bus to snoopy cache hardware. A few implementations using both hardware and software have been proposed. Both Chaiken et al. [5] and Hill et al. <ref> [12] </ref> describe shared memory implementations that handle the most common cache coherence operations in hardware and the most unusual operations in software, thereby reducing the complexity of the hardware without significantly impacting the performance. 5 Conclusions In this paper we have assessed the performance tradeoffs between hardware and software implementations of
Reference: [13] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software: Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 13-fold decrease in the amount of data movement compared to the AS architecture, its performance does not match AH because the number of synchronization messages and the wait time to acquire the locks remain high (see Figure 12). 3.2.4 Reduced Software Overhead By optimizing the software structure, as in Pere-grine <ref> [13] </ref>, or a user-level hardware interface, as in SHRIMP [3], lower software overheads can be achieved. In this section, we examine the effect of reducing both the fixed and per word overheads.
Reference: [14] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The similarity between the two platforms "from the neck up" avoids many distinctions that often blur comparative studies, and allows us to focus on the differences caused by the shared-memory implementation. TreadMarks supports lazy release consistency <ref> [14] </ref> and is implemented as a user-level library on top of Ultrix [15]. The SGI 4D/480 provides processor consistency, using a bus snooping protocol [21]. We use four applications in our comparison (ILINK, SOR, TSP, and Water). <p> Section 4 examines related work. Section 5 presents our conclusions. 2 SGI 4D/480 versus TreadMarks 2.1 TreadMarks In this section we briefly describe the release consistency (RC) model [11] and the lazy release consistency (LRC) implementation <ref> [14] </ref> used by TreadMarks. Further details on TreadMarks may be found in Keleher et al. [15]. RC is a relaxed memory consistency model. In RC, ordinary shared memory accesses are distinguished from synchronization accesses, with the latter category subdivided into acquire and release accesses. <p> To do so, LRC uses the happened-before-1 partial order [1]. The happened-before-1 partial order is the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs. Vector timestamps are used to represent the partial order <ref> [14] </ref>. When a processor executes an acquire, it sends its current vector timestamp in the acquire message. The last releaser then piggybacks on its response a set of write notices. These write notices describe the shared data modifications that precede the acquire according to the partial order.
Reference: [15] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Up to eight processors, our results are based on an experimental comparison of a software and a hardware implementation. Specifically, we compare the Tread-Marks software distributed shared memory system <ref> [15] </ref> running on a 100Mbit/second ATM network connecting 8 DECstation-5000/240s to an 8-processor Silicon Graphics 4D/480. These configurations have identical processors, clock speeds, primary caches, compilers, and parallel programming interfaces (the ANL PAR-MACS macros [19]). <p> The similarity between the two platforms "from the neck up" avoids many distinctions that often blur comparative studies, and allows us to focus on the differences caused by the shared-memory implementation. TreadMarks supports lazy release consistency [14] and is implemented as a user-level library on top of Ultrix <ref> [15] </ref>. The SGI 4D/480 provides processor consistency, using a bus snooping protocol [21]. We use four applications in our comparison (ILINK, SOR, TSP, and Water). TSP uses only locks for synchronization, SOR and ILINK use only barriers, and Water uses both. <p> Section 5 presents our conclusions. 2 SGI 4D/480 versus TreadMarks 2.1 TreadMarks In this section we briefly describe the release consistency (RC) model [11] and the lazy release consistency (LRC) implementation [14] used by TreadMarks. Further details on TreadMarks may be found in Keleher et al. <ref> [15] </ref>. RC is a relaxed memory consistency model. In RC, ordinary shared memory accesses are distinguished from synchronization accesses, with the latter category subdivided into acquire and release accesses.
Reference: [16] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> To appear in Proceedings of the 21st Annual International Conference on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Cache coherence is maintained using a directory-based protocol. A cache miss satisfied by remote memory takes 92 to 130 processor cycles, depending on the block's location and whether it has been modified. These cycle counts are similar to those for the Stan-ford DASH [17] and FLASH <ref> [16] </ref> multiprocessors. In both the AS and the HS models, the general-purpose network is an ATM switch with a point-to-point bandwidth of 622 Mbit/second and a latency of 1 microsecond. Memory consistency between the nodes is maintained using the TreadMarks LRC invalidate protocol (see Section 2.1).
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., <ref> [10, 17] </ref>. Several studies have compared software to hardware cache coherence mechanisms [20, 22], but these systems still rely on hardware initiated data movement and a dedicated interconnect. <p> Cache coherence is maintained using a directory-based protocol. A cache miss satisfied by remote memory takes 92 to 130 processor cycles, depending on the block's location and whether it has been modified. These cycle counts are similar to those for the Stan-ford DASH <ref> [17] </ref> and FLASH [16] multiprocessors. In both the AS and the HS models, the general-purpose network is an ATM switch with a point-to-point bandwidth of 622 Mbit/second and a latency of 1 microsecond. Memory consistency between the nodes is maintained using the TreadMarks LRC invalidate protocol (see Section 2.1).
Reference: [18] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Over the last decade, considerable effort has been spent on software implementations of shared memory on general-purpose networks, e.g., <ref> [2, 4, 18] </ref>. We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., [10, 17].
Reference: [19] <editor> E. L. Lusk and R. A. Overbeek et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc, </publisher> <year> 1987. </year>
Reference-contexts: Specifically, we compare the Tread-Marks software distributed shared memory system [15] running on a 100Mbit/second ATM network connecting 8 DECstation-5000/240s to an 8-processor Silicon Graphics 4D/480. These configurations have identical processors, clock speeds, primary caches, compilers, and parallel programming interfaces (the ANL PAR-MACS macros <ref> [19] </ref>). The similarity between the two platforms "from the neck up" avoids many distinctions that often blur comparative studies, and allows us to focus on the differences caused by the shared-memory implementation. TreadMarks supports lazy release consistency [14] and is implemented as a user-level library on top of Ultrix [15].
Reference: [20] <author> S. Owicki and A. Agarwal. </author> <title> Evaluating the performance of software cache coherence. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 230-242, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., [10, 17]. Several studies have compared software to hardware cache coherence mechanisms <ref> [20, 22] </ref>, but these systems still rely on hardware initiated data movement and a dedicated interconnect. In this paper, we compare a shared-memory implementation that runs entirely in software on a general-purpose network of computers to a hardware implementation on a dedicated interconnect. <p> Cytron et al. [8] and Cheong and Veidenbaum [6] describe algorithms for compiler-based software cache coherence. Owicki and Agarwal compare analytically the performance of such a scheme to snoopy cache coherence hardware <ref> [20] </ref>. Petersen, on the other hand, describes a software cache coherence scheme using the virtual memory management hardware [22]. This scheme is transparent to the programmer. It does not require the programmer or compiler to insert cache flush in structions.
Reference: [21] <author> M. Papamarcos and J. Patel. </author> <title> A low overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: TreadMarks supports lazy release consistency [14] and is implemented as a user-level library on top of Ultrix [15]. The SGI 4D/480 provides processor consistency, using a bus snooping protocol <ref> [21] </ref>. We use four applications in our comparison (ILINK, SOR, TSP, and Water). TSP uses only locks for synchronization, SOR and ILINK use only barriers, and Water uses both. For ILINK, SOR, and TSP, we present results for two different sets of input data.
Reference: [22] <author> K. Petersen. </author> <title> Operating System Support for Modern Memory Hierarchies. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We are, however, unaware of any study comparing the performance of any of these systems to the performance of a hardware implementation of shared memory on a dedicated interconnect, e.g., [10, 17]. Several studies have compared software to hardware cache coherence mechanisms <ref> [20, 22] </ref>, but these systems still rely on hardware initiated data movement and a dedicated interconnect. In this paper, we compare a shared-memory implementation that runs entirely in software on a general-purpose network of computers to a hardware implementation on a dedicated interconnect. <p> Owicki and Agarwal compare analytically the performance of such a scheme to snoopy cache coherence hardware [20]. Petersen, on the other hand, describes a software cache coherence scheme using the virtual memory management hardware <ref> [22] </ref>. This scheme is transparent to the programmer. It does not require the programmer or compiler to insert cache flush in structions. Using trace-driven simulation, she compared the performance of her software scheme on a shared-bus to snoopy cache hardware.
Reference: [23] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We use 18- and 19-city problems as input. Although the program exhibits nondeterministic behavior, occasionally resulting in super-linear speedup, executions with the same input produce repeatable results. Water, from the SPLASH suite <ref> [23] </ref>, is a molecular dynamics simulation.
References-found: 23

