URL: http://www.bell-labs.com/user/seung/papers/online.ps.gz
Refering-URL: http://www.bell-labs.com/user/seung/papers/index.html
Root-URL: 
Email: naama@fiz.huji.ac.il  seung@physics.att.com  haim@fiz.huji.ac.il  
Title: On-line Learning of Dichotomies  
Author: N. Barkai H. S. Seung H. Sompolinsky 
Address: Jerusalem, Israel 91904  Murray Hill, NJ 07974  Jerusalem, Israel 91904  
Affiliation: Racah Institute of Physics The Hebrew University  AT&T Bell Laboratories  Racah Institute of Physics The Hebrew University  and AT&T Bell Laboratories  
Abstract: The performance of on-line algorithms for learning dichotomies is studied. In on-line learning, the number of examples P is equivalent to the learning time, since each example is presented only once. The learning curve, or generalization error as a function of P , depends on the schedule at which the learning rate is lowered. For a target that is a perceptron rule, the learning curve of the perceptron algorithm can decrease as fast as P 1 , if the schedule is optimized. If the target is not realizable by a perceptron, the perceptron algorithm does not generally converge to the solution with lowest generalization error. For the case of unrealizability due to a simple output noise, we propose a new on-line algorithm for a perceptron yielding a learning curve that can approach the optimal generalization error as fast as P 1=2 . We then generalize the perceptron algorithm to any class of thresholded smooth functions learning a target from that class. For "well-behaved" input distributions, if this algorithm converges to the optimal solution, its learning curve can decrease as fast as P 1 .
Abstract-found: 1
Intro-found: 1
Reference: <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 605-618, </pages> <year> 1992. </year>
Reference: <author> E. B. Baum. </author> <title> The perceptron algorithm is fast for nonmalicious distributions. </title> <journal> Neural Comput., </journal> <volume> 2 </volume> <pages> 248-260, </pages> <year> 1990. </year>
Reference-contexts: The perceptron algorithm, however, is not efficient in the sense of distribution-free PAC learning (Valiant, 1984), for one can construct input distributions that require an arbitrarily long convergence time. In a recent paper <ref> (Baum, 1990) </ref> Baum proved that the perceptron algorithm applied in an on-line mode, converges as P 1=3 when learning a half space under a uniform input distribution, where P is the number of presented examples drawn at random. For on-line learning P is also the number of time steps. <p> The optimal schedule leads to an inverse power law learning curve, * g ~ ff 1 . Baum's results <ref> (Baum, 1990) </ref> of a non-normalized perceptron with a constant learning rate can be viewed as a special case of the above analysis. In the non-normalized perceptron algorithm, the magnitude of the student's weights grow with ff as jWj ~ ff 1=3 .
Reference: <author> H. J. Kushner and D. S. Clark. </author> <title> Stochastic approximation methods for constrained and unconstrained systems. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1978. </year>
Reference-contexts: In many situations, it is more natural to consider on-line learning paradigms, in which at each time step a new example is chosen. The examples are never recycled, and the learner is not allowed to simply store them (see e.g, Heskes, 1991; Hansen, 1993; Radons, 1993). Stochastic approximation theory <ref> (Kushner, 1978) </ref> provides a framework for understanding of the local convergence properties of on-line learning of smooth functions. This paper addresses the problem of on-line learning of dichotomies, for which no similarly complete theory yet exists. We begin with on-line learning of perceptron rules.
Reference: <author> L. K. Hansen, R. Pathria, and P. Salamon. </author> <title> Stochastic dynamics of supervised learning. </title> <journal> J. Phys., </journal> <volume> A26:63-71, </volume> <year> 1993. </year>
Reference: <author> T. Heskes and B. Kappen. </author> <title> Learning processes in neural networks. </title> <journal> Phys. Rev., </journal> <volume> A44:2718-2762, </volume> <year> 1991. </year>
Reference: <author> T. Heskes, E. T. P. Slijpen, and B. Kappen. </author> <title> Learning in neural networks with local minima. </title> <journal> Phys. Rev., </journal> <volume> A46:5221-5231, </volume> <year> 1992. </year>
Reference-contexts: The above conclusions assume that the equilibrium state at small learning rates is unique, which in general is not the case. The issue of overcoming local minima in on-line learning is a difficult problem <ref> (Heskes, 1992) </ref> Finally, the theoretical results for on-line learning has the important advantage of not requiring the use of the often problematic replica formalism. Acknowledgements We are grateful for helpful discussions with Y. Freund, M. Kearns, R. Schapire, and E. Shamir, and thank Y.
Reference: <author> Y. Kabashima. </author> <title> Perfect loss of generalization due to noise in k = 2 parity machines. </title> <journal> J. Phys., </journal> <volume> A27:1917-1927, </volume> <year> 1994. </year>
Reference: <author> Y. Kabashima and S. Shinomoto. </author> <title> Incremental learning with and without queries in binary choice problems. </title> <booktitle> In Proc. of IJCNN, </booktitle> <year> 1993. </year>
Reference-contexts: The numerical results are in excellent agreement with the prediction * g (ff) = 1:27=ff for the asymptotic behavior. Finally, we note that our analysis of the time-dependent case is similar to that of Kabashima and Shinomoto for a different on-line learning problem <ref> (Kabashima, 1993) </ref>. 3 On-line learning of a perceptron with output noise In the case discussed above, the task can be fully realized by a perceptron, i.e., there is a perceptron W such that * g = 0.
Reference: <author> G. Radons. </author> <title> On stochastic dynamics of supervised learning. </title> <journal> J. Phys., </journal> <volume> A26:3455-3461, </volume> <year> 1993. </year>
Reference: <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Phys. Rev., </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: Thus the optimal asymptotic change of is 2 p 2=ff, in which case the error will behave asymptotically as * g (ff) ~ 1:27=ff. This is not far from the batch asymptotic <ref> (Seung, 1992) </ref> * g (ff) ~ 0:625=ff. We have confirmed these results by numerical simulation of the algorithm Eq. (2). p 2=ff. The numerical results are in excellent agreement with the prediction * g (ff) = 1:27=ff for the asymptotic behavior. <p> In this case, the convergence to the optimal error is as ff 1=2 . This is the same power law as obtained in the standard sample complexity upper bounds (Vapnik, 1982) and in the approximate replica symmetric calculations <ref> (Seung, 1992) </ref> for batch learning of unrealizable rules. It should be stressed however, that the success of the modified algorithm in the case of an output noise depends on the fact that the errors made by the optimal solution are uncorrelated with the input.
Reference: <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The algorithm has been proven to converge in finite time and to yield a half plane separating any set of linearly separable examples. The perceptron algorithm, however, is not efficient in the sense of distribution-free PAC learning <ref> (Valiant, 1984) </ref>, for one can construct input distributions that require an arbitrarily long convergence time.
Reference: <author> N. G. Van Kampen. </author> <title> Stochastic processes in physics and chemistry. </title> <publisher> North holland 1981. </publisher>
Reference-contexts: us consider for simplicity the one-dimensional case, w 0 = w + g (w; s), where g (w; s) = (f (w; s)f (w 0 ; s)) sgn (f (w 0 ; s)) @w This equation can be converted into a Markov equation for the probability distribution, P (w; n) <ref> (Van Kam-pen, 1981) </ref> P (w; n + 1) = dw 0 W (w 0 jw)P (w 0 ; n) (22) where W (wjw 0 ) =&lt; ffi (w 0 w g (w; s)) &gt; is the transition rate from w to w'.
Reference: <author> V. N. Vapnik. </author> <title> Estimation of Dependences based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year> <title> 0 0.01 0.02 0.03 * g (solid curve) are compared with the theoretical prediction * g = 1:27=ff (dashed curve). 0 0.1 0.2 0.3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 1= ff N = 250, u = 4, and q 0 = 1:95. The regular perceptron learning (dashed curve) is compared with the modified algorithm (solid curve). The dashed line shows the theoretical prediction Eq. </title> <type> (18) </type> . 
Reference-contexts: In this case, the convergence to the optimal error is as ff 1=2 . This is the same power law as obtained in the standard sample complexity upper bounds <ref> (Vapnik, 1982) </ref> and in the approximate replica symmetric calculations (Seung, 1992) for batch learning of unrealizable rules.
References-found: 13

