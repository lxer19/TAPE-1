URL: http://www.cs.umd.edu/~keleher/papers/ijpp98.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/papers.html
Root-URL: 
Title: Eliminating Barrier Synchronization for Compiler-Parallelized Codes on Software DSMs  
Author: Hwansoo Han, Chau-Wen Tseng, Pete Keleher 
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  
Abstract: Software distributed-shared-memory (DSM) systems provide an appealing target for par-allelizing compilers due to their flexibility. Previous studies demonstrate such systems can provide performance comparable to message-passing compilers for dense-matrix kernels. However, synchronization and load imbalance are significant sources of overhead. In this paper, we investigate the impact of compilation techniques for eliminating barrier synchronization overhead in software DSMs. Our compile-time barrier elimination algorithm extends previous techniques in three ways: 1) we perform inexpensive communication analysis through local subscript analysis when using chunk iteration partitioning for parallel loops, 2) we exploit delayed updates in lazy-release-consistency DSMs to eliminate barriers guarding only anti-dependences, 3) when possible we replace barriers with customized nearest-neighbor synchronization. Experiments on an IBM SP-2 indicate these techniques can improve parallel performance by 20% on average and by up to 60% for some applications.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Stoher and O'Boyle [23] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 7, 18] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** [27]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2, 3].
Reference: [2] <author> S. Chandra and J.R. Larus. </author> <title> HPF on fine-grain distributed shared memory: Early experience. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** [27]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system <ref> [2, 3] </ref>. Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [8].
Reference: [3] <author> S. Chandra and J.R. Larus. </author> <title> Optimizing communication in HPF programs for fine-grain distributed shared memory. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** [27]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system <ref> [2, 3] </ref>. Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [8].
Reference: [4] <author> A. Cox, S. Dwarkadas, H. Lu, and W. Zwaenepoel. </author> <title> Evaluating the performance of software distributed shared memory as a target for parallelizing compilers. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: The resulting programs are portable since they can be run on the large-scale parallel machines as well as the low-end, but more pervasive multiprocessor workstations. 1 Shared-memory parallelizing compilers are easy to use, flexible, and can accept a wide range of applications. Results from several recent studies <ref> [4, 14] </ref> indicate they can approach the performance of current message-passing compilers or explicitly-parallel message-passing programs on distributed-memory machines. However, load imbalance and synchronization overhead were identified as sources of inefficiency when compared with message-passing programs. <p> Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR <ref> [4] </ref>. Results show that SPF/TreadMarks is slightly less efficient for dense-matrix programs, but outperforms compiler-generated message-passing code for irregular programs. They also identify opportunities for the compiler to eliminate unneeded barrier synchronization and aggregating messages in the shared-memory programs.
Reference: [5] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> A compiler-assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors <ref> [5] </ref>. They concentrated on safety concerns and the effect on privatization. In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [11]. O'Boyle and Bodin [19] present techniques similar to local subscript analysis.
Reference: [6] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols [17]. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance on a software DSM <ref> [6] </ref>. By combining analysis in the ParaScope programming environment with TreadMarks, they were able to compute data access patterns at compile time and use it to help the runtime system aggregate communication and synchronization.
Reference: [7] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Lazy release consistency postpones the propagation of modifications further; updates to shared data do not have to be made visible to a processor 3 until the next time that processor acquires a released synchronization variable. Experiments show that lazy-release-consistency protocols generally cause less communication than release consistency <ref> [7] </ref>. Consistency information in CVM is piggybacked on synchronization messages. Multiple updates are also aggregated in a single message where possible. 2.3 SUIF/CVM Interface SUIF was retargeted to generate code for CVM by providing a run-time interface based on CVM thread creation and synchronization primitives. <p> Stoher and O'Boyle [23] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 7, 18] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** [27]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2, 3].
Reference: [8] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. Granston and Wishoff suggest a number of compiler optimizations for software DSMs <ref> [8] </ref>. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols [17].
Reference: [9] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance. Our techniques are evaluated in a prototype system [14] using the 2 CVM [12] software distributed-shared-memory (DSM) as a compilation target for the SUIF <ref> [9] </ref> shared-memory compiler. <p> We also review previous compiler techniques for reducing synchronization overhead. 2.1 SUIF Shared-Memory Compiler SUIF is an optimizing and parallelizing compiler developed at Stanford <ref> [9] </ref>. It has been successful in finding parallelism in many standard scientific applications. <p> In our experiments, CVM [12] applications written in Fortran 77 were automatically paral-lelized by the Stanford SUIF parallelizing compiler version 1.1.2 <ref> [9] </ref>, with close to 100% of the computation in parallel regions. A simple chunk scheduling policy assigns contiguous iterations of equal or near-equal size to each processor, resulting in a consistent computation partition that encourages good locality.
Reference: [10] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 20, 21] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10]. <p> Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers <ref> [10] </ref>. Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results [20]. Eliminating barriers in compiler-parallelized codes is more difficult.
Reference: [11] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Our implementation of nearest-neighbor synchronization solves this problem by invoking a global barrier the first time it is invoked at each location in the program. Since anti-dependences may be ignored, the algorithm for inserting barrier synchronization becomes similar to the algorithm for message vectorization <ref> [11] </ref>. The level of the deepest true/flow cross-processor dependence becomes the point where synchronization must be inserted to prevent data races. Synchronization at lower loop levels is not needed. Finally, in Figure 4 (c) a cross-processor true/flow dependence (read-after-write) exists which does need synchronization. <p> They concentrated on safety concerns and the effect on privatization. In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication <ref> [11] </ref>. O'Boyle and Bodin [19] present techniques similar to local subscript analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence.
Reference: [12] <author> P. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance. Our techniques are evaluated in a prototype system [14] using the 2 CVM <ref> [12] </ref> software distributed-shared-memory (DSM) as a compilation target for the SUIF [9] shared-memory compiler. <p> After each parallel computation worker threads spin or go to sleep, waiting for additional work from the master thread. 2.2 CVM Software DSM CVM is a software DSM that supports coherent shared memory for multiple protocols and consistency models <ref> [12] </ref>. It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency [13], a derivation of release consistency. <p> Both communication and computation are slow, but the communication/computation ratio should be similar to newer systems. Since the impact of synchronization optimizations is proportional to the ratio (more impact when communication costs are relatively high), our experimental results should also apply to newer parallel architectures. In our experiments, CVM <ref> [12] </ref> applications written in Fortran 77 were automatically paral-lelized by the Stanford SUIF parallelizing compiler version 1.1.2 [9], with close to 100% of the computation in parallel regions.
Reference: [13] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed 18 shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency <ref> [13] </ref>, a derivation of release consistency. Release consistency allows a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur.
Reference: [14] <author> P. Keleher and C.-W. Tseng. </author> <title> Enhancing software DSM for compiler-parallelized applications. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: The resulting programs are portable since they can be run on the large-scale parallel machines as well as the low-end, but more pervasive multiprocessor workstations. 1 Shared-memory parallelizing compilers are easy to use, flexible, and can accept a wide range of applications. Results from several recent studies <ref> [4, 14] </ref> indicate they can approach the performance of current message-passing compilers or explicitly-parallel message-passing programs on distributed-memory machines. However, load imbalance and synchronization overhead were identified as sources of inefficiency when compared with message-passing programs. <p> It is clear from these measurements that reducing load imbalance caused by synchronization overhead is important for achieving good performance. In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance. Our techniques are evaluated in a prototype system <ref> [14] </ref> using the 2 CVM [12] software distributed-shared-memory (DSM) as a compilation target for the SUIF [9] shared-memory compiler. <p> Performance was improved by adding customized support for reductions, as well as a flush update protocol that at barriers automatically sends updates to processors possessing copies of recently modified shared data <ref> [14] </ref>. Compiler analysis needed to use the flush update protocol is much simpler than communication analysis needed in HPF compilers. <p> The resulting C output code was compiled by g++ version 2.7.2 with the -O2 flag, then linked with the SUIF run-time system and the CVM libraries to produce executable code on the IBM SP-2. Customized support for reductions and the flush update protocol were used to improve overall performance <ref> [14] </ref>. These enhancements reduce communication overhead by exploiting communication patterns in iterative scientific computations to combine communication for multiple nonlocal accesses with barrier synchronization messages.
Reference: [15] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: However, users are willing to write message-passing programs only for a few important applications because it takes too much time and effort. Compilers for languages such as High Performance Fortran <ref> [15] </ref> provide a partial solution because they allow users to avoid writing explicit message-passing code, but HPF compilers currently only support a limited class of data-parallel applications.
Reference: [16] <author> Z. Li. </author> <title> Compiler algorithms for event variable synchronization. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: sensitivity to load imbalance in software DSMs also encourages the development of new synchronization optimizations. 5 Related Work Before studying methods for eliminating barrier synchronization, researchers investigated efficient use of data and event synchronization, where post and wait statements are used to synchronize between data items [24] or loop iterations <ref> [16] </ref>. Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation [10, 20, 21]. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance.
Reference: [17] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of DSM systems via compiler involvement. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols <ref> [17] </ref>. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance on a software DSM [6].
Reference: [18] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Stoher and O'Boyle [23] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 7, 18] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** [27]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2, 3].
Reference: [19] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: They concentrated on safety concerns and the effect on privatization. In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [11]. O'Boyle and Bodin <ref> [19] </ref> present techniques similar to local subscript analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence.
Reference: [20] <author> M. Philippsen and E. Heinz. </author> <title> Automatic synchronization elimination in synchronous FORALLs. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 20, 21] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10]. <p> Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results <ref> [20] </ref>. Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors [5]. They concentrated on safety concerns and the effect on privatization.
Reference: [21] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 20, 21] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10].
Reference: [22] <author> R. Rajamony and A.L. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems (MASCOTS), </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Many of their suggestions are implemented in the SUIF/CVM system and are evaluated in this paper. Rajamony and Cox developed a performance debugger for detecting unnecessary synchronization at run-time by instrumenting all loads and stores <ref> [22] </ref>. In the SPLASH application Water, it was able to detect barriers guarding only anti and output dependences that may be eliminated by applying odd-even renaming. In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences. Tzeng and Kongmunvattana improve the efficiency of barriers for software DSMs [26].
Reference: [23] <author> E. Stohr and M. O'Boyle. </author> <title> A graph based approach to barrier synchronisation minimisation. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: O'Boyle and Bodin [19] present techniques similar to local subscript analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence. Stoher and O'Boyle <ref> [23] </ref> extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs [1, 7, 18]. More recently, groups have examined combining compilers and software DSMs.
Reference: [24] <author> P. Tang, P. Yew, and C. Zhu. </author> <title> Compiler techniques for data synchronization in nested parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The sensitivity to load imbalance in software DSMs also encourages the development of new synchronization optimizations. 5 Related Work Before studying methods for eliminating barrier synchronization, researchers investigated efficient use of data and event synchronization, where post and wait statements are used to synchronize between data items <ref> [24] </ref> or loop iterations [16]. Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation [10, 20, 21]. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance.
Reference: [25] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Barrier synchronization overhead is particularly significant as the number of processors increases, since the interval between barriers decreases as computation is partitioned across more processors. In previous work, we developed compiler algorithms for barrier elimination <ref> [25] </ref>. We first generate code for parallel loops using the single-program, multiple-data (SPMD) programming model found in message-passing programs, where all threads execute the entire program. Sequential computation is either replicated or explicitly guarded to limit execution to a single thread, while parallel computation is partitioned and executed across processors. <p> In this section we discuss novel extensions to compiler techniques for eliminating barrier synchronization. 3.1 Communication Analysis Using Local Subscripts Previous communication analysis relied on compile-time information on the data and computation decomposition selected for a program to precisely determine whether interprocessor communication takes place <ref> [25] </ref>. We find that an alternative communication analysis technique based on local subscript analysis can yield good results with much less complex analysis. The Local subscript analysis algorithm is shown in Figure 3. <p> The compiler normally inserts a barrier as the last statement of the time loop, but local subscript analysis can show only nearest-neighbor synchronization is needed. 3.3 Customized Nearest-Neighbor Synchronization At some barriers, the compiler can detect communication only takes place between neighboring processors <ref> [25] </ref>. To take advantage of this information, we implemented a customized routine for nearest-neighbor synchronization (where each processor has either zero, one, or two neighbors) directly in CVM. <p> Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors [5]. They concentrated on safety concerns and the effect on privatization. In previous work <ref> [25] </ref> we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [11]. O'Boyle and Bodin [19] present techniques similar to local subscript analysis.
Reference: [26] <author> N.-F. Tzeng and A. Kongmunvattana. </author> <title> Distributed shared memory systems with improved barrier synchronization and data transfer. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: In the SPLASH application Water, it was able to detect barriers guarding only anti and output dependences that may be eliminated by applying odd-even renaming. In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences. Tzeng and Kongmunvattana improve the efficiency of barriers for software DSMs <ref> [26] </ref>.
Reference: [27] <author> G. Viswanathan and J.R. Larus. </author> <title> Compiler-directed shared-memory communication for iterative parallel computations. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year> <month> 19 </month>
Reference-contexts: There has been a large amount of research on software DSMs [1, 7, 18]. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a 16 two-part predictive protocol for iterative computations for use in the data-parallel language C** <ref> [27] </ref>. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2, 3]. Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated.
References-found: 27

