URL: ftp://svr-ftp.eng.cam.ac.uk/pub/reports/tham_ml94abs.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/robot-learning.html
Root-URL: 
Title: A Modular Q-Learning Architecture for Manipulator Task Decomposition `Data storage in the cerebellar model ar
Author: Chen K. Tham Richard W. Prager Holland, J. R. Michalski, J. Carbonell T. Mitchell, Watkins, C. 
Note: 1975) structures. NOTE The full paper can be found in the main ML'94 proceedings. RESEARCH WORK AND INTERESTS From  References Albus, J. (1975),  eds, `Machine Learning: An Artificial Intelligence Approach', Vol. II, Morgan Kaufmann, Los Altos, CA, chapter 20, pp.  Singh, S. (1992),  
Address: Cambridge CB2 1PZ United Kingdom  Cambridge CB2 1PZ United Kingdom  Singapore.  Cambridge, Cambridge, UK.  
Affiliation: Department of Engineering University of Cambridge  Department of Engineering University of Cambridge  Department of Electrical Engineering, National University of  University of  
Email: ckt@eng.cam.ac.uk  rwp@eng.cam.ac.uk  e-mail: eletck@leonis.nus.sg  
Degree: thesis,  
Date: January 1995:  593-623.  
Abstract: Compositional Q-Learning (CQ-L) (Singh 1992) is a modular approach to learning to perform composite tasks made up of several elemental tasks by reinforcement learning. Skills acquired while performing elemental tasks are also applied to solve composite tasks. Individual skills compete for the right to act and only winning skills are included in the decomposition of the composite task. We extend the original CQ-L concept in two ways: (1) a more general reward function, and (2) the agent can have more than one actuator. We use the CQ-L architecture to acquire skills for performing composite tasks with a simulated two-linked manipulator having large state and action spaces. The manipulator is a non-linear dynamical system and we require its end-effector to be at specific positions in the workspace. Fast function approximation in each of the Q-modules is achieved through the use of an array of Cerebellar Model Articulation Controller (CMAC) (Albus Our research interests involve the scaling up of machine learning methods, especially reinforcement learning, for autonomous robot control. We are interested in function approximators suitable for reinforcement learning in problems with large state spaces, such as the Cerebellar Model Articulation Controller (CMAC) (Albus 1975) which permit fast, online learning and good local generalization. In addition, we are interested in task decomposition by reinforcement learning and the use of hierarchical and modular function approximator architectures. We are examining the effectiveness of a modified Hierarchical Mixtures of Experts (HME) (Jordan & Jacobs 1993) approach for reinforcement learning since the original HME was developed mainly for supervised learning and batch learning tasks. The incorporation of domain knowledge into reinforcement learning agents is an important way of extending their capabilities. Default policies can be specified, and domain knowledge can also be used to restrict the size of the state-action space, leading to faster learning. We are investigating the use of Q-Learning (Watkins 1989) in planning tasks, using a classifier system (Holland 1986) to encode the necessary condition-action rules. Jordan, M. & Jacobs, R. (1993), Hierarchical mixtures of experts and the EM algorithm, Technical Report 9301, MIT Computational Cognitive Science. 
Abstract-found: 1
Intro-found: 0
Reference: <author> Albus, J. </author> <year> (1975), </year> <title> `Data storage in the cerebellar model ar ticulation controller (CMAC)', Journal of Dynamic Systems, </title> <booktitle> Measurement and Control 97(3), </booktitle> <pages> 228-233. </pages>
Reference: <author> Holland, J. </author> <year> (1986), </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems, </title> <editor> in R. Michalski, J. Carbonell & T. Mitchell, eds, </editor> <booktitle> `Machine Learning: An Artificial Intelligence Approach', </booktitle> <volume> Vol. II, </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, chapter 20, </address> <pages> pp. 593-623. </pages>
Reference: <author> Jordan, M. & Jacobs, R. </author> <year> (1993), </year> <title> Hierarchical mixtures of experts and the EM algorithm, </title> <type> Technical Report 9301, </type> <institution> MIT Computational Cognitive Science. </institution>
Reference: <author> Singh, S. </author> <year> (1992), </year> <title> `Transfer of learning by composing solu tions of elemental sequential tasks', </title> <booktitle> Machine Learning 8(3/4), </booktitle> <pages> 323-339. </pages>
Reference: <author> Watkins, C. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> Cambridge, UK. </address>
References-found: 5

