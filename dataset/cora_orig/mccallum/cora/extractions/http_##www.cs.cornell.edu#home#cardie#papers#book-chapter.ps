URL: http://www.cs.cornell.edu/home/cardie/papers/book-chapter.ps
Refering-URL: http://www.cs.cornell.edu/home/cardie/
Root-URL: 
Title: Embedded Machine Learning Systems for Natural Language Processing: A General Framework Connectionist, Statistical and Symbolic
Author: Claire Cardie In Wermter, S. and Riloff, E. and Scheler, Gabriele (eds.), 
Date: 1996.  
Note: Processing, Lecture Notes in Artificial Intelligence, 315-328, Springer,  
Address: Ithaca NY 14853, USA  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: This paper presents Kenmore, a general framework for knowledge acquisition for natural language processing (NLP) systems. To ease the acquisition of knowledge in new domains, Kenmore exploits an online corpus using robust sentence analysis and embedded symbolic machine learning techniques while requiring only minimal human intervention. By treating all problems in ambiguity resolution as classification tasks, the framework uniformly addresses a range of subproblems in sentence analysis, each of which traditionally had required a separate computational mechanism. In a series of experiments, we demonstrate the successful use of Kenmore for learning solutions to several problems in lexical and structural ambiguity resolution. We argue that the learning and knowledge acquisition components should be embedded components of the NLP system in that (1) learning should take place within the larger natural language understanding system as it processes text, and (2) the learning components should be evaluated in the context of prac tical language-processing tasks.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: In fact, a number of existing systems successfully combine traditional parsing techniques and distributed connectionist modules for sentence analysis (e.g., [26, 27, 10]). Symbolic machine learning systems, however, provide at least two advantages. First, there are many incremental symbolic machine learning algorithms (e.g., ID5 [24], instance-based learning methods <ref> [1] </ref>) that would allow the training phase to become progressively easier for the human supervisor: Kenmore can access the existing concept description to suggest solutions to each ambiguity and rely on the supervisor only to override incorrect predictions. This type of incremental learning is generally not feasible in connectionist systems.
Reference: 2. <author> Chinatsu Aone and William Bennett. </author> <title> Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <pages> pages 122-129. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1995. </year>
Reference-contexts: In related work, symbolic machine learning algorithms, and case-based algorithms in particular, have also been found to work well for a number of low-level language acquisition tasks (e.g., stress acquisition [13] and grapheme-to-phoneme conversion [3]) and discourse-level tasks (e.g., anaphora resolution <ref> [2, 19] </ref> and segmentation [18]). This work provides additional evidence for the viability of a machine learning approach to natural language learning and knowledge acquisition for natural language processing systems.
Reference: 3. <author> A. van den Bosch and W. Daelemans. </author> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the Sixth Conference of the EACL, </booktitle> <pages> pages 45-53, </pages> <address> Utrecht, </address> <year> 1993. </year> <note> Also available as ITK Research Report 42. </note>
Reference-contexts: In related work, symbolic machine learning algorithms, and case-based algorithms in particular, have also been found to work well for a number of low-level language acquisition tasks (e.g., stress acquisition [13] and grapheme-to-phoneme conversion <ref> [3] </ref>) and discourse-level tasks (e.g., anaphora resolution [2, 19] and segmentation [18]). This work provides additional evidence for the viability of a machine learning approach to natural language learning and knowledge acquisition for natural language processing systems.
Reference: 4. <author> E. Brill. </author> <title> Some Advances in Transformation-Based Part of Speech Tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 722-727. </pages> <publisher> AAAI Press / MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In the following section, we summarize the advantages of using embedded machine learning components in an NLP system. 5 Advantages of Embedded Machine Learning Mechanisms for NLP Systems Corpus-based language acquisition methods recently have been the focus of much attention in the natural language processing community <ref> [4, 9, 12, 13, 28] </ref>. With very few exceptions, however, these systems treat language learning and knowledge acquisition in isolation from the actual natural language system that will use the acquired knowledge and in isolation from any specific natural language understanding task.
Reference: 5. <author> C. Cardie. </author> <title> Corpus-Based Acquisition of Relative Pronoun Disambiguation Heuris--tics. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 216-223, </pages> <institution> University of Delaware, Newark, DE, 1992. Association for Computational Linguistics. </institution>
Reference-contexts: In this task, the machine learning component acquires heuristics for locating the antecedents of relative pronouns <ref> [5, 6] </ref>.
Reference: 6. <author> C. Cardie. </author> <title> Learning to Disambiguate Relative Pronouns. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 38-43, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: In this task, the machine learning component acquires heuristics for locating the antecedents of relative pronouns <ref> [5, 6] </ref>.
Reference: 7. <author> C. Cardie. </author> <title> Using Decision Trees to Improve Case-Based Learning. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32, </pages> <institution> University of Massachusetts, </institution> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: of machine learning algorithms have been used as Kenmore's inductive learning component: a decision tree system (C4.5 [23]), a simple case-based learning algorithm (k-nearest neighbors), a conceptual clustering system (COBWEB [14]), and a hybrid inductive learning system that combines a nearest-neighbor algorithm with a decision tree system for feature selection <ref> [7] </ref>. In experiments to date, the hybrid algorithm has achieved the highest accuracies and we focus on these results below. Finally, all experiments use a 10-fold cross validation evaluation scheme. A more detailed description of the experiments and an analysis of the results can be found in [8]. Fig. 3.
Reference: 8. <author> C. Cardie. </author> <title> Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1994. </year> <note> Available as University of Massachusetts, CMPSCI Technical Report 94-74. </note>
Reference-contexts: Together, the context and solution portions of the case represent a single ambiguity resolution episode. (See <ref> [8] </ref> for a general discussion of how to convert both lexical and structural ambiguity problems into classification tasks.) To give a more concrete idea of what training cases look like, we show a portion of a sample training case for part-of-speech and word-sense disambiguation in Fig. 2. <p> Note, however, that Kenmore has been evaluated for additional language learning tasks along a number of dimensions <ref> [8] </ref> | this section summarizes just a subset of those experiments. In all experiments, training and test sentences were drawn from the MUC/- TIPSTER corpora [20, 21] and Kenmore was instantiated with the CIRCUS parser. <p> In experiments to date, the hybrid algorithm has achieved the highest accuracies and we focus on these results below. Finally, all experiments use a 10-fold cross validation evaluation scheme. A more detailed description of the experiments and an analysis of the results can be found in <ref> [8] </ref>. Fig. 3. Kenmore application phase. 4.1 Lexical Ambiguity We tested Kenmore's ability to handle simultaneously two lexical ambiguity tasks: (1) part-of-speech ambiguity and (2) word sense ambiguity. There were 18 possible parts of speech including: noun, noun modifier, auxiliary, adverb, preposition, verb particle.
Reference: 9. <author> E. Charniak. </author> <title> Equations for Part-of-Speech Tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 784-789, </pages> <address> Washington, DC, 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: In addition, the part-of-speech tagging heuristics match the accuracy of those obtained using the trigram-based tagger of the UMass/Hughes MUC-5 system and are comparable to those achievable using probabilistic taggers which generally reach accuracies in the 95-96% range <ref> [9] </ref>. <p> In the following section, we summarize the advantages of using embedded machine learning components in an NLP system. 5 Advantages of Embedded Machine Learning Mechanisms for NLP Systems Corpus-based language acquisition methods recently have been the focus of much attention in the natural language processing community <ref> [4, 9, 12, 13, 28] </ref>. With very few exceptions, however, these systems treat language learning and knowledge acquisition in isolation from the actual natural language system that will use the acquired knowledge and in isolation from any specific natural language understanding task.
Reference: 10. <author> T. Chen, V. Soo, and A. Lin. </author> <title> Learning to Parse with Recurrent Neural Networks. </title> <booktitle> In Proceedings of European Conference on Machine Learning Workshop on Machine Learning and Text Analysis, </booktitle> <pages> pages 63-68, </pages> <year> 1993. </year>
Reference-contexts: Connectionist approaches to classification could also be used as Kenmore's inductive learning component. In fact, a number of existing systems successfully combine traditional parsing techniques and distributed connectionist modules for sentence analysis (e.g., <ref> [26, 27, 10] </ref>). Symbolic machine learning systems, however, provide at least two advantages.
Reference: 11. <author> N. Chinchor, L. Hirschman, and D. Lewis. </author> <title> Evaluating Message Understanding Systems: An Analysis of the Third Message Undestanding Conference (MUC-3). </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 409-449, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Although current natural language processing (NLP) systems cannot yet perform in-depth text understanding, they can read an arbitrary text and summarize its major events provided that those events fall within a particular domain of interest (e.g., stories about natural disasters or terrorist events) <ref> [11, 17] </ref>.
Reference: 12. <author> K. Church. </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1988. </year>
Reference-contexts: In the following section, we summarize the advantages of using embedded machine learning components in an NLP system. 5 Advantages of Embedded Machine Learning Mechanisms for NLP Systems Corpus-based language acquisition methods recently have been the focus of much attention in the natural language processing community <ref> [4, 9, 12, 13, 28] </ref>. With very few exceptions, however, these systems treat language learning and knowledge acquisition in isolation from the actual natural language system that will use the acquired knowledge and in isolation from any specific natural language understanding task.
Reference: 13. <author> W. Daelemans, G. Durieux, and S. Gillis. </author> <title> The Acquisition of Stress: A Data-Oriented Approach. </title> <journal> Computational Linguistics, </journal> <volume> 20(3) </volume> <pages> 421-451, </pages> <year> 1994. </year>
Reference-contexts: In the following section, we summarize the advantages of using embedded machine learning components in an NLP system. 5 Advantages of Embedded Machine Learning Mechanisms for NLP Systems Corpus-based language acquisition methods recently have been the focus of much attention in the natural language processing community <ref> [4, 9, 12, 13, 28] </ref>. With very few exceptions, however, these systems treat language learning and knowledge acquisition in isolation from the actual natural language system that will use the acquired knowledge and in isolation from any specific natural language understanding task. <p> In related work, symbolic machine learning algorithms, and case-based algorithms in particular, have also been found to work well for a number of low-level language acquisition tasks (e.g., stress acquisition <ref> [13] </ref> and grapheme-to-phoneme conversion [3]) and discourse-level tasks (e.g., anaphora resolution [2, 19] and segmentation [18]). This work provides additional evidence for the viability of a machine learning approach to natural language learning and knowledge acquisition for natural language processing systems.
Reference: 14. <author> D. Fisher. </author> <title> Knowledge Acquisition Via Incremental Conceptual Clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: A variety of machine learning algorithms have been used as Kenmore's inductive learning component: a decision tree system (C4.5 [23]), a simple case-based learning algorithm (k-nearest neighbors), a conceptual clustering system (COBWEB <ref> [14] </ref>), and a hybrid inductive learning system that combines a nearest-neighbor algorithm with a decision tree system for feature selection [7]. In experiments to date, the hybrid algorithm has achieved the highest accuracies and we focus on these results below. Finally, all experiments use a 10-fold cross validation evaluation scheme.
Reference: 15. <author> W. Lehnert. </author> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <editor> In J. Barnden and J. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <pages> pages 135-164. </pages> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ, </address> <year> 1990. </year>
Reference-contexts: In this example, Kenmore was instantiated with the CIRCUS conceptual sentence analyzer <ref> [15] </ref> as its parser and training sentences were taken from the TIPSTER business joint ventures (JV) corpus. 2 The training case of Fig. 2 contains 15 context features and two solution features and was generated in response to the word "parts" in the sentence: Daihatsu...has so far been in alliance with
Reference: 16. <author> W. Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peterson, F. Feng, C. Dolan, and S. Goldman. </author> <title> University of Massachusetts/Hughes: Description of the CIRCUS System as Used in MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-291, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When trained on a larger set of examples (see Table 2), the learned heuristics for semantic class tagging performed well enough to be used in the UMass/Hughes CIRCUS system that participated in the the MUC-5 performance evaluation of information extraction systems <ref> [16] </ref>. In addition, the part-of-speech tagging heuristics match the accuracy of those obtained using the trigram-based tagger of the UMass/Hughes MUC-5 system and are comparable to those achievable using probabilistic taggers which generally reach accuracies in the 95-96% range [9].
Reference: 17. <author> W. Lehnert and B. Sundheim. </author> <title> A performance evaluation of text analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12(3) </volume> <pages> 81-94, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Although current natural language processing (NLP) systems cannot yet perform in-depth text understanding, they can read an arbitrary text and summarize its major events provided that those events fall within a particular domain of interest (e.g., stories about natural disasters or terrorist events) <ref> [11, 17] </ref>.
Reference: 18. <author> Diane J. Litman and Rebecca J. Passonneau. </author> <title> Combining Multiple Knowledge Sources for Discourse Segmentation. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <pages> pages 108-115. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1995. </year>
Reference-contexts: In related work, symbolic machine learning algorithms, and case-based algorithms in particular, have also been found to work well for a number of low-level language acquisition tasks (e.g., stress acquisition [13] and grapheme-to-phoneme conversion [3]) and discourse-level tasks (e.g., anaphora resolution [2, 19] and segmentation <ref> [18] </ref>). This work provides additional evidence for the viability of a machine learning approach to natural language learning and knowledge acquisition for natural language processing systems.
Reference: 19. <author> Joseph F. McCarthy and Wendy G. Lehnert. </author> <title> Using Decision Trees for Corefer-ence Resolution. </title> <editor> In C. Mellish, editor, </editor> <booktitle> Proceedings of the Fourteenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 1050-1055, </pages> <year> 1995. </year>
Reference-contexts: In related work, symbolic machine learning algorithms, and case-based algorithms in particular, have also been found to work well for a number of low-level language acquisition tasks (e.g., stress acquisition [13] and grapheme-to-phoneme conversion [3]) and discourse-level tasks (e.g., anaphora resolution <ref> [2, 19] </ref> and segmentation [18]). This work provides additional evidence for the viability of a machine learning approach to natural language learning and knowledge acquisition for natural language processing systems.
Reference: 20. <editor> Proceedings of the Third Message Understanding Conference (MUC-3). </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Note, however, that Kenmore has been evaluated for additional language learning tasks along a number of dimensions [8] | this section summarizes just a subset of those experiments. In all experiments, training and test sentences were drawn from the MUC/- TIPSTER corpora <ref> [20, 21] </ref> and Kenmore was instantiated with the CIRCUS parser.
Reference: 21. <editor> Proceedings of the Fifth Message Understanding Conference (MUC-5). </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Very generally, NLP systems for information extraction process a collection of texts, search for all information related to a predefined domain of interest, and then produce for 2 The JV corpus contains over 1000 documents that describe world-wide activity in the area of joint ventures or "tie-ups" between businesses <ref> [21] </ref>. Unless otherwise noted, all examples are derived from sentences in this corpus. Fig. 2. Training case for "parts": part-of-speech and word-sense disambiguation. (Ar--rows indicate which parts of the sentence are responsible for which attribute-value pairs.) each text a summary of this domain-relevant information in a rigid template format. <p> Note, however, that Kenmore has been evaluated for additional language learning tasks along a number of dimensions [8] | this section summarizes just a subset of those experiments. In all experiments, training and test sentences were drawn from the MUC/- TIPSTER corpora <ref> [20, 21] </ref> and Kenmore was instantiated with the CIRCUS parser.
Reference: 22. <author> J. R. Quinlan. </author> <title> Learning Logical Definitions from Relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Some features are better described as relations rather than attribute-value pairs. Using relational learning systems (e.g., FOIL <ref> [22] </ref>) will be important here. While most standard machine learning algorithms handle a subset of the listed problems, it proved difficult to find a single system that simultaneously addressed all of them for the NL datasets.
Reference: 23. <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: In all experiments, training and test sentences were drawn from the MUC/- TIPSTER corpora [20, 21] and Kenmore was instantiated with the CIRCUS parser. A variety of machine learning algorithms have been used as Kenmore's inductive learning component: a decision tree system (C4.5 <ref> [23] </ref>), a simple case-based learning algorithm (k-nearest neighbors), a conceptual clustering system (COBWEB [14]), and a hybrid inductive learning system that combines a nearest-neighbor algorithm with a decision tree system for feature selection [7].
Reference: 24. <author> P. Utgoff. </author> <title> An Improved Algorithm for Incremental Induction of Decision Trees. </title> <editor> In W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 318-325, </pages> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In fact, a number of existing systems successfully combine traditional parsing techniques and distributed connectionist modules for sentence analysis (e.g., [26, 27, 10]). Symbolic machine learning systems, however, provide at least two advantages. First, there are many incremental symbolic machine learning algorithms (e.g., ID5 <ref> [24] </ref>, instance-based learning methods [1]) that would allow the training phase to become progressively easier for the human supervisor: Kenmore can access the existing concept description to suggest solutions to each ambiguity and rely on the supervisor only to override incorrect predictions.
Reference: 25. <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <year> 1967. </year>
Reference-contexts: Hidden Markov Models (HMM) trained using the Viterbi algorithm <ref> [25] </ref> on the same data as the Kenmore exper-iments above (Fig. 1, for example, achieved accuracies of 89.0% correct (bigram model) and 73.8% versus Kenmore's 95.0% for the part-of-speech tagging task.
Reference: 26. <author> S. Wermter. </author> <title> Combining Symbolic and Connectionist Techniques for Coordination in Natural Language. </title> <booktitle> In Proceedings of the 14th German Workshop on Artificial Intelligence, </booktitle> <address> Eringerfeld, Germany, </address> <year> 1990. </year>
Reference-contexts: Connectionist approaches to classification could also be used as Kenmore's inductive learning component. In fact, a number of existing systems successfully combine traditional parsing techniques and distributed connectionist modules for sentence analysis (e.g., <ref> [26, 27, 10] </ref>). Symbolic machine learning systems, however, provide at least two advantages.
Reference: 27. <author> S. Wermter and W. Lehnert. </author> <title> A hybrid symbolic/connectionist model for noun-phrase understanding. </title> <journal> Connection Science, </journal> <volume> 1(3), </volume> <year> 1989. </year>
Reference-contexts: Connectionist approaches to classification could also be used as Kenmore's inductive learning component. In fact, a number of existing systems successfully combine traditional parsing techniques and distributed connectionist modules for sentence analysis (e.g., <ref> [26, 27, 10] </ref>). Symbolic machine learning systems, however, provide at least two advantages.
Reference: 28. <author> David Yarowsky. </author> <title> Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French. </title> <booktitle> In Proceedings of the 32th Annual Meeting of the ACL, </booktitle> <year> 1994. </year>
Reference-contexts: In the following section, we summarize the advantages of using embedded machine learning components in an NLP system. 5 Advantages of Embedded Machine Learning Mechanisms for NLP Systems Corpus-based language acquisition methods recently have been the focus of much attention in the natural language processing community <ref> [4, 9, 12, 13, 28] </ref>. With very few exceptions, however, these systems treat language learning and knowledge acquisition in isolation from the actual natural language system that will use the acquired knowledge and in isolation from any specific natural language understanding task.
References-found: 28

