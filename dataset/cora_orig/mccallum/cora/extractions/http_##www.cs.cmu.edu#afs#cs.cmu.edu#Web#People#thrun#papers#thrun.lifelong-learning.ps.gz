URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.lifelong-learning.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Phone: 3  
Title: Lifelong Robot Learning  
Author: Sebastian Thrun and Tom M. Mitchell 
Address: Romerstr. 164, 53117 Bonn, Germany  Pittsburgh, PA 15213, USA  
Affiliation: 2 University of Bonn, Institut fur Informatik III,  School of Computer Science, Carnegie Mellon University,  
Abstract: Learning provides a useful tool for the automatic design of autonomous robots. Recent research on learning robot control has predominantly focussed on learning single tasks that were studied in isolation. If robots encounter a multitude of control learning tasks over their entire lifetime, however, there is an opportunity to transfer knowledge between them. In order to do so, robots may learn the invariants of the individual tasks and environments. This task-independent knowledge can be employed to bias generalization when learning control, which reduces the need for real-world experimentation. We argue that knowledge transfer is essential if robots are to learn control with moderate learning times in complex scenarios. Two approaches to lifelong robot learning which both capture invariant knowledge about the robot and its environments are presented. Both approaches have been evaluated using a HERO-2000 mobile robot. Learning tasks included navigation in unknown indoor environments and a simple find-and-fetch task. 
Abstract-found: 1
Intro-found: 1
Reference: [ Atkeson, 1991 ] <author> Christopher A. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958-962, </pages> <address> Sacramento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Learning inductive function approximation bias. Another, more straightforward approach to learning and transferring knowledge is to learn the inductive bias of the function approximators used for learning control directly. Atkeson <ref> [ Atkeson, 1991 ] </ref> presents a scheme for learning distance measures for instance-based, local approximation schemes. In his algorithm, scaling factors are learned that allows to weight different input features differently.
Reference: [ Bachrach and Mozer, 1991 ] <author> Jonathan R. Bachrach and Michael C. Mozer. </author> <title> Connectionist modeling and control of finite state systems given partial state information. </title> <year> 1991. </year>
Reference-contexts: The robot 4 We intentionally avoid the complex problem of incomplete and noisy perception perception, since the algorithm presented in this section is kind of orthogonal to research on these issues. See <ref> [ Bachrach and Mozer, 1991 ] </ref> , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception.
Reference: [ Barto et al., 1989 ] <author> Andrew G. Barto, Richard S. Sutton, and Chris J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report COINS 89-95, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> MA, </address> <month> Septem-ber </month> <year> 1989. </year>
Reference-contexts: ] , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example <ref> [ Barto et al., 1989 ] </ref> , [ Jordan, 1989 ] , [ Munro, 1987 ] , [ Thrun, 1992 ] for more approaches to learning action models with neural networks. agent begins at the initial state s 1 and performs the action sequence a 1 ; a 2 ; :
Reference: [ Barto et al., 1990 ] <author> Andrew G. Barto, Richard S. Sutton, and Chris J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In M. Gabriel and J.W. Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pages 539-602, </pages> <address> Cambridge, Massachusetts, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In general, F i is difficult to learn directly. Following the ideas of reinforcement learning ( [ Samuel, 1959 ] , [ Sutton, 1988 ] , <ref> [ Barto et al., 1990 ] </ref> , [ Watkins, 1989 ] ), we decompose the problem of learning F i into the problem of learning an evaluation function, Q i , defined over states and actions.
Reference: [ Barto et al., 1991 ] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report COINS 91-57, </type> <institution> Department of Computer Science, University of Mas-sachusetts, </institution> <address> MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: be described in the following way: Control learning problem: hS; A; W; Ri ! F : S fl ! A such that F maximizes R over time This formulation of the robot control learning problem has been extensively studied in the field of Reinforcement Learning. [ Sutton, 1984 ] , <ref> [ Barto et al., 1991 ] </ref> . <p> If this is the case, it suffices to learn the policy as a function of the most recent sensation to action: F : S ! A, i.e., the control policy is purely reactive. As Barto et al. pointed out <ref> [ Barto et al., 1991 ] </ref> , the problem of learning a control policy can then be attacked by Dynamic Programming techniques [ Bellman, 1957 ] .
Reference: [ Bellman, 1957 ] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: As Barto et al. pointed out [ Barto et al., 1991 ] , the problem of learning a control policy can then be attacked by Dynamic Programming techniques <ref> [ Bellman, 1957 ] </ref> . As it turns out, even if robots have access to complete state descriptions of their environments, learning control in complex robot worlds with large state spaces is practically not feasible.
Reference: [ Brooks, 1989 ] <author> Rodney A. Brooks. </author> <title> A robot that walks; emergent behaviors from a carefully evolved network. </title> <booktitle> Neural Computation, </booktitle> <address> 1(2):253, </address> <year> 1989. </year>
Reference-contexts: The robot device must be precise enough to accurately execute plans that were generated using the internal models of the world. Recent research on autonomous robots has changed the design of autonomous agents and pointed out a promising direction for future research in robotics (see for example <ref> [ Brooks, 1989 ] </ref> and several papers in [ Maes, 1991 ] ). Reactivity and real-time operation have received considerably more attention than, for example, optimality and completeness.
Reference: [ Buhmann et al., to appear ] <author> Joachim Buhmann, Wolfram Burgard, Armin B. Cre-mers, Dieter Fox, Thomas Hofmann, Frank Schneider, Jiannis Strikos, and Se-bastian Thrun. </author> <title> The mobile robot Rhino. </title> <journal> AI Magazine, </journal> <note> 16(1), to appear. </note>
Reference-contexts: The approach to interpreting sonar sensors for building occupancy maps reported in Sect. 4 has been, with slight modifications, successfully employed in the University of Bonn's entry Rhino at the 1994 AAAI mobile robot competition <ref> [ Buhmann et al., to appear ] </ref> . Currently, maps are routinely built for large indoor areas.
Reference: [ Bylander, 1991 ] <author> Tom Bylander. </author> <title> Complexity results for planning. </title> <booktitle> In Proceedings of IJCAI-91, </booktitle> <pages> pages 274-279, </pages> <address> Darling Habour, Sydney, Australia, </address> <year> 1991. </year> <title> IJCAI, </title> <publisher> Inc. </publisher>
Reference-contexts: It was early recognized that many realistic robot domains are too complex to be handled efficiently [ Schwartz et al., 1987 ] , [ Canny, 1 This paper is also available as Technical Report IAI-TR-93-7, University of Bonn, Dept. of Computer Science III, March 1993. 1987 ] , <ref> [ Bylander, 1991 ] </ref> . Computational tractability turned out to be a severe ob-stacle for designing control structures for complex robots in complex domains, and robots were far from being reactive. * Precision bottleneck.
Reference: [ Canny, 1987 ] <author> John Canny. </author> <title> The Complexity of Robot Motion Planning. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference: [ Caruana, 1993 ] <author> Richard Caruana. </author> <title> Multitask learning: A knowledge-based of source of inductive bias. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 41-48, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In his approach, he gives hints to neural networks in form of additional output units that learn a closely related task. These hints constrain the internal representation developed by the network. In a more general way, Caruana <ref> [ Caruana, 1993 ] </ref> recently proposed to learn whole collections of tasks in parallel, using a shared internal representation. He conjectures that multi-task learning will make neural network learning algorithms scale to more complex learning tasks.
Reference: [ Chrisman, 1992 ] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinction approach. </title> <booktitle> In Proceedings of 1992 AAAI Conference, </booktitle> <address> Menlo Park, CA, July 1992. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: The robot 4 We intentionally avoid the complex problem of incomplete and noisy perception perception, since the algorithm presented in this section is kind of orthogonal to research on these issues. See [ Bachrach and Mozer, 1991 ] , <ref> [ Chrisman, 1992 ] </ref> , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et
Reference: [ Dayan and Hinton, 1993 ] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: He assumes that a human instructor teaches a robot a set of elemental behaviors which suffice for all tasks which the robot will face over its lifetime. Unlike Singh, his approach does not guarantee that optimal controller can be learned in the limit. Recently, Dayan and Hinton <ref> [ Dayan and Hinton, 1993 ] </ref> proposed a system that uses a pre-given hierarchical decomposition to learn control on different levels of abstraction. Each level of abstraction differs in the grain-size of the sensory information, resulting in differently specialized controllers.
Reference: [ DeJong and Mooney, 1986 ] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: In EBNN, the agent employs these action models to explain, analyze and generalize better from the observed episodes. This is done in the following three steps: 1. Explain. An explanation is a post-facto prediction of the observed state-action sequence <ref> [ DeJong and Mooney, 1986 ] </ref> , [ Mitchell et al., 1986 ] , [ Mitchell and Thrun, 1993a ] .
Reference: [ Elfes, 1987 ] <author> Alberto Elfes. </author> <title> Sonar-based real-world mapping and navigation. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(3):249-265, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: More specifically, consider a testing phase for the interpretation network R. For some of the testing examples, R will manage to predict the target value closely. For others, however, there will be a significant residual error for the 10 See [ Moravec, 1988 ] and <ref> [ Elfes, 1987 ] </ref> for related approaches to map building and robot navigation. San Jose, July 1992. The map reflects the knowledge of the robot after Stage 3 of the competition.
Reference: [ Jordan, 1989 ] <author> Michael I. Jordan. </author> <title> Generic constraints on underspecified target trajectories. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, </address> <year> 1989. </year> <institution> IEEE TAB Neural Network Committee. </institution>
Reference-contexts: [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , <ref> [ Jordan, 1989 ] </ref> , [ Munro, 1987 ] , [ Thrun, 1992 ] for more approaches to learning action models with neural networks. agent begins at the initial state s 1 and performs the action sequence a 1 ; a 2 ; : : : ; a n .
Reference: [ Kalman, 1960 ] <author> R.E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Trans. ASME, Journal of Basic Engineering, </journal> <volume> 82 </volume> <pages> 35-45, </pages> <year> 1960. </year>
Reference-contexts: In his algorithm, scaling factors are learned that allows to weight different input features differently. Sutton [ Sutton, 1992 ] reports a family of learning schemes that allow to learn inductive bias similar to Kalman filters <ref> [ Kalman, 1960 ] </ref> . Although he did not describe his methods in the context of learning control, his research has been motivated by transferring knowledge across multiple control learning tasks. 4. Learning representations. Representations, together with inductive bias, determine the way a function approximator generalizes from examples.
Reference: [ Kuipers and Byun, 1990 ] <author> Benjamin Kuipers and Yung-Tai Byun. </author> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Texas at Austin, Austin, Texas 78712, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The control learning problem is the problem of finding a control policy that generates actions such that the reward is maximized over time. [ Mahadevan and Connell, 1991 ] , [ Lin, 1992b ] , and <ref> [ Kuipers and Byun, 1990 ] </ref> . What exactly is the concrete problem addressed by robot learning? Let us outline a general definition of learning robot control. Assume the robot acts in an environment W (the world). Each time step, the environment is in a certain state z 2 Z.
Reference: [ Lin and Mitchell, 1992 ] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: The robot 4 We intentionally avoid the complex problem of incomplete and noisy perception perception, since the algorithm presented in this section is kind of orthogonal to research on these issues. See [ Bachrach and Mozer, 1991 ] , [ Chrisman, 1992 ] , <ref> [ Lin and Mitchell, 1992 ] </ref> , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [
Reference: [ Lin, 1992a ] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: The z axis plots the expected success of the grasp action, i.e., the probability that the grasping succeeds. with experience replay <ref> [ Lin, 1992a ] </ref> for learning control. without (left row) employing the action models and EBNN. In this initial stage of learning, when little data is yet available, the generalization bias from the pre-learned action models is apparent. <p> In his experiments he found a tremendous speedup in learning control when using these action models. EBNN differs from this approach in that it uses its action models for explaining real-world observations, and in that provides a mechanism to recover from errors in the action models. Lin <ref> [ Lin, 1992a ] </ref> describes a mechanism where past experience is memorized and repeatedly replayed when learning control. The collection of past experience forms a non-generalizing action model. Lin also reports significant speedups when using his replay mechanism.
Reference: [ Lin, 1992b ] <author> Long-Ji Lin. </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: A reward function that maps sensations to rewards measures the performance of the robot. The control learning problem is the problem of finding a control policy that generates actions such that the reward is maximized over time. [ Mahadevan and Connell, 1991 ] , <ref> [ Lin, 1992b ] </ref> , and [ Kuipers and Byun, 1990 ] . What exactly is the concrete problem addressed by robot learning? Let us outline a general definition of learning robot control. Assume the robot acts in an environment W (the world). <p> Sonar sensors return approximate echo distances. Such sensors are inexpensive but very noisy. and delivering laser printer output). As, for example, Singh [ Singh, 1992b ] and Lin <ref> [ Lin, 1992b ] </ref> demonstrate, learning related control tasks with increasing complexity can result in a remarkable synergy between these control learning tasks, and an improved problem solving power. <p> The high-level policy is learned in the abstract action space formed by the low level behaviors. In order to represent even the high-level controller as a purely reactive function, Singh makes several restricting assumptions on the type of tasks and sensor information. In his doctoral thesis, Lin <ref> [ Lin, 1992b ] </ref> describes a related scheme for learning behaviors, action hierarchies and abstraction. He assumes that a human instructor teaches a robot a set of elemental behaviors which suffice for all tasks which the robot will face over its lifetime.
Reference: [ Maes, 1991 ] <editor> Pattie Maes, editor. </editor> <title> Designing Autonomous Agents. </title> <publisher> The MIT Press (and Elsevier), </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Recent research on autonomous robots has changed the design of autonomous agents and pointed out a promising direction for future research in robotics (see for example [ Brooks, 1989 ] and several papers in <ref> [ Maes, 1991 ] </ref> ). Reactivity and real-time operation have received considerably more attention than, for example, optimality and completeness. Many approaches have dropped the assumption that perfect world knowledge is available-some systems even operate in the extreme where no domain-specific initial knowledge is available at all.
Reference: [ Mahadevan and Connell, 1991 ] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 328-332, </pages> <year> 1991. </year>
Reference-contexts: A reward function that maps sensations to rewards measures the performance of the robot. The control learning problem is the problem of finding a control policy that generates actions such that the reward is maximized over time. <ref> [ Mahadevan and Connell, 1991 ] </ref> , [ Lin, 1992b ] , and [ Kuipers and Byun, 1990 ] . What exactly is the concrete problem addressed by robot learning? Let us outline a general definition of learning robot control. Assume the robot acts in an environment W (the world).
Reference: [ Mel, 1989 ] <author> Bartlett W. Mel. Murphy: </author> <title> A neurally-inspired connectionist approach to learning and performance in vision-based robot motion planning. </title> <type> Technical Report CCSR-89-17A, </type> <institution> Center for Complex Systems Research Beckman Institute, University of Illinois, </institution> <year> 1989. </year>
Reference-contexts: Recent research has produced a variety of rigorous learning techniques that allow a robot to acquire huge chunks of knowledge by itself. See for example <ref> [ Mel, 1989 ] </ref> , [ Moore, 1990 ] , [ Pomerleau, 1989 ] , [ Tan, 1991 ] , state of its environment by its sensors, and change it using its effectors (actions). A reward function that maps sensations to rewards measures the performance of the robot.
Reference: [ Mitchell and Thrun, 1993a ] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Explanation based learning: A comparison of symbolic and neural network approaches. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 197-204, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is done in the following three steps: 1. Explain. An explanation is a post-facto prediction of the observed state-action sequence [ DeJong and Mooney, 1986 ] , [ Mitchell et al., 1986 ] , <ref> [ Mitchell and Thrun, 1993a ] </ref> . Starting with the initial state-action pair (s 1 ; a 1 ), the agent post-facto predicts subsequent states up to the final state s n using its neural network action models.
Reference: [ Mitchell and Thrun, 1993b ] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: EBNN can be applied to arbitrary reward functions. See <ref> [ Mitchell and Thrun, 1993b ] </ref> for more details. 7 Note that more sophisticated learning schemes for learning evaluation functions have been developed. In his dissertation, Watkins [ Watkins, 1989 ] describes Q-Learning, a scheme for learning evaluation function Q i (s k ; a k ) recursively. <p> A second extension, also used widely, is to discount reward over time. If actions are to be chosen such that the number of actions is minimal, reward is typically discounted with a discount factor fl 1. The resulting control policy consequently prefers sooner reward to more distant reward. See <ref> [ Mitchell and Thrun, 1993b ] </ref> or [ Thrun and Mitchell, 1993 ] for a more detailed description of these issues. hx 1 ; f (x 1 )i, hx 2 ; f (x 2 )i, and hx 3 ; f (x 3 )i are known. <p> and biases of a neural network such that both the value and the slope error are simultaneously minimized. 3.3 Accommodating Imperfect Action Models Initial experiments with EBNN on a simulated robot navigation task showed a significant speedup in learning when the robot agent had access to highly accurate action models <ref> [ Mitchell and Thrun, 1993b ] </ref> . If the action models are not sufficiently accurate, however, the robot performance can seriously suffer from the analysis. This is because the extracted slopes might very well be wrong and mislead generalization. <p> More specifically, in EBNN the step-size for weight updates is multiplied by the estimated slope accuracy when learning slopes. As illustrated elsewhere <ref> [ Mitchell and Thrun, 1993b ] </ref> , weighting slope training by their accuracies was found to successfully reduce the impact of malicious slopes resulting from inaccurate action models. We evaluated EBNN using nine different sets of action models that were trained with different amounts of training data. <p> The EBNN results in this section are initial. They are presented because they indicate that task-independent knowledge, once learned, can be successfully transferred by EBNN when learning the grasping task. They are also presented since they evaluated EBNN in a real robot domain, unlike the results presented in <ref> [ Mitchell and Thrun, 1993b ] </ref> , [ Thrun and Mitchell, 1993 ] . <p> In related experiments with a simulated robot presented elsewhere <ref> [ Mitchell and Thrun, 1993b ] </ref> , we observed a significant speed-up by a factor of 3 to 4 when pre-learned action models were employed. It is important to mention that we expect EBNN to be even more efficient if the dimension of the state space is higher.
Reference: [ Mitchell and Thrun, 1995 ] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Learning analytically and inductively. </title> <editor> In Steier and Mitchell, editors, </editor> <title> Mind Matters: A Tribute to Allen Newell. </title> <publisher> Lawrence Erlbaum Associates Publisher, </publisher> <year> 1995. </year>
Reference-contexts: Government or Siemens Corp. Addendum Since this paper was submitted, EBNN was successfully applied to a variety of real-world learning tasks. In <ref> [ Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a ] </ref> , result of applying EBNN to mobile robot navigation using the CMU Xavier robot are reported.
Reference: [ Mitchell et al., 1986 ] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: In EBNN, the agent employs these action models to explain, analyze and generalize better from the observed episodes. This is done in the following three steps: 1. Explain. An explanation is a post-facto prediction of the observed state-action sequence [ DeJong and Mooney, 1986 ] , <ref> [ Mitchell et al., 1986 ] </ref> , [ Mitchell and Thrun, 1993a ] . Starting with the initial state-action pair (s 1 ; a 1 ), the agent post-facto predicts subsequent states up to the final state s n using its neural network action models.
Reference: [ Mitchell et al., 1994 ] <author> Tom M. Mitchell, Joseph O'Sullivan, and Sebastian Thrun. </author> <title> Explanation-based learning for mobile robot perception. In Workshop on Robot Learning, </title> <booktitle> Eleventh Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: In [ Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a ] , result of applying EBNN to mobile robot navigation using the CMU Xavier robot are reported. EBNN has also been applied to robot perception <ref> [ Mitchell et al., 1994 ] </ref> , [ O'Sullivan et al., to appear ] , object recognition [ Thrun and Mitchell, 1994 ] and the game of chess [ Thrun, 1995b ] .
Reference: [ Moore, 1990 ] <author> Andrew W. Moore. </author> <title> Efficient Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <address> England, </address> <year> 1990. </year>
Reference-contexts: Recent research has produced a variety of rigorous learning techniques that allow a robot to acquire huge chunks of knowledge by itself. See for example [ Mel, 1989 ] , <ref> [ Moore, 1990 ] </ref> , [ Pomerleau, 1989 ] , [ Tan, 1991 ] , state of its environment by its sensors, and change it using its effectors (actions). A reward function that maps sensations to rewards measures the performance of the robot.
Reference: [ Moravec, 1988 ] <author> Hans P. Moravec. </author> <title> Sensor fusion in certainty grids for mobile robots. </title> <journal> AI Magazine, </journal> <pages> pages 61-74, </pages> <month> Summer </month> <year> 1988. </year>
Reference-contexts: More specifically, consider a testing phase for the interpretation network R. For some of the testing examples, R will manage to predict the target value closely. For others, however, there will be a significant residual error for the 10 See <ref> [ Moravec, 1988 ] </ref> and [ Elfes, 1987 ] for related approaches to map building and robot navigation. San Jose, July 1992. The map reflects the knowledge of the robot after Stage 3 of the competition.
Reference: [ Mozer and Bachrach, 1989 ] <author> Michael C. Mozer and Jonathan R. Bachrach. </author> <title> Discovering the structure of a reactive environment by exploration. </title> <type> Technical Report CU-CS-451-89, </type> <institution> Dept. of Computer Science, University of Colorado, Boulder, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: See [ Bachrach and Mozer, 1991 ] , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , <ref> [ Mozer and Bachrach, 1989 ] </ref> , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , [ Munro, 1987
Reference: [ Munro, 1987 ] <author> Paul Munro. </author> <title> A dual backpropagation scheme for scalar-reward learning. </title> <booktitle> In Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 165-176, </pages> <address> Hillsdale, NJ, 1987. </address> <publisher> Cognitive Science Society, Lawrence Erlbaum. </publisher>
Reference-contexts: ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , <ref> [ Munro, 1987 ] </ref> , [ Thrun, 1992 ] for more approaches to learning action models with neural networks. agent begins at the initial state s 1 and performs the action sequence a 1 ; a 2 ; : : : ; a n .
Reference: [ O'Sullivan et al., to appear ] <author> Joseph O'Sullivan, Tom M. Mitchell, and Sebastian Thrun. </author> <title> Explanation-based neural network learning from mobile robot perception. </title> <editor> In Katsushi Ikeuchi and Manuela Veloso, editors, </editor> <title> Symbolic Visual Learning. </title> <institution> Oxford University Press, </institution> <note> to appear. </note>
Reference-contexts: In [ Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a ] , result of applying EBNN to mobile robot navigation using the CMU Xavier robot are reported. EBNN has also been applied to robot perception [ Mitchell et al., 1994 ] , <ref> [ O'Sullivan et al., to appear ] </ref> , object recognition [ Thrun and Mitchell, 1994 ] and the game of chess [ Thrun, 1995b ] . In [ Thrun and Mitchell, 1994 ] , a definition of the lifelong learning problem in the context of supervised learning can be found.
Reference: [ Pomerleau, 1989 ] <author> Dean A. Pomerleau. ALVINN: </author> <title> an autonomous land vehicle in a neural network. </title> <type> Technical Report CMU-CS-89-107, </type> <institution> Computer Science Dept. Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1989. </year>
Reference-contexts: Recent research has produced a variety of rigorous learning techniques that allow a robot to acquire huge chunks of knowledge by itself. See for example [ Mel, 1989 ] , [ Moore, 1990 ] , <ref> [ Pomerleau, 1989 ] </ref> , [ Tan, 1991 ] , state of its environment by its sensors, and change it using its effectors (actions). A reward function that maps sensations to rewards measures the performance of the robot.
Reference: [ Pratt, 1993 ] <author> Lori Y. Pratt. </author> <title> Discriminability-based transfer between neural networks. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning representations. Representations, together with inductive bias, determine the way a function approximator generalizes from examples. Many researchers have focussed on learning appropriate representations in order to learn bias. For example, Pratt <ref> [ Pratt, 1993 ] </ref> describes several approaches that allow to re-use learned representations in hidden units of neural networks.
Reference: [ Rivest and Schapire, 1987 ] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <booktitle> In Proceedings of Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference-contexts: See [ Bachrach and Mozer, 1991 ] , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , <ref> [ Rivest and Schapire, 1987 ] </ref> , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , [ Munro, 1987 ] , [ Thrun, 1992 ] for
Reference: [ Rumelhart et al., 1986 ] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: As pointed out in the previous section, learning control reduces to learning a reactive controller 4 . If the environment is sufficiently predictable, neural network learning techniques such as the Back-propagation training procedure <ref> [ Rumelhart et al., 1986 ] </ref> can be used to model the environment.
Reference: [ Samuel, 1959 ] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on research and development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: Each individual control learning problem requires a different policy, i.e., learning a control function F i : S ! A that, when employed by the agent, maximizes the corresponding reward R i . In general, F i is difficult to learn directly. Following the ideas of reinforcement learning ( <ref> [ Samuel, 1959 ] </ref> , [ Sutton, 1988 ] , [ Barto et al., 1990 ] , [ Watkins, 1989 ] ), we decompose the problem of learning F i into the problem of learning an evaluation function, Q i , defined over states and actions.
Reference: [ Schwartz et al., 1987 ] <author> Jacob T. Schwartz, Micha Scharir, and John Hopcroft. </author> <title> Planning, Geometry and Complexity of Robot Motion. </title> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1987. </year>
Reference-contexts: Even if sufficiently detailed knowledge is available, making it computer-accessible,i.e., hand-coding explicit models of robot hardware, sensors and environments, has often been found to require unreasonable amounts of programming time. * Tractability bottleneck. It was early recognized that many realistic robot domains are too complex to be handled efficiently <ref> [ Schwartz et al., 1987 ] </ref> , [ Canny, 1 This paper is also available as Technical Report IAI-TR-93-7, University of Bonn, Dept. of Computer Science III, March 1993. 1987 ] , [ Bylander, 1991 ] .
Reference: [ Sharkey and Sharkey, 1992 ] <author> Noel E. Sharkey and Amanda J.C. Sharkey. </author> <title> Adaptive generalization and the transfer of knowledge. </title> <booktitle> In Proceedings of the Second Irish Neural Networks Conference, </booktitle> <address> Belfast, </address> <year> 1992. </year>
Reference-contexts: Although she could empirically demonstrate that this transfer could significantly reduce the number of training epochs required for the convergence of the Back-propagation algorithm, she only found occasional improvements in the generalization. A similar technique is reported by Sharkey and Sharkey <ref> [ Sharkey and Sharkey, 1992 ] </ref> . Some researchers have studied knowledge transfer if several tasks are learned simultaneously. For example, Suddarth and Kergosien [ Suddarth and Kergosien, 1990 ] demonstrated that multiple learning tasks of certain types can successfully guide and improve generalization.
Reference: [ Simard et al., 1992 ] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since this bias is knowledgeable, it will partially replace the need for real-world experimentation, hence accelerate learning. 8 As Simard and colleagues pointed out, the Back-propagation algorithm can be extended to fit target slopes as well as target values <ref> [ Simard et al., 1992 ] </ref> .
Reference: [ Singh, 1992a ] <author> Satinder P. Singh. </author> <title> The efficient learning of multiple task sequences. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: First, the number of behaviors is often smaller than the number of actions. Second, behaviors typically are selected for longer periods of time. The latter argument is usually more important and provides a stronger bias for learning control. Singh [ Singh, 1992b ] , <ref> [ Singh, 1992a ] </ref> reports a technique to learn complex tasks by first learning controllers for simple tasks based on reinforcement learning. These controllers represent reactive behaviors. The high-level policy is learned in the abstract action space formed by the low level behaviors.
Reference: [ Singh, 1992b ] <author> Satinder P. Singh. </author> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: Sonar sensors return approximate echo distances. Such sensors are inexpensive but very noisy. and delivering laser printer output). As, for example, Singh <ref> [ Singh, 1992b ] </ref> and Lin [ Lin, 1992b ] demonstrate, learning related control tasks with increasing complexity can result in a remarkable synergy between these control learning tasks, and an improved problem solving power. <p> First, the number of behaviors is often smaller than the number of actions. Second, behaviors typically are selected for longer periods of time. The latter argument is usually more important and provides a stronger bias for learning control. Singh <ref> [ Singh, 1992b ] </ref> , [ Singh, 1992a ] reports a technique to learn complex tasks by first learning controllers for simple tasks based on reinforcement learning. These controllers represent reactive behaviors. The high-level policy is learned in the abstract action space formed by the low level behaviors.
Reference: [ Suddarth and Kergosien, 1990 ] <author> Steven C. Suddarth and Y. L. Kergosien. </author> <title> Rule-injection hints as a means of improving network performance and learning time. </title> <booktitle> In Proceedings of the EURASIP Workshop on Neural Networks, </booktitle> <address> Sesimbra, Portugal, </address> <month> Feb </month> <year> 1990. </year> <month> EURASIP. </month>
Reference-contexts: A similar technique is reported by Sharkey and Sharkey [ Sharkey and Sharkey, 1992 ] . Some researchers have studied knowledge transfer if several tasks are learned simultaneously. For example, Suddarth and Kergosien <ref> [ Suddarth and Kergosien, 1990 ] </ref> demonstrated that multiple learning tasks of certain types can successfully guide and improve generalization. In his approach, he gives hints to neural networks in form of additional output units that learn a closely related task.
Reference: [ Sutton, 1984 ] <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: a control learning problem can be described in the following way: Control learning problem: hS; A; W; Ri ! F : S fl ! A such that F maximizes R over time This formulation of the robot control learning problem has been extensively studied in the field of Reinforcement Learning. <ref> [ Sutton, 1984 ] </ref> , [ Barto et al., 1991 ] .
Reference: [ Sutton, 1988 ] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: In general, F i is difficult to learn directly. Following the ideas of reinforcement learning ( [ Samuel, 1959 ] , <ref> [ Sutton, 1988 ] </ref> , [ Barto et al., 1990 ] , [ Watkins, 1989 ] ), we decompose the problem of learning F i into the problem of learning an evaluation function, Q i , defined over states and actions. <p> Indeed, in all our experiments we applied a linear combination of Watkins' recursive scheme and the non-recursive scheme described in the paper. This combination is strongly related to Sutton's T D () algorithms <ref> [ Sutton, 1988 ] </ref> . Since the exact procedure is not essential for the ideas presented in this paper, we will omit any details. A second extension, also used widely, is to discount reward over time. <p> After learning the action models, the six training episodes for the evaluation networks shown in Figure 7 were provided by a human teacher who controlled the robot. We applied a version of T D () <ref> [ Sutton, 1988 ] </ref> with = 0:7 and Watkins' Q-Learning [ Watkins, 1989 ] the angle and distance to the cup.
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Approaches that utilize action models differ in the type of action models they employ, and the way the action models are used to bias learning control. Sutton <ref> [ Sutton, 1990 ] </ref> presents a system that learns action models, like EBNN. He uses these models for synthesizing hypothetical experiences that refine the control policy. In his experiments he found a tremendous speedup in learning control when using these action models.
Reference: [ Sutton, 1992 ] <author> Richard S. Sutton. </author> <title> Adapting bias by gradient descent: An incremental version of delta-bar-delta. </title> <booktitle> In Proceeding of Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> pages 171-176, </pages> <address> Menlo Park, CA, July 1992. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: Atkeson [ Atkeson, 1991 ] presents a scheme for learning distance measures for instance-based, local approximation schemes. In his algorithm, scaling factors are learned that allows to weight different input features differently. Sutton <ref> [ Sutton, 1992 ] </ref> reports a family of learning schemes that allow to learn inductive bias similar to Kalman filters [ Kalman, 1960 ] .
Reference: [ Tan, 1991 ] <author> Ming Tan. </author> <title> Learning a cost-sensitive internal representation for reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 358-362, </pages> <year> 1991. </year>
Reference-contexts: Recent research has produced a variety of rigorous learning techniques that allow a robot to acquire huge chunks of knowledge by itself. See for example [ Mel, 1989 ] , [ Moore, 1990 ] , [ Pomerleau, 1989 ] , <ref> [ Tan, 1991 ] </ref> , state of its environment by its sensors, and change it using its effectors (actions). A reward function that maps sensations to rewards measures the performance of the robot. <p> See [ Bachrach and Mozer, 1991 ] , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , <ref> [ Tan, 1991 ] </ref> , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , [ Munro, 1987 ] , [ Thrun, 1992 ] for more approaches to learning action models with
Reference: [ Thrun and Mitchell, 1993 ] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <title> IJCAI, </title> <publisher> Inc. </publisher>
Reference-contexts: If actions are to be chosen such that the number of actions is minimal, reward is typically discounted with a discount factor fl 1. The resulting control policy consequently prefers sooner reward to more distant reward. See [ Mitchell and Thrun, 1993b ] or <ref> [ Thrun and Mitchell, 1993 ] </ref> for a more detailed description of these issues. hx 1 ; f (x 1 )i, hx 2 ; f (x 2 )i, and hx 3 ; f (x 3 )i are known. Based on these points the learner might generate the hypothesis g. <p> They are presented because they indicate that task-independent knowledge, once learned, can be successfully transferred by EBNN when learning the grasping task. They are also presented since they evaluated EBNN in a real robot domain, unlike the results presented in [ Mitchell and Thrun, 1993b ] , <ref> [ Thrun and Mitchell, 1993 ] </ref> . However, thus far we did not collect enough 1- 3+ 4 6 turn forward axis measures the angle of the cup, relative to the robot's body, and the vertical axis measures the distance to the cup in a logarithmic scale.
Reference: [ Thrun and Mitchell, 1994 ] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Learning one more thing. </title> <type> Technical Report CMU-CS-94-184, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: EBNN has also been applied to robot perception [ Mitchell et al., 1994 ] , [ O'Sullivan et al., to appear ] , object recognition <ref> [ Thrun and Mitchell, 1994 ] </ref> and the game of chess [ Thrun, 1995b ] . In [ Thrun and Mitchell, 1994 ] , a definition of the lifelong learning problem in the context of supervised learning can be found. <p> EBNN has also been applied to robot perception [ Mitchell et al., 1994 ] , [ O'Sullivan et al., to appear ] , object recognition <ref> [ Thrun and Mitchell, 1994 ] </ref> and the game of chess [ Thrun, 1995b ] . In [ Thrun and Mitchell, 1994 ] , a definition of the lifelong learning problem in the context of supervised learning can be found.
Reference: [ Thrun, 1992 ] <author> Sebastian Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <title> Handbook of intelligent control: neural, fuzzy and adaptive approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> Florence, Ken-tucky 41022, </address> <year> 1992. </year>
Reference-contexts: Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , [ Whitehead and Ballard, 1991 ] for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , [ Munro, 1987 ] , <ref> [ Thrun, 1992 ] </ref> for more approaches to learning action models with neural networks. agent begins at the initial state s 1 and performs the action sequence a 1 ; a 2 ; : : : ; a n .
Reference: [ Thrun, 1993 ] <author> Sebastian Thrun. </author> <title> Exploration and model building in mobile robot domains. </title> <booktitle> In Proceedings of the ICNN-93, </booktitle> <pages> pages 175-180, </pages> <address> San Francisco, CA, </address> <month> March </month> <year> 1993. </year> <institution> IEEE Neural Network Council. </institution>
Reference-contexts: In this section we will describe a neural network approach to learning the characteristics of the robot's sensors, as well as those of typical indoor environments. The task of the mobile HERO-2000 robot is to explore unknown buildings <ref> [ Thrun, 1993 ] </ref> . Facing a new indoor environment such as the laboratory environment depicted in Figure 9, the robot has to wander around and to use its sensors to avoid collisions. <p> The robot at hand, however, is not precise enough, and after 10 to 20 minutes of operation the real coordinated usually deviate significantly from the internal estimates. An approach to compensating such control errors is briefly described in <ref> [ Thrun, 1993 ] </ref> . (a) sensor interpretation 1 2 3 (b) confidence 1 2 3 - 10.5 feet interpretations and (b) confidence values are shown for the following examples: 1. hallway, 2. hallway with open door. 3. hallway with human walking by, 4. corner of a room, 5. corner with
Reference: [ Thrun, 1994 ] <author> Sebastian Thrun. </author> <title> A lifelong learning perspective for mobile robot control. </title> <booktitle> In Proceedings of the IEEE/RSJ/GI International Conference on Intelligent Robots and Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: Government or Siemens Corp. Addendum Since this paper was submitted, EBNN was successfully applied to a variety of real-world learning tasks. In <ref> [ Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a ] </ref> , result of applying EBNN to mobile robot navigation using the CMU Xavier robot are reported.
Reference: [ Thrun, 1995a ] <author> Sebastian Thrun. </author> <title> An approach to learning mobile robot navigation. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <year> 1995. </year> <note> (in press). </note>
Reference-contexts: Government or Siemens Corp. Addendum Since this paper was submitted, EBNN was successfully applied to a variety of real-world learning tasks. In <ref> [ Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a ] </ref> , result of applying EBNN to mobile robot navigation using the CMU Xavier robot are reported.
Reference: [ Thrun, 1995b ] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, </address> <year> 1995. </year> <note> Morgan Kaufmann. (to appear). </note>
Reference-contexts: EBNN has also been applied to robot perception [ Mitchell et al., 1994 ] , [ O'Sullivan et al., to appear ] , object recognition [ Thrun and Mitchell, 1994 ] and the game of chess <ref> [ Thrun, 1995b ] </ref> . In [ Thrun and Mitchell, 1994 ] , a definition of the lifelong learning problem in the context of supervised learning can be found.
Reference: [ Watkins, 1989 ] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: In general, F i is difficult to learn directly. Following the ideas of reinforcement learning ( [ Samuel, 1959 ] , [ Sutton, 1988 ] , [ Barto et al., 1990 ] , <ref> [ Watkins, 1989 ] </ref> ), we decompose the problem of learning F i into the problem of learning an evaluation function, Q i , defined over states and actions. <p> EBNN can be applied to arbitrary reward functions. See [ Mitchell and Thrun, 1993b ] for more details. 7 Note that more sophisticated learning schemes for learning evaluation functions have been developed. In his dissertation, Watkins <ref> [ Watkins, 1989 ] </ref> describes Q-Learning, a scheme for learning evaluation function Q i (s k ; a k ) recursively. <p> After learning the action models, the six training episodes for the evaluation networks shown in Figure 7 were provided by a human teacher who controlled the robot. We applied a version of T D () [ Sutton, 1988 ] with = 0:7 and Watkins' Q-Learning <ref> [ Watkins, 1989 ] </ref> the angle and distance to the cup. The z axis plots the expected success of the grasp action, i.e., the probability that the grasping succeeds. with experience replay [ Lin, 1992a ] for learning control. without (left row) employing the action models and EBNN.
Reference: [ Whitehead and Ballard, 1991 ] <author> Steven D. Whitehead and Dana H. Ballard. </author> <title> Learning to perceive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: See [ Bachrach and Mozer, 1991 ] , [ Chrisman, 1992 ] , [ Lin and Mitchell, 1992 ] , [ Mozer and Bachrach, 1989 ] , [ Rivest and Schapire, 1987 ] , [ Tan, 1991 ] , <ref> [ Whitehead and Ballard, 1991 ] </ref> for approaches to learning with incomplete perception. 5 See for example [ Barto et al., 1989 ] , [ Jordan, 1989 ] , [ Munro, 1987 ] , [ Thrun, 1992 ] for more approaches to learning action models with neural networks. agent begins at
References-found: 59

