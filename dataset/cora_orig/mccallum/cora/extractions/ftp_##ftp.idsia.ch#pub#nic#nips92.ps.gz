URL: ftp://ftp.idsia.ch/pub/nic/nips92.ps.gz
Refering-URL: http://www.cnl.salk.edu/~schraudo/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nici@cs.ucsd.edu  tsejnowski@ucsd.edu  
Title: Unsupervised Discrimination of Clustered Data via Optimization of Binary Information Gain  
Author: Nicol N. Schraudolph Terrence J. Sejnowski 
Address: La Jolla, CA 92093-0114  San Diego, CA 92186-5800  
Affiliation: Computer Science Engr. Dept. University of California, San Diego  Computational Neurobiology Laboratory The Salk Institute for Biological Studies  
Abstract: We present the information-theoretic derivation of a learning algorithm that clusters unlabelled data with linear discriminants. In contrast to methods that try to preserve information about the input patterns, we maximize the information gained from observing the output of robust binary discriminators implemented with sigmoid nodes. We derive a local weight adaptation rule via gradient ascent in this objective, demonstrate its dynamics on some simple data sets, relate our approach to previous work and suggest directions in which it may be extended.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. </author> <year> (1972). </year> <title> Logistic discrimination. </title> <journal> Biometrika, </journal> <volume> 59 </volume> <pages> 19-35. </pages>
Reference-contexts: To make this notion more precise, consider y a random variable with bimodal distribution, namely an even mixture of two Gaussian distributions. Then if their means equal half their variance, z is the posterior probability for discriminating between the two source distributions <ref> (Anderson, 1972) </ref>. This probabilitstic interpretation of z can be used to design a learning algorithm that seeks such bimodal projections of the input data. In particular, we search for highly informative discriminants by maximizing the information gained about the binary discrimination through observation of z.
Reference: <editor> Anderson, J. and Rosenfeld, E., editors (1988). Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Becker, S. and Hinton, G. E. </author> <year> (1992). </year> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355 </volume> <pages> 161-163. </pages>
Reference-contexts: This Infomax principle provides for optimal reconstruction of the input in the face of noise and resource limitations. The I-max algorithm <ref> (Becker and Hinton, 1992) </ref>, by contrast, focusses on coherent aspects of the input, which are extracted by maximizing the mutual information between networks looking at different patches of input.
Reference: <author> Bienenstock, E., Cooper, L., and Munro, P. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 2. </volume> <editor> Reprinted in (Anderson and Rosenfeld, </editor> <year> 1988). </year>
Reference: <author> Intrator, N. </author> <year> (1992). </year> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 98-107. </pages>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <booktitle> Computer, </booktitle> <pages> pages 105-117. </pages>
Reference-contexts: 1 INTRODUCTION Unsupervised learning algorithms may perform useful preprocessing functions by preserving some aspects of their input while discarding others. This can be quantified as maximization of the information the network's output carries about those aspects of the input that are deemed important. <ref> (Linsker, 1988) </ref> suggests maximal preservation of information about all aspects of the input. This Infomax principle provides for optimal reconstruction of the input in the face of noise and resource limitations.
Reference: <author> Munro, P. W. </author> <year> (1992). </year> <title> Visualizations of 2-d hidden unit space. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 468-473, </pages> <address> Baltimore 1992. </address> <publisher> IEEE. </publisher>
Reference: <author> Peterson, G. E. and Barney, H. L. </author> <year> (1952). </year> <title> Control methods used in a study of the vowels. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 24 </volume> <pages> 175-184. </pages>
Reference-contexts: To illustrate this effect, we have tested a single node running our algorithm on a set of vowel formant frequency data due to <ref> (Peterson and Barney, 1952) </ref>. <p> this figure, our algorithm is capable of proceeding smoothly from a small initial weight vector that responds in principal component direction to a solution which uses a large weight vector in near-orthogonal direction to successfully discriminate between the two data clusters. 3 ! ! set of 1514 multi-speaker vowel utterances <ref> (Peterson and Barney, 1952) </ref>. Superimposed on a scatter plot of the data are the pre-images of y = 0 (solid center line) and y = 1:31696 (flanking lines) in input space.
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference-contexts: The dynamic stability of our algorithm is a significant asset for its expansion into an efficient unsupervised multi-layer network. In such a network, linear estimators are no longer sufficient to fully remove redundancy between nodes. In his closely related predictability minimization architecture, <ref> (Schmidhuber, 1992) </ref> uses backpropagation networks as nonlinear estimators for this purpose with some success. Since the notion of estimator in our framework is completely general, it may combine evidence from multiple, disparate sources.
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 4 </volume> <pages> 303-321. 8 </pages>
Reference-contexts: The weight change in proportion to a difference in activity is reminiscent of the covariance rule <ref> (Sejnowski, 1977) </ref> but generalizes it in two important respects: * it explicitly incorporates a sigmoid nonlinearity, and * ^y need not necessarily be the average net input.
References-found: 10

