URL: http://www.cs.columbia.edu/robotics/publications/allen-tracking.ps.gz
Refering-URL: http://www.cs.columbia.edu/robotics/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Title: Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System  
Author: Peter K. Allen Aleksandar Timcenko Billibon Yoshimi Paul Michelman 
Note: This work was supported in part by DARPA contract N00039-84-C-0165, NSF grants DMC-86-05065, DCI-86-08845, CCR-86-12709, IRI-86-57151, IRI-88-1319, North American Philips Labo ratories, Siemens Corporation and Rockwell Inc.  
Date: November 26, 1991  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: Technical Report CUCS-034-91  
Abstract: Most robotic grasping tasks assume a stationary or fixed object. In this paper, we explore the requirements for tracking and grasping a moving object. The focus of our work is to achieve a high level of interaction between a real-time vision system capable of tracking moving objects in 3-D and a robot arm equipped with a dexterous hand that can be used pick up a moving object. We are interested in exploring the interplay of hand-eye coordination for dynamic grasping tasks such as grasping of parts on a moving conveyor system, assembly of articulated parts or for grasping from a mobile robotic system. Coordination between an organism's sensing modalities and motor control system is a hallmark of intelligent behavior, and we are pursuing the goal of building an integrated sensing and actuation system that can operate in dynamic as opposed to static environments. The system we have built addresses three distinct problems in robotic hand-eye coordination for grasping moving objects: fast computation of 3-D motion parameters from vision, predictive control of a moving robotic arm to track a moving object, and grasp planning. The system is able to operate at approximately human arm movement rates, and we present experimental results in which a moving model train is tracked, stably grasped, and picked up by the system. The algorithms we have developed that relate sensing to actuation are quite general and applicable to a variety of complex robotic tasks that require visual feedback for arm and hand control. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. </author> <title> Abramowitz, editor. Handbook of Mathematical Functions. </title> <institution> National Bureau of Standards, </institution> <year> 1964. </year>
Reference: [2] <author> E. H. Adelson and J. R. Bergen. </author> <title> Spatio-temporal energy models for the perception of motion. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2(2) </volume> <pages> 284-299, </pages> <year> 1985. </year> <month> 21 </month>
Reference-contexts: A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques [5, 11, 40], gradient based techniques [23, 33, 12] and spatio-temporal energy methods <ref> [21, 2] </ref>. Optic-flow was chosen as the primitive upon which to base the tracking algorithm for the following reasons: * The ability to track an object in three dimensions implies that there will be motion across the retinas (image planes) that are imaging the scene.
Reference: [3] <author> P. Allen. </author> <title> Real-time motion tracking using spatio-temporal filters. </title> <booktitle> In Proceedings of DARPA Image Understanding Workshop, </booktitle> <address> Palo Alto, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: This control information will be used to compute the Drive transform to correctly move the hand to intercept the object. We have implemented such a tracking system with a different robotic system <ref> [3] </ref> and can adapt this method to this particular task.
Reference: [4] <author> P. K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Real-time visual servoing. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1991. </year>
Reference-contexts: Some of the papers addressing this interesting problem are [15, 17, 47]. The majority of literature on the control problems encountered in motion tracking experiments is concerned with the problem of generating smooth, up-to-date trajectories from noisy and delayed outputs from different vision algorithms. Our previous work <ref> [4] </ref> coped with that problem in a similar way as in [39], using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by [34, 29, 6]. <p> However, the assumption the Kalman filter makes is that the noise applied to the system is white. That fact directly depends on the parametrization of the trajectory and, unfortunately in our case, the simplest possible parametrization Cartesian- does not support this noise model. Our previous work <ref> [4] </ref> used a variant of this approach and obtained tracking that was smooth but not accurate enough to allow actual grasping of the moving object. <p> Our initial experiments (described below) tracked a planar curve, allowing us to use this simplification. Motion in the Z direction is tracked with a Cartesian displacement as outlined in <ref> [4] </ref>. Our model assumes the following coordinate transformation that relates the moving object's coordinate frame at one instant with the next instant in time: Rot (z; 0 ) ffi Trans (x; s) ffi Trans (z; 4z) (6) where Rot and Trans are rotation about and translation along a given axis.
Reference: [5] <author> P. Anandan. </author> <title> Measuring visual motion from image sequences. </title> <type> Technical Report COINS TR-87-21, </type> <institution> COINS Dept., University of Massachusetts-Amherst, </institution> <year> 1987. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques <ref> [5, 11, 40] </ref>, gradient based techniques [23, 33, 12] and spatio-temporal energy methods [21, 2].
Reference: [6] <author> N. A. Andersen, O. Ravn, and A. T. Sorensen. </author> <title> Using vision in real-time control systems. </title> <booktitle> In American Control Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Our previous work [4] coped with that problem in a similar way as in [39], using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by <ref> [34, 29, 6] </ref>. In [34] a sophisticated control scheme is described which combines a Kalman filter's estimation and filtering power with an optimal (LQG) controller which computes the robot's motion. <p> Again, the estimation of moving object's position and orientation is done in the Cartesian space and a simple error model is assumed. Andersen et al. <ref> [6] </ref> adopts a 3rd-order Kalman filter in order to allow a robotic system (consisting of two degrees of freedom) to play the labyrinth game. A somewhat different approach has been explored in the work of Papanikolopoulos et al. [35], Houshangi [24] and Koivo et al. [27].
Reference: [7] <author> M. Arbib, T. Iberall, and D. Lyons. </author> <title> Coordinated control programs for movements of the hand. </title> <type> Technical Report COINS TR 83-25, </type> <institution> Dept. of CS University of Massachusetts, </institution> <month> August </month> <year> 1983. </year>
Reference-contexts: A coordinated motion is a combination of perceptual schemas and motor schemas (see Iberall and Arbib <ref> [7] </ref> ). Vision is used during the reaching phase of the task for what psychologists call "prospective control". Prospective control corresponds to predictive filtering, as used by control theorists.
Reference: [8] <author> Aspex. </author> <note> PIPE User's Manual. </note>
Reference-contexts: A second, iterative process is usually employed to propagate velocities in image neighborhoods, based upon a variety of smoothness and heuristic constraints. We have overcome the first of these problems by using the PIPE image processor <ref> [26, 8] </ref>.
Reference: [9] <author> C. Brown. </author> <title> Gaze controls with interaction delays. </title> <booktitle> Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 200-218, </pages> <month> May 23-26 </month> <year> 1989. </year>
Reference-contexts: Miller [32] has integrated a camera and arm for a tracking task where the emphasis is on learning kinematic and control parameters of the system. Weiss et al. [45] also use visual feedback to develop control laws for manipulation. Brown <ref> [9] </ref> has implemented a gaze control system that links a robotic "head" containing binocular cameras with a servo controller that allows one to maintain a fixed gaze on a moving object. Clark and Ferrier 3 [13] also have implemented a gaze control system for a mobile robot.
Reference: [10] <author> P. J. Burt, J. R. Bergen, R. Hingorani, R. Kolczynski, W. A. Lee, A. Leung, J. Lubin, and H. Shvayster. </author> <title> Object tracking with a moving camera. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 2-12, </pages> <address> Irvine, CA, </address> <month> March 20-22 </month> <year> 1988. </year>
Reference-contexts: We instead list some notable efforts that have inspired us or use similar approaches. Burt et al. <ref> [10] </ref> has focused on high-speed feature detection and hierarchical scaling of images in order to meet the real-time demands of surveillance and other robotic applications. Related work has been reported by Lee and Wohn [30] and Wiklund and Granlund [46] who use image differencing methods to track motion.
Reference: [11] <author> P. J. Burt, C. Yen, and X. Xu. </author> <title> Multi-resolution flow-through motion analysis. </title> <booktitle> In Proceedings of the IEEE CVPR Conference, </booktitle> <pages> pages 246-252, </pages> <year> 1983. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques <ref> [5, 11, 40] </ref>, gradient based techniques [23, 33, 12] and spatio-temporal energy methods [21, 2].
Reference: [12] <author> B. F. Buxton and H. Buxton. </author> <title> Computation of optic flow from the motion of edge features in image sequences. </title> <journal> Image and Vision Computing, </journal> <volume> 2, </volume> <year> 1984. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques [5, 11, 40], gradient based techniques <ref> [23, 33, 12] </ref> and spatio-temporal energy methods [21, 2].
Reference: [13] <author> J. J. Clark and N. J. Ferrier. </author> <title> Control of visual attention in mobile robots. </title> <booktitle> IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 826-831, </pages> <month> May 15-19, </month> <year> 1989. </year>
Reference-contexts: Brown [9] has implemented a gaze control system that links a robotic "head" containing binocular cameras with a servo controller that allows one to maintain a fixed gaze on a moving object. Clark and Ferrier 3 <ref> [13] </ref> also have implemented a gaze control system for a mobile robot. A variation of the tracking problems is the case of moving cameras. Some of the papers addressing this interesting problem are [15, 17, 47].
Reference: [14] <author> P. Corke, R. Paul, and K. Wohn. </author> <title> Video-rate visual servoing for sensory-based robotics. </title> <type> Technical report, </type> <institution> GRASP Laboratory, Department of Computer and Information Science, University of Pennsylvania, </institution> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: Related work has been reported by Lee and Wohn [30] and Wiklund and Granlund [46] who use image differencing methods to track motion. Corke, Paul and Wohn <ref> [14] </ref> report a feature-based tracking method that uses special purpose hardware to drive a servo controller of an arm-mounted camera. Goldenberg et al.[18] have developed a method that uses temporal filtering with vision hardware similar to our own.
Reference: [15] <author> P. J. B. et al. </author> <title> Object tracking with a moving camera. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1989. </year>
Reference-contexts: Clark and Ferrier 3 [13] also have implemented a gaze control system for a mobile robot. A variation of the tracking problems is the case of moving cameras. Some of the papers addressing this interesting problem are <ref> [15, 17, 47] </ref>. The majority of literature on the control problems encountered in motion tracking experiments is concerned with the problem of generating smooth, up-to-date trajectories from noisy and delayed outputs from different vision algorithms.
Reference: [16] <author> I. D. Faux and M. J. Pratt. </author> <title> Computational Geometry for Design and Manufacture. </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference: [17] <author> J. T. Feddema and C. S. G. Lee. </author> <title> Adaptive image feature prediction and control for visual tracking with a hand-eye coordinated camera. </title> <journal> IEEE Transaction on Systems, Man and Cybernetics, </journal> <volume> 20(5), </volume> <year> 1990. </year>
Reference-contexts: Clark and Ferrier 3 [13] also have implemented a gaze control system for a mobile robot. A variation of the tracking problems is the case of moving cameras. Some of the papers addressing this interesting problem are <ref> [15, 17, 47] </ref>. The majority of literature on the control problems encountered in motion tracking experiments is concerned with the problem of generating smooth, up-to-date trajectories from noisy and delayed outputs from different vision algorithms.
Reference: [18] <author> R. Goldenberg, W. C. Lau, A. She, and A. Waxman. </author> <title> Progress on the prototype pipe. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <address> Raleigh, N. C., </address> <month> March 31-April 3 </month> <year> 1987. </year>
Reference: [19] <author> H. H. H. Cruse, J. Dean and R. Schmidt. </author> <title> Utilization of sensory information for motor control. </title> <editor> In H. Heuer and A. F. Sanders, editors, </editor> <booktitle> Perspectives on Perception and Action, </booktitle> <pages> pages 43-79. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1987. </year>
Reference-contexts: In this section we briefly describe some relevant theories and their relation to our own work. There are several theories on the organization of skilled human motor control. Richard Schmidt <ref> [19] </ref> has proposed a theory of generalized motor programs, or movement schemas. In this view, a skilled action is composed of an ordered set of parametrized motor control programs of short duration (less than 200 msec), each of which accomplishes one part of the task.
Reference: [20] <author> V. Hayward and R. Paul. </author> <title> Robot manipulator control under unix. </title> <booktitle> In Proc. of the 13th ISIR, </booktitle> <pages> pages 20 </pages> <address> 32-20:44, Chicago, </address> <month> April 17-21 </month> <year> 1983. </year>
Reference-contexts: Therefore, we have implemented a fixed gain ff fi fl filter as part of the trajectory generator [39]. This filter provides a small amount of prediction to the trajectory parameters if the control signals from the host are delayed. We are using RCCL <ref> [20] </ref> to control the robotic arm (a PUMA 560). RCCL (Robot Control C Language) allows the use of C programming constructs to control the robot as well as defining transformation equations (as described in [36]).
Reference: [21] <author> D. Heeger. </author> <title> A model for extraction of image flow. </title> <booktitle> In First International Conference on Computer Vision, </booktitle> <address> London, </address> <year> 1987. </year> <month> 22 </month>
Reference-contexts: A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques [5, 11, 40], gradient based techniques [23, 33, 12] and spatio-temporal energy methods <ref> [21, 2] </ref>. Optic-flow was chosen as the primitive upon which to base the tracking algorithm for the following reasons: * The ability to track an object in three dimensions implies that there will be motion across the retinas (image planes) that are imaging the scene.
Reference: [22] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> M.I.T. Press, </publisher> <year> 1986. </year>
Reference-contexts: While this work does not specifically use these ideas, we have future plans to try to adapt this algorithm to such a framework. Our method begins with an implementation of the Horn-Schunck method of computing optic-flow <ref> [22] </ref>.
Reference: [23] <author> B. K. P. Horn and B. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1983. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques [5, 11, 40], gradient based techniques <ref> [23, 33, 12] </ref> and spatio-temporal energy methods [21, 2].
Reference: [24] <author> N. Houshangi. </author> <title> Control of a robotic manipulator to grasp a moving target using vision. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1990. </year>
Reference-contexts: Andersen et al. [6] adopts a 3rd-order Kalman filter in order to allow a robotic system (consisting of two degrees of freedom) to play the labyrinth game. A somewhat different approach has been explored in the work of Papanikolopoulos et al. [35], Houshangi <ref> [24] </ref> and Koivo et al. [27]. The auto-regressive (AR) and auto-regressive moving-average with exogenous input (ARMAX) models are investigated.
Reference: [25] <author> L. B. Jackson. </author> <title> Digital Filters and Signal Processing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: The smoothing filter we use to compute these signals is a moving-average (MA) filter using a Kaiser window <ref> [25] </ref>. This filter provides the largest ratio of signal energy in the main lobe and a side lobe, which usually results in a filter of lower order. <p> The windowing function is given by w K (n) = p I 0 (fi) where I 0 is the modified zeroth-order Bessel function, fi is the shape parameter which defines the width of the main lobe and M is the order of the filter. According to <ref> [25] </ref>, fi and M are given by M 14:364! fi = 0:1102 (A 8:7); A 50 where A is the stopband attenuation and 4! = (! r ! c )=! s , ! r is the stopband frequency, ! c is the passband frequency and ! s is the sampling frequency.
Reference: [26] <author> E. W. Kent, M. O. Shneier, and R. Lumia. </author> <title> Pipe: Pipelined image processing engine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (2):50-78, 1985. 
Reference-contexts: A second, iterative process is usually employed to propagate velocities in image neighborhoods, based upon a variety of smoothness and heuristic constraints. We have overcome the first of these problems by using the PIPE image processor <ref> [26, 8] </ref>.
Reference: [27] <author> A. J. Koivo and N. Houshangi. </author> <title> Real-time vision feedback for servoing robotic manipulator with self-tuning controller. </title> <journal> IEEE Transaction on Systems, Man and Cybernetics, </journal> <volume> 21(1), </volume> <year> 1991. </year>
Reference-contexts: Andersen et al. [6] adopts a 3rd-order Kalman filter in order to allow a robotic system (consisting of two degrees of freedom) to play the labyrinth game. A somewhat different approach has been explored in the work of Papanikolopoulos et al. [35], Houshangi [24] and Koivo et al. <ref> [27] </ref>. The auto-regressive (AR) and auto-regressive moving-average with exogenous input (ARMAX) models are investigated.
Reference: [28] <author> D. Lee, D. Young, P. Reddish, S. Lough, and T. Clayton. </author> <title> Visual timing in hitting an accelerating ball. </title> <journal> Quarterly Journal of Experimental Psychology, </journal> <volume> 35A:333-346, </volume> <year> 1983. </year>
Reference-contexts: There are two predominant theories about what visual schema is used to track a moving object and aid in predicting the intersection of the reaching hand and that object. Lee <ref> [28] </ref> proposes the use of vision to measure the expansion of the image on the retina in order to estimate the time until contact.
Reference: [29] <author> S. Lee and Y. Kay. </author> <title> An accurate estimation of 3d position and orientation of a moving object for robot stereo vision: Kalman filter approach. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1990. </year>
Reference-contexts: Our previous work [4] coped with that problem in a similar way as in [39], using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by <ref> [34, 29, 6] </ref>. In [34] a sophisticated control scheme is described which combines a Kalman filter's estimation and filtering power with an optimal (LQG) controller which computes the robot's motion. <p> The choice of gain matrices in the cost function and the best set of noise variances is done empirically. The work of Lee and Kay <ref> [29] </ref> addresses the problem of uncertainty of cameras in the robot's coordinate frame. The fact that cameras have to be strictly fixed in robot's frame might be quite annoying since each time they are (most often incidentally) displaced, one has to undertake a tedious job of their recalibration.
Reference: [30] <author> S. W. Lee and K. Wohn. </author> <title> Tracking moving objects by a mobile camera. </title> <type> Technical Report MS-CIS-88-97, </type> <institution> University of Pennsylvania, Department of Computer and Information Science, </institution> <address> Philadelphia, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Burt et al. [10] has focused on high-speed feature detection and hierarchical scaling of images in order to meet the real-time demands of surveillance and other robotic applications. Related work has been reported by Lee and Wohn <ref> [30] </ref> and Wiklund and Granlund [46] who use image differencing methods to track motion. Corke, Paul and Wohn [14] report a feature-based tracking method that uses special purpose hardware to drive a servo controller of an arm-mounted camera.
Reference: [31] <author> R. C. Luo, R. E. M. Jr., and D. E. Wessell. </author> <title> An adaptive robotic tracking system using optical flow. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 568-573, </pages> <address> Philadelphia, </address> <year> 1988. </year>
Reference-contexts: Corke, Paul and Wohn [14] report a feature-based tracking method that uses special purpose hardware to drive a servo controller of an arm-mounted camera. Goldenberg et al.[18] have developed a method that uses temporal filtering with vision hardware similar to our own. Luo, Mullen and Wessel <ref> [31] </ref> report a real-time implementation of motion tracking in 1-D based on Horn and Schunk's method. Verghese et al. [42] report real-time, short-range visual tracking of objects using a pipelined system similar to our own.
Reference: [32] <author> W. T. Miller. </author> <title> Real-time application of neural networks for sensor-based control of robots with vision. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 825-831, </pages> <month> July/Aug </month> <year> 1989. </year>
Reference-contexts: Safadi [38] uses a tracking filter similar to our own and a pyramid-based vision system, but few results are reported with this system. Rao and Durrant-Whyte [37] have implemented a Kalman filter-based de-centralized tracking system that tracks moving objects with multiple cameras. Miller <ref> [32] </ref> has integrated a camera and arm for a tracking task where the emphasis is on learning kinematic and control parameters of the system. Weiss et al. [45] also use visual feedback to develop control laws for manipulation.
Reference: [33] <author> H. H. Nagel. </author> <title> On the estimation of dense displacement vector fields from image sequences. </title> <booktitle> In Workshop on motion: Representation and Perception, </booktitle> <pages> pages 59-65, </pages> <address> Toronto, </address> <year> 1983. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques [5, 11, 40], gradient based techniques <ref> [23, 33, 12] </ref> and spatio-temporal energy methods [21, 2].
Reference: [34] <author> N. Papanikolopoulos, T. Kanade, and P. Khosla. </author> <title> Vision and control techniques for robotic visual tracking. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1991. </year>
Reference-contexts: Our previous work [4] coped with that problem in a similar way as in [39], using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by <ref> [34, 29, 6] </ref>. In [34] a sophisticated control scheme is described which combines a Kalman filter's estimation and filtering power with an optimal (LQG) controller which computes the robot's motion. <p> Our previous work [4] coped with that problem in a similar way as in [39], using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by [34, 29, 6]. In <ref> [34] </ref> a sophisticated control scheme is described which combines a Kalman filter's estimation and filtering power with an optimal (LQG) controller which computes the robot's motion.
Reference: [35] <author> N. Papanikolopoulos, P. K. Khosla, and T. Kanade. </author> <title> Adaptive robotic visual tracking. </title> <booktitle> In American Control Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Andersen et al. [6] adopts a 3rd-order Kalman filter in order to allow a robotic system (consisting of two degrees of freedom) to play the labyrinth game. A somewhat different approach has been explored in the work of Papanikolopoulos et al. <ref> [35] </ref>, Houshangi [24] and Koivo et al. [27]. The auto-regressive (AR) and auto-regressive moving-average with exogenous input (ARMAX) models are investigated. It is noteworthy to point out, as stated in [35], that this is more of an implementation than a conceptual difference from the classical Kalman 4 filter approach since the <p> A somewhat different approach has been explored in the work of Papanikolopoulos et al. <ref> [35] </ref>, Houshangi [24] and Koivo et al. [27]. The auto-regressive (AR) and auto-regressive moving-average with exogenous input (ARMAX) models are investigated. It is noteworthy to point out, as stated in [35], that this is more of an implementation than a conceptual difference from the classical Kalman 4 filter approach since the coefficients of polynomials in ARMAX model depend on the Kalman gains. 3 VISION SYSTEM In a visual tracking problem, motion in the imaging system has to be translated into 3-D
Reference: [36] <author> R. Paul. </author> <title> Robot Manipulators. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1981. </year>
Reference-contexts: We are using RCCL [20] to control the robotic arm (a PUMA 560). RCCL (Robot Control C Language) allows the use of C programming constructs to control the robot as well as defining transformation equations (as described in <ref> [36] </ref>).
Reference: [37] <author> B. S. Y. Rao and H. F. Durrant-Whyte. </author> <title> A fully decentralized algorithm for multi-sensor Kalman filtering. </title> <type> Technical Report OUEL 1787/89, </type> <institution> Dept. of Engineering Science, University of Oxford, </institution> <year> 1989. </year>
Reference-contexts: Verghese et al. [42] report real-time, short-range visual tracking of objects using a pipelined system similar to our own. Safadi [38] uses a tracking filter similar to our own and a pyramid-based vision system, but few results are reported with this system. Rao and Durrant-Whyte <ref> [37] </ref> have implemented a Kalman filter-based de-centralized tracking system that tracks moving objects with multiple cameras. Miller [32] has integrated a camera and arm for a tracking task where the emphasis is on learning kinematic and control parameters of the system.
Reference: [38] <author> R. B. Safadi. </author> <title> An adaptive algorithm for robotics and computer vision application. </title> <type> Technical Report MS-CIS-88-05, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Luo, Mullen and Wessel [31] report a real-time implementation of motion tracking in 1-D based on Horn and Schunk's method. Verghese et al. [42] report real-time, short-range visual tracking of objects using a pipelined system similar to our own. Safadi <ref> [38] </ref> uses a tracking filter similar to our own and a pyramid-based vision system, but few results are reported with this system. Rao and Durrant-Whyte [37] have implemented a Kalman filter-based de-centralized tracking system that tracks moving objects with multiple cameras.
Reference: [39] <author> R. B. Safadi. </author> <title> An adaptive tracking algorithm for robotics and computer vision application. </title> <type> Master's thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1988. </year>
Reference-contexts: The majority of literature on the control problems encountered in motion tracking experiments is concerned with the problem of generating smooth, up-to-date trajectories from noisy and delayed outputs from different vision algorithms. Our previous work [4] coped with that problem in a similar way as in <ref> [39] </ref>, using an ff fi fl filter, which is a form of a steady-state Kalman filter. A similar approach can be found in papers by [34, 29, 6]. <p> Therefore, we have implemented a fixed gain ff fi fl filter as part of the trajectory generator <ref> [39] </ref>. This filter provides a small amount of prediction to the trajectory parameters if the control signals from the host are delayed. We are using RCCL [20] to control the robotic arm (a PUMA 560).
Reference: [40] <author> G. L. Scott. </author> <title> Four-line method of locally estimating optic flow. </title> <journal> Image and Vision Computing, </journal> <volume> 5(2), </volume> <year> 1986. </year>
Reference-contexts: Our approach is to initially compute local optic-flow fields that measure image velocity at each pixel in the image. A variety of techniques for computing optic-flow fields have been used with varying results including matching based techniques <ref> [5, 11, 40] </ref>, gradient based techniques [23, 33, 12] and spatio-temporal energy methods [21, 2].
Reference: [41] <author> A. Singh. </author> <title> An estimation-theoretic framework for image-flow computation. </title> <booktitle> In Proc. International Conference on Computer Vision (ICCV-90), </booktitle> <address> Kyoto, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Hence, we needed to be able to compute image motion quickly and robustly. The Horn-Schunck optic-flow algorithm (described below) is well suited for real-time computation on our PIPE image processing engine. * We have developed a new framework for computing optic-flow robustly using an estimation-theoretic framework <ref> [41] </ref>. While this work does not specifically use these ideas, we have future plans to try to adapt this algorithm to such a framework. Our method begins with an implementation of the Horn-Schunck method of computing optic-flow [22].
Reference: [42] <author> G. Verghese, K. G. Lynch, and C. R. Dyer. </author> <title> Real-time motion tracking of three-dimensional objects. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> Cincinnati, </address> <month> May 13-18 </month> <year> 1990. </year>
Reference-contexts: Goldenberg et al.[18] have developed a method that uses temporal filtering with vision hardware similar to our own. Luo, Mullen and Wessel [31] report a real-time implementation of motion tracking in 1-D based on Horn and Schunk's method. Verghese et al. <ref> [42] </ref> report real-time, short-range visual tracking of objects using a pipelined system similar to our own. Safadi [38] uses a tracking filter similar to our own and a pyramid-based vision system, but few results are reported with this system.
Reference: [43] <editor> C. von Hofsten. Catching. In H. Heuer and A. F. Sanders, editors, </editor> <booktitle> Perspectives on Perception and Action, </booktitle> <pages> pages 33-36. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1987. </year>
Reference-contexts: Experimental evidence has shown that there is a window of approximately 14 msec during which the hand must begin closing. Unlike Schmidt, however, Von Hofsten does not consider vision and grasping to be two mutually exclusive tasks <ref> [43] </ref> Visual tracking is used to guide the reaching arm during its motion, not only before motion. A coordinated motion is a combination of perceptual schemas and motor schemas (see Iberall and Arbib [7] ).
Reference: [44] <author> C. von Hofsten. </author> <title> Early development of grasping an object in space-time. </title> <editor> In M. A. Goodale, editor, </editor> <booktitle> Vision and Action: The Control of Grasping, </booktitle> <pages> pages 65-79. </pages> <publisher> Ablex Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: The schema concept maps into Von Hofsten's ideas about the development of grasping skills in children <ref> [44] </ref> He believes there are two separate sensorimotor systems responsible for reaching: one for approaching the target and one for grasping it. During early childhood, the precise timing between these two systems develops as the child learns how to catch.
Reference: [45] <author> L. E. Weiss, A. Sanderson, and C. P. Neuman. </author> <title> Dynamic sensor-based control of robots with visual feedback. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(5):404-417, </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: Rao and Durrant-Whyte [37] have implemented a Kalman filter-based de-centralized tracking system that tracks moving objects with multiple cameras. Miller [32] has integrated a camera and arm for a tracking task where the emphasis is on learning kinematic and control parameters of the system. Weiss et al. <ref> [45] </ref> also use visual feedback to develop control laws for manipulation. Brown [9] has implemented a gaze control system that links a robotic "head" containing binocular cameras with a servo controller that allows one to maintain a fixed gaze on a moving object.
Reference: [46] <author> J. Wiklund and G. Granlund. </author> <title> Tracking of multiple moving objects. </title> <editor> In V. Cappelini, editor, </editor> <booktitle> Time Varying Image Processing and Moving Object Recognition, </booktitle> <pages> pages 241-249, </pages> <year> 1987. </year>
Reference-contexts: Burt et al. [10] has focused on high-speed feature detection and hierarchical scaling of images in order to meet the real-time demands of surveillance and other robotic applications. Related work has been reported by Lee and Wohn [30] and Wiklund and Granlund <ref> [46] </ref> who use image differencing methods to track motion. Corke, Paul and Wohn [14] report a feature-based tracking method that uses special purpose hardware to drive a servo controller of an arm-mounted camera.
Reference: [47] <author> M. Xie. </author> <title> Dynamic vision: Does 3d scene perception necessarily need two cameras or just one? Technical report, </title> <institution> Institut National de Recherche en Informatique et en Automatique, </institution> <year> 1989. </year>
Reference-contexts: Clark and Ferrier 3 [13] also have implemented a gaze control system for a mobile robot. A variation of the tracking problems is the case of moving cameras. Some of the papers addressing this interesting problem are <ref> [15, 17, 47] </ref>. The majority of literature on the control problems encountered in motion tracking experiments is concerned with the problem of generating smooth, up-to-date trajectories from noisy and delayed outputs from different vision algorithms.
References-found: 47

