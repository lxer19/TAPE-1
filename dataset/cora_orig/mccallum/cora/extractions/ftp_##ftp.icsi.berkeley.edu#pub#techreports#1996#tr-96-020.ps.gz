URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1996/tr-96-020.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1996.html
Root-URL: http://www.icsi.berkeley.edu
Title: Parallel Balanced Allocations a very simple optimal class of protocols achieving maximum load O q
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Volker Stemann m n log n. 
Note: n  log(m=n)  
Date: June 1996  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-96-020  
Abstract: We study the well known problem of throwing m balls into n bins. If each ball in the sequential game is allowed to select more than one bin, the maximum load of the bins can be exponentially reduced compared to the `classical balls into bins' game. We consider a static and a dynamic variant of a randomized parallel allocation where each ball can choose a constant number of bins. All results hold with high probability. In the static case all m balls arrive at the same time. We analyze for m = if r rounds of communication are allowed. This matches the lower bound of [ACMR95]. Furthermore, we generalize the protocols to the case of m &gt; n balls. An optimal load of O(m=n) can be achieved using log log n In the dynamic variant n of the m balls arrive at the same time and have to be allocated. Each of these initial n balls has a list of m=n successor-balls. As soon as a ball is allocated its successor will be processed. We present an optimal parallel process that allocates all m = n log n balls in O(m=n) rounds. Hence, the expected allocation time is constant. The main contribution of this process is that the maximum allocation time is additionally bounded by O(log log n). 
Abstract-found: 1
Intro-found: 1
Reference: [ABK95] <author> M. Adler, J.W. Byers, and R.M. Karp. </author> <title> Parallel sorting with limited bandwith. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference: [ABKU94] <author> Y. Azar, A. Z. Broder, A. R. Karlin, and E. Upfal. </author> <title> Balanced allocations. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 593-602, </pages> <year> 1994. </year>
Reference-contexts: The distribution of the balls in the bins is well known ([KSC78]). For m = n there exists a bin getting fi log log n balls with high probability (w.h.p.) 1 . Azar et al. <ref> [ABKU94] </ref> consider a modified sequential game where each ball chooses i.u.a.r. d 2 bins and is placed in the bin with the smaller load. They show a fi (log log n= log d) bound on the maximum load of the bins for this Greedy strategy, w.h.p.. <p> They show a fi (log log n= log d) bound on the maximum load of the bins for this Greedy strategy, w.h.p.. This game has many applications to computing problems, e.g. dynamic resource allocation, hashing, and competitive on-line load balancing (see <ref> [ABKU94] </ref>). In this paper we consider the parallel version of the 'balls into bins' game. In the parallel case the balls do not arrive sequentially, instead several balls arrive simultaneously and have to be placed into the bins. <p> Decker et al. [DDLM95] look at this problem and want to construct an algorithm which can be integrated in a distributed runtime system like PVM or MPI. In [DDLM95] they consider only the sequential process of Azar et al. <ref> [ABKU94] </ref>. If more than one task arrives at the same time, the Collision protocols and the parallel allocation process can be used to bound the maximum load and the maximum allocation time. 1.4 Organization In the next section we first introduce the class of Collision protocols.
Reference: [ACMR95] <author> M. Adler, S. Chakrabarti, M. Mitzenmacher, and L. Rasmussen. </author> <title> Parallel randomized load balancing. </title> <booktitle> In Proceedings of the 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 238-247, </pages> <year> 1995. </year>
Reference-contexts: Therefore, they can be viewed as generalizations of this special case in different directions. 1.1 Parallel protocols In the classical balls into bins game each of m balls is placed i.u.a.r. in one of n bins. We consider an extension of this model described by Adler et al. <ref> [ACMR95] </ref>. At the beginning each of the m balls chooses i.u.a.r. d bins. In this paper we focus on the case d = 2. The balls decide their final destination using r rounds of communication. A round of communication consists of two phases. <p> The simplicity of this protocol makes it very practical. Therefore, we analyze the constants in the number of rounds and in the maximum load. The Collision protocols can be reformulated such that they perform almost asynchronously. This answers the open question of Adler et al. <ref> [ACMR95] </ref> if there exists a simple protocol that achieves the same performance as their generalization of the Greedy protocol. Furthermore, we extend the k-Collision protocols to values m &gt; n. <p> Similar processes which try to achieve constant expected allocation time have also been studied in other contexts, e.g. shared memory simulations (see e.g. [DM93, LAB93]). 1.3 Applications Distributed load balancing Consider a scenario described by Adler et al. <ref> [ACMR95] </ref> where m client workstations issue jobs (balls) that have to be allocated at decentralized compute-servers. The clients are ignorant about other clients submitting jobs. The main goal is to minimize the maximum load at each server. Using a random strategy avoids the high cost of a global coordination. <p> Hence, for k and r in the stated bounds the theorem follows. 2 r can be bounded by two for r 2, Theorem 2.1 matches the lower bound of Adler et al. <ref> [ACMR95] </ref>. Interesting special cases of Theorem 2.1 are for constant k, i.e., we want to keep the maximum load in each bin as small as possible, and also for a constant number of rounds r. <p> = O (log log n= log log log n) the maximum load k of each bin and the number of rounds r of the k-Collision protocol are O (). 3 Optimal parallel processes In this section we extend the protocols described in the last section and by Adler et al. <ref> [ACMR95] </ref> to the case where each player wants to allocate more than one ball. Each of the n players has a list of t balls to be placed in the n bins (servers). <p> Extensions of this protocol in two different directions for different settings of the parameters have shown that the multiple 13 choices of the balls either effect the maximum load of a bin or the maximum allocation time of the balls. As mentioned in <ref> [ACMR95] </ref> a future direction would be to associate a weight to each ball and minimize the maximum weight over all bins. Also we believe that the natural process (log n, log n)-Allocation, or a slight extension, should achieve the same maximum allocation time as the more technical (log n)-Allocation-Phase process.
Reference: [CMS95] <editor> A. Czumaj, F. Meyer auf der Heide, and V. Stemann. </editor> <title> Improved optimal shared memory simulations, and the power of reconfiguration. </title> <booktitle> In Proceedings of the 3rd Israel Symposium on Theory of Computing and Systems, </booktitle> <pages> pages 11-19, </pages> <year> 1995. </year>
Reference-contexts: An extension of the majority technique due to Upfal and Wigderson [UW87] allows to restrict to the case that only one copy has to be allocated (see e.g. <ref> [CMS95] </ref>). Hence an m-processor PRAM can be simulated on an m-processor PRAM (n) with O (m=n + log log n) delay, w.h.p.. Universal dynamic mapping on MIMD machines Consider the problem of mapping dynamically generated tasks onto processors of a MIMD-system. <p> To analyze the k-Collision protocols we model the 'balls into bins' game as a game on a graph G = (V; E). A similar modeling is also used in <ref> [KLM92, GMR94, CMS95] </ref>. As we need this model also in the subsequent sections we define it for the general case m n. The bins are the nodes V of the graph G, jV j = n. The balls are represented by the edges E, jEj = m.
Reference: [DDLM95] <author> T. Decker, R. Diekmann, R. Lueling, and B. Monien. </author> <title> Towards developing universal dynamic mapping algorithms. </title> <booktitle> In Proceedings of the 7th Symposium on Parallel and Distributed Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Hence an m-processor PRAM can be simulated on an m-processor PRAM (n) with O (m=n + log log n) delay, w.h.p.. Universal dynamic mapping on MIMD machines Consider the problem of mapping dynamically generated tasks onto processors of a MIMD-system. Decker et al. <ref> [DDLM95] </ref> look at this problem and want to construct an algorithm which can be integrated in a distributed runtime system like PVM or MPI. In [DDLM95] they consider only the sequential process of Azar et al. [ABKU94]. <p> Universal dynamic mapping on MIMD machines Consider the problem of mapping dynamically generated tasks onto processors of a MIMD-system. Decker et al. <ref> [DDLM95] </ref> look at this problem and want to construct an algorithm which can be integrated in a distributed runtime system like PVM or MPI. In [DDLM95] they consider only the sequential process of Azar et al. [ABKU94].
Reference: [DM93] <editor> M. Dietzfelbinger and F. Meyer auf der Heide. </editor> <title> Simple, efficient shared memory simulations. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 110-119, </pages> <year> 1993. </year>
Reference-contexts: The main goal is to minimize the delivering time, i.e. the number of rounds to satisfy all requests ([KLM92, GMR94, MPR94, CMS95, MSS95]). Nevertheless, several ideas and techniques can be transferred to this problem. Especially, k-Collision protocols for m n and constant k are also studied in <ref> [DM93, GMR94, MPR94, MSS95] </ref>. They achieve the same bounds for this special setting of the parameters. 2 1.2 Parallel processes A parallel process can be viewed as having n players and n bins. Each player has a list of t balls to be allocated. <p> Within this class of processes we want to minimize the maximum allocation time. In a naive approach the players just try to allocate one ball after the other, choosing for each ball d = 1 bin i.u.a.r.. Aiello et al. [LAB93] and independently Dietzfelbinger and Meyer auf der Heide <ref> [DM93] </ref> show that for t = log n this Simple process is optimal. But the maximum allocation time is (log n= log log n). On the other hand gluing the Collision protocols together decreases the maximum allocation time to O (log log n) but the resulting process is not optimal. <p> It chooses d = 3 possible destinations i.u.a.r. for each ball and decreases the maximum allocation time exponentially. Similar processes which try to achieve constant expected allocation time have also been studied in other contexts, e.g. shared memory simulations (see e.g. <ref> [DM93, LAB93] </ref>). 1.3 Applications Distributed load balancing Consider a scenario described by Adler et al. [ACMR95] where m client workstations issue jobs (balls) that have to be allocated at decentralized compute-servers. The clients are ignorant about other clients submitting jobs. <p> In each round each bin accepts one ball from the queue. All players of successfully allocated balls become non-busy. The protocol stops after O (t ) rounds or when all balls are allocated. Aiello et al. [LAB93] and independently Dietzfelbinger and Meyer auf der Heide <ref> [DM93] </ref> show the following result. Lemma 3.1 For t = log n the t -Simple process allocates all balls within O (log n) rounds, w.h.p.. Hence, the t -Simple process is optimal but the maximum allocation can only be bounded by (log n= log log n). <p> A process similar to (log n; log n)-Allocation but with a stronger collision rule for the bins is analyzed in <ref> [DM93] </ref>. They split the players into a constant number of groups and handle the groups sequentially to achieve the stronger result. Moreover, one can only get an O (log n) upper bound for the maximum allocation time from their proof. <p> we need the following lemma: Lemma 3.2 For t fllog n the t Simple process allocates in O (t ) rounds all but n 2 t balls, w.h.p., for a sufficiently small constant fl. 11 For the proof we need a lemma that is implicitly stated in [LAB93] and also <ref> [DM93] </ref>. Lemma 3.3 A player has not allocated all its t balls after c 1 t rounds of the t Simple process with probability 1=2 2t for c 1 &gt; 8 and t log n.
Reference: [GMR94] <author> L. A. Goldberg, Y. Matias, and S. Rao. </author> <title> An optical simulation of shared memory. </title> <booktitle> In Proceedings of the 6th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 257-267, </pages> <year> 1994. </year>
Reference-contexts: The main goal is to minimize the delivering time, i.e. the number of rounds to satisfy all requests ([KLM92, GMR94, MPR94, CMS95, MSS95]). Nevertheless, several ideas and techniques can be transferred to this problem. Especially, k-Collision protocols for m n and constant k are also studied in <ref> [DM93, GMR94, MPR94, MSS95] </ref>. They achieve the same bounds for this special setting of the parameters. 2 1.2 Parallel processes A parallel process can be viewed as having n players and n bins. Each player has a list of t balls to be allocated. <p> To analyze the k-Collision protocols we model the 'balls into bins' game as a game on a graph G = (V; E). A similar modeling is also used in <ref> [KLM92, GMR94, CMS95] </ref>. As we need this model also in the subsequent sections we define it for the general case m n. The bins are the nodes V of the graph G, jV j = n. The balls are represented by the edges E, jEj = m.
Reference: [KLM92] <author> R. M. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient PRAM simulation on a distributed memory machine. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 318-326, </pages> <year> 1992. </year>
Reference-contexts: To analyze the k-Collision protocols we model the 'balls into bins' game as a game on a graph G = (V; E). A similar modeling is also used in <ref> [KLM92, GMR94, CMS95] </ref>. As we need this model also in the subsequent sections we define it for the general case m n. The bins are the nodes V of the graph G, jV j = n. The balls are represented by the edges E, jEj = m. <p> The following two technical lemmas are the basis of our proofs. The first lemma is already stated for the general case m n. A simpler version for m &lt; n is already included in <ref> [KLM92] </ref>. Lemma 2.2 Let G be a random graph with n nodes and m = n edges, 1 log n.
Reference: [KLM93] <author> R. M. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient PRAM simulation on a distributed memory machine. </title> <type> Technical report, </type> <institution> University of Paderborn, </institution> <month> September </month> <year> 1993. </year> <note> To appear in Algorithmica. A preliminary version is [KLM92]. 14 </note>
Reference-contexts: Let G = S r G r be the graph consisting of all m = n log log n edges. Using a modification of Lemma 2.2 for m &lt; n that already appeared in <ref> [KLM93] </ref> we get that G has connected components of size at most O (log n) and the connected components are trees with only a constant number of additional edges. <p> As in each step each bin allocates a ball, the size of each connected component decreases in each step by at least a half Therefore, a ball will be allocated in O (log log n) rounds (see also the proof of Theorem 6.5 in <ref> [KLM93] </ref>). 2 4 Conclusions We have studied parallel allocation strategies, i.e. parallel 'balls into bins' games. Especially, we have investigated the effects if each ball chooses not only one but a constant number of possible destinations where it can be allocated.
Reference: [KSC78] <author> V.F. Kolchin, B.A. Sevsatyanov, </author> <title> and V.P. Chistyakov. Random Allocation. </title> <publisher> V.H. Winston and Sons, </publisher> <year> 1978. </year>
Reference: [LAB93] <author> P. Liu, W. Aiello, and S. Bhatt. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 153-163, </pages> <year> 1993. </year>
Reference-contexts: Within this class of processes we want to minimize the maximum allocation time. In a naive approach the players just try to allocate one ball after the other, choosing for each ball d = 1 bin i.u.a.r.. Aiello et al. <ref> [LAB93] </ref> and independently Dietzfelbinger and Meyer auf der Heide [DM93] show that for t = log n this Simple process is optimal. But the maximum allocation time is (log n= log log n). <p> It chooses d = 3 possible destinations i.u.a.r. for each ball and decreases the maximum allocation time exponentially. Similar processes which try to achieve constant expected allocation time have also been studied in other contexts, e.g. shared memory simulations (see e.g. <ref> [DM93, LAB93] </ref>). 1.3 Applications Distributed load balancing Consider a scenario described by Adler et al. [ACMR95] where m client workstations issue jobs (balls) that have to be allocated at decentralized compute-servers. The clients are ignorant about other clients submitting jobs. <p> In each round each bin accepts one ball from the queue. All players of successfully allocated balls become non-busy. The protocol stops after O (t ) rounds or when all balls are allocated. Aiello et al. <ref> [LAB93] </ref> and independently Dietzfelbinger and Meyer auf der Heide [DM93] show the following result. Lemma 3.1 For t = log n the t -Simple process allocates all balls within O (log n) rounds, w.h.p.. <p> the running time we need the following lemma: Lemma 3.2 For t fllog n the t Simple process allocates in O (t ) rounds all but n 2 t balls, w.h.p., for a sufficiently small constant fl. 11 For the proof we need a lemma that is implicitly stated in <ref> [LAB93] </ref> and also [DM93]. Lemma 3.3 A player has not allocated all its t balls after c 1 t rounds of the t Simple process with probability 1=2 2t for c 1 &gt; 8 and t log n.
Reference: [McD89] <author> C. McDiarmid. </author> <title> On the method of bounded differences. </title> <editor> In J. Siemons, editor, </editor> <booktitle> Surveys in Combinatorics, London Mathematical Society Lecture Note Series 141, </booktitle> <pages> pages 148-188. </pages> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: Lemma 3.3 bounds the expected value of X = P n E (X) 2 2t : We want to bound the deviation from the expected value using a martingale tale estimate <ref> [McD89] </ref>. Let ! j , j = 1; : : :; t n, be the random choice made by ball b j . The ! j are independent and uniformly distributed over f1; : : : ; ng.
Reference: [MNV94] <author> Y. Mansour, N. Nisan, and U. Vishkin. </author> <title> Trade-offs between communication throughput and parallel time. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <year> 1994. </year>
Reference-contexts: We use a constant time simulation of a PRAM (n) on a module parallel computer (MPC) with n memory modules described in <ref> [MNV94] </ref>. c copies of each shared memory cells of the large PRAM are i.u.a.r. distributed among the modules using c random functions. The memory accesses to the shared memory can be viewed as a `balls into bins' game.
Reference: [MPR94] <author> P. D. MacKenzie, C. G. Plaxton, and R. Rajaraman. </author> <title> On contention resolution protocols and associated probabilistic phenomena. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 153-162, </pages> <year> 1994. </year>
Reference-contexts: The main goal is to minimize the delivering time, i.e. the number of rounds to satisfy all requests ([KLM92, GMR94, MPR94, CMS95, MSS95]). Nevertheless, several ideas and techniques can be transferred to this problem. Especially, k-Collision protocols for m n and constant k are also studied in <ref> [DM93, GMR94, MPR94, MSS95] </ref>. They achieve the same bounds for this special setting of the parameters. 2 1.2 Parallel processes A parallel process can be viewed as having n players and n bins. Each player has a list of t balls to be allocated.
Reference: [MSS95] <editor> F. Meyer auf der Heide, C. Scheideler, and V. Stemann. </editor> <title> Exploiting storage redundancy to speed up randomized shared memory simulations. </title> <booktitle> In Proceedings of the 12th Annual Symposium on Theoretical Aspects of Computer Science, </booktitle> <pages> pages 267-278, </pages> <year> 1995. </year>
Reference-contexts: The main goal is to minimize the delivering time, i.e. the number of rounds to satisfy all requests ([KLM92, GMR94, MPR94, CMS95, MSS95]). Nevertheless, several ideas and techniques can be transferred to this problem. Especially, k-Collision protocols for m n and constant k are also studied in <ref> [DM93, GMR94, MPR94, MSS95] </ref>. They achieve the same bounds for this special setting of the parameters. 2 1.2 Parallel processes A parallel process can be viewed as having n players and n bins. Each player has a list of t balls to be allocated.
Reference: [UW87] <author> E. Upfal and A. Wigderson. </author> <title> How to share memory in a distributed system. </title> <journal> Journal of the ACM, </journal> <volume> 34 </volume> <pages> 116-127, </pages> <year> 1987. </year> <month> 15 </month>
Reference-contexts: Each request of a processor is a ball thrown to the c random locations in the modules of the memory cell that it wants to access. An extension of the majority technique due to Upfal and Wigderson <ref> [UW87] </ref> allows to restrict to the case that only one copy has to be allocated (see e.g. [CMS95]). Hence an m-processor PRAM can be simulated on an m-processor PRAM (n) with O (m=n + log log n) delay, w.h.p..
References-found: 16

