URL: http://www.eecs.berkeley.edu/~ananth/NetworkCRC.ps.gz
Refering-URL: http://www.eecs.berkeley.edu/~ananth/
Root-URL: 
Email: E-mail: fsvenkat,ananthg@vyasa.eecs.berkeley.edu  
Title: The Common Randomness Capacity of a Network of Discrete Memoryless Channels  
Author: Sivarama Venkatesan Venkat Anantharam 
Keyword: Common randomness, interactive communication, strong con verse, spanning arborescences, polyhedral characterization.  
Address: Ithaca, NY 14853  Berkeley, CA 94720  
Affiliation: School of Electrical Engg. Cornell University  Dept. of EECS Univ. of California  
Abstract: In this paper, we generalize our previous results on generating common randomness at two terminals to a situation where any finite number of agents, interconnected by an arbitrary network of independent, point-to-point, discrete memoryless channels, wish to generate common randomness by interactive communication over the network. Our main result is an exact characterization of the common randomness capacity of such a network, i.e. the maximum number of bits of randomness that all the agents can agree on per step of communication. As a by-product, we also obtain a purely combinatorial result, viz., a characterization of (the incidence vectors of) the spanning arborescences rooted at a specified vertex in a digraph, and having exactly one edge exiting the root, as precisely the extreme points of a certain unbounded convex polyhedron, described by a system of linear inequalities. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ahlswede. </author> <title> Elimination of correlation in random codes for arbitrarily varying channels. </title> <journal> Z. Wahrsch. Verw. Gebiete, </journal> <volume> 33 </volume> <pages> 159-175, </pages> <year> 1978. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels <ref> [1] </ref>, [10], [8]. In the theory of identification over noisy channels [4], [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. <p> First note that H (S V ) = s l=1 l=1 1 ! 1 + = (1 ) log K (1 ) log (1 + ) 7 Here, the first inequality holds because z log z 0 for z 2 <ref> [0; 1] </ref>, and the second inequality is by (8). In (10), we have used 2 (0; 1). Now (9) implies P rfS u 6= S u 0 g for all u and u 0 . Hence H (S u 0 jS u ) 1 + log K, by Fano's inequality.
Reference: [2] <author> R. Ahlswede and I. Csiszar. </author> <title> Common randomness in information theory and cryptography, Part 1: Secret sharing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 4), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20]. Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers <ref> [2] </ref>, [18], [19]. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography. To begin with, in [2], they addressed the problem of generating common randomness at two terminals without giving information about it to an eavesdropper (secret <p> Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers <ref> [2] </ref>, [18], [19]. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography. To begin with, in [2], they addressed the problem of generating common randomness at two terminals without giving information about it to an eavesdropper (secret sharing) in various "source-type" and "channel-type" models, and proved bounds on the rates at which the two terminals could generate common randomness in these models. <p> Interestingly, the common randomness results obtained in [3] also led to new insights into the data transmission capacity of an arbitrarily varying channel (AVC) with feedback. In both <ref> [2] </ref> and [3], however, the possibility of exploiting channel noise to generate common randomness was not explored, except in the simple case of two terminals connected by a noisy discrete memoryless channel with noiseless feedback.
Reference: [3] <author> R. Ahlswede and I. Csiszar. </author> <title> Common randomness in information theory and cryptography, Part 2: </title> <journal> Cr capacity. IEEE Transactions on Information Theory, </journal> <volume> Vol. 44, </volume> <year> 1998. </year>
Reference-contexts: Later, in <ref> [3] </ref>, they studied the rates at which common randomness could be generated at two terminals without any secrecy requirements, but under various other resource constraints, such as access to side information and communication links. Interestingly, the common randomness results obtained in [3] also led to new insights into the data transmission <p> Later, in <ref> [3] </ref>, they studied the rates at which common randomness could be generated at two terminals without any secrecy requirements, but under various other resource constraints, such as access to side information and communication links. Interestingly, the common randomness results obtained in [3] also led to new insights into the data transmission capacity of an arbitrarily varying channel (AVC) with feedback. In both [2] and [3], however, the possibility of exploiting channel noise to generate common randomness was not explored, except in the simple case of two terminals connected by a noisy discrete <p> Interestingly, the common randomness results obtained in <ref> [3] </ref> also led to new insights into the data transmission capacity of an arbitrarily varying channel (AVC) with feedback. In both [2] and [3], however, the possibility of exploiting channel noise to generate common randomness was not explored, except in the simple case of two terminals connected by a noisy discrete memoryless channel with noiseless feedback.
Reference: [4] <author> R. Ahlswede and G. Dueck. </author> <title> Identification in the presence of feedback a discovery of new capacity formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], [10], [8]. In the theory of identification over noisy channels <ref> [4] </ref>, [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20].
Reference: [5] <author> R. Ahlswede and G. Dueck. </author> <title> Identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], [10], [8]. In the theory of identification over noisy channels [4], <ref> [5] </ref>, [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20].
Reference: [6] <author> R. Ahlswede and B. Verboven. </author> <title> On identification via multiway channels with feedback. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 37(No. 5), </volume> <month> Septem-ber </month> <year> 1991. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], [10], [8]. In the theory of identification over noisy channels [4], [5], <ref> [6] </ref>, [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20].
Reference: [7] <author> R. Ahlswede and Z. Zhang. </author> <title> New directions in the theory of identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41 </volume> <pages> 1040-1050, </pages> <year> 1995. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], [10], [8]. In the theory of identification over noisy channels [4], [5], [6], <ref> [7] </ref>, the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20].
Reference: [8] <author> D. Blackwell, L. Breiman, and A. Thomasian. </author> <title> The capacities of certain channel classes under random coding. </title> <journal> Ann. Math. Statist., </journal> <volume> 31 </volume> <pages> 558-567, </pages> <year> 1960. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], [10], <ref> [8] </ref>. In the theory of identification over noisy channels [4], [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20].
Reference: [9] <author> I. Csiszar and J. Korner. </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems. </title> <publisher> Academic Press, </publisher> <year> 1981. </year> <month> 42 </month>
Reference-contexts: The o (n) term does not depend on R 0 or P . Proof: This is a classical result in channel coding | see Theorem 5.2 on p. 165 of <ref> [9] </ref> for a proof. 2 Lemma 3.2 (Equipartition Lemma) Let W = (W (yjx) : (x; y) 2 X fi Y) be a finite-alphabet DMC. Suppose c 2 X n is of type P , and C Y n . <p> But first we will state a simple and useful result on the continuity of the entropy function. This is a weaker but more convenient form of the standard bound in <ref> [9] </ref>, Lemma 2.7 on p. 33.
Reference: [10] <author> I. Csiszar and P. Narayan. </author> <title> The capacity of the arbitrarily varying channel revisited: Positivity, constraints. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34 </volume> <pages> 181-193, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: For example, common randomness available to a transmitter and receiver allows them to use random codes for data transmission, which can outperform deterministic codes in certain situations, e.g., with arbitrarily varying channels [1], <ref> [10] </ref>, [8]. In the theory of identification over noisy channels [4], [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up.
Reference: [11] <author> L.R. Ford and D.R. Fulkerson. </author> <title> Flows in Networks. </title> <publisher> Princeton University Press, </publisher> <year> 1962. </year>
Reference-contexts: This will imply A 1 conv (A 1 ) + R ~ E Then, in Section 6.3, we will use Theorem 2.4 and the classical max-flow min-cut theorem <ref> [11] </ref>, to prove that A 1 has no extreme points other than the vectors in A 1 . <p> So, by (175) applied to U j and U j 0 , both ( ~ V U j ; U j ) and ( ~ V U j 0 ; U j 0 ) are min-cuts. But by <ref> [11] </ref>, Corollary I.5.4, this means that ( ~ V (U j [ U j 0 ); U j [ U j 0 ) is also a min-cut, which is the same as saying z (U j [ U j 0 ) ~ = 1.
Reference: [12] <author> D.R. Fulkerson. </author> <title> Blocking polyhedra. </title> <editor> In B. Harris, editor, </editor> <title> Graph Theory and its Applications. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: The elegant "duality" between (55) and (56) is a fundamental result in the theory of blocking polyhedra, developed by Fulkerson in <ref> [12] </ref> and [13]: Lemma 2.1 Let A and D be finite subsets of R n + , and let D and A be their respective "blockers": D = z 2 R n o A = ~ 2 R n o Then, A = conv (A) + R n + if and
Reference: [13] <author> D.R. Fulkerson. </author> <title> Blocking and anti-blocking pairs of polyhedra. </title> <journal> Mathematical Programming, </journal> <volume> 1 </volume> <pages> 168-194, </pages> <year> 1971. </year>
Reference-contexts: The elegant "duality" between (55) and (56) is a fundamental result in the theory of blocking polyhedra, developed by Fulkerson in [12] and <ref> [13] </ref>: Lemma 2.1 Let A and D be finite subsets of R n + , and let D and A be their respective "blockers": D = z 2 R n o A = ~ 2 R n o Then, A = conv (A) + R n + if and only if <p> of Farkas's lemma. 2 Denoting the polyhedron fz 2 R ~ E + : ~ z 1 for each ~ 2 A 1 g on the LHS of (56) by D 1 , we have D 1 = conv (D 1 ) + R ~ E In the terminology of <ref> [13] </ref>, A 1 and D 1 constitute a blocking pair of polyhedra, and the matrices whose rows are the vectors in D 1 and A 1 , respectively, constitute a blocking pair of matrices. <p> This blocking relation has many interesting consequences, one of which is the max-min equality we wish to prove (see <ref> [13] </ref> for others). Consider the linear program (LP) max T 2 ~ T 1 r T subject to r 2 P (c) (58) on the LHS of (47). We will now write down the dual to this LP.
Reference: [14] <author> D.R. Fulkerson. </author> <title> Packing rooted directed cuts in a weighted directed graph. </title> <journal> Mathematical Programming, </journal> <volume> 6 </volume> <pages> 1-13, </pages> <year> 1974. </year>
Reference-contexts: The simple proof given here for d = 1 does not seem to extend to higher values of d. Details of the statement and proof of the result for general d can be found in [22]. That result can be regarded as a generalization of Fulkerson's <ref> [14] </ref> well-known characterization of all the spanning arborescences rooted at a specified vertex of a digraph (no degree constraints). <p> This is similar to a well-known result of Fulkerson <ref> [14] </ref> that gives an analogous characterization of all the spanning arborescences in ~ G. 2.7 Combinatorial result For each T 2 ~ T , define an incidence vector ~(T ) 2 R ~ E + as follows: ~ e (T ) = 1 if e 2 T ; 0 otherwise. (48) <p> Consider the polyhedron conv (A) + R ~ E + , i.e., the set of vectors in R ~ E + that lie on or above the convex hull of the incidence vectors of all spanning arborescences in ~ G. An important and well-known result of Fulkerson <ref> [14] </ref> characterizes this polyhedron as the solution set of a certain system of linear inequalities: Theorem 2.4 (Fulkerson) Let A = ~ 2 R + : z ~ 1 for all z 2 D ; (50) where D is the finite set consisting of all the vectors z (U ) defined <p> v i 2 U i and z (U i ) ~ = 1: (175) We claim that z (U j [ U j 0 ) ~ = 1 if U j " U j 0 is non-empty: (176) The proof of this claim is by a neat trick adapted from <ref> [14] </ref>, which uses the max-flow min-cut theorem. Let u 2 U j "U j 0 . Suppose we think of ~ G as a flow network with source r and sink u, in which the capacity of edge e is ~ e .
Reference: [15] <author> R.L. Graham, M. Grotschel, and L. Lovasz. </author> <title> Handbook of Combinatorics. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Polyhedral characterizations of the above kind are useful in the study of linear 4 objective combinatorial optimization (LOCO) problems (see, e.g., [16] or Chapter 30 of <ref> [15] </ref>), since they allow such problems to be reduced to linear programs. Often a polyhedral characterization, in conjunction with the LP duality theorem, leads naturally to an efficient algorithm for solving an associated LOCO problem. The rest of the paper is organized as follows. <p> G that is equivalent to "C fl (P) = C fl (P) for every P" is max X r T = min X c e z e for every c 2 R ~ E In the terminology of polyhedral combinatorics, (47) is a "max-min equality" (see, e.g., Chapter 30 of <ref> [15] </ref>). It is clear that a proof of (47) would constitute a proof of Theorem 2.3 as well. Now (47) can, in turn, be regarded as a corollary of 14 another combinatorial result, which we state in Section 2.7. <p> Proof: See, e.g., Theorem 6.3 in Chapter 30 of <ref> [15] </ref>.
Reference: [16] <author> A. Hoffman. </author> <title> Polyhedral aspects of discrete optimization. </title> <journal> Annals of Discrete Mathematics, </journal> <pages> pages 183-190, </pages> <year> 1979. </year>
Reference-contexts: Polyhedral characterizations of the above kind are useful in the study of linear 4 objective combinatorial optimization (LOCO) problems (see, e.g., <ref> [16] </ref> or Chapter 30 of [15]), since they allow such problems to be reduced to linear programs. Often a polyhedral characterization, in conjunction with the LP duality theorem, leads naturally to an efficient algorithm for solving an associated LOCO problem. The rest of the paper is organized as follows.
Reference: [17] <author> L. Lovasz. </author> <title> Communication complexity: A survey. </title> <editor> In B.H. Korte et al., editors, </editor> <title> Paths, Flows and VLSI layout. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: In the theory of identification over noisy channels [4], [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations <ref> [17] </ref>, [20]. Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers [2], [18], [19]. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography.
Reference: [18] <author> U.M. Maurer. </author> <title> Perfect cryptographic security from partially independent channels. </title> <booktitle> Proc. of the 23rd Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20]. Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers [2], <ref> [18] </ref>, [19]. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography.
Reference: [19] <author> U.M. Maurer. </author> <title> Secret key agreement by public discussion from common information. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 3), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], [20]. Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers [2], [18], <ref> [19] </ref>. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography.
Reference: [20] <author> A. Orlitsky and A. El Gamal. </author> <title> Communication complexity. </title> <editor> In Y. Abu-Mostafa, editor, </editor> <booktitle> Complexity in Information Theory. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: In the theory of identification over noisy channels [4], [5], [6], [7], the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. Common randomness can also significantly reduce the communication complexity of certain distributed computations [17], <ref> [20] </ref>. Finally, secret common randomness available to a transmitter and receiver allows them to communicate securely over a channel with eavesdroppers [2], [18], [19]. With these considerations in mind, Ahlswede and Csiszar initiated a systematic study of the role of common randomness in information theory and cryptography.
Reference: [21] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley, </publisher> <year> 1986. </year>
Reference-contexts: The polyhedron A 1 can be decomposed as the vector sum of the convex hull of its extreme points, and the cone generated by its extreme directions. (Every polyhedron of non-negative vectors can be so decomposed; see, e.g., <ref> [21] </ref>.) Now, the cone generated by the extreme directions of A 1 equals all of R ~ E + because A 1 is unbounded along every co-ordinate direction: if ~ 2 A 1 and ~ 0 ~ then ~ 0 2 A 1 .
Reference: [22] <author> S. Venkatesan. </author> <title> Generating Common Randomness from Channel Noise: Capacity Formulas and Combinatorial Results. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: The simple proof given here for d = 1 does not seem to extend to higher values of d. Details of the statement and proof of the result for general d can be found in <ref> [22] </ref>. That result can be regarded as a generalization of Fulkerson's [14] well-known characterization of all the spanning arborescences rooted at a specified vertex of a digraph (no degree constraints).
Reference: [23] <author> S. Venkatesan and V. Anantharam. </author> <title> The common randomness capacity of a pair of independent discrete memoryless channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 44, </volume> <year> 1998. </year>
Reference-contexts: context is: what is the maximum rate, in bits per step of communication, at which Alice and Bob can generate common randomness from the noise on the two channels, i.e., what is the common randomness capacity of the pair of channels connecting them? This question was posed and answered in <ref> [23] </ref>, under the assumption that the two channels are independently operating discrete memoryless channels (DMC's). The main result of [23] is that Alice and Bob can generate max fmin [H (Y B jX B ); I (X A ; Y A )] + min [H (Y A jX A ); I <p> generate common randomness from the noise on the two channels, i.e., what is the common randomness capacity of the pair of channels connecting them? This question was posed and answered in <ref> [23] </ref>, under the assumption that the two channels are independently operating discrete memoryless channels (DMC's). The main result of [23] is that Alice and Bob can generate max fmin [H (Y B jX B ); I (X A ; Y A )] + min [H (Y A jX A ); I (X B ; Y B )]g (1) bits of common randomness per step of communication over such a pair <p> Moreover, the common randomness capacity equals its maximum of 1 bit/step whenever h (p) + h (q) = 1 h (p) + 1 h (q), i.e., whenever the entropies and capacities of the two channels balance each other. For details and other examples, see <ref> [23] </ref>. 1.2 Subject of this paper The results of [23] apply only to situations where common randomness is to be generated at two distant terminals. <p> For details and other examples, see <ref> [23] </ref>. 1.2 Subject of this paper The results of [23] apply only to situations where common randomness is to be generated at two distant terminals. In this paper, we study a much more general problem where any finite number of agents, interconnected by an arbitrary network of point-to-point channels, wish to generate common randomness. <p> It makes use of the following pair of pleasingly dual lemmas, which formed the basis for the achievability proof in <ref> [23] </ref> also: Lemma 3.1 (Equitype Code Lemma) Let W = (W (yjx) : (x; y) 2 X fi Y) be a finite-alphabet DMC. <p> The o (n) term does not depend on R 00 or P . 17 Proof: See Lemma 3.2 in <ref> [23] </ref>. 2 Suppose n 1 and P e is an n-type on the input alphabet X e of channel e. Fix an * &gt; 0.
Reference: [24] <author> S. Venkatesan and V. Anantharam. </author> <title> Identification plus transmission over channels with perfect feedback. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 44, </volume> <year> 1998. </year>
Reference-contexts: Lemma 5.2 (Continuity of the entropy function) If P 1 and P 2 are probability distributions on a finite set Z, and jP 1 (z) P 2 (z)j fi for all z 2 Z, then jH (P 1 ) H (P 2 )j 2jZj fi. Proof: See <ref> [24] </ref>. 2 Lemma 5.3 If is -typical, then jH (Q e ) H (P e W e )j (2jX e jjY e j) n 1=4 (129) fi fi fi X e (a e ; b e ) log W e (b e ja e ) + H (W e jP e
Reference: [25] <author> J. Wolfowitz. </author> <title> Coding Theorems of Information Theory. </title> <publisher> Springer Verlag, </publisher> <year> 1978. </year> <month> 43 </month>
Reference-contexts: As is to be expected, its proof is also much harder, requiring an elaborate "typical sequence" machinery for dealing with interactive communication over a network of DMC's. However, such strong converses greatly strengthen the significance of coding theorems in information theory, as argued by Wolfowitz <ref> [25] </ref>, and it is therefore desirable that they be proved wherever possible. We provide separate proofs for (16) and (18), the weak and strong converses. Although (16) is clearly implied by (18), its proof is much simpler, and based on more conventional techniques, which is why we include it here.
References-found: 25

