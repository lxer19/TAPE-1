URL: http://polaris.cs.uiuc.edu/reports/1479.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Chun Xia 
Date: 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Clark [15] reported a lower performance of the VAX-11/780 cache when OS activities were taken into account. Similarly, Agarwal et al <ref> [1] </ref> pointed out that many cache misses are caused by OS. Torrellas et al [41] reported that OS causes a large fraction of the cache misses in a bus-based shared-memory multiprocessor.
Reference: [2] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: The loop locality is the combined effect of both spatial locality and temporal locality in loops. The spatial locality exists in the sequence of the loop body, while the temporal locality exhibits in many iterations of the loop. To identify the loops, we use dataflow analysis <ref> [2] </ref>. For our analysis, we divide the loops into those that do not call procedures and those that do. We now consider each category in turn. Loops Without Procedure Calls Our measurements show that these loops do not dominate the execution time of OS. <p> Filling 1 Kbyte of each logical cache with seldom-executed code is not difficult given that there is abundant seldom-executed code in OS. to top within a cache-sized chunk and then from left to right. 4.1.3 Exploiting Loop Locality Loops are identified using dataflow analysis as discussed by Aho et al <ref> [2] </ref>. Recall that loop locality is the combined effect of spatial locality and temporal locality. Whenever we place the loop body in sequence, the loop locality is exposed implicitly. Therefore, 40 we are able to eliminate deterministic conflict misses in loops. For this reason, we treat loops as normal sequences.
Reference: [3] <author> T. Alexander and G. Kedem. </author> <title> Distributed Prefetch-buffer/Cache Design for High Performance Memory Systems. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> To address the problem of irregular applications, Zhang and Torrellas proposed memory binding and group prefetching to reduce misses in irregular data accesses in technical and scientific applications [46]. Alexander and Kedem recently proposed a table-based prediction scheme that is able to prefetch irregular data accesses in SPEC benchmarks <ref> [3] </ref>. Finally, to reduce data coherence misses in shared-memory multiprocessors, Archibald and Baer investigated various cache coherence protocols and pointed out that no single protocol fits all applications. To solve the problem, Dahlgren and Stenstrom proposed a hybrid cache coherence protocol for reducing write traffic [18]. <p> Second, the insertion of prefetches is not easy for data accessed in sequences. The addresses of prefetch variables are not usually available far in advance. On the other hand, for hardware prefetching, the irregular access patterns of OS data are difficult to be captured by table-based schemes in <ref> [12, 3] </ref>. Moreover, OS data often suffer clustered misses in sequences, like cold misses, making prefetching less effective. We have measured the effect of our optimizations on several different cache configurations. The results show that the optimizations work well for all of them.
Reference: [4] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: They also pointed out that, in comparison to applications, OS presents a higher load to the write buffer. In addition, they found that OS performance is not affected by OS/application competition in memory hierarchies. Other researchers, like Ousterhout [34] and Anderson et al <ref> [4] </ref>, also indicated the different nature of OS activities and emphasized many unique costly activities performed by OS, such as block copying. 1.2.3 Performance Optimizations Focused on Applications Clearly, given the practical importance of achieving high performance in memory hierarchies, the caching behavior of OS needs to be better understood and
Reference: [5] <author> J. B. Andrews. </author> <title> A Hardware Tracing Facility for a Multiprocessing Supercomputer. </title> <type> Technical Report 1009, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: We also explain the workloads selected for trace collection. 2.2 Hardware System We collect traces from a 4-processor bus-based Alliant FX/8 multiprocessor. We use hardware performance monitors that gathers uninterrupted reference traces of applications and OS in real time without introducing significant perturbation. The performance monitors <ref> [5] </ref> have one probe connected to each of the four processors. The probes collect all instruction and data references issued by the processors except those that hit in the per-processor 16-Kbyte first level instruction cache. Each probe has a trace buffer that stores over one million references.
Reference: [6] <author> J. Archibald and J. L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year> <month> 139 </month>
Reference-contexts: Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching [12, 21, 32, 19, 46, 3] and careful design of cache coherence protocols in shared-memory multiprocessors <ref> [6, 18, 36] </ref>. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. Data prefetching has been studied by many researchers. <p> However, if we apply the update protocol only to the variables that create most of the coherence misses (Figure 6.7-(c)), we may eliminate many misses while causing only a moderate amount of extra traffic. Based on the data shown in Table 6.6, we use the Firefly <ref> [6] </ref> update protocol on three sets of variables. The first set is barriers, amounting to 48 bytes overall. The sharing behavior of barriers clearly favors an update protocol over an invalidate protocol. The second set of variables is the 10 most active locks.
Reference: [7] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Workload Characteristic TRFD 4 TRFD ARC2D Shell +Make +Fsck User Time (%) 49.9 38.2 42.7 23.8 Idle Time (%) 8.0 8.2 11.5 29.2 OS Time (%) 42.1 53.6 45.8 47.0 TRFD 4 is a mix of 4 copies of a hand parallelized version of the TRFD Perfect Club code <ref> [7] </ref>. Each program is run with 4 processes. The code is predominately composed of matrix multiplies and data interchanges. It is highly parallel yet synchronization intensive. The most important OS activity in this application is process scheduling, cross-processor interrupts, processor synchronization, and other multiprocessor-management functions.
Reference: [8] <author> B. N. Bershad, Dennis Lee, Theodore H. Romer, and J. B. Chen. </author> <title> Avoiding Conflict Misses Dynamically in Large Direct-Mapped Caches. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-170, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Further optimizations are possible. However, the number of OS misses remaining in the primary data caches is so small that optimizations are likely to have a low impact. Possible optimizations that can be tried are page placement schemes that reduce conflicts in the secondary cache <ref> [8, 27] </ref>, or applying more aggressive prefetching for frequently accessed data. Page placement optimization has the shortcoming that the data placement is done at the grain size of a page, which is not optimal for the many small data structures in the kernel.
Reference: [9] <author> P. P. Chang and W. W. Hwu. </author> <title> Trace Selection for Compiling Large C Application Programs to Microcode. </title> <booktitle> In Proceedings of the 21st Annual Workshop on Microprogramming and Microarchitectures, </booktitle> <pages> pages 21-29, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: We present a similar algorithm in Section 4.1.4 and find it ineffective in Section 4.2.5 because OS is not loop intensive. Hwu and Chang's technique <ref> [9, 13] </ref> is based on identifying groups of basic blocks within a routine that tend to execute in sequence. These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers.
Reference: [10] <author> J. Chapin, S. A. Herrod, M. Rosenblum, and A. Gupta. </author> <title> Memory System Performance of UNIX on CC-NUMA Multiprocessors. </title> <booktitle> In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: They pointed out that the OS code suffers considerable self-interference in the instruction cache and the dominant sources of data misses in OS are coherence activity and block operations. Chapin et al <ref> [10] </ref> have recently reported similar findings for a NUMA multiprocessor running UNIX. They showed that a surprisingly large fraction of OS time (79%) is spent on memory system stalls that is divided equally between instruction and data cache miss time. <p> While all this past work has successfully characterized the problem, very little work has been done towards eliminating it. In discussions on support for block operations, for example, while Torrellas et al [41] suggested cache bypassing and prefetching, Chapin et al <ref> [10] </ref> suggested cache bypassing and some OS policies to reduce the remote caching of data, and Cheriton et al [14] proposed the deferred-copy scheme, none of them actually evaluated their proposed schemes. 2 1.2.2 Comparison of OS and Application Cache Performance Some researchers have studied and compared the different characteristics of <p> The exception is Shell which, because it is a sequential workload, has few coherence misses (6.2%). Overall, the distribution of the misses into the different categories is somewhat similar to the distribution measured in IRIX by Torrellas et al [41] and Chapin et al <ref> [10] </ref>. In their papers, these researchers pointed out the impact of block operations and coherence activity. We now focus on removing the misses in each these three categories in turn. 85 Table 6.2: Breakdown of OS data misses. Only read misses are measured. <p> In particular, locks play a major role in Shell. This is because Shell invokes many lock-intensive system calls. Important kernel locks include the ones associated with accounting, physical memory allocation, job scheduling, and the high resolution timer. This agrees with the data in [41] and <ref> [10] </ref>. Finally, 12-26% of the coherence misses are due to other effects. This includes the sharing of dynamic data structures such as u and proc, or the false sharing of other variables. Note that, unlike in [41], we do not find process migration misses.
Reference: [11] <author> J. B. Chen and B. N. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: For data cache misses, they found that a small number of routines account for 96% of OS data cache stall time. Chen and Bershad <ref> [11] </ref> pointed out that self-interference accounts for a significant number of OS misses, particularly in OS code, and benefits less from cache associativity. They found that OS code shows less locality than OS data and claimed that block operations are particularly costly. <p> They pointed out that, although kernel code and data structures may be smaller than application code/data structures, the random branching characteristics of the kernel cause more unique cache lines to be touched, thus resulting in a larger footprint and a higher kernel miss rate. Similarly, Chen and Bershad <ref> [11] </ref> reported that OS code/data locality is measurably worse than application locality, and that the performance impact can be significant. They also pointed out that, in comparison to applications, OS presents a higher load to the write buffer. <p> For our purposes, we distinguish three types of locality, namely spatial, temporal, and loop locality. We consider each of them in turn. 3.2.1 Spatial Locality Previous work had indicated that the OS code had low spatial locality <ref> [11] </ref>. This is a result of the many branches necessary to get around seldom-executed code. However, we do not care about the original spatial locality of the code.
Reference: [12] <author> T. F. Chen and J. L. Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> Data prefetching has been studied by many researchers. Fu and Patel studied data prefetching for vectorized numerical programs [21]. Mowry et al studied software prefetching techniques for loop-intensive applications. Chen and Baer proposed a hardware prefetcher that detects access stride patterns for prefetching regular data <ref> [12] </ref>. Dahlgren and Sten-strom found that sequential prefetching is more effective than stride prefetching [19]. To address the problem of irregular applications, Zhang and Torrellas proposed memory binding and group prefetching to reduce misses in irregular data accesses in technical and scientific applications [46]. <p> Second, the insertion of prefetches is not easy for data accessed in sequences. The addresses of prefetch variables are not usually available far in advance. On the other hand, for hardware prefetching, the irregular access patterns of OS data are difficult to be captured by table-based schemes in <ref> [12, 3] </ref>. Moreover, OS data often suffer clustered misses in sequences, like cold misses, making prefetching less effective. We have measured the effect of our optimizations on several different cache configurations. The results show that the optimizations work well for all of them.
Reference: [13] <author> W. Y. Chen, P. P. Chang, T. M. Conte, and W. W. Hwu. </author> <title> The Effect of Code Expanding Optimizations on Instruction Cache Design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(9) </volume> <pages> 1045-1057, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: In function inlining, the whole callee routine is inserted between the caller's basic blocks, not just a few basic blocks of the callee. Function inlining, however, expands the active code size and may increase the chance of conflicts. Indeed, while Chen et al. <ref> [13] </ref> limited inlining to frequent routines only, their results revealed that inlining may not be a stable and effective scheme. <p> We present a similar algorithm in Section 4.1.4 and find it ineffective in Section 4.2.5 because OS is not loop intensive. Hwu and Chang's technique <ref> [9, 13] </ref> is based on identifying groups of basic blocks within a routine that tend to execute in sequence. These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. <p> These basic blocks are then placed in contiguous cache locations. Furthermore, routines are placed such that frequent callee routines follow immediately after their callers. The resulting spatial locality and low interference save many misses. They also investigate function inlining <ref> [13] </ref> but find it largely ineffective because code expansion increases cache conflicts.
Reference: [14] <author> D. Cheriton, A. Gupta, P. Boyle, and H. Goosen. </author> <title> The VMP Multiprocessor: Initial Experience, Refinements and Performance Evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 410-421, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: In discussions on support for block operations, for example, while Torrellas et al [41] suggested cache bypassing and prefetching, Chapin et al [10] suggested cache bypassing and some OS policies to reduce the remote caching of data, and Cheriton et al <ref> [14] </ref> proposed the deferred-copy scheme, none of them actually evaluated their proposed schemes. 2 1.2.2 Comparison of OS and Application Cache Performance Some researchers have studied and compared the different characteristics of OS and applications, and have pointed out that OS performs poorer than applications in memory hierarchies. <p> The total number of misses increases relative to OptA in all workloads. An alternative to the previous scheme is to provide a very small cache dedicated to the important sections of the OS code only. A similar idea has been suggested in the literature <ref> [14] </ref>. We have set up a 1 Kbyte such cache (about the size of SelfConfFree) where the most important parts of the sequences are saved. An additional 7 Kbyte cache has been made available to the application and rest of the OS code. <p> Unfortunately, this scheme cannot be easily applied to blocks smaller than a page. However, Cheriton et al proposed a deferred copy scheme for blocks of various granularities in the VMP machine <ref> [14] </ref>. The VMP machine has special cache management mechanisms that support deferred copy. The authors, however, did not evaluate the gains of this mechanism. To evaluate this mechanism, we first identify all operations that copy blocks whose size is smaller than a page.
Reference: [15] <author> D. Clark. </author> <title> Cache Performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Clark <ref> [15] </ref> reported a lower performance of the VAX-11/780 cache when OS activities were taken into account. Similarly, Agarwal et al [1] pointed out that many cache misses are caused by OS.
Reference: [16] <author> Z. Cvetanovic and D. Bhandarkar. </author> <title> Characterization of Alpha AXP Performance Using TP and SPEC Workloads. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 60-70, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Commercial applications, such as database transaction processing and web servers, are important workloads because of their tremendous markets. Based on measurements by <ref> [29, 37, 16, 17] </ref>, commercial applications seem to have characteristics similar to OS. Unfortunately, these measurements give only a high-level view of the characteristics of commercial applications, such as the breakdown of memory stall time, cache misses, and branch characteristics.
Reference: [17] <author> Z. Cvetanovic and D. Bhandarkar. </author> <title> Performance Characterization of the Alpha 21164 Microprocessor Using tp and spec Workloads. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 270-280, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Commercial applications, such as database transaction processing and web servers, are important workloads because of their tremendous markets. Based on measurements by <ref> [29, 37, 16, 17] </ref>, commercial applications seem to have characteristics similar to OS. Unfortunately, these measurements give only a high-level view of the characteristics of commercial applications, such as the breakdown of memory stall time, cache misses, and branch characteristics.
Reference: [18] <author> F. Dahlgren and P. Stenstrom. </author> <title> Reducing the Write Traffic for a Hybrid Cache Protocol. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <pages> pages 166-173, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching [12, 21, 32, 19, 46, 3] and careful design of cache coherence protocols in shared-memory multiprocessors <ref> [6, 18, 36] </ref>. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. Data prefetching has been studied by many researchers. <p> Finally, to reduce data coherence misses in shared-memory multiprocessors, Archibald and Baer investigated various cache coherence protocols and pointed out that no single protocol fits all applications. To solve the problem, Dahlgren and Stenstrom proposed a hybrid cache coherence protocol for reducing write traffic <ref> [18] </ref>. Raynaud et al further studied the update distance and proposed distance-adaptive update protocols to make updates more efficient. Of course, there are a large number of other schemes that are not listed here. 4 Overall, these studies focused on applications only.
Reference: [19] <author> F. Dahlgren and P. Stenstrom. </author> <title> Effectiveness of Hardware-based Stride and Sequential Prefetching in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 68-77, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> Mowry et al studied software prefetching techniques for loop-intensive applications. Chen and Baer proposed a hardware prefetcher that detects access stride patterns for prefetching regular data [12]. Dahlgren and Sten-strom found that sequential prefetching is more effective than stride prefetching <ref> [19] </ref>. To address the problem of irregular applications, Zhang and Torrellas proposed memory binding and group prefetching to reduce misses in irregular data accesses in technical and scientific applications [46].
Reference: [20] <author> D. Ferrari. </author> <title> Improving Locality by Critical Working Sets. </title> <journal> CACM, </journal> <volume> 17(11) </volume> <pages> 614-620, </pages> <month> November </month> <year> 1974. </year>
Reference-contexts: Most of the early work has focused on reducing page faults of virtual memory machines. Hatfield and Gerald [23] used a graph clustering algorithm to group data used simultaneously on the same page. Similarly, Ferrari <ref> [20] </ref> examined the potential of restructuring programs to improve program paging behavior. Hartley [22] described a routine-level program restructuring technique to improve the page-level locality and to reduce page faults using a call graph.
Reference: [21] <author> J. W. C. Fu and J. H. Patel. </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. Data prefetching has been studied by many researchers. Fu and Patel studied data prefetching for vectorized numerical programs <ref> [21] </ref>. Mowry et al studied software prefetching techniques for loop-intensive applications. Chen and Baer proposed a hardware prefetcher that detects access stride patterns for prefetching regular data [12]. Dahlgren and Sten-strom found that sequential prefetching is more effective than stride prefetching [19].
Reference: [22] <author> S. </author> <title> Hartley. Compiler-Time Program Restruction in Multiprogrammed Virtual Memory Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(11) </volume> <pages> 1640-1644, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Most of the early work has focused on reducing page faults of virtual memory machines. Hatfield and Gerald [23] used a graph clustering algorithm to group data used simultaneously on the same page. Similarly, Ferrari [20] examined the potential of restructuring programs to improve program paging behavior. Hartley <ref> [22] </ref> described a routine-level program restructuring technique to improve the page-level locality and to reduce page faults using a call graph. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique [24, 30, 31].
Reference: [23] <author> D. Hatfield and J. Gerald. </author> <title> Program Restructuring for Virtual Memory. </title> <journal> IBM Systems Journal, </journal> <volume> 10(3) </volume> <pages> 168-192, </pages> <year> 1971. </year> <month> 141 </month>
Reference-contexts: However, reorganizing programs to improve the performance of applications has been studied by many researchers. Most of the early work has focused on reducing page faults of virtual memory machines. Hatfield and Gerald <ref> [23] </ref> used a graph clustering algorithm to group data used simultaneously on the same page. Similarly, Ferrari [20] examined the potential of restructuring programs to improve program paging behavior.
Reference: [24] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 242-251, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. However, all previous work has focused on applications only. Some examples of the solutions to applications are given below. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. <p> These two aspects of placement optimizations are already addressed by Chang-Hwu's <ref> [24] </ref> algorithm. However, our algorithm further exposes spatial locality by generating sequences that cross routine boundaries. <p> To focus on the conflict misses on the on-chip instruction cache, we do not use data references in the traces. We examine different levels of optimization: Base refers to the original unoptimized layout; C-H refers to the layout generated by Chang-Hwu's <ref> [24] </ref> algorithm; OptS refers to our layout with SelfConfFree area, sequences, and no loop optimization; OptL is OptS plus the simple loop optimization described in Section 4.1.3; finally, OptA is OptS plus the layout of the application optimized with sequences and the simple loop optimization described in Section 4.1.3. <p> Hartley [22] described a routine-level program restructuring technique to improve the page-level locality and to reduce page faults using a call graph. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. The technique is based on repositioning or replicating code, usually to reduce cache conflicts. In all cases, the results are good. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling [30], Hwu and Chang <ref> [24] </ref>, and Torrellas et al [42] studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past.
Reference: [25] <author> N. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, A. Smith examined three next-line sequential prefetching schemes that prefetch one cache line under different policies [39]; Jouppi proposed the stream buffers to prefetch more successive lines after a miss <ref> [25] </ref>; J. Smith and Hsu investigated target prefetching with the support of a target prediction table [40]; and, Pierce and Mudge proposed the wrong-path prefetching scheme to prefetch both paths of a conditional branch [35]. <p> These misses slow down the guarded sequential prefetcher. This suggests that we can further reduce the execution time by placing stream buffers <ref> [25] </ref> between the secondary caches and the memory bus to complement the guarded sequential prefetcher. We add two 8-entry direct-access stream buffers per processor. The buffers are allowed to look up the secondary cache, thus avoiding unnecessary bus accesses when the line is in the secondary cache. <p> Our results agree with his on the relative performance of the schemes. These schemes do not tolerate long latencies because they prefetch only one line ahead. An effective improvement is the stream buffer proposed by Jouppi <ref> [25] </ref>. Stream buffers prefetch successive lines after a miss. We do not compare stream buffers to guarded sequential prefetching because our scheme is a much cheaper addition to next-line prefetchers. Instead, we add stream buffers between the secondary caches and the bus.
Reference: [26] <author> N. Jouppi and S. Wilton. </author> <title> Tradeoffs in Two Level On-Chip Caching. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 34-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Examples of such processors are Alpha 21164 and Pentium-Pro 1 . As Jouppi and Wilton discussed in <ref> [26] </ref>, deep submicron process allows a larger cache area. However, a large associative L1 cache increases hit time and thereby hinders the clock rate. Alternatively, the cache area can be utilized as two small direct-mapped L1 instruction and data caches backed by a large unified associative L2 cache.
Reference: [27] <author> R. Kessler and M. Hill. </author> <title> Page Placement Algorithms for Large Real-Indexed Caches. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement <ref> [27] </ref>, data prefetching [12, 21, 32, 19, 46, 3] and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. <p> Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed <ref> [27] </ref>. Data prefetching has been studied by many researchers. Fu and Patel studied data prefetching for vectorized numerical programs [21]. Mowry et al studied software prefetching techniques for loop-intensive applications. Chen and Baer proposed a hardware prefetcher that detects access stride patterns for prefetching regular data [12]. <p> The misses that remain after this optimization tend to be spread out in the code in a uniform manner; there are no obvious hot spot areas of conflict misses. Consequently, no simple changes in the layout algorithm are likely to produce further significant gains. The page placement <ref> [27] </ref> for OS text pages is not appropriate either because kernel text cannot be paged. Instead, to hide these spread-out misses, we can try some form of prefetching. Given that there are only a few misses to be removed, however, any cost-effective prefetching scheme has to be very cheap. <p> Further optimizations are possible. However, the number of OS misses remaining in the primary data caches is so small that optimizations are likely to have a low impact. Possible optimizations that can be tried are page placement schemes that reduce conflicts in the secondary cache <ref> [8, 27] </ref>, or applying more aggressive prefetching for frequently accessed data. Page placement optimization has the shortcoming that the data placement is done at the grain size of a page, which is not optimal for the many small data structures in the kernel.
Reference: [28] <author> D. Lee, J. Baer, B. Calder, and D. Grunwald. </author> <title> Instruction Cache Fetch Policies for Speculative Execution. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 357-367, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Conceptually, the implementation needs to use a decoder [35] or a connection to a branch target buffer <ref> [28, 40] </ref> in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Smith and Hsu [40] investigated the fetch-ahead distance in a next-line prefetch scheme. This distance is the number of instructions that remain to be issued in a line before a prefetch request for the next line should be issued. Next-line prefetching has also been studied by Lee et al <ref> [28] </ref> in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines [39, 40, 43]. Smith and Hsu [40] studied target prefetching with the support of a target prediction table.
Reference: [29] <author> A. Maynard, C. Donnelly, and B. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145-156, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Maynard et al <ref> [29] </ref> found that kernel instruction and data miss rates are usually higher than user miss rates. <p> Commercial applications, such as database transaction processing and web servers, are important workloads because of their tremendous markets. Based on measurements by <ref> [29, 37, 16, 17] </ref>, commercial applications seem to have characteristics similar to OS. Unfortunately, these measurements give only a high-level view of the characteristics of commercial applications, such as the breakdown of memory stall time, cache misses, and branch characteristics.
Reference: [30] <author> S. McFarling. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. However, all previous work has focused on applications only. Some examples of the solutions to applications are given below. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. <p> Hartley [22] described a routine-level program restructuring technique to improve the page-level locality and to reduce page faults using a call graph. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. The technique is based on repositioning or replicating code, usually to reduce cache conflicts. In all cases, the results are good. <p> It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. The technique is based on repositioning or replicating code, usually to reduce cache conflicts. In all cases, the results are good. McFarling's technique <ref> [30] </ref> uses a profile of the conditional, loop, and routine structure of the program. With this information, he places the basic blocks so that callers of routines, loops, and conditionals do not interfere with the callee routines or their descendants. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling <ref> [30] </ref>, Hwu and Chang [24], and Torrellas et al [42] studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past.
Reference: [31] <author> A. Mendlson, S. Pinter, and R. Shtokhamer. </author> <title> Compile Time Intruction Cache Optimizations. </title> <booktitle> In Computer Architecture News, </booktitle> <pages> pages 44-51, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Improving the performance of caches has been addressed by many researchers. However, all previous work has focused on applications only. Some examples of the solutions to applications are given below. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. <p> Hartley [22] described a routine-level program restructuring technique to improve the page-level locality and to reduce page faults using a call graph. To improve instruction cache performance, McFarling, Hwu and Chang, and Mendlson et al studied the compiler code reorganization technique <ref> [24, 30, 31] </ref>. It has been shown that it is feasible to reduce the instruction misses in applications via improved code layout in the cache. The technique is based on repositioning or replicating code, usually to reduce cache conflicts. In all cases, the results are good. <p> Our algorithm differs from theirs in that we place basic blocks across the routine boundaries to expose more 56 spatial locality, and we use a SelfConfFree area and iteratively order the placement of the sequences by frequency of invocations to expose temporal locality. Finally, Mendlson et al <ref> [31] </ref> perform code replication based on static information to eliminate conflicts. We do not replicate basic blocks because code expansion will increase capacity misses. The OS code is too large to afford code replication. 4.4 Summary We present and evaluate a new code placement algorithm specifically tailored to OS code.
Reference: [32] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural 142 Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> Software-based prefetching is in fact quite easy to support and is already present in commodity microprocessors like Alpha [38]. We can use software pipelining to hide more latency and loop unrolling to reduce instruction overhead <ref> [32] </ref>. However, prefetching may not be able to hide the latency of the first 88 Table 6.4: Characteristics of the block operations. In the table, Src and Dst stand for source and destination blocks respectively. Unless otherwise noted, the data refers to the 32-Kbyte primary data cache. <p> Therefore, hiding the latency of the first access is not that crucial. Furthermore, the instruction overhead of prefetching is very small. After loop unrolling <ref> [32] </ref>, prefetch instructions account for slightly over 5% of the instructions in the block operations. Their effect is practically negligible. An alternative scheme to reduce the read stall time is to perform the block operations in a DMA-like fashion without involving the processor. <p> Once the miss hot spots have been identified, we manually insert the prefetches. We use a non-blocking prefetch instruction like the one used by Blk Pref in Section 6.4.2. For the loops that access the array of page table entries, we perform loop unrolling and software pipelining <ref> [32] </ref>. For the loop that accesses the linked list, software pipelining involves prefetching the pointer in a first iteration and using the pointer to prefetch the data in a second iteration. For the sequences, we move the prefetches as early as possible in the sequence.
Reference: [33] <author> D. Nagle, R. Uhlig, T. Mudge, and S. Sechrest. </author> <title> Optimal Allocation of On-chip Memory for Multiple-API Operating Systems. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 358-369, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Chen and Bershad [11] pointed out that self-interference accounts for a significant number of OS misses, particularly in OS code, and benefits less from cache associativity. They found that OS code shows less locality than OS data and claimed that block operations are particularly costly. Finally, Nagle et al <ref> [33] </ref> pointed out that cache performance is becoming increasingly important in new-generation OS because OS trends are shifting the way that components of existing architectures are utilized. While all this past work has successfully characterized the problem, very little work has been done towards eliminating it.
Reference: [34] <author> J. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster as Fast as Hardware? In Proceedings Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: They also pointed out that, in comparison to applications, OS presents a higher load to the write buffer. In addition, they found that OS performance is not affected by OS/application competition in memory hierarchies. Other researchers, like Ousterhout <ref> [34] </ref> and Anderson et al [4], also indicated the different nature of OS activities and emphasized many unique costly activities performed by OS, such as block copying. 1.2.3 Performance Optimizations Focused on Applications Clearly, given the practical importance of achieving high performance in memory hierarchies, the caching behavior of OS needs
Reference: [35] <author> J. Pierce and T. Mudge. </author> <title> Wrong-Path Instruction Prefetching. </title> <type> Technical Report CSE-222-94, </type> <institution> University of Michigan, </institution> <year> 1994. </year>
Reference-contexts: Smith and Hsu investigated target prefetching with the support of a target prediction table [40]; and, Pierce and Mudge proposed the wrong-path prefetching scheme to prefetch both paths of a conditional branch <ref> [35] </ref>. All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. <p> Conceptually, the implementation needs to use a decoder <ref> [35] </ref> or a connection to a branch target buffer [28, 40] in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Smith and Hsu [40] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. A different approach taken by Pierce and Mudge <ref> [35] </ref> is the wrong-path prefetching. This scheme prefetches both paths of a conditional branch. They found that 70-80% of the 81 performance gain came from prefetching the fall-through path instead of prefetching the target.
Reference: [36] <author> A. Raynaud, Z. Zhang, and J. Torrellas. </author> <title> Distance-Adaptive Update Protocols for Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 323-334, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching [12, 21, 32, 19, 46, 3] and careful design of cache coherence protocols in shared-memory multiprocessors <ref> [6, 18, 36] </ref>. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. Data prefetching has been studied by many researchers.
Reference: [37] <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The Impact of Architectural Trends on Operating System Performance. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Our simulated architecture projects the performance of the new-generation high-performing bus-based shared-memory multiprocessors to appear in two years. An architectural trend is that these machines will be equipped with high speed processors, such as Alpha 21164 and Pentium-Pro, which have on-chip two-level caches <ref> [37] </ref>. This architecture is more advanced than the ones we simulated in previous chapters. Analysis on memory hierarchies of this advanced architecture shows that off-chip memory stall time in OS dominates OS time. However, by applying our optimizations comprehensively, both on-chip and off-chip OS stall time is effectively reduced. <p> The data are similar to that in Table 6.1 in Section 6.3, which is based on a similar simulated architecture but the clock rates of processors are 200 MHz instead of 500 MHz. This supports the idea 115 from <ref> [37] </ref> that the relative importance of kernel execution time will likely remain the same on next-generation 500-MHz processors. The fourth row (OS Stall) shows that as much as 26-36% of the total time is wasted on processor stalls due to OS accesses to memory hierarchies. <p> Multi-issue processors, particularly those with out-of-order speculative execution, are able to hide processor memory stall time. Rosenblum et al found that memory stall time is reduced slightly when the architecture is moved to next-generation multi-issue processors <ref> [37] </ref>. Along with non-blocking cache, this architectural trend indeed adds a member to the family of latency hiding techniques, and also imposes greater resource requirements, such as the bandwidth of cache ports. <p> Commercial applications, such as database transaction processing and web servers, are important workloads because of their tremendous markets. Based on measurements by <ref> [29, 37, 16, 17] </ref>, commercial applications seem to have characteristics similar to OS. Unfortunately, these measurements give only a high-level view of the characteristics of commercial applications, such as the breakdown of memory stall time, cache misses, and branch characteristics.
Reference: [38] <author> R. L. </author> <title> Sites. Alpha AXP Architecture. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4), </volume> <year> 1992. </year>
Reference-contexts: The processor stall time caused by these misses can be reduced by prefetching the source block before the actual data use. Software-based prefetching is in fact quite easy to support and is already present in commodity microprocessors like Alpha <ref> [38] </ref>. We can use software pipelining to hide more latency and loop unrolling to reduce instruction overhead [32]. However, prefetching may not be able to hide the latency of the first 88 Table 6.4: Characteristics of the block operations.
Reference: [39] <author> A. J. Smith. </author> <title> Cache Memories. </title> <booktitle> In Computing Surveys, </booktitle> <pages> pages 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: The technique is based on repositioning or replicating code, usually to reduce cache conflicts. 3 Alternatively, instruction cache misses can be hidden by sequential and target prefetch--ing schemes. For example, A. Smith examined three next-line sequential prefetching schemes that prefetch one cache line under different policies <ref> [39] </ref>; Jouppi proposed the stream buffers to prefetch more successive lines after a miss [25]; J. Smith and Hsu investigated target prefetching with the support of a target prediction table [40]; and, Pierce and Mudge proposed the wrong-path prefetching scheme to prefetch both paths of a conditional branch [35]. <p> Finally, we consider the microarchitecture support required. We design it as an add-on feature to traditional sequential prefetching schemes. We consider two types of traditional sequential prefetching schemes: non-tagged and tagged <ref> [39] </ref>. Non-tagged schemes are simple schemes that prefetch line i + 1 after a reference to line i, or after a miss on line i. In tagged schemes, a bit is added to each cache line to improve the prefetching algorithm. <p> We first consider three traditional schemes described in <ref> [39] </ref>: next-line prefetch on access (NLalways), next-line prefetch on miss (NLmiss), and tagged prefetch as discussed in Section 5.3.2.1 (NLtagged). <p> However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past. For example, Smith examined the three next-line sequential prefetch-ing schemes described in Section 5.3.2.1 <ref> [39] </ref>. Our results agree with his on the relative performance of the schemes. These schemes do not tolerate long latencies because they prefetch only one line ahead. An effective improvement is the stream buffer proposed by Jouppi [25]. Stream buffers prefetch successive lines after a miss. <p> Next-line prefetching has also been studied by Lee et al [28] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [39, 40, 43] </ref>. Smith and Hsu [40] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best.
Reference: [40] <author> J. E. Smith and W.-C. Hsu. </author> <title> Prefetching in Supercomputer Instruction Caches. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 588-597, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For example, A. Smith examined three next-line sequential prefetching schemes that prefetch one cache line under different policies [39]; Jouppi proposed the stream buffers to prefetch more successive lines after a miss [25]; J. Smith and Hsu investigated target prefetching with the support of a target prediction table <ref> [40] </ref>; and, Pierce and Mudge proposed the wrong-path prefetching scheme to prefetch both paths of a conditional branch [35]. All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. <p> Conceptually, the implementation needs to use a decoder [35] or a connection to a branch target buffer <ref> [28, 40] </ref> in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Prefetching the next N lines after a miss was evaluated by Uhlig et al for large codes [43] and found effective. To reduce cache pollution, they suggested caching prefetched lines only if they are used. This optimization, however, actually reduced performance. Smith and Hsu <ref> [40] </ref> investigated the fetch-ahead distance in a next-line prefetch scheme. This distance is the number of instructions that remain to be issued in a line before a prefetch request for the next line should be issued. <p> Next-line prefetching has also been studied by Lee et al [28] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [39, 40, 43] </ref>. Smith and Hsu [40] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. <p> Next-line prefetching has also been studied by Lee et al [28] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines [39, 40, 43]. Smith and Hsu <ref> [40] </ref> studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. A different approach taken by Pierce and Mudge [35] is the wrong-path prefetching.
Reference: [41] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 162-174, </pages> <month> October </month> <year> 1992. </year> <month> 143 </month>
Reference-contexts: Clark [15] reported a lower performance of the VAX-11/780 cache when OS activities were taken into account. Similarly, Agarwal et al [1] pointed out that many cache misses are caused by OS. Torrellas et al <ref> [41] </ref> reported that OS causes a large fraction of the cache misses in a bus-based shared-memory multiprocessor. They pointed out that the OS code suffers considerable self-interference in the instruction cache and the dominant sources of data misses in OS are coherence activity and block operations. <p> While all this past work has successfully characterized the problem, very little work has been done towards eliminating it. In discussions on support for block operations, for example, while Torrellas et al <ref> [41] </ref> suggested cache bypassing and prefetching, Chapin et al [10] suggested cache bypassing and some OS policies to reduce the remote caching of data, and Cheriton et al [14] proposed the deferred-copy scheme, none of them actually evaluated their proposed schemes. 2 1.2.2 Comparison of OS and Application Cache Performance Some <p> To do this with little perturbation, we execute single machine instructions that cause data reads to specified addresses. Recall that there is no first-level data cache. The performance monitors can capture the addresses read from and interpret them according to an agreed-upon protocol. This methodology was suggested in <ref> [41] </ref>. To distinguish these escape accesses from real accesses, we do as follows. One first type of escapes, used for operating system instrumentation, reads odd addresses in the operating system code segment. We can easily distinguish these escapes from instruction reads since the latter are aligned on even address boundaries. <p> The exception is Shell which, because it is a sequential workload, has few coherence misses (6.2%). Overall, the distribution of the misses into the different categories is somewhat similar to the distribution measured in IRIX by Torrellas et al <ref> [41] </ref> and Chapin et al [10]. In their papers, these researchers pointed out the impact of block operations and coherence activity. We now focus on removing the misses in each these three categories in turn. 85 Table 6.2: Breakdown of OS data misses. Only read misses are measured. <p> The breakdown of the misses into these categories is shown in Table 6.3. From the table, we see that block copy and clear are the most costly operations. As pointed out in <ref> [41] </ref>, many of these operations are performed on page-sized chunks (4 Kbytes). In Shell, block clear is slightly more frequent than block copy because there are many programs, and they all invoke block clear operations in the frequent page faults during program initialization for BSS section. <p> In particular, locks play a major role in Shell. This is because Shell invokes many lock-intensive system calls. Important kernel locks include the ones associated with accounting, physical memory allocation, job scheduling, and the high resolution timer. This agrees with the data in <ref> [41] </ref> and [10]. Finally, 12-26% of the coherence misses are due to other effects. This includes the sharing of dynamic data structures such as u and proc, or the false sharing of other variables. Note that, unlike in [41], we do not find process migration misses. <p> This agrees with the data in <ref> [41] </ref> and [10]. Finally, 12-26% of the coherence misses are due to other effects. This includes the sharing of dynamic data structures such as u and proc, or the false sharing of other variables. Note that, unlike in [41], we do not find process migration misses. This is because Concentrix, unlike IRIX, does not allow processes to migrate. However, we now capture misses on synchronization variables, while the performance monitor in [41] could not do it. <p> Note that, unlike in <ref> [41] </ref>, we do not find process migration misses. This is because Concentrix, unlike IRIX, does not allow processes to migrate. However, we now capture misses on synchronization variables, while the performance monitor in [41] could not do it. Overall, based on our observations, we propose three optimizations to reduce the number of coherence misses, namely data privatization, data relocation, and selective update. <p> The sharing behavior of barriers clearly favors an update protocol over an invalidate protocol. The second set of variables is the 10 most active locks. These locks include those associated with kernel accounting, virtual memory allocation, the scheduler, and the high resolution timer. It was indicated in <ref> [41] </ref> that most OS locks tend to be acquired by the same processor several times in a row. Clearly, this is not the optimal pattern for update protocols. However, we are willing to tolerate some useless traffic if misses are eliminated.
Reference: [42] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Workloads. </title> <booktitle> In IEEE Trans. on Computers, to appear. A shorter version appeared in Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: accesses and the corresponding solutions; Chapter 7 combines all effective optimization schemes together and discusses the cost-performance trade-offs among them; and, finally, Chapter 8 concludes this work and discusses issues to be considered in future work 1 . 1 Most part of Chapter 3 to 6 can be found in <ref> [42, 45, 44] </ref>. The electronic form of this thesis and papers is available at http://www.csrd.uiuc.edu/iacoma/iacomapapers.html. 8 Chapter 2 Experiment Method and Setup 2.1 Methodology This thesis study is conducted using empirical methodology. We carry out a series of experiments using the following procedures which are depicted in Figure 2.1: 1. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling [30], Hwu and Chang [24], and Torrellas et al <ref> [42] </ref> studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past. For example, Smith examined the three next-line sequential prefetch-ing schemes described in Section 5.3.2.1 [39].
Reference: [43] <author> R. Uhlig, D. Nagle, T. Mudge, S. Sechrest, and J. Emer. </author> <title> Instruction Fetching: Coping with Code Bloat. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 345-356, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Instead, we add stream buffers between the secondary caches and the bus. Prefetching the next N lines after a miss was evaluated by Uhlig et al for large codes <ref> [43] </ref> and found effective. To reduce cache pollution, they suggested caching prefetched lines only if they are used. This optimization, however, actually reduced performance. Smith and Hsu [40] investigated the fetch-ahead distance in a next-line prefetch scheme. <p> Next-line prefetching has also been studied by Lee et al [28] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [39, 40, 43] </ref>. Smith and Hsu [40] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best.
Reference: [44] <author> C. Xia and J. Torrellas. </author> <title> Improving the Data Cache Performance of Multiprocesor Operating Systems. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 85-94, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: accesses and the corresponding solutions; Chapter 7 combines all effective optimization schemes together and discusses the cost-performance trade-offs among them; and, finally, Chapter 8 concludes this work and discusses issues to be considered in future work 1 . 1 Most part of Chapter 3 to 6 can be found in <ref> [42, 45, 44] </ref>. The electronic form of this thesis and papers is available at http://www.csrd.uiuc.edu/iacoma/iacomapapers.html. 8 Chapter 2 Experiment Method and Setup 2.1 Methodology This thesis study is conducted using empirical methodology. We carry out a series of experiments using the following procedures which are depicted in Figure 2.1: 1.
Reference: [45] <author> C. Xia and J. Torrellas. </author> <title> Instruction Prefetching of Systems Codes with Layout Optimized for Reduced Cache Misses. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: accesses and the corresponding solutions; Chapter 7 combines all effective optimization schemes together and discusses the cost-performance trade-offs among them; and, finally, Chapter 8 concludes this work and discusses issues to be considered in future work 1 . 1 Most part of Chapter 3 to 6 can be found in <ref> [42, 45, 44] </ref>. The electronic form of this thesis and papers is available at http://www.csrd.uiuc.edu/iacoma/iacomapapers.html. 8 Chapter 2 Experiment Method and Setup 2.1 Methodology This thesis study is conducted using empirical methodology. We carry out a series of experiments using the following procedures which are depicted in Figure 2.1: 1.
Reference: [46] <author> Z. Zhang and J. Torrellas. </author> <title> Speeding up Irregular Applications in Shared-Memory Multiprocessors: Memory Binding and Group Prefetching. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year> <month> 144 </month>
Reference-contexts: All of these researchers evaluated their schemes with applications like SPEC benchmarks and found them to be effective. Improving data cache performance has been a topic consistently addressed in the past, including work on page placement [27], data prefetching <ref> [12, 21, 32, 19, 46, 3] </ref> and careful design of cache coherence protocols in shared-memory multiprocessors [6, 18, 36]. Kessler and Hill investigated page placement policies and found that by carefully placing pages in memory up to 50% of misses on a large secondary cache can be removed [27]. <p> Dahlgren and Sten-strom found that sequential prefetching is more effective than stride prefetching [19]. To address the problem of irregular applications, Zhang and Torrellas proposed memory binding and group prefetching to reduce misses in irregular data accesses in technical and scientific applications <ref> [46] </ref>. Alexander and Kedem recently proposed a table-based prediction scheme that is able to prefetch irregular data accesses in SPEC benchmarks [3]. Finally, to reduce data coherence misses in shared-memory multiprocessors, Archibald and Baer investigated various cache coherence protocols and pointed out that no single protocol fits all applications.
References-found: 46

