URL: ftp://ftp.cs.indiana.edu/pub/vmenkov/lowrank/cm96jou.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/vmenkov/resume.html
Root-URL: http://www.cs.indiana.edu
Email: fbramley,vmenkovg@cs.indiana.edu  
Title: Parallel Preconditioners with Low Rank Off-Diagonal Blocks 1  
Author: Randall Bramley and Vladimir Me~nkov a 
Keyword: Low-rank approximation, Sherman-Morrison-Woodbury formula, parallel preconditioning.  
Address: Indiana University-Bloomington  
Affiliation: a Department of Computer Science  
Abstract: A common approach for blocked dense linear systems occurring in integral equations is to replace off-diagonal blocks with low rank approximations. In [3] this idea was applied to sparse linear systems occuring in PDE discretizations. Diagonal blocks are replaced with approximate factorizations and off-diagonal blocks with low rank approximations, and the resulting matrix is used as a precondi-tioner for an iterative method. This paper shows that high parallel efficiencies can be obtained when applying the Sherman-Morrison-Woodbury formula with these methods. Order estimates are developed showing that preconditioners based on low rank approximations can be implemented with roughly twice the cost per iteration of a block-diagonal preconditioner. The results apply as well to BSSOR-like preconditioning where the off-diagonal blocks of the coefficient matrix are replaced by low rank approximations. The order estimates also provide an operational definition of "low rank". Testing results are presented for the proposed approaches showing excellent performance on a shared memory multiprocessor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Alvarado, </author> <title> Ordering schemes for partitioned sparse inverses, 1989. SIAM Symposium on Sparse Matrices, </title> <address> Salishan Lodge, Gleneden Beach, OR, </address> <month> May 22-24 </month> <year> 1989. </year>
Reference: [2] <author> E. Anderson, </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations, </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <year> 1988. </year>
Reference: [3] <author> R. Bramley and V. </author> <title> Me ~ nkov, Low rank off-diagonal block preconditioners for solving sparse linear systems on parallel computers, </title> <type> Tech. Rep. 446, </type> <institution> Department of Computer Science, Indiana University, Bloomington, </institution> <note> IN, 1996. Available via World Wide Web at http://ftp.cs.indiana.edu /pub/vmenkov/lowrank/tr446.dvi. </note>
Reference-contexts: Earlier work <ref> [3] </ref> analyzed the effects of replacing off-diagonal blocks by lower rank approximations for PDE's on a regular mesh. This work examines the parallel implementation of those preconditioners, and shows under what circumstances they can achieve high parallel efficiency. <p> Some techniques for designing such low rank approximations are discussed in <ref> [3] </ref>. 8 Testing Results The numerical experiments use the preconditioners within a biconjugate gradients stabilized [4] iterative method. <p> The algorithms were implemented with a parallel factorization and solution of I + G sytems. Table 3 summarizes the methods tested. The preconditioners either used no off-diagonal blocks, the original off-diagonal blocks of A, or a low rank approximation obtained by a lumping-based low rank approximation method <ref> [3] </ref> of the respective blocks of A, with rank Q kl 3. For comparison, the BSSOR-like preconditioners SORG and SLRA were also tested using conventional block triangular forward and backward solves; these are called SORG-LU and SLRA-LU respectively.
Reference: [4] <author> H. V. der Vorst, </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 631-644. </pages>
Reference-contexts: Some techniques for designing such low rank approximations are discussed in [3]. 8 Testing Results The numerical experiments use the preconditioners within a biconjugate gradients stabilized <ref> [4] </ref> iterative method. The program is written in pC++, a parallel extension [5] of C++, and run on a ten-processor SGI Power Challenge with 75 MHz R8000 chips running the 64 bit IRIX 6.1 operating system.
Reference: [5] <author> D. Gannon, S. X. Yang, and P. Beckman, </author> <title> User Guide for a Portable Parallel C++ Programming System, pC++, </title> <institution> Department of Computer Science 17 and CICA, Indiana University, Bloomington, </institution> <note> IN, 1994. Available via World Wide Web at http://www.extreme.indiana.edu/sage/pcxx ug/pcxx ug.html. </note>
Reference-contexts: Some techniques for designing such low rank approximations are discussed in [3]. 8 Testing Results The numerical experiments use the preconditioners within a biconjugate gradients stabilized [4] iterative method. The program is written in pC++, a parallel extension <ref> [5] </ref> of C++, and run on a ten-processor SGI Power Challenge with 75 MHz R8000 chips running the 64 bit IRIX 6.1 operating system. The machine has 2Gbytes of main memory, and each processor accesses memory through a 16 Kbyte primary and 4 Mbyte secondary cache.
Reference: [6] <author> A. George and W.-H. Liu, </author> <title> The Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Engelwood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The equation for H jkl shows that the block G kl can be nonzero only if U kl is nonzero, which, in its turn, can be nonzero only if Q kl is nonzero. 2 If no exact cancellation <ref> [6, Section 2.2.2] </ref> is assumed Proposition 1 shows that G has the same nonzero block structure as Q. Corollary 2 If Q is strictly block upper or lower triangular, then so is G.
Reference: [7] <author> B. Hendrickson and R. Leland, </author> <title> The Chaco User's Guide Version 1.0, </title> <type> Tech. Rep. SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, N.M., </address> <month> October </month> <year> 1993. </year> <title> [8] , An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations, </title> <journal> SIAM Journal on Scientific Computing, </journal> <year> (1995), </year> <pages> pp. 452-469. </pages>
Reference: [9] <author> J.-F. H etu and D. Pelletier, </author> <title> Fast, adaptive finite element scheme for viscous incompressible flow, </title> <journal> AIAA Journal, </journal> <volume> 30 (1992), </volume> <pages> pp. 2677-2681. </pages>
Reference-contexts: Approximation Precond D Jacobi System D + Q C DORG DLRA Matrix: (D + Q L )D 1 SORG SLRA fl (D + Q R ) SORG-LU SLRA-LU Table 3 Methods Tested The test problem is from the fluid dynamics steady state modeling of a backward facing step in 2D <ref> [9] </ref>. The resulting matrix is of order n = 20284 and has 452752 nonzeros. This test problem was chosen because simple block diagonal (Jacobi) preconditioning allows convergence, and one purpose of our testing is to compare the parallelization of the low rank approximation method with Jacobi preconditioning.
Reference: [10] <author> J. Meijerink and H. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp., </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: This paper borrows an idea from the solution of the dense systems that occur in integral equations, and uses low rank approximations (LRA) of the off-diagonal blocks of the sparse matrix A. Diagonal blocks of A are implicitly approximated using standard preconditioners such as ILU (s) <ref> [10] </ref>.
Reference: [11] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Block columns in (4) and (5) corresponding to zero blocks Q kl are absent. Since B = D + U V T , the Sherman-Morrison-Woodbury (SMW) formula <ref> [11] </ref> gives B 1 = (D + U V T ) 1 = D 1 D 1 U (I + G) 1 V T D 1 ; (6) where G = V T D 1 U is of order M .
Reference: [12] <author> Y. Saad, </author> <title> Iterative methods for sparse linear systems, </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: A second way is to find a representation of the inverse of C, and apply this approximate inverse preconditioner with easily parallelized matrix-vector products <ref> [12] </ref>. However, for a given amount of storage the quality of approximate inverse precondition-ers is typically lower than methods that solve Cx = y.
Reference: [13] <author> A. Yeremin and L. </author> <title> Kolotilina, On a family of two-level preconditionings of the incomplete block factorization type, </title> <journal> Sov. J. Numer. Anal. Math. Modeling, </journal> <volume> 1 (1986), </volume> <pages> pp. 293-320. </pages>
Reference-contexts: In particular they can be implemented with parallelism almost matching that of block diagonal preconditioning. By varying the rank of the off-diagonal blocks the new methods can be parametrized to vary in quality between that of block diagonal and block SSOR <ref> [13] </ref> preconditioning, providing robustness for difficult systems. 2 Notation and Data Distribution Both preconditioners require solving one or two block linear systems of the form Bx = y; where the nonsingular n fi n matrix B has the splitting B = 2 D + Q with a nonsingular block-diagonal matrix D
References-found: 12

