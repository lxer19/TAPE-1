URL: http://www.cs.huji.ac.il/~feit/parsched/p-97-1.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched97.html
Root-URL: http://www.cs.huji.ac.il
Email: ffeit,rudolphg@cs.huji.ac.il  uwe@carla.e-technik.uni-dortmund.de  kcs@cs.toronto.edu  parkson@nas.nasa.gov  
Phone: 2  3  
Title: Theory and Practice in Parallel Job Scheduling  
Author: Dror G. Feitelson Larry Rudolph Uwe Schwiegelshohn Kenneth C. Sevcik and Parkson Wong 
Note: 4 MRJ, Inc., NASA Contract  
Address: 91904 Jerusalem, Israel  Dortmund, 44221 Dortmund, Germany  Toronto, Toronto, Ontario, Canada M5S 3G4  Moffett Field, CA 94035-1000, USA  
Affiliation: 1 Institute of Computer Science The Hebrew University,  Computer Engineering Institute University  Department of Computer Science University of  NAS 2-14303  
Abstract: The scheduling of jobs on parallel supercomputer is becoming the subject of much research. However, there is concern about the divergence of theory and practice. We review theoretical research in this area, and recommendations based on recent results. This is contrasted with a proposal for standard interfaces among the components of a scheduling system, that has grown from requirements in the field.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> I. Ahmad, </author> <title> "Editorial: resource management of parallel and distributed systems with static scheduling: challenges, solutions, and new problems". </title> <journal> Concurrency | Pract. & Exp. </journal> <volume> 7(5), </volume> <pages> pp. 339-347, </pages> <month> Aug </month> <year> 1995. </year>
Reference-contexts: See, for example the proceedings of three workshops [40], and a survey of a large number of proposed and implemented approaches [19]. It has become a distinct research topic from the largely unrelated, but better known problem of DAG scheduling <ref> [1] </ref>. DAG scheduling assumes that all tasks have fixed and specified dependencies, whereas job scheduling assumes that the jobs are mostly independent. This paper is about scheduling jobs on distributed memory massively parallel processors (MPPs), which currently dominate the supercomputing arena. <p> to expand, the scheduler will request additional resources from the resource manager, then inform the task manager to start the new sub-task and allow the job to proceed. main () - tm_handle handle [3]; /* connect to 3 different machines */ tm_connect (server_1, NULL, &handle [0]); tm_connect (server_2, NULL, &handle <ref> [1] </ref>); tm_connect (server_3, NULL, &handle [2]); while (1) - /* wait for events from any of the servers */ tm_get_event (handle, 3, &which_handle, &event, &job_id, &args); ack = process_event (handle [which_handle], event, job_id, args); /* acknowledge the event */ tm_ack_event (handle [which_handle], event, job_id, ack); - process_event (handle, tm_event, job_id, ...)
Reference: 2. <author> G. Alverson, S. Kahan, R. Korry, C. McCann, and B. Smith, </author> <title> "Scheduling on the Tera MTA". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitel-son and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 19-44, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Other Factors in Scheduling McCann and Zahorjan [52] studied the scheduling problem where each job has a minimum processor allocation due to its memory requirement. They find that a discipline based on allocation by a buddy system consistently does well. Alverson et al. <ref> [2] </ref> describe the scheduling policy for the Tera MTA, which includes consideration of memory requirements. Brecht [5] has carried out an experimental evaluation of scheduling in systems where processors are identified with clusters or pools, and intracluster memory access is faster than intercluster access. <p> request additional resources from the resource manager, then inform the task manager to start the new sub-task and allow the job to proceed. main () - tm_handle handle [3]; /* connect to 3 different machines */ tm_connect (server_1, NULL, &handle [0]); tm_connect (server_2, NULL, &handle [1]); tm_connect (server_3, NULL, &handle <ref> [2] </ref>); while (1) - /* wait for events from any of the servers */ tm_get_event (handle, 3, &which_handle, &event, &job_id, &args); ack = process_event (handle [which_handle], event, job_id, args); /* acknowledge the event */ tm_ack_event (handle [which_handle], event, job_id, ack); - process_event (handle, tm_event, job_id, ...) - switch (tm_event) - PSCHED_EVENT_JOB_ARRIVED:
Reference: 3. <author> S. V. Anastiadis and K. C. Sevcik, </author> <title> "Parallel application scheduling on networks of workstations". </title> <journal> J. Parallel & Distributed Comput., </journal> <month> Jun </month> <year> 1997. </year> <note> (to appear). </note>
Reference-contexts: Chiang et al. [8] show that use of knowledge of some job characteristics plus permission to use a single preemption per job allows run-to-completion policies to approach ideal (i.e., no overhead) EQUI, and Anastasiadis et al. <ref> [3] </ref> show that, by setting the processor allocation of moldable jobs based on some known job characteristics, disciplines with little or no preemption can do nearly as well as EQUI. <p> If the policy allows the job to expand, the scheduler will request additional resources from the resource manager, then inform the task manager to start the new sub-task and allow the job to proceed. main () - tm_handle handle <ref> [3] </ref>; /* connect to 3 different machines */ tm_connect (server_1, NULL, &handle [0]); tm_connect (server_2, NULL, &handle [1]); tm_connect (server_3, NULL, &handle [2]); while (1) - /* wait for events from any of the servers */ tm_get_event (handle, 3, &which_handle, &event, &job_id, &args); ack = process_event (handle [which_handle], event, job_id, args);
Reference: 4. <author> J. M. Barton and N. Bitar, </author> <title> "A scalable multi-discipline, multiple-processor scheduling framework for IRIX". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 45-69, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 5. <author> T. B. Brecht, </author> <title> "An experimental evaluation of processor pool-based scheduling for shared-memory NUMA multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: They find that a discipline based on allocation by a buddy system consistently does well. Alverson et al. [2] describe the scheduling policy for the Tera MTA, which includes consideration of memory requirements. Brecht <ref> [5] </ref> has carried out an experimental evaluation of scheduling in systems where processors are identified with clusters or pools, and intracluster memory access is faster than intercluster access.
Reference: 6. <author> J. Bruno, E. G. Coffman, Jr., and R. Sethi, </author> <title> "Scheduling independent tasks to reduce mean finishing time". </title> <journal> Comm. ACM 17(7), </journal> <pages> pp. 382-387, </pages> <month> Jul </month> <year> 1974. </year>
Reference-contexts: Algorithmic Methods The intractability for many scheduling problems has been well studied [28]. Examples in the specific area of parallel job scheduling include preemptive and non-preemptive gang scheduling by Du and Leung [13] using makespan, and Bruno, Coffman, and Sethi <ref> [6] </ref> and McNaughton [53] who use weighted completion time as optimization criterion. With the intractability of many scheduling problems being established, polynomial algorithms guaranteeing a small deviation from the optimal schedule appear more attractive.
Reference: 7. <author> S. Chakrabarti, C. Phillips, A. S. Achulz, D. B. Shmoys, C. Stein, and J. Wein, </author> <title> "Improved approximation algorithms for minsum criteria". </title> <booktitle> In Intl. Colloq. Automata, Lang., & Prog., </booktitle> <pages> pp. 646-657, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1099. </note>
Reference-contexts: On the other hand during periods of low user activity large batch jobs are started. Also moldable jobs are run in a way to increase efficiency, i.e. using fewer processors while requiring longer execution time. Recent studies, e.g. Charkrabarti et al. <ref> [7] </ref>, explicitly address the problem of bicriteria scheduling where scheduling methods are introduced which generate good schedules with respect to the makespan and the weighted completion time metric. The Model A large variety of different machine and scheduling models have been used in studies of scheduling problems.
Reference: 8. <author> S-H. Chiang, R. K. Mansharamani, and M. K. Vernon, </author> <title> "Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Similar schemes in which co-scheduling is triggered by communication events were described by Sobalvarro and Weihl [83] and by Dusseau, Arpaci, and Culler [15]. Taking system load and minimum and maximum parallelism of each job into account as well, still higher throughputs can be sustained [77]. Chiang et al. <ref> [8] </ref> show that use of knowledge of some job characteristics plus permission to use a single preemption per job allows run-to-completion policies to approach ideal (i.e., no overhead) EQUI, and Anastasiadis et al. [3] show that, by setting the processor allocation of moldable jobs based on some known job characteristics, disciplines
Reference: 9. <author> S-H. Chiang and M. K. Vernon, </author> <title> "Dynamic vs. static quantum-based parallel processor allocation". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 200-223, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: They find that the former approach suffices unless jobs are irregular (i.e., evolving) in their pattern of resource consumption. Similarly, in the context of quantum-based allocation of processing intervals, Chiang et al. <ref> [9] </ref> showed that static processor allocations (for which jobs need only be moldable) led to performance nearly as good as that obtained by dynamic processor allocation (which requires that jobs be malleable).
Reference: 10. <author> R. W. Conway, W. L. Maxwell, and L. W. Miller, </author> <title> Theory of Scheduling. </title> <publisher> Addison-Wesley, </publisher> <year> 1967. </year>
Reference-contexts: Further, if the service time distribution is know to have high variability, then feedback (FB) disciplines can exploit this, and yield lower average response times as the variability of the service time distribution grows <ref> [10] </ref>. When no knowledge of service times is available and malleability can be exploited, the ideal EQUI discipline, which attempts to assign an equal number of processors to each job available for execution is optimal.
Reference: 11. <author> X. Deng, N. Gu, T. Brecht, and K. Lu, </author> <title> "Preemptive scheduling of parallel jobs on multiprocessors". </title> <booktitle> In 7th SIAM Symp. Discrete Algorithms, </booktitle> <pages> pp. 159-167, </pages> <month> Jan </month> <year> 1996. </year>
Reference-contexts: However, once a partition for a job has been selected its size cannot change anymore. Finally, in dynamic partitioning the size of a partition may change at run time. This model has, for instance, been used by Deng et al. <ref> [11] </ref>. 2. Job Flexibility As already mentioned advanced partitioning methods must not only be supported by the multiprocessor system but by the application as well. Therefore, Feitelson and Rudolph [25] characterize applications as follows: Rigid jobs. <p> Gang scheduling may be implemented with or without mi gration. While many theoretical scheduling studies only use a model without preemption, more recently preemption has also been taken into account. Schwiegelshohn [71] uses a gang scheduling model without migration. The work of Deng et al. <ref> [11] </ref> is based upon migratable preemption. In a real system the preemption of a job requires that all the job's threads be stopped in a consistent state (i.e., without any messages being lost), and the full state of each thread must be preserved. <p> Several studies have revealed that EQUI does very well, even when some moderate charge for the overhead of frequent preemptions is made [86,48]. Squillante [84] provides an analysis of the performance of dynamic partitioning. Deng et al. show that EQUI is optimally competitive <ref> [11] </ref>. Dussa et al. [14] compares space-slicing against no partitioning, and finds that space-partitioning pays off.
Reference: 12. <author> A. B. Downey, </author> <title> "Using queue time predictions for processor allocation". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: Similarly, in the context of quantum-based allocation of processing intervals, Chiang et al. [9] showed that static processor allocations (for which jobs need only be moldable) led to performance nearly as good as that obtained by dynamic processor allocation (which requires that jobs be malleable). Foregoing Optimal Utilization Downey <ref> [12] </ref> studies the problem of scheduling in an environment where moldable jobs are activated from an FCFS queue, and run to completion.
Reference: 13. <author> J. Du and J. Y-H. Leung, </author> <title> "Complexity of scheduling parallel task systems". </title> <journal> SIAM J. Discrete Math. </journal> <volume> 2(4), </volume> <pages> pp. 473-487, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: Algorithmic Methods The intractability for many scheduling problems has been well studied [28]. Examples in the specific area of parallel job scheduling include preemptive and non-preemptive gang scheduling by Du and Leung <ref> [13] </ref> using makespan, and Bruno, Coffman, and Sethi [6] and McNaughton [53] who use weighted completion time as optimization criterion. With the intractability of many scheduling problems being established, polynomial algorithms guaranteeing a small deviation from the optimal schedule appear more attractive.
Reference: 14. <author> K. Dussa, K. Carlson, L. Dowdy, and K-H. Park, </author> <title> "Dynamic partitioning in a transputer environment". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 203-213, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several studies have revealed that EQUI does very well, even when some moderate charge for the overhead of frequent preemptions is made [86,48]. Squillante [84] provides an analysis of the performance of dynamic partitioning. Deng et al. show that EQUI is optimally competitive [11]. Dussa et al. <ref> [14] </ref> compares space-slicing against no partitioning, and finds that space-partitioning pays off.
Reference: 15. <author> A. Dusseau, R. H. Arpaci, and D. E. Culler, </author> <title> "Effective distributed scheduling of parallel workloads". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 25-36, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Similar schemes in which co-scheduling is triggered by communication events were described by Sobalvarro and Weihl [83] and by Dusseau, Arpaci, and Culler <ref> [15] </ref>. Taking system load and minimum and maximum parallelism of each job into account as well, still higher throughputs can be sustained [77].
Reference: 16. <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska, </author> <title> "Speedup versus efficiency in parallel systems". </title> <journal> IEEE Trans. Comput. </journal> <volume> 38(3), </volume> <pages> pp. 408-423, </pages> <month> Mar </month> <year> 1989. </year>
Reference-contexts: et al. [29] propose several processor allocation schemes based on the processor working set (PWS), which is the number of allocated processors for which the ratio of execution time to efficiency is minimized. (The PWS differs from the average parallelism of the job by at most a factor of two <ref> [16] </ref>.) The best of the variants of PWS gives jobs at most their processor working set, but under heavy load gives fewer and fewer processors to each job, thus increasing efficiency and therefore system capacity. <p> Knowledge of the average parallelism of a job makes it possible to allocate each job an appropriate number of processors to make it operate at a near-optimal ratio of execution time to efficiency <ref> [16] </ref>. With the knowledge of how many processors each job uses, policies for packing the jobs into frames for gang scheduling are investigated by Feitelson [18].
Reference: 17. <author> D. G. Feitelson, </author> <title> "Memory usage in the LANL CM-5 workload". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: The common conclusion is that much information about a job's resource requirements can be uncovered without demanding the user's cooperation. Feitelson <ref> [17] </ref> studied the memory requirements of parallel jobs in a CM-5 environment. He found that memory is a significant resource in high-performance computing, although he observed that users typically request more processors than naturally correspond to their memory requirements.
Reference: 18. <author> D. G. Feitelson, </author> <title> "Packing schemes for gang scheduling". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 89-110, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: With the knowledge of how many processors each job uses, policies for packing the jobs into frames for gang scheduling are investigated by Feitelson <ref> [18] </ref>. Feitel-son and Rudolph [22] describe a discipline in which processes that communicate frequently are identified, and it is assured that the corresponding threads are all activated at the same time.
Reference: 19. <author> D. G. Feitelson, </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Research Report RC 19790 (87657), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Oct </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The scheduling of jobs on parallel supercomputers is becoming the subject of much research activity. See, for example the proceedings of three workshops [40], and a survey of a large number of proposed and implemented approaches <ref> [19] </ref>. It has become a distinct research topic from the largely unrelated, but better known problem of DAG scheduling [1]. DAG scheduling assumes that all tasks have fixed and specified dependencies, whereas job scheduling assumes that the jobs are mostly independent.
Reference: 20. <author> D. G. Feitelson and M. A. Jette, </author> <title> "Improved utilization and responsiveness with gang scheduling". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: In parallel systems, preemption is also useful in reducing fragmentation. For example, with preemption it is not necessary to accumulate idle processors in order to run a large job. Feitelson and Jette <ref> [20] </ref> demonstrate that the preemptions inherent in time-slicing allow the system to escape from bad processor allocation decisions, boosting utilization over space-slicing for rigid jobs, and avoiding the need for non-work conserving algorithms. Also, preemtion is needed in order to migrate processes to actively counter fragmentation. <p> Reported utilization figures vary from 50% for the NASA Ames iPSC/860 hypercube [21], through around 70% for the CTC SP2 [37], 74% for the SDSC Paragon [92] and 80% for the Touchstone Delta [54], up to more than 90% for the LLNL Cray T3D <ref> [20] </ref>. Utilization figures in the 80-90% range are now becoming more common, due to the use of more elaborate batch queueing mechanisms [49,79,92] and gang scheduling [20]. These figures seem to leave only little room for improvement. <p> [37], 74% for the SDSC Paragon [92] and 80% for the Touchstone Delta [54], up to more than 90% for the LLNL Cray T3D <ref> [20] </ref>. Utilization figures in the 80-90% range are now becoming more common, due to the use of more elaborate batch queueing mechanisms [49,79,92] and gang scheduling [20]. These figures seem to leave only little room for improvement. However, it should be noted that these figures only reflect one factor contributing to utilization.
Reference: 21. <author> D. G. Feitelson and B. Nitzberg, </author> <title> "Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 337-360, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: They reveal characteristics of actual workloads that can be exploited in scheduling. Feitelson and Nitzberg <ref> [21] </ref> noted that repeated runs of the same application occurred frequently, and later runs tended to have similar resource consumption patterns as the corresponding earlier ones. Hotovy [37] studied a quite different system, yet found many of the same observations to hold. <p> In practice utilization is a very commonly used metric, as it is easy to measure and reflects directly on the degree to which large investments in parallel hardware are used efficiently. Throughput figures are hardly ever used. Reported utilization figures vary from 50% for the NASA Ames iPSC/860 hypercube <ref> [21] </ref>, through around 70% for the CTC SP2 [37], 74% for the SDSC Paragon [92] and 80% for the Touchstone Delta [54], up to more than 90% for the LLNL Cray T3D [20]. <p> The case of the response time metric is more complex, because little direct evidence exists. Theory suggests that preemption be used to ensure good response times for small jobs [64], especially since workloads have a high variability in computational requirements <ref> [21] </ref>. This comes close on the heels of actual systems that implement gang scheduling for just this reason [46,32,27,20].
Reference: 22. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Coscheduling based on runtime identification of activity working sets". </title> <booktitle> Intl. J. Parallel Programming 23(2), </booktitle> <pages> pp. 135-160, </pages> <month> Apr </month> <year> 1995. </year>
Reference-contexts: With the knowledge of how many processors each job uses, policies for packing the jobs into frames for gang scheduling are investigated by Feitelson [18]. Feitel-son and Rudolph <ref> [22] </ref> describe a discipline in which processes that communicate frequently are identified, and it is assured that the corresponding threads are all activated at the same time.
Reference: 23. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Evaluation of design choices for gang scheduling using distributed hierarchical control". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 35(1), </volume> <pages> pp. 18-34, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: They find that long jobs benefit substantially from this approach, but only at the cost of longer response times for short jobs. Feitelson and Rudolph <ref> [23] </ref> and Hori et al. [36] analyze a more flexible policy in which there is time slicing among multiple active sets of partitions.
Reference: 24. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Gang scheduling performance benefits for fine-grain synchronization". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 16(4), </volume> <pages> pp. 306-318, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Time slicing is typically implemented by gang scheduling, that is, all the threads in a job are scheduled (and de-scheduled) simultaneously. Gang scheduling is compared to local scheduling and is found to be superior by Feitelson and Rudolph <ref> [24] </ref>. Squillante et al. [85] and Wang et al. [94] have analyzed a variation of gang scheduling that involves providing service cyclically among a set of fixed partition configurations, each having a number of partitions equal to some power of two.
Reference: 25. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Toward convergence in job schedulers for parallel supercomputers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 1-26, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: Many studies, however, show that more flexibility in both the scheduler actions and the way programs make use of parallelism result in better performance. But there is hope for convergence <ref> [25] </ref>. For example, theoretical analysis underscores the effectiveness of preemption in achieving low average response times, and also shows that considerable benefits are possible if the scheduler is allowed to tailor the partition sizes in accordance with the current system load. <p> The following roughly classifies these models according to five criteria: 1. Partition Specification Each parallel job is executed in a partition that consists of a number of processors. The size of such a partition may depend on the multiprocessor, the application, and the load of multiprocessor <ref> [25] </ref>. Moreover, the size of the partition of a specific job may change during the lifetime of this job in some models: Fixed. The partition size is defined by the system administrator and can be modified only by reboot. Variable. <p> This model has, for instance, been used by Deng et al. [11]. 2. Job Flexibility As already mentioned advanced partitioning methods must not only be supported by the multiprocessor system but by the application as well. Therefore, Feitelson and Rudolph <ref> [25] </ref> characterize applications as follows: Rigid jobs. The number of processors assigned to a job is specified external to the scheduler, and precisely that number of processors are made available to the job throughout its execution. Moldable jobs. <p> Generally, a scheduler can start a job sooner if the job is moldable or even malleable than if it is rigid [87]. If jobs are moldable, then processor allocations can be selected in accordance with the current system load, which delays the onset of saturation as system load increases <ref> [25] </ref>. It is generally not difficult to write an application so that it is moldable, and is able to execute with processor allocations over some range (e.g., any power of two from four to 256). <p> are: The value of local preemption is restricted by the fragmentation that occurs because jobs must be restarted on the same set of processors on which they previously ran. (In this case, either clever packing strategies or even fixed partitioning are beneficial, because the dependencies among jobs are then limited <ref> [25] </ref>.) Even with moldable jobs, performance is poor unless preemption is supported, because if inappropriate allocations are occasionally made to very long jobs, then only preemption can remedy the situation. 3 Recommendations and Future Directions The current state-of-the-art regarding scheduling on large-scale parallel machines is to use simple and inflexible mechanisms. <p> If users are satisfied with the system, and the system does not saturate, more jobs will be submitted, leading to higher utilization and throughput. The role of the scheduler is therefore to delay the onset of saturation, by reducing fragmentation and assuring efficient usage of processors <ref> [25] </ref>. Also, good support for batch jobs can move some of the load to off hours, further increasing the overall utilization.
Reference: 26. <author> A. Feldmann, J. Sgall, and S-H. Teng, </author> <title> "Dynamic scheduling on parallel machines". </title> <journal> Theoretical Comput. Sci. </journal> <volume> 130(1), </volume> <pages> pp. 49-72, </pages> <month> Aug </month> <year> 1994. </year>
Reference-contexts: The partition size is determined by the scheduler at the time the job is initiated, based on the system load, and taking the user request into account. Dynamic. The partition size may change during the execution of a job, to reflect changing requirements and system load. Feldmann et al. <ref> [26] </ref> have considered fixed partitions generated by different architectures such as hypercubes, trees, or meshes. Many other authors use the variable partitioning paradigm, in which each job requires a specific number of processors but can be scheduled on any subset of processors of the system.
Reference: 27. <author> H. Franke, P. Pattnaik, and L. Rudolph, </author> <title> "Gang scheduling for highly efficient distributed multiprocessor systems". </title> <booktitle> In 6th Symp. Frontiers Massively Parallel Comput., </booktitle> <pages> pp. 1-9, </pages> <month> Oct </month> <year> 1996. </year>
Reference: 28. <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: Mostly, memory requirements have been ignored in real scheduling systems and are not even part of the model in theoretical studies, although this is changing [65,73,62,61]. Algorithmic Methods The intractability for many scheduling problems has been well studied <ref> [28] </ref>. Examples in the specific area of parallel job scheduling include preemptive and non-preemptive gang scheduling by Du and Leung [13] using makespan, and Bruno, Coffman, and Sethi [6] and McNaughton [53] who use weighted completion time as optimization criterion.
Reference: 29. <author> D. Ghosal, G. Serazzi, and S. K. Tripathi, </author> <title> "The processor working set and its use in scheduling multiprocessor systems". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 17(5), </volume> <pages> pp. 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Zahorjan and McCann [97] found that allocating processors to evolving jobs according to their dynamic needs led to much better performance than either run-to-completion with a rigid allocation or round-robin. For the overhead parameters they chose, round-robin beat run-to-completion only at quite low system loads. Ghosal et al. <ref> [29] </ref> propose several processor allocation schemes based on the processor working set (PWS), which is the number of allocated processors for which the ratio of execution time to efficiency is minimized. (The PWS differs from the average parallelism of the job by at most a factor of two [16].) The best
Reference: 30. <author> R. Gibbons, </author> <title> A Historical Application Profiler for Use by Parallel Schedulers. </title> <type> Master's thesis, </type> <institution> Dept. Computer Science, University of Toronto, </institution> <month> Dec </month> <year> 1996. </year> <note> Available as Technical Report CSRI-TR 354. </note>
Reference-contexts: Feitelson and Nitzberg [21] noted that repeated runs of the same application occurred frequently, and later runs tended to have similar resource consumption patterns as the corresponding earlier ones. Hotovy [37] studied a quite different system, yet found many of the same observations to hold. Gibbons <ref> [30] </ref> also analyzed workload data from the Cornell site in addition to two sites where parallel applications are executed on a network of workstations, concluding that in all three systems classifying the jobs by user, execution script, and requested degree of parallelism led to classes of jobs in which execution time
Reference: 31. <author> R. Gibbons, </author> <title> "A historical application profiler for use by parallel schedulers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: All identifying characteristics associated with the submital of a job can potentially be used to determine its class. These characteristics include the user id, the file to be executed, the memory size specified, and possibly others. An estimate of the efficiency [59] or the execution time <ref> [31] </ref> of a job being scheduled can be obtained from retained statistics on the actual resource usage of jobs from the same (or a similar) class that have been previously submitted and executed.
Reference: 32. <author> B. Gorda and R. Wolski, </author> <title> "Time sharing massively parallel machines". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 214-217, </pages> <month> Aug </month> <year> 1995. </year>
Reference: 33. <author> R. L. Graham, </author> <title> "Bounds on multiprocessing timing anomalies". </title> <journal> SIAM J. Applied Mathematics 17(2), </journal> <pages> pp. 416-429, </pages> <month> Mar </month> <year> 1969. </year>
Reference-contexts: This result was further improved to the factor 1.4 by using the job order of the SMART schedule as input for a list schedule <ref> [33] </ref>. But note that SMART generates non-preemptive off-line schedules and requires the knowledge of the execution time of all jobs. The consideration of more complex constraints may make any general approximation algorithm impossible [42,47].
Reference: 34. <author> A. S. Grimshaw, J. B. Weissman, E. A. West, and E. C. Loyot, Jr., "Metasys-tems: </author> <title> an approach combining parallel processing and heterogeneous distributed computing systems". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 21(3), </volume> <pages> pp. 257-270, </pages> <month> Jun </month> <year> 1994. </year>
Reference-contexts: A metacenter is a computing resource where jobs can be scheduled and run on a variety of machines physically located in different facilities <ref> [34] </ref>. This concept ran into several road blocks: Some schedulers are tightly integrated with the message passing library: Condor and PVM. Almost all schedulers are tightly integrated with the batch queue system. Lack of support for parallel jobs.
Reference: 35. <author> R. L. Henderson, </author> <title> "Job scheduling under the portable batch system". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 279-294, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: It was found that user satisfaction was greatly increased since smaller jobs tended to get through faster, because they could bypass the very big ones. Henderson <ref> [35] </ref> describes the Portable Batch System (PBS), another system in which performance gains are achieved by moving away from strict FCFS scheduling. Wan et al. [92] also implement a non-FCFS scheduler that uses a variation of a 2-D buddy system to do processor allocation for the Intel Paragon. <p> The supercomputing working group has since been inactive. PBS was developed at NASA Ames Research Center as a second generation batch queue system that conforms to the IEEE Std. 1003.2d-1994. The project started in June 1993, and was first released in June 1994 <ref> [35] </ref>. However, both NQS and PBS were designed to schedule serial jobs, and have no understanding of the needs of parallel jobs. The only support for parallelism is regarding "processors" as another resource during allocation, on the same standing as time, memory, or software licenses.
Reference: 36. <author> A. Hori et al., </author> <title> "Time space sharing scheduling and architectural support". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 92-105, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: They find that long jobs benefit substantially from this approach, but only at the cost of longer response times for short jobs. Feitelson and Rudolph [23] and Hori et al. <ref> [36] </ref> analyze a more flexible policy in which there is time slicing among multiple active sets of partitions.
Reference: 37. <author> S. Hotovy, </author> <title> "Workload evolution on the Cornell Theory Center IBM SP2". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 27-40, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: They reveal characteristics of actual workloads that can be exploited in scheduling. Feitelson and Nitzberg [21] noted that repeated runs of the same application occurred frequently, and later runs tended to have similar resource consumption patterns as the corresponding earlier ones. Hotovy <ref> [37] </ref> studied a quite different system, yet found many of the same observations to hold. <p> Throughput figures are hardly ever used. Reported utilization figures vary from 50% for the NASA Ames iPSC/860 hypercube [21], through around 70% for the CTC SP2 <ref> [37] </ref>, 74% for the SDSC Paragon [92] and 80% for the Touchstone Delta [54], up to more than 90% for the LLNL Cray T3D [20].
Reference: 38. <author> N. Islam, A. Prodromidis, and M. S. Squillante, </author> <title> "Dynamic partitioning in different distributed-memory environments". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 244-270, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference: 39. <author> J. Jann, P. Pattnaik, H. Franke, F. Wang, J. Skovira, and J. Riodan, </author> <title> "Modeling of workload in mpps". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: Feitelson [17] studied the memory requirements of parallel jobs in a CM-5 environment. He found that memory is a significant resource in high-performance computing, although he observed that users typically request more processors than naturally correspond to their memory requirements. Jann et al. <ref> [39] </ref> have produced a workload model based on measurements of the workload on the Cornell Theory Center SP2 machine. This model is intended to be used by other researchers, leading to easier and more meaningful comparison of results.
Reference: 40. <editor> Job Scheduling Strategies for Parallel Processing, D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <month> 949. </month> <title> Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <month> 1162. </month> <title> Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: 1 Introduction The scheduling of jobs on parallel supercomputers is becoming the subject of much research activity. See, for example the proceedings of three workshops <ref> [40] </ref>, and a survey of a large number of proposed and implemented approaches [19]. It has become a distinct research topic from the largely unrelated, but better known problem of DAG scheduling [1].
Reference: 41. <author> T. Kawaguchi and S. Kyan, </author> <title> "Worst case bound of an LRF schedule for the mean weighted flow-time problem". </title> <journal> SIAM J. Comput. </journal> <volume> 15(4), </volume> <pages> pp. 1119-1129, </pages> <month> Nov </month> <year> 1986. </year>
Reference-contexts: If the evaluation of real traces reveals that such critical workloads rarely or even never occur then they can either be ignored or the approach can be enhanced with a procedure to specifically handle those situations. For instance Kawaguchi and Kyan's LRF schedule <ref> [41] </ref> can be easily extended to parallel jobs. As long as no parallel job requires more than 50% of the processors, this will only increase the approximation factor from 1.21 to 2 [87].
Reference: 42. <author> H. Kellerer, T. Tautenhahn, and G. J. Woginger, </author> <title> "Approximability and nonap-proximability results for minimizing total flow time on a single machine". </title> <booktitle> In 28th Ann. Symp. Theory of Computing, </booktitle> <pages> pp. 418-426, </pages> <year> 1996. </year>
Reference-contexts: A schedule with optimal weighted completion time also has the optimal weighted flow time. This equality does not hold, however, when they deviate by even a constant factor from the optimum as shown by Kellerer et al. <ref> [42] </ref> and by Leonardi and Raz [47]. In reality, the metrics attempt to formalize the real goals of a scheduler: 1. Satisfy the users. 2. Maximize the profit. For instance, a reduction of the job response time will most likely improve user satisfaction. Example 1.
Reference: 43. <author> R. Krishnamurti and E. Ma, </author> <title> "An approximation algorithm for scheduling tasks on varying partition sizes in partitionable multiprocessor systems". </title> <journal> IEEE Trans. Comput. </journal> <volume> 41(12), </volume> <pages> pp. 1572-1579, </pages> <month> Dec </month> <year> 1992. </year>
Reference: 44. <author> R. N. Lagerstrom and S. K. Gipp, "PScheD: </author> <title> political scheduling on the CRAY T3E". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference: 45. <author> W. Lee, M. Frank, V. Lee, K. Mackenzie, and L. Rudolph, </author> <title> "Implications of I/O for gang scheduled workloads". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: Another use of preemption that is also known from conventional uniprocessor systems is that it allows the overlap of computation and I/O. This is especially important in large scale systems that perform I/O to mass storage devices, an operation that may take several minutes to complete. Lee et al. <ref> [45] </ref> have shown that some jobs are more sensitive to perturbations than others, therefore some jobs have a stronger requirement for gang scheduling. However, all parallel jobs benefit from rate-equivalent scheduling, that is all threads get to run for the same fraction of the wallclock time, but not necessarily simultanously. <p> Feitelson and Rudolph [23] and Hori et al. [36] analyze a more flexible policy in which there is time slicing among multiple active sets of partitions. Lee et al. <ref> [45] </ref> study the interaction of gang scheduling and I/O, and found that many jobs may tolerate the perturbations caused by I/O, that I/O bound jobs suffer under gang scheduling, and therefore argue in favor a flexible gang scheduling.
Reference: 46. <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Gan--mukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong-Chan, S-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 33(2), </volume> <pages> pp. 145-158, </pages> <month> Mar </month> <year> 1996. </year>
Reference: 47. <author> S. Leonardi and D. Raz, </author> <title> "Approximating total flow time on parallel machines". </title> <booktitle> In 29th Ann. Symp. Theory of Computing, </booktitle> <year> 1997. </year>
Reference-contexts: A schedule with optimal weighted completion time also has the optimal weighted flow time. This equality does not hold, however, when they deviate by even a constant factor from the optimum as shown by Kellerer et al. [42] and by Leonardi and Raz <ref> [47] </ref>. In reality, the metrics attempt to formalize the real goals of a scheduler: 1. Satisfy the users. 2. Maximize the profit. For instance, a reduction of the job response time will most likely improve user satisfaction. Example 1.
Reference: 48. <author> S. T. Leutenegger and M. K. Vernon, </author> <title> "The performance of multiprogrammed multiprocessor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference: 49. <author> D. Lifka, </author> <title> "The ANL/IBM SP scheduling system". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 295-303, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 50. <author> W. Ludwig and P. Tiwari, </author> <title> "Scheduling malleable and nonmalleable parallel tasks". </title> <booktitle> In 5th SIAM Symp. Discrete Algorithms, </booktitle> <pages> pp. 167-176, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Hence, a combined benefit can be derived if processor allocations are changed only at times when checkpoints are taken. 1 Some theoretical studies use different terminology. For example, Ludwig and Tiwari <ref> [50] </ref> speak about "malleable" jobs which are equivalent to moldable jobs in our terminology. 2 In the "work crew" model, processors pick up relatively small and independent units of computation from a central queue.
Reference: 51. <author> C. McCann and J. Zahorjan, </author> <title> "Processor allocation policies for message passing parallel computers". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 19-32, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: McCann and Zahorjan <ref> [51] </ref> found that "efficiency-preserving" scheduling using folding allowed performance to remain much better than with equipartition-ing (EQUI) as load increases. Padhye and Dowdy [60] compare the effectiveness of treating jobs as moldable to that of exploiting their malleability by folding.
Reference: 52. <author> C. McCann and J. Zahorjan, </author> <title> "Scheduling memory constrained jobs on distributed memory parallel computers". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 208-219, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Other Factors in Scheduling McCann and Zahorjan <ref> [52] </ref> studied the scheduling problem where each job has a minimum processor allocation due to its memory requirement. They find that a discipline based on allocation by a buddy system consistently does well.
Reference: 53. <author> R. McNaughton, </author> <title> "Scheduling with deadlines and loss functions". </title> <booktitle> Management Science 6(1), </booktitle> <pages> pp. 1-12, </pages> <month> Oct </month> <year> 1959. </year>
Reference-contexts: Algorithmic Methods The intractability for many scheduling problems has been well studied [28]. Examples in the specific area of parallel job scheduling include preemptive and non-preemptive gang scheduling by Du and Leung [13] using makespan, and Bruno, Coffman, and Sethi [6] and McNaughton <ref> [53] </ref> who use weighted completion time as optimization criterion. With the intractability of many scheduling problems being established, polynomial algorithms guaranteeing a small deviation from the optimal schedule appear more attractive. Some polynomial algorithms are still very complex, [70], while others are particular simple algorithms, like list scheduling methods [41,95].
Reference: 54. <author> P. Messina, </author> <title> "The Concurrent Supercomputing Consortium: </title> <booktitle> year 1". IEEE Parallel & Distributed Technology 1(1), </booktitle> <pages> pp. 9-16, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: Throughput figures are hardly ever used. Reported utilization figures vary from 50% for the NASA Ames iPSC/860 hypercube [21], through around 70% for the CTC SP2 [37], 74% for the SDSC Paragon [92] and 80% for the Touchstone Delta <ref> [54] </ref>, up to more than 90% for the LLNL Cray T3D [20]. Utilization figures in the 80-90% range are now becoming more common, due to the use of more elaborate batch queueing mechanisms [49,79,92] and gang scheduling [20]. These figures seem to leave only little room for improvement.
Reference: 55. <author> R. Motwani, S. Phillips, and E. Torng, </author> <title> "Non-clairvoyant scheduling". </title> <journal> Theoretical Comput. Sci. </journal> <volume> 130(1), </volume> <pages> pp. 17-47, </pages> <month> Aug </month> <year> 1994. </year>
Reference-contexts: Preemption may have great benefit in leading to improved performance, even if it is used infrequently and on only a small fraction of all jobs. Preemption in real machines has an overhead cost, e.g. Motwani et al. <ref> [55] </ref> address the overhead by minimizing the number of preemptions. In order to compare a preemptive schedule with non-preemptive ones Schwiegelshohn [71] includes a time penalty for each preemption. 4.
Reference: 56. <author> V. K. Naik, S. K. Setia, and M. S. Squillante, </author> <title> "Performance analysis of job scheduling policies in parallel supercomputing environments". </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pp. 824-833, </pages> <month> Nov </month> <year> 1993. </year>
Reference-contexts: In a later study [75], they go on to show that dynamic partitioning of the system beats static partitioning at moderate and heavy loads. Naik, Setia and Squillante <ref> [56] </ref> show that dynamic partitioning allows much better performance than fixed partitioning, but that much of the difference in 4 Actually, EASY only guarantees that the first job in the queue will not be delayed. performance can be obtained by using knowledge of job characteristics, and as-signing non-preemptive priorities to certain
Reference: 57. <author> R. Nelson, D. Towsley, and A. N. Tantawi, </author> <title> "Performance analysis of parallel processing systems". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 14(4), </volume> <pages> pp. 532-540, </pages> <month> Apr </month> <year> 1988. </year>
Reference-contexts: Wan et al. [92] also implement a non-FCFS scheduler that uses a variation of a 2-D buddy system to do processor allocation for the Intel Paragon. Thread-oriented scheduling Nelson, Towsley, and Tantawi <ref> [57] </ref> compare four cases in which parallel jobs are scheduled in either a centralized or de-centralized fashion, and the threads of a job are either spread across all processors or all executed on one processor. They found that best performance resulted from centralized scheduling and spreading the threads across processors.
Reference: 58. <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan, </author> <title> "Parallel application characterization for multiprocessor scheduling policy design". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 175-199, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: Jann et al. [39] have produced a workload model based on measurements of the workload on the Cornell Theory Center SP2 machine. This model is intended to be used by other researchers, leading to easier and more meaningful comparison of results. Nguyen at al. <ref> [58] </ref> have measured the speedup characteristics of a variety of applications. Batch Job Scheduling In an effort to improve the way current schedulers behave, several groups have modified NQS implementations to allow queue reordering in order to achieve better packing.
Reference: 59. <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan, </author> <title> "Using runtime measured workload characteristics in parallel processor scheduling". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 155-174, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: All identifying characteristics associated with the submital of a job can potentially be used to determine its class. These characteristics include the user id, the file to be executed, the memory size specified, and possibly others. An estimate of the efficiency <ref> [59] </ref> or the execution time [31] of a job being scheduled can be obtained from retained statistics on the actual resource usage of jobs from the same (or a similar) class that have been previously submitted and executed.
Reference: 60. <author> J. D. Padhye and L. Dowdy, </author> <title> "Dynamic versus adaptive processor allocation policies for message passing parallel computers: an empirical comparison". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 224-243, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: McCann and Zahorjan [51] found that "efficiency-preserving" scheduling using folding allowed performance to remain much better than with equipartition-ing (EQUI) as load increases. Padhye and Dowdy <ref> [60] </ref> compare the effectiveness of treating jobs as moldable to that of exploiting their malleability by folding. They find that the former approach suffices unless jobs are irregular (i.e., evolving) in their pattern of resource consumption.
Reference: 61. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Benefits of speedup knowledge in memory-constrained multiprocessor scheduling". </title> <booktitle> Performance Evaluation 27&28, </booktitle> <pages> pp. 253-272, </pages> <month> Oct </month> <year> 1996. </year>
Reference: 62. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Coordinated allocation of memory and processors in multiprocessors". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 57-67, </pages> <month> May </month> <year> 1996. </year>
Reference: 63. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Implementing multiprocessor scheduling disci-plines". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: If better knowledge of job service times than queue identities is available, then it is best to try to activate the jobs in order of increasing expected remaining service time <ref> [63] </ref>. If the service times are known to be highly variable, but the service times of individual jobs cannot be predicted in advance, then the discipline that executes the job with least acquired service first is best because it emulates the behavior of least expected remaining work first.
Reference: 64. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Multiprocessor scheduling for high-variability service time distributions". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 127-145, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: This is especially true when the workload has a very high variability (which is the case in real production systems). Parsons and Sevcik <ref> [64] </ref> show the importance of preemption under high variance by comparing versions with and without preemption of the same policy. Good support for short running jobs is important because it allows for interactive feedback. <p> This is possible with moldable or malleable jobs, but not with rigid ones. The case of the response time metric is more complex, because little direct evidence exists. Theory suggests that preemption be used to ensure good response times for small jobs <ref> [64] </ref>, especially since workloads have a high variability in computational requirements [21]. This comes close on the heels of actual systems that implement gang scheduling for just this reason [46,32,27,20].
Reference: 65. <author> V. G. J. Peris, M. S. Squillante, and V. K. Naik, </author> <title> "Analysis of the impact of memory in distributed parallel processing systems". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 5-18, </pages> <month> May </month> <year> 1994. </year>
Reference: 66. <author> J. Pruyne and M. Livny, </author> <title> "Managing checkpoints for parallel programs". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 140-154, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: The latter approach is somewhat more effective, but it requires more effort from the application writer, as well as significantly more system support. Much of the required mechanisms for supporting malleable job scheduling is present in facilities for checkpointing parallel jobs <ref> [66] </ref>. Hence, a combined benefit can be derived if processor allocations are changed only at times when checkpoints are taken. 1 Some theoretical studies use different terminology.
Reference: 67. <author> J. Pruyne and M. Livny, </author> <title> "Parallel processing on dynamic resources with CARMI". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 259-278, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Certain parallel applications, such as those based on the "work crew" model 2 , can be modified to be malleable relatively easily. An increased processor allocation allows more processors to take work from the queue, while a reduction means that some processors cease picking up work and are deallocated <ref> [67] </ref>. In most cases, however, it is more difficult to support malleability in the way an application is written.
Reference: 68. <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carlson, </author> <title> "Robust partitioning schemes of multiprocessor systems". </title> <booktitle> Performance Evaluation 19(2-3), </booktitle> <pages> pp. 141-165, </pages> <month> Mar </month> <year> 1994. </year>
Reference: 69. <author> E. Rosti, E. Smirni, G. Serazzi, and L. W. Dowdy, </author> <title> "Analysis of non-work-conserving processor partitioning policies". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 165-181, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The tradeoff is between starting a job sooner with fewer processors and delaying its start (causing processors to be left idle) until a larger number of processors is available. Algorithms that leave processors idle in anticipation of future arrivals were also investigated by Rosti et al. <ref> [69] </ref> and Smirni et al. [81]. The Need for Preemption A number of studies have demonstrated that despite the overheads of preemption, the flexibility derived from the ability to preempt jobs allows for much better schedules.
Reference: 70. <author> S. K. Sahni, </author> <title> "Algorithms for scheduling independent tasks". </title> <editor> J. </editor> <booktitle> ACM 23(1), </booktitle> <pages> pp. 116-127, </pages> <month> Jan </month> <year> 1976. </year>
Reference-contexts: With the intractability of many scheduling problems being established, polynomial algorithms guaranteeing a small deviation from the optimal schedule appear more attractive. Some polynomial algorithms are still very complex, <ref> [70] </ref>, while others are particular simple algorithms, like list scheduling methods [41,95]. The latter promises to be of the greatest help for the selection of scheduling methods in real systems.
Reference: 71. <author> U. Schwiegelshohn, </author> <title> "Preemptive weighted completion time scheduling of parallel jobs". </title> <booktitle> In 4th European Symp. Algorithms, </booktitle> <pages> pp. 39-51, </pages> <publisher> Springer-Verlag, </publisher> <month> Sep </month> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1136. </note>
Reference-contexts: Gang Scheduling. All active threads of a job are suspended and resumed simultaneously. Gang scheduling may be implemented with or without mi gration. While many theoretical scheduling studies only use a model without preemption, more recently preemption has also been taken into account. Schwiegelshohn <ref> [71] </ref> uses a gang scheduling model without migration. The work of Deng et al. [11] is based upon migratable preemption. <p> Preemption in real machines has an overhead cost, e.g. Motwani et al. [55] address the overhead by minimizing the number of preemptions. In order to compare a preemptive schedule with non-preemptive ones Schwiegelshohn <ref> [71] </ref> includes a time penalty for each preemption. 4. Amount of Job and Workload Knowledge Available Systems differ in the type, quantity, and accuracy of information available to and used by the scheduler.
Reference: 72. <author> U. Schwiegelshohn, W. Ludwig, J. L. Wolf, J. J. Turek, and P. Yu, </author> <title> "Smart SMART bounds for weighted response time scheduling". </title> <note> SIAM J. Comput. To appear. </note>
Reference-contexts: However, these high costs are only encountered for a few job systems which may never be part of real workloads. Turek et al. [89] proposed "SMART" schedules for the off-line non-preemptive completion time scheduling of parallel jobs. They prove an approximation factor of 8 <ref> [72] </ref> and give a worst case example with a deviation of 4.5. However, applying the algorithm on job systems obtained from the traces of the Intel Paragon at the San Diego Supercomputing Center gave an average deviation from the optimum by 2.
Reference: 73. <author> S. K. Setia, </author> <title> "The interaction between memory allocation and adaptive partitioning in message-passing multicomputers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 146-165, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 74. <author> S. K. Setia, M. S. Squillante, and S. K. Tripathi, </author> <title> "Analysis of processor allocation in multiprogrammed, distributed-memory parallel processing systems". </title> <journal> IEEE Trans. Parallel & Distributed Syst. </journal> <volume> 5(4), </volume> <pages> pp. 401-420, </pages> <month> Apr </month> <year> 1994. </year>
Reference-contexts: Setia, Squillante, and Tripathi <ref> [74] </ref> use a queuing theoretic model to investigate how parallel processing overheads cause efficiency to decrease with larger processor allocations. In a later study [75], they go on to show that dynamic partitioning of the system beats static partitioning at moderate and heavy loads.
Reference: 75. <author> S. K. Setia, M. S. Squillante, and S. K. Tripathi, </author> <title> "Processor scheduling on mul-tiprogrammed, distributed memory parallel computers". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 158-170, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Setia, Squillante, and Tripathi [74] use a queuing theoretic model to investigate how parallel processing overheads cause efficiency to decrease with larger processor allocations. In a later study <ref> [75] </ref>, they go on to show that dynamic partitioning of the system beats static partitioning at moderate and heavy loads.
Reference: 76. <author> K. C. Sevcik, </author> <title> "Application scheduling and processor allocation in multipro-grammed parallel processing systems". </title> <booktitle> Performance Evaluation 19(2-3), </booktitle> <pages> pp. 107-140, </pages> <month> Mar </month> <year> 1994. </year>
Reference-contexts: Both the mean and the variance of the response times are lower with the latter approach unless <ref> [76] </ref>: S (P ) &gt; 2 N + 2 S (P=2) This condition seldom holds when either the number of processors or the number of jobs is moderately large. <p> We suggest the name "bounded slowdown" for this metric, as it is similar to the slowdown metric, but bounded away from high values for very short jobs. Two possible roles for theory, that have relatively few parallels in practice, are how to use knowledge about specific jobs <ref> [76] </ref>, and how to tune algorithmic parameters [93]. In practice, knowledge about jobs is limited to that supplied by the users, typically in the form of choosing a queue with a certain combination of resource limits.
Reference: 77. <author> K. C. Sevcik, </author> <title> "Characterization of parallelism in applications and their use in scheduling". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Dynamically Changing A Job's Processor Allocation Because the efficiency of parallel jobs generally decreases as their processor allocation increases, it is necessary to decrease processor allocations to moldable jobs as the overall system load increases in order to avoid system saturation (see Sevcik <ref> [77] </ref>). Zahorjan and McCann [97] found that allocating processors to evolving jobs according to their dynamic needs led to much better performance than either run-to-completion with a rigid allocation or round-robin. For the overhead parameters they chose, round-robin beat run-to-completion only at quite low system loads. <p> Similar schemes in which co-scheduling is triggered by communication events were described by Sobalvarro and Weihl [83] and by Dusseau, Arpaci, and Culler [15]. Taking system load and minimum and maximum parallelism of each job into account as well, still higher throughputs can be sustained <ref> [77] </ref>.
Reference: 78. <author> D. Shmoys, J. Wein, and D. Williamson, </author> <title> "Scheduling parallel machines on-line". </title> <journal> SIAM J. Comput. </journal> <volume> 24(6), </volume> <pages> pp. 1313-1331, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: Presuming job knowledge in modeling studies sets a standard in performance against which practically realizable scheduling algorithms, which use class knowledge at most, can be compared. On-line scheduling has been addressed more frequently in recent years. For instance Shmoys et al. <ref> [78] </ref> discussed makespan scheduling if the job characteristics are not known until the release time and the execution time requirements of the job are also not available before the job has been executed to completion.
Reference: 79. <author> J. Skovira, W. Chan, H. Zhou, and D. Lifka, </author> <title> "The EASY - LoadLeveler API project". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 41-47, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference: 80. <author> D. D. Sleator and R. E. Tarjan, </author> <title> "Amortized efficiency of list update and paging rules". </title> <journal> Comm. ACM 28(2), </journal> <pages> pp. 202-208, </pages> <month> Feb </month> <year> 1985. </year>
Reference-contexts: The consideration of more complex constraints may make any general approximation algorithm impossible [42,47]. The evaluation of any scheduler can be either done by comparing its schedule against the optimal schedule or against schedules generated by other methods. Sleator and Tarjan <ref> [80] </ref> introduced the notion of competitive analysis. An on-line algorithm is competitive if it is guaranteed to produce a result that is within a constant factor of the optimal result.
Reference: 81. <author> E. Smirni, E. Rosti, G. Serazzi, L. W. Dowdy, and K. C. Sevcik, </author> <title> "Performance gains from leaving idle processors in multiprocessor systems". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <volume> vol. III, </volume> <pages> pp. 203-210, </pages> <month> Aug </month> <year> 1995. </year>
Reference-contexts: Algorithms that leave processors idle in anticipation of future arrivals were also investigated by Rosti et al. [69] and Smirni et al. <ref> [81] </ref>. The Need for Preemption A number of studies have demonstrated that despite the overheads of preemption, the flexibility derived from the ability to preempt jobs allows for much better schedules.
Reference: 82. <author> W. Smith, </author> <title> "Various optimizers for single-stage production". </title> <journal> Naval Research Logistics Quarterly 3, </journal> <pages> pp. 59-66, </pages> <year> 1956. </year>
Reference-contexts: The origin of these criteria often goes back to the fifties. For instance Smith <ref> [82] </ref> showed in 1956 that the sum of the weighted completion times for a system of jobs on a single processor can be minimized if the tasks are scheduled by increasing execution time to weight ratio, the so called Smith ratio.
Reference: 83. <author> P. G. Sobalvarro and W. E. Weihl, </author> <title> "Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 106-126, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Feitel-son and Rudolph [22] describe a discipline in which processes that communicate frequently are identified, and it is assured that the corresponding threads are all activated at the same time. Similar schemes in which co-scheduling is triggered by communication events were described by Sobalvarro and Weihl <ref> [83] </ref> and by Dusseau, Arpaci, and Culler [15]. Taking system load and minimum and maximum parallelism of each job into account as well, still higher throughputs can be sustained [77].
Reference: 84. <author> M. S. Squillante, </author> <title> "On the benefits and limitations of dynamic partitioning in parallel computer systems". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 219-238, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Several studies have revealed that EQUI does very well, even when some moderate charge for the overhead of frequent preemptions is made [86,48]. Squillante <ref> [84] </ref> provides an analysis of the performance of dynamic partitioning. Deng et al. show that EQUI is optimally competitive [11]. Dussa et al. [14] compares space-slicing against no partitioning, and finds that space-partitioning pays off.
Reference: 85. <author> M. S. Squillante, F. Wang, and M. Papaefthymiou, </author> <title> "Stochastic analysis of gang scheduling in parallel and distributed systems". </title> <booktitle> Performance Evaluation 27&28, </booktitle> <pages> pp. 273-296, </pages> <month> Oct </month> <year> 1996. </year>
Reference-contexts: Time slicing is typically implemented by gang scheduling, that is, all the threads in a job are scheduled (and de-scheduled) simultaneously. Gang scheduling is compared to local scheduling and is found to be superior by Feitelson and Rudolph [24]. Squillante et al. <ref> [85] </ref> and Wang et al. [94] have analyzed a variation of gang scheduling that involves providing service cyclically among a set of fixed partition configurations, each having a number of partitions equal to some power of two.
Reference: 86. <author> A. Tucker and A. Gupta, </author> <title> "Process control and scheduling issues for multipro-grammed shared-memory multiprocessors". </title> <booktitle> In 12th Symp. Operating Systems Principles, </booktitle> <pages> pp. 159-166, </pages> <month> Dec </month> <year> 1989. </year>
Reference: 87. <author> J. Turek, W. Ludwig, J. L. Wolf, L. Fleischer, P. Tiwari, J. Glasgow, U. Schwiegels-hohn, and P. S. Yu, </author> <title> "Scheduling parallelizable tasks to minimize average response time". </title> <booktitle> In 6th Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 200-209, </pages> <month> Jun </month> <year> 1994. </year>
Reference-contexts: Generally, a scheduler can start a job sooner if the job is moldable or even malleable than if it is rigid <ref> [87] </ref>. If jobs are moldable, then processor allocations can be selected in accordance with the current system load, which delays the onset of saturation as system load increases [25]. <p> For instance Kawaguchi and Kyan's LRF schedule [41] can be easily extended to parallel jobs. As long as no parallel job requires more than 50% of the processors, this will only increase the approximation factor from 1.21 to 2 <ref> [87] </ref>. However, if jobs requiring more processors are allowed in addition, no constant approximation factor can be guaranteed. 2.2 Some Specific Studies Workload Characterization Several workload characterization studies of production high-performance computing facilities have been carried out. They reveal characteristics of actual workloads that can be exploited in scheduling.
Reference: 88. <author> J. Turek, J. L. Wolf, and P. S. Yu, </author> <title> "Approximate algorithms for scheduling par-allelizable tasks". </title> <booktitle> In 4th Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 323-332, </pages> <month> Jun </month> <year> 1992. </year>
Reference-contexts: Many other authors use the variable partitioning paradigm, in which each job requires a specific number of processors but can be scheduled on any subset of processors of the system. An example of a theoretical study based on the adaptive approach is the work of Turek et al. <ref> [88] </ref>. Here, the application does not require a specific number of processors, but can use different numbers. However, once a partition for a job has been selected its size cannot change anymore. Finally, in dynamic partitioning the size of a partition may change at run time.
Reference: 89. <author> J. J. Turek, U. Schwiegelshohn, J. L. Wolf, and P. Yu, </author> <title> "Scheduling parallel tasks to minimize average response time". </title> <booktitle> In 5th ACM-SIAM Symp. Discrete Algorithms, </booktitle> <pages> pp. 112-121, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: In other words, up to 50% of the nodes of a multiprocessor system may be left idle. However, these high costs are only encountered for a few job systems which may never be part of real workloads. Turek et al. <ref> [89] </ref> proposed "SMART" schedules for the off-line non-preemptive completion time scheduling of parallel jobs. They prove an approximation factor of 8 [72] and give a worst case example with a deviation of 4.5. <p> Unfortunately, the optimal schedule cannot be obtained easily, but an analysis of an approximation algorithm can use lower bounds for the optimal schedule to determine the competitive factor, e.g. the squashed area bound introduced by Turek et al. <ref> [89] </ref>. Moreover, the theoretical analysis may be able to pinpoint the conditions which may lead to a bad schedule. These methods can also be applied to any practical approach and help to determine critical workloads.
Reference: 90. <author> C. A. Waldspurger and W. E. Weihl, </author> <title> "Lottery scheduling: flexible proportional-share resource management". </title> <booktitle> In 1st Symp. Operating Systems Design & Implementation, </booktitle> <pages> pp. 1-11, </pages> <publisher> USENIX, </publisher> <month> Nov </month> <year> 1994. </year>
Reference: 91. <author> C. A. Waldspurger, </author> <title> Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management. </title> <type> Ph.D. dissertation, </type> <institution> Massachusetts Institute of Technology, </institution> <type> Technical Report MIT/LCS/TR-667, </type> <month> Sep </month> <year> 1995. </year>
Reference: 92. <author> M. Wan, R. Moore, G. Kremenek, and K. Steube, </author> <title> "A batch scheduler for the Intel Paragon with a non-contiguous node allocation algorithm". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 48-64, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: Henderson [35] describes the Portable Batch System (PBS), another system in which performance gains are achieved by moving away from strict FCFS scheduling. Wan et al. <ref> [92] </ref> also implement a non-FCFS scheduler that uses a variation of a 2-D buddy system to do processor allocation for the Intel Paragon. <p> Throughput figures are hardly ever used. Reported utilization figures vary from 50% for the NASA Ames iPSC/860 hypercube [21], through around 70% for the CTC SP2 [37], 74% for the SDSC Paragon <ref> [92] </ref> and 80% for the Touchstone Delta [54], up to more than 90% for the LLNL Cray T3D [20]. Utilization figures in the 80-90% range are now becoming more common, due to the use of more elaborate batch queueing mechanisms [49,79,92] and gang scheduling [20].
Reference: 93. <author> F. Wang, H. Franke, M. Papaefthymiou, P. Pattnaik, L. Rudolph, and M. S. Squil-lante, </author> <title> "A gang scheduling design for multiprogrammed parallel computing environments". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 111-125, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: Two possible roles for theory, that have relatively few parallels in practice, are how to use knowledge about specific jobs [76], and how to tune algorithmic parameters <ref> [93] </ref>. In practice, knowledge about jobs is limited to that supplied by the users, typically in the form of choosing a queue with a certain combination of resource limits. This approach has two main drawbacks: first, it leads to a combinatorical explosion of queues, that are hard to deal with.
Reference: 94. <author> F. Wang, M. Papaefthymiou, and M. Squillante, </author> <title> "Performance evaluation of gang scheduling for parallel and distributed multiprogramming". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference-contexts: Time slicing is typically implemented by gang scheduling, that is, all the threads in a job are scheduled (and de-scheduled) simultaneously. Gang scheduling is compared to local scheduling and is found to be superior by Feitelson and Rudolph [24]. Squillante et al. [85] and Wang et al. <ref> [94] </ref> have analyzed a variation of gang scheduling that involves providing service cyclically among a set of fixed partition configurations, each having a number of partitions equal to some power of two.
Reference: 95. <author> Q. Wang and K. H. Cheng, </author> <title> "A heuristic of scheduling parallel tasks and its analysis". </title> <journal> SIAM J. Comput. </journal> <volume> 21(2), </volume> <pages> pp. 281-294, </pages> <month> Apr </month> <year> 1992. </year>
Reference: 96. <author> K. K. Yue and D. J. Lilja, </author> <title> "Loop-level process control: an effective processor allocation policy for multiprogrammed shared-memory multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 182-199, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Evolving jobs arise when applications go through distinct phases, and their natural parallelism is different in different phases. For such jobs, system calls are inserted at the appropriate points in the application code to indicate where parallelism changes <ref> [96] </ref>. Certain parallel applications, such as those based on the "work crew" model 2 , can be modified to be malleable relatively easily. <p> This is a result of using a model of evolving jobs, where it is best to leave these jobs space to grow. Yue <ref> [96] </ref> describes the creation of evolving jobs by selecting (in the compiler) at the top of each loop what degree of parallelism should be used for that loop.
Reference: 97. <author> J. Zahorjan and C. McCann, </author> <title> "Processor scheduling in shared memory multiprocessors". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 214-225, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Dynamically Changing A Job's Processor Allocation Because the efficiency of parallel jobs generally decreases as their processor allocation increases, it is necessary to decrease processor allocations to moldable jobs as the overall system load increases in order to avoid system saturation (see Sevcik [77]). Zahorjan and McCann <ref> [97] </ref> found that allocating processors to evolving jobs according to their dynamic needs led to much better performance than either run-to-completion with a rigid allocation or round-robin. For the overhead parameters they chose, round-robin beat run-to-completion only at quite low system loads.
References-found: 97

