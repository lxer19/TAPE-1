URL: http://www.cs.umn.edu/Research/Agassiz/Paper/li.register.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: An Evaluation of the Potential Benefits of Register Allocation for Array References  
Author: Zhiyuan Li Junjie Gu Gyungho Lee 
Keyword: Key phrases registers, array references, memory traces, simulations, compilers.  
Note: This work is supported in part by the National Science Foundation, Grant NSF/CCR-9210913 and a funding from Samsung Electronics.  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science Department of Electrical Engineering University of Minnesota  
Abstract: Array references are common in scientific and engineering programs. However, current compilers for scalar processors do not keep array elements in registers for reuse. This is mainly because it is unclear whether the potential benefits would justify the more difficult array data flow analysis and also because processor architectures have yet to provide efficient mechanisms for addressing array elements in registers. The goal of this paper is to evaluate the potential benefits of register allocation for array elements in scalar processors. Using trace-driven simulations of a set of benchmarking programs, which include nine from the Perfect Club Suite, four from SPEC92, and four from an image processing benchmark suite, we study the potential benefit. We relate the register reference percentage to the data reference characteristics of different programs and examine the effect of varying the number of registers from 16 to 512. We also discuss the effect of code reordering and compiler analysis scopes. The overall results indicate that allocating registers for array elements has significant potential benefits, especially when the number of registers increases to about 100. However, in order to achieve these benefits, compiler techniques must improve substantially. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.J. Chaitin, M.A. Auslander, A.K. Chandra, J. Cocke, M.E. Hopkins, and P.W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> Vol 6, No. 1, </volume> <month> January </month> <year> 1981, </year> <pages> PP 47-57. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions. <p> Recently, a few papers propose compiler techniques to allocate array elements. Callahan, Carr and Kennedy [7] propose the array scalarization technique to replace array references with semantically equivalent scalar references. The following gives a simple example: (before scalarization) (after scalarization) do i = 1,100 t10 = a <ref> [1] </ref> + x do i=2,100 enddo t12 = a [2] a [i+2] = t10 t11 = t10 t12 = t11 enddo In the above, the read of array a in the DO loop is replaced by a read of scalar t12.
Reference: [2] <author> G.J. Chaitin, </author> <title> Register allocation and spilling via coloring. </title> <booktitle> Proceedings of the ACM SIGPLAN '82 Symposium on compiler Construction, SIGPLAN Notices, </booktitle> <volume> Vol. 17, No. 6, </volume> <month> June </month> <year> 1982, </year> <pages> pp 98-105. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions. <p> Callahan, Carr and Kennedy [7] propose the array scalarization technique to replace array references with semantically equivalent scalar references. The following gives a simple example: (before scalarization) (after scalarization) do i = 1,100 t10 = a [1] + x do i=2,100 enddo t12 = a <ref> [2] </ref> a [i+2] = t10 t11 = t10 t12 = t11 enddo In the above, the read of array a in the DO loop is replaced by a read of scalar t12.
Reference: [3] <author> P. Briggs, K.D. Cooper, K. Kennedy, and L. Torcson. </author> <title> Coloring Heuristics for register allocation. </title> <booktitle> Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, SIGPLAN Notices, </booktitle> <volume> Vol. 24, No. </volume> <pages> 6, </pages> <address> June,1989, pp275-384. </address>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [4] <author> D. Bernstein, D.Q. Goldin, M.C. Golumbic, H. Krawczyk, Y. Mansour, I. Nahshon, and R.Y. Pinter. </author> <title> Spill code minimization techniques for optimizing compilers. </title> <booktitle> Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, SIGPLAN Notices, </booktitle> <volume> Vol. 24, No. 6, June,1989, </volume> <pages> pp 258-263. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [5] <author> F.C. Chow and J.L. Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> Proceedings of the ACM SIGPLAN'84 Symposium on Compiler Construction, SIGPLAN Notices, </booktitle> <volume> Vol. 19, No. 6, </volume> <month> June </month> <year> 1984, </year> <pages> pp 222-232. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [6] <author> F.C. Chow and J.L. Hennessy, </author> <title> The priority-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 12, No. 10, </volume> <month> October </month> <year> 1990, </year> <pages> pp 501-536. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [7] <author> D. Callahan, S. Carr and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> Proceedings of the ACM SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, New York,June 20-22,1990, </address> <pages> pp 53 - 65. </pages>
Reference-contexts: Currently, register addresses are fixed in the instructions, which are not adequate when the program needs to index a number of registers. Also, when the number of registers increases, the processor's data path becomes more complex. Three recent papers <ref> [7, 8, 9] </ref> address the software issue by proposing relatively simple forms of array data flow analysis to support allocating array elements to registers in limited classes of DO loops. <p> Current register allocation schemes are highly successful for scalar references. Difficulties arise when the allocation schemes deal with array references, whose exact data flow relationship can be quite elusive <ref> [7, 8, 9] </ref>. Given the limited number of available registers in most processors, reusing array data before they are replaced from the registers is also more difficult than reusing scalar data. <p> Given the limited number of available registers in most processors, reusing array data before they are replaced from the registers is also more difficult than reusing scalar data. Although array reuse is found to be quite often in a few short 2 benchmark kernels <ref> [7] </ref>, it is unclear whether this is the case when whole application programs are considered, To study the array reuse issue, we perform simulations driven by the traces from 17 sample programs written in Fortran 77. The sizes of these programs range from 60 lines to 6,000 lines. <p> Most compilers apply register allocators to scalars only. Recently, a few papers propose compiler techniques to allocate array elements. Callahan, Carr and Kennedy <ref> [7] </ref> propose the array scalarization technique to replace array references with semantically equivalent scalar references. <p> These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. <ref> [7] </ref> consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques [26, 27, 28, 29]. <p> Allocating arrays to registers across procedure boundaries is the most difficult. Current techniques are quite restrictive in this regard. No existing works allocate array references across procedure boundaries. Within procedure boundaries, all existing works handle innermost loops only <ref> [7, 8, 9] </ref>. The work by Callahan et al., however, also considers using loop interchange and unroll-and-jam to enhance the performance. We examine the effect of compiler analysis scope on the RRP by performing two additional sets of simulations for two kinds of limited scopes.
Reference: [8] <author> E. Duesterwald, R. Gupta, </author> <title> M.L. Soffa. A Practical Data Flow Framework for Array Reference Analysis and its Use in Optimizations. </title> <booktitle> Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, New Mexico, </address> <month> June, </month> <year> 1993, </year> <pages> pp. 68-77. </pages>
Reference-contexts: Currently, register addresses are fixed in the instructions, which are not adequate when the program needs to index a number of registers. Also, when the number of registers increases, the processor's data path becomes more complex. Three recent papers <ref> [7, 8, 9] </ref> address the software issue by proposing relatively simple forms of array data flow analysis to support allocating array elements to registers in limited classes of DO loops. <p> Current register allocation schemes are highly successful for scalar references. Difficulties arise when the allocation schemes deal with array references, whose exact data flow relationship can be quite elusive <ref> [7, 8, 9] </ref>. Given the limited number of available registers in most processors, reusing array data before they are replaced from the registers is also more difficult than reusing scalar data. <p> In the same paper, Callahan, Carr and Kennedy also present program transformation techniques such as loop interchange and unroll and jam to increase the opportunities for array reuse. Duesterwald, Gupta and Soffa <ref> [8] </ref> propose an array data flow analysis to allocate array references directly. Instead of scalarizing array references, they consider transferring array elements between memory and registers directly. The final machine code of the above example should be essentially the same for both this method and the scalarization method. <p> Instead of scalarizing array references, they consider transferring array elements between memory and registers directly. The final machine code of the above example should be essentially the same for both this method and the scalarization method. Both require transferring intermediate results in a pipelined fashion (called register pipelining in <ref> [8] </ref>.) Ensuring correct array data flow can be quite difficult if it involves complex control flow or complex array subscript patterns. The initial works in the above three papers focus on relatively simple forms of program constructs and array subscripts. <p> The initial works in the above three papers focus on relatively simple forms of program constructs and array subscripts. The work by Callahan et al. analyzes references in innermost loops only, and it considers data dependences which have constant distances. Duesterwald et al. <ref> [8] </ref> also consider innermost DO loops, but they allow more general array subscripts in the form of a fi i + b with induction variable i and integer constants a and b. A more recent paper [9] by Bodik and Gupta extends the work in [8] by dealing with IF statements <p> Duesterwald et al. <ref> [8] </ref> also consider innermost DO loops, but they allow more general array subscripts in the form of a fi i + b with induction variable i and integer constants a and b. A more recent paper [9] by Bodik and Gupta extends the work in [8] by dealing with IF statements in innermost DO loops. With more compiler designers showing interest in allocating registers for complex data types, one may expect the improvement of the compilers' capabilities. <p> Allocating arrays to registers across procedure boundaries is the most difficult. Current techniques are quite restrictive in this regard. No existing works allocate array references across procedure boundaries. Within procedure boundaries, all existing works handle innermost loops only <ref> [7, 8, 9] </ref>. The work by Callahan et al., however, also considers using loop interchange and unroll-and-jam to enhance the performance. We examine the effect of compiler analysis scope on the RRP by performing two additional sets of simulations for two kinds of limited scopes.
Reference: [9] <author> B. Bodik and R. Gupta. </author> <title> Array data flow analysis for load-store optimizations in superscalar architectures. </title> <booktitle> Eighth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, Ohio, </address> <month> August, </month> <year> 1995. </year>
Reference-contexts: Currently, register addresses are fixed in the instructions, which are not adequate when the program needs to index a number of registers. Also, when the number of registers increases, the processor's data path becomes more complex. Three recent papers <ref> [7, 8, 9] </ref> address the software issue by proposing relatively simple forms of array data flow analysis to support allocating array elements to registers in limited classes of DO loops. <p> Current register allocation schemes are highly successful for scalar references. Difficulties arise when the allocation schemes deal with array references, whose exact data flow relationship can be quite elusive <ref> [7, 8, 9] </ref>. Given the limited number of available registers in most processors, reusing array data before they are replaced from the registers is also more difficult than reusing scalar data. <p> Duesterwald et al. [8] also consider innermost DO loops, but they allow more general array subscripts in the form of a fi i + b with induction variable i and integer constants a and b. A more recent paper <ref> [9] </ref> by Bodik and Gupta extends the work in [8] by dealing with IF statements in innermost DO loops. With more compiler designers showing interest in allocating registers for complex data types, one may expect the improvement of the compilers' capabilities. <p> Allocating arrays to registers across procedure boundaries is the most difficult. Current techniques are quite restrictive in this regard. No existing works allocate array references across procedure boundaries. Within procedure boundaries, all existing works handle innermost loops only <ref> [7, 8, 9] </ref>. The work by Callahan et al., however, also considers using loop interchange and unroll-and-jam to enhance the performance. We examine the effect of compiler analysis scope on the RRP by performing two additional sets of simulations for two kinds of limited scopes.
Reference: [10] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the omega test. </title> <booktitle> In ACM SIGPLAN Conf. on Programming Languages Design and Implementation, </booktitle> <pages> pages 140-151, </pages> <month> June </month> <year> 1992. </year>
Reference: [11] <author> A.V. Aho, R. Sethi and J.D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: We adapt an existing trace generator [18] to the need of our simulation. The trace generator is instrumented in the Parafrase 2 Fortran program analyzer [19, 20]. The Parafrase 2 analyzer performs several source level transformations (including induction variable removal <ref> [11] </ref>), none of which should affect the simulation results in any significant way. Based on the information gathered by Parafrase 2, the trace generator produces a virtual address map for all program variables. Compiler-generated temporaries are excluded because they almost always reside in the registers even with few registers available. <p> reordering scheme and its results, we highlight several important points regarding the dependence graph as follows: * We require that the references represented by each node be moved as a whole during the reordering. * The trace before reordering is generated after source level transformations such as induction variable removal <ref> [11] </ref> have been performed. Thus, we allow code motion that is made possible through these source level transformations. * To make the size of the dependence graph manageable, we partition a trace into segments with a fixed size (20k statement instances each).
Reference: [12] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer architecture: A quantitative approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Hence, even though the MRU policy is ideal, compilers can approach its result quite closely for simple data types. For scalars without the aliasing problem, over 90% of register reference percentage (RRP) can be achieved <ref> [12, Chapter 3.7] </ref>. Therefore, we do not expect the MRU policy to skew the results for scalars significantly.
Reference: [13] <author> J. E. Smith and J. R. Goodman. </author> <title> Instruction cache replacement policies and organizations. </title> <journal> IEEE Transactions on Computer, </journal> <volume> Vol. c-34, No. 3, </volume> <month> March </month> <year> 1985, </year> <pages> pp. 234-241. </pages>
Reference-contexts: If the contents of two registers have equally remote uses, the one which does not need memory write-back is selected over the other. This Most-Remote-Use (MRU) policy is ideal as in the cases of memory page replacement and cache block replacement <ref> [13] </ref>, [14, Chapter 9.5.2]. Unlike operating systems and cache management hardware, however, compilers can approximate the MRU policy by statically predicting the most remote references (via the live variable analysis) and using the result to guide register replacement.
Reference: [14] <author> A. Silberschatz and P.B. Galvin. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: If the contents of two registers have equally remote uses, the one which does not need memory write-back is selected over the other. This Most-Remote-Use (MRU) policy is ideal as in the cases of memory page replacement and cache block replacement [13], <ref> [14, Chapter 9.5.2] </ref>. Unlike operating systems and cache management hardware, however, compilers can approximate the MRU policy by statically predicting the most remote references (via the live variable analysis) and using the result to guide register replacement.
Reference: [15] <author> P. Kolte and M.J. Harrold. </author> <title> Load/store range analysis for for global register allocation. </title> <booktitle> Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993, </year> <pages> pp 268-277. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [16] <author> A. Nicolau, R. Potasman, and H. Wang. </author> <title> Register allocation, renaming and their impact on fine-grain parallelism. Languages and Compilers for Parallel Computing. </title> <editor> U. Banerjee, etc. (Eds). </editor> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <pages> pp 218 - 235. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [17] <author> V. Santhanam and D. Odnert. </author> <title> Register allocation across procedure and module boundaries. </title> <booktitle> Proceedings of the ACM SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, New York,June 20-22,1990, </address> <pages> pp 28-39. </pages>
Reference-contexts: Such an assessment becomes increasingly important because of the growing number of numerical applications moving from vector supercomputers to scalar processors (on workstations and on multiprocessors). The problem of effectively allocating scalar values in registers has been extensively studied <ref> [1, 2, 5, 6, 4, 3, 15, 16, 17] </ref>. Typically, a register allocation scheme includes four steps: a live range analysis for different variables; constructing an interference graph; coloring the interference graph with a register spilling scheme; and inserting variable load/store and register write-back instructions.
Reference: [18] <author> T.N. Nguyen, Z. Li, and D.J. Lilja. </author> <title> Efficient Use of Dynamically Tagged Directories Through Compiler Analysis. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993, </year> <pages> pp II:102-110. </pages>
Reference-contexts: For several programs in Perfect and SPEC92 suites, we reduce the number of the outmost loop iterations which repeat similar operations on the same data <ref> [18] </ref> and normally encompass a large proportion of the program bodies. <p> The average of p s for these programs is around 62%. 5 2.2 A trace-driven simulation The simulator used in this study is driven by memory reference traces of the selected programs. We adapt an existing trace generator <ref> [18] </ref> to the need of our simulation. The trace generator is instrumented in the Parafrase 2 Fortran program analyzer [19, 20]. The Parafrase 2 analyzer performs several source level transformations (including induction variable removal [11]), none of which should affect the simulation results in any significant way.
Reference: [19] <author> C.D. Polychronopoulos, M.B. Girkar, M.R. Haghighat, L. Lee, B.P. Leung, and D.A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on Multiprocessors. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989, </year> <pages> pp 39-48. </pages>
Reference-contexts: We adapt an existing trace generator [18] to the need of our simulation. The trace generator is instrumented in the Parafrase 2 Fortran program analyzer <ref> [19, 20] </ref>. The Parafrase 2 analyzer performs several source level transformations (including induction variable removal [11]), none of which should affect the simulation results in any significant way. Based on the information gathered by Parafrase 2, the trace generator produces a virtual address map for all program variables.
Reference: [20] <author> M.B. Girkar, M. Haghighat, C.L. Lee, B.P. Leung, and D.A. Schouten. </author> <title> Parafrase-2 Programmer's Manual. </title> <month> November </month> <year> 1990. </year>
Reference-contexts: We adapt an existing trace generator [18] to the need of our simulation. The trace generator is instrumented in the Parafrase 2 Fortran program analyzer <ref> [19, 20] </ref>. The Parafrase 2 analyzer performs several source level transformations (including induction variable removal [11]), none of which should affect the simulation results in any significant way. Based on the information gathered by Parafrase 2, the trace generator produces a virtual address map for all program variables.
Reference: [21] <author> Edited by P. Sinvhal-Sharma. </author> <title> Perfect Benchmarks Documentation: </title> <type> Suite 1. </type> <institution> CSRD, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The sizes of these programs range from 60 lines to 6,000 lines. These programs include nine from the Perfect Club benchmarking suite <ref> [21, 22] </ref>, four from the SPEC92 Floating Point Benchmark Suite [35], and four from an image processing benchmark suite obtained from Computing Devices International (CDI).
Reference: [22] <author> M. Berry, </author> <title> etc. The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> Vol. 3, No. 3, </volume> <year> 1989, </year> <pages> pp 5-40. </pages>
Reference-contexts: The sizes of these programs range from 60 lines to 6,000 lines. These programs include nine from the Perfect Club benchmarking suite <ref> [21, 22] </ref>, four from the SPEC92 Floating Point Benchmark Suite [35], and four from an image processing benchmark suite obtained from Computing Devices International (CDI).
Reference: [23] <author> D.E. Knuth. </author> <booktitle> The Art of Computer Programming. </booktitle> <volume> Vol. 1, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: [24] <author> D. Kuck. </author> <title> The Structure of Computers and Computation. </title> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year> <month> 32 </month>
Reference-contexts: In order to eliminate the artificial restrictions on the reordering due to storage conflicts, our reordering algorithm first transforms the whole trace to be in a single assignment form via a thorough renaming such that each variable is modified only once. This renaming eliminates all anti-and output dependences <ref> [24, 25] </ref> and thus creates more opportunities for reordering. However, all flow dependences [24] remain and therefore the data flow stands correct. The reordering algorithm then constructs a data dependence graph, which is acyclic, for the trace. Each node in the dependence graph represents a statement instance. <p> This renaming eliminates all anti-and output dependences [24, 25] and thus creates more opportunities for reordering. However, all flow dependences <ref> [24] </ref> remain and therefore the data flow stands correct. The reordering algorithm then constructs a data dependence graph, which is acyclic, for the trace. Each node in the dependence graph represents a statement instance.
Reference: [25] <author> R. Cytron, and J. Ferrante. </author> <title> What's in a name? Or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <year> 1987, </year> <month> August, </month> <pages> pp 19-27. </pages>
Reference-contexts: In order to eliminate the artificial restrictions on the reordering due to storage conflicts, our reordering algorithm first transforms the whole trace to be in a single assignment form via a thorough renaming such that each variable is modified only once. This renaming eliminates all anti-and output dependences <ref> [24, 25] </ref> and thus creates more opportunities for reordering. However, all flow dependences [24] remain and therefore the data flow stands correct. The reordering algorithm then constructs a data dependence graph, which is acyclic, for the trace. Each node in the dependence graph represents a statement instance.
Reference: [26] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> Ph.D. thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <note> DCS Report No. UIUCDCS-R-82-1105, </note> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques <ref> [26, 27, 28, 29] </ref>. The work by Carr, McKinley and Tseng [34] reports that it is rather difficult to improve the data locality of benchmark programs, including the Perfect programs and the SPEC92 programs, via program transformations, because writers of benchmark programs tend to already consider the data locality issue.
Reference: [27] <author> M. J. Wolfe. </author> <title> Advanced Loop Interchange. </title> <booktitle> Proceedings of the 1986 Int'l Conf. on Parallel Processing, </booktitle> <pages> pp. 536-543, </pages> <year> 1986. </year>
Reference-contexts: Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques <ref> [26, 27, 28, 29] </ref>. The work by Carr, McKinley and Tseng [34] reports that it is rather difficult to improve the data locality of benchmark programs, including the Perfect programs and the SPEC92 programs, via program transformations, because writers of benchmark programs tend to already consider the data locality issue.
Reference: [28] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4, </volume> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques <ref> [26, 27, 28, 29] </ref>. The work by Carr, McKinley and Tseng [34] reports that it is rather difficult to improve the data locality of benchmark programs, including the Perfect programs and the SPEC92 programs, via program transformations, because writers of benchmark programs tend to already consider the data locality issue.
Reference: [29] <author> W. Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> Proceedings of ACM Int'l Conference on Supercomputing, </booktitle> <month> June, </month> <year> 1991. </year>
Reference-contexts: Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques <ref> [26, 27, 28, 29] </ref>. The work by Carr, McKinley and Tseng [34] reports that it is rather difficult to improve the data locality of benchmark programs, including the Perfect programs and the SPEC92 programs, via program transformations, because writers of benchmark programs tend to already consider the data locality issue.
Reference: [30] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, No. 5, </volume> <pages> pp. 587-616. </pages>
Reference-contexts: To better understand the issue, we also examine the source code of a few small but important routines to find opportunities for source-level transformations. Many researchers have discussed compile time reordering techniques for data locality enhancement, especially for increasing cache or local memory hits <ref> [30, 31, 32, 33, 34] </ref>. These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation.
Reference: [31] <author> M. Lam, E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> Proceedings of the Fourth Int'l Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: To better understand the issue, we also examine the source code of a few small but important routines to find opportunities for source-level transformations. Many researchers have discussed compile time reordering techniques for data locality enhancement, especially for increasing cache or local memory hits <ref> [30, 31, 32, 33, 34] </ref>. These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation.
Reference: [32] <author> M.E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> Proceedings of the SIGPLAN'91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: To better understand the issue, we also examine the source code of a few small but important routines to find opportunities for source-level transformations. Many researchers have discussed compile time reordering techniques for data locality enhancement, especially for increasing cache or local memory hits <ref> [30, 31, 32, 33, 34] </ref>. These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation.
Reference: [33] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <booktitle> Proceedings of the Fifth Int'l Conference on Architectural Support for Programming languages and Operating Systems, </booktitle> <address> Bonston, MA, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: To better understand the issue, we also examine the source code of a few small but important routines to find opportunities for source-level transformations. Many researchers have discussed compile time reordering techniques for data locality enhancement, especially for increasing cache or local memory hits <ref> [30, 31, 32, 33, 34] </ref>. These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation.
Reference: [34] <author> S. Carr , K. S. McKinley and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> Proceedings of Sixth Int'l Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> Vol. 29, No. 11, </volume> <pages> pp. 252-262. </pages>
Reference-contexts: To better understand the issue, we also examine the source code of a few small but important routines to find opportunities for source-level transformations. Many researchers have discussed compile time reordering techniques for data locality enhancement, especially for increasing cache or local memory hits <ref> [30, 31, 32, 33, 34] </ref>. These include loop interchange, loop tiling (also known as loop blocking or unroll-and-jam), statement reordering, loop reversal, loop fusion and loop peeling. Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. <p> Callahan et al. [7] consider loop interchange and unroll-and-jam to improve register allocation. Data dependences restrict the validity of such techniques and therefore must be examined before applying the techniques [26, 27, 28, 29]. The work by Carr, McKinley and Tseng <ref> [34] </ref> reports that it is rather difficult to improve the data locality of benchmark programs, including the Perfect programs and the SPEC92 programs, via program transformations, because writers of benchmark programs tend to already consider the data locality issue. <p> For a recent discussion of these transformation techniques, the readers are referred to <ref> [34] </ref>. In order to apply these techniques to the source code of the subroutines in Table 4, some minor statement reordering is required in a few cases and certain IF statements may also need special handling. We omit the case-by-case discussions here.
Reference: [35] <author> Standard Performance Evaluation Corporation (SPEC), </author> <title> SPEC Component CPU Floating Point Release 2/1992 (CFP92), </title> <month> January 15, </month> <year> 1992. </year> <month> 33 </month>
Reference-contexts: The sizes of these programs range from 60 lines to 6,000 lines. These programs include nine from the Perfect Club benchmarking suite [21, 22], four from the SPEC92 Floating Point Benchmark Suite <ref> [35] </ref>, and four from an image processing benchmark suite obtained from Computing Devices International (CDI).
References-found: 35

