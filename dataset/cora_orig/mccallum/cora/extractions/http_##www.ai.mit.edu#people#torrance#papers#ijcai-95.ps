URL: http://www.ai.mit.edu/people/torrance/papers/ijcai-95.ps
Refering-URL: http://www.ai.mit.edu/people/torrance/papers/papers.html
Root-URL: 
Title: Natural Communication with Mobile Robots  
Author: Mark C. Torrance AP Group 
Keyword: Robotic User Interfaces, Natural Language in Embedded Systems  
Address: 545 Technology Square #705 Cambridge, MA 02139 USA  
Affiliation: MIT Artificial Intelligence Laboratory  
Email: torrance@ai.mit.edu  
Phone: (617) 471-7517  
Date: January 6, 1995  
Abstract: We describe an implemented system which mediates between an unmodified reactive mobile robot architecture and domain-restricted natural language. We introduce reactive-odometric plans and demonstrate their use in plan recognition, fault-tolerant plan execution, and learning to associate human terms with locations in the environment which are perceptually unremarkable to the robot. The communication component of our architecture supports typewritten natural language discourse with people. It lets users name places either immediately or in relation to other known places, ask questions about the robot's plans and the spatial relationships of known places, and give the robot short and long term goals. We have implemented this architecture in a physical mobile robot system, and describe experiments performed in several environments and with several different users. This paper has not already been accepted by, and is not currently under review for, a journal or another conference. Nor will it be submitted for such during IJCAI's review period. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ronald C. Arkin. </author> <title> Motor schema-based mobile robot navigation. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 8(4):92 - 112, </volume> <month> August </month> <year> 1989. </year>
Reference-contexts: Kuipers and Byun introduce the spatial semantic hierarchy which uses hill-climbing in sensor space to define distinctive landmarks [8]; this architecture is good for point-to-point navigation among places that are perceptually distinctive, though to our knowledge it has so far been demonstrated only in simulation. Arkin <ref> [1] </ref> describes a robot control architecture based on motor schemas which define potential fields guiding reactive progress to achieve each leg of a piecewise linear path developed by a navigation planner. Horswill presents a place recognition algorithm that combines vision and odometry, implemented on a mobile robot [5].
Reference: [2] <author> Jonathan H. Connell. </author> <title> SSS: A hybrid architecture applied to robot nav 14 igation. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2719-2724, </pages> <address> Nice, France, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: on include "You are at Pat's office," "Where is Chris's demo room," "Go to the mail chute," and "How would I get from the playroom to the copier room?" This paper describes results obtained with our implementation of this architecture on a physical mobile robot system designed by Jonathan Connell <ref> [2] </ref>. These results reflect experiments performed by the author and by other users. Our primary contribution is in the representations for spatial plans we have developed, and our analysis of the way these representations mediate between communication and reactive navigation to solve the task. <p> The reactive robot, TJ, was designed and 2 implemented by Jonathan Connell <ref> [2] </ref>, and generously loaned to MIT by IBM T.J. Watson Research Center. It includes the Servo and Subsumption layers of Connell's implementation of his SSS architecture, implemented in Motorola 6811 assembly language on a distributed network of 5 processors that reside on the robot. <p> There have been numerous more recent efforts to develop architectures useful for interactive robot control. These include Firby's Reactive Action Packages (RAPs) [3], Gat's ATLANTIS architecture [4], and Connell's SSS architecture <ref> [2] </ref>. Each of these architectures provides much more sophisticated condition-monitoring and error recovery mechanisms than the architecture we present here.
Reference: [3] <author> R. James Firby. </author> <title> Adaptive execution in complex dynamic worlds. </title> <type> Technical Report YALEU/CSD/RR-672, </type> <institution> Yale University, </institution> <year> 1989. </year>
Reference-contexts: There have been numerous more recent efforts to develop architectures useful for interactive robot control. These include Firby's Reactive Action Packages (RAPs) <ref> [3] </ref>, Gat's ATLANTIS architecture [4], and Connell's SSS architecture [2]. Each of these architectures provides much more sophisticated condition-monitoring and error recovery mechanisms than the architecture we present here.
Reference: [4] <author> Erann Gat. </author> <title> Reliable Goal-Directed Reactive Control of Autonomous Mobile Robots. </title> <type> PhD thesis, </type> <institution> Virginia Polytechnic Institute and State University, </institution> <year> 1991. </year>
Reference-contexts: There have been numerous more recent efforts to develop architectures useful for interactive robot control. These include Firby's Reactive Action Packages (RAPs) [3], Gat's ATLANTIS architecture <ref> [4] </ref>, and Connell's SSS architecture [2]. Each of these architectures provides much more sophisticated condition-monitoring and error recovery mechanisms than the architecture we present here.
Reference: [5] <author> Ian Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Arkin [1] describes a robot control architecture based on motor schemas which define potential fields guiding reactive progress to achieve each leg of a piecewise linear path developed by a navigation planner. Horswill presents a place recognition algorithm that combines vision and odometry, implemented on a mobile robot <ref> [5] </ref>. In each of the systems described in the preceding paragraph, the places defined are either perceptually distinctive to the robot or are "taught" in a hardcoded, static way (i.e. at system-design time).
Reference: [6] <author> Scott B. Huffman and John E. Laird. </author> <title> Learning procedures from interactive natural language instructions. </title> <booktitle> In Proceedings of the Tenth International Workshop on Machine Learning, </booktitle> <pages> pages 143-150, </pages> <address> Los Altos, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our unusual grounding of string descriptions for places in terms of robot plans to get to those places leverages the static nature of the environment to finesse the perception problem for this limited but important task (although it introduces representational difficulties of its own). Huffman and Laird <ref> [6, 7] </ref> present Instructo-Soar, a system which learns new procedures from sequences of instruction and also learns how to extend its knowledge of previously known procedures to new situations. The system has not, to our knowledge, been applied to the robot navigation domain or to any physical robot.
Reference: [7] <author> Scott B. Huffman, Craig S. Miller, and John E. Laird. </author> <title> Learning from instruction: A knowledge-level capability within a unified theory of cog-nition. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 114-119. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1993. </year>
Reference-contexts: Our unusual grounding of string descriptions for places in terms of robot plans to get to those places leverages the static nature of the environment to finesse the perception problem for this limited but important task (although it introduces representational difficulties of its own). Huffman and Laird <ref> [6, 7] </ref> present Instructo-Soar, a system which learns new procedures from sequences of instruction and also learns how to extend its knowledge of previously known procedures to new situations. The system has not, to our knowledge, been applied to the robot navigation domain or to any physical robot.
Reference: [8] <author> Benjamin Kuipers and Yung-Tai Byun. </author> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 8 </volume> <pages> 47-63, </pages> <year> 1991. </year>
Reference-contexts: Each of these architectures provides much more sophisticated condition-monitoring and error recovery mechanisms than the architecture we present here. Kuipers and Byun introduce the spatial semantic hierarchy which uses hill-climbing in sensor space to define distinctive landmarks <ref> [8] </ref>; this architecture is good for point-to-point navigation among places that are perceptually distinctive, though to our knowledge it has so far been demonstrated only in simulation.
Reference: [9] <author> Charles Martin. </author> <title> The situated language project. </title> <institution> Presented at the Massachusetts Institute of Technology Artificial Intelligence Laboratory Revolving Seminar, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Likewise, none of these architectures are designed to support online learning of new important places (even with nonlinguistic communication); this is a key feature of our system. Martin and Firby are exploring an exciting new approach to integrating natural language and reactive agent-control systems <ref> [9] </ref>. This work turns existing natural language systems on end, being data-driven and generating expectations about the continuation of utterances based on the current linguistic and environmental context.
Reference: [10] <author> Nils J. Nilsson. </author> <title> Shakey the robot. </title> <type> Technical Note 323, </type> <institution> SRI International, </institution> <address> Menlo Park, CA, </address> <month> April </month> <year> 1984. </year>
Reference-contexts: Related work by this author has explored this kind of plan queueing from natural language in a realistically simulated mobile robot domain [11]. 6 Related Work Shakey the robot <ref> [10] </ref> was perhaps the paradigmatic system to connect natural language instruction to action in a mobile robot. Its operation was, however, far from real-time or interactive. It moved slowly when it moved, and deliberated for tens of minutes on end between moves.
Reference: [11] <author> C. Rich, R. C. Waters, C. Strohecker, Y. Schabes, W. T. Freeman, M. C. Torrance, A. R. Golding, and M. Roth. </author> <title> An animated online community with artificial agents and spoken interaction. </title> <journal> IEEE MultiMedia, </journal> <volume> 1(4), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: We believe this initial work points the way toward a capability to learn plans from language while not simultaneously executing them, for possible later use. Related work by this author has explored this kind of plan queueing from natural language in a realistically simulated mobile robot domain <ref> [11] </ref>. 6 Related Work Shakey the robot [10] was perhaps the paradigmatic system to connect natural language instruction to action in a mobile robot. Its operation was, however, far from real-time or interactive. It moved slowly when it moved, and deliberated for tens of minutes on end between moves.
Reference: [12] <author> Mark C. Torrance. </author> <title> Natural communication with robots. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, Cambridge, Massachusetts, </institution> <month> February </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Our primary contribution is in the representations for spatial plans we have developed, and our analysis of the way these representations mediate between communication and reactive navigation to solve the task. This work is described in much greater detail in <ref> [12] </ref>. The paper is organized as follows. Section 2 provides an overview of the system and the representations which drive its operation, and includes an extended annotated example which illustrates the system in action. <p> New concepts are introduced where they are first needed. The scenario below takes place in the environment depicted in Figure 3 B. TJ has been run in environments of significantly greater complexity, including multi-step rops and paths with irregular angles; transcripts of these experiments are given in <ref> [12] </ref>. In this example, Pat's office is at the top, or north. <p> This is because the primary contribution of the system is in the domain of usability for mobile robots, and there are no comparable systems available. We have documented the performance of the system in a number of ways, including videotape and annotated transcripts <ref> [12] </ref>. In addition, the system has been exercised in several different parts of the author's research facility by novice and expert users. <p> Its operation was, however, far from real-time or interactive. It moved slowly when it moved, and deliberated for tens of minutes on end between moves. Furthermore, its place and object recognition strategy involved human-like perception and 12 building corresponding representations, which we have argued in <ref> [12] </ref> is quite challenging, and indeed overkill for the type of application we target. There have been numerous more recent efforts to develop architectures useful for interactive robot control. These include Firby's Reactive Action Packages (RAPs) [3], Gat's ATLANTIS architecture [4], and Connell's SSS architecture [2].
References-found: 12

