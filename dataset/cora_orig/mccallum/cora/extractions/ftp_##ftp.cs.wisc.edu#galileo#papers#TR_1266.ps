URL: ftp://ftp.cs.wisc.edu/galileo/papers/TR_1266.ps
Refering-URL: http://www.cs.wisc.edu/~alain/publications.html
Root-URL: 
Email: -alain, dburger, goodman-@cs.wisc.edu, nagi@cs.uno.edu  
Title: An Analysis of the Interactions of Overhead-Reducing Techniques for Shared-Memory Multiprocessors  
Author: Alain Kgi, Nagi Aboulenein, Douglas C. Burger, James R. Goodman 
Keyword: Abstract  
Address: 1210 West Dayton Street Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This work is supported in part by NSF Grant CCR-9207971, an unrestricted grant from the Apple Computer Advanced Technology Group, and donations from Thinking Machines Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618, with matching funding from the University of Wisconsin Graduate School. 1995 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. A version of this paper appears in: International Conference on Supercomputing, July 1995. Reprinted by permission of ACM. The fine-grain nature of shared-memory multiprocessor communication introduces overheads that can be substantial. Using the Scalable Coherent Interface (SCI) as a base hardware platform and the SPLASH benchmark suite for applications, we analyze three techniques to reduce this overhead: (i) efficient synchronization primitives, and in particular a hardware primitive called QOLB; (ii) weakened memory ordering constraints; and (iii) optimization of the cache-coherence protocol for two nodes sharing data. We perform simulations both for current technology and technology that we anticipate will be available five years hence. We find that QOLB (of which this study performs the first detailed simulations) shows a large and consistent improvement, much larger than that predicted by Mellor-Crummey and Scott [20]. The relaxation of memory ordering constraints also provides a consistent performance improvement. In accordance with prior results, we show that a more aggressive memory model produces more substantial performance improvements. The optimization for two-node sharing shows mixed results, correlating unsurprisingly with the presence of that sharing pattern in an application. Our most important results are (i) that the overheads eliminated with these optimizations are largely orthogonalthe performance gains from supporting multiple optimizations concurrently are for the most part additiveand (ii) that technological improvements increase both these overheads and the success of the optimizations at reducing them. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Scalable Coherent Interface (SCI). </institution> <address> ANSI/IEEE Std 1596-1992, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In order to establish a complete and detailed environment, we simulate a target system of 32 nodes providing hardware-guaranteed cache coherence by means of the ANSI/IEEE standard 1596 Scalable Coherent Interface (SCI) <ref> [1] </ref>. The target system consists of workstation-like nodes possessing a processor, cache memory, transaction queue (similar to a functionally-extended write buffer), network interface, and some fraction of the distributed, globally-shared memory with the associated directory entries (see Figure 1). <p> The internal details of the simulated network correspond closely to those of the SCI transport layer standard. A messages delay through the network includes staging time at the source and target nodes, parsing and wire delay through each intermediate node, and possibly a delay through an agent queue <ref> [1] </ref>, if the message switches dimensions. Table 2 lists the specific times for these delays, for both current and future networks. Table 3 shows the errors (in terms of target execution time) that the constant latency network model suffers when compared against the detailed network simulation.
Reference: [2] <author> Nagi M. Aboulenein, James R. Goodman, Stein Gjessing, and Philip J. Woest. </author> <title> Hardware Support for Synchronization in the Scalable Coherent Interface (SCI). </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 141150, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These algorithms are also unable to prefetch data without extending them and adding significantly to their complexity. Aboulenein et al. <ref> [2] </ref> showed that Andersons solution performs no better than the MCS solution; therefore in this study we restrict ourselves to comparing MCS with QOLB. <p> If we combine the speedups resulting from release consistency with those resulting from simple sequentially consistent transaction queues (we support asynchronous ushes in the base case), we obtain results comparable to those reported by Gupta et al. Aboulenein et al. <ref> [2] </ref> present a detailed analysis study of the QOLB synchronization primitive. A QOLB implementation in the framework of the Scalable Coherent Interface (SCI) is presented.
Reference: [3] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering A New Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Most memory operations could in fact be completed out of order without affecting the program result. The identification of those which could affect the result is difficult, however, as synchronization through shared variables can be extremely subtle. Memory modelssuch as data-race-free-0 <ref> [3] </ref> and release consistency [13]allow the system to relax the constraints of sequential consistency by creating a contract between the software and the hardware, defining what memory orderings are legal. Implementations may then be conservative or aggressive in supporting the memory model.
Reference: [4] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin-Waiting Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, volume II Software, </booktitle> <pages> pages 170174, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Mellor-Crummey and Scott (MCS) implement a queue as a linked list, and use atomic operations such as swap or compare-and-swap to update the list correctly [19]. Anderson presented a scheme that implements a queue as a circular array <ref> [4] </ref>. Inspired by QOLB, these algorithms also reduce the network traffic to a constant number of traversals per synchronization access, allowing processors to spin locally while waiting for the release of the lock.
Reference: [5] <author> Douglas C. Burger and James R. Goodman. </author> <title> Simulation of the SCI Transport Layer on the Wisconsin Wind Tunnel. </title> <type> Technical Report 1265, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: To validate this process, we used a detailed, event-driven SCI network simulator (based on the original WWT network simulator [6]) that accurately simulates message buffering, message retransmission, and ow control <ref> [5] </ref>. The implementation serializes the network simulation at a central node, making simulation performance suffer by roughly a factor of 15. The target network that we used to derive the validation was an mesh of rings.
Reference: [6] <author> Douglas C. Burger and David A. Wood. </author> <title> Accuracy vs. Performance in Parallel Simulation of Interconnection Networks. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This assumption of constant latency provides sufficient lookahead at each node to allow efficient parallel simulation. Reducing the minimum end-to-end network latency reduces the node lookahead, which causes severe increases in simulation time <ref> [6] </ref>. The constant latency assumption ignores network contention, which can play a pivotal role in evaluating various optimizations. Optimizations that reduce target execution time without a corresponding reduction in communication raise the effective load on the network. Other optimizations that reduce the number of messages lower the offered load. <p> We iterated this process until the difference between the network latency constant and the value produced by the model for that run converged to within one cycle per message. To validate this process, we used a detailed, event-driven SCI network simulator (based on the original WWT network simulator <ref> [6] </ref>) that accurately simulates message buffering, message retransmission, and ow control [5]. The implementation serializes the network simulation at a central node, making simulation performance suffer by roughly a factor of 15. The target network that we used to derive the validation was an mesh of rings.
Reference: [7] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 152 164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 25] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [17] and producer-consumer data sharing.
Reference: [8] <institution> Convex Computer Corporation, Richardson, Texas. SPP1000 Systems Overview, </institution> <year> 1994. </year>
Reference-contexts: The establishment of standardssuch as the IEEE Scalable Coherent Interface [1]has resulted from this growing industrial inertia. Consequently, parts are becoming available that integrate entire aspects of these standards, reducing system design complexity, time-to-market, and total system cost. Convex based their Exemplar system <ref> [8] </ref>, for instance, largely on third-party components. The growing number of bus-based shared-memory systems will further strengthen the success of the shared-memory processing paradigm. The increasing prevalence of these systems will create a large base of parallel applications, which should ease the acceptance of larger-scale shared-memory systems.
Reference: [9] <author> Alan L. Cox and Robert J. Fowler. </author> <title> Adaptive Cache Coherency for Detecting Migratory Shared Data. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98108, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 25] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [17] and producer-consumer data sharing. <p> This analysis shows that QOLB outperforms both of these algorithms, both in terms of interconnect messages and memory accesses needed to gain access to a critical section. Cox and Fowler <ref> [9] </ref>, and Stenstrm, Brorsson, and Sandberg [25] present studies that propose different solutions for dealing with the problem of migratory sharing patterns. Both studies present adaptive schemes that can be implemented by a hardware cache coherence protocol.
Reference: [10] <institution> Cypress Semiconductor, </institution> <address> San Jose, California. </address> <note> CY7C601 SPARC RISC Users Guide, second edition, </note> <year> 1990. </year>
Reference-contexts: The first class of optimizations we study is improved synchronization primitives. A straightforward approach to building synchronization functions uses instructions provided by the commodity microprocessor (such as the atomic swap instruction in the SPARC instruction set <ref> [10] </ref>) in much the same way as uniprocessor platforms use them. Typically, the processor accesses a lock repeatedly until the processor finds it unlocked. On a multiprocessor, these repeated accesses often translate directly into network traffic that leads to heavy network contention and potentially severe performance degradation. <p> The simulation target is a 32-node shared-memory multiprocessor supporting the SCI cache-coherence protocol. WWT executes SPARC binaries, and assumes fixed execution time for the instructions (the actual values correspond to the instruction delays listed by the CY7C601 SPARC users guide <ref> [10] </ref>). The execution times for EnQOLB and DeQOLB (executed at the entry and the exit of a critical section) are 3 and 2 cycles respectively. Both instruction and stack accesses to the cache are not simulated in WWT; they are assumed always to hit.
Reference: [11] <author> Frederick Dahlgren, Michel Dubois, and Per Stenstrm. </author> <title> Combined Performance Gains of Simple Cache Protocol Extensions. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 187197, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 25] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [17] and producer-consumer data sharing.
Reference: [12] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Programming for Different Memory Consistency Models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4):399407, </volume> <year> 1992. </year>
Reference-contexts: We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. [13]. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo <ref> [12, 13] </ref>. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing [15].
Reference: [13] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Philip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1526, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Sequential consistency is overly restrictive with respect to multiprocessor memory orderings. Systems can achieve higher performance by relaxing the memory orderings, without compromising the correctness of the program. The class of weakened consistency models that we implemented belong to the release consistency model <ref> [13] </ref>, which divides groups of memory accesses with acquire, release, and special accesses. So long as a program obeys the rules specified by this model, many memory accesses can bypass others, allowing the processor to tolerate the longer latencies associated with remote transactions. <p> A description of these benchmarks appears in the original article [24]. We focus the following discussion on specifics related to our study. We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. <ref> [13] </ref>. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo [12, 13]. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. <p> We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. [13]. We then inserted memory fences to achieve release consistency on our simulated hardware platform. The memory fences are consistent with those proposed by Gharachorloo <ref> [12, 13] </ref>. We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing [15].
Reference: [14] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> A Set of Efficient Synchronization Primitives for a Large-Scale Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Third Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6475, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Typically, the processor accesses a lock repeatedly until the processor finds it unlocked. On a multiprocessor, these repeated accesses often translate directly into network traffic that leads to heavy network contention and potentially severe performance degradation. We therefore compare two more advanced primitives, QOLB <ref> [14] </ref> and MCS locks [20]. The second class of optimizations that we examine consists of a range of memory models. Programmers naturally assume a memory model formally called sequential consistency, defined by Lam 2 port [18]. <p> Therefore, we have attempted to estimate performance by adjusting relative delays for instruction times versus memory access times and network delays. The main contributions of this paper are threefold. We present the first quantitative performance analysis of the QOLB synchronization primitive <ref> [14] </ref>. We analyze the subtle interaction of QOLB and other synchronization primitives with weakened memory ordering constraints. Finally, we demonstrate the effect of advancing technology on the three machine optimizations in general, with an emphasis on QOLB. <p> Goodman, Vernon, and Woest proposed the Queue-On-Lock-Bit primitive (QOLBorigi-nally called QOSB) as a hardware solution to this problem <ref> [14] </ref>. QOLB provides a direct implementation of a binary semaphore (with approximate first-come-first-serve service) by building a hardware queue of waiting processors. It avoids unnecessary network traffic as waiting processors spin locally, repeatedly accessing a local shadow copy of the locks cache line without generating network traffic.
Reference: [15] <author> James R. Goodman and Philip J. Woest. </author> <title> The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 422431, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: SCI does not specify the memory model; instead, it permits a variety of models to be supported, including sequential consistency. The SCI standard allows the processor to continue execu tion concurrently while multiple, sequential transactions are in struct _locked_data - int lock; int data <ref> [15] </ref>; -; void critical_section (struct _locked_data *x) - EnQOLB (x-&gt;lock); /* Prefetch lock, data */ /* Various computation here */ while (! EnQOLB (x-&gt;lock)); /* Spin */ /* Critical section using x-&gt;data */ DeQOLB (x-&gt;lock); /* Release lock */ - 4 progress. <p> We performed additional optimizations on each benchmark to maximize performance on the simulated hardware. For simulations evaluating QOLB, data structures were modified to couple locks in the same line with the data that they protect. We padded data in each benchmark, where necessary, to eliminate false sharing <ref> [15] </ref>. We compiled the benchmarks using GCC version 2.6.2 with the option -O3. Barnes originally used locks to protect the higher levels of the tree during its tree-building phase, which results in an often-locked root.
Reference: [16] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For this experiment we set the cache size to 8KB. We chose our cache size so that the number of sets is comparable to that in a previous study <ref> [16] </ref>. 3.2.5 Future technology Simulation results for two types of systems are presented in this paper: target systems using current technology and target systems using future technology (our estimates are for approximately 5 years in the future). <p> The only exception is the pairwise sharing optimization, which is quite sensitive to the size of the cache. For caches smaller than an applications working set size, the effectiveness of pairwise sharing remains to be demonstrated. 5 Related work Gupta et al. <ref> [16] </ref> present a comparative evaluation of some latency reduction and tolerance techniques. They study the performance impact of coherent caches, relaxation of the memory consistency model, non-binding prefetching, and multiple-context processors on shared-memory parallel programs. They report consistent performance improvements for coherent caches, relaxed memory ordering and prefetching.
Reference: [17] <author> Anoop Gupta and Wolf-Dietrich Weber. </author> <title> Cache Invalidation Patterns in Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7):794810, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: Consequently, researchers have investigated many protocol extensions [7, 9, 11, 25] that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data <ref> [17] </ref> and producer-consumer data sharing. In this paper we study only pairwise sharingan optimization aimed at two-node sharingprimarily because pairwise sharing is a feature of the SCI cache-coherence protocol.
Reference: [18] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: We therefore compare two more advanced primitives, QOLB [14] and MCS locks [20]. The second class of optimizations that we examine consists of a range of memory models. Programmers naturally assume a memory model formally called sequential consistency, defined by Lam 2 port <ref> [18] </ref>. The strict ordering of sequential consistency severely limits concurrency of memory operations in a parallel computer. Most memory operations could in fact be completed out of order without affecting the program result. <p> consistency as follows: [A memory system is sequentially consistent if] the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program <ref> [18] </ref>. Sequential consistency is overly restrictive with respect to multiprocessor memory orderings. Systems can achieve higher performance by relaxing the memory orderings, without compromising the correctness of the program.
Reference: [19] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1):21 65, </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: Mellor-Crummey and Scott (MCS) implement a queue as a linked list, and use atomic operations such as swap or compare-and-swap to update the list correctly <ref> [19] </ref>. Anderson presented a scheme that implements a queue as a circular array [4]. Inspired by QOLB, these algorithms also reduce the network traffic to a constant number of traversals per synchronization access, allowing processors to spin locally while waiting for the release of the lock.
Reference: [20] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Synchronization Without Contention. </title> <booktitle> In Proceedings of the Fourth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269278, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Typically, the processor accesses a lock repeatedly until the processor finds it unlocked. On a multiprocessor, these repeated accesses often translate directly into network traffic that leads to heavy network contention and potentially severe performance degradation. We therefore compare two more advanced primitives, QOLB [14] and MCS locks <ref> [20] </ref>. The second class of optimizations that we examine consists of a range of memory models. Programmers naturally assume a memory model formally called sequential consistency, defined by Lam 2 port [18]. The strict ordering of sequential consistency severely limits concurrency of memory operations in a parallel computer. <p> Apply ing r2 in the same way was less successful: The r2 design clearly removes some of the same overhead eliminated by QOLB. 4.5 Future technology In a previous study Mellor-Crummey and Scott conclude <ref> [20] </ref> that special-purpose synchronization mechanisms such as the [QOLB] instruction are unlikely to outperform our MCS lock by more than 30%. This claim does not hold as shown by the results in Table 5, where QOLB improves the performance by nearly 70% for Mp3d.
Reference: [21] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: SCI accomplishes this transition without extra messages in most cases, although there are circumstances where performance suffers because pairwise sharing defers the breaking-down of a two-element list when a third requester joins the list. 2.5 Wisconsin Wind Tunnel Our experiments were performed on the Wisconsin Wind Tunnel virtual prototyping system <ref> [21] </ref>. WWT executes parallel shared-memory programs on a parallel message-passing computer (the host). It uses execution-driven, distributed, discrete-event simulation techniques that accurately calculate program execution time. Execution occurs in fixed windows of time, called quanta, which the simulator alternates with synchronization events that maintain causality.
Reference: [22] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th 11 Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1425, </pages> <month> Jun </month> <year> 1993. </year>
Reference-contexts: Assuming a large cache, benchmarks with small data sets may tend to overemphasize the benefits of improvements in the efficiency of sharing <ref> [22] </ref>. Thus, we reran some of the experiments with a smaller cache size, to understand the behavior of the optimizations in the presence of capacity and conict misses. For this experiment we set the cache size to 8KB.
Reference: [23] <author> Steven L. Scott, James R. Goodman, and Mary K. Vernon. </author> <title> Performance of the SCI Ring. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 403414, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We used the constant latency model for our experiments. In order to account for network contention, we derived a constant network latency to use for each benchmark, which we obtained with an analytical model. The analytical model that we used <ref> [23] </ref> requires the network load as a parameter. We estimated this aggregate value from the traffic statistics of previous simulations and their total execution times. The model produces the mean latency of a network traversal, to which we set the constant for the network latency.
Reference: [24] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Our target applications are five programs selected from the SPLASH benchmark suite <ref> [24] </ref>. 2.1 The Scalable Coherent Interface SCI defines both an interface to a network and a cache coherence protocol. The protocol is a robust hardware solution to the cache coherence problem. Messages may be addressed to any node in the system, but the protocol does not use broadcast. <p> They are Bar nes, Mp3d, Ocean, Pthor, and Water (see Table 1). A description of these benchmarks appears in the original article <ref> [24] </ref>. We focus the following discussion on specifics related to our study. We labeled all memory accesses as aggressively as possible according to the structure proposed by Gharachorloo et al. [13]. We then inserted memory fences to achieve release consistency on our simulated hardware platform.
Reference: [25] <author> Per Stenstrm, Mats Brorsson, and Lars Sandberg. </author> <title> Adaptive Cache Coherence Protocol Optimized for Migratory Sharing. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 109118, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Implementations may then be conservative or aggressive in supporting the memory model. The final optimization we study reduces overhead for one particular pattern of data sharing. General cache-coherence protocols may not optimally handle the majority of common data sharing patterns. Consequently, researchers have investigated many protocol extensions <ref> [7, 9, 11, 25] </ref> that allow existing protocols to perform better under specific classes of data sharing patterns. These extensions attempt to reduce network traversals and/or memory accesses. Two common classes of sharing patterns are migratory data [17] and producer-consumer data sharing. <p> This analysis shows that QOLB outperforms both of these algorithms, both in terms of interconnect messages and memory accesses needed to gain access to a critical section. Cox and Fowler [9], and Stenstrm, Brorsson, and Sandberg <ref> [25] </ref> present studies that propose different solutions for dealing with the problem of migratory sharing patterns. Both studies present adaptive schemes that can be implemented by a hardware cache coherence protocol.
References-found: 25

