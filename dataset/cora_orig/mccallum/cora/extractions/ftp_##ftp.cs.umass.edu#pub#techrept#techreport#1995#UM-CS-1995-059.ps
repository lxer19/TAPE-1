URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1995/UM-CS-1995-059.ps
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: weemsg@cs.umass.edu  
Phone: (413) 545-1249 (fax)  
Title: Compiling for Heterogeneous Systems: A Survey and an Approach  
Author: Kathryn S. McKinley, Sharad K. Singhai, Glen E. Weaver, Charles C. Weems fmckinley, singhai, weaver, 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Date: July 1995  
Pubnum: CMPSCI Techincal Report 95-59  
Abstract: Large applications tend to contain several models of parallelism, but only a few of these map efficiently to the single model of parallelism embodied in a homogeneous parallel system. Heterogeneous parallel systems incorporate diverse models of parallelism within a single machine or across machines. These systems are already pervasive in industrial and academic settings and offer a wealth of underutilized resources for achieving high performance. Unfortunately, heterogeneity complicates software development. We believe that compilers can and should assist in managing this complexity. We identify four goals for extending compilers to assist with managing heterogeneity: exploiting available resources, targeting changing resources, adjusting optimization to suit a target, and allowing programming models and languages to evolve. These goals do not require changes to the individual pieces of a compiler so much as a restructuring of a compiler's software architecture to increase its flexibility. We examine the features and flexibility of six important parallelizing compilers to find existing solutions for flexibility. Where no solutions exist, we propose architectural changes to compilers. 
Abstract-found: 1
Intro-found: 1
Reference: [AALL93] <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 7 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [WFW + 94, AALL93, HMA95, Gro94] </ref>. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's organization maximizes flexibility in reordering compiler passes at the expense of protecting programmers from simple mistakes (e.g., not running analyses or using antiquated analysis data).
Reference: [BBG + 93] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, and S. X. Yang. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, YGS + 94, MMB + 94, BBG + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [BBG + 94] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and B. Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. </title> <booktitle> In Second Object-Oriented Numerics Conference, </booktitle> <year> 1994. </year>
Reference-contexts: flexibility if, for example, another language is sufficiently different so as to require a different analysis routine. 6 The authors of Polaris were not interested in supporting a wide variety of source languages or dramatically different architectures. 2.4 Sage++ Sage++ from Indiana University is a toolkit for building source-to-source translators <ref> [BBG + 94] </ref>. The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90.
Reference: [BE94] <author> William Blume and Rudolf Eigenmann. </author> <title> The range test: A dependence test for symbolic, non-linear expressions. </title> <type> CSRD 1345, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Instead of using traditional dependence analysis, Polaris builds symbolic lower and upper bounds for each variable reference and propagates these ranges throughout the program using symbolic execution. Polaris' range test then uses these ranges to disprove dependences <ref> [BE94] </ref>. For optimization, Polaris includes scalar and array privatization, induction variable substitution, and reduction recognition and replacement.
Reference: [BEF + 94] <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Peterson, B. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which also makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [BEF + 94, PEH + 93, FHP + 94] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a production quality compiler in terms of reliability and speed. The authors focus on parallelization for distributed shared memory machines (e.g., KSR and T3D).
Reference: [BHMM94] <author> D. Brown, S. Hackstadt, A. Malony, and B. Mohr. </author> <title> Program analysis environments for parallel language systems: the TAU environment. </title> <booktitle> In Proceedings of the 2nd Workshop on Environments and Tools For Parallel Scientific Computing, </booktitle> <pages> pages 162-171, </pages> <address> Townsend, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: But, Sage++ has also been used to construct a suit of programming environment tools: tuning and analysis utility (TAU), file and class display (Fancy), call graph extended display (Cagey), class hierarchy browser (Classy), routine and data access profile display (Racy) and event and state display (Easy) <ref> [MBM94, BHMM94] </ref>. Summary Sage++ is a convenient tool for building source-to-source translators. It is not tied to any particular hardware architecture which makes it portable, and it provides basic routines needed by compilers.
Reference: [BKK94] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation (PACT), </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Descriptions of the applicability and safety of these transformations are built into ParaScope, and it interactively advises about the profitability of transformations. The Fortran-D compiler adds communication transformations such as message vectorization, communication selection, message coalescing, message aggregation, and message pipelining. In addition, recent work addresses automatic data partitioning <ref> [BKK94] </ref>. Summary 5 ParaScope is a mature system for automatic and interactive parallelization which provides a wide selection of analyses, optimizations, and transformations. As a source-to-source translator, it is not tightly coupled to any particular machine.
Reference: [BL94] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 <ref> [BL94] </ref>, and MPI [For94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93].
Reference: [BPMG94] <author> F. Bodin, T. Priol, P. Mehrotra, and D. Gannon. </author> <title> Directions in parallel programming: HPF, shared virtual memory and object parallelism in pC++. </title> <type> Technical Report 94-54, </type> <institution> Institute for Computer Applications in Science and Engineering, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, YGS + 94, MMB + 94, BBG + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [CHH + 93] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: When combined with techniques for finding loop parallelism, the compiler has more choices when transforming a program to match a particular target architecture. 2.2 ParaScope Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [CHH + 93, KMT93] </ref>. It provides sophisticated global program analysis and a rich set of program transformations. ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system.
Reference: [CMZ92a] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [CMZ92b] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: For example, headers for Fortran DO loops are represented by a subclass of Statement that has additional fields for items such as the end of the loop and the index variable. Optimizations The authors of Polaris performed a study in which they parallelized the Perfect Club benchmark by hand <ref> [EHLP91] </ref>. They found that a few new analyses and optimizations, in addition to ones already in systems like KAP, would significantly improve a translator's ability to parallelize code. For analysis, they added constant propagation, inlining (also used for optimization), and their own version of symbolic data dependence analysis.
Reference: [Fah94] <author> T. Fahringer. </author> <title> Using the P 3 T to guide the parallelization and optimization effort under the vienna fortran compilation system. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: However, dynamic data distribution is quite expensive and Vienna Fortran does not provide mechanisms to measure the trade-off between the cost of redistribution and the additional communication cost. Some recent work uses static performance measurements to guide optimization process <ref> [Fah94, FZ93] </ref>. Summary The design goal of Vienna Fortran is to generate optimized code for distributed-memory machines. Vienna Fortran borrows upon SUPERB, a predecessor of VFCS. In turn, many features of Vienna Fortran have been incorporated into HPF which is a proper subset of Vienna Fortran.
Reference: [FGMS93] <author> S.I. Feldman, David M. Gay, Mark W. Maimone, </author> <title> and N.L. Schryer. A Fortran-to-C converter. </title> <institution> Computing Science 149, AT&T Bell Laboratories, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: SUIF has been used to study parallelization for both shared memory and distributed shared memory machines [WFW + 94]. SUIF accepts source code written in either Fortran 77 or C, but a modified version of f2c <ref> [FGMS93] </ref> is used to convert Fortran code to C code. The modifications to f2c retain some Fortran specific information that further aids in analysis. System architecture SUIF has a flexible organization, with each analysis and optimization coded as a separate, independent pass.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications <ref> [FHK + 90] </ref>. The output of the D System is an efficient message-passing distributed memory program [HKT91, HKT92, Tse93]. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation.
Reference: [FHP + 94] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris internal representation. </title> <institution> Center for Supercomputing Research and Development CSRD-1317, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which also makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [BEF + 94, PEH + 93, FHP + 94] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a production quality compiler in terms of reliability and speed. The authors focus on parallelization for distributed shared memory machines (e.g., KSR and T3D).
Reference: [For94] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <type> v1.0. Technical report, </type> <institution> University of Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI <ref> [For94] </ref>. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [FZ93] <author> T. Fahringer and H. Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: However, dynamic data distribution is quite expensive and Vienna Fortran does not provide mechanisms to measure the trade-off between the cost of redistribution and the additional communication cost. Some recent work uses static performance measurements to guide optimization process <ref> [Fah94, FZ93] </ref>. Summary The design goal of Vienna Fortran is to generate optimized code for distributed-memory machines. Vienna Fortran borrows upon SUPERB, a predecessor of VFCS. In turn, many features of Vienna Fortran have been incorporated into HPF which is a proper subset of Vienna Fortran.
Reference: [GP94] <author> Milind B. Girkar and Constantine Polychronopoulos. </author> <title> The hierarchical task graph as a universal intermediate representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5), </volume> <year> 1994. </year>
Reference-contexts: ParaScope and SUIF are mature compilers with a wide assortment of features; Parafrase-2, Polaris, and VFCS are more recent systems. Sage++ is the only compiler framework. 2.1 Parafrase-2 Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [PGH + 89, GP94] </ref>. The goal of Parafrase-2 is to be a research tool for investigating compiler support for multiple languages and target architectures. Rather than try to build everything into Parafrase-2 from the beginning, the authors strive to make it flexible enough for later additions.
Reference: [Gro94] <author> Stanford Compiler Group. </author> <title> The SUIF library. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 7 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [WFW + 94, AALL93, HMA95, Gro94] </ref>. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's organization maximizes flexibility in reordering compiler passes at the expense of protecting programmers from simple mistakes (e.g., not running analyses or using antiquated analysis data).
Reference: [GY93] <author> Arif Ghafoor and Jaehyung Yang. </author> <title> A distributed heterogeneous supercomputing management system. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 78-86, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR91-149, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [FHK + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [FHK + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [HMA95] <author> Mary W. Hall, Brian R. Murphy, and Saman P. Amarasinghe. </author> <title> Interprocedural parallelization analysis: A case study. </title> <institution> Stanford university technical report, Stanford University, </institution> <year> 1995. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 7 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [WFW + 94, AALL93, HMA95, Gro94] </ref>. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's organization maximizes flexibility in reordering compiler passes at the expense of protecting programmers from simple mistakes (e.g., not running analyses or using antiquated analysis data). <p> The data dependence analyzer uses a suite of tests of varying complexity to obtain accurate results quickly. The interprocedural information is computed by an unusually extensive collection of analyses: induction variable recognition, cloning, array summary, array reshape detection, reduction recognition, and data dependence (for array summaries) <ref> [HMA95] </ref>. SUIF structures most of its interprocedural analyses as a bottom-up pass that summarizes the behavior of each subroutine, followed by a top-down pass that applies calling contexts to each subroutine's summary description to compute its final analysis result.
Reference: [KMCP93] <author> Alan E. Klietz, Andrei V. Malevsky, and Ken Chin-Purcell. </author> <title> A case study in metacomputing: Distributed simulations of mixing in turbulent convection. </title> <booktitle> In Workshop on Heterogeneous Processing, </booktitle> <pages> pages 101-106. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1993. </year> <month> 25 </month>
Reference: [KMT93] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel pro-gramming tool. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 575-602, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: When combined with techniques for finding loop parallelism, the compiler has more choices when transforming a program to match a particular target architecture. 2.2 ParaScope Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [CHH + 93, KMT93] </ref>. It provides sophisticated global program analysis and a rich set of program transformations. ParaScope is a research tool and has been specialized for several Fortran derivatives. This section concentrates on the Fortran-D version of the ParaScope called the D system.
Reference: [KPSW93] <author> Ashfaq A. Khokhar, Viktor K. Prasanna, Muhammad E. Shaaaban, and Cho-Li Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 18-27, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask. <p> A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application <ref> [KPSW93] </ref>. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [Kuc88] <author> Kuck & Associates, Inc. </author> <title> KAP User's Guide. </title> <address> Champaign, IL 61820, </address> <year> 1988. </year>
Reference-contexts: System architecture The organization of Polaris reflects the authors' belief that combining a few new optimizations with existing ones will make automatic parallelization feasible. Rather than rewrite existing optimizations, Polaris provides only a few analyses and transformations, and expects KAP <ref> [Kuc88] </ref> to provide the rest. Polaris organizes its analyses and optimizations as separate passes that operate over the whole program. Programmers can affect the operation of passes through command line parameters. Polaris can also output an intermediate representation of a program that KAP can read.
Reference: [MBM94] <author> B. Mohr, D. Brown, and A. Malony. </author> <title> TAU: A portable parallel program analysis environment for pC++. </title> <booktitle> In Proceedings of CONPAR 94 - VAPP VI, </booktitle> <institution> University of Linz, Austria, </institution> <month> September </month> <year> 1994. </year> <note> LNCS 854. </note>
Reference-contexts: But, Sage++ has also been used to construct a suit of programming environment tools: tuning and analysis utility (TAU), file and class display (Fancy), call graph extended display (Cagey), class hierarchy browser (Classy), routine and data access profile display (Racy) and event and state display (Easy) <ref> [MBM94, BHMM94] </ref>. Summary Sage++ is a convenient tool for building source-to-source translators. It is not tied to any particular hardware architecture which makes it portable, and it provides basic routines needed by compilers.
Reference: [MMB + 94] <author> A. Malony, B. Mohr, P. Beckmen, D. Gannon, S. Yang, and F. Bodin. </author> <title> Performance analysis of pC++: A portable data-parallel programming system for scalable parallel computers. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, YGS + 94, MMB + 94, BBG + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [PE95] <author> William Pottenger and Rudolf Eigenmann. </author> <title> Idiom recognition in the Polaris parallelizing compiler. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference: [PEH + 93] <author> David A. Padua, Rudolf Eigenmann, Jay Hoeflinger, Paul Petersen, Peng Tu, Stephen Weatherford, and Keith Faigin. </author> <title> Polaris: A new-generation parallelizing compiler for MPPs. </title> <institution> Center for Supercomputing Research and Development CSRD-1306, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Changing the IR to support new languages or models of parallelism requires modifications throughout ParaScope. The Fortran-D compiler has a monolithic structure which also makes it difficult to extend. 2.3 Polaris Polaris from the University of Illinois, is an optimizing source-to-source translator <ref> [BEF + 94, PEH + 93, FHP + 94] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a production quality compiler in terms of reliability and speed. The authors focus on parallelization for distributed shared memory machines (e.g., KSR and T3D).
Reference: [PGH + 89] <author> Constantine Polychronopoulos, Milind B. Girkar, Mohammad R. Haghighat, Chia L. Lee, Bruce P. Le-ung, and Dale A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1), </volume> <year> 1989. </year>
Reference-contexts: ParaScope and SUIF are mature compilers with a wide assortment of features; Parafrase-2, Polaris, and VFCS are more recent systems. Sage++ is the only compiler framework. 2.1 Parafrase-2 Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [PGH + 89, GP94] </ref>. The goal of Parafrase-2 is to be a research tool for investigating compiler support for multiple languages and target architectures. Rather than try to build everything into Parafrase-2 from the beginning, the authors strive to make it flexible enough for later additions.
Reference: [SC92] <author> Larry Smarr and Charles E. Catlett. </author> <title> Metacomputing. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 45-52, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [SCMB90] <author> J. Saltz, K. Crowely, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: Optimizations work directly on the program database, and transformations are guided by an interactive kernel. Optimizations VFCS performs traditional data flow and data dependence analyses. It also has interprocedural communication and dynamic distribution analysis. It optimizes irregular access pattern using PARTI routines <ref> [SCMB90] </ref>. 9 VFCS was specifically designed for distributed-memory machines and has an extensive set of communi-cation optimizations besides loop optimizations. It performs overlap analysis to determine which non-local elements of a partitioned array are used in a processor and inserts communication primitives based on the analysis.
Reference: [SGDM94] <author> V.S. Sunderam, G.A. Geist, J. Dongarra, and P. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM <ref> [SGDM94] </ref>, p4 [BL94], and MPI [For94]. Heterogeneous processing [SC92, Tur93, Tur95, KPSW93, GY93] is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93].
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This section concentrates on the Fortran-D version of the ParaScope called the D system. Fortran-D enhances Fortran 77 and 90 with data decomposition specifications [FHK + 90]. The output of the D System is an efficient message-passing distributed memory program <ref> [HKT91, HKT92, Tse93] </ref>. System architecture The D System accepts programs written in either a subset of Fortran 77D or Fortran 90D and converts them into an abstract syntax tree (AST) representation. The Fortran-D back end uses loop transformations and communication optimizations to build efficient message-passing, SPMD node programs.
Reference: [Tur93] <author> L. H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical Report MSSU-EIRS-ERC-93-2, </type> <institution> NSF Engineering Research Center, Mississippi State University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [Tur95] <author> L. H. Turcotte. </author> <title> Cluster computing. </title> <editor> In Albert Y. Zomaya, editor, </editor> <booktitle> Handbook of Parallel and Distributed Computing. </booktitle> <address> McGraw-Hill, </address> <note> To Appear 1995. </note>
Reference-contexts: Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94]. Heterogeneous processing <ref> [SC92, Tur93, Tur95, KPSW93, GY93] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [KPSW93]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [WFW + 94] <author> Robert P. Wilson, Robert S. French, Christopher S. Wilson, Saman P. Amarasinghe, Jennifer M. Ander-son, Steve W. K. Tjiang, Shih-Wei Liao, Chau-Wen Tseng, Mary W. Hall, Monica S. Lam, and John L. Hennessy. </author> <title> The SUIF compiler system: A parallelizing and optimizing research compiler. </title> <journal> SIGPLAN, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: It also lacks many of the sophisticated analyses, optimizations, and transformations because of its compiler framework nature. 7 2.5 SUIF Stanford University Intermediate Format (SUIF) is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [WFW + 94, AALL93, HMA95, Gro94] </ref>. SUIF serves as a research tool for investigating automatic parallelization of sequential programs. SUIF's organization maximizes flexibility in reordering compiler passes at the expense of protecting programmers from simple mistakes (e.g., not running analyses or using antiquated analysis data). <p> SUIF's organization maximizes flexibility in reordering compiler passes at the expense of protecting programmers from simple mistakes (e.g., not running analyses or using antiquated analysis data). SUIF has been used to study parallelization for both shared memory and distributed shared memory machines <ref> [WFW + 94] </ref>. SUIF accepts source code written in either Fortran 77 or C, but a modified version of f2c [FGMS93] is used to convert Fortran code to C code. The modifications to f2c retain some Fortran specific information that further aids in analysis.
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For optimization, SUIF has a wide assortment of traditional 8 and loop optimizations (see Table 2), including unimodular loop transformations (i.e., interchange, reversal, and skewing) <ref> [WL91] </ref>. Nevertheless, SUIF is most note worthy for its interprocedural optimizations: par-allelization, data privatization, cloning, and reduction recognition. Lastly, SUIF includes a code generator which allows it to not only transform a sequential program to a parallel program, but also immediately generate object code for the parallel program.
Reference: [WLH + 89] <author> Charles Weems, Steven Levitan, Allen Hanson, Edward Riseman, J. Gregory Nash, and David Shu. </author> <title> The image understanding architecture. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3) </volume> <pages> 251-282, </pages> <year> 1989. </year>
Reference-contexts: Tightly-coupled heterogeneous computers integrate different parallel This work was supported in part by the Advanced Research Projects Agency under contract N00014-94-1-0742, monitored by the Office of Naval Research. architectures together within a single machine (e.g., Meiko CS-2, IBM SP-2, and IUA <ref> [WLH + 89] </ref>). Virtual heterogeneous machines treat a network of machines as a single virtual computer. This processing model is sometimes called metacomputing or, when the machines are similar, cluster computing. A virtual machine is often connected via message passing interfaces such as PVM [SGDM94], p4 [BL94], and MPI [For94].
Reference: [YGS + 94] <author> S. Yang, D. Gannon, S. Srinivas, F. Bodin, and P. Bode. </author> <title> High performance fortran interface to the parallel C++. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Using Sage++, a compiler writer can implement tests for legality and a richer set of transformations and optimizations. The compiler writer is responsible for writing the code to update the dependence graph after program changes. Applications of Sage++ Sage++ has been used to support new language extensions <ref> [BPMG94, YGS + 94, MMB + 94, BBG + 93] </ref>. Sage++ is useful not only for building source-to-source translators but also other tools that depend on programs as input. Some traditional uses of Sage++ include optimizing expressions in scientific library code, instrumenting user code, and preprocessing user annotations in Fortran-S.
Reference: [ZC93] <author> H. Zima and B. Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
Reference: [ZCMM93] <author> H. Zima, B. Chapman, H. Moritsch, and P. Mehrotra. </author> <title> Dynamic data distributions in Vienna Fortran. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year> <title> 26 This article was processed using the L A T E X macro package with LLNCS style 27 </title>
Reference-contexts: Though this flexibility facilitates exploration of optimization ordering for different architectures, SUIF neither assists in dynamically selecting passes nor provides guards against inappropriate orderings. 2.6 Vienna Fortran Compilation System (VFCS) The Vienna Fortran Compilation System (VFCS) from the University of Vienna is an interactive, source-to-source translator for Vienna Fortran <ref> [CMZ92a, CMZ92b, ZC93, ZCMM93] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. The Vienna Fortran language extends Fortran with constructs for distributing data across the processors of a distributed-memory multiprocessing system (DMMP).
References-found: 46

