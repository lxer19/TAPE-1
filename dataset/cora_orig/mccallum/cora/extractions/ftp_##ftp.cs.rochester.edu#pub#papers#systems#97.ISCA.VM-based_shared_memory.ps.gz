URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/97.ISCA.VM-based_shared_memory.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Title: VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks  
Author: Leonidas Kontothanassis Galen Hunt, Robert Stets, Nikolaos Hardavellas, Micha Cierniak, Srinivasan Parthasarathy, Wagner Meira, Jr., Sandhya Dwarkadas, and Michael Scott Hunt) and CNPq-Brazil (Wagner Meira, Jr., 
Note: This work was supported in part by NSF grants CDA-9401142, CCR-9319445, CCR-9409120, and CCR-9510173; ARPA contract F19628-94-C-0057; an external research grant from Digital Equipment Corporation; and graduate fellowships from  Grant 200.862/93-6). To appear in the Proceedings of the Twenth-Fourth Annual International Symposium on Computer Architecture, Denver, CO, June 1997.  
Address: Rochester One Kendall Sq., Bldg. 700 Rochester, NY 14627-0226 Cambridge, MA 02139  
Affiliation: Department of Computer Science DEC Cambridge Research Lab University of  Microsoft Research (Galen  
Abstract: Recent technological advances have produced network interfaces that provide users with very low-latency access to the memory of remote machines. We examine the impact of such networks on the implementation and performance of software DSM. Specifically, we compare two DSM systemsCashmere and TreadMarkson a 32-processor DEC Alpha cluster connected by a Memory Channel network. Both Cashmere and TreadMarks use virtual memory to maintain coherence on pages, and both use lazy, multi-writer release consistency. The systems differ dramatically, however, in the mechanisms used to track sharing information and to collect and merge concurrent updates to a page, with the result that Cashmere communicates much more frequently, and at a much finer grain. Our principal conclusion is that low-latency networks make DSM based on fine-grain communication competitive with more coarse-grain approaches, but that further hardware improvements will be needed before such systems can provide consistently superior performance. In our experiments, Cashmere scales slightly better than TreadMarks for applications with false sharing. At the same time, it is severely constrained by limitations of the current Memory Channel hardware. In general, performance is better for TreadMarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) is an attractive design alternative for large-scale shared memory multiprocessing. Traditional DSM systems rely on virtual memory hardware and simple message passing to implement shared memory. State-of-the-art DSM systems (e.g. TreadMarks <ref> [1, 16] </ref>) employ sophisticated protocol optimizations, such as relaxed consistency models, multiple writable copies of a page, and lazy processing of all coherence-related events. <p> Further information on TreadMarks can be found in other papers <ref> [1] </ref>. 3 Implementation Issues 3.1 Memory Channel Digital Equipment's Memory Channel (MC) network provides applications with a global address space using memory mapped regions.
Reference: [2] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proc. of the 21st Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory. Aggressive compiler optimizations attempt to keep the cost of checks as low as possible. AURC [14] is a multi-writer protocol designed for the Shrimp network interface <ref> [2] </ref>. Like Cashmere, AURC relies on remote memory access to write shared data updates to home nodes. Like TreadMarks, however, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information. <p> In some systems, such as Split-C [7] and Shrimp's Deliberate Update <ref> [2] </ref>, the programmer must use special primitives to read and write remote data. In others, including Shared Regions [27], Cid [24], and CRL [15], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations. <p> Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp <ref> [2] </ref>, the HP Hamlyn interface [5] to Myrinet [3], and Dolphin's snooping interface [22] for the SCI cache coherence protocol [13]. 6 Conclusion and Future Work We have presented results for two different DSM protocols Cashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [3] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. E. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pp. 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [2], the HP Hamlyn interface [5] to Myrinet <ref> [3] </ref>, and Dolphin's snooping interface [22] for the SCI cache coherence protocol [13]. 6 Conclusion and Future Work We have presented results for two different DSM protocols Cashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [4] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proc. of the 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 212-221, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Nitzberg and Lo [25] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [4, 19] </ref>. Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [26].
Reference: [5] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [2], the HP Hamlyn interface <ref> [5] </ref> to Myrinet [3], and Dolphin's snooping interface [22] for the SCI cache coherence protocol [13]. 6 Conclusion and Future Work We have presented results for two different DSM protocols Cashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [6] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proc. of the Twelfth ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 147-158, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The Midway system [34] requires the programmer to associate shared data with synchronization objects, allowing ordinary synchronization acquires and releases to play the role of the bracketing operations. Several other systems use the member functions of an object-oriented programming model to trigger coherence operations <ref> [6, 11, 31] </ref>. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance. It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden.
Reference: [7] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. Supercomputing '93,pp. </booktitle> <pages> 262-273, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The Barnes-Hut tree construction is performed sequentially, while all other phases are parallelized and dynamically load balanced. Synchronization consists of barriers between phases. Em3d: a program to simulate electromagnetic wave propagation through 3D objects <ref> [7] </ref>. The major data structure is an array that contains the set of magnetic and electric nodes. These are equally distributed among the processors in the system. For each phase in the computation, each processor updates the electromagnetic potential of its nodes based on the potential of neighboring nodes. <p> In some systems, such as Split-C <ref> [7] </ref> and Shrimp's Deliberate Update [2], the programmer must use special primitives to read and write remote data.
Reference: [8] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of General Linkage Analysis Problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: A synchronization flag for each row indicates when it is available to other rows for use as a pivot. Ilink: a widely used genetic linkage analysis program from the FASTLINK 2.3P package that locates disease genes on chromosomes. We use the parallel algorithm described by Dwarkadas et al. <ref> [8] </ref>. The main shared data is a pool of sparse arrays of genotype probabilities. Updates to each array are parallelized. A master processor assigns individual array elements to processors in a round robin fashion in order to improve load balance.
Reference: [9] <author> S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. </author> <title> An Integrated Compile-Time/Run-Time Software Distributed Shared Memory System. </title> <booktitle> In Proc. of the 7th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden. In some cases, it may be possible for an optimizing compiler to obtain the performance of the special programming model without the special syntax <ref> [9] </ref>. 5.3 Fast User-Level Messages The Memory Channel is not unique in its support for user-level messages, though it is the first commercially-available work 11 station network with such an interface. <p> Finally, we are continuing our research into the relationship between run-time coherence management and static compiler analysis <ref> [9] </ref>.
Reference: [10] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Proc. of the 7th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: An 8-processor run of Gauss, for example, is 40% faster with 2 processors on each of 4 nodes than it is with 4 processors on each of 2 nodes. This result is consistent with the findings of Erlichson et al. <ref> [10] </ref>; it indicates that there is insufficient bandwidth on the link between each SMP and the hub, something that should be remedied in the next generation of the network. 5 Related Work Distributed shared memory for workstation clusters is an active area of research: many systems have been built, and more <p> Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [26]. Recent work by the Alewife group at MIT [33] and the FLASH group at Stanford <ref> [10] </ref> has addressed the implementation of software coherence on a collection of hardware-coherent nodes. Wisconsin's Blizzard system [29] maintains coherence for cache-line-size blocks, either in software or by using ECC. It runs on the Thinking Machines CM-5 and provides a sequentially-consistent programming model.
Reference: [11] <author> M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M. Levy. </author> <title> Integrating Coherency and Recovery in Distributed Systems. </title> <booktitle> In Proc. of the 1st Symp. on Operating Systems Design and Implementation, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The Midway system [34] requires the programmer to associate shared data with synchronization objects, allowing ordinary synchronization acquires and releases to play the role of the bracketing operations. Several other systems use the member functions of an object-oriented programming model to trigger coherence operations <ref> [6, 11, 31] </ref>. Because they provide the coherence system with information not available in more general-purpose systems, special programming models have the potential to provide superior performance. It is not yet clear to what extent the extra effort required of programmers will be considered an acceptable burden.
Reference: [12] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: In this paper we compare implementations of TreadMarks and a modified version of Cashmere on a 32-processor cluster (8 nodes, 4 processors each) of DEC AlphaServers, connected by DEC's Memory Channel <ref> [12] </ref> network. Memory Channel allows a user-level application to write to the memory of remote nodes. The remote-write capability can be used for (noncoherent) shared memory, for broadcast/multicast, and for very fast user-level messages. Remote reads are not directly supported.
Reference: [13] <author> D. B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [2], the HP Hamlyn interface [5] to Myrinet [3], and Dolphin's snooping interface [22] for the SCI cache coherence protocol <ref> [13] </ref>. 6 Conclusion and Future Work We have presented results for two different DSM protocols Cashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel. TreadMarks uses the Memory Channel only for fast messaging, while Cashmere uses it for directory maintenance and for fine-grained updates to shared data.
Reference: [14] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving Release-Consistent Shared Virtual Memory Using Automatic Update. </title> <booktitle> In Proc. of the 2nd Intl. Symp. on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Like Cashmere, Shasta runs on the Memory Channel, with polling for remote requests. Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory. Aggressive compiler optimizations attempt to keep the cost of checks as low as possible. AURC <ref> [14] </ref> is a multi-writer protocol designed for the Shrimp network interface [2]. Like Cashmere, AURC relies on remote memory access to write shared data updates to home nodes. Like TreadMarks, however, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information.
Reference: [15] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proc. of the Fifteenth ACM Symp. on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: In some systems, such as Split-C [7] and Shrimp's Deliberate Update [2], the programmer must use special primitives to read and write remote data. In others, including Shared Regions [27], Cid [24], and CRL <ref> [15] </ref>, remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [16] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 19th Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) is an attractive design alternative for large-scale shared memory multiprocessing. Traditional DSM systems rely on virtual memory hardware and simple message passing to implement shared memory. State-of-the-art DSM systems (e.g. TreadMarks <ref> [1, 16] </ref>) employ sophisticated protocol optimizations, such as relaxed consistency models, multiple writable copies of a page, and lazy processing of all coherence-related events. <p> These two enhancements improve Cashmere's ability to efficiently handle private pages and producer-consumer sharing patterns. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) <ref> [16] </ref>. Lazy release consistency is a variant of release consistency [20]. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Lazy release consistency divides time on each node into intervals delineated by remote synchronization operations. <p> Nitzberg and Lo [25] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [4, 19]. Lazy, multi-writer protocols were pioneered by Keleher et al. <ref> [16] </ref>, and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [26].
Reference: [17] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High Performance Software Coherence for Current and Future Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29(2) </volume> <pages> 179-195, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: These networks suggest the need to re-evaluate the assumptions underlying the design of DSM protocols, and specifically to consider protocols that communicate at a much finer grain. The Cashmere system employs this sort of protocol <ref> [17, 18] </ref>. It uses directories to keep track of sharing information, and merges concurrent writes to the same coherence block via write-through to a unique (and possibly remote) main-memory copy of each page.
Reference: [18] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In Proc. of the 2nd Intl. Symp. on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: These networks suggest the need to re-evaluate the assumptions underlying the design of DSM protocols, and specifically to consider protocols that communicate at a much finer grain. The Cashmere system employs this sort of protocol <ref> [17, 18] </ref>. It uses directories to keep track of sharing information, and merges concurrent writes to the same coherence block via write-through to a unique (and possibly remote) main-memory copy of each page. <p> Our write-doubling mechanism increases the first-level working set for certain applications beyond the 16K available, dramatically reducing performance. The larger caches of the 21264 should largely eliminate this problem. We are optimistic about the future of Cashmere-like systems as network interfaces continue to evolve. Based on previous simulations <ref> [18] </ref>, it is in fact somewhat surprising that Cashmere performs as well as it does on the current generation of hardware. The second-generation Memory Channel, due on the market very soon, will have something like half the latency, and an order of magnitude more bandwidth. <p> Normal write-back then updates this local copy. To update the copy at the home node, we insert additional code into the program executable at every shared memory write (write doubling). This protocol differs in a number of ways from the Cashmere protocol employed in simulation-based studies <ref> [18] </ref>. The most important differences stem from differences in hardware platforms.
Reference: [19] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Nitzberg and Lo [25] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [4, 19] </ref>. Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors [26].
Reference: [20] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These two enhancements improve Cashmere's ability to efficiently handle private pages and producer-consumer sharing patterns. 2.2 TreadMarks TreadMarks is a distributed shared memory system based on lazy release consistency (LRC) [16]. Lazy release consistency is a variant of release consistency <ref> [20] </ref>. It guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Lazy release consistency divides time on each node into intervals delineated by remote synchronization operations.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: them into systems that support more-or-less generic shared-memory programs, such as might run on a machine with hardware coherence, and those that require a special programming notation or style. 5.1 Generic DSM The original idea of using virtual memory to implement coherence on networks dates from Kai Li's thesis work <ref> [21] </ref>. Nitzberg and Lo [25] provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [4, 19]. Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups.
Reference: [22] <author> O. Lysne, S. Gjessing, and K. Lochsen. </author> <title> Running the SCI Protocol over HIC Networks. </title> <booktitle> In 2nd Intl. Workshop on SCI-based Low-cost/High-performance Computing (SCIzzL-2), </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Fast user-level messages were supported without shared memory on the CM-5, though the protection mechanism was relatively static. Among workstation networks, user-level IPC can also be found in the Princeton Shrimp [2], the HP Hamlyn interface [5] to Myrinet [3], and Dolphin's snooping interface <ref> [22] </ref> for the SCI cache coherence protocol [13]. 6 Conclusion and Future Work We have presented results for two different DSM protocols Cashmere and TreadMarkson a remote-memory-access network, namely DEC's Memory Channel.
Reference: [23] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Proc. of the Ninth Intl. Parallel Processing Symp., </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: The home node itself can access the page directly, while the remaining processors have to use the slower Memory Channel interface. We assign home nodes at run time, based on which processor first touches a page after the program has completed any initialization phase <ref> [23] </ref>. The home node is set only once during the lifetime of a program, and thus the use of locks does not impact performance. In addition to the directory data structure, each processor also holds two globally accessible lists, the write notice list and the no longer exclusive (NLE) list.
Reference: [24] <author> R. S. Nikhil. Cid: </author> <title> A Parallel, Shared-memory C for Distributed-Memory Machines. </title> <booktitle> In Proc. of the 7th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In some systems, such as Split-C [7] and Shrimp's Deliberate Update [2], the programmer must use special primitives to read and write remote data. In others, including Shared Regions [27], Cid <ref> [24] </ref>, and CRL [15], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [25] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Nitzberg and Lo <ref> [25] </ref> provide a survey of early VM-based systems. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [4, 19]. Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups.
Reference: [26] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proc. of the 7th Intl. Parallel Processing Symp., </booktitle> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Lazy, multi-writer protocols were pioneered by Keleher et al. [16], and later adopted by several other groups. Several of the ideas in Cashmere were based on Petersen's coherence algorithms for small-scale, non-hardware-coherent multiprocessors <ref> [26] </ref>. Recent work by the Alewife group at MIT [33] and the FLASH group at Stanford [10] has addressed the implementation of software coherence on a collection of hardware-coherent nodes. Wisconsin's Blizzard system [29] maintains coherence for cache-line-size blocks, either in software or by using ECC.
Reference: [27] <author> H. S. Sandhu, B. Gamsa, and S. Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Proc. of the 4th ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In some systems, such as Split-C [7] and Shrimp's Deliberate Update [2], the programmer must use special primitives to read and write remote data. In others, including Shared Regions <ref> [27] </ref>, Cid [24], and CRL [15], remote data is accessed with the same notation used for local data, but only in regions of code that have been bracketed by special operations.
Reference: [28] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <note> Shasta: </note>
Reference-contexts: Wisconsin's Blizzard system [29] maintains coherence for cache-line-size blocks, either in software or by using ECC. It runs on the Thinking Machines CM-5 and provides a sequentially-consistent programming model. The more recent Shasta system <ref> [28] </ref>, developed at DEC WRL, extends the software-based Blizzard approach with a relaxed consistency model and variable-size coherence blocks. Like Cashmere, Shasta runs on the Memory Channel, with polling for remote requests. Rather than rely on VM, however, it inserts consistency checks in-line when accessing shared memory.
References-found: 28

