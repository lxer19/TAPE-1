URL: http://www.cs.huji.ac.il/papers/parallel-processing/envelopes.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/parallel-processing/header.html
Root-URL: 
Email: E-mail: rudolph@cs.huji.ac.il  
Title: Envelopes in Adaptive Local Queues for MIMD Load Balancing  
Author: Konstantin Shteiman Dror Feitelson Larry Rudolph Iaakov Exman 
Keyword: ity, shared memory  
Address: 91904 Jerusalem, Israel  
Affiliation: Department of Computer Science The Hebrew University,  
Abstract: The local queues, one for each PE, contain a get work task which pulls activities from a global list. In addition, they contain one or more envelopes within which activities are actually performed. These queues are adaptive because each get work task competes with its own envelopes. The more load the P E has, the less additional work it will get. Envelopes are reused for successive activities, thus increasing the granularity. New envelopes are only created to cope with program data and synchronization dependencies, thereby avoiding deadlocks. Experiments with envelopes performed and efficiency results are reported. keywords: envelopes, run-time systems, MIMD, load balancing, granular
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Ben-Asher, D. G. Feitelson, and L. Rudolph, </author> <title> "ParC | an extension of C for shared memory parallel processing". </title> <month> Oct </month> <year> 1990. </year> <type> Manuscript, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem. </institution> <note> Submitted for publication. </note>
Reference: [2] <author> E. Mohr, D. A. Kranz, and R. H. Halstead, Jr., </author> <title> "Lazy task creation: a technique for increasing the granularity of parallel programs". </title> <journal> IEEE Trans. Parallel & Distributed Syst. </journal> <volume> 2(3), </volume> <pages> pp. 264-280, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: Envelopes are a transparent instrument to keep simplicity and generality of the parallel programming language. One of the novel aspects of this work is that envelopes deal with dependencies among activities in run-time, and not just independent tasks as most previous work <ref> [3, 2] </ref>. The experiments performed with typical program structures show that envelopes actually support both coarse and fine-grain activities automatically with high efficiency. In order to minimize overhead, there is no task migration between PE's.
Reference: [3] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> "Guided self scheduling: a practical scheduling scheme for parallel supercomputers". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-36(12), </volume> <pages> pp. 1425-1439, </pages> <month> Dec </month> <year> 1987. </year>
Reference-contexts: How to divide the activities between the representatives? Equal division may not be optimal if the activities are not identical. A better approach is to always allocate 1=P of the remaining activities. Polychronopoulos and Kuck <ref> [3] </ref> showed theoretically that independently of the startup time of the P processors that are scheduled under this policy, all processors finish executing a parallel construct within B units of time difference from each other (where B is the execution time of one activity). 3 Measurements on a Testbed We have <p> Envelopes are a transparent instrument to keep simplicity and generality of the parallel programming language. One of the novel aspects of this work is that envelopes deal with dependencies among activities in run-time, and not just independent tasks as most previous work <ref> [3, 2] </ref>. The experiments performed with typical program structures show that envelopes actually support both coarse and fine-grain activities automatically with high efficiency. In order to minimize overhead, there is no task migration between PE's.
Reference: [4] <author> L. Rudolph, M. Slivkin-Allalouf, and E. Upfal, </author> <title> "A simple load balancing scheme for task allocation in parallel machines". </title> <booktitle> In 3rd Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 237-245, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: Note that the frequency with which the get work task executes is inversely proportional to the number of envelopes on the local task queue. Thus, it will be scheduled more often on lightly loaded processors, causing them to take more work <ref> [4] </ref>. Performance can be improved even more, in certain cases. Instead of creating new envelopes each time get work receives control, it first checks the running time of the last activity executing within the envelope.
Reference: [5] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: the one hand, programs can be written to make use of the exact amount of available parallelism; i.e. the parallel machine is exposed and there is only one process per processor (PE); either the programmer or an automatic parallelizer (of some data parallel language) generates such restrictive code (see, e.g., <ref> [5] </ref>). The other approach gives the programmer freedom to dynamically create as much parallelism as dictated by the problem and the system allocates one process to each parallel activity in the program. A light-weight process mechanism is needed so that the cost of context switching is not prohibitive.
References-found: 5

