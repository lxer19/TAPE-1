URL: ftp://ftp.cs.sandia.gov/pub/papers/wehart/opt/foga3.tech.ps.gz
Refering-URL: http://www.ing.unlp.edu.ar/cetad/mos/memetic_home.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: wehart@cs.sandia.gov  ftkammeye,rikg@cs.ucsd.edu  
Title: The Role of Development in Genetic Algorithms  
Author: William E. Hart Thomas E. Kammeyer Richard K. Belew 
Note: November 14, 1994  
Address: 01422 P.O. Box 5800 Mail Stop 1110 Albuquerque, NM 87185-1110  La Jolla, CA 92037-0114  
Affiliation: Sandia National Laboratories Department  Computer Science and Engineering University of California, San Diego  
Abstract: Technical Report Number CS94-394 Computer Science and Engineering, U.C.S.D. Abstract The developmental mechanisms transforming genotypic to phenotypic forms are typically omitted in formulations of genetic algorithms (GAs) in which these two representational spaces are identical. We argue that a careful analysis of developmental mechanisms is useful when understanding the success of several standard GA techniques, and can clarify the relationships between more recently proposed enhancements. We provide a framework which distinguishes between two developmental mechanisms | learning and maturation | while also showing several common effects on GA search. This framework is used to analyze how maturation and local search can change the dynamics of the GA. We observe that in some contexts, maturation and local search can be incorporated into the fitness evaluation, but illustrate reasons for considering them seperately. Further, we identify contexts in which maturation and local search can be distinguished from the fitness evaluation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. K. Belew. </author> <title> Interposing an ontogenic model between Genetic Algorithms and Neural Networks. </title> <editor> In J. Cowan, editor, </editor> <booktitle> Advances in Neural Information Processing (NIPS5), </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This representation, however, does not capture the structure of a network. For example, a sorting network can be built by building two networks of half as many inputs, renumbering their inputs so that one operates on A <ref> [1] </ref>; : : : A [N=2 + 1] and the other on A [N=2]; : : : A [N ]), and adding some CMPXs to the result. 3 For example, we would have to deal with or eliminate operations like [5 : 5] or [7 : 2] or any operations [i <p> Two examples will help to illustrate this point. Belew noted an evolutionary bias in his work on evolving polynomials <ref> [1] </ref>. In this work, the GA is used to search for polynomials to fit a chaotic time series. The search space is thus that of polynomials. Belew's representation used a set of rules that governed the growth of a "polynomial network" which computed a polynomial.
Reference: [2] <author> Richard K. Belew and Thomas E. Kammeyer. </author> <title> Evolving aesthetic sorting networks using developmental grammars. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: These authors use learning to refine solutions and speed up the GA. Gruau [4, 5] and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. Similarly, Belew and Kammeyer <ref> [2] </ref> used maturation in an application of the GA to sorting networks. In GAs, we have complete knowledge of all of the algorithm's mechanisms, and distinctions between "learning" and "maturation" may be considered matters of nomenclature. <p> This network is a merge sort based on the well-known Batcher merge. Belew and Kammeyer <ref> [2] </ref> searched for small (width 8) sorting networks using a GA with a representation designed to allow the expression of such structural relationships. In their GA, each genome represents a nonterminal and a sequence of rewrite rules (i.e., grammar productions).
Reference: [3] <author> Richard K. Belew, John McInerney, and Nicol N. Schraudolph. </author> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In Chris G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Proceedings of the Second Conference on Artificial Life, </booktitle> <pages> pages 511-548. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: If ffi 1 does not exist, it is sometimes possible to perform Lamarckian local search with approximations to ffi 1 (e.g., see Hart [7, pages 105-110]). GAs using Lamar-ckian local search are typically more efficient than GAs using non-Lamarckian local search <ref> [3, 8, 11] </ref> when ffi is invertible. Consequently, GAs using Lamarckian local search based an approximation to ffi 1 are likely to be of practical interest. search. 2 Maturation and local search are applied in lines 13a-b. <p> Similarly, a surjective maturation function can also be used to focus the GA's search on a region where the best local minima (and global optima) are located. For example, Belew, Schraudolph and The Role of Development in Genetic Algorithms 11 McInerney <ref> [3] </ref> argue that the best initial weights to perform local search for neural networks have a small absolute magnitude. <p> This maturational transformation focuses the GAs search on phenotypes with small weights. Belew et al. <ref> [3] </ref> observe that GAs which use this maturational transformation with local search find better solutions than GAs which use a uniform maturational transformation. Similarly, a surjective maturation function can be used to bias the coverage of local minima in the search space.
Reference: [4] <author> Frederic Gruau. </author> <title> Genetic synthesis of boolean neural networks with a cell rewriting developmental process. </title> <booktitle> In Intl Workshop on Combinations of Genetic Algorithms and Neural Networks, </booktitle> <pages> pages 55-74, </pages> <year> 1992. </year>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" [16, 17] and local search [8]. These authors use learning to refine solutions and speed up the GA. Gruau <ref> [4, 5] </ref> and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. Similarly, Belew and Kammeyer [2] used maturation in an application of the GA to sorting networks. <p> The maturation function can be used to "decode" the genotype once, after which the fitness is iteratively evaluated on the phenotype. The maturation function used for neural networks in Gruau <ref> [4, 5] </ref> can be distinguished from the fitness evaluation on this basis. Similarly, sorting networks are evaluated with such a decomposable fitness function, since a sorting network's fitness is defined by its performance on several input sequences. <p> For example, Gruau [5] solves a neural network problem by searching for the network architecture and network weights simultaneously. The fitness evaluation is a function of the network, so Ph is the space of network architectures with weights. Gruau <ref> [4] </ref> compares the performance of GAs in which G is a grammar with GAs in which G = Ph. Gruau found that GAs that use the grammar-based genotypic space have better performance.
Reference: [5] <author> Frederic Gruau. </author> <title> Genetic synthesis of modular neural networks. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the 5th Intl. Conference on Genetic Algorithms, </booktitle> <pages> pages 318-325, </pages> <year> 1993. </year>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" [16, 17] and local search [8]. These authors use learning to refine solutions and speed up the GA. Gruau <ref> [4, 5] </ref> and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. Similarly, Belew and Kammeyer [2] used maturation in an application of the GA to sorting networks. <p> The maturation function can be used to "decode" the genotype once, after which the fitness is iteratively evaluated on the phenotype. The maturation function used for neural networks in Gruau <ref> [4, 5] </ref> can be distinguished from the fitness evaluation on this basis. Similarly, sorting networks are evaluated with such a decomposable fitness function, since a sorting network's fitness is defined by its performance on several input sequences. <p> The Role of Development in Genetic Algorithms 14 7 Evolutionary Bias Maturation can also be used to allow a GA to search a genotypic space G 0 that is more easily searched than the genotypic space G = Ph. For example, Gruau <ref> [5] </ref> solves a neural network problem by searching for the network architecture and network weights simultaneously. The fitness evaluation is a function of the network, so Ph is the space of network architectures with weights.
Reference: [6] <author> Frederic Gruau and Darrell Whitley. </author> <title> Adding learning to to the cellular development of neural networks: Evolution and the Baldwin effect. </title> <journal> Evolutionary Computation, </journal> <volume> 3(1) </volume> <pages> 213-233, </pages> <year> 1993. </year>
Reference-contexts: Non-Lamarckian local search transforms the fitness landscape by associating the fitness of a genotype with the fitness of the phenotype generated by a local search algorithm. This type of transformation tends to broaden the "shoulders" of the local minima <ref> [9, 6] </ref>. Hinton and Nowlan [9], Nolfi, Elman and Parisi [18], Keesing and Stork [12] have shown how this type of fitness transformation can improve the rate at which the GA generates good solutions. <p> While this definition of maturation has clarified the discussion in this paper, it precludes other methods of maturation which may be interesting. For example, it precludes maturation methods for which fitness information can be used to evaluate the phenotypic representation even when the phenotype is incomplete. Gruau and Whitley <ref> [6] </ref> describe a similar developmental method which interleaves maturation steps with learning steps. While we have illustrated the utility of developmental mechanisms, we have only The Role of Development in Genetic Algorithms 16 described some of the computational costs which must be considered when using them.
Reference: [7] <author> William E. Hart. </author> <title> Adaptive Global Optimization with Local Search. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <month> May </month> <year> 1994. </year> <title> The Role of Development in Genetic Algorithms 17 </title>
Reference-contexts: We can imagine more general definitions of local search and mutation which use information about a GA's current or previous populations. For example, the definition of local search does not encompass the methods described in Hart <ref> [7] </ref> that use statistics from the population to selectively apply local search. Our notion of mutation does not include the dynamically adjusted mutation operator used in evolutionary strategies, in which the standard deviation is adapted using the frequency of previously successful mutations. <p> Thus, non-LaMarckian local search is still useful in the absence of ffi 1 . If ffi 1 does not exist, it is sometimes possible to perform Lamarckian local search with approximations to ffi 1 (e.g., see Hart <ref> [7, pages 105-110] </ref>). GAs using Lamar-ckian local search are typically more efficient than GAs using non-Lamarckian local search [3, 8, 11] when ffi is invertible. <p> Further, it suggests that local search may not be as important when using this genotypic space since the solutions are already close to the nearby minima. We measured the utility of non-Lamarckian local search for this problem by varying the frequency of local search <ref> [7, 8] </ref>. The experiment compared GAs using the following two genotypic spaces: (a) the bond angles and bond lengths and (b) the bond angles. The space of atom coordinates was the phenotypic space for both GAs. A GA with floating point representation was used to search these genotypic spaces [7]. <p> The experiment compared GAs using the following two genotypic spaces: (a) the bond angles and bond lengths and (b) the bond angles. The space of atom coordinates was the phenotypic space for both GAs. A GA with floating point representation was used to search these genotypic spaces <ref> [7] </ref>. Local search was performed in the coordinate space, using the method of Solis-Wets [16, 21]. The performance of the GAs was measured as the best solution found after 150,000 function evaluations. Results were averaged over 20 trials.
Reference: [8] <author> William E. Hart and Richard K. Belew. </author> <title> Optimization with genetic algorithm hybrids that use local search. In Plastic Individuals in Evolving Populations, </title> <note> 1994. (to appear). </note>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" [16, 17] and local search <ref> [8] </ref>. These authors use learning to refine solutions and speed up the GA. Gruau [4, 5] and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. <p> If ffi 1 does not exist, it is sometimes possible to perform Lamarckian local search with approximations to ffi 1 (e.g., see Hart [7, pages 105-110]). GAs using Lamar-ckian local search are typically more efficient than GAs using non-Lamarckian local search <ref> [3, 8, 11] </ref> when ffi is invertible. Consequently, GAs using Lamarckian local search based an approximation to ffi 1 are likely to be of practical interest. search. 2 Maturation and local search are applied in lines 13a-b. <p> The cost of the transformed fitness evaluation, f ( f (x)), may be substantially greater than the cost of the original fitness evaluation, f (x). However, even given the cost of local search, GAs using non-Lamarckian local search can be more efficient than the GA alone <ref> [8] </ref>. 4.2.2 Maturation Maturation offers a variety of possible fitness transformations, since the maturation function can specify an arbitrary mapping between G and Ph. The following examples illustrate important types of maturation functions that we have identified. Consider maturation functions that are bijections from G to Ph. <p> Further, it suggests that local search may not be as important when using this genotypic space since the solutions are already close to the nearby minima. We measured the utility of non-Lamarckian local search for this problem by varying the frequency of local search <ref> [7, 8] </ref>. The experiment compared GAs using the following two genotypic spaces: (a) the bond angles and bond lengths and (b) the bond angles. The space of atom coordinates was the phenotypic space for both GAs. A GA with floating point representation was used to search these genotypic spaces [7].
Reference: [9] <author> Geoffrey E. Hinton and Steven J. Nowlan. </author> <title> How learning can guide evolution. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 495-502, </pages> <year> 1987. </year>
Reference-contexts: Non-Lamarckian local search transforms the fitness landscape by associating the fitness of a genotype with the fitness of the phenotype generated by a local search algorithm. This type of transformation tends to broaden the "shoulders" of the local minima <ref> [9, 6] </ref>. Hinton and Nowlan [9], Nolfi, Elman and Parisi [18], Keesing and Stork [12] have shown how this type of fitness transformation can improve the rate at which the GA generates good solutions. <p> Non-Lamarckian local search transforms the fitness landscape by associating the fitness of a genotype with the fitness of the phenotype generated by a local search algorithm. This type of transformation tends to broaden the "shoulders" of the local minima [9, 6]. Hinton and Nowlan <ref> [9] </ref>, Nolfi, Elman and Parisi [18], Keesing and Stork [12] have shown how this type of fitness transformation can improve the rate at which the GA generates good solutions. <p> In specific applications, it is important to consider the cost of developmental methods, since it is possible for development to introduce a computational cost which outweighs the improvements in the GA's search. For example, Hinton and Nowlan <ref> [9] </ref>, Nolfi, Elman and Parisi [18], Keesing and Stork [12] describe how using non-Lamarckian local search with GA's improves the rate at which good solutions are generated.
Reference: [10] <author> Richard S. Judson. </author> <title> Teaching polymers to fold. </title> <journal> J. Phys. Chem., </journal> <volume> 96 </volume> <pages> 10102-10104, </pages> <year> 1992. </year>
Reference-contexts: These problems are used in subsequent sections to illustrate the relative roles of maturation and local search. The Role of Development in Genetic Algorithms 5 3.1 A Simple Example Consider the function f (x) = 1=(1 + x 2 ) where G = Ph = <ref> [10; 10] </ref>. Let ffi (x) = sgn (x) 10 be the maturation function. To perform local search, we use a simple gradient method that takes a step of times the gradient, where = 0:5. <p> Such a search would probably be expensive, since the fitness evaluation is expensive, because the network must be tested on 2 width strings after each modification. 3.3 Molecular Conformation A simple molecular conformation problem is taken from Judson <ref> [10] </ref>. Consider a molecule composed of a chain of 19 identical atoms which are connected by stiff springs.
Reference: [11] <author> R.S. Judson, M.E. Colvin, J.C. Meza, A. Huffer, and D. Gutierrez. </author> <title> Do intelligent configuration search techniques outperform random search for large molecules? International Journal of Quantum Chemistry, </title> <address> pages 277-290, </address> <year> 1992. </year>
Reference-contexts: If ffi 1 does not exist, it is sometimes possible to perform Lamarckian local search with approximations to ffi 1 (e.g., see Hart [7, pages 105-110]). GAs using Lamar-ckian local search are typically more efficient than GAs using non-Lamarckian local search <ref> [3, 8, 11] </ref> when ffi is invertible. Consequently, GAs using Lamarckian local search based an approximation to ffi 1 are likely to be of practical interest. search. 2 Maturation and local search are applied in lines 13a-b.
Reference: [12] <author> Ron Keesing and David G. Stork. </author> <title> Evolution and learning in neural networks: The number and distribution of learning trials affect the rate of evolution. </title> <editor> In Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors, </editor> <booktitle> NIPS 3, </booktitle> <pages> pages 804-810. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: For example, a local search algorithm is more likely to find a solution near a minimum when run for many iterations. Keesing and Stork <ref> [12] </ref> apply local search at several different lengths to alleviate this problem when using long searches. 4.2 Comparing Maturation and Local Search Although maturation and non-Lamarckian local search perform a similar transformation of the fitness landscape in Figure 6a, they offer distinctly different approaches to fitness transformation. 5 We realize that <p> This type of transformation tends to broaden the "shoulders" of the local minima [9, 6]. Hinton and Nowlan [9], Nolfi, Elman and Parisi [18], Keesing and Stork <ref> [12] </ref> have shown how this type of fitness transformation can improve the rate at which the GA generates good solutions. Although non-Lamarckian local search really only offers one type of fitness transformation, there is a great deal of flexibility in the application of local search. <p> In specific applications, it is important to consider the cost of developmental methods, since it is possible for development to introduce a computational cost which outweighs the improvements in the GA's search. For example, Hinton and Nowlan [9], Nolfi, Elman and Parisi [18], Keesing and Stork <ref> [12] </ref> describe how using non-Lamarckian local search with GA's improves the rate at which good solutions are generated. However, the computational cost in these analyses is based on the number of generations of the GA, which ignores the costs introduced by the local search.
Reference: [13] <author> H. Kitano. </author> <title> Designing neural networks using genetic algorithms with graph generation systems. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476, </pages> <year> 1990. </year>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" [16, 17] and local search [8]. These authors use learning to refine solutions and speed up the GA. Gruau [4, 5] and Kitano <ref> [13] </ref> have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. Similarly, Belew and Kammeyer [2] used maturation in an application of the GA to sorting networks.
Reference: [14] <author> Mauro Manela, Nina Thornhill, and J.A. Campbell. </author> <title> Fitting spline functions to noisy data using a genetic algorithm. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the 5th Intl. Conference on Genetic Algorithms, </booktitle> <pages> pages 549-553, </pages> <year> 1993. </year>
Reference-contexts: Let D = fx j c i (x) 0g T Solutions in D are known as feasible solutions, and solutions in D D are infeasible solutions. Manela, Thornhill and Campbell <ref> [14] </ref> describes a GA that performs constrained optimization using a representational mapping (decoder) that maps from G to D. Michalewicz and Janikow [15] observe that this type of GA can use a representational map that generates some infeasible solutions, which are mapped to feasible solutions by a repair mechanism.
Reference: [15] <author> Zbigniew Michalewicz and Cezary Z. Janikow. </author> <title> Handling constraints in genetic algorithms. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the 4th Intl. Conference on Genetic Algorithms, </booktitle> <pages> pages 151-157, </pages> <year> 1991. </year>
Reference-contexts: This mapping has been called repair The Role of Development in Genetic Algorithms 12 Frequency Angle/Bond Repn Angle Repn 1.0 0.119 -3.373 0.0625 18.470 -9.450 Table 1: Conformation experiments using a GA and varying local search frequency. by a number of authors <ref> [15, 19] </ref>. For example, consider constrained optimization problems. <p> Manela, Thornhill and Campbell [14] describes a GA that performs constrained optimization using a representational mapping (decoder) that maps from G to D. Michalewicz and Janikow <ref> [15] </ref> observe that this type of GA can use a representational map that generates some infeasible solutions, which are mapped to feasible solutions by a repair mechanism. Since the representation map may generate infeasible solutions, it is equivalent to the ~ ffi function described above.
Reference: [16] <author> H. Muhlenbein, M. Schomisch, and J. Born. </author> <title> The parallel genetic algorithm as function optimizer. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth Intl. Conf. on Genetic Algorithms, </booktitle> <pages> pages 271-278. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" <ref> [16, 17] </ref> and local search [8]. These authors use learning to refine solutions and speed up the GA. Gruau [4, 5] and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation. <p> The space of atom coordinates was the phenotypic space for both GAs. A GA with floating point representation was used to search these genotypic spaces [7]. Local search was performed in the coordinate space, using the method of Solis-Wets <ref> [16, 21] </ref>. The performance of the GAs was measured as the best solution found after 150,000 function evaluations. Results were averaged over 20 trials. Table 1 shows the performance for the GAs using different genotypic spaces and different local search frequencies.
Reference: [17] <author> Heinz Muhlenbein. </author> <title> Evolution in time and space the parallel genetic algorithm. </title> <editor> In Gregory J.E. Rawlins, editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> pages 316-337. </pages> <address> Morgan-Kauffmann, </address> <year> 1991. </year>
Reference-contexts: Maturation and learning have been used with GAs in several contexts. Learning has been studied under the guise of "hill climbing" <ref> [16, 17] </ref> and local search [8]. These authors use learning to refine solutions and speed up the GA. Gruau [4, 5] and Kitano [13] have used maturation in applications of GAs to neural networks. These authors used maturation to construct neural networks from a grammar representation.
Reference: [18] <author> Stefano Nolfi, Jeffrey L. Elman, and Domenico Parisi. </author> <title> Learning and evolution in neural networks. </title> <type> Technical Report CRL 9019, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Non-Lamarckian local search transforms the fitness landscape by associating the fitness of a genotype with the fitness of the phenotype generated by a local search algorithm. This type of transformation tends to broaden the "shoulders" of the local minima [9, 6]. Hinton and Nowlan [9], Nolfi, Elman and Parisi <ref> [18] </ref>, Keesing and Stork [12] have shown how this type of fitness transformation can improve the rate at which the GA generates good solutions. Although non-Lamarckian local search really only offers one type of fitness transformation, there is a great deal of flexibility in the application of local search. <p> In specific applications, it is important to consider the cost of developmental methods, since it is possible for development to introduce a computational cost which outweighs the improvements in the GA's search. For example, Hinton and Nowlan [9], Nolfi, Elman and Parisi <ref> [18] </ref>, Keesing and Stork [12] describe how using non-Lamarckian local search with GA's improves the rate at which good solutions are generated. However, the computational cost in these analyses is based on the number of generations of the GA, which ignores the costs introduced by the local search.
Reference: [19] <author> David Orvosh and Lawrence Davis. </author> <title> Shall we repair? genetic algorithms, combinatorial optimization, and feasibility constraints. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the 5th Intl. Conference on Genetic Algorithms, </booktitle> <pages> page 650, </pages> <year> 1993. </year>
Reference-contexts: This mapping has been called repair The Role of Development in Genetic Algorithms 12 Frequency Angle/Bond Repn Angle Repn 1.0 0.119 -3.373 0.0625 18.470 -9.450 Table 1: Conformation experiments using a GA and varying local search frequency. by a number of authors <ref> [15, 19] </ref>. For example, consider constrained optimization problems.
Reference: [20] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vet-terling. </author> <title> Numerical Recipies in C The Art of Scientific Computing. </title> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference-contexts: A local search algorithm employs information about the fitness landscape when performing a search. For example, the fitness values of individuals from previous iterations are typically used. For smooth, differentiable functions, sophisticated methods have been developed that use derivative information to guide the search (e.g., conjugate gradient <ref> [20] </ref>). Because information about the fitness function is employed, a local search algorithm can be modeled as a mapping f : Ph ! Ph.
Reference: [21] <author> F.J. Solis and R.J-B. Wets. </author> <title> Minimization by random search techniques. </title> <journal> Mathematical Operations Research, </journal> <volume> 6 </volume> <pages> 19-30, </pages> <year> 1981. </year>
Reference-contexts: The space of atom coordinates was the phenotypic space for both GAs. A GA with floating point representation was used to search these genotypic spaces [7]. Local search was performed in the coordinate space, using the method of Solis-Wets <ref> [16, 21] </ref>. The performance of the GAs was measured as the best solution found after 150,000 function evaluations. Results were averaged over 20 trials. Table 1 shows the performance for the GAs using different genotypic spaces and different local search frequencies.
References-found: 21

