URL: http://www.ma.ic.ac.uk/statistics/links/ralinks/dgtd.link/cart.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail:d.denison@ic.ac.uk  
Title: A Bayesian CART Algorithm  
Author: By DAVID G.T. DENISON, BANI K. MALLICK and ADRIAN F.M. SMITH 
Keyword: Some key words: Bayesian methods; Classification trees; Regression trees; Reversible jump Markov chain Monte Carlo.  
Date: Summary  
Address: College, London, SW7 2BZ, U.K.  
Affiliation: Department of Mathematics, Imperial  
Abstract: A stochastic search form of Classification and Regression Tree (CART) analysis (Breiman et al., 1984) is proposed, motivated by a Bayesian model. An approximation to a probability distribution over the space of possible trees is explored using reversible jump Markov Chain Monte Carlo methods (Green, 1995). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Becker, R., Chambers, J.M. & Wilks, A. </author> <year> (1988). </year> <title> The New S Language. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: T min is the chosen minimum number of data points assigned to each terminal node. This is commonly taken to be 5, as in the tree.control function in Splus <ref> (Becker et al., 1988) </ref>. For illustration, in what follows we shall use this value for T min . <p> As in Cleveland & Devlin (1988) we work with the cube root of ozone. This dataset is known as `air' and is available in Splus <ref> (Becker et al., 1988) </ref>. For illustration we focus on wind speed and temperature. We initially remove a smooth effect for radiation and fit the tree to the partial residuals as a function of temperature and wind. A similar approach using was adopted in Hastie & Tibshirani (1990, pp. 271-4). <p> Figs. 4 and 5 about here 32 Classification tree We illustrate our method of producing a classification tree using the `kyphosis' dataset which is also available in Splus <ref> (Becker et al., 1988) </ref>. It is a binary dataset which consists of measurements on 81 children after corrective 18 spinal surgery.
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R. & Stone, C.J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Prediction is determined by terminal nodes, and takes the form either of a class level in classification problems, or the average of the response variable in least squares regression problems <ref> (Breiman et al., 1984, Chapters 2 and 8) </ref>. A tree T has a root node whose descendant nodes, also known as daughters, can be divided into terminal nodes and split nodes. <p> The tree itself is grown as follows <ref> (Breiman et al., 1984, Chapter 2) </ref>. 2 At each node do the following 1. Examine every allowable split on each predictor variable. Usually the binary splits are generated by binary questions. 2. Select and execute the `best' of these splits. 3. <p> Those cases in t answering `yes' go to the left descendant node and those answering `no' to the right descendant node. The `best' in step 2 above is assessed in terms of some choice of goodness-of-split criterion <ref> (Breiman et al., 1984, Chapter 4) </ref>. Two popular criteria are Least Squares and Least Absolute Deviations. Both afford a comparison based on a subadditive `between/within' decomposition, where `between' alludes to the homogeneity or loss measure applied to the parent node. <p> Both afford a comparison based on a subadditive `between/within' decomposition, where `between' alludes to the homogeneity or loss measure applied to the parent node. The rules for step 3 that have been considered seem not to work well in practice <ref> (Breiman et al., 1984, pp. 59-62) </ref>. The tree tends to grow too big and have too few data points in each terminal node to make the study worthwhile. <p> Different ways 3 of doing this are examined in Breiman et al. (1984, Chapter 3), with the most common method being the minimal cost complexity procedure <ref> (Breiman et al., 1984, pp. 66-71) </ref>. Frequently, the pruned subtree is constrained to have more than some minimum number of data points in each terminal node. At each node, the tree algorithm searches through the variables one by one, beginning with X 1 and continuing up to X m . <p> Instead of producing just one tree which is, in a sense, a `point' estimate of the `true' structure, as in the classical CART method, we produce a variety of tree structures together with relative weights. The Bayesian analogue of `pruning the tree' <ref> (Breiman et al., 1984, Chapter 3) </ref> is achieved by putting a suitably chosen prior distribution over the number of terminal nodes in the structure, and, in the case of classification trees, a distribution over the classification of data points within each node. Details are given in x22.
Reference: <author> Bruntz, S.M., Cleveland, W.S., Kleiner, B. & Warner, J.L. </author> <year> (1974). </year> <title> The dependence of ambient ozone on solar radiation, temperature and mixing height. </title> <booktitle> In Symposium on atmospheric diffusion and air pollution, </booktitle> <pages> pp. 125-8. </pages> <address> Boston: </address> <publisher> American Meteorological Society pp. </publisher> <pages> 125-8. </pages>
Reference: <author> Clark, L.A. & Pregibon, D. </author> <year> (1992). </year> <title> Tree based models. In Statistical Models in S, </title> <editor> Ed. J.M. Chambers & T.J. </editor> <booktitle> Hastie, </booktitle> <pages> pp. 377-420. </pages> <address> Pacific Grove, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: the ff i s, for the 2 i , (ff i ) = constant; ( 2 i ) = Ga (10 2 ; 10 2 ): For classification trees, we assume that each data point y i (i = 1; : : : ; n) comes from a Multinomial distribution <ref> (Clark & Pregibon, 1992) </ref>. <p> These were calculated using the equation RSS = i=1 j2T i where y t i is the response mean in terminal node t i . We used the Splus func tions tree and prune.tree to find the standard CART structure <ref> (Clark & 17 Pregibon, 1992) </ref> and we display the result with the same number of terminal nodes as our modal structure in Fig. 3.
Reference: <author> Cleveland, W.S. & Devlin, S.J. </author> <year> (1988). </year> <title> Locally-weighted regression: an approach to regression analysis by local fitting. </title> <journal> J. Am. Statist. Assoc. </journal> <volume> 83, </volume> <pages> 597-610. </pages>
Reference: <author> Gelfand, A.E & Smith, A.F.M. </author> <year> (1990). </year> <title> Sampling-based approaches to calculating marginal densities. </title> <journal> J. Am. Statist. Assoc. </journal> <volume> 85, </volume> <pages> 398-409. </pages>
Reference-contexts: If we assume that maximum likelihood estimates are used throughout then there is nothing more to sample from in the classification tree case but in the regression tree case we need to update the i s. This is done straightforwardly using a Gibbs step <ref> (Gelfand & Smith, 1990) </ref> as we know the full conditionals of the i s which are ( 2 i ) ~ Ga @ 10 2 + 2 1 X (y j ff i ) 2 A ; where the i subscript denotes all those elements which do not have index i.
Reference: <author> Green, P.J. </author> <year> (1995). </year> <title> Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. </title> <type> Biometrika 82 711-32. 24 Hastie, T.J. </type> & <address> Tibshirani, R.J. </address> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: We 6 change the dimension of the model when we change k and so, for Bayesian computation, we use a reversible jump Markov chain Monte Carlo approach <ref> (Green, 1995) </ref> when we are considering changes in the number of terminal nodes in the tree structure. <p> It will often be useful to consider this in the factorised form p (k; (k) jy) = p (kjy)p ( (k) jk; y): We will generate samples from the joint posterior of (k; (k) ) by using a class of reversible jump Metropolis-Hastings algorithms <ref> (Green, 1995) </ref>. Full details of the method can be found in the reference cited.
Reference: <author> Hastings, W.K. </author> <year> (1970). </year> <title> Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrika 57, </journal> <pages> 97-109. </pages>
Reference: <author> Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. & Teller, E. </author> <year> (1953). </year> <title> Equations of state calculations by fast computing machines. </title> <journal> J. Chem. Phy. </journal> <volume> 21, </volume> <pages> 1087-91. </pages>

References-found: 9

