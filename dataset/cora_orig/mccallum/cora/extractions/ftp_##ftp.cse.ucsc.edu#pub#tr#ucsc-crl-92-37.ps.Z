URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-92-37.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Email: haussler@cse.ucsc.edu  barron@andrew.stat.uiuc.edu  
Title: How Well do Bayes Methods Work for On-Line Prediction of f1g values?  
Author: D. Haussler A. Barron Wright Street 
Address: Santa Cruz, CA 95064  Champaign IL 61820  
Affiliation: Baskin Center for Computer Engineering and Information Sciences University of California,  Dept. of Statistics, U. Ill.  
Abstract: Technical Report UCSC-CRL-92-37 Computer and Information Sciences University of California at Santa Cruz July, 1992 Abstract We look at sequential classification and regression problems in which f1g-labeled instances are given on-line, one at a time, and for each new instance, before seeing the label, the learning system must either predict the label, or estimate the probability that the label is +1. We look at the performance of Bayes method for this task, as measured by the total number of mistakes for the classification problem, and by the total log loss (or information gain) for the regression problem. Our results are given by comparing the performance of Bayes method to the performance of a hypothetical "omniscient scientist" who is able to use extra information about the labeling process that would not be available in the standard learning protocol. The results show that Bayes methods perform only slightly worse than the omniscient scientist in many cases. These results generalize previous results of Haussler, Kearns and Schapire, and Opper and Haussler. 
Abstract-found: 1
Intro-found: 1
Reference: [AFS92] <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 605-619, </pages> <year> 1992. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> In particular, the total loss of the omniscient scientist is usually linear in n, whereas the additional loss of Bayes method is only logarithmic in n. We obtain upper bounds on this additional loss that generalize related bounds obtained in [HKS91] and [OH91a] (see also <ref> [Ama92, AFS92, SST92] </ref>). We also look at the performance of the Bayes method on an arbitrary sequence of examples, as compared with the performance of an omniscient scientist who already knows the best target function to use in predicting the labels of that particular sequence of examples. <p> Finally, it would be nice to have good bounds on the average loss for the nth example, in addition to bounds on the average total loss for the first n examples. Yamanishi gives bounds of this type for some special cases [Yam91, Yam92] (see also <ref> [Ama92, AFS92] </ref>). The former quantity is what is usually displayed as a "learning curve".
Reference: [Ama92] <author> S. Amari. </author> <title> A universal theorem on learning curves. </title> <type> Unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> In particular, the total loss of the omniscient scientist is usually linear in n, whereas the additional loss of Bayes method is only logarithmic in n. We obtain upper bounds on this additional loss that generalize related bounds obtained in [HKS91] and [OH91a] (see also <ref> [Ama92, AFS92, SST92] </ref>). We also look at the performance of the Bayes method on an arbitrary sequence of examples, as compared with the performance of an omniscient scientist who already knows the best target function to use in predicting the labels of that particular sequence of examples. <p> Finally, it would be nice to have good bounds on the average loss for the nth example, in addition to bounds on the average total loss for the first n examples. Yamanishi gives bounds of this type for some special cases [Yam91, Yam92] (see also <ref> [Ama92, AFS92] </ref>). The former quantity is what is usually displayed as a "learning curve".
Reference: [Bar87] <author> Andrew Barron. In T. M. Cover and B. Gopinath, </author> <title> editors, Open Problems in Communication and Computation, chapter 3.20. Are Bayes rules consistent in information?, </title> <address> pages 85-91. </address> <year> 1987. </year> <month> 25 </month>
Reference-contexts: The same result follows similarly for continuous fi (see <ref> [Bar87] </ref>). 13 From (17) and (19), in analogy with (13), we have r ( fl ) inf frn log P (N r ( fl ))g: (20) 3.3 Bayes Risk Finally we look at the Bayes risk.
Reference: [BEHW89] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. War--muth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: The VC Dimension of F , denoted dim (F ), is defined as the length m of the longest such shattered sequence, over all possible finite sequences of points in X. Further discussion and examples of this concept can be found in <ref> [Vap82, BEHW89] </ref>. 16 Now let us consider the case where F = ff : 2 fig and x 1 ; : : : ; x n is a fixed instance sequence.
Reference: [CB90] <author> Bertrand Clarke and Andrew Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: This can happen when Y is continuous and the index set fi gives a smooth enough finite dimensional real vector-valued parameterization of the class fP : 2 fig (see e.g. <ref> [Ris86, CB90, Dawa, Yam92] </ref>). <p> of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy.
Reference: [CT91] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: argmin ^ P 2A X P (Y t = yjy t1 ) log ^ P (y) = P Y t jy t1 (whenever this distribution is in A) (2) The latter equality follows from the fact that the relative entropy 1 is minimal between a distribution and itself (see e.g. <ref> [CT91] </ref>). Hence in this case Bayes method simply produces the posterior distribution on Y t as its action.
Reference: [Dawa] <author> A.P. Dawid. </author> <title> Prequential analysis, stochastic complexity and Bayesian inference. Bayesian Statistics 4. </title> <note> To appear. </note>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> This can happen when Y is continuous and the index set fi gives a smooth enough finite dimensional real vector-valued parameterization of the class fP : 2 fig (see e.g. <ref> [Ris86, CB90, Dawa, Yam92] </ref>). <p> of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy.
Reference: [Dawb] <author> A.P. Dawid. </author> <title> Prequential data analysis. Current Issues in Statistical Inference. </title> <note> To appear. </note>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability.
Reference: [Daw84] <author> A.P. Dawid. </author> <title> Statistical theory: The prequential approach. </title> <journal> J. Roy. Statist. Soc. A, </journal> <pages> pages 278-292, </pages> <year> 1984. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> The notion that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to extract information about parameters, is known as the prequential approach in statistics <ref> [Daw84] </ref>. To measure the performance of a sequential regression system of this type, it is common to use the log loss function. <p> In the extreme case, even though the Bayes method uses a prior distribution, we analyze the performance of the method assuming nothing about the way the actual outcome sequence is generated <ref> [Daw84, Vov90b, LW89] </ref>. We are often interested in certain special types of prior information that may be available to help choose an appropriate action. The type of prior information available determines the kind of learning problem one has.
Reference: [DMW88] <author> Alfredo DeSantis, George Markowski, and Mark N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This observation was made in <ref> [DMW88] </ref>. However, it may be something of a "folk theorem" in the statistics/information theory community.
Reference: [Efr79] <author> S. Yu. Efroimovich. </author> <title> Information contained in a sequence of observations. Problems in Information Transmission, </title> <booktitle> 15 </booktitle> <pages> 178-189, </pages> <year> 1979. </year>
Reference-contexts: of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy.
Reference: [FM92] <author> Meir Feder and Neri Merhav. </author> <title> Relations between entropy and error probability. </title> <type> unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: Again, in many cases Bayes methods do not do much worse. These results extend work in [Vov90b, LW89], and also ties in with the coding/information theory approach in <ref> [MF92, FMG92, FM92] </ref> (see also [Yu]). Throughout the paper, our emphasis is on obtaining performance bounds that hold for all sample sizes n, rather than asymptotic bounds that hold only for large n. 2 Formal Framework Here we outline the general decision theoretic framework we use.
Reference: [FMG92] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE Trans. Info. Th., </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: Again, in many cases Bayes methods do not do much worse. These results extend work in [Vov90b, LW89], and also ties in with the coding/information theory approach in <ref> [MF92, FMG92, FM92] </ref> (see also [Yu]). Throughout the paper, our emphasis is on obtaining performance bounds that hold for all sample sizes n, rather than asymptotic bounds that hold only for large n. 2 Formal Framework Here we outline the general decision theoretic framework we use.
Reference: [GT90] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In K. Thuemann and R. Koeberle, editors, </editor> <title> Neural Networks and Spin Glasses. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: Many of our techniques should generalize easily to other kinds of outcome spaces, as well as other decision spaces and loss functions. Some of these techniques may also help in analyzing other learning methods, such as the "Gibbs" method <ref> [GT90, HKS91, OH91b, OH91a, SST92] </ref>. However, a major problem remaining is to develop equally simple and general techniques to obtain lower bounds on the risk, so that we can see how tight these upper bounds are.
Reference: [HKLW91] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95 </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference-contexts: The latter function is often called a learning curve (see also <ref> [HKLW91] </ref>). Sequential regression problems have also been studied [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92].
Reference: [HKS] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning. </journal> <note> to appear. </note>
Reference-contexts: Some general results on the performance of Bayes methods in this case for noise-free outcomes are reported in <ref> [HKS] </ref>. These results should be (and can be) extended to the noisy case. Finally, it would be nice to have good bounds on the average loss for the nth example, in addition to bounds on the average total loss for the first n examples.
Reference: [HKS91] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the Fourth Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <year> 1991. </year> <month> 26 </month>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> In particular, the total loss of the omniscient scientist is usually linear in n, whereas the additional loss of Bayes method is only logarithmic in n. We obtain upper bounds on this additional loss that generalize related bounds obtained in <ref> [HKS91] </ref> and [OH91a] (see also [Ama92, AFS92, SST92]). <p> The performance of Bayes methods for this case was studied in <ref> [HKS91, OH91b] </ref>. Case 2: Functions corrupted by i.i.d. noise. Here a state of nature is represented by a function, but the the observations are altered by an independent noise process. <p> Here we also average over random choices of the instances x n = (x 1 ; : : : ; x n ), and instead of using the VC dimension, we use the VC entropy [VC71] (see also <ref> [HKS91] </ref>). Let Q be a distribution on the instance space X, and assume that each x t is chosen independently at random according to Q. <p> Equation (42) looks a bit strange at first, but all that is really being introduced here is superfluous averaging over possibilities that we are already averaging over in the outermost sum. Thus the overall expectation is unchanged. This method is analogous to that used in <ref> [HKS91] </ref>. The last equation, (43), follows directly from the definition of the noise model. It is easily verified that we always have p t 1 . We now derive analogous equations for the 0-1 loss. <p> However, a direct analysis yields a bound that is better by a factor of 2. As above we consider only the case of f1g-valued functions times independent sign noise with rate . Our analysis is similar to that given in <ref> [HKS91] </ref> for the noise-free case, and generalizes the corresponding results given there. It also parallels the analysis of the previous section. <p> P (Y t = +1jy t1 ), it follows from (49) and (51) that R C R 23 where 1 ln 2 C 2. ln 2 and from (52) we get R 01 (P ) 1 ln (P ) = 1 log 2 (P ); which was the result from <ref> [HKS91] </ref>. <p> Many of our techniques should generalize easily to other kinds of outcome spaces, as well as other decision spaces and loss functions. Some of these techniques may also help in analyzing other learning methods, such as the "Gibbs" method <ref> [GT90, HKS91, OH91b, OH91a, SST92] </ref>. However, a major problem remaining is to develop equally simple and general techniques to obtain lower bounds on the risk, so that we can see how tight these upper bounds are.
Reference: [HLW90] <author> David Haussler, Nick Littlestone, and Manfred Warmuth. </author> <title> Predicting f0; 1g--functions on randomly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> December </month> <year> 1990. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples.
Reference: [Kul59] <author> Solomon Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: P (y n j fl ) log P (y n ) = I (P Y n j fl k P Y n ); (17) where I (P k Q) = E P log P (X) Q (X) denotes the relative entropy or Kullback-Leibler divergence between distribution P and distribution Q <ref> [Kul59] </ref>.
Reference: [Lit89] <author> Nick Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> L T bayes;P;01 (y n ) = jft : y t 6= ^y t ; 1 t ngj: (35) This is often called the number of mistakes. 4.1 Performance on Arbitrary Outcome Sequences Littlestone and Warmuth <ref> [LW89, Lit89] </ref> and Vovk [Vov90b] have obtained bounds on the number of mistakes made by Bayes method for arbitrary f1g-valued outcome sequences for the case that fi is a countable class ff : 2 fig of functions and the prior information assumes that outcomes are generated by applying sign noise with
Reference: [LW89] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In 30th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-261, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> Again, in many cases Bayes methods do not do much worse. These results extend work in <ref> [Vov90b, LW89] </ref>, and also ties in with the coding/information theory approach in [MF92, FMG92, FM92] (see also [Yu]). <p> In the extreme case, even though the Bayes method uses a prior distribution, we analyze the performance of the method assuming nothing about the way the actual outcome sequence is generated <ref> [Daw84, Vov90b, LW89] </ref>. We are often interested in certain special types of prior information that may be available to help choose an appropriate action. The type of prior information available determines the kind of learning problem one has. <p> L T bayes;P;01 (y n ) = jft : y t 6= ^y t ; 1 t ngj: (35) This is often called the number of mistakes. 4.1 Performance on Arbitrary Outcome Sequences Littlestone and Warmuth <ref> [LW89, Lit89] </ref> and Vovk [Vov90b] have obtained bounds on the number of mistakes made by Bayes method for arbitrary f1g-valued outcome sequences for the case that fi is a countable class ff : 2 fig of functions and the prior information assumes that outcomes are generated by applying sign noise with <p> The bound from <ref> [LW89] </ref> is particularly easy to derive, and generalizes to continuous fi easily as well. We give a variant of this derivation now. First, let us define the Heavyside function fi by letting fi (x) = 1 if x 0 and fi (x) = 0 if x &lt; 0.
Reference: [MF92] <author> Neri Merhav and Meir Feder. </author> <title> Universal sequential learning and decision from individual data sequences. </title> <booktitle> The 1992 Workshop on Computational Learning Theory, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> Again, in many cases Bayes methods do not do much worse. These results extend work in [Vov90b, LW89], and also ties in with the coding/information theory approach in <ref> [MF92, FMG92, FM92] </ref> (see also [Yu]). Throughout the paper, our emphasis is on obtaining performance bounds that hold for all sample sizes n, rather than asymptotic bounds that hold only for large n. 2 Formal Framework Here we outline the general decision theoretic framework we use.
Reference: [OH91a] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Computational Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <pages> pages 75-87. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> In particular, the total loss of the omniscient scientist is usually linear in n, whereas the additional loss of Bayes method is only logarithmic in n. We obtain upper bounds on this additional loss that generalize related bounds obtained in [HKS91] and <ref> [OH91a] </ref> (see also [Ama92, AFS92, SST92]). We also look at the performance of the Bayes method on an arbitrary sequence of examples, as compared with the performance of an omniscient scientist who already knows the best target function to use in predicting the labels of that particular sequence of examples. <p> The performance of Bayes method for this case was studied in <ref> [OH91a] </ref> for a particular class of functions. Case 3: Conditionally independent Y t s. In this case, the random variables Y 1 ; : : : ; Y n are conditionally independent given and x 1 ; : : : ; x n (and completely independent of the actions taken). <p> Many of our techniques should generalize easily to other kinds of outcome spaces, as well as other decision spaces and loss functions. Some of these techniques may also help in analyzing other learning methods, such as the "Gibbs" method <ref> [GT90, HKS91, OH91b, OH91a, SST92] </ref>. However, a major problem remaining is to develop equally simple and general techniques to obtain lower bounds on the risk, so that we can see how tight these upper bounds are.
Reference: [OH91b] <author> M. Opper and D. Haussler. </author> <title> Generalization performance of Bayes optimal classification algorithm for learning a perceptron. </title> <journal> Physical Review Letters, </journal> <volume> 66(20) </volume> <pages> 2677-2680, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The performance of Bayes methods for this case was studied in <ref> [HKS91, OH91b] </ref>. Case 2: Functions corrupted by i.i.d. noise. Here a state of nature is represented by a function, but the the observations are altered by an independent noise process. <p> Many of our techniques should generalize easily to other kinds of outcome spaces, as well as other decision spaces and loss functions. Some of these techniques may also help in analyzing other learning methods, such as the "Gibbs" method <ref> [GT90, HKS91, OH91b, OH91a, SST92] </ref>. However, a major problem remaining is to develop equally simple and general techniques to obtain lower bounds on the risk, so that we can see how tight these upper bounds are.
Reference: [Ris86] <author> Jorma Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: This can happen when Y is continuous and the index set fi gives a smooth enough finite dimensional real vector-valued parameterization of the class fP : 2 fig (see e.g. <ref> [Ris86, CB90, Dawa, Yam92] </ref>). <p> of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy.
Reference: [Sau72] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (Series A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Let us define the partition of fi by letting () = ( fl ) $ f (x t ) = f fl By the most basic theorem connected with the VC dimension, known as the Sauer/VC lemma <ref> [VC71, Sau72] </ref>, the number jj of distinct equivalence classes in is bounded by jj i=0 n ! d ; (29) where d = dim (F ) and e is the base of the natural logarithm. (The latter inequality holds for n d.) It follows that for any prior P on fi,
Reference: [Sch78] <author> Gideon Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy.
Reference: [SST92] <author> H.S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review A, </journal> <volume> 45(8) </volume> <pages> 6056-6091, </pages> <month> April 15, </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> In particular, the total loss of the omniscient scientist is usually linear in n, whereas the additional loss of Bayes method is only logarithmic in n. We obtain upper bounds on this additional loss that generalize related bounds obtained in [HKS91] and [OH91a] (see also <ref> [Ama92, AFS92, SST92] </ref>). We also look at the performance of the Bayes method on an arbitrary sequence of examples, as compared with the performance of an omniscient scientist who already knows the best target function to use in predicting the labels of that particular sequence of examples. <p> Many of our techniques should generalize easily to other kinds of outcome spaces, as well as other decision spaces and loss functions. Some of these techniques may also help in analyzing other learning methods, such as the "Gibbs" method <ref> [GT90, HKS91, OH91b, OH91a, SST92] </ref>. However, a major problem remaining is to develop equally simple and general techniques to obtain lower bounds on the risk, so that we can see how tight these upper bounds are.
Reference: [Vap82] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The VC Dimension of F , denoted dim (F ), is defined as the length m of the longest such shattered sequence, over all possible finite sequences of points in X. Further discussion and examples of this concept can be found in <ref> [Vap82, BEHW89] </ref>. 16 Now let us consider the case where F = ff : 2 fig and x 1 ; : : : ; x n is a fixed instance sequence. <p> It would be nice to have more general bounds for 0-1 loss. There also many cases of interest in which the class of functions indexed by fi can be decomposed into classes F 1 ; F 2 ; : : : of increasing VC dimension <ref> [Vap82] </ref>, including the case where fi consists of a sequence of smooth real parameterizations as above of increasing dimension [Yam92]. Some general results on the performance of Bayes methods in this case for noise-free outcomes are reported in [HKS].
Reference: [VC71] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-80, </pages> <year> 1971. </year> <month> 27 </month>
Reference-contexts: Let us define the partition of fi by letting () = ( fl ) $ f (x t ) = f fl By the most basic theorem connected with the VC dimension, known as the Sauer/VC lemma <ref> [VC71, Sau72] </ref>, the number jj of distinct equivalence classes in is bounded by jj i=0 n ! d ; (29) where d = dim (F ) and e is the base of the natural logarithm. (The latter inequality holds for n d.) It follows that for any prior P on fi, <p> Here we also average over random choices of the instances x n = (x 1 ; : : : ; x n ), and instead of using the VC dimension, we use the VC entropy <ref> [VC71] </ref> (see also [HKS91]). Let Q be a distribution on the instance space X, and assume that each x t is chosen independently at random according to Q.
Reference: [Vov90a] <author> V. G. Vovk. </author> <title> Prequential probability theory. </title> <type> unpublished manuscript, </type> <year> 1990. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability.
Reference: [Vov90b] <author> Volodimir Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Several recent papers in the area of computational learning theory have studied sequential classification problems in which f1g-labeled instances (examples) are given on-line, one at a time, and for each new instance, the learning system must predict the label before it sees it <ref> [HLW90, Lit89, LW89, Vov90b, HKS91, OH91a, SST92, MF92] </ref>. Such systems adapt on-line, learning to make better predictions as they see more examples. <p> Again, in many cases Bayes methods do not do much worse. These results extend work in <ref> [Vov90b, LW89] </ref>, and also ties in with the coding/information theory approach in [MF92, FMG92, FM92] (see also [Yu]). <p> In the extreme case, even though the Bayes method uses a prior distribution, we analyze the performance of the method assuming nothing about the way the actual outcome sequence is generated <ref> [Daw84, Vov90b, LW89] </ref>. We are often interested in certain special types of prior information that may be available to help choose an appropriate action. The type of prior information available determines the kind of learning problem one has. <p> L T bayes;P;01 (y n ) = jft : y t 6= ^y t ; 1 t ngj: (35) This is often called the number of mistakes. 4.1 Performance on Arbitrary Outcome Sequences Littlestone and Warmuth [LW89, Lit89] and Vovk <ref> [Vov90b] </ref> have obtained bounds on the number of mistakes made by Bayes method for arbitrary f1g-valued outcome sequences for the case that fi is a countable class ff : 2 fig of functions and the prior information assumes that outcomes are generated by applying sign noise with known rate (even though
Reference: [Vov92] <author> V. G. Vovk. </author> <title> Universal forcasting algorithms. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 245-277, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability.
Reference: [Yam91] <author> Kenji Yamanishi. </author> <title> A loss bound model for on-line stochastic prediction strategies. </title> <editor> In L. G. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 290-302, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy. <p> Finally, it would be nice to have good bounds on the average loss for the nth example, in addition to bounds on the average total loss for the first n examples. Yamanishi gives bounds of this type for some special cases <ref> [Yam91, Yam92] </ref> (see also [Ama92, AFS92]). The former quantity is what is usually displayed as a "learning curve".
Reference: [Yam92] <author> Kenji Yamanishi. </author> <title> A Statistical Approach to Computational Learning Theory. </title> <type> PhD thesis, </type> <institution> University of Tokyo, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The latter function is often called a learning curve (see also [HKLW91]). Sequential regression problems have also been studied <ref> [Daw84, Dawa, Dawb, Vov90a, Vov92, Yam91, Yam92, Ama92, AFS92, SST92, MF92] </ref>. In this case, instead of predicting either +1 or 1, the learning system outputs a probability distribution, predicting that the label will be +1 with a certain probability, and 1 with one minus that probability. <p> This can happen when Y is continuous and the index set fi gives a smooth enough finite dimensional real vector-valued parameterization of the class fP : 2 fig (see e.g. <ref> [Ris86, CB90, Dawa, Yam92] </ref>). <p> of Bayes methods under log loss are available for the case when fi is a compact subset of R d and the relative entropy I (P k P fl ) is twice continuously differentiable at = fl for almost all fl 2 fi, so that the Fisher information is well-defined <ref> [Sch78, Efr79, Ris86, CB90, Dawa, Yam91, Yam92] </ref>. This is very often not the case for a discrete outcome space such as f1g, so we have concentrated on more general bounds here, which involve much weaker assumptions, such as finiteness of the VC dimension or bounds on the VC entropy. <p> many cases of interest in which the class of functions indexed by fi can be decomposed into classes F 1 ; F 2 ; : : : of increasing VC dimension [Vap82], including the case where fi consists of a sequence of smooth real parameterizations as above of increasing dimension <ref> [Yam92] </ref>. Some general results on the performance of Bayes methods in this case for noise-free outcomes are reported in [HKS]. These results should be (and can be) extended to the noisy case. <p> Finally, it would be nice to have good bounds on the average loss for the nth example, in addition to bounds on the average total loss for the first n examples. Yamanishi gives bounds of this type for some special cases <ref> [Yam91, Yam92] </ref> (see also [Ama92, AFS92]). The former quantity is what is usually displayed as a "learning curve".
Reference: [Yu] <author> Bin Yu. </author> <title> On optimal rate universal d-semifaithful coding. </title> <journal> IEEE Transactions on Information Theory. </journal> <note> To appear. 28 </note>
Reference-contexts: Again, in many cases Bayes methods do not do much worse. These results extend work in [Vov90b, LW89], and also ties in with the coding/information theory approach in [MF92, FMG92, FM92] (see also <ref> [Yu] </ref>). Throughout the paper, our emphasis is on obtaining performance bounds that hold for all sample sizes n, rather than asymptotic bounds that hold only for large n. 2 Formal Framework Here we outline the general decision theoretic framework we use.
References-found: 36

