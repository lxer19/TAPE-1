URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/95-47.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: Symmetrical Hopping: A Scalable Scheduling Algorithm for Irregular Problems  
Author: Min-You Wu 
Address: Buffalo, NY 14260  
Affiliation: Department of Computer Science State University of New York at Buffalo  
Abstract: A runtime support is necessary for parallel computations with irregular and dynamic structures. One important component in the support system is the runtime scheduler which balances the working load in the system. We present a new algorithm, Symmetrical Hopping, for dynamic scheduling of ultra-lightweight processes. It is a dynamic, distributed, adaptive, and scalable scheduling algorithm. This algorithm is described and compared to four other algorithms that have been proposed in this context, namely the randomized allocation, the sender-initiated scheduling, the receiver-initiated scheduling, and the gradient model. The performance of these algorithms on Intel Touchstone Delta is presented. The experimental results show that the Symmetrical Hopping algorithm achieves much better performance due to its adaptiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Shu and L. V. Kale, </author> <title> "Chare Kernel | a runtime support system for parallel computations," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 11, </volume> <pages> pp. 198-211, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: In Section 6, we discuss the applicability of the Symmetrical Hopping algorithm. 2. Programming Model The Symmetrical Hopping algorithm is implemented on top of the process kernel, which is a runtime support system designed to support machine independent parallel programming <ref> [1, 2] </ref>. The kernel is responsible for dynamically managing and scheduling ultra-lightweight processes, which are called processes for short in this paper. Programmers use kernel primitives to create processes and send messages between them, without concerning themselves with mapping these processes to processors, or deciding which process to execute next.
Reference: [2] <author> W. Shu, </author> <title> Chare Kernel and its Implementation on Multicomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: In Section 6, we discuss the applicability of the Symmetrical Hopping algorithm. 2. Programming Model The Symmetrical Hopping algorithm is implemented on top of the process kernel, which is a runtime support system designed to support machine independent parallel programming <ref> [1, 2] </ref>. The kernel is responsible for dynamically managing and scheduling ultra-lightweight processes, which are called processes for short in this paper. Programmers use kernel primitives to create processes and send messages between them, without concerning themselves with mapping these processes to processors, or deciding which process to execute next. <p> The user can write a program in the process kernel language, deal with the creation of processes, and send messages between them. For details of the computation model and language, refer to <ref> [2] </ref>. In the following, we will illustrate how to write a program in the process kernel language using the exhaustive search of the N-queen problem as an example. The algorithm used here attempts to place queens on the board one row at a time if the particular position is valid. <p> In addition, information may travel a long distance, resulting in information aging. A minimum, but sufficient information exchange gives the best overall performance. Based on the amount of information exchanged, the information strategies can be classified as follows <ref> [2] </ref>: 7 type-i strategies involve using no status information. type-ii strategies calculate the status information by using local load information only. type-iii strategies calculate the status information by collecting load information from neighbors. type-iv strategies calculate the status information by collecting status information from neigh bors. type-v strategies calculate the status <p> The randomized allocation performs fairly well, especially for these medium-grained processes. A similar conclusion is also made by Shu and Grunwald <ref> [2, 33] </ref>. The sender-initiated algorithm is not good for a small system, because the system is heavily loaded for a fixed size of problem. The receiver-initiated algorithm does not perform well in a large system which is lightly loaded. Both sender-initiated algorithm and receiver-initiated algorithm suffer from their frequent polling.
Reference: [3] <author> G. S. Almasi and A. Gottlieb, </author> <title> Highly Parallel Computing. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: The computation model for the process kernel is a message-driven, nonpreemptive, thread-based model. Here, a parallel computation will be viewed as a collection of processes, each of 2 which in turn consists of a set of threads, called atomic computations. An atomic computation is a run-to-completion (RC) thread <ref> [3] </ref>. Once a RC thread starts execution, it will run to completion without being blocked. Processes communicate with each other via messages. Each atomic computation is then the result of processing a message. During its execution, it can create new processes or generate new messages [4].
Reference: [4] <author> G. A. Agha, </author> <title> A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Once a RC thread starts execution, it will run to completion without being blocked. Processes communicate with each other via messages. Each atomic computation is then the result of processing a message. During its execution, it can create new processes or generate new messages <ref> [4] </ref>. A message can trigger an atomic computation, whereas an atomic computation cannot wait for messages. All atomic computations of the same process share one common data area.
Reference: [5] <author> M. Haines, D. Cronk, and P. Mehrotra, </author> <title> "On the design of Chant: A talking threads package," </title> <booktitle> in Supercomputing 94', </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: However, it should be clear that the scheduling strategies that are applicable in this context can also be used in other contexts which involve dynamic creation of medium-grained processes. For example, the Chant system <ref> [5] </ref> for lightweight threads, Nexus runtime support [6] for task-parallel programming languages, Mentat system [7] for object-oriented parallel processing, and Cantor for actor-based languages [8] can all benefit from such strategies.
Reference: [6] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke, </author> <title> "Nexus: An interoperability layer for parallel and distributed computer systems," </title> <type> Tech. Rep. </type> <institution> ANL/MCS-TM-189, Argonne National Lab, </institution> <year> 1994. </year>
Reference-contexts: However, it should be clear that the scheduling strategies that are applicable in this context can also be used in other contexts which involve dynamic creation of medium-grained processes. For example, the Chant system [5] for lightweight threads, Nexus runtime support <ref> [6] </ref> for task-parallel programming languages, Mentat system [7] for object-oriented parallel processing, and Cantor for actor-based languages [8] can all benefit from such strategies. Many previous research efforts have been directed towards the process allocation in distributed systems [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19].
Reference: [7] <author> A. S. Grimshaw, </author> <title> "Easy-to-use object-oriented parallel processing with Mentat," </title> <journal> IEEE Computer, </journal> <volume> vol. 26, </volume> <pages> pp. 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, it should be clear that the scheduling strategies that are applicable in this context can also be used in other contexts which involve dynamic creation of medium-grained processes. For example, the Chant system [5] for lightweight threads, Nexus runtime support [6] for task-parallel programming languages, Mentat system <ref> [7] </ref> for object-oriented parallel processing, and Cantor for actor-based languages [8] can all benefit from such strategies. Many previous research efforts have been directed towards the process allocation in distributed systems [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19].
Reference: [8] <author> W. C. Athas and C. L. Seitz, </author> <title> "Cantor user report," </title> <type> tech. rep., </type> <institution> Dept. of Computer Science, California Institute of Technology, </institution> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: For example, the Chant system [5] for lightweight threads, Nexus runtime support [6] for task-parallel programming languages, Mentat system [7] for object-oriented parallel processing, and Cantor for actor-based languages <ref> [8] </ref> can all benefit from such strategies. Many previous research efforts have been directed towards the process allocation in distributed systems [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19].
Reference: [9] <author> N. G. Shivaratri, P. Krieger, and M. Singhal, </author> <title> "Load distributing for locally distributed systems," </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 33-44, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [10] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan, </author> <title> "Adaptive load sharing in homogeneous distributed systems," </title> <journal> IEEE Trans. Software Eng., </journal> <volume> vol. SE-12, </volume> <pages> pp. 662-674, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11]. <p> This leads to a higher communication load on large systems. Since the bandwidth consumed by a long-distance message is certainly larger, the system is more likely to be communication bound compared to a system using other load balancing strategies that encourage locality. Eager et al. <ref> [10] </ref> have modified the naive randomized allocation algorithm. They use threshold, a kind of local load information, to determine whether to process a process locally or locate a process randomly.
Reference: [11] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan, </author> <title> "A comparison of receiver-initiated and sender-initiated adaptive load sharing," </title> <booktitle> Performance Eval., </booktitle> <volume> vol. 6, </volume> <pages> pp. 53-68, </pages> <month> Mar. </month> <year> 1986. </year> <month> 23 </month>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11]. <p> A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm <ref> [11] </ref>. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective [22, 23, 24, 25]. These strategies are discussed in this section. <p> In a sender-initiated algorithm, scheduling activity is initiated by an overloaded processor that attempts to send a process to an underloaded processor; whereas, in a receiver-initiated algorithm, scheduling activity is initiated by an underloaded processor that attempts to request a process from an overloaded processors <ref> [11] </ref>. Both sender-initiated algorithms and receiver-initiated algorithms are threshold algorithms. A threshold T is predefined to determine a processor being a sender or a receiver. When the load in a processor excesses T , the processor is identified as a sender. <p> In a receiver-initiated algorithm, a receiver randomly polls other processors until a sender is found. Each of them has its advantage. The sender-initiated strategy is best for a lightly loaded system and the receiver-initiated strategy is more efficient in a heavily loaded system <ref> [11] </ref>. A scheduling algorithm that combines both sender-initiated and sender-initiated algorithms is called a symmetrical-initiated algorithm [26]. The gradient model [21] is an information-driven strategy.
Reference: [12] <author> J. A. Stankovic, </author> <title> "Simulations of three adaptive, decentralized controlled, job scheduling algorithms," </title> <journal> Computer Networks, </journal> <volume> vol. 8, </volume> <pages> pp. 199-217, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [13] <author> T. L. Casavant and J. G. Kuhl, </author> <title> "A formal model of distributed decision-making and its application to distributed load balancing," </title> <booktitle> in Int'l Conf. on Distributed Computing System, </booktitle> <pages> pp. 232-239, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [14] <author> T. L. Casavant and J. G. Kuhl, </author> <title> "Analysis of three dynamic distributed load-balancing strategies with varying global information requirements," </title> <booktitle> in Int'l Conf. on Distributed Computing System, </booktitle> <pages> pp. 185-192, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [15] <author> A. Hac and X. Jin, </author> <title> "Dynamic load balancing in a distributed system using a decentralized algorithm," </title> <booktitle> in Int'l Conf. on Distributed Computing System, </booktitle> <pages> pp. 170-177, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [16] <author> V. Singh and M. R. Genesereth, </author> <title> "A variable supply model for distributing deductions," </title> <booktitle> in 9th Intel. Joint Conf. Artificial Intelligence, </booktitle> <pages> pp. 39-45, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [17] <author> Y.-T. Wang and R. J. T. Morris, </author> <title> "Load sharing in distributed systems," </title> <journal> IEEE Trans. Com-put., </journal> <volume> vol. C-34, </volume> <pages> pp. 204-217, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [18] <author> A. Barak and A. Shiloh, </author> <title> "A distributed load-balancing policy for a multicomputer," </title> <journal> Software-Practice and Experience, </journal> <volume> vol. 15, </volume> <pages> pp. 901-913, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [19] <author> Z. Lin, </author> <title> "A distributed fair polling scheme applied to parallel logic programming," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 20, </volume> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems <ref> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11].
Reference: [20] <author> M. Willebeek-LeMair and A. P. Reeves, </author> <title> "Strategies for dynamic load balancing on highly parallel computers," </title> <journal> IEEE Trans. Parallel and Distributed System, </journal> <volume> vol. 9, </volume> <pages> pp. 979-993, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Many previous research efforts have been directed towards the process allocation in distributed systems [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves <ref> [20] </ref>. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective [22, 23, 24, 25]. <p> Symmetrical Hopping The sender-initiated and receiver-initiated algorithms request information from a randomly chosen pe in the system, resulting in heavy communication and network contention. The information exchange can be limited within a neighborhood to increase locality <ref> [20] </ref>. With this limitation, the sender-initiated and receiver-initiated strategies are called sender-initiated hopping and receiver-initiated hopping, respectively. Symmetrical Hopping is a symmetrical-initiated algorithm that combines the sender-initiated hopping and receiver-initiated hopping strategies. Symmetrical Hopping uses the type-iii information strategy.
Reference: [21] <author> F. C. H. Lin and R. M. Keller, </author> <title> "The gradient model load balancing method," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 13, </volume> <pages> pp. 32-38, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [20]. 6 Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [11]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller <ref> [21] </ref>. The randomized allocation algorithms developed by different authors are quite simple and effective [22, 23, 24, 25]. These strategies are discussed in this section. <p> Each of them has its advantage. The sender-initiated strategy is best for a lightly loaded system and the receiver-initiated strategy is more efficient in a heavily loaded system [11]. A scheduling algorithm that combines both sender-initiated and sender-initiated algorithms is called a symmetrical-initiated algorithm [26]. The gradient model <ref> [21] </ref> is an information-driven strategy. As stated by Lin [27], instead of trying to allocate a newly generated process to other pes, the process is queued at the generating pe and waits for some pe to request it.
Reference: [22] <author> W. C. Athas, </author> <title> Fine Grain Concurrent Computations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, California Institute of Technology, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [22, 23, 24, 25] </ref>. These strategies are discussed in this section. Table I: The Solution Space of Dynamic Scheduling No information Passive information Active information Non-neighborhood Random Allocation Sender-initiated Strategy Receiver-initiated Neighborhood Gradient Model Strategy Symmetrical Hopping The solutions of dynamic scheduling can be described in a two-dimension space. <p> The advantages of this strategy are its locality and low communication overhead. Next, we briefly describe four scheduling algorithms: the randomized allocation, the sender-initiated algorithm, the receiver-initiated algorithm, and the gradient model. Athas and Seitz have proposed a global randomized allocation algorithm <ref> [22, 23] </ref>. A randomized allocation algorithm dictates that each pe, when it generates a new process, should send it to a randomly chosen pe. One advantage of this algorithm is simplicity of implementation. No local load information needs to be maintained, nor is any load information sent to other pes.
Reference: [23] <author> W. C. Athas and C. L. Seitz, </author> <title> "Multicomputers: Message-passing concurrent computers," </title> <journal> IEEE Computer, </journal> <volume> vol. 21, </volume> <pages> pp. 9-24, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [22, 23, 24, 25] </ref>. These strategies are discussed in this section. Table I: The Solution Space of Dynamic Scheduling No information Passive information Active information Non-neighborhood Random Allocation Sender-initiated Strategy Receiver-initiated Neighborhood Gradient Model Strategy Symmetrical Hopping The solutions of dynamic scheduling can be described in a two-dimension space. <p> The advantages of this strategy are its locality and low communication overhead. Next, we briefly describe four scheduling algorithms: the randomized allocation, the sender-initiated algorithm, the receiver-initiated algorithm, and the gradient model. Athas and Seitz have proposed a global randomized allocation algorithm <ref> [22, 23] </ref>. A randomized allocation algorithm dictates that each pe, when it generates a new process, should send it to a randomly chosen pe. One advantage of this algorithm is simplicity of implementation. No local load information needs to be maintained, nor is any load information sent to other pes.
Reference: [24] <author> R. M. Karp and Y. Zhang, </author> <title> "A randomized parallel branch-and-bound procedure," </title> <journal> Journal of ACM, </journal> <volume> vol. 40, </volume> <pages> pp. 765-789, </pages> <year> 1993. </year>
Reference-contexts: Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [22, 23, 24, 25] </ref>. These strategies are discussed in this section. Table I: The Solution Space of Dynamic Scheduling No information Passive information Active information Non-neighborhood Random Allocation Sender-initiated Strategy Receiver-initiated Neighborhood Gradient Model Strategy Symmetrical Hopping The solutions of dynamic scheduling can be described in a two-dimension space.
Reference: [25] <author> S. Chakrabarti, A. Ranade, and K. Yelick, </author> <title> "Randomized load balancing for tree structured computation," </title> <booktitle> in IEEE Scalable High Performance Computing Conference, </booktitle> <pages> pp. 666-673, </pages> <year> 1994. </year>
Reference-contexts: Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [21]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [22, 23, 24, 25] </ref>. These strategies are discussed in this section. Table I: The Solution Space of Dynamic Scheduling No information Passive information Active information Non-neighborhood Random Allocation Sender-initiated Strategy Receiver-initiated Neighborhood Gradient Model Strategy Symmetrical Hopping The solutions of dynamic scheduling can be described in a two-dimension space.
Reference: [26] <author> P. Krueger and M. Livny, </author> <title> "The diverse objectives of distributed scheduling policies," </title> <booktitle> in Proc. of the 7th International Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 242-249, </pages> <year> 1987. </year>
Reference-contexts: Each of them has its advantage. The sender-initiated strategy is best for a lightly loaded system and the receiver-initiated strategy is more efficient in a heavily loaded system [11]. A scheduling algorithm that combines both sender-initiated and sender-initiated algorithms is called a symmetrical-initiated algorithm <ref> [26] </ref>. The gradient model [21] is an information-driven strategy. As stated by Lin [27], instead of trying to allocate a newly generated process to other pes, the process is queued at the generating pe and waits for some pe to request it.
Reference: [27] <author> F. C. H. Lin, </author> <title> Load Balancing and Fault Tolerance in Applicative Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Utah, </institution> <month> Aug. </month> <year> 1985. </year> <month> 24 </month>
Reference-contexts: A scheduling algorithm that combines both sender-initiated and sender-initiated algorithms is called a symmetrical-initiated algorithm [26]. The gradient model [21] is an information-driven strategy. As stated by Lin <ref> [27] </ref>, instead of trying to allocate a newly generated process to other pes, the process is queued at the generating pe and waits for some pe to request it. A separate, asynchronous process on each pe is responsible for balancing the load.
Reference: [28] <author> W. Shu, </author> <title> "Adaptive dynamic process scheduling on distributed memory parallel computers," </title> <journal> Scientific Programming, </journal> <volume> vol. 3, </volume> <pages> pp. 341-352, </pages> <year> 1994. </year>
Reference-contexts: The Symmetrical Hopping scheduling consists of sender-initiated hopping and receiver-initiated hopping strategies. The sender-initiated hopping strategy of the Symmetrical Hopping algorithm is similar to that in the Adaptive Contracting Within Neighborhood (ACWN) algorithm <ref> [28] </ref>, which is shown in Figure 3. Here, a newly created process moves m hops, where 0 m d and d is the network diameter. We set an upper limit of traveling distance d for each process to prevent unbounded message oscillation.
Reference: [29] <author> R. E. Korf, </author> <title> "Depth-first iterative-deepening: An optimal admissible tree search," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 27, </volume> <pages> pp. 97-109, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: In this problem, the grain-size is not even, since whenever a new queen is placed, the search either successfully continues to the next row or fails. The number of processes generated is unpredictable. The second one, iterative deepening A* (IDA*) search, is a good example of parallel search techniques <ref> [29] </ref>. The sample problem is the 15-puzzle. The grain-size may vary substantially, since it dynamically depends on the current estimated cost. Also, synchronization at each iteration reduces the effective parallelism. Performance of this problem is therefore not as good as others.
Reference: [30] <author> W. F. van Gunsteren and H. J. C. Berendsen, "GROMOS: </author> <title> GROningen MOlecular Simulation software," </title> <type> tech. rep., </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: The grain-size may vary substantially, since it dynamically depends on the current estimated cost. Also, synchronization at each iteration reduces the effective parallelism. Performance of this problem is therefore not as good as others. The third one, a molecular dynamics program GROMOS, is a real application problem <ref> [30, 31] </ref>. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms [32]. The cutoff radius is predefined to 16 A. GROMOS has a more predictable structure.
Reference: [31] <author> R. v. Hanxleden and K. Kennedy, </author> <title> "Relaxing SIMD control flow constraints using loop transformations," </title> <type> Tech. Rep. </type> <institution> CRPC-TR92207, Center for Research on Parallel Computation, Rice University, </institution> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: The grain-size may vary substantially, since it dynamically depends on the current estimated cost. Also, synchronization at each iteration reduces the effective parallelism. Performance of this problem is therefore not as good as others. The third one, a molecular dynamics program GROMOS, is a real application problem <ref> [30, 31] </ref>. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms [32]. The cutoff radius is predefined to 16 A. GROMOS has a more predictable structure.
Reference: [32] <author> J. Shen and J. A. McCammon, </author> <title> "Molecular dynamics simulation of superoxide interacting with superoxide dismutase," </title> <journal> Chemical Physics, </journal> <volume> vol. 158, </volume> <pages> pp. 191-198, </pages> <year> 1991. </year>
Reference-contexts: Performance of this problem is therefore not as good as others. The third one, a molecular dynamics program GROMOS, is a real application problem [30, 31]. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms <ref> [32] </ref>. The cutoff radius is predefined to 16 A. GROMOS has a more predictable structure. The number of processes is known with the given input data, but the computation density in each process varies. Thus a load balancing mechanism is necessary.
Reference: [33] <author> D. C. Grunwald, </author> <title> Circuit Switched Multicomputers and Heuristic Load Placement. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, UIUCDCS-R-89-1514, </institution> <month> Sept. </month> <year> 1989. </year> <month> 25 </month>
Reference-contexts: The randomized allocation performs fairly well, especially for these medium-grained processes. A similar conclusion is also made by Shu and Grunwald <ref> [2, 33] </ref>. The sender-initiated algorithm is not good for a small system, because the system is heavily loaded for a fixed size of problem. The receiver-initiated algorithm does not perform well in a large system which is lightly loaded. Both sender-initiated algorithm and receiver-initiated algorithm suffer from their frequent polling.
References-found: 33

