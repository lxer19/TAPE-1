URL: http://www.cs.orst.edu:80/~tadepall/research/papers/auto-exploratory.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: fokd,tadepallig@research.cs.orst.edu  
Title: Auto-exploratory Average Reward Reinforcement Learning  
Author: DoKyeong Ok and Prasad Tadepalli 
Address: Corvallis,Oregon 97331-3202  
Affiliation: Computer Science Department Oregon State University  
Abstract: We introduce a model-based average reward Reinforcement Learning method called H-learning and compare it with its discounted counterpart, Adaptive Real-Time Dynamic Programming, in a simulated robot scheduling task. We also introduce an extension to H-learning, which automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this "Auto-exploratory H-learning" performs better than the original H-learning under previously studied exploration methods such as random, recency-based, or counter-based exploration. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> 1995. </year> <title> Learning to Act using Real-Time Dynamic Programming. </title> <journal> Artificial Intelligence, </journal> <volume> 73(1), </volume> <pages> 81-138. </pages>
Reference-contexts: Introduction Reinforcement Learning (RL) is the study of learning agents that improve their performance at some task by receiving rewards and punishments from the environment. Most approaches to reinforcement learning, including Q-learning (Watkins and Dayan 92) and Adaptive Real-Time Dynamic Programming (ARTDP) <ref> (Barto, Bradtke, & Singh 95) </ref>, optimize the total discounted reward the learner receives. In other words, a reward which is received after one time step is considered equivalent to a fraction of the same reward received immediately. <p> This raises the question whether and when discounted RL methods are appropriate to use to optimize the average reward. In this paper, we describe an Average reward RL (ARL) method called H-learning, which is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) <ref> (Barto, Bradtke, & Singh 95) </ref>. Unlike Schwartz's R-learning (Schwartz 93) and Singh's ARL algorithms (Singh 94), it is model-based, in that it learns and uses explicit action models.
Reference: <author> Bertsekas, D. P. </author> <year> 1982. </year> <title> Distributed Dynamic Programming. In IEEE Transactions in Automatic Control, </title> <publisher> AC-27(3). </publisher>
Reference-contexts: In White's relative value iteration method, the resulting equations are solved by synchronous successive approximation (Bert-sekas 95). Unfortunately, the asynchronous version of this algorithm does not always converge, as was shown by Tsitsiklis, and cannot be the basis of an ARL al-gorithm <ref> (Bertsekas 82) </ref>. Hence, instead of using Equation (1) to solve for , H-learning estimates it from on-line rewards (see Figure 1).
Reference: <author> Bertsekas, D. P. </author> <year> 1995. </year> <title> Dynamic Programming and Optimal Control, </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: Although lim t!1 * t (s) may not exist for periodic MDPs, the Cesaro-limit of * t (s), defined as lim l!1 l t=1 * t (s), always exists, and is called the bias of state s, denoted by h (s) <ref> (Bertsekas 95) </ref>. Intuitively, h (i) h (j) is the average relative advantage in long-term total reward for starting in state i as opposed to state j.
Reference: <author> Howard, R. A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Jalali, A. and Ferguson, M. </author> <year> 1989. </year> <title> Computationally Efficient Adaptive Control Algorithms for Markov Chains. </title> <booktitle> In IEEE Proceedings of the 28'th Conference on Decision and Control, </booktitle> <address> Tampa, FL. </address>
Reference-contexts: H-learning is very similar to Jalali and Ferguson's Algorithm B, which is proved to converge to the gain-optimal policy for ergodic MDPs <ref> (Jalali and Ferguson 89) </ref>. Ergodicity assumption allows them to ignore the issue of exploration, which is otherwise crucial for convergence to the optimal policy.
Reference: <author> Kaelbling, L. P. </author> <year> 1990. </year> <title> Learning in Embedded Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: An analytical convergence result of this kind for AH-learning would be very interesting. In the Interval Estimation algorithm (IE) of Kaelbling <ref> (Kaelbling 90) </ref>, which is similar, the agent maintains a confidence interval of the value function, and always chooses an action that maximizes its upper bound. This ensures that the chosen action either has a high utility, or has a large confidence interval which needs exploration to reduce it.
Reference: <author> Koenig, S., and Simmons, R. G. </author> <year> 1996. </year> <title> The Effect of Representation and Knowledge on Goal-Directed Exploration with Reinforcement-Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 227-250. </pages>
Reference-contexts: Mahade-van extends both H-learning and R-learning to find the bias-optimal policies for general unichains, and illustrates that they improve their performance in an admission control queuing system (Mahadevan 96b; Mahadevan 96c). Auto-exploratory H-learning is similar in spirit to the action-penalty representation of reward functions analyzed by Koenig and Simmons <ref> (Koenig & Simmons 96) </ref>. They showed that a minimax form of Q-learning, which always takes greedy actions, can find the shortest cost paths from all states to a goal state in O (n 3 ) time, where n is the size of the state space.
Reference: <author> Lin, L-J. </author> <title> 1992 Self-improving Reactive Agents Based on Reinforcement Learning, Planning, and Teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Mahadevan, S. and Connell, J. </author> <year> 1992. </year> <title> Automatic Programming of Behavior-based Robots Using Reinforcement Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference: <author> Mahadevan, S. </author> <year> 1996a. </year> <title> Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 159-195. </pages>
Reference-contexts: We also found similar differences between Q-learning and R-learning. Our results are consistent with those of Mahadevan who compared Q-learning and R-learning in a robot simulator domain and a maze domain and found that R-learning can be tuned to perform better <ref> (Mahadevan 96a) </ref>. Auto-exploratory H-learning Recall that H-learning needs exploratory actions to ensure that every state is visited infinitely often during training. Unfortunately, actions executed exclusively for exploratory purpose could lead to decreased average reward, because they do not fully exploit the agent's currently known best policy. <p> Discussion and Future Work There is an extensive literature on average reward optimization using dynamic programming approaches (Howard 60; Puterman 94; Bertsekas 95). <ref> (Mahadevan 96a) </ref> gives a useful survey of this literature from Reinforcement Learning point of view. Bias-optimality, or Schwartz's T-optimality, is a more refined notion that seeks to find a policy that maximizes the cumulated discounted total reward for all states as the discount factor fl ! 1.
Reference: <author> Mahadevan, S. </author> <year> 1996b. </year> <title> An Average Reward Reinforcement Learning Algorithm for Computing Bias-optimal Policies. </title> <booktitle> In Proceedings of AAAI-96, </booktitle> <address> Port-land, OR. </address>
Reference-contexts: We also found similar differences between Q-learning and R-learning. Our results are consistent with those of Mahadevan who compared Q-learning and R-learning in a robot simulator domain and a maze domain and found that R-learning can be tuned to perform better <ref> (Mahadevan 96a) </ref>. Auto-exploratory H-learning Recall that H-learning needs exploratory actions to ensure that every state is visited infinitely often during training. Unfortunately, actions executed exclusively for exploratory purpose could lead to decreased average reward, because they do not fully exploit the agent's currently known best policy. <p> Discussion and Future Work There is an extensive literature on average reward optimization using dynamic programming approaches (Howard 60; Puterman 94; Bertsekas 95). <ref> (Mahadevan 96a) </ref> gives a useful survey of this literature from Reinforcement Learning point of view. Bias-optimality, or Schwartz's T-optimality, is a more refined notion that seeks to find a policy that maximizes the cumulated discounted total reward for all states as the discount factor fl ! 1.
Reference: <author> Mahadevan, S. </author> <year> 1996c. </year> <title> Sensitive Discount Optimality: Unifying Discounted and Average Reward Reinforcement Learning. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <address> Bari, Italy. </address>
Reference-contexts: We also found similar differences between Q-learning and R-learning. Our results are consistent with those of Mahadevan who compared Q-learning and R-learning in a robot simulator domain and a maze domain and found that R-learning can be tuned to perform better <ref> (Mahadevan 96a) </ref>. Auto-exploratory H-learning Recall that H-learning needs exploratory actions to ensure that every state is visited infinitely often during training. Unfortunately, actions executed exclusively for exploratory purpose could lead to decreased average reward, because they do not fully exploit the agent's currently known best policy. <p> Discussion and Future Work There is an extensive literature on average reward optimization using dynamic programming approaches (Howard 60; Puterman 94; Bertsekas 95). <ref> (Mahadevan 96a) </ref> gives a useful survey of this literature from Reinforcement Learning point of view. Bias-optimality, or Schwartz's T-optimality, is a more refined notion that seeks to find a policy that maximizes the cumulated discounted total reward for all states as the discount factor fl ! 1.
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes: Discrete Dynamic Stochastic Programming. </title> <publisher> John Wi-ley. </publisher>
Reference-contexts: For unichain MDPs the expected long-term average reward per time step for any policy is independent of the starting state s 0 . We call it the "gain" of the policy , denoted by (), and consider the problem of finding a "gain-optimal policy," fl , that maximizes () <ref> (Puterman 94) </ref>. Derivation of H-learning Even though the gain of a policy, (), is independent of the starting state, the total expected reward in time t may not be. <p> We are primarily interested in non-ergodic MDPs here because ergodic MDPs do not need exploration. Unfortunately, the gain of a stationary policy for a multichain (non-unichain) MDP depends on the initial state <ref> (Puterman 94) </ref>. Hence we consider some restricted classes of MDPs. An MDP is communicating if for every pair of states i; j, there is a stationary policy under which they communicate. For example, our Delivery domain is communicating.
Reference: <author> Schwartz, A. </author> <year> 1993. </year> <title> A Reinforcement Learning Method for Maximizing Undiscounted Rewards. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In this paper, we describe an Average reward RL (ARL) method called H-learning, which is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) (Barto, Bradtke, & Singh 95). Unlike Schwartz's R-learning <ref> (Schwartz 93) </ref> and Singh's ARL algorithms (Singh 94), it is model-based, in that it learns and uses explicit action models. <p> Instead, we use R-learning's method of estimating the average reward <ref> (Schwartz 93) </ref>.
Reference: <author> Singh, S. P. </author> <year> 1994. </year> <title> Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <publisher> MIT press. </publisher>
Reference-contexts: In this paper, we describe an Average reward RL (ARL) method called H-learning, which is an undiscounted version of Adaptive Real-Time Dynamic Programming (ARTDP) (Barto, Bradtke, & Singh 95). Unlike Schwartz's R-learning (Schwartz 93) and Singh's ARL algorithms <ref> (Singh 94) </ref>, it is model-based, in that it learns and uses explicit action models. We compare H-learning with its discounted counterpart ARTDP to optimize the average reward in the task of scheduling a simulated Automatic Guided Vehicle (AGV), a material handling robot used in manufacturing.
Reference: <author> Tadepalli, P. and Ok, D. </author> <year> 1994. </year> <title> H-learning: A Reinforcement Learning Method for Optimizing Undis-counted Average Reward. </title> <type> Technical Report, </type> <institution> 94-30-1, Dept. of Computer Science, Oregon State University. </institution>
Reference-contexts: In more exhaustive experiments with 75 different parameter settings for p, q and K, it was found that H-learning always converges to the gain-optimal policy, and does so in fewer steps in all but 16 short range cases, where ARTDP is slightly faster <ref> (Tadepalli & Ok 94) </ref>. We also found similar differences between Q-learning and R-learning. Our results are consistent with those of Mahadevan who compared Q-learning and R-learning in a robot simulator domain and a maze domain and found that R-learning can be tuned to perform better (Mahadevan 96a).
Reference: <author> Tadepalli, P. and Ok, D. </author> <year> 1996. </year> <title> Scaling Up Average Reward Reinforcement Learning by Approximating the Domain Models and the Value Function. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <address> Bari, Italy. </address>
Reference-contexts: To scale H-learning to large domains, it is necessary to approximate its value function and action models. Elsewhere, we describe our results of learning action models in the form of conditional probability tables of a Bayesian network, and using local linear regression to approximate its value function <ref> (Tadepalli & Ok 96) </ref>. Both these extensions improve the space requirement of H-learning and the number of steps needed for its convergence. We also plan to explore other ways of approximating the value function which can be effective in multi-dimensional spaces.
Reference: <author> Thrun, S. </author> <year> 1994. </year> <title> The Role of Exploration in Learning Control. In Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </title> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: Like most other RL methods, H-learning needs exploration to find a globally optimal policy. A number of exploration strategies have been studied in RL, including occasionally executing random actions, and preferring states which are least visited (counter-based) or actions least recently executed (recency-based) <ref> (Thrun 94) </ref>. We introduce a version of H-learning which has the property of automatically exploring the unexplored parts of the state space while always taking a greedy action with respect to the current value function. <p> Experimental Results on AH-learning In this section, we compare AH-learning with some other exploration strategies in the Delivery domain of AH-learning does not explicitly store the policy. We compared AH to three other exploration methods: random exploration, counter-based exploration and recency-based exploration <ref> (Thrun 94) </ref>. Random exploration was used as before, except that we optimized the probability with which random actions are selected.
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <month> 279-292. </month> <title> Acknowledgments We gratefully acknowledge the support of NSF under grant number IRI-9520243. We thank Tom Dietterich, Sridhar Mahadevan, and Toshi Minoura for many interesting discussions on this topic. Thanks to Sridhar Mahadevan and the reviewers of this paper for their thorough and helpful comments. </title>
Reference-contexts: Introduction Reinforcement Learning (RL) is the study of learning agents that improve their performance at some task by receiving rewards and punishments from the environment. Most approaches to reinforcement learning, including Q-learning <ref> (Watkins and Dayan 92) </ref> and Adaptive Real-Time Dynamic Programming (ARTDP) (Barto, Bradtke, & Singh 95), optimize the total discounted reward the learner receives. In other words, a reward which is received after one time step is considered equivalent to a fraction of the same reward received immediately.
References-found: 19

