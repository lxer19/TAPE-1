URL: ftp://ftp.speech.sri.com/pub/papers/eurospeech97-worderror.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Email: fstolcke,konig,mwg@speech.sri.com  
Title: EXPLICIT WORD ERROR MINIMIZATION IN N-BEST LIST RESCORING  
Author: Andreas Stolcke Yochai K onig Mitchel Weintraub 
Web: http://www.speech.sri.com/  
Address: Menlo Park, CA, U.S.A.  
Affiliation: Speech Technology and Research Laboratory SRI International,  
Abstract: We show that the standard hypothesis scoring paradigm used in maximum-likelihood-based speech recognition systems is not optimal with regard to minimizing the word error rate, the commonly used performance metric in speech recognition. This can lead to sub-optimal performance, especially in high-error-rate environments where word error and sentence error are not necessarily monotonically related. To address this discrepancy, we developed a new algorithm that explicitly minimizes expected word error for recognition hypotheses. First, we approximate the posterior hypothesis probabilities using N-best lists. We then compute the expected word error for each hypothesis with respect to the posterior distribution, and choose the hypothesis with the lowest error. Experiments show improved recognition rates on two spontaneous speech corpora. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. R. Bahl, F. Jelinek, and R. L. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2):179190, </volume> <year> 1983. </year>
Reference-contexts: 1. INTRODUCTION The standard selection criterion for speech recognition hypotheses aims at maximizing the posterior probability of a hypothesis W given the acoustic evidence X <ref> [1] </ref>: W fl = argmax W = argmax W P (X) = argmax W Here P (W ) is the prior probability of a word sequence according to a language model, and P (XjW ) is given by the acoustic model.
Reference: [2] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Equation (1) is Bayes' Rule, while (2) is due to the fact that P (X) does not depend on W and can therefore be ignored during maximization. Bayes decision theory (see, e.g., <ref> [2] </ref>) tells us that this criterion (assuming accurate language and acoustic models) maximizes the probability of picking the correct W ; i.e., it minimizes sentence error rate. However, speech recog-nizers are usually evaluated primarily for their word error rates.
Reference: [3] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 517520, </pages> <address> San Fran-cisco, </address> <year> 1992. </year>
Reference-contexts: EXPERIMENTS We tested the new rescoring algorithm on 2000-best lists for two test sets taken from spontaneous speech corpora. Test set 1 consisted of 25 conversations from the Switchboard corpus <ref> [3] </ref>. Test set 2 were 25 conversations from the Spanish CallHome corpus collected by the Linguistic Data Consortium. Due to the properties of spontaneous speech, error rates are relative high on these data, making word error minimization more promising, as discussed earlier.
Reference: [4] <author> R. Moore, D. Appelt, J. Dowding, J. M. Gawron, and D. Moran. </author> <title> Combining linguistic and statistical knowledge sources in natural language processing for ATIS. </title> <booktitle> In Proceedings ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 261264, </pages> <address> Austin, Texas, </address> <year> 1995. </year>
Reference-contexts: Other knowledge sources and weight optimiza tion Often other knowledge sources are added to the standard language model and acoustic scores to improve recognition, such as word transition penalties or scores expressing syntactic or semantic well-formedness (e.g., <ref> [4] </ref>). Even though these additional scores cannot always be interpreted as probabilities, they can still be combined with exponential weights; the weights are then optimized on a held-out set to minimize WER [5].
Reference: [5] <author> M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek. </author> <title> Integration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Processing Workshop, </booktitle> <pages> pp. 8387, </pages> <address> Pacific Grove, CA, </address> <year> 1991. </year> <institution> Defense Advanced Research Projects Agency, Information Science and Technology Office. </institution>
Reference-contexts: Thus, if we had an algorithm to optimize the expected word error directly, we would expect to see its benefits mostly at high error rates. 3. THE ALGORITHM We now give an algorithm that minimizes the expected word error rate (WER) in the N-best rescoring paradigm <ref> [5] </ref>. The algorithm has two components: (1) approximating the posterior distribution over hypotheses and (2) computing the expected WER for N-best hypotheses (and picking the one with lowest expected WER). 3.1. <p> Even though these additional scores cannot always be interpreted as probabilities, they can still be combined with exponential weights; the weights are then optimized on a held-out set to minimize WER <ref> [5] </ref>. This weight optimization should not be confused with the word error minimization discussed here; instead, the two methods complement each other. The additional knowledge sources can be used to yield improved posterior probability estimates, based on which the algorithm described here can be applied.
Reference: [6] <author> M. Weintraub, F. Beaufays, Z. Rivlin, Y. Konig, and A. Stol-cke. </author> <title> Neural-network based measures of confidence for word recognition. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 887 890, </pages> <address> Munich, </address> <year> 1987. </year>
Reference-contexts: Our experiments so far have been based on the commonly used acoustic and language model scores, but we are already experimenting with more complex posterior estimator methods based on neural network models <ref> [6] </ref>.
Reference: [7] <author> M. Weintraub. </author> <title> LVCSR log-likelihood ratio rescoring for keyword spotting. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 297 300, </pages> <address> Detroit, </address> <year> 1995. </year>
Reference-contexts: Let W i be the ith hypothesis in the N -best list; the posterior estimate is thus P (W i jX) 1 P N 1 This N-best approximation to the posterior has previously been used, e.g., in the computation of posterior word probabilities for keyword spotting <ref> [7] </ref>. 3.2. Computing expected WER Given a list of N-best hypotheses and their posterior probability estimates, we approximate the expected WER as the weighted average word error relative to all the hypotheses in the N-best list.
References-found: 7

