URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P463.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Email: bouarich@mcs.anl.gov.  bobby@cs.colorado.edu.  
Title: TENSOLVE: A Software Package for Solving Systems of Nonlinear Equations and Nonlinear Least Squares Problems
Author: Ali Bouaricha Robert B. Schnabel 
Keyword: Categories and Subject Descriptors: G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations-systems of equations; G.1.6 [Numerical Analysis]: Optimization-least squares methods; G.4 [Mathematics of Computing]: Mathematical Software General Terms: Algorithms Additional Key Words and Phrases: tensor methods, nonlinear equations, nonlinear least squares, rank-deficient matrices  
Note: Research supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  Research supported by AFOSR Grants No. AFOSR-90-0109 and F49620-94-1-0101, ARO Grants No. DAAL03-91-G-0151 and DAAH04-94-G-0228, and NSF Grant No. CCR-9101795.  
Address: 60439,  Boulder, Colorado 80309-0430,  
Affiliation: Argonne National Laboratory and  University of Colorado  Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois  Department of Computer Science, University of Colorado,  
Abstract: This paper describes a modular software package for solving systems of nonlinear equations and nonlinear least squares problems, using a new class of methods called tensor methods. It is intended for small to medium-sized problems, say with up to 100 equations and unknowns, in cases where it is reasonable to calculate the Jacobian matrix or approximate it by finite differences at each iteration. The software allows the user to select between a tensor method and a standard method based upon a linear model. The tensor method approximates F (x) by a quadratic model, where the second-order term is chosen so that the model is hardly more expensive to form, store, or solve than the standard linear model. Moreover, the software provides two different global strategies, a line search and a two-dimensional trust region approach. Test results indicate that, in general, tensor methods are significantly more efficient and robust than standard methods on small and medium-sized problems in iterations and function evaluations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bouaricha, </author> <title> A Software Package for Solving Systems of Nonlinear Equations and Nonlinear Least Squares Problems Using Tensor Methods, M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1986. </year> <title> 30 Table 10.9: Comparison of Tensor Method with NL2SOL on the Nonlinear Equations and Nonlinear Least Squares Problems Listed in Table B-1 Global strategy Tensor versus NL2SOL Average Ratio-Tensor/NL2SOL Better Worse Tie Itn Feval Tensor w/ LS 25 8 2 0.42 0.71 Tensor w/ TR 24 9 2 0.76 0.93 </title>
Reference-contexts: The reader may refer to <ref> [1] </ref>, [2], [6], and [11] for more details on tensor methods for nonlinear equations and nonlinear least squares problems.
Reference: [2] <author> A. Bouaricha, </author> <title> Solving Large Sparse Systems of Nonlinear Equations and Nonlinear Least Squares Problems Using Tensor Methods on Sequential and Parallel Computers, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: The reader may refer to [1], <ref> [2] </ref>, [6], and [11] for more details on tensor methods for nonlinear equations and nonlinear least squares problems. <p> The initial trust radius can be supplied by the user; if not, it is set to the length of the initial Cauchy step. Our software solves the one-variable global optimization problem by a straightforward partitioning scheme described in <ref> [2] </ref>. 4. Overview of the Software Package This section summarizes the key features of the software package. The user has the option to solve systems of nonlinear equations or nonlinear least squares problems. <p> Some of the test problems were run at various dimensions. All of these problems were also run with the standard method. The list of test problems is given in Appendix A; the detailed test results are given in <ref> [2] </ref>. Our computational results for the test problems whose Jacobians at the solution have ranks n, n 1, and n 2 are summarized in Tables 10.1 to 10.4.
Reference: [3] <author> R. H. Byrd, R. B. Schnabel, and G. A. Shultz, </author> <title> Approximation Solution of the Trust Region Problem by Minimization over Two-Dimensional Subspaces, </title> <journal> Mathematical Programming, </journal> <volume> 40 (1988), </volume> <pages> 247-263. </pages>
Reference-contexts: The main reasons that led us to adopt this approach are that it is easy to construct and is closely related to dogleg-type algorithms over the same subspace. In addition, the resultant step may be close to the optimal trust region step in practice. Byrd, Schnabel, and Shultz <ref> [3] </ref> have shown that for unconstrained optimization using a standard quadratic model, the analogous two-dimensional minimization approach produces nearly as much decrease in the quadratic model as the optimal trust region step in almost all cases.
Reference: [4] <author> J. E. Dennis and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: we use the values of ^ f (0); ^ f 0 (0), and ^ f () to model ^ f and then take the value of that minimizes this model as the next value of in Algorithm 3.3 subject to restrictions on how much can decrease at once (see, e.g., <ref> [4] </ref>, pages 126-127 for more details). This results in the following algorithm. Algorithm 3.3. <p> a piecewise linear function in the subspace spanned by the Newton direction and the steepest descent direction J (x c ) T F (x c ), and takes x + as the point on this piecewise curve for which jjx + x c jj = ffi c . (See, e.g., <ref> [4] </ref> for more details.) Unfortunately, these two methods are difficult to extend to the tensor model, because certain key properties do not generalize to this model. <p> GLOBAL = 1 specifies the trust region. The user may supply an analytic routine to evaluate the Jacobian matrix. If it is not supplied, the package computes the Jacobian by finite differences. The finite difference routine is described in detail by Dennis and Schnabel <ref> [4] </ref>. The parameter JACFLG specifies whether an analytic Jacobian has been provided. The default value, which specifies finite differences, is JACFLG = 0.
Reference: [5] <author> J. E. Dennis, D. M. Gay, and R. E. Welsch, </author> <title> An Adaptive Nonlinear Least Squares Algorithm, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 7 (1981), </volume> <pages> 348-368. </pages>
Reference-contexts: Finally, we compared our tensor method with the NL2SOL package <ref> [5] </ref> on the set of nonlinear least squares problems used in [5] that is listed in Appendix B. The reason we were interested in making this comparison is that theoretically the NL2SOL method is superlinearly convergent on nonzero residual problems ([5]), whereas the tensor method of this paper, like Gauss-Newton methods, <p> Finally, we compared our tensor method with the NL2SOL package <ref> [5] </ref> on the set of nonlinear least squares problems used in [5] that is listed in Appendix B. The reason we were interested in making this comparison is that theoretically the NL2SOL method is superlinearly convergent on nonzero residual problems ([5]), whereas the tensor method of this paper, like Gauss-Newton methods, is only linearly convergent on nonzero residual problems. (This difference is
Reference: [6] <author> D. Feng, P. Frank, R. B. Schnabel, </author> <title> An Analysis of Tensor Methods for Nonlinear Equations, </title> <type> Technical Report CS-CS-729-94, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: The reader may refer to [1], [2], <ref> [6] </ref>, and [11] for more details on tensor methods for nonlinear equations and nonlinear least squares problems. <p> 10.5: Average Ratios of the Tensor Method versus the Gauss-Newton Method on Zero Residual Problems for Line Search and Trust Region Rank Line Search Trust Region F 0 (x fl ) Itn Feval Itn Feval n 0.43 0.44 0.43 0.56 n 2 0.48 0.48 0.51 0.57 with constant 1 2 <ref> [6] </ref>. The tensor method is also significantly more robust than the standard Newton-based method for the nonlinear equations test set.
Reference: [7] <author> A. O. Griewank, </author> <title> Analysis and Modification of Newton's Method at Singularities, </title> <type> Ph.D. thesis, </type> <institution> Australian National University, Canberra, </institution> <year> 1980. </year>
Reference-contexts: 0 (x fl ) Better Worse Tie Tensor/Newton Newton Tensor Itn Feval Solved Solved n 25 2 13 0.60 0.69 1 5 n 2 27 1 5 0.46 0.56 0 8 We tested our tensor algorithm on 17 test functions for systems of nonlinear equations (also including 4 functions from <ref> [7] </ref> whose Jacobian at the solution x fl is singular and are designated as Griewank functions) and 11 test functions for nonlinear least squares. Some of the test problems were run at various dimensions. All of these problems were also run with the standard method.
Reference: [8] <author> J. J. </author> <title> More, The Levenberg-Marquardt Algorithm: Implementation and Theory, in Numerical Analysis, </title> <editor> G. A. Watson, ed., </editor> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> vol. 630, </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977, </year> <pages> 105-116. </pages>
Reference-contexts: When ffi c is shorter than the standard step, the locally constrained optimal method <ref> [8] </ref> finds a c such that jjd ( c )jj 2 ffi c , where d ( c ) = (J (x c ) T J (x c ) + I) 1 J (x c ) T F (x c ).
Reference: [9] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom, </author> <title> Testing Unconstrained Optimization Software, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 7 (1981), </volume> <pages> 17-41. </pages>
Reference-contexts: (x c )d k 2 2 kF (x c ) k 2 endif if ratio 10 4 then the global step d is successful else decrease trust region go to Step 0 endif The methods used for adjusting the trust radius during and between steps are given in Algorithm A6.4.5 <ref> [9, p. 338] </ref>. The initial trust radius can be supplied by the user; if not, it is set to the length of the initial Cauchy step. Our software solves the one-variable global optimization problem by a straightforward partitioning scheme described in [2]. 4. <p> All our computations were performed on a Sun SPARCstation 2 computer in the Computer Science Department at the University of Colorado at Boulder, using double-precision arithmetic. First we tested the software package on the set of nonlinear equations and nonlinear least squares problems in More, Garbow, and Hillstrom <ref> [9] </ref>. These problems all have nonsingular Jacobians at the solution with the exception of Powell's singular function.
Reference: [10] <author> M. J. D. Powell, </author> <title> A New Algorithm for Unconstrained Optimization, in Nonlinear Programming, </title> <editor> J. B. Rosen, O.L. Mangasarian, and K. Ritter, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970, </year> <pages> 33-65. </pages>
Reference-contexts: Then it takes x + = x c + d ( c ). The dogleg method is a modification of the trust region algorithm introduced by Powell <ref> [10] </ref>.
Reference: [11] <author> R. B. Schnabel and P. D. Frank, </author> <title> Tensor Methods for Nonlinear Equations, </title> <journal> SIAM. J. Num. Anal., </journal> <volume> 21 (1984), </volume> <pages> 815-843. </pages>
Reference-contexts: The tensor methods upon which this software package is based were originally introduced by Schnabel and Frank <ref> [11] </ref>, for nonlinear equations. One main contribution of this paper is the provision and extensive testing of a software package incorporating these methods. In addition, the extension of these methods to nonlinear least squares, and the incorporation of a trust region strategy with tensor methods, are new contributions. <p> The procedure of finding linearly independent directions is implemented easily by using a modified Gram-Schmidt algorithm, and usually results in p = 1 or 2. After selecting the linearly independent past directions s k , the tensor term is chosen by the procedure of Schnabel and Frank <ref> [11] </ref>, which generalizes in a straightforward way to nonlinear least squares. <p> It is possible that no root exists; in this case a least squares solution of the model is found instead. Thus, in general, we solve the problem minimize d2R n kM (x c + d) k 2 : (2:6) A generalization of the process in Schnabel and Frank <ref> [11] </ref> shows that the solution to (2.6) can be reduced to the solution of a small number of quadratic equations, mn+q quadratic equations in p unknowns, plus the solution of n q linear equations in n p unknowns. <p> The reader may refer to [1], [2], [6], and <ref> [11] </ref> for more details on tensor methods for nonlinear equations and nonlinear least squares problems. <p> These problems all have nonsingular Jacobians at the solution with the exception of Powell's singular function. Then we created singular test problems as proposed in Schnabel and Frank <ref> [11] </ref> by modifying the nonsingular test problems of More, Garbow, and Hillstrom to the form ^ F (x) = F (x) F 0 (x fl )A (A T A) 1 A T (x x fl ); (10:1) where F (x) is the standard nonsingular test function, x fl is its root
Reference: [12] <author> R. B. Schnabel, J. E. Koontz, and B. E. Weiss, </author> <title> A Modular System of Algorithms of Unconstrained Minimization, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 11 (1985), </volume> <pages> 419-440. 31 </pages>
Reference-contexts: A nonlinear unconstrained optimization software package, UNCMIN <ref> [12] </ref>, is used to minimize the l 2 norm of the m n + q quadratic equations in the p unknowns ^ d 2 . (If p = 1, this procedure is done analytically instead.) 4. <p> chosen whenever the tensor step is not a descent direction, when the tensor step is a minimizer of the tensor model and does not provide enough decrease in the tensor model, or when the quadratic system of m n + q equations in p unknowns cannot be solved by UNCMIN <ref> [12] </ref> within the iteration limit. Otherwise, the tensor step is chosen. In the definitions of d t and M T , the Newton step and model are used for nonlinear equations, while the Gauss-Newton step and model are used for nonlinear least squares. Algorithm 3.1. <p> The program package consists of approximately 9060 lines of code, of which 2540 lines are subroutines from the software package UNCMIN <ref> [12] </ref> for unconstrained nonlinear optimization, 900 lines are blas subroutines, and about 25% are comments. The total data storage required is about M fi (N + 2 p p double-precision floating points.
References-found: 12

