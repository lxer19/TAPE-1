URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/fisher96a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/fisher96a.html
Root-URL: 
Email: dfisher@vuse.vanderbilt.edu  
Title: Iterative Optimization and Simplification of Hierarchical Clusterings  
Author: Doug Fisher 
Address: Box 1679, Station B  Nashville, TN 37235 USA  
Affiliation: Department of Computer Science,  Vanderbilt University,  
Note: Journal of Artificial Intelligence Research 4 (1996) 147-179 Submitted 3/95; published 4/96  
Abstract: Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahn, W., & Medin, D. L. </author> <year> (1989). </year> <title> A two-stage categorization model of family resemblance sorting.. </title> <booktitle> In Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 315-322. </pages> <address> Ann Arbor, MI: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Observations are at leaves and are described by three variables: Size, Color, and Shape. 2.3 Hierarchical Sorting Our strategy for initial clustering is sorting, which is a term adapted from a psychological task that requires subjects to perform roughly the same procedure that we describe here <ref> (Ahn & Medin, 1989) </ref>.
Reference: <author> Anderson, J. R., & Matessa, M. </author> <year> (1991). </year> <title> An iterative Bayesian algorithm for categorization. </title>
Reference-contexts: Nonetheless, this measure is commonly used, we will take this opportunity to note its problems, and none of the techniques that we describe is tied to this measure. 2.2 The Structure of Clusters As in Cobweb, Autoclass (Cheeseman et al., 1988), and other systems <ref> (Anderson & Matessa, 1991) </ref>, we will assume that clusters, C k , are described probabilistically: each variable value has an associated conditional probability, P (A i = V ij jC k ), which reflects the proportion of observations in C k that exhibit the value, V ij , along variable A
Reference: <author> In Fisher, D., Pazzani, M., & Langley, P. (Eds.), </author> <title> Concept formation: Knowledge and Experience in Unsupervised Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Biswas, G., Weinberg, J., & Li, C. </author> <year> (1994). </year> <title> Iterate: A conceptual clustering method for knowledge discovery in databases. </title> <editor> In Braunschweig, B., & Day, R. (Eds.), </editor> <booktitle> Innovative Applications of Artificial Intelligence in the Oil and Gas Industry. </booktitle> <publisher> Editions Technip. </publisher>
Reference: <author> Biswas, G., Weinberg, J. B., Yang, Q., & Koller, G. R. </author> <year> (1991). </year> <title> Conceptual clustering and exploratory data analysis. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pp. 591-595. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The basic strategy has been used in one form or another by numerous sort-based algorithms as well (Fisher et al., 1992). The idea behind iterative redistribution <ref> (Biswas, Weinberg, Yang, & Koller, 1991) </ref> is simple: observations in a single-level clustering are `removed' from their original cluster and resorted relative to the clustering. If a cluster contains only one observation, then the cluster is `removed' and its single observation is resorted.
Reference: <author> Carpineto, C., & Romano, G. </author> <year> (1993). </year> <title> Galois: An order-theoretic approach to conceptual clustering. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 33-40. </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <pages> AutoClass: </pages>
Reference-contexts: This allows an analyst to get an early indication of the possible presence and form of structure in data, but search can continue as long as it seems worthwhile. This seems to be a primary motivation behind the design of systems such as Autoclass <ref> (Cheeseman, Kelly, Self, Stutz, Taylor, & Freeman, 1988) </ref> and Snob (Wallace & Dowe, 1994). This paper describes and evaluates three strategies for iterative optimization, one inspired by the iterative `seed' selection strategy of Cluster/2 (Michalski & Stepp, 1983a, c fl1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. <p> Nonetheless, this measure is commonly used, we will take this opportunity to note its problems, and none of the techniques that we describe is tied to this measure. 2.2 The Structure of Clusters As in Cobweb, Autoclass <ref> (Cheeseman et al., 1988) </ref>, and other systems (Anderson & Matessa, 1991), we will assume that clusters, C k , are described probabilistically: each variable value has an associated conditional probability, P (A i = V ij jC k ), which reflects the proportion of observations in C k that exhibit the <p> Our method of validation and pruning is inspired by retrospective pruning strategies in decision tree induction such as reduced error pruning (Quinlan, 1987, 1993; Mingers, 1989a). In a Bayesian clustering system such as Autoclass <ref> (Cheeseman et al., 1988) </ref>, or 9. <p> For example, we might consider Bayesian variants like those found in Autoclass <ref> (Cheeseman et al., 1988) </ref> and Anderson and Matessa's (1991) system, or the closely related MML approach of Snob (Wallace & Dowe, 1994). We do not evaluate alternative measures such as these here, but do suggest a number of other candidates. 12.
References-found: 7

