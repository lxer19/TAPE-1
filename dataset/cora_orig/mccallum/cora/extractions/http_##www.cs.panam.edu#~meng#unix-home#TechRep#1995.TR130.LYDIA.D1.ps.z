URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/1995.TR130.LYDIA.D1.ps.z
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/
Root-URL: http://www.cs.panam.edu
Title: Adaptive Resource Management Problem, Cost Functions and Performance Objectives  FORTH Type: Deliverable  Status: Deliverable Confidentiality: Public  
Author: Authors: G. Georgiannakis, C. Houstis, S. Kapidakis, M. Karavassili, A. Labrinidis, M. Marazakis, E. Markatos, M. Mavronicolas, C. Nikolaou FORTH S. Chabridon, E. Gelenbe EHEI E. Born SNI L. Richter, R. Riedl U. Editors: M. Karavassili, C. Nikolaou LYDIA Id: 
Date: December 1994  
Affiliation: of Zurich  
Note: Title: Description of the  LYDIA/WP.1/T1.1/D1 Date:  
Abstract-found: 0
Intro-found: 1
Reference: [AA92] <author> E. Aharonson and H. Attiya. </author> <title> "Counting Networks with Arbitrary FanOut". </title> <booktitle> In Proceedings of the 3rd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 104-113, </pages> <year> 1992. </year>
Reference: [AADW94] <author> M. Ajtai, J. Aspnes, C. Dwork, and O. Waarts. </author> <title> "A Theory of Competitive Analysis for Distributed Algorithms". </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1994. </year>
Reference: [AAF + 93] <author> J. Aspnes, Y. Azar, A. Fiat, S. Plotkin, and O. Waarts. </author> <title> "On-line load balancing with applications to machine scheduling and virtual circuit routing". </title> <booktitle> In Proc. 25th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 623-631, </pages> <year> 1993. </year>
Reference: [AALL93] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and A. W. Lim. </author> <title> "An Overview of a Compiler for Scalable Parallel Machines". </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing". The Stanford SUIF Compiler Group, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: They combine the mathematical rigor in the matrix transformation model with the generality of the vectorizing and concurrentizing compiler approach. Problems on loop parallelism, more specific to scientific applications are also dealed with. Amarasinghe et al, in <ref> [AALL93] </ref>, deal with loop-level parallelism in dense matrix computations. They present an overview of the parallelizing compiler which first transforms the program to expose loop-level parallelism in the computation and then finds a decomposition of the computation and data such that parallelism is exploited and the communication overhead is minimized.
Reference: [AB90] <author> N. Audsley and A. Burns. </author> <title> "Real-Time System Scheduling". </title> <type> Technical Report YCS 134, </type> <institution> Department of Computer Science, Univ. of York, </institution> <month> January </month> <year> 1990. </year>
Reference: [ABF93] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> "Competitive and Distributed File Allocation". </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of C omputing, </booktitle> <pages> pages 164-173, </pages> <month> May </month> <year> 1993. </year>
Reference: [ABK92] <author> Y. Azar, A. Broder, and A. Karlin. </author> <title> "On-Line Load Balancing". </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 218-225, </pages> <month> October </month> <year> 1992. </year>
Reference: [ABLL92] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. "Sched-uler Activations: </author> <title> Effective Kernel Support for the User-Level Management of Parallelism". </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: When the scheduler descheduled a process, it checks the bit first. If the bit is set, the scheduler does not preempt the process. To prevent malicious users from monopolizing the CPU, the scheduler abides to the do-not-preempt-me bit only a limited amount of times. Scheduler activations <ref> [ABLL92] </ref> is another way to address the overhead associated with preemption inside a critical section. 4 Leutenegger's simulations run on top of the Condor load balancing system! Page 96 6.3.
Reference: [ACF86] <author> Y. Artsy, H.-Y. Chang, and R. Finkel. </author> <title> "Processes Migrate in Charlotte". </title> <type> Technical report, </type> <institution> Univ. of Wisconsin, </institution> <month> August </month> <year> 1986. </year>
Reference: [ACRS94] <author> A. K. Agrawala, M. Calzarossa, P. Rossaro, and G. </author> <title> Serazzi. "Workload Modelling in Multi-Window Environments (in preparation)", </title> <booktitle> 1994. </booktitle> <pages> 107 </pages>
Reference: [AF87] <author> Y. Artsy and R. Finkel. </author> <title> "Simplicity, Efficiency, and Functionality in De--signing a Process Migration Facility". </title> <booktitle> Proceedings of the Second Israel conference on Computer Systems and Software Engineering, </booktitle> <month> May </month> <year> 1987. </year>
Reference: [AF89] <author> Y. Artsy and R. Finkel. </author> <title> "Designing a Process Migration Facility: The Charlotte Experience". </title> <journal> IEEE Computer, </journal> <volume> 22(9) </volume> <pages> 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference: [AGM88] <author> R. Abbott and H. Garcia-Molina. </author> <title> "Scheduling Real-Time Transactions: A Performance Evaluation". </title> <booktitle> In Proceedings of the 14th International Conference on Very Large Data Bases, </booktitle> <address> Los Angeles, Ca, </address> <year> 1988. </year>
Reference: [AHS91] <author> J. Aspnes, M. Herlihy, and N. Shavit. </author> <title> "Counting Networks and MultiProcessor Coordination". </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 348-358, </pages> <month> May </month> <year> 1991. </year>
Reference: [AKP92] <author> B. Awerbuch, S. Kutten, and D. Peleg. </author> <title> "Competitive Distributed Job Scheduling". </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 571-580, </pages> <month> May </month> <year> 1992. </year>
Reference: [AKS83] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> "Sorting in c log n Steps". </title> <journal> Combi-natorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference: [AL93a] <author> S. P. Amarasinghe and M. S. Lam. </author> " <title> Communication Optimization and Code Generation for Distributed Memory Machines". </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation. The Stanford SUIF Compiler Group, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Hinrichs and Gross, in [HG92], discuss some optimizations that can be included in a compiler for the iWarp system, an example private-memory parallel system with a novel communication architecture. The compiler produced by Amarasinghe and Lam, in <ref> [AL93a] </ref>, generates the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor and translates global data addresses to local addresses. 6.2.7 Synchronization Synchronization is one of the main problems in parallel execution.
Reference: [AL93b] <author> J. M. Anderson and M. S. Lam. </author> <title> "Global Optimizations for Parallelism and Locality on Scalable Parallel Machines". </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation. The Stanford SUIF Compiler Group, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: They build a framework to methodically choose a task and data parallel mapping based on these program characteristics. The central idea is to find the best use of available memory and communication resources to minimize the global inter-task and intra-task communication overhead. Anderson and Lam, in <ref> [AL93b] </ref>, describe a compiler algorithm that automatically finds computation and data decompositions that optimize both parallelism and locality.
Reference: [ALL89] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> "The Performance Implications of Thread Management Alternatives for Shared Memory Multiprocessors". </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Processors take threads from this queue and run them to completion. The load is evenly balanced in that no processor remains idle as long as there is work to be done. Anderson et. al. <ref> [ALL89] </ref> argued for the use of per-processor ready queues within a thread package to improve scalability (reducing contention for the single ready queue) and to preserve processor affinity. Under this scheme, a newly created thread is placed on the ready queue of the processor on which it was created.
Reference: [And90] <author> T. E. Anderson. </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference: [ANR92] <author> Y. Azar, J. Naor, and R. </author> <title> Rom. "The Competitiveness of On-line Assignment". </title> <booktitle> In 3rd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 203-210, </pages> <month> January </month> <year> 1992. </year>
Reference: [AS93] <author> T. M. Austin and G. S. Sohi. "Tetra: </author> <title> Evaluation of Serial Program Performance on Fine-Grain Parallel Processors". </title> <type> Technical Report 1162, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> July </month> <year> 1993. </year> <month> 108 </month>
Reference-contexts: The decoupled architecture was generally better at hiding the floating point latency. The register sharing enabled the VLIW architecture to do more load balancing which resulted in shorter initialization times, and shorter loop iteration times. Performance evaluation methodologies and tools have been developed: Austin and Sohi, in <ref> [AS93] </ref>, present the extraction, scheduling, and analysis methodologies used by Tetra. They detail their implementation and discuss a number of performance optimizations used. Tetra is a tool for evaluating serial program performance under the resource and control constraints of fine-grain parallel processors.
Reference: [ASS93] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> "Compiler and Runtime Support for Structured and Block Structured Applications". </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587, </pages> <year> 1993. </year>
Reference-contexts: Lamport, in [Lam90], developed algorithms to implement both a monotonic and a cyclic multiple-word clock that is updated by one process and read by one or more other processes, using synchronization without mutual exclusion. Agrawal et al, in <ref> [ASS93] </ref>, integrate runtime library calls to compilers for High Performance Fortran, for applications involving structured meshes, and in [ASS94], present a combined runtime and compile-time approach for parallelizing applications involving structured meshes on distributed memory machines in an efficient and machine-independent fashion.
Reference: [ASS94] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> "An Integrated Runtime and Compile-Time Approach for Parallelizing Structured and Block Structured Applications". </title> <type> Technical Report CS-TR-3143, </type> <institution> University of Maryland, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Agrawal et al, in [ASS93], integrate runtime library calls to compilers for High Performance Fortran, for applications involving structured meshes, and in <ref> [ASS94] </ref>, present a combined runtime and compile-time approach for parallelizing applications involving structured meshes on distributed memory machines in an efficient and machine-independent fashion.
Reference: [ATR82] <author> A. K. Agrawala, S. K. Tripathi, and G. Ricart. </author> <title> "Adaptive Routing Using a Virtual Waiting Time Technique". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 8(1) </volume> <pages> 76-81, </pages> <month> January </month> <year> 1982. </year>
Reference: [Avr90] <author> A. Avritzer et al. </author> <title> "The advantage of dynamic tuning in distributed asymmetric systems". </title> <booktitle> In Proc. Infocom '90, </booktitle> <year> 1990. </year>
Reference: [AVY94] <author> W. Aiello, R. Venkatesan, and M. Yung. </author> <title> "Coins, weights and contention in balancing networks". </title> <booktitle> In 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994. </year>
Reference: [Bar93] <author> V. Barbu. </author> <title> "Analysis and Control of Nonlinear Infinite Dimensional Systems". </title> <address> Boston: </address> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. [BC85], <ref> [Bar93] </ref>, [FC93], [Isi89], [KK85], [SB89]). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations.
Reference: [Bat68] <author> K. E. Batcher. </author> <title> "Sorting Networks and Their Applications". </title> <booktitle> In "Proceedings of the AFIPS Spring Joint Computer Conference", </booktitle> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference: [BB92] <author> A. Bestavros and S. Braoudakis. </author> <title> "A family of Speculative Concurrency Control Algorithms". </title> <type> Technical Report TR-17-92, </type> <institution> Computer Science Department, Boston Univ., </institution> <address> Boston, MA, </address> <month> July </month> <year> 1992. </year>
Reference: [BBG83] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> "A Message System Supporting Fault tolerance". </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 90-99, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: In [LB88] each process in the system can initiate the checkpoint/rollback algorithm and force a minimal number of processes to make new checkpoints or roll back. In pessimistic (i.e. synchronous) log-based methods, every message received is logged to stable storage before it is processed <ref> [BBG83, BBG + 89] </ref>. This insures that the stable information across nodes is always consistent. However, this method slows down every step of the application computation, because of the synchronization needed between logging and processing of incoming messages. To decrease the overhead involved in checkpointing hardware mechanisms may be used.
Reference: [BBG + 89] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, and W. Oberle. </author> <title> "Fault tolerance under UNIX". </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In [LB88] each process in the system can initiate the checkpoint/rollback algorithm and force a minimal number of processes to make new checkpoints or roll back. In pessimistic (i.e. synchronous) log-based methods, every message received is logged to stable storage before it is processed <ref> [BBG83, BBG + 89] </ref>. This insures that the stable information across nodes is always consistent. However, this method slows down every step of the application computation, because of the synchronization needed between logging and processing of incoming messages. To decrease the overhead involved in checkpointing hardware mechanisms may be used.
Reference: [BBLN92] <author> M. A. Blumrich, C. J. Brownhill, K. Li, and A. Nicolau. </author> <title> "An Empirical Comparison of Loop Scheduling Algorithms on a Shared Memory Multiprocessor". </title> <type> Technical Report 360-92, </type> <institution> Princeton Univ., </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: They provide algorithms to compute the cache state on exiting a parallel region given the cache state on entry; and methods to compute the overall cache-coherency traffic and choose block-cyclic parameters to optimize cache-coherency traffic. Other problem parameters have also been taken into account. Blumrich et al, in <ref> [BBLN92] </ref>, study several methods of instruction level parallelization applied at the statement level on a shared memory multiprocessor, and report the results of an empirical evaluation to determine which of the methods yields the best results.
Reference: [BC85] <author> S. Barnett and R. G. Cameron. </author> <title> "Introduction to Mathematical Control Theory". </title> <publisher> Oxford: Clarendon Press, </publisher> <address> 2nd edition, </address> <year> 1985. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. <ref> [BC85] </ref>, [Bar93], [FC93], [Isi89], [KK85], [SB89]). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations.
Reference: [BCD + 92] <author> K. P. Brown, M. J. Carey, D. J. DeWitt, M. Mehta, and J. F. Naughton. </author> <title> "Resource Allocation and Scheduling for Mixed Database Workloads". </title> <type> Technical Report TR 1095, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> July </month> <year> 1992. </year> <month> 109 </month>
Reference: [BCL93] <author> K. P. Brown, M. J. Carey, and M. Livny. </author> <title> "managing memory to meet multiclass workload response time goals". </title> <booktitle> In Proceedings of the 19-th International VLDB Conference, </booktitle> <year> 1993. </year> <note> Also available as Technical Report No. 1146, </note> <institution> University of Wisconsin. </institution>
Reference: [BE93] <author> W. Blume and R. Eigenmann. </author> <title> "Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks programs". </title> <type> Technical Report TR-1218, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: The upper bound on the optimal parallel execution time that they use for their evaluation is the maximum length over all flow-dependence chains generated by an execution of the program. This value is computed by instrumenting the program to keep track of all memory references. Blume and Eigenmann, in <ref> [BE93] </ref>, study the effectiveness of parallelizing compilers and the underlying transformation techniques. They report the speedups of the Perfect Benchmarks T M codes that result from automatic parallelization.
Reference: [Bec94] <author> C. J. Beckmann. </author> <title> "Hardware and Software for Functional and Fine Grain Parallelism". </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Cen-tre of Supercompter Research and Development, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Some programming languages, like Lisp and Scheme, do not even use the loop construct, and iterations are expressed through function calls. Beckmann, in <ref> [Bec94] </ref>, examines nonloop parallelism at both fine and coarse levels of granularity in numerical FORTRAN programs and explores the impact of fine grain functional parallelism on instruction-level architecture.
Reference: [BENP93] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D.A. Padua. </author> <title> "Automatic Program Parallelization". </title> <type> Technical Report TR-1250, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: Therefore static scheduling often suffers from load imbalance. Lilja and Banerjee give an overview of the loop optimization issues. Lilja, in [Lil94], explains scheduling techniques and compares results on different architectures, examining both fine-grained and coarse-grained parallel architectures. Banerjee et al, in <ref> [BENP93] </ref>, present an overview of automatic program parallelization techniques. They cover dependence analysis techniques, followed by a discussion of program transformations, including straight-line code parallelization, do loop transforma Page 78 6.2. COMPILER LAYER tions, and parallelization of recursive routines.
Reference: [BF81] <author> R. Bryant and F. A. Finkel. </author> <title> "A Stable Distributed Scheduling Algorithm". </title> <booktitle> In Proc. 2-nd Int. Conf. on Distr. Comp. Syst., </booktitle> <pages> pages 314-323, </pages> <year> 1981. </year>
Reference: [BFKV92] <author> Y. Bartal, A. Fiat, H. Karloff, and R. Vohra. </author> <title> "New Algorithms for an Ancient Scheduling Problem". </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of C omputing, </booktitle> <pages> pages 51-58, </pages> <month> May </month> <year> 1992. </year>
Reference: [BFLN95] <author> V. Bohn, D. Ferguson, A. Labrinidis, and C. Nikolaou. </author> <title> "CLUE: A tool for workload characterization in OLTP systems (in preparation)", </title> <year> 1995. </year>
Reference: [BFR92] <author> Y. Bartal, A. Fiat, and Y. Rabani. </author> <title> "Competitive Algorithms for Distributed Data Management". </title> <booktitle> In 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 39-50, </pages> <month> May </month> <year> 1992. </year>
Reference: [BGS93] <author> D. F. Bacon, S. L. Graham, and O. J. Sharp. </author> <title> "Compiler Transformations for High-Performance Computing". </title> <type> Technical report, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Specific methods and code transformations have been studied. Bacon et al, in <ref> [BGS93] </ref>, give an overview of the important high-level program restructuring techniques for imperative languages such as C and Fortran, and describe transformations that optimize programs written in such languages. Many computations perform operations that, first, a loop iterates over an input array, producing an array of (partial) results (Do computation).
Reference: [BGS94] <author> P. Brezany, M. Gerndt, and V. Sipkova. </author> <title> "SVM Support in the Vi-enna Fortran Compilation System". </title> <type> Technical Report KFA-ZAM-IB-9401, </type> <institution> Forschungszentrum Juelich GmbH (KFA), Germany, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Their experimental evidence used the Mercury system. Brezany et al, in <ref> [BGS94] </ref>, propose to generate shared virtual memory code, instead of code for distributed-memory systems using global addresses, for Vienna Fortran that can benefit from appropriate operating system or hardware support. They present the shared virtual memory code generation, compare both approaches and gives first performance results.
Reference: [BGT91] <author> P. Bhattacharya, L. Georgiadis, and P. Tsoucas. </author> <title> "Optimal Adaptive Scheduling in Multi-Class M=GI=1 Queues with Feedback". </title> <booktitle> In Proc. of the 29th Allerton Conference on Communication, Control and Computing, </booktitle> <month> September </month> <year> 1991. </year>
Reference: [BGTV90] <author> P. Bhattacharya, L. Georgiadis, P. Tsoucas, and I. Viniotis. </author> <title> "Optimality and Finite Time Behavior of an Adaptive Multi-Objective Scheduling Algorithm". </title> <booktitle> In Proc. of the 29th Conference on Decision and Control, </booktitle> <month> December </month> <year> 1990. </year>
Reference: [BGTV92] <author> P. Bhattacharya, L. Georgiadis, P. Tsoucas, and I. Viniotis. </author> <title> "Adaptive Lexicographic Optimization in multi-class M=GI=1 queues". </title> <note> Mathematics of Operations Research, 1992. To appear. </note>
Reference: [BHG87] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> "Concurrency control and recovery in database systems". </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: If a process fails, all the processes must automatically be backed up to the start of the conversation to attempt their alternates. Dependability has also been a central concern in distributed databases and in the concur-rency control algorithms which have been designed for such systems <ref> [BHG87, CGM88] </ref>. 5.4.2 Fault-tolerance based on the replication of processes Another approach to fault-tolerance consists in creating multiple copies of the processes on different processors.
Reference: [BHM94] <author> C. Busch, N. Hardavellas, and M. Mavronicolas. </author> <title> "Contention in Counting Networks". </title> <booktitle> In 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference: [BK90] <author> F. Bonomi and A. Kumar. </author> <title> "Adaptive optimal load balacing in a nonhomogeneous multiserver system with a central job scheduler". </title> <journal> IEEE trans. on comp., </journal> <volume> C-39, 10 </volume> <pages> 1232-1250, </pages> <year> 1990. </year>
Reference-contexts: In the most optimal case, this vector would specify indeed a decision by consisting of one 1 and 0 otherwise. The problem is solved up to now only for some special cases, cf. [NH81], [NH85b] or <ref> [BK90] </ref>. We can apply this approach also in order to evaluate a given load balancing algorithm instead of finding an optimal one.
Reference: [BL88] <author> B. Bhargava and S-R. Lian. </author> <title> "Independent checkpointing and concurrent rollback for recovery in distributed systems An optimistic approach". </title> <booktitle> In 7th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3-12, </pages> <year> 1988. </year>
Reference-contexts: In [JZ90], the authors prove the existence of a unique maximal consistent global state requiring to undo the minimal amount of the computation performed before a failure, but this state is not easy to determine. In <ref> [BL88] </ref>, processes take checkpoints independently. During recovery after a failure, a process invokes a two-phase rollback algorithm.
Reference: [Bla90a] <author> D. L. Black. </author> <title> "Scheduling and Resource Management Techniques for Multiprocessors". </title> <type> Research Report (Ph.D. thesis) CMU-CS-90-152, CMU, </type> <year> 1990. </year>
Reference: [Bla90b] <author> D. L. Black. </author> <title> "Scheduling Support for Concurrency and Parallelism in the Mach Operating System". </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Black <ref> [Bla90b] </ref> has reached similar conclusion when he implemented space-sharing on top of Mach. In his policy, each application is guaranteed a number of processors for a long period of time (several minutes). <p> A parallel application creates as many threads as it needs to express its parallelism. All threads run within the same address space (within the same process). Creation and scheduling of threads is sometimes being done by the kernel (with user-hints) <ref> [Bla90b] </ref>, or completely more frequently, by a library running in user-space [SLM + 90]. Thread creation, destruction and scheduling are inexpensive operations.
Reference: [BLLW88] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy, and D. B. Wagner. </author> <title> "An Open Environment for Building Parallel Programming Systems". </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988 Symposium on Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pages 1-9, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: For example, in the process control scheme [TG89], Uniform System [TC88], Brown Threads [Doe87], and Presto <ref> [BLLW88] </ref>, all threads of the same application are placed in a FIFO central work queue. Processors take threads from this queue and run them to completion. The load is evenly balanced in that no processor remains idle as long as there is work to be done.
Reference: [BMa] <author> C. Busch and M. Mavronicolas. </author> <title> "A Logarithmic-Depth Counting Network". </title> <note> Submitted for publication. </note>
Reference: [BMb] <author> C. Busch and M. Mavronicolas. </author> <title> "Odd-Even Counting Networks". </title> <note> In preparation. </note>
Reference: [BM94a] <author> C. Busch and M. Mavronicolas. </author> <title> "A Combinatorial Treatment of Balancing Networks". </title> <booktitle> In 13th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference: [BM94b] <author> C. Busch and M. Mavronicolas. </author> <title> "Proving Correctness of Balancing Networks". </title> <booktitle> In Proceedings of the DIMACS Workshop on Parallel Processing of Discrete Optimization Problems, </booktitle> <institution> DIMACS (Center for Discrete Mathematics and Theoretical Computer Science), Rutgers Univ., </institution> <month> April </month> <year> 1994. </year>
Reference: [BMCL94] <author> K. P. Brown, M. Mehta, K. J. Carey, and M. Livny. </author> <title> "towards automated performance tuning for complex workloads". </title> <booktitle> In Proceedings of the 20-th International VLDB Conference, </booktitle> <pages> pages 578-587, </pages> <year> 1994. </year> <month> 111 </month>
Reference: [Bok81] <author> S. Bokhari. </author> <title> "On the Mapping Problem". </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(3) </volume> <pages> 207-214, </pages> <year> 1981. </year>
Reference-contexts: Without loss of generality, it is assumed that the size of the two graphs is the same (jV M j = jV A j). For general acyclic graphs the graph allocation problem is stated in <ref> [Bok81] </ref> and is equivalent to the minimization of the cost function max c (D i ; D j )d (m 1 (D i ); m 1 (D j )) (6.2) where m is the mapping, c is the adjacency matrix of the graph G M and d is the distance (shortest
Reference: [Bop87] <author> R. B. Boppana. </author> <title> "Eigenvalues and graph bisection: An average case analysis". </title> <booktitle> In 28th Foundation of Computer Science (ACM), </booktitle> <pages> pages 280-285, </pages> <year> 1987. </year>
Reference-contexts: Note that this is the allocation phase of the original problem defined by equation 6.1, and satisfies objective (v). The Page 72 6.1. APPLICATION LAYER Table 6.1: Mesh Partitioning Algorithms Name Description 1 fi P 1-D strips P fi Q 2-D strips ORB-E Eigenvalue Ortho. Rec. Bisection <ref> [Bop87] </ref> ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition
Reference: [BS79] <author> C. Brown and M. Schwartz. </author> <title> "Adaptive routing in central computer communication networks". </title> <booktitle> In Proc. IEEE Int. Comp. Commum., </booktitle> <pages> pages 12-16, </pages> <month> June </month> <year> 1979. </year>
Reference: [BS89] <author> D. L. Black and D. D. Sleator. </author> <title> "Competitive Algorithms for Replication and Migration Problems". </title> <type> Technical report, </type> <institution> CMU-CS-89-201, </institution> <year> 1989. </year>
Reference: [BS93] <author> A. Berlin and R. Surati. </author> <title> "Exploiting the Parallelism Exposed by Partial Evaluation". </title> <type> Technical Report 1414, </type> <institution> MIT, AI Lab, </institution> <year> 1993. </year>
Reference-contexts: This results in an effective scheme for exploitation of parallelism and optimization of the code. Compilers, taking advantage of partial evaluation, have been build by Berlin and Surati, and Surati, for the Supercomputer Toolkit parallel processor machine. Berlin and Surati, in <ref> [BS93] </ref>, have constructed a compiler for the Supercomputer Toolkit parallel processor that uses partial evaluation to break down data abstractions and program structure, producing huge basic blocks that contain large amounts of fine-grain parallelism.
Reference: [BS94] <author> A. Berlin and R. Surati. </author> <title> "Partial Evaluation for Scientific Computing: the Super Computer Experience". </title> <type> Technical Report AIM-1487, </type> <institution> MIT, AI Lab, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: New static scheduling techniques are used to utilize the fine-grained parallelism of the computations. The compiler maps the computation graph resulting from partial evaluation onto the Supercomputer Toolkit. Berlin and Surati, in <ref> [BS94] </ref>, use the Supercomputer Toolkit and use partial evaluation to convert a high-level, data-independent program in to a low-level, purely numerical data-flow graph was measured by expressing the data-flow graph in an rtl-style program expressed in the C programming language, by using a C vector to store the numerical value produced
Reference: [CA82] <author> T. C. K. Chou and J. A. Abraham. </author> <title> "Load Balancing in Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-8(4):401-412, </volume> <month> July </month> <year> 1982. </year>
Reference: [CDD + 91] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> "multiprogramming on multiprocessors". </title> <booktitle> Proceedings of the Third Sysmposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year> <note> Also published as TR 385, URCSD February 1991 (revised May 1991). </note>
Reference-contexts: So, even if the preemted process holds a lock, no other process will ask for the lock. Coscheduling has been implemented in the Medusa distributed operating system [OSS80], and in the Psyche multiprocessor operating system <ref> [CDD + 91] </ref>. Although coscheduling addresses the synchronization problems mentioned above, it may lead to processor un-derutilization. For example, suppose a 10-processor system, and two applications: the one has 10 processes while the other has 5 processes. When the second application runs, only five processors are used effectively. <p> Subsequent work by the same authors [GTU91] verifies via simulation that space-sharing outperforms coscheduling and naive time-sharing. Crovella et. al. implemented a semi- dynamic space sharing policy on top of a BBN Butterfly plus parallel processor, and compare it to coscheduling and naive time-sharing <ref> [CDD + 91] </ref>. Their result suggest that space sharing outperforms all other policies. They suggest that the major advantage of space sharing stems from the fact that parallel applications has sublinear speedup.
Reference: [CDY86] <author> D. W. Cornell, D. M. Dias, and P. S. Yu. </author> <title> "On Multisystem Coupling through Function-Request Shipping". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(10) </volume> <pages> 1006-1017, </pages> <month> October </month> <year> 1986. </year>
Reference: [CF86] <author> M. Calzarossa and D. Ferrari. </author> <title> "A sensitivity study of the clustering approach to workload modeling". Performance Evaluation, </title> <booktitle> 6 </booktitle> <pages> 25-33, </pages> <year> 1986. </year>
Reference: [CFR93] <author> J.-F. Collard, P. Feautrier, and T. Risset. </author> <title> "Construction of DO Loop from Systems of Affine Constraints". </title> <note> to appear in PPL 93-15, LIP, </note> <year> 1993. </year>
Reference-contexts: By counting these, we obtain an estimate of the execution time of the loop. In more complicated applications, we can evaluate the computation/memory balance of a computation, determine if a loop is load balanced and evaluate message traffic and allocate message buffers Collard et al, in <ref> [CFR93] </ref>, detail an algorithm for parallelization techniques for DO loop nests based on reindexation, which relies on a parameterized version of the Dual Simplex and show how the resulting loop nest and especially the loop bounds can be kept simple and streamlined, so as not to reduce the benefits of parallelization.
Reference: [CFW + 94] <author> J. Y. Chung, D. Ferguson, G. Wang, C. Nikolaou, and J. Teng. </author> <title> "Goal Oriented Dynamic Buffer Pool Management for Data Base Systems". </title> <type> Technical Report RC 19807, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> October </month> <year> 1994. </year> <note> Also published as FORTH-ICS TR-125, October 94. </note>
Reference: [CG94] <author> S. Chabridon and E. Gelenbe. </author> <title> "Scheduling of distributed tasks for survivability of the application". </title> <booktitle> LYDIA ESPRIT III BRA Deliverable WP1/T1.1/D4, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: surges are known to eliminate certain processes from memory. 5.4.1 Checkpointing and rollback techniques in dis tributed systems The existence of stable memories accessible by one processor or by a group of processors makes it possible to apply to distributed systems the methods of checkpointing and rollback recovery presented in <ref> [CG94] </ref>. In the case of local stable memories, the data stored in the local memory of a faulty processor stay unavailable until the processor is recovered or repaired. One possible improvement is to have several stable memories accessible to several machines.
Reference: [CGM88] <author> W. Cellary, E. Gelenbe, and J. Morzy. </author> <title> "Concurrency control in distributed databases". </title> <publisher> Elsevier North-Holland, </publisher> <address> Amsterdam and New York, </address> <year> 1988. </year> <month> 112 </month>
Reference-contexts: If a process fails, all the processes must automatically be backed up to the start of the conversation to attempt their alternates. Dependability has also been a central concern in distributed databases and in the concur-rency control algorithms which have been designed for such systems <ref> [BHG87, CGM88] </ref>. 5.4.2 Fault-tolerance based on the replication of processes Another approach to fault-tolerance consists in creating multiple copies of the processes on different processors.
Reference: [Che73] <author> P. Chen. </author> <title> "Optimal file allocation in Multi-level Storage Systems". </title> <booktitle> In Proc. AFIPS National Computer Conference, </booktitle> <volume> volume 42, </volume> <pages> pages 277-282, </pages> <year> 1973. </year>
Reference: [Che94] <author> D.-K. Chen. </author> <title> "Compiler optimizations for parallel loops with fine-grained synchronization". </title> <type> Technical Report TR-1863, </type> <institution> Department of Computer Science,University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: They examine these constraint sets to identify the kinds of transformations that are needed to exploit scalable parallelism. Their tests will identify conditional parallelism and parallelism that can be exposed by combinations of transformations that reorder the iteration space (such as loop interchange and loop peeling). Chen, in <ref> [Che94] </ref>, examines issues involved in the DOACROSS parallelization, the par-allelization with fine-grained synchronization of loops with loop-carried dependencies, which are largely serialized using the traditional techniques. The focus is in reducing communication overhead.
Reference: [CHH + 89] <author> N. P. Chrisochoides, C. E. Houstis, E. N. Houstis, S. K. Kortesis, and J. R. Rice. </author> <title> "Automatic load balanced partitioning strategies for PDE computations". </title> <editor> In E. N. Houstis and D. Gannon, editors, </editor> <booktitle> Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 99-107, </pages> <address> New York, 1989. </address> <publisher> ACM Publications. </publisher>
Reference-contexts: The objective function for the mapping of a mesh/grid M onto a distributed memory Page 69 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK MIMD machine so that the workload of the processors is balanced and the required communication and synchronization among processors is minimum, is formulated in <ref> [CHH + 89] </ref> and is as follows: min max fW (m (D i )) + D j 2C D i where D i is the set of mesh points (subdomain) that are assigned to the same processor, C D i is the set of subdomains that are adjacent to the subdomain <p> A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh <ref> [CHH + 89] </ref>, [CHH90], [Sim90]. <p> The mathematical foundations of the optimization problem are stated in [CHH90]. A domain decomposer tool [Chr91] has a library of various P-way algorithms for minimizing the above cost functions. It includes the well-known kernighan-Lin heuristic [KL70], <ref> [CHH + 89] </ref> technique for minimizing the cut-cost of the mesh graph, assuming the solution satisfies the constraints of the balanced partitioning. The idea of this approach is to identify an improved feasible solution by interchanging elements among the subdomains that optimize a profit function. <p> The idea of this approach is to identify an improved feasible solution by interchanging elements among the subdomains that optimize a profit function. In [CHH90] a P-way partitioning algorithm has been developed with a modified profit function based on Kernighan-Lin's idea of selecting improved feasible solutions <ref> [CHH + 89] </ref>. This method is referred as the GGP (Geometric Graph Partitioning) algorithm. A recursive variation of this algorithm based on a modified 2-way Kernighan-Lin algorithm has been also developed [CHH + 89] and named GGP-rec; this heuristic is also Page 71 CHAPTER 6. <p> partitioning algorithm has been developed with a modified profit function based on Kernighan-Lin's idea of selecting improved feasible solutions <ref> [CHH + 89] </ref>. This method is referred as the GGP (Geometric Graph Partitioning) algorithm. A recursive variation of this algorithm based on a modified 2-way Kernighan-Lin algorithm has been also developed [CHH + 89] and named GGP-rec; this heuristic is also Page 71 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK called orthogonal recursive bisection [Fox86], [Wil90]. These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. <p> In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in <ref> [CHH + 89] </ref>, [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings. <p> The Page 72 6.1. APPLICATION LAYER Table 6.1: Mesh Partitioning Algorithms Name Description 1 fi P 1-D strips P fi Q 2-D strips ORB-E Eigenvalue Ortho. Rec. Bisection [Bop87] ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB <ref> [CHH + 89] </ref> ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length between the two subdomains D <p> Rec. Bisection [Bop87] ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB <ref> [CHH + 89] </ref> ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length between the two subdomains D i , D j or the communication requirements between them.
Reference: [CHH90] <author> N. P. Chrisochoides, C. E. Houstis, and E. N. Houstis. </author> <title> "Geometry based mapping strategies for PDE computation". </title> <type> Technical Report CER-90-16, </type> <institution> Computer Science Department, Purdue Univ., West Lafayette, </institution> <note> IN 47907, </note> <year> 1990. </year>
Reference-contexts: A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], <ref> [CHH90] </ref>, [Sim90]. <p> The simplest graph partitioning problem is the 2-way one, in which the graph nodes are divided between two partitions (subdomains), D 1 and D 2 . The mathematical foundations of the optimization problem are stated in <ref> [CHH90] </ref>. A domain decomposer tool [Chr91] has a library of various P-way algorithms for minimizing the above cost functions. It includes the well-known kernighan-Lin heuristic [KL70], [CHH + 89] technique for minimizing the cut-cost of the mesh graph, assuming the solution satisfies the constraints of the balanced partitioning. <p> The idea of this approach is to identify an improved feasible solution by interchanging elements among the subdomains that optimize a profit function. In <ref> [CHH90] </ref> a P-way partitioning algorithm has been developed with a modified profit function based on Kernighan-Lin's idea of selecting improved feasible solutions [CHH + 89]. This method is referred as the GGP (Geometric Graph Partitioning) algorithm. <p> These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem. In [HKB90] various artificial neural networks (ANN) have been considered for solving the partitioning phase of the mapping problem. In <ref> [CHH90] </ref> a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. <p> In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], <ref> [CHH90] </ref>, [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings. <p> For this purpose we adopt G A (V A ; E A ) to represent the interconnection graph of the architecture and G M (V M ; E M ) to represent the topological graph of the mesh <ref> [CHH90] </ref>. Without loss of generality, it is assumed that the size of the two graphs is the same (jV M j = jV A j). <p> A rather general approach is to project both G M and G A graphs to the same space (i.e., Euclidean space) and solve the assignment (allocation) problem for the projected graphs. Some projection techniques to Euclidean space are described in <ref> [CHH90] </ref>. <p> 1 (i) ) 2 + (y M m 1 (i) ) 2 ] 1=k (6.3) where k = 1; 2 and (x M i ; y M i ; y A i ) are the coordinates of the graph nodes (G M ; G A ) projected in Euclidean space, <ref> [CHH90] </ref>. Several techniques have been implemented for solving the above minimization problems. They are classified as explicit, implicit or naive. <p> For the explicit solution of the optimization problem, several algorithms have been tested [HaN72], [Wei71], [CT80], and [Wes83], the one selected in <ref> [CHH90] </ref> is called EXPLICIT H. In this heuristic c (D i ; D j ) models the interface length of the subdomains. Two implicit approaches based on subdomain exchange among processors and greedy procedures to achieve these goal are used in [Got81], [CHH90]. <p> [Wei71], [CT80], and [Wes83], the one selected in <ref> [CHH90] </ref> is called EXPLICIT H. In this heuristic c (D i ; D j ) models the interface length of the subdomains. Two implicit approaches based on subdomain exchange among processors and greedy procedures to achieve these goal are used in [Got81], [CHH90]. These algorithms are referred as SUBD-EXCH and GREEDY, respectively. Table 6.2 summarizes the allocation algorithms. Stochastic optimization techniques for the allocation problem have been considered explicitly or implicitly by several authors. A review of these methodologies is presented in [Far89]. Page 73 CHAPTER 6.
Reference: [Cho93] <author> J.-H. Chow. </author> <title> "Compile-Time Analysis of Explicitly Parallel Programs". </title> <type> Technical Report TR-1301, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: They propose a new Temporal Communication Graph model that captures communication dependency and overlap of communication with computation. Chow, in <ref> [Cho93] </ref>, proposes a general framework for analyzing parallel programs where concurrent activities interact through shared variables and presents the analyses of side effects, data dependencies, object lifetimes, and concurrent expressions for a language with dynamic allocations, pointers, first-class functions, and cobegin parallelism. He eliminates redundant interleavings and uses abstract interpretation.
Reference: [Chr91] <author> N. P. Chrisochoides. </author> <title> "On the mapping of PDE computations to distributed memory machines". </title> <type> PhD thesis, </type> <institution> Purdue Univ., </institution> <year> 1991. </year>
Reference-contexts: The simplest graph partitioning problem is the 2-way one, in which the graph nodes are divided between two partitions (subdomains), D 1 and D 2 . The mathematical foundations of the optimization problem are stated in [CHH90]. A domain decomposer tool <ref> [Chr91] </ref> has a library of various P-way algorithms for minimizing the above cost functions. It includes the well-known kernighan-Lin heuristic [KL70], [CHH + 89] technique for minimizing the cut-cost of the mesh graph, assuming the solution satisfies the constraints of the balanced partitioning. <p> In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], <ref> [Chr91] </ref>. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings. <p> We refer to these techniques as P fi Q. Variations of it have been proposed in [PAF90], and the Simulog's system. Another important class of heuristics included in this library are the so-called Orthogonal Recursive Bisection (ORB) techniques based on different 2-way partitioning heuristics. In <ref> [Chr91] </ref> a number of geometry based, mesh/grid heuristic algorithms have been implemented. Comparative performance analysis of these algorithms is also reported. They are listed in table 6.1. 6.1.1.4 Allocation Strategies In this section we review the problem of assigning processors to subdomains.
Reference: [CHR94] <author> N. P. Chrisochoides, E. N. Houstis, and J. R. Rice. </author> <title> "Mapping algorithms and software environment for Data Parallel PDE iterative Solvers". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21 </volume> <pages> 75-95, </pages> <year> 1994. </year>
Reference-contexts: Continuous DD is also possible and the approach is similar. The discrete DD has been extensively studied for PDE elliptic solvers and is regarded as the most suitable for such problems <ref> [CHR94] </ref>, [MQ88]. The basic idea is to decompose the grid or mesh of the PDE domain into subdomains. This results into splitting the discrete equations corresponding to the node or grid points of the subdomain and their interfaces (boundary). <p> i ! i 1 processor EXPLICIT H Munkres algorithm for equation (6.2) SUBD EXCH implicit algorithm for equations (6.2) or (6.3) GREEDY implicit algorithm for equations (6.2) or (6.3) 6.1.1.5 Performance Evaluation of Grid/Mesh Partitioning Heuristics A comparative extensive performance evaluation of partitioning algorithms for mesh/grid data is presented in <ref> [CHR94] </ref>. Here we summarize the results pertinent to load balancing. The following algorithms of Section 3 are included, which refer to discrete domain decomposition.
Reference: [CHS88] <author> M. Calzarossa, G. Haring, and G. Serrazi. </author> <title> "Workload Modeling for Computer Networks". </title> <editor> In U. Kastens and F. J. Ramming, editors, </editor> <booktitle> Architekture und Betrieb von Rechensystemen, </booktitle> <pages> pages 324-339. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference: [CJ91] <author> F. Cristian and F. Jahanian. </author> <title> "A timestamp-based checkpointing protocol for long-lived distributed computations". </title> <booktitle> In Proceedings of the 10th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 2-20, </pages> <year> 1991. </year>
Reference-contexts: In [SBY88], two enhancements to optimistic recovery are presented which allow volatile logging of messages without performing any I/O to stable storage; only those messages received from the outside world and a very small number of additional messages are stored on stable storage. <ref> [CJ91] </ref> presents a timestamp-based periodic checkpointing protocol in an environment in which processor clocks are synchronized within a known maximum deviation. A global state is defined in terms of the local states of the processes and the contents of the communication channels between them.
Reference: [CK79] <author> Y. C. Chow and W. Kohler. </author> <title> "Models for Dynamic Load Balancing in a Heterogeneous Multiple Processor System". </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28 </volume> <pages> 354-361, </pages> <year> 1979. </year>
Reference: [CK88] <author> T. Casavant and J. Kuhl. </author> <title> "A Taxonomy of Scheduling in General Purpose Distributed Computing Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(2) </volume> <pages> 141-154, </pages> <month> February </month> <year> 1988. </year>
Reference: [CL85] <author> K. M. Chandy and L. Lamport. </author> <title> "Distributed Snapshots: Determining Global States of Distributed Systems". </title> <journal> Transactions of the ACM, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year> <month> 113 </month>
Reference-contexts: The recovery of a single process should not introduce any incoherency in the execution of the other processes. Roll-back recovery guarantees that despite of the failure of some processes, the system stays in a coherent global state; a technique for determining global states in distributed systems is introduced in <ref> [CL85] </ref> and refined in [SK86]. Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. <p> Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. In <ref> [CL85, KT87, TKT89] </ref>, the nodes must periodically cooperate in computing a consistent global checkpoint. In [KT87], the checkpoint and roll-back recovery algorithms tolerate failures that occur during their execution.
Reference: [CLS90] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> "A Compiler-Assisted Approach to SPMD Execution". </title> <type> Technical Report TR-542-U170, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: COMPILER LAYER [SCvE91a] [SCvE91b], explore how multithreaded execution can be addressed as a compilation problem, to achieve switching rates approaching what hardware mechanisms might provide. Compiler-controlled multithreading is examined through compilation of a lenient parallel language, Id90, for a threaded abstract machine, TAM. Cytron et al, in <ref> [CLS90] </ref>, describe an automatic method for approaching the efficiency of SPMD-style execution for programs written in the more structured fork-join style. Their approach is to accept a fork-join program and automatically introduce SPMD regions in to the program where appropriate.
Reference: [CM89] <author> J.-D. Choi and B. P. Miller. </author> <title> "Code Generation and Separate Compilation in a Parallel Program Debugger". </title> <type> Technical Report TR-874, </type> <institution> Computer SciencesDepartment, University of Wisconsin-Madison, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: They relate synchronization objects with the data they protect. 6.2.9 Program Correctness/Debugging In parallel programs, nondeterminacy is a serious problem on testing program correctness and debugging. Choi and Miller, in <ref> [CM89] </ref>, are addressing the class of parallel programs that use explicit synchronization primitives, such as the semaphore, monitor, or Ada rendezvous and reduce the run time overhead of producing the debugger log by applying interprocedu-ral analysis and data flow analysis techniques commonly used in optimizing compilers. Page 90 6.2.
Reference: [CM94] <author> M. Calzarossa and L. Massari. </author> <title> "Measurement-Based Approach to Workload Characterization". </title> <editor> In R. Marie, G. Haring, and G. Kotsis, editors, </editor> <booktitle> Proc. of the 7th International Conference on Modeling Techniques and Tools for Computer Perfomance Evaluation, </booktitle> <pages> pages 123-147, </pages> <address> Vienna, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [CMM + 94] <author> M. Calzarossa, L. Massari, A. Merlo, M. Pantano, and D. Tessera. "MEDEA: </author> <title> A Tool for Workload Characterization of Parallel Systems (in preparation)", </title> <year> 1994. </year>
Reference: [CPR + 92] <author> M. Chereque, D. Powell, P. Reynier, J-L. Richier, and J. Voiron. </author> <title> "Active replication in Delta-4". </title> <booktitle> In Proceedings of the 22d International Symposium on Fault- tolerant Computing Systems (FTCS-22), </booktitle> <pages> pages 28-37, </pages> <address> Boston, MA, USA, July 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: SEQUENTIAL EXECUTION OF UNITS OF WORK fully synchronized. This technique makes it possible to realize a vote on the outputs in order to tolerate arbitrary failures <ref> [CPR + 92] </ref>. * passive replication Only one replicated process (the primary process) treats input messages and provides output messages.
Reference: [CS93] <author> M. Calzarossa and G. </author> <title> Serazzi. "Workload Characterization: A Survey". </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(8), </volume> <month> August </month> <year> 1993. </year>
Reference: [CSR88] <author> S. Cheng, J. A. Stankovic, and K. Ramamritham. </author> <title> "Scheduling Algorithms for Hard Real-Time Systems-A Brief Survey". </title> <editor> In J. A. Stankovic and K. Ramamritham, editors, </editor> <booktitle> "Tutorial on Hard Real-Time Systems", </booktitle> <pages> pages 150-173. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference: [CSS + 91] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> "Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine". </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 26(4) </volume> <pages> 164-175, </pages> <year> 1991. </year>
Reference-contexts: There are cases where all parallel threads have to synchronize their execution and proceed together. Thus, synchronization is an inherently sequential operation, that spares computing resources, and its optimization is important. Culler et al, in <ref> [CSS + 91] </ref>, present a relatively primitive execution model for fine-grain parallelism, in which all synchronization, scheduling, and storage management is explicit and under compiler control. This is defined by a threaded abstract machine (TAM) with a multilevel scheduling hierarchy. Considerable temporal locality of logically related threads is demonstrated.
Reference: [CT80] <author> G. Carpaneto and P. Toth. </author> <title> "Solution of the assignment problem [H]". </title> <journal> ACM Trans. Database Software, </journal> <volume> 6 </volume> <pages> 104-111, </pages> <year> 1980. </year>
Reference-contexts: The naive approach includes the RANDOM algorithm where the assignment is done at random and the SHIFT algorithm which assigns each subdomain D i to processor (i 1) of the G A graph. For the explicit solution of the optimization problem, several algorithms have been tested [HaN72], [Wei71], <ref> [CT80] </ref>, and [Wes83], the one selected in [CHH90] is called EXPLICIT H. In this heuristic c (D i ; D j ) models the interface length of the subdomains. Two implicit approaches based on subdomain exchange among processors and greedy procedures to achieve these goal are used in [Got81], [CHH90].
Reference: [DF82a] <author> D. Dowdy and D. Foste. </author> <title> "Comparative Models of the File Assignment Problem". </title> <journal> Computing Surveys, </journal> <volume> 14(2), </volume> <year> 1982. </year>
Reference: [DF82b] <author> L. Dowdy and D. Foster. </author> <title> "Comparative Models of the File Assignment Problem". </title> <journal> ACM Computing Surveys, </journal> <volume> 14, </volume> <month> June </month> <year> 1982. </year>
Reference: [DHW93] <author> C. Dwork, M. Herlihy, and O. Waarts. </author> <title> "Contention in Shared Memory Algorithms". </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 174-183, </pages> <month> May </month> <year> 1993. </year> <month> 114 </month>
Reference: [DO87] <author> F. Douglis and J. Ousterhout. </author> <title> "Process Migration in the Sprite Operat--ing System". </title> <booktitle> Proc. 7-th Int. Conf. on Distr. Comp. Syst., </booktitle> <pages> pages 18-27, </pages> <month> September </month> <year> 1987. </year>
Reference: [DO89a] <author> F. Douglis and J. Ousterhout. </author> <title> "Process Migration in Sprite: A Status Report". </title> <journal> IEEE Comp. Soc. Techn. Comm. on Oper. Syst. and Appl. Env. Newsletter, </journal> <volume> 3(1) </volume> <pages> 8-10, </pages> <month> winter </month> <year> 1989. </year>
Reference: [DO89b] <author> F. Douglis and J. Ousterhout. </author> <title> "Transparent Process Migration: Design Alternatives and the Sprite Implementation". </title> <journal> Software Practice and Experience, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: In these cases, load balancing is necessary to provide efficient use of idle cycles. Statistical measurements suggest that between 30% [TLC87] and 80% <ref> [DO89b] </ref> of all workstations in a typical environments are idle within a day, and almost 100% of workstations are idle during the night. During the time when several workstations are idle, some other workstations are overloaded running CPU-intensive applications like simulations, and compilations.
Reference: [Doe87] <author> T. W. Doeppner Jr. </author> <title> "Threads: A System for the Support of Concurrent Programming". </title> <type> Technical Report CS-87-11, </type> <institution> Department of Computer Science, Brown Univ., </institution> <year> 1987. </year>
Reference-contexts: In this section we survey user-level schedulers for thread libraries and run-time environments of parallelizing compilers. 6.4.1 Thread Libraries Most work on thread scheduling within an application has focused on the goal of load balancing alone. For example, in the process control scheme [TG89], Uniform System [TC88], Brown Threads <ref> [Doe87] </ref>, and Presto [BLLW88], all threads of the same application are placed in a FIFO central work queue. Processors take threads from this queue and run them to completion. The load is evenly balanced in that no processor remains idle as long as there is work to be done.
Reference: [Dou89] <author> F. Douglis. </author> <title> "Experience with Process Migration in Sprite". </title> <booktitle> Proceedings of the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </booktitle> <pages> pages 59-72, </pages> <month> October </month> <year> 1989. </year>
Reference: [Dou90] <author> F. Douglis. </author> <title> "Transparent Process Migration in the Sprite Operating System". </title> <type> Technical Report 24723, </type> <institution> Univ. of California - Berkeley, </institution> <month> September </month> <year> 1990. </year>
Reference: [DRP94] <author> V. Dixit-Radiya and D. Panda. </author> <title> "Clustering and Intra- Processor Scheduling for Explicitly-Parallel Programs on Distributed- Memory Systems". </title> <type> Technical Report 11, </type> <institution> Ohio State Univ., </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: This model recommends isolating the task of discovering what parts of the program can be done in parallel from the task of selecting a particular parallel execution. The source program is first validated and transformed by a classic front end. Dixit-Drdiya and Panda, in <ref> [DRP94] </ref>, show that the two common models of program representation, the precedence graph and the interaction graph models, are insufficient to capture the temporal behaviour of programs and hence are not ideal for solving the clustering and the scheduling problems.
Reference: [DSH93] <author> R. Das, J. Saltz, and R. V. Hanxleden. </author> <title> "Slicing Analysis and Indirect Access to Distributed Arrays". </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168, </pages> <year> 1993. </year>
Reference-contexts: Stichnoth et al, in [SOG94a], describe a practical method to compute the set of array elements that are to be moved. Das et al, in <ref> [DSH93] </ref>, examine the use of indirection arrays for indexing data arrays. Such irregular access patterns make it difficult for a compiler to generate efficient parallel code.
Reference: [DUSH93] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> "Communication Optimizations for Irregular Scientific Computations on Distributed Memory Ar-chitechtures". </title> <type> Technical Report UMIACS-TR-93-109, </type> <institution> University of Mary-land, Department of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: They formulate the reduction of inter-node communication as an optimization on a colored graph and write compiler pragmas to be used in the application program. Das et al, in <ref> [DUSH93] </ref>, give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance. They describe a number of optimizations that can be used to support the efficient execution of irregular problems on distributed memory parallel machines.
Reference: [EJZ92] <author> E. N. Elnozahy, D. B. Johnson, and W. Zwaenepoel. </author> <title> "The performance of consistent checkpointing". </title> <booktitle> In Proceedings of the 11th IEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <year> 1992. </year>
Reference-contexts: A global state is defined in terms of the local states of the processes and the contents of the communication channels between them. Process execution is supposed to be a deterministic function of process input messages. <ref> [EJZ92] </ref> presents a performance evaluation of consistent checkpointing and compares it with optimistic protocols. [EZ92b] describes the Manetho system which avoids synchronous checkpointing most of the time and uses sender-based message logging. 3. Checkpointing is pre-programmed in order to generate a set of checkpoints corresponding to a global consistent state.
Reference: [ELS88] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> "Process Management for Highly Parallel UNIX Systems". </title> <type> Technical Report 136, </type> <institution> Ultracomputer Research Laboratory, New York Univ., </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: Experimental results suggest that the two-minute warning may improve performance by a factor of three is some cases [MSLM91]. In the UltraComputer system <ref> [ELS88] </ref> processes share a do-not-preempt-me bit with the kernel. When the process is about to enter a critical section it sets the do-not-preempt-me bit, and resets it when it exits the critical section. When the scheduler descheduled a process, it checks the bit first.
Reference: [ELZ86] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> "Adaptive Load Sharing in Homogeneous Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(5):662-675, </volume> <month> May </month> <year> 1986. </year> <month> 115 </month>
Reference: [ELZ88] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> "The Limited Performance Benefits of Migrating Active Processes for Load Sharing". </title> <booktitle> Proceedings of the 1988 SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 63-72, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: There exist several environments where a good initial placement provides an acceptable balance of the load, so that migration for load balancing is not necessary, and sometimes, it is even harmful <ref> [ELZ88] </ref>. For example, in scientific and engineering applications most of the computation has the form of matrix operations. In these cases, each process is assigned a part of the matrix and is responsible for computing the values in its part alone.
Reference: [EM94] <author> R. Eigenmann and P. McClaughry. </author> <title> "Practical Tools for Optimizing Parallel Programs". </title> <type> Technical Report TR-1276, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1994. </year>
Reference-contexts: PTOPP focuses on the creation of a consistent set of experimental program variants and the interpretation of compilation and performance result. The interface is based on the EMACS editor environment. Eigen-mann and McClaughry, in <ref> [EM94] </ref>, describe a set of tools that help a programmer to be more efficient in optimizing scientific programs for a parallel computer. They have developed a number of utilities that complement available Unix tools.
Reference: [Enc87] <institution> Encore Computer Corporation. "Multimax Technical Summary", </institution> <year> 1987. </year>
Reference-contexts: Small scale bus-based shared-memory multiprocessors like the Sequent Symmetry and Balance [Seq85, Seq91], the Encore <ref> [Enc87] </ref>, and the Firefly workstation use a central ready queue. All ready processes created by all parallel and sequential applications are put in the same workqueue (ready queue). Idle processors take the first process from the queue and start executing it.
Reference: [EZ92a] <author> D. L. Eager and J. Zahorjan. </author> <title> "Adaptive Guided Self-Scheduling". </title> <type> Technical report, </type> <institution> UWASHCSD, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: If each iteration takes a short time to complete, then processors spend most of their time competing to take iterations from the work-queue, rather than executing iterations. Adaptive guided self-scheduling <ref> [EZ92a] </ref> addresses this problem by using a back-off method to reduce the number of processors competing for iterations during periods of contention.
Reference: [EZ92b] <author> E. N. Elnozahy and W. Zwaenepoel. "Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback and fast output Commit". </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5) </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Process execution is supposed to be a deterministic function of process input messages. [EJZ92] presents a performance evaluation of consistent checkpointing and compares it with optimistic protocols. <ref> [EZ92b] </ref> describes the Manetho system which avoids synchronous checkpointing most of the time and uses sender-based message logging. 3. Checkpointing is pre-programmed in order to generate a set of checkpoints corresponding to a global consistent state.
Reference: [Far88] <author> C. Farhat. </author> <title> "A simple and efficient automatic FEM domain decomposer". </title> <journal> Computers and Structures, </journal> <volume> 28 </volume> <pages> 579-602, </pages> <year> 1988. </year>
Reference-contexts: A more computationally expensive alternative is to use a stochastic optimization technique, which locates a global minimum "most" of the time. In [HKB90], simulated annealing and neural network techniques have been implemented for the mesh partitioning problem. Next, we give a brief review of these methodologies. Clustering Techniques Farhat <ref> [Far88] </ref> proposed a method for ordering the topological data of a mesh. <p> Rec. Bisection [Bop87] ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee <ref> [Far88] </ref> allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length between the two subdomains D i , D j or the communication requirements between them.
Reference: [Far89] <author> C. Farhat. </author> <title> "On the mapping of massively parallel processors onto finite element graphs". </title> <journal> Computers and Structures, </journal> <volume> 32 </volume> <pages> 347-353, </pages> <year> 1989. </year>
Reference-contexts: These algorithms are referred as SUBD-EXCH and GREEDY, respectively. Table 6.2 summarizes the allocation algorithms. Stochastic optimization techniques for the allocation problem have been considered explicitly or implicitly by several authors. A review of these methodologies is presented in <ref> [Far89] </ref>. Page 73 CHAPTER 6.
Reference: [FC93] <author> R. J. P. de Figueiredo and G. Chen. </author> <title> "Nonlinear Feedback Control Systems An Operator Theory Approach". </title> <address> Boston: </address> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. [BC85], [Bar93], <ref> [FC93] </ref>, [Isi89], [KK85], [SB89]). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations.
Reference: [Fer84] <author> D. Ferrari. </author> <booktitle> "On the Foundations of Artificial Workload Design". In Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 8-14, </pages> <year> 1984. </year>
Reference: [FERN84] <author> J. A. Fisher, J. R. Ellis, J. C. Ruttenberg, and A. Nicolau. </author> <title> "Parallel Processing: A Smart Compiler and a Dumb Machine". </title> <address> pages 37-47, </address> <year> 1984. </year>
Reference-contexts: The compiler tries to optimize any code that it processes, but some programs, mainly specific algorithms, have more parallelism in them, than the compiler can discover. Thus, some problems can have better parallelized solutions and are studied separately. Fisher et al, in <ref> [FERN84] </ref>, have developed a new fine-grained parallel architecture and a compiler that together offer order-of-magnitude speedups for ordinary scientific code. Singh et al, in [SHT + 92], study the partitioning and scheduling techniques required to obtain scalable parallel performance on a range of hierarchical N-body methods.
Reference: [FFL91] <author> C. Farhat, L. Fezoui, and S. Lanteri. </author> <title> "Computational fluid dynamics with irregular grids on the connection machine". </title> <type> Technical Report 1411, </type> <institution> INRIA, </institution> <address> Le Chesnay Cedex, France, </address> <year> 1991. </year>
Reference-contexts: PDEs are considered the fundamental tool for describing the physical behavior of many applications in science and engineering. The same methods can be easily extended to computations associated with numerical simulation and to more complicated mathematical models [Soc91], [NZZ94], <ref> [FFL91] </ref>. Mesh/grid related problems can be classified as single grid or multigrid depending on the approximation of the domain of computation.
Reference: [FK92] <author> R. J. Fowler and L. I. Kontothanassis. </author> <title> "Improving Processor and Cache Locality in Fine-Grain Parallel Computations Using Object-Affinity Scheduling and Continuation Passing (Revised)". </title> <type> TR 411, </type> <institution> URCSD, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Different organization models for memory, data, or even hardware/system software specific issues have been evaluated: Fowler and Kontothanassis, in <ref> [FK92] </ref>, claim that the locality issue in fine-grain parallel programs can be addressed effectively by using object affinity scheduling and that the overhead can be reduced substantially by representing tasks as templates that are managed using continuation-passing style mechanisms. Their experimental evidence used the Mercury system.
Reference: [FK94] <author> R. J. Fowler and L. I. Kontothanassis. </author> <title> "Mercury: Object-Affinity Scheduling and Continuation Passing on Multiprocessors". </title> <booktitle> In Parallel Languages and Architecture Europe (PARLE 94). </booktitle> <address> Athens., </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Once scheduled, Tetra provides a number of ways to analyze the program's performance under the specified processor model. These include parallelism profiles, storage demand profiles, data sharing distributions, data lifetime analysis, and control distance (branch, loop, and call stack) distributions. Fowler and Kontothanassis, in <ref> [FK94] </ref>, describe the implementation of Mercury, a system designed to explore methods for improving the performance of "natural grain" parallel object-oriented programs on shared memory multiprocessors with hardware-coherent caches, and present results on its performance. <p> PARALLEL EXECUTION OF UNITS OF WORK bringing the data it needs into the local cache. To avoid the cache-misses associated with naive thread scheduling decisions, recent thread libraries take communication costs into account. For example, U-threads [ML92], and Mercury <ref> [FK94] </ref> assigns threads for scheduling in processors close to their data. 6.4.2 Run-time Environments of Parallelizing Com pilers Loops are the largest source of parallelism in most applications.
Reference: [FKST92] <author> A. Feldmann, M. Y. Kao, J. Sgall, and S. H. Teng. </author> <title> "Optimal Online Scheduling of Parallel Jobs with Dependencies". </title> <type> Technical report, </type> <institution> Carnegie-Mellon Univ., </institution> <month> September </month> <year> 1992. </year> <month> 116 </month>
Reference-contexts: He also considers the code generation problem for message passing architectures. He proposes a new optimized method for code generation in executing a schedule of an arbitrary task graph based on an asynchronous communication model. Feldmann et al, in <ref> [FKST92] </ref>, consider compile-time static scheduling when communication overhead is not negligible. They provide a new quantitative analysis of granularity issues to identify the impact of partitioning on optimal scheduling.
Reference: [FLL93] <author> E. W. Felten, A. LaMarca, and R. Ladner. </author> <title> "Building Counting Networks from Larger Balancers". </title> <type> Technical Report 93-04-09, </type> <institution> Department of Computer Science and Engineering, Univ. of Washington, </institution> <month> April </month> <year> 1993. </year>
Reference: [FMS + 94] <author> R. Friedrich, J. Martinka, T. Sienknecht, M. Chelliah, and A. Ranganathan. </author> <title> "A Distributed Performance Measurement System for Large-Scale Heterogeneous Environments". </title> <type> Technical Report NSA-93-031, </type> <institution> Hewlett-Packard Company, </institution> <year> 1994. </year>
Reference: [FNGD92] <author> D. Ferguson, C. Nikolaou, L. Georgiadis, and K. Davies. </author> <title> "Goal Oriented, Adaptive Transaction Routing for High Performance Transaction Processing Systems". </title> <type> Technical Report Research Report RC18139, </type> <institution> IBM, </institution> <year> 1992. </year>
Reference: [FNGD93] <author> D. Ferguson, C. Nikolaou, L. Georgiadis, and K. Davies. </author> <title> "Goal Oriented, Adaptive Transaction Routing for High Performance Transaction Processing Systems". </title> <booktitle> In Proc. 2nd Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <address> San Diego, </address> <month> January </month> <year> 1993. </year>
Reference: [FNY89] <author> D. Ferguson, C. Nikolaou, and Y. Yemini. </author> <title> "Microeconomic Algorithms for Dynamic Load Balancing in Distributed Computer Systems". </title> <type> Technical Report Research Report RC15034, </type> <institution> IBM, </institution> <year> 1989. </year>
Reference-contexts: In this context, Page 104 unified resource management paradigms, like the feedback loop model of [WHMZ93a] [WHMZ93b] and the microeconomic model of <ref> [FNY89] </ref> [FNY92], may prove useful. On TP workload characterization In the area of workload characterization for OLTP systems, one could create a library of clustering algorithms for affinity clustering.
Reference: [FNY92] <author> D. Ferguson, C. Nikolaou, and Y. Yemini. </author> <title> "An Economy for Managing Replicated Data in Autonomous Distributed Systems". </title> <type> Technical Report Research Report RC18164, </type> <institution> IBM, </institution> <year> 1992. </year>
Reference-contexts: In this context, Page 104 unified resource management paradigms, like the feedback loop model of [WHMZ93a] [WHMZ93b] and the microeconomic model of [FNY89] <ref> [FNY92] </ref>, may prove useful. On TP workload characterization In the area of workload characterization for OLTP systems, one could create a library of clustering algorithms for affinity clustering.
Reference: [Fol92] <author> B. </author> <month> Folliot. </month> <institution> "Methodes et outils de partage de charge pour la conception et la mise en oeuvre d'applications dans les systemes repartis heterogenes". These de doctorat, MASI, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Compare with old values of f i for convergence 11. end. 5.4.4 The GATOSTAR system GATOSTAR is based on two independent systems: GATOS and STAR. GATOS is a load balancing manager which automatically distributes parallel applications among heterogeneous hosts <ref> [Fol92] </ref>. STAR is a fault-tolerant manager which automatically recovers processes of faulty machines [SF93]. GATOSTAR relies on a UNIX BSD (SunOs 4.1.1) operating system and works on a set of workstations connected by a local area network (Ethernet).
Reference: [FOS88] <author> J. Flower, S. Otto, and M. Salana. </author> <title> "Optimal mapping of irregular finite element domains to parallel processors". </title> <booktitle> Parallel Computers and Their Impact on Mechanics, </booktitle> <volume> 86 </volume> <pages> 239-250, </pages> <year> 1988. </year>
Reference-contexts: In general, it can be included in the W (m (D i )). One approach to solve the optimization problem is to approximate its objective function (6.1) by another function which is smoother, more robust and suitable for the existing optimization methods [Fox86] <ref> [FOS88] </ref>, [Wil90], [Man92]. A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], [CHH90], [Sim90]. <p> These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in <ref> [FOS88] </ref>, [Fox86] and [Wil90]. Hopfield neural networks [Hop82] constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem.
Reference: [Fox86] <author> G. C. Fox. </author> <title> "A review of automatic load balancing and decomposition methods for the hypercube". </title> <editor> In M. H. Schultz, editor, </editor> <booktitle> Proceedings of the IMA Institute, </booktitle> <pages> pages 63-76. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: In general, it can be included in the W (m (D i )). One approach to solve the optimization problem is to approximate its objective function (6.1) by another function which is smoother, more robust and suitable for the existing optimization methods <ref> [Fox86] </ref> [FOS88], [Wil90], [Man92]. A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], [CHH90], [Sim90]. <p> A recursive variation of this algorithm based on a modified 2-way Kernighan-Lin algorithm has been also developed [CHH + 89] and named GGP-rec; this heuristic is also Page 71 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK called orthogonal recursive bisection <ref> [Fox86] </ref>, [Wil90]. These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in [FOS88], [Fox86] and <p> bisection <ref> [Fox86] </ref>, [Wil90]. These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in [FOS88], [Fox86] and [Wil90]. Hopfield neural networks [Hop82] constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem.
Reference: [FR90a] <author> D. G. Feitelson and L. Rudolph. </author> <title> "Distributed Hierarchical Control for Parallel Processing". </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In a small scale system where all processors can be connected to the same interrupt line, this requirement can be easily satisfied. In a large scale system, such a physical line does not exist, and other scalable software methods must be used <ref> [FR90a, FR90b] </ref>. Summarizing, coscheduling was one of the first attempts to develop a scheduler that takes into account synchronization problems of parallel applications.
Reference: [FR90b] <author> D. G. Feitelson and L. Rudolph. </author> <title> "Mapping and Scheduling in a Shared Parallel Environment Using Distributed Hierarchical Control". </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I:1-8, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In a small scale system where all processors can be connected to the same interrupt line, this requirement can be easily satisfied. In a large scale system, such a physical line does not exist, and other scalable software methods must be used <ref> [FR90a, FR90b] </ref>. Summarizing, coscheduling was one of the first attempts to develop a scheduler that takes into account synchronization problems of parallel applications.
Reference: [Gal77] <author> R. G. Gallager. </author> <title> "A Minimum Delay Routing Algorithm Using Distributed Computation". </title> <journal> IEEE Transactions on Communications, </journal> <volume> 25 </volume> <pages> 73-85, </pages> <year> 1977. </year>
Reference: [Geo73] <author> A. George. </author> <title> "Nested dissection of a regular finite element mesh". </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 10(2) </volume> <pages> 345-363, </pages> <year> 1973. </year> <month> 117 </month>
Reference-contexts: Next, we give a brief review of these methodologies. Clustering Techniques Farhat [Far88] proposed a method for ordering the topological data of a mesh. The underlying idea of his scheme is equivalent to the well known Cuthill-McKee method of ordering, [LS76b], <ref> [Geo73] </ref>, [GL78], [GM78], as applied to order finite element meshes so that the corresponding linear algebraic system of equations has minimum bandwidth and profile. According to this technique one finds all the unlabeled neighbors of element (node) i and labels them in order of increasing connectivity (communication with neighbors).
Reference: [GFT86] <author> E. Gelenbe, D. Finkel, and S. K. Tripathi. </author> <title> "Availability of a distributed computer system with failures". </title> <journal> Acta Informatica, </journal> <volume> 23 </volume> <pages> 643-655, </pages> <year> 1986. </year>
Reference-contexts: They must then be combined with a fault-tolerant procedure consisting essentially of initializing and scheduling new replicated processes. 5.4.3 Performance evaluation of failure and reco very in a distributed environment In <ref> [GFT86] </ref>, a model for distributed systems with failing components is presented. Each node may fail and during its recovery the load is distributed to other nodes that are up. The model assumes periodic checkpointing for rollback-recovery and testing of the status of other nodes for the distribution of load.
Reference: [Gho93] <author> S. Ghosh. </author> <title> "Automatic Detection of Nondeterminacy and Scalar Optimization in Parallel Programs". </title> <type> Technical Report TR-1270, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: Page 90 6.2. COMPILER LAYER However, there is a trade-off between the trace size during execution and response time during debugging. Ghosh, in <ref> [Gho93] </ref>, focuses on techniques to automatically detect timing-dependent be-haviour. Since nondeterminacy is intimately related to the ordering between operations executed by the program, the main problem studied is that of computing ordering between operations. The ordering is determined by the synchronization operations used.
Reference: [GJ79] <author> M. R. Gary and D. S. Johnson. </author> <title> "Computers and Intractability, A Guide to the Theory of NP-Completeness". </title> <year> 1979. </year>
Reference-contexts: In the data partition problem formulation a min (max) criterion is used. The maximum sum of the computation plus communication is minimized for every processor assigned a certain subdomain and for all the possible assignments of subdomains to processors. The solution to this problem is NP-Complete <ref> [GJ79] </ref> and many heuristic methods have been proposed for finding good sub-optimal partitions of the data. These heuristics are divided into three classes, namely, data clustering, deterministic optimization and stochastic optimization. The allocation of subdomains to processors is also an NP-Complete problem.
Reference: [GL78] <author> A. George and J. W. Liu. </author> <title> "An automatic nested dissection algorithm for irregular finite element problems". </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 15(5) </volume> <pages> 1053-1069, </pages> <year> 1978. </year>
Reference-contexts: Next, we give a brief review of these methodologies. Clustering Techniques Farhat [Far88] proposed a method for ordering the topological data of a mesh. The underlying idea of his scheme is equivalent to the well known Cuthill-McKee method of ordering, [LS76b], [Geo73], <ref> [GL78] </ref>, [GM78], as applied to order finite element meshes so that the corresponding linear algebraic system of equations has minimum bandwidth and profile. According to this technique one finds all the unlabeled neighbors of element (node) i and labels them in order of increasing connectivity (communication with neighbors).
Reference: [GLJ76] <author> D. P. Gaver, S. S. Lavenberg, and T. G. Price Jr. </author> <title> "Exploratory Analysis of Access Path Length Data for a Data Base Management System". </title> <journal> IBM Journal on Research and Development, </journal> <volume> 20 </volume> <pages> 449-464, </pages> <year> 1976. </year>
Reference: [GLLK79] <author> R. L. Graham, E. L. Lawler, J. K. Lenstra, and A. H. G. Rinnoy Kan. </author> <title> "Optimization and Approximation in Deterministic Sequencing and Scheduling: a Survey". </title> <journal> Annals of Discrete Mathematics, </journal> <volume> 5 </volume> <pages> 287-326, </pages> <year> 1979. </year>
Reference: [GLR84] <author> C. Gao, J. W. S. Liu, and M. Railey. </author> <title> "Load Balancing Algorithms in Homogeneous Distributed Systems". </title> <booktitle> In IEEE Computer Society Silver Spring, MD, editor, Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 302-306, </pages> <year> 1984. </year>
Reference: [GLS93] <author> S. Graham, S. Lucco, and O. Sharp. </author> <title> "Orchestrating Interactions Among Parallel Computations". </title> <booktitle> Proceedings of the ACM SIGPLAN 93 Symposium on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Their graph incorporates the order on operations given by the program text, enabling us to do without locks even when database conflict graphs would suggest that locks are necessary. Their work offers new compiler optimization techniques for parallel languages that support shared variables. Graham et al, in <ref> [GLS93] </ref>, develop a methodology for managing the interactions among sub-computations, avoiding strict synchronization where concurrent or pipelined relationships are possible. They use symbolic data access analysis and adaptive runtime support.
Reference: [GM78] <author> A. George and D. R. McIntyre. </author> <title> "On the application of the minimum degree algorithm to finite element systems". </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 15(1) </volume> <pages> 1053-1069, </pages> <year> 1978. </year>
Reference-contexts: Next, we give a brief review of these methodologies. Clustering Techniques Farhat [Far88] proposed a method for ordering the topological data of a mesh. The underlying idea of his scheme is equivalent to the well known Cuthill-McKee method of ordering, [LS76b], [Geo73], [GL78], <ref> [GM78] </ref>, as applied to order finite element meshes so that the corresponding linear algebraic system of equations has minimum bandwidth and profile. According to this technique one finds all the unlabeled neighbors of element (node) i and labels them in order of increasing connectivity (communication with neighbors).
Reference: [GM80] <author> E. Gelenbe and I. Mitrani. </author> <title> "Analysis and Synthesis of Computer Systems". </title> <address> London New York: </address> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Observe that also S is a function of N . The corresponding equation for the macroscopic dynamics of the observables as for instance the overall response time R (t) for the units of work in the system at time t may be complicated (cf. <ref> [GM80] </ref>, 3.5). Only for non-preemptive and non-clairvoyant load balancing strategies it will depend on actual system state and control function value only, which is an indispensable assumption for this approach. The main problem encountered is the kind of smoothing, necessary to obtain differentiability of the above equation. <p> The main problem encountered is the kind of smoothing, necessary to obtain differentiability of the above equation. In heavy traffic, a diffusion approximation may be sensible. This approach is discussed for instance in <ref> [GM80] </ref>. Instead we can define N as the expected number of units of work waiting or being processed. But this requires an underlying stochastic model, which is discussed in the next section. 5.3.5.3 Stochastic control for the system states The above deterministic approach has several drawbacks.
Reference: [Gon91] <author> C. A. Q. Gonzalez. </author> <title> "Systematic Detection of Parallelism in Ordinary Programs". </title> <type> PhD thesis, </type> <institution> University of Rochester, Department of Computer Science, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Nicolau, in [Nic85], uses Percolation Scheduling, a new technique for compiling programs into parallel code based on rigorous definitions of the computational model and of the core transformations, to globally rearrange code past basic block boundaries. Gonzalez, in <ref> [Gon91] </ref>, provides a new model for compilers, and shows that the model's descriptive and prescriptive powers are similar to those of the standard model.
Reference: [Got81] <author> S. Goto. </author> <title> "An efficient algorithm for the two-dimensional placement problem in electrical circuit layout". </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> CAS-28, </volume> <year> 1981. </year>
Reference-contexts: In this heuristic c (D i ; D j ) models the interface length of the subdomains. Two implicit approaches based on subdomain exchange among processors and greedy procedures to achieve these goal are used in <ref> [Got81] </ref>, [CHH90]. These algorithms are referred as SUBD-EXCH and GREEDY, respectively. Table 6.2 summarizes the allocation algorithms. Stochastic optimization techniques for the allocation problem have been considered explicitly or implicitly by several authors. A review of these methodologies is presented in [Far89]. Page 73 CHAPTER 6.
Reference: [Gra66] <author> R. L. Graham. </author> <title> "Bounds for Certain Multiprocessing Anomalies". </title> <journal> Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <year> 1966. </year> <month> 118 </month>
Reference: [GRS93] <author> L. Gross, C. Roll, and W. Schoenauer. </author> <title> "VECFEM for Mixed Finite Ele--ments". </title> <type> Technical Report 50/93, </type> <institution> Rechenzentrum der Universitat Karlsruhe, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], <ref> [GRS93] </ref>, [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes.
Reference: [GTU91] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> "The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications". </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: If the application uses more processes, various overheads (including preemption inside a critical region) hurt performance. They implemented their approach on top of an Encore shared-memory multiprocessor and showed that it outperformed other time-sharing approaches [TG89, TTG92]. Subsequent work by the same authors <ref> [GTU91] </ref> verifies via simulation that space-sharing outperforms coscheduling and naive time-sharing. Crovella et. al. implemented a semi- dynamic space sharing policy on top of a BBN Butterfly plus parallel processor, and compare it to coscheduling and naive time-sharing [CDD + 91].
Reference: [HA93a] <author> W. L. Harrison and Z. Ammarguellat. </author> <title> "PARCEL and MIPRAC: Paral-lelizers for Symbolic and Numeric Programs". </title> <type> Technical Report TR-1222, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: Beckmann, in [Bec94], examines nonloop parallelism at both fine and coarse levels of granularity in numerical FORTRAN programs and explores the impact of fine grain functional parallelism on instruction-level architecture. Ludwell Harrison III and Ammarguellat, in <ref> [HA93a] </ref> [HA93b], present a compiler, Parcel, and run-time system that automatically parallelizes a Scheme program for execution on a shared-memory multiprocessor and Miprac, an interprocedural, parallelizing compiler that operates upon a semantically-oriented intermediate form, and attempts to fuse the techniques of Parcel (for symbolic programs) with those developed for Fortran (for
Reference: [HA93b] <author> W. L. Harrison and Z. Ammarguellat. </author> <title> "The Design of Automatic Paral-lelizers for Symbolic and Numeric Programs". </title> <type> Technical Report TR-1223, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: Beckmann, in [Bec94], examines nonloop parallelism at both fine and coarse levels of granularity in numerical FORTRAN programs and explores the impact of fine grain functional parallelism on instruction-level architecture. Ludwell Harrison III and Ammarguellat, in [HA93a] <ref> [HA93b] </ref>, present a compiler, Parcel, and run-time system that automatically parallelizes a Scheme program for execution on a shared-memory multiprocessor and Miprac, an interprocedural, parallelizing compiler that operates upon a semantically-oriented intermediate form, and attempts to fuse the techniques of Parcel (for symbolic programs) with those developed for Fortran (for numeric
Reference: [HACL89] <author> J. J. Hwang, F. D. Anger, Y. C. Chow, and C. Y. Lee. </author> <title> "Scheduling Precedence Graphs in Systems with Interprocessor Commu nication Times". </title> <journal> SIAM Journ. Comp., </journal> <volume> 18(2) </volume> <pages> 244-257, </pages> <month> April </month> <year> 1989. </year> <title> [HaN72] "A review of the placement and quadratic assignment problems". </title> <journal> SIAM Review, </journal> <volume> 14 </volume> <pages> 324-342, </pages> <year> 1972. </year>
Reference-contexts: We encounter it in every aspect of multiprocessor scheduling both within the operating system and in the run-time environment. Although the tradeoffs of the problem sound simple, the problem has been shown to be NP-complete. Thus, most research has focused on finding suboptimal solutions <ref> [Sar87, HACL89, LHCA88] </ref> on placement of tasks on processors.
Reference: [Har83] <author> G. Haring. </author> <title> "On Stochastic Models of Interactive Workloads". </title> <editor> In A. K. Agrawala and S. K. Tripathi, editors, </editor> <volume> Perfomance '83, </volume> <pages> pages 133-152. </pages> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference: [HCL90] <author> J. R. Haritsa, M. J. Carey, and M. Linvy. </author> <title> "Dynamic Real-Time Optimistic Concurrency Control". </title> <booktitle> In Prooceedings of the 11th Real-Time Systems Symposium, </booktitle> <month> December </month> <year> 1990. </year>
Reference: [HG92] <author> S. Hinrichs and T. Gross. </author> <title> "Utilizing new communication features in compilation for private-memory machines". </title> <booktitle> In Fifth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <month> August </month> <year> 1992. </year> <booktitle> Springer Lecture Notes on Computer Science. </booktitle>
Reference-contexts: These features allow a compiler to find innovative solutions when compiling data-parallel programs for a private-memory machine. Hinrichs and Gross, in <ref> [HG92] </ref>, discuss some optimizations that can be included in a compiler for the iWarp system, an example private-memory parallel system with a novel communication architecture.
Reference: [HHCL92] <author> K. Hadavi, W.-L. Hsu, T. Chen, and C.-N. Lee. </author> <title> "An Architecture for Real-Time Distributed Scheduling". </title> <journal> AI Magazine, </journal> <volume> 14(3) </volume> <pages> 46-56, </pages> <month> Fall </month> <year> 1992. </year>
Reference: [HKB90] <author> E. N. Houstis, S. K. Kortesis, and H. Byun. </author> <title> "A workload partitioning strategy for PDEs by a generalized neural network". </title> <type> Technical Report CSD-TR-934, </type> <institution> Purdue Univ., W. Lafayette, IN, </institution> <year> 1990. </year>
Reference-contexts: These Page 70 6.1. APPLICATION LAYER methods tend to terminate in some local minimum without always being able to move to a global minimum. A more computationally expensive alternative is to use a stochastic optimization technique, which locates a global minimum "most" of the time. In <ref> [HKB90] </ref>, simulated annealing and neural network techniques have been implemented for the mesh partitioning problem. Next, we give a brief review of these methodologies. Clustering Techniques Farhat [Far88] proposed a method for ordering the topological data of a mesh. <p> SA has been used in [FOS88], [Fox86] and [Wil90]. Hopfield neural networks [Hop82] constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem. In <ref> [HKB90] </ref> various artificial neural networks (ANN) have been considered for solving the partitioning phase of the mapping problem. In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90].
Reference: [HKK + 92] <author> R. V. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> "Compiler Analysis for Irregular Problems in Fortran-D". </title> <booktitle> In "Proceedings of 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1992. </year> <month> 119 </month>
Reference-contexts: They find that statement level parallelism does yield speedups on the shared memory multiprocessor. Li et al, in [LTSS93], introduce a new loop scheduling algorithm that takes data locality into consideration, and compare its performance with other well known scheduling algorithms. Hanxleden et al, in <ref> [HKK + 92] </ref>, compute global flow information about the communication characteristics of the loops around a flow graph, at compile time. The dataflow framework presented pertains to collections of loops with no loop-carried data dependencies except accumulation type dependencies (data-parallel loops).
Reference: [HKM93] <author> N. Hardavellas, D. Karakos, and M. Mavronicolas. </author> <title> "Notes on Sorting and Counting Networks". </title> <editor> In A. Schiper, editor, </editor> <booktitle> Proceedings of the 7th International Workshop on Distributed Algorithms (WDAG-93), Lecture Notes in Computer Science, </booktitle> <volume> volume 725, </volume> <pages> pages 234-248, </pages> <publisher> Springer-Verlag, </publisher> <address> Lausanne, Switzerland, </address> <month> September </month> <year> 1993. </year>
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> "Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines". </title> <type> Technical Report CRPC-TR91162, </type> <institution> Rice University, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The dataflow framework presented pertains to collections of loops with no loop-carried data dependencies except accumulation type dependencies (data-parallel loops). Their compiler determines when it is safe to reuse copies of off-processor data and identifies situations which allow run time preprocessing overheads to be amortized. Hiranandani et al, in <ref> [HKT91] </ref>, present compiler algorithms to automatically derive efficient message-passing programs based on data decompositions. Optimizations are presented to minimize load imbalance and communication costs for both loosely synchronous and pipelined loops.
Reference: [HL91] <author> S. Ha and E. A. Lee. </author> <title> "Compile-time scheduling and assignment of data-flow program graphs with data-dependent iteration". </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(11) </volume> <pages> 1225-1238, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: They present a mathematical framework that enables them to systematically derive the decompositions. Thread scheduling based on probability functions, emulations, or even on different thread models have also been a research issue: Ha and Lee, in <ref> [HL91] </ref>, propose scheduling techniques for static thread assignments, using data-flow graphs representing data-dependent iteration. They assume a known probability mass function for the number of cycles in the data-dependent iteration and show how a compile-time decision about assignment and/or ordering as well as timing can be made.
Reference: [HLS92] <author> M. Herlihy, B.-C. Lim, and N. Shavit. </author> <title> "Low Contention Load Balancing on Large-Scale Multiprocessors". </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 219-227, </pages> <month> July </month> <year> 1992. </year>
Reference: [Hop82] <author> J. </author> <title> Hopfield. "Neural networks and physical systems with emergent collective computational abilities". </title> <journal> In Proc. Natl. Acad. Sci., USA, </journal> <volume> volume 79, </volume> <pages> pages 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in [FOS88], [Fox86] and [Wil90]. Hopfield neural networks <ref> [Hop82] </ref> constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem.
Reference: [Hou90] <author> C. E. Houstis. </author> <title> "Module Allocation of Real-Time Applications to Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(7) </volume> <pages> 699-709, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: APPLICATION LAYER Table 6.1: Mesh Partitioning Algorithms Name Description 1 fi P 1-D strips P fi Q 2-D strips ORB-E Eigenvalue Ortho. Rec. Bisection [Bop87] ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net <ref> [Hou90] </ref> ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length between the two subdomains D i , D j or the
Reference: [HP93] <author> M. Haghighat and C. Polychronopoulos. </author> <title> "Symbolic Program Analysis and Optimization for Parallel Compilers". </title> <type> Technical Report TR-1237, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: The partial evaluator only leaves as residual those operations whose numerical input values will not be available until execution time. Partial evaluation was studied by Haghighat and Polychronopoulos, in <ref> [HP93] </ref>. They use symbolic analysis as an abstract interpretation technique to solve many of the flow analysis problems in a unified way. Some of these problems are constant propagation, global forward substitution, detection of loop in variant computations, and induction variable substitution.
Reference: [HRC + 90] <author> E. N. Houstis, J. R. Rice, N. P. Chrisochoides, H. C. Karathonases, P. N. Papachiou, M. K. Samartzis, E. A. Vavalis, K. Y. Wang, and S. Weer-awarana. </author> <title> "ELLPACK: A numerical simulation programming environment for parallel MIMD machines". </title> <booktitle> Supercomputing, </booktitle> <pages> pages 3-23, </pages> <year> 1990. </year>
Reference-contexts: In [HKB90] various artificial neural networks (ANN) have been considered for solving the partitioning phase of the mapping problem. In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system <ref> [HRC + 90] </ref>. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91].
Reference: [HS93] <author> S. Haldar and D. K. Subramanian. </author> <title> "an affinity-based dynamic load balancing protocol for distributed transaction processing systems". Performance Evaluation, </title> <booktitle> 17(1) </booktitle> <pages> 53-71, </pages> <month> January </month> <year> 1993. </year>
Reference: [HSF92] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> "Factoring: A Method for Scheduling Parallel Loops". </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In some cases guided-self scheduling might assign too much work to the first few processors, so that the remaining iterations are not sufficiently time-consuming to balance the workload. This situation arises when the initial iterations of a loop are much more time-consuming than later iterations. The factoring algorithm <ref> [HSF92] </ref> addresses this problem. Under factoring, allocation of loop iterations to processors proceeds in phases. During each phase, only a subset of the remaining loop iterations (usually half) is divided equally among the available processors.
Reference: [HSW91] <author> M. Herlihy, N. Shavit, and O. Waarts. </author> <title> "Low Contention Linearizable Counting Networks". </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 526-535, </pages> <month> October </month> <year> 1991. </year>
Reference: [IBM94] <author> IBM Corp. </author> <title> "Sysplex Overview Introducing Data Sharing and Parallelism in a Sysplex", </title> <address> gc28-1208-00 edition, </address> <month> April </month> <year> 1994. </year> <month> 120 </month>
Reference: [Isi89] <author> A. Isidori. </author> <title> "Nonlinear Control Systems". </title> <address> Berlin - Heidelberg New York: </address> <publisher> Springer, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. [BC85], [Bar93], [FC93], <ref> [Isi89] </ref>, [KK85], [SB89]). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations.
Reference: [JH89] <author> L. Johnston and C.-T. Ho. </author> <title> "Optimum broadcasting and personalized communication in hypercubes". </title> <journal> IEEE Trans. Comput., </journal> <volume> 38 </volume> <pages> 1248-1268, </pages> <year> 1989. </year>
Reference-contexts: The high performance of these computations on distributed memory MIMD machines, depends on the minimization of the local and global communication time and on the synchronization delays. Global communication/synchronization depends on the efficient hardware/software implementation of broadcast operations of parallel machines <ref> [JH89] </ref>, [SW90]. The main issue in most studies has been the minimization of local communication time per iteration. The local communication time depends both on data partitioning characteristics such as, interface length, degree of connectivity of the subdomains, and machine characteristics such as the interconnection network and routing.
Reference: [JZ90] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> "Recovery in Distributed Systems using Optimistic Message Logging and Checkpointing". </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 462-491, </pages> <year> 1990. </year>
Reference-contexts: When a processor fails, the processes have to find a set of the previous checkpoints representing a consistent system state. In order to avoid the domino effect, all the messages received since the last checkpoint was created must be logged. These messages are then replayed during the recovery <ref> [SY85, JZ90, SW89] </ref>; this involves that processes must be deterministic. Because there is no synchronization among computation, communication and check-pointing, optimistic recovery can tolerate the failure of an arbitrary number of processors. <p> Because there is no synchronization among computation, communication and check-pointing, optimistic recovery can tolerate the failure of an arbitrary number of processors. When failures are very rare, this technique yields better throughput and response time than other general recovery techniques [SY85]. In <ref> [JZ90] </ref>, the authors prove the existence of a unique maximal consistent global state requiring to undo the minimal amount of the computation performed before a failure, but this state is not easy to determine. In [BL88], processes take checkpoints independently.
Reference: [Kat94] <author> M. Katevenis. "Telegraphos: </author> <title> High-Speed Communication Architecture for Parallel and Distributed Computer Systems". </title> <type> Technical Report TR-123, </type> <address> FORTH-ICS Heraklion, Crete, Greece, </address> <month> May </month> <year> 1994. </year>
Reference: [KF84] <author> P. Krueger and R. Finkel. </author> <title> "An Adaptive Load Balancing Algorithm for a Multicomputer". </title> <type> Technical Report 539, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> April </month> <year> 1984. </year>
Reference: [KGV83] <author> S. Kirkpatrick, C. Gelatt, and M. Vecchi. </author> <title> "Optimization by simulated annealing". </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) <ref> [KGV83] </ref>. SA has been used in [FOS88], [Fox86] and [Wil90]. Hopfield neural networks [Hop82] constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem.
Reference: [KK85] <author> H. W. Knobloch and H. Kwakernaak. </author> <title> "Lineare Kontrolltheorie". </title> <address> Berlin - Heidelberg New York: </address> <publisher> Springer, </publisher> <year> 1985. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. [BC85], [Bar93], [FC93], [Isi89], <ref> [KK85] </ref>, [SB89]). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations. <p> FORMALIZATIONS by a differential equation involving x and u. In the case of a linear control system, X, Y and U are euclidean spaces and x, y and r depend linear on each other (cf. <ref> [KK85] </ref>). Then such a system can be written as @ x (t) = A (t)x (t) + B (t)u (t); with continuous matrix functions A, B, C and D of suitable dimensions. The first equation describes microscopic dynamics of the system state.
Reference: [KK92] <author> O. Kremien and J. Kramer. </author> <title> "Methodical Analysis of Adaptive Load Sharing Algorithms". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 747-759, </pages> <month> November </month> <year> 1992. </year>
Reference: [KKB91] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> "Generalized Unimodular Loop Transformations for Distributed Memory Multiprocessors". </title> <booktitle> In International Conference of Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The restructuring involves transforming and partitioning the loop structures and the data so as to improve parallelism, static and dynamic locality, and load balance. Kumar et al, in <ref> [KKB91] </ref>, present a generalized unimodular loop transformation as a simple, systematic and elegant method for partitioning the iteration spaces of nested loops for execution on distributed memory multiprocessors.
Reference: [KKB92] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> "Deriving Good Transformations for Mapping Nested Loops on Hierarchical Parallel Machines in Polynomial Time". </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: The parameters of interest are the parallelism factor, the load imbalance, and the volume of communication. Kumar et al, in <ref> [KKB92] </ref>, present a computationally efficient method for deriving the most appropriate transformation and mapping of a nested loop for a given hierarchical parallel machine. Page 81 CHAPTER 6.
Reference: [KL70] <author> B. W. Kernighan and S. Lin. </author> <title> "An efficient heuristic procedure for partitioning graphs". </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 291-307, </pages> <year> 1970. </year>
Reference-contexts: The mathematical foundations of the optimization problem are stated in [CHH90]. A domain decomposer tool [Chr91] has a library of various P-way algorithms for minimizing the above cost functions. It includes the well-known kernighan-Lin heuristic <ref> [KL70] </ref>, [CHH + 89] technique for minimizing the cut-cost of the mesh graph, assuming the solution satisfies the constraints of the balanced partitioning. The idea of this approach is to identify an improved feasible solution by interchanging elements among the subdomains that optimize a profit function. <p> The Page 72 6.1. APPLICATION LAYER Table 6.1: Mesh Partitioning Algorithms Name Description 1 fi P 1-D strips P fi Q 2-D strips ORB-E Eigenvalue Ortho. Rec. Bisection [Bop87] ORB-M Mass Center ORB [Wil90] ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB <ref> [KL70] </ref> ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length
Reference: [Kla92] <author> O. Klaassen. </author> <title> "Modeling Database Reference Behaviour". </title> <editor> In G. Balbo and G. Serrazi, editors, </editor> <booktitle> Computer Perfomance Evaluation: Modelling Techniques and Tools, </booktitle> <pages> pages 47-60. </pages> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference: [Kle75] <author> L. Kleinrock. </author> <title> "Queueing Systems I and II". </title> <address> York - London - Sydney: </address> <publisher> Wiley, </publisher> <year> 1975. </year>
Reference: [KM] <author> S. Kapidakis and M. Mavronicolas. </author> <title> "Load Balancing Networks". </title> <note> Submitted for publication. </note>
Reference: [KM94] <author> K. Kunchithapadam and B. P. Miller. </author> <title> "Optimizing Array Distributions in Data-Parallel Programs". </title> <booktitle> In 7th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <month> 121 </month>
Reference-contexts: The experimental results show that the performance of automatically produced code is comparable with that of hand-written code. Data scheduling with emphasis on communication minimization has also been studied: Kunchithapadam and Miller, in <ref> [KM94] </ref>, provide a set of recommendations for compiler writers that are needed to both write efficient programs and to build the next generation of tools for parallel systems.
Reference: [Knu73] <author> D. Knuth. </author> <title> "The Art of Computer Programming", chapter Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference: [KP92] <author> M. Klugerman and C. Plaxton. </author> <title> "Small-Depth Counting Networks". </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 417-428, </pages> <month> May </month> <year> 1992. </year>
Reference: [KP93] <author> W. Kelly and W. Pugh. </author> <title> "A Framework for Unifying Reordering Transformations". </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Standard array data dependence testing algorithms give information about the aliasing of array references. New frameworks have been developed, to approach the problem of loop optimization in a more general way. Kelly and Pugh, in <ref> [KP93] </ref> [KP94], have developed a framework for unifying iteration reordering transformations, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that all reordering transformation can be represented as a mapping from the Page 80 6.2.
Reference: [KP94] <author> W. Kelly and W. Pugh. </author> <title> "Finding Legal Reordering Transformations using Mappings". </title> <type> Technical Report CS-TR-3297, </type> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Standard array data dependence testing algorithms give information about the aliasing of array references. New frameworks have been developed, to approach the problem of loop optimization in a more general way. Kelly and Pugh, in [KP93] <ref> [KP94] </ref>, have developed a framework for unifying iteration reordering transformations, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that all reordering transformation can be represented as a mapping from the Page 80 6.2.
Reference: [KPR94] <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> "Code Generation for Multiple Mappings". </title> <type> Technical Report CS-TR-3317, </type> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: They also provide algorithms to test the legality of mappings, to align mappings and to generate optimized code for mappings. Kelly et al, in <ref> [KPR94] </ref>, describe their unified transformation framework, which is much more powerful than unimodular techniques because it allows separate mappings to be specified for each statement, and algorithms to generate efficient code for both the single mapping case and the multiple mapping case.
Reference: [KS92] <author> D. Kulkarni and M. Stumm. </author> <title> "Computational Alignment: A new, unified program transformation for local and global optimization". </title> <type> Technical Report CSRI Tech report 292, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: The primitive transformations are just some special instances. Transformations have been studied and proposed by many researchers. Kulkarni and Stumm, in <ref> [KS92] </ref>, present a new class of linear transformations, Computational Alignment, that extends traditional linear loop transformations along a new dimension, that is more powerful than the existing linear loop transformations and can perform local optimizations that were not possible with earlier techniques.
Reference: [KS93] <author> D. Kulkarni and M. Stumm. </author> <title> "Loop and Data Transformations: A tutorial". Internal document, a tutorial guide., </title> <year> 1993. </year>
Reference-contexts: Computational Alignment is a transformation of all the computations associated with a portion of the loop body in order to align them to other computations either in the same loop or in another loop. Kulkarni and Stumm, in <ref> [KS93] </ref>, present previous and ongoing work on loop and data transformations and motivate a unified framework to restructuring of a sequence of loops and data so as to execute efficiently on parallel machines with several levels of hierarchy.
Reference: [KT87] <author> R. Koo and S. Toueg. </author> <title> "Checkpointing and rollback recovery for distributed systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. In <ref> [CL85, KT87, TKT89] </ref>, the nodes must periodically cooperate in computing a consistent global checkpoint. In [KT87], the checkpoint and roll-back recovery algorithms tolerate failures that occur during their execution. <p> Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. In [CL85, KT87, TKT89], the nodes must periodically cooperate in computing a consistent global checkpoint. In <ref> [KT87] </ref>, the checkpoint and roll-back recovery algorithms tolerate failures that occur during their execution. The period of checkpointing is a critical issue; a small period will involve a lot of overhead, while the longer the period will be the more work will be lost following a failure.
Reference: [KW85] <author> C. P. Kruskal and A. Weiss. </author> <title> "Allocating Independent Subtasks on Parallel Processors". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(10) </volume> <pages> 1001-1016, </pages> <year> 1985. </year>
Reference-contexts: Unfortunately, this algorithm incurs significant synchronization overhead; each iteration requires atomic access to the central work queue. This synchronization overhead can quickly become a bottleneck in large-scale systems, or even in small-scale systems if the time to execute one iteration is small. Uniform-sized chunking <ref> [KW85] </ref> reduces synchronization overhead by having each processor take K iterations, instead of one. This algorithm amortizes the cost of each synchronization operation over the execution time of K iterations, resulting in less synchronization overhead.
Reference: [Lam90] <author> L. Lamport. </author> <title> "Parallel Compilation on a Tightly Coupled Multiprocessor". </title> <type> Technical Report DEC-SRC-27, </type> <institution> Digital Equipment Corporation, Systems Research Centre, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: PARALLEL EXECUTION OF UNITS OF WORK in the methods themselves require the techniques to be significantly enhanced for the latter. However, no single approach is successful for all applications of the hierarchical N-body principle: The radiosity method requires completely different techniques than the classical problems. Lamport, in <ref> [Lam90] </ref>, developed algorithms to implement both a monotonic and a cyclic multiple-word clock that is updated by one process and read by one or more other processes, using synchronization without mutual exclusion.
Reference: [LB88] <author> P-J. Leu and B. Bhargava. </author> <title> "Concurrent robust checkpointing and recovery in distributed systems". </title> <booktitle> In Proceedings of the 4th IEEE International Conference on Data Engineering, </booktitle> <pages> pages 154-163, </pages> <year> 1988. </year>
Reference-contexts: Some methods using this approach require suspending the application while the checkpoint computation takes place; this is not feasible with all applications. In <ref> [LB88] </ref> each process in the system can initiate the checkpoint/rollback algorithm and force a minimal number of processes to make new checkpoints or roll back. In pessimistic (i.e. synchronous) log-based methods, every message received is logged to stable storage before it is processed [BBG83, BBG + 89].
Reference: [LC94] <author> B. Lisper and J. F. Collard. </author> <title> "Extent Analysis of Data Fields". </title> <type> Technical Report 03, </type> <institution> Royal Inst Tech (Sweden), Teleinformatics, </institution> <year> 1994. </year>
Reference-contexts: They perform program slicing on the subscript expressions of the indirect array accesses. Lisper and Collard, in <ref> [LC94] </ref>, describe a framework for analysis of data fields defined as functions with explicit restrictions.
Reference: [LCD93] <author> D. Levine, D. Callaahan, and J. Dongarra. </author> <title> "A Comparative Study of Automatic Vectorizing Compilers". </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1223-1244, </pages> <year> 1993. </year>
Reference-contexts: They report the speedups of the Perfect Benchmarks T M codes that result from automatic parallelization. They have further measured the performance gains caused by individual restructuring techniques and examine reasons for the successes and failures of the transformations are discussed, and potential improvements. Levine et al, in <ref> [LCD93] </ref>, compare the capabilities of several commercially available, vectorizing Fortran compilers using a test suite of Fortran loops. They present the results of compiling and executing these loops on a variety of supercomputers, mini-supercomputers, and mainframes.
Reference: [LCR91] <author> H.-C. Lin, G.-M. Chiu, and C. Raghavendra. </author> <title> "Performance Study of Dynamic Load Balancing Policies for Distributed Systems with Service Interruptions". </title> <booktitle> In IEEE INFOCOM 91 Proceedings, </booktitle> <pages> pages 797-805, </pages> <year> 1991. </year> <month> 122 </month>
Reference: [Leu90] <author> S. </author> <title> Leutenegger. "Issues in Multiprogrammed Multiprocessor Scheduling". </title> <type> Technical Report 954, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1990. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: To avoid starvation, processes are periodically interrupted by the hardware clock, suspended and put back in the workqueue so that all processes have a chance to make forward progress. Although conceptually simple, the single ready queue approach has several limitations. Leutenegger <ref> [LV90, Leu90] </ref> observed that under the single ready queue approach, the more processes a program has, the larger portion of the processor's time it gets.
Reference: [LG87a] <author> S.-P. Lo and V. D. Gligor. </author> <title> "A Comparative Analysis of Multiprocessor Scheduling Algorithms". </title> <booktitle> Proc. 7-th Int. Conf. on Distr. Comp. Syst., </booktitle> <pages> pages 356-363, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: For example, processes of the same application communicate via critical sections, and synchronize using barriers. It has been shown that synchronization primitives interact with the scheduling policy used, and sometimes this interaction results in significant overhead <ref> [ZLE91, LG87a, LG87b] </ref>. For example, if a process is preemted while holding a lock, all other processes that want to get the lock will be forced to spin-wait for it, or suspend, and in any case, they will not be able to make forward progress.
Reference: [LG87b] <author> S.-P. Lo and V. D. Gligor. </author> <title> "Properties of Multiprocessor Scheduling Algorithms". </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: For example, processes of the same application communicate via critical sections, and synchronize using barriers. It has been shown that synchronization primitives interact with the scheduling policy used, and sometimes this interaction results in significant overhead <ref> [ZLE91, LG87a, LG87b] </ref>. For example, if a process is preemted while holding a lock, all other processes that want to get the lock will be forced to spin-wait for it, or suspend, and in any case, they will not be able to make forward progress.
Reference: [LHCA88] <author> C. Y. Lee, J. J. Hwang, Y. C. Chow, and F. D. </author> <title> Anger. "Multiprocessor Scheduling with Interprocessor Communication Delays". </title> <journal> Operations Research Letters, </journal> <volume> 7(3) </volume> <pages> 141-147, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We encounter it in every aspect of multiprocessor scheduling both within the operating system and in the run-time environment. Although the tradeoffs of the problem sound simple, the problem has been shown to be NP-complete. Thus, most research has focused on finding suboptimal solutions <ref> [Sar87, HACL89, LHCA88] </ref> on placement of tasks on processors.
Reference: [Lil94] <author> D. J. Lilja. </author> <title> "Exploiting the Parallelism Available in Loops". </title> <journal> IEEE Computer, </journal> <volume> 27(2) </volume> <pages> 13-26, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Therefore static scheduling often suffers from load imbalance. Lilja and Banerjee give an overview of the loop optimization issues. Lilja, in <ref> [Lil94] </ref>, explains scheduling techniques and compares results on different architectures, examining both fine-grained and coarse-grained parallel architectures. Banerjee et al, in [BENP93], present an overview of automatic program parallelization techniques.
Reference: [Lit87] <author> M. Litzkow. </author> <title> "Remote UNIX". </title> <booktitle> In Proceedings of the USENIX 1987 Summer Conference, </booktitle> <month> june </month> <year> 1987. </year>
Reference: [Liu86] <author> T. Liu. </author> <title> "Dynamic load balancing algorithm in homogeneous distributed systems". </title> <booktitle> In Proc. of the Sixth Int. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 216-222, </pages> <month> May </month> <year> 1986. </year>
Reference: [LJ90] <author> C. E. Love and H. F. Jordan. </author> <title> "An investigation of static versus dynamic scheduling". </title> <booktitle> In Proceedings of the 17th annual international symposium on computer architecture. </booktitle> <address> Seattle, Washington. </address> <note> May 28 - 31. (ISBN: 0 81862047 1) Also published as Computer Architecture News vol 18 (2) June, pages 192-201. </note> <institution> IEEE Computer Society, </institution> <address> Los Alamitos, </address> <year> 1990. </year>
Reference-contexts: There must be tools, methodologies and results that evaluate the possible parallelizations or existing architectures. Love and Jordan, in <ref> [LJ90] </ref>, investigate two techniques for instruction scheduling, dynamic Page 88 6.2. COMPILER LAYER and static scheduling. A decoupled access/execute architecture consists of an execution unit and a memory unit with separate program counters and separate instruction memories.
Reference: [LL73] <author> C. L. Liu and J. W. Layland. </author> <title> "Scheduling Algorithms for Multiprogramming in a Hard Real-Time Environment". </title> <journal> Journal of the ACM, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <year> 1973. </year>
Reference: [LL83] <author> L. Laning and M. Leonard. </author> <title> "File allocation in a Distributed Computer and Communication Network". </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32:232-244, </volume> <year> 1983. </year>
Reference: [LLM88] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka. "Condor A Hunter of Idle Workstations". </title> <booktitle> In Proc. 8-th Int. Conf. on Distr. Comp. Syst., </booktitle> <year> 1988. </year>
Reference: [LM82] <author> M. Livny and M. Melman. </author> <title> "Load balancing in homogeneous broadcast distributed systems". </title> <journal> ACM perf. eval. rev. (SIGMETRICS), </journal> <volume> 11 </volume> <pages> 47-55, </pages> <year> 1982. </year>
Reference: [LMP93] <author> G. Le Lann, P. Minet, and D. Powell. </author> <title> "Fault tolerance and distributed systems Concepts and mechanisms". </title> <institution> Rapport de recherche 2108, INRIA, </institution> <month> November </month> <year> 1993. </year> <month> 123 </month>
Reference-contexts: Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished <ref> [LMP93] </ref>. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. In [CL85, KT87, TKT89], the nodes must periodically cooperate in computing a consistent global checkpoint.
Reference: [Lo89] <author> V. M. Lo. </author> <title> "Process Migration for Communication Performance". </title> <journal> IEEET--COS, </journal> <volume> 3(1) </volume> <pages> 28-30, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: Multiprocessor performance is maximized when parallelism between tasks is optimally traded off with communication and synchronization overhead. Sarkar and Hennessy, in [SH86], present compile-time partitioning and scheduling techniques to achieve this trade-off. Lo, in <ref> [Lo89] </ref>, also investigates the static task assignment problem in distributed computing systems, and proposes heuristic algorithms that take into account interference costs. Yang, in [Yan93], proposes efficient algorithms and analyze their performances for automatic partitioning, scheduling and code generation.
Reference: [LOT94] <author> H. Lu, B. C. Ooi, and K. L. Tan. </author> <title> Query Processing in Parallel Relational Database Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference: [LP94] <author> W. Li and K. Pingali. </author> <title> "The Lambda Loop Transformation Toolkit (User's Reference Manual)". </title> <type> Technical Report TR 511, </type> <institution> URCSD, University of Rochester CS, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: A new approach is developed by Maslov, and by Li and Keshav. Maslov, in [Mas94], introduces the Global Value Propagation algorithm that unifies several manually applying automatable techniques. Global propagation is performed using a program abstraction called Value Flow Graph. Li and Keshav, in <ref> [LP94] </ref>, describe the Lambda loop transformation toolkit, an implementation of the non-singular matrix transformation theory, which can represent any linear one-to-one transformation, even compound transformations of the primitive transformations. The primitive transformations are just some special instances. Transformations have been studied and proposed by many researchers.
Reference: [LR92] <author> Y.-F. Lee and B. G. Ryder. </author> <title> "A Comprehensive Approach to Parallel Data Flow Analysis". </title> <booktitle> In Proceedings of the 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D. C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: He eliminates redundant interleavings and uses abstract interpretation. His results can be used in program optimization, memory management and race detection. As new approaches become more complex, some approaches that run in parallel are developed. Lee and Ryder, and Sgro and Ryder try such approaches. Lee and Ryder, in <ref> [LR92] </ref>, present a comprehensive approach to performing data flow analysis in parallel. They identify three types of parallelism inherent in the data flow solution process: independent-problem parallelism, separate-unit parallelism and algorithmic parallelism; and describe a unified framework to exploit them.
Reference: [LS76a] <author> P. A. Lewis and G. S. Shedler. </author> <title> "Statistical Analysis of Non-stationary Series of Events in a Data Base System". </title> <journal> IBM Journal on Research and Development, </journal> <volume> 20 </volume> <pages> 465-482, </pages> <year> 1976. </year>
Reference: [LS76b] <author> W. H. Liu and A. H. Sherman. </author> <title> "Comparative analysis for the Cuthill-McKee and the reverse Cuthill-McKee ordering algorithms for sparse matrices". </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13(2) </volume> <pages> 199-213, </pages> <year> 1976. </year>
Reference-contexts: Next, we give a brief review of these methodologies. Clustering Techniques Farhat [Far88] proposed a method for ordering the topological data of a mesh. The underlying idea of his scheme is equivalent to the well known Cuthill-McKee method of ordering, <ref> [LS76b] </ref>, [Geo73], [GL78], [GM78], as applied to order finite element meshes so that the corresponding linear algebraic system of equations has minimum bandwidth and profile. According to this technique one finds all the unlabeled neighbors of element (node) i and labels them in order of increasing connectivity (communication with neighbors).
Reference: [LSS89] <author> J. P. Lehoczky, L. Sha, and B. Sprunt. </author> <title> "Aperiodic Task Scheduling for Hard Real-Time Systems". </title> <journal> The Journal of Real-Time Systems, </journal> <volume> 1 </volume> <pages> 27-69, </pages> <year> 1989. </year>
Reference: [LT86] <author> K. J. Lee and D. Towsley. </author> <title> "A Comparision of Priority Based Decentralized Load Balancing Policies". In ACM Performance Evaluation REview: </title> <booktitle> Proc. Performance '86 and ACM SIGMETRICS 1986. </booktitle> <volume> Vol. 14, volume 14, </volume> <pages> pages 70-77, </pages> <year> 1986. </year>
Reference: [LTG90] <author> A. A. Lazar, A. Temple, and R. Gidron. </author> <title> "An Architecture for Integrated Networks that Guarantees Quality of Service". </title> <journal> International Journal of Digital and Analog Communication Systems, </journal> <volume> 3(2) </volume> <pages> 229-238, </pages> <month> June </month> <year> 1990. </year>
Reference: [LTSS93] <author> H. Li, S. Tandri, M. Stumm, and K. C. Sevcik. </author> <title> "Locality and Loop Scheduling on NUMA Multiprocessors". </title> <booktitle> PROC of the 1993 ICPP, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The experiments were performed on loops both with and without loop carried dependencies. They find that statement level parallelism does yield speedups on the shared memory multiprocessor. Li et al, in <ref> [LTSS93] </ref>, introduce a new loop scheduling algorithm that takes data locality into consideration, and compare its performance with other well known scheduling algorithms. Hanxleden et al, in [HKK + 92], compute global flow information about the communication characteristics of the loops around a flow graph, at compile time. <p> Li at. al. have applied a similar idea in the Hector NUMA multiprocessor: each iteration is assigned for execution on the processor where the data it uses have been allocated if load imbalance happens the iteration may be rescheduled <ref> [LTSS93] </ref>. 102 Chapter 7 Future Research Directions On Transaction Routing Several important issues have to be addressed for transaction routing in large-scale systems.
Reference: [Luc92] <author> S. Lucco. </author> <title> "A Dynamic Scheduling Method for Irregular Parallel Programs". </title> <booktitle> ACM SIGPLAN 92 CONF on Programming Language Design and Implementation, </booktitle> <pages> pages 200-211, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In addition, the synchronization overhead of factoring is not significantly greater than that of guided-self scheduling. Like the factoring algorithm, the tapering algorithm <ref> [Luc92] </ref> is designed for loops where the execution time of iterations varies in such a way as to cause load imbalance under guided-self scheduling. Tapering is used for irregular loops, where the execution time of iterations varies widely and unpredictably.
Reference: [LV90] <author> S. C. Leutenegger and M. K. Vernon. </author> <title> "The Performance of Multipro-grammed Multiprocessor Scheduling Policies". </title> <journal> Performance Evaluation Review, </journal> <volume> 18(1) </volume> <pages> 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To avoid starvation, processes are periodically interrupted by the hardware clock, suspended and put back in the workqueue so that all processes have a chance to make forward progress. Although conceptually simple, the single ready queue approach has several limitations. Leutenegger <ref> [LV90, Leu90] </ref> observed that under the single ready queue approach, the more processes a program has, the larger portion of the processor's time it gets.
Reference: [LY90] <author> A. Leff and P. S. Yu. </author> <title> "Dynamic Load Sharing in Distributed Database Systems with Information Lags". </title> <type> Technical Report Research Report RC15402, </type> <institution> IBM, </institution> <year> 1990. </year> <month> 124 </month>
Reference-contexts: With the exception of the adaptive regression-based models of <ref> [LY90] </ref>, [LYL88], all the routing strategies considered in this survey assume that a common front-end system maintains accurate state information about the processing nodes to which incoming transactions can be routed for execution. This system structure does not scale to a large number of processing nodes. <p> Moreover, the effect of information aging and inaccuracy has to be taken explicitly into account by the routing algorithms. In <ref> [LY90] </ref>, [LYL88] this is done in the context of regression-based models for adjusting an analytic estimate of transaction response time for 103 CHAPTER 7. FUTURE RESEARCH DIRECTIONS each possible routing decision, and simulations show that attempting to take into account state information aging improves the robustness of the routing algorithm.
Reference: [LY91] <author> A. Leff and P. S. Yu. </author> <title> "An Adaptive Strategy for Load Sharing in Distributed Database Environments with Information Lags". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 91-103, </pages> <month> September </month> <year> 1991. </year>
Reference: [LY92] <author> A. Leff and P. S. Yu. </author> <title> "A Comparison of Regression-Based Load Sharing Strategies for Distributed Database Environments". </title> <type> Technical Report Research Report RC17839, </type> <institution> IBM, </institution> <year> 1992. </year>
Reference: [LYL88] <author> A. Leff, P. S. Yu, and Y. H. Lee. </author> <title> "Adaptive Transaction Routing in a Heterogeneous Database Environment". </title> <type> Technical Report Research Report RC14114, </type> <institution> IBM, </institution> <year> 1988. </year>
Reference-contexts: With the exception of the adaptive regression-based models of [LY90], <ref> [LYL88] </ref>, all the routing strategies considered in this survey assume that a common front-end system maintains accurate state information about the processing nodes to which incoming transactions can be routed for execution. This system structure does not scale to a large number of processing nodes. <p> Moreover, the effect of information aging and inaccuracy has to be taken explicitly into account by the routing algorithms. In [LY90], <ref> [LYL88] </ref> this is done in the context of regression-based models for adjusting an analytic estimate of transaction response time for 103 CHAPTER 7. FUTURE RESEARCH DIRECTIONS each possible routing decision, and simulations show that attempting to take into account state information aging improves the robustness of the routing algorithm.
Reference: [Man92] <author> N. Mansour. </author> <title> "Physical optimization algorithms for mapping data to distributed-memory multiprocessors". </title> <type> PhD thesis, </type> <institution> Syracuse Univ., </institution> <year> 1992. </year>
Reference-contexts: In general, it can be included in the W (m (D i )). One approach to solve the optimization problem is to approximate its objective function (6.1) by another function which is smoother, more robust and suitable for the existing optimization methods [Fox86] [FOS88], [Wil90], <ref> [Man92] </ref>. A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], [CHH90], [Sim90].
Reference: [Mas93] <author> V. Maslov. </author> <title> "Lazy Array Data-Flow Dependence Analysis". </title> <type> Technical Report CS-TR-3110.1, </type> <institution> University of Maryland, College Park CS, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: In contrast, if there are no intervening writes, the references touch the same value and the dependence is called a value-based dependence. Value-based dependencies reflect true flow of values in a program unobscured by details of storing data in memory. Maslov, in <ref> [Mas93] </ref>, introduces a data-flow dependence analysis algorithm which exactly computes value-based dependence relations for program fragments in which all subscripts, loop bounds and IF conditions are affine. Memory-based dependencies often can be removed by program transformations such as array expansion and privatization.
Reference: [Mas94] <author> V. Maslov. </author> <title> "Global Value Propagation Through Value Flow Graph and Its Use in Dependence Analysis". </title> <type> Technical Report CS-TR-3310, </type> <institution> Dept. of Computer Science, Univ. ofMaryland, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Generating efficient code to enumerate the points in the new iteration space is much more difficult for the multiple mappings cases, and has not been adequately addressed before. A new approach is developed by Maslov, and by Li and Keshav. Maslov, in <ref> [Mas94] </ref>, introduces the Global Value Propagation algorithm that unifies several manually applying automatable techniques. Global propagation is performed using a program abstraction called Value Flow Graph.
Reference: [Mav94a] <author> M. Mavronikolas. </author> <title> "Balancing Networks: State-of-the-Art". </title> <booktitle> LYDIA ESPRIT III BRA Deliverable WP1/T1.1/D2, </booktitle> <month> December </month> <year> 1994. </year>
Reference: [Mav94b] <author> M. Mavronikolas. </author> <title> "Competitive Analysis of On-Line Algorithms". </title> <booktitle> LYDIA ESPRIT III BRA Deliverable WP1/T1.1/D5, </booktitle> <month> December </month> <year> 1994. </year>
Reference: [MB92] <author> D. Menasce and L. Barroso. </author> <title> "A methodology for performance evaluation of parallel applications on multiprocessors". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 1-14, </pages> <year> 1992. </year>
Reference: [MCD + 91] <author> E. Markatos, M. Crovella, P. Das, C. Dubnicki, and T. LeBlanc. </author> <title> "The Effects of Multiprogramming on Barrier Synchronization". </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: If some one of them is preemted by the scheduler, the other processes will probably reach the barrier, but they will not be able to proceed beyond the barrier, unless the preemted process starts running and reaches the barrier <ref> [MCD + 91] </ref>. Existing operating systems incorporate mechanisms to address the above problems. Psyche for example, [MSLM91, SLM + 90] incorporates a mechanism called the two-minute warning. When a process is about to be descheduled the kernel sends it an upcall called the two-minute warning.
Reference: [MD78] <author> A. K. Mok and M. L. Dertouzos. </author> <title> "Exploiting Unused Periodic Time for Aperiodic Service Using The Extended Priority Exchange Algorithm". </title> <booktitle> In Proceedings of the 7th Texas Conference in Computer Systems, </booktitle> <month> November </month> <year> 1978. </year>
Reference: [ME93] <author> McClaughry and P. Earl. </author> <title> "PTOPP A Practical Toolset for the Optimization of Parallel Programs". </title> <type> Technical Report TR-1225, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1993. </year>
Reference-contexts: The programmer may have enough knowledge to indicate the existing parallelism in his program. He can use directives to indicate optimizations to the compiler. McClaughry, in <ref> [ME93] </ref>, develops PTOPP, a set of tools that help a programmer to efficiently optimize scientific programs for a parallel computer. PTOPP focuses on the creation of a consistent set of experimental program variants and the interpretation of compilation and performance result. The interface is based on the EMACS editor environment.
Reference: [MFL + 91] <author> O. A. McBryan, P. O. Frederickson, J. Linden, A. Schueller, K. Solchenbach, K. Stueben, C. A. Thole, and U. Tzottenberg. </author> <title> "Multigrid methods on parallel computers a survey of recent developments". Impact Comput. </title> <journal> Sci. Eng., </journal> <volume> 3 </volume> <pages> 1-75, </pages> <year> 1991. </year> <month> 125 </month>
Reference-contexts: Mesh/grid related problems can be classified as single grid or multigrid depending on the approximation of the domain of computation. In general, multigrids are used for non smooth and complicated domains of computation, moreover, grid refinements may be necessary as an intermediate step to the PDE computation <ref> [MFL + 91] </ref>, [SSSP94]. We shall be reviewing single mesh/grid problems. The parallelism and load balancing methods of such problems can easily be adopted for multigrid problems. Most of these problems use the mesh/grid as a static data structure of the geometry of the domain.
Reference: [MHC94] <author> B. Miller, J. Hollingsworth, and M. Callaghan. </author> <title> "the Paradyn Parallel Per--formance Tools and pvm". </title> <type> Technical Report TR-1240, </type> <institution> University of Wis-consin - Madison, </institution> <year> 1994. </year>
Reference: [Mit91] <author> W. F. Mitchell. </author> <title> "Adaptive refinement for arbitrary finite element spaces with hierarchical bases". </title> <journal> Journal on Computational and Applied Math., </journal> <volume> 36 </volume> <pages> 65-78, </pages> <year> 1991. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], [MS81], <ref> [Mit91] </ref>. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings.
Reference: [ML] <author> E. Markatos and T. J. LeBlanc. </author> <title> "Locality-Based Scheduling in Shared-Memory Multiprocessors". In Albert Zomaya, editor, "Current and Future Trends in Parallel and Distributed Computing". World Scientific Publishing. </title> <note> To appear. </note>
Reference-contexts: By exploiting processor affinity, we can reduce the amount of communication required to execute a parallel loop, and thereby improve performance. Affinity Loop Scheduling is one loop scheduling policy that exploits the affinity an iteration may have for a particular processor <ref> [ML94, ML] </ref>. The idea behind Affinity Loop Scheduling is that iterations are assigned statically to processors (to reduce run-time overhead), but can be rescheduled when load imbalance happens. Affinity Loop Scheduling has been implemented in several shared-memory multiprocessors, and has shown performance improvements of up to a factor of three.
Reference: [ML87a] <author> M. W. Mutka and M. Livny. </author> <title> "Profiling Workstations' Available Capacity for Remote Execution". </title> <booktitle> In Performance, </booktitle> <year> 1987. </year>
Reference: [ML87b] <author> M. W. Mutka and M. Livny. </author> <title> "Scheduling Remote Processing Capacity in a Workstation-Processor Bank Network". </title> <booktitle> In Proc. 7-th Int. Conf. on Distr. Comp. Syst., </booktitle> <pages> pages 2-9, </pages> <year> 1987. </year>
Reference: [ML92] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> "Load Balancing Versus Locality Management in Shared-Memory Multiprocessors". </title> <booktitle> In PROC of the 1992 ICPP, pages I:258-267, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: PARALLEL EXECUTION OF UNITS OF WORK bringing the data it needs into the local cache. To avoid the cache-misses associated with naive thread scheduling decisions, recent thread libraries take communication costs into account. For example, U-threads <ref> [ML92] </ref>, and Mercury [FK94] assigns threads for scheduling in processors close to their data. 6.4.2 Run-time Environments of Parallelizing Com pilers Loops are the largest source of parallelism in most applications.
Reference: [ML94] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> "Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: By exploiting processor affinity, we can reduce the amount of communication required to execute a parallel loop, and thereby improve performance. Affinity Loop Scheduling is one loop scheduling policy that exploits the affinity an iteration may have for a particular processor <ref> [ML94, ML] </ref>. The idea behind Affinity Loop Scheduling is that iterations are assigned statically to processors (to reduce run-time overhead), but can be rescheduled when load imbalance happens. Affinity Loop Scheduling has been implemented in several shared-memory multiprocessors, and has shown performance improvements of up to a factor of three.
Reference: [MP94] <author> V. Maslov and W. Pugh. </author> <title> "Simplifying Polynomial Constraints Over Integers to Make Dependence Analysis More Precise". </title> <type> Technical Report CS-TR-3109.1, </type> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: He does not attack the problem of expanding the available parallelism to execute efficiently on a multiprocessor. Algorithms that can be used on constraints, or similar problems arising at loop optimization, have been developed. Maslov and Pugh, in <ref> [MP94] </ref>, introduce an algorithm which exactly and efficiently solves a class of polynomial constraints which arise in dependence testing. Another important application of their algorithm is to generate code for loop transformation known as symbolic blocking (tiling).
Reference: [MQ88] <author> L. D. Marini and A. Quarteroni. </author> <title> "An iterative procedure for Domain Decomposition Methods : A finite element approach". First International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <year> 1988. </year>
Reference-contexts: Continuous DD is also possible and the approach is similar. The discrete DD has been extensively studied for PDE elliptic solvers and is regarded as the most suitable for such problems [CHR94], <ref> [MQ88] </ref>. The basic idea is to decompose the grid or mesh of the PDE domain into subdomains. This results into splitting the discrete equations corresponding to the node or grid points of the subdomain and their interfaces (boundary).
Reference: [MS79] <author> N. K. Madsen and R. F. Sincovec. </author> <title> "ALGORITHM 540: PDECOL General Allocation Software for Partial Differential Equations". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 326-351, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], <ref> [MS79] </ref>, [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings.
Reference: [MS81] <author> D. K. Melgaard and R. F. Sincovec. </author> <title> "General software for two-dimensional nonlinear partial differential equations". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(1) </volume> <pages> 106-125, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], [SM75], [MS79], <ref> [MS81] </ref>, [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings.
Reference: [MSD93] <author> M. Mehta, V. Soloviev, and D. J. DeWitt. </author> <title> "Batch Scheduling in Parallel Database Systems". </title> <type> Technical Report TR 1147, </type> <institution> Computer Science Department, Univ. of Wisconsin-Madison, </institution> <month> April </month> <year> 1993. </year> <month> 126 </month>
Reference: [MSLM91] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos. </author> <title> "First-Class User-Level Threads". </title> <booktitle> In Proceedings 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Existing operating systems incorporate mechanisms to address the above problems. Psyche for example, <ref> [MSLM91, SLM + 90] </ref> incorporates a mechanism called the two-minute warning. When a process is about to be descheduled the kernel sends it an upcall called the two-minute warning. <p> When the process receives this upcall, knows that it is about to get preempted, so it should refrain from getting into a critical region, or do whatever cleanup is necessary. Experimental results suggest that the two-minute warning may improve performance by a factor of three is some cases <ref> [MSLM91] </ref>. In the UltraComputer system [ELS88] processes share a do-not-preempt-me bit with the kernel. When the process is about to enter a critical section it sets the do-not-preempt-me bit, and resets it when it exits the critical section. When the scheduler descheduled a process, it checks the bit first.
Reference: [MT94] <author> L. Mullin and S. Thibault. </author> <title> "A reduction semantics for array expressions: the PSI compiler". </title> <type> Technical Report CSC-94-05, </type> <institution> Computer Science Department, University of Missouri-Rolla, </institution> <year> 1994. </year>
Reference-contexts: They describe a simple and efficient construct (called the Pdo loop) that is included in an experimental HPF-like compiler for private-memory parallel systems. New models have been proposed, to make more transformations possible. Mullin and Thibault, in <ref> [MT94] </ref>, employ the Psi Calculus as a back end compiler to scientific languages, and describe their results. The reduction rules of the Psi Calculus, which describe how to simplify array expressions | the data structure most widely used in scientific computation | were recently automated in the Psi Compiler.
Reference: [MTS89] <author> R. Mirchandaney, D. Towsley, and J. Stankovic. </author> <title> "Adaptive Load Sharing in Hetrogeneous Systems". </title> <booktitle> In Proc. of the Ninth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 298-306, </pages> <month> June </month> <year> 1989. </year>
Reference: [Mut92] <author> M. W. </author> <title> Mutka. "Estimating Capacity For Sharing in a Privately Owned Workstation Environment". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(4) </volume> <pages> 319-328, </pages> <month> April </month> <year> 1992. </year>
Reference: [MVZ90] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> "A Dynamic Processor Allocation Policy for Multiprogrammed Shared Memory Multiprocessors". </title> <type> Technical Report 90-03-02, </type> <institution> Univ. of Washington, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: In such cases, allocating a fixed number of processors to each application would result in significant load imbalance, Page 98 6.4. RUN TIME LAYER because applications with decreasing parallelism will underutilize their processors, while applications with increasing parallelism could use some extra processors <ref> [ZM90, MVZ90] </ref>. 6.4 Run Time Layer Most process-based parallel systems are expensive, because they use the process as the unit of parallelism. However, the process as implemented by most operating systems is both the unit of parallelism and the unit of protection.
Reference: [NH81] <author> L. M. Ni and K. Hwang. </author> <title> "Optimal load balancing strategies for a multiple processor system". </title> <booktitle> In Proc. 1981 Int. Conf. on Parallel Processing, </booktitle> <pages> pages 352-, </pages> <year> 1981. </year>
Reference-contexts: In the most optimal case, this vector would specify indeed a decision by consisting of one 1 and 0 otherwise. The problem is solved up to now only for some special cases, cf. <ref> [NH81] </ref>, [NH85b] or [BK90]. We can apply this approach also in order to evaluate a given load balancing algorithm instead of finding an optimal one.
Reference: [NH85a] <author> L. M. Ni and K. Hwang. </author> <title> "Optimal Load Balancing in Homogeneous Broadcast Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(5):491-496, </volume> <month> May </month> <year> 1985. </year>
Reference: [NH85b] <author> L. M. Ni and K. Hwang. </author> <title> "Optimal load balancing strategies in a multiple processor system with many job classes". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11:491-496, </volume> <year> 1985. </year>
Reference-contexts: In the most optimal case, this vector would specify indeed a decision by consisting of one 1 and 0 otherwise. The problem is solved up to now only for some special cases, cf. [NH81], <ref> [NH85b] </ref> or [BK90]. We can apply this approach also in order to evaluate a given load balancing algorithm instead of finding an optimal one.
Reference: [Ni82] <author> L. Ni. </author> <title> "A Distributed Load Balancing Algorithm for Point to Point Computer Networks". </title> <booktitle> In Proc. of IEEE COMPCON, </booktitle> <pages> pages 116-123, </pages> <year> 1982. </year>
Reference: [Nic85] <author> A. Nicolau. </author> <title> "Percolation Scheduling: A Parallel Compilation Technique". </title> <type> Technical Report TR85-678, </type> <institution> Cornell University, Computer Science Department, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: They use computers interfaces that are rapid and accurate with the ability to move across many platforms and adapt quickly to change. Nicolau, in <ref> [Nic85] </ref>, uses Percolation Scheduling, a new technique for compiling programs into parallel code based on rigorous definitions of the computational model and of the core transformations, to globally rearrange code past basic block boundaries.
Reference: [Noo89] <author> J. Noonan. </author> <title> "Automated Service Level Management and its Supporting Technologies". </title> <journal> Mainframe Journal, </journal> <month> October </month> <year> 1989. </year>
Reference: [NXG85] <author> L. M. Ni, C. W. Xu, and T. B. Gendreau. </author> <title> "A Distributed Drafting Algorithm for Load Balancing". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11:1153-1161, </volume> <month> October </month> <year> 1985. </year>
Reference: [NZZ94] <author> S. T. Nguyen, B. J. Zook, and X. Zhang. </author> <title> "Distributed computation of electromagnetic scattering problems using finite-difference time-domain decompositions". </title> <booktitle> In IEEE International Symposium on High-Performance Distributed Computing, </booktitle> <publisher> IEEE CS Press, </publisher> <month> August </month> <year> 1994. </year> <month> 127 </month>
Reference-contexts: PDEs are considered the fundamental tool for describing the physical behavior of many applications in science and engineering. The same methods can be easily extended to computations associated with numerical simulation and to more complicated mathematical models [Soc91], <ref> [NZZ94] </ref>, [FFL91]. Mesh/grid related problems can be classified as single grid or multigrid depending on the approximation of the domain of computation.
Reference: [O'B93] <author> M. O'Boyle. </author> <title> "A Data Partioning Algorithm for Distributed Memory Compi--lation". </title> <type> Technical Report UMCS-93-7-1, </type> <institution> Department of Computer Science, University of Manchester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: He also presents several optimizations for common distributions and easily-recognized communication patterns. There are many approaches that try to do static data and thread placement at the same time, each of them having different characteristics: O'Boyle, in <ref> [O'B93] </ref>, proposes a compiler strategy for mapping FORTRAN programs onto distributed memory computers, to minimize overhead. The minimization of different costs will suggest different data and computation partitions. He describes an automatic data partition algorithm which is based on four different analysis techniques and a data partitioning decision is made.
Reference: [OSS80] <author> J. K. Ousterhout, D. A. Scelza, and P. S. Sindu. </author> <title> "Medusa An Experiment in Distributed Operating System Structure". </title> <journal> Communications of the ACM, </journal> <volume> 23 </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Moreover, if a process is preempted, all other processes of the same application are pre-emted. So, even if the preemted process holds a lock, no other process will ask for the lock. Coscheduling has been implemented in the Medusa distributed operating system <ref> [OSS80] </ref>, and in the Psyche multiprocessor operating system [CDD + 91]. Although coscheduling addresses the synchronization problems mentioned above, it may lead to processor un-derutilization. For example, suppose a 10-processor system, and two applications: the one has 10 processes while the other has 5 processes.
Reference: [Ous82] <author> J. K. Ousterhout. </author> <title> "Scheduling Techniques for Concurrent Systems". </title> <booktitle> In Proc. 2-nd Int. Conf. on Distr. Comp. Syst., </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: Then, the running process starts executing its own code and enters the critical section. Scheduler activations are based on the close cooperation of the compiler, the thread library and the operating system. Coscheduling is the first policy that presents a general solution to all synchronization problems <ref> [Ous82] </ref>. In coscheduling, either all processes of an application run at the same time, or none of them runs. Thus, when a process wants to interact with the other processes of the same application, they are all running, so the interaction can happen.
Reference: [Ouy90] <author> P. Ouyang. </author> <title> "Execution of Regular DO Loops on Asynchronous Multiprocessors". </title> <type> Technical Report TR-523, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: COMPILER LAYER tions, and parallelization of recursive routines. They also survey several experimental studies on the effectiveness of parallelizing compilers. Issues related to the possible parallelism are studied by Ouyang and Pieper. Ouyang, in <ref> [Ouy90] </ref>, studies the characteristics of Fortran DO loops on an asynchronous shared-memory multiprocessor which are related to the efficiency of the execution.
Reference: [PAF90] <author> C. Pommerell, M. Annaratone, and W. Fichtner. </author> <title> "A set of new mapping and coloring heuristics for distributed-memory parallel processors". </title> <booktitle> In Proceedings of Copper Mountain Conference on Iterative Methods, </booktitle> <volume> volume 4, </volume> <pages> pages 1-27, </pages> <year> 1990. </year>
Reference-contexts: This is referred as algorithm 1 fi P. A P fi Q-way partitioning is obtained by sorting the coordinates of the element centroid in x and y directions. We refer to these techniques as P fi Q. Variations of it have been proposed in <ref> [PAF90] </ref>, and the Simulog's system. Another important class of heuristics included in this library are the so-called Orthogonal Recursive Bisection (ORB) techniques based on different 2-way partitioning heuristics. In [Chr91] a number of geometry based, mesh/grid heuristic algorithms have been implemented. Comparative performance analysis of these algorithms is also reported.
Reference: [PBS88] <author> D. Powell, G. Bonn, and D. Seaton. </author> <title> "The Delta-4 Appproach to Dependability in Open Distributed Computing Systems". </title> <booktitle> In Proceedings of the 18th International Symposium on Fault -tolerant Computing Systems (FTCS-18), </booktitle> <pages> pages 246-251, </pages> <address> Tokyo, Japan, June 1988. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In the Delta-4 project <ref> [Pow91, PBS88] </ref>, three replication techniques have been studied and implemented. * active replication All the replicated processes treat all the input messages broadcasted to the group in a concurrent manner in order to keep their internal states Page 63 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK fully synchronized.
Reference: [PE94] <author> D. A. Padua and R. Eigenmann. </author> <title> "Polaris: A New Generation Parallelizing Compiler for MPP's". </title> <type> Technical Report TR-1306, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1994. </year>
Reference-contexts: Levine et al, in [LCD93], compare the capabilities of several commercially available, vectorizing Fortran compilers using a test suite of Fortran loops. They present the results of compiling and executing these loops on a variety of supercomputers, mini-supercomputers, and mainframes. Pandua et al, in <ref> [PE94] </ref>, develop an industrial-strength Fortran compiler for massively parallel processors with a global address space for different target machines, and discuss the compiler transformation they are implementing.
Reference: [Pie93] <author> K. L. Pieper. </author> <title> "Parallelizing Compilers: Implementation and Effectiveness". </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: He investigates the number of initial iterations, the maximum number of ready iterations at any instances during the execution, the maximum number of pending iterations at any instances during the execution, a hash function to disperse different pending iterations, and the parallel execution time. Pieper, in <ref> [Pie93] </ref>, explores the reasons why transformation techniques fail to uncover all of the parallelism, by investigating the parallelism that can be uncovered in a set of real programs and comparing this to what is available within the application.
Reference: [PK87] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> "Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers". </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Uniform-sized chunking has a greater potential for imbalance than self-scheduling however, as processors finish within K iterations of each other in the worst case. In addition, choosing an appropriate value for K is a difficult problem, which has been solved for limited cases only. Guided self-scheduling <ref> [PK87] </ref> is a dynamic algorithm that changes the size of chunks at run-time, allocating large chunks of iterations at Page 100 6.4. RUN TIME LAYER the beginning of a loop so as to reduce synchronization overhead, while allocating small chunks towards the end of the loop to balance the workload.
Reference: [PLC92] <author> H. Pang, M. Livny, and M. J. Carey. </author> <title> "Transaction Scheduling in Muli-class Real-Time Database Systems". </title> <type> Technical report, </type> <institution> Computer Sciences Department , Univ. of Wisconsin, </institution> <year> 1992. </year>
Reference: [PM83] <author> M. L. Powell and B. P. Miller. </author> <title> "Process Migration in DEMOS/MP". </title> <booktitle> In Proc. 6-th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 110-119, </pages> <month> November </month> <year> 1983. </year>
Reference: [Pow91] <author> D. Powell. "Delta-4: </author> <title> A Generic Architecture for Dependable Distributed Computing". </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1991. </year>
Reference-contexts: In the Delta-4 project <ref> [Pow91, PBS88] </ref>, three replication techniques have been studied and implemented. * active replication All the replicated processes treat all the input messages broadcasted to the group in a concurrent manner in order to keep their internal states Page 63 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK fully synchronized.
Reference: [PP93] <author> P. Petersen and D. Padua. </author> <title> "Machine-Independent Evaluation of Paralleliz-ing Compilers". </title> <type> Technical Report TR-1173, </type> <institution> Center for Supercomputing Research and Development (CSRD, </institution> <year> 1993. </year> <month> 128 </month>
Reference-contexts: Mercury uses new scheduling techniques based on object affinity coarse-grain and dataflow techniques to reduce the cost of scheduling. Most work on evaluation have been done on compiler evaluation. An interesting point is to find out what part of the existing parallelism has been discovered: Petersen and Padua, in <ref> [PP93] </ref>, present a method for measuring the degree of success of a compiler at extracting implicit parallelism, in a machine-independent way.
Reference: [PSC93] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> "Runtime-Compilation Tech--niques for Data Partitioning and Communication Schedule Reuse". </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370, </pages> <year> 1993. </year>
Reference-contexts: The minimization of different costs will suggest different data and computation partitions. He describes an automatic data partition algorithm which is based on four different analysis techniques and a data partitioning decision is made. Ponnusamy et al, in <ref> [PSC93] </ref>, describe new ideas by which HPF compiler can deal with irregular computations effectively. They define a user specified mapping procedure via a Page 85 CHAPTER 6.
Reference: [PT91] <author> M. Philippsen and W. F. Tichy. </author> <title> "Compiling for Massively Parallel Machines". </title> <editor> In Robert Giegerich and Susan L. Graham, editors, </editor> <title> "Code Generation | Concepts, Tools, Techniques", </title> <booktitle> Proceedings of the International Workshop on Code Generation, </booktitle> <address> Dagstuhl, Germany, </address> <month> 20-24 May </month> <year> 1991, </year> <booktitle> Workshops in Computing, </booktitle> <pages> pages 92-111. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Compilation of Id, an extended functional language requiring fine-grain synchronization, under this model yields performance approaching that of conventional languages on current uniprocessors. Philippsen and Tichy, in <ref> [PT91] </ref>, explore techniques for compiling high-level, explicitly-parallel languages for massively parallel machines. They present mechanisms for translating asynchronous as well as synchronous parallelism for both SIMD and MIMD machines.
Reference: [PU89] <author> D. Peleg and E. Upfal. </author> <title> "The Token Distribution Problem". </title> <journal> SIAM Journal on Computing, </journal> <volume> 18 </volume> <pages> 229-241, </pages> <year> 1989. </year>
Reference: [Pug93] <author> W. Pugh. </author> <title> "Definitions of Dependence Distance". </title> <type> Technical Report CS-TR-3192, </type> <institution> Dept. of ComputerScience, Univ. of Maryland, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: PARALLEL EXECUTION OF UNITS OF WORK asking. This is because the questions we really should be asking go beyond integer programming and require decision procedures for a subclass of Presburger formulas. Pugh, in <ref> [Pug93] </ref>, deals with the definitions of the data dependence distance, which is widely used to characterize data dependencies in advanced optimizing compilers.
Reference: [Pug94] <author> W. Pugh. </author> <title> "Counting Solutions to Presburger Formulas: How and Why". </title> <booktitle> In "Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: Maslov and Pugh, in [MP94], introduce an algorithm which exactly and efficiently solves a class of polynomial constraints which arise in dependence testing. Another important application of their algorithm is to generate code for loop transformation known as symbolic blocking (tiling). Pugh, in <ref> [Pug94] </ref>, describe methods that are able to count the number of integer solutions to selected free variables of a Presburger formula, or sum a polynomial over all integer solutions of selected free variables of a Presburger formula. This answer is given symbolically, in terms of symbolic constants.
Reference: [PW85] <author> G. J. Popek and B. J. Walker. </author> <title> "The LOCUS Distributed System". </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference: [PW92] <author> W. Pugh and D. Wonnacott. </author> <title> "Eliminating False Data Dependences using the Omega Test". </title> <booktitle> In ACM SIGPLAN PLDI'92 conference, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: This theory make it possible to apply an algorithmic approach to solving optimization goals. Much of the effort on loop optimization is on parallelizing loops with dependencies. Pugh and Wonnacott, in <ref> [PW92] </ref>, describe how to extend the Omega test so that it can answer queries and eliminate the false data dependencies. False dependencies arise because the questions asked are conservative approximations to the questions we really should be Page 79 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK asking.
Reference: [PW93] <author> W. Pugh and D. Wonnacott. </author> <title> "An Exact Method for Analysis of Value-based Array Data Dependences. </title> <booktitle> In "Proceedings of the Sixth Annual Workshop on rogramming Languages and Compilers for Parallel Computing", </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Memory-based dependencies often can be removed by program transformations such as array expansion and privatization. He focuses on value-based dependencies, which need to be computed to perform these transformations. Pugh and Wonnacot, in <ref> [PW93] </ref>, describe a technique for analysis of value-based array data dependencies that is exact over programs without control flow (other than loops) and non-linear references and compare their proposal with related work. Standard array data dependence testing algorithms give information about the aliasing of array references.
Reference: [PW94] <author> W. Pugh and D. Wonnacott. </author> <title> "Static Analysis of Upper and Lower Bounds on Dependences and Parallelism". </title> <type> Technical Report CS-TR-3250, </type> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: He identifies several potential definitions, all of which give the same answer for normalized loops. There are a number of subtleties involved in choosing between these definitions, and no one definition is suitable for all applications. Pugh and Wonnacot, in <ref> [PW94] </ref>, deal with static analysis of upper and lower bounds on dependencies and parallelism and propose a two step approach for the search for parallelism in sequential programs.
Reference: [Rah92] <author> E. Rahm. </author> <title> "A Framework for Workload Allocation in Distributed Transaction Systems". </title> <institution> Department of Computer Science, Univ. of Kaiserslautern, West Germany, </institution> <year> 1992. </year>
Reference: [Ram92] <author> K. Ramamritham. </author> <title> "Real-Time Databases". </title> <journal> International Journal of Distributed and Parallel Databases, </journal> <year> 1992. </year>
Reference: [Ran75] <author> B. Randell. </author> <title> "System structure for software fault tolerance". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(2) </volume> <pages> 220-232, </pages> <month> June </month> <year> 1975. </year>
Reference-contexts: Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect <ref> [Ran75] </ref> and works even if processes are not deterministic. In [CL85, KT87, TKT89], the nodes must periodically cooperate in computing a consistent global checkpoint. In [KT87], the checkpoint and roll-back recovery algorithms tolerate failures that occur during their execution. <p> This can be done by taking a local checkpoint each time a message is to be sent; this can generate a substantial overhead and degrade performance. Another solution is to use conversations as introduced in <ref> [Ran75] </ref>. A conversation provides recovery blocks for local communication. Within a conversation two or more processes may communicate freely between themselves, but may not communicate with any other processes. <p> When a process fails, it is necessary to redo all its interactions with the other ones since the last checkpoint; the domino effect <ref> [Ran75] </ref> is avoided by keeping all received messages and by detecting all retransmissions using timestamps. 66 Chapter 6 Parallel Execution of Units of Work The problem of balancing the load of a parallel system has been investigated from many different points of view.
Reference: [RB93] <author> S. Ramaswamy and P. Banerjee. </author> <title> "Processor Allocation and Scheduling of Macro Dataflow Graphs on Distributed Memory Multicomputers by the PARADIGM Compiler". </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, volume II Software, </booktitle> <pages> pages 134-138, </pages> <address> Boca Raton, FL, August 1993. </address> <publisher> CRC Press. </publisher> <pages> 129 </pages>
Reference-contexts: Programs from a variety of source languages, including Scheme, Common Lisp, C, and Fortran have a straightforward translation to Miprac's intermediate form. Ramaswamy and Banerjee, in <ref> [RB93] </ref>, deal with functional parallelism exploitation.
Reference: [RM93] <author> E. Rahm and R. Marek. </author> <title> "Analysis of dynamic load balancing strategies for parallel shared nothing database systems". </title> <type> Technical report, </type> <institution> Univ. of Kaiserslautern, Department of Computer Science, Kaiserslautern, Germany, </institution> <year> 1993. </year>
Reference: [RP94] <author> L. Rauchwerger and D.A. Padua. </author> <title> "The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privati-zation". </title> <type> Technical Report TR-1329, </type> <institution> Center for Supercomputing Research and Development (CSRD), </institution> <year> 1994. </year>
Reference-contexts: The computation of ordering can be done either at compile time, or execution time and later. He describes techniques for both. Static analysis is applicable to every input data set, but is approximate due to the incompleteness of information available at compile time. Rauchwerger and Padua, in <ref> [RP94] </ref>, developed a test, a technique for identifying fully parallel loops at run-time, and dynamically privatizing scalars and arrays. The test can also be used for debugging parallel programs.
Reference: [RR94a] <author> R. Riedl and L. Richter. </author> <title> "An Extended Formal Framework for Dicussion of Algorithms". </title> <booktitle> LYDIA ESPRIT III BRA Deliverable WP1/T1.1/D7. In preparation., </booktitle> <month> December </month> <year> 1994. </year>
Reference: [RR94b] <author> R. Riedl and L. Richter. </author> <title> "Load Characterization in Distributed Systems". </title> <booktitle> LYDIA ESPRIT III BRA Deliverable WP1/T1.1/D3. In preparation., </booktitle> <month> De-cember </month> <year> 1994. </year>
Reference: [RS84] <author> K. Ramamritham and J. A. Stankovic. </author> <title> "Dynamic Task Scheduling in Hard Real-Time Distributed Systems". </title> <journal> IEEE Software, </journal> <volume> 6 </volume> <pages> 27-69, </pages> <month> July </month> <year> 1984. </year>
Reference: [RS88] <author> P. Ramanathan and K. G. Singh. </author> <title> "Checkpointing and rollback recovery in a distributed system using common time base". </title> <booktitle> Proceedings of the IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 13-21, </pages> <year> 1988. </year>
Reference-contexts: This insures that the stable information across nodes is always consistent. However, this method slows down every step of the application computation, because of the synchronization needed between logging and processing of incoming messages. To decrease the overhead involved in checkpointing hardware mechanisms may be used. In <ref> [RS88] </ref> a common time base is provided through a hardware algorithm to synchronize the clocks of all the processors. 2. Processes create checkpoints asynchronously and independently of each other. When a processor fails, the processes have to find a set of the previous checkpoints representing a consistent system state.
Reference: [RSH79] <author> G. Rao, H. Stone, and T. Hu. </author> <title> "Assignment of tasks in a distributed processor system with limited memory". </title> <journal> IEEE Transactions on Computers, </journal> <volume> SE-28(4):291-298, </volume> <month> April </month> <year> 1979. </year>
Reference: [Rus91a] <author> V. F. Russo. </author> <title> "An Object-Oriented Operating System". </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: To alleviate this contention problem, distributed ready queues have been proposed. For example, in the Renaissance parallel operating system that has been implemented on top of the Encore shared-memory multiprocessor, the process-container abstraction is proposed <ref> [Rus91b, Rus91a] </ref>. Each process belongs to one process-container. Each processor can take ready processes to run from one process-container only. By controlling the mapping of processes and processors to process-containers, various scheduling policies can be implemented.
Reference: [Rus91b] <author> V. F. Russo. </author> <title> "Process Scheduling and Synchronization in the Renaissance Object-Oriented Multiprocessor Operating System". </title> <booktitle> In Proceedings of the Second Symp. on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 117-132, </pages> <address> Atlanta, GA, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: To alleviate this contention problem, distributed ready queues have been proposed. For example, in the Renaissance parallel operating system that has been implemented on top of the Encore shared-memory multiprocessor, the process-container abstraction is proposed <ref> [Rus91b, Rus91a] </ref>. Each process belongs to one process-container. Each processor can take ready processes to run from one process-container only. By controlling the mapping of processes and processors to process-containers, various scheduling policies can be implemented.
Reference: [RVH94] <author> S. V. Raghavan, D. Vasukiammaiyar, and G. Haring. </author> <title> "Generative Net-workload Models for a Single Server Environmena"t. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 118-127. </pages> <publisher> ACM Publications, </publisher> <year> 1994. </year>
Reference: [SAB + 94] <author> S. Sistare, D. Allen, R. Bowker, K. Jourdenais, J. Simons, and R. </author> <title> Title. "A Sclalable Debugger for Massively Parallel Message-Passing Programs". </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 50-56, </pages> <year> 1994. </year>
Reference: [SAD93] <author> M. Stonebraker, P. M. Aoki, and R. Devine. "Mariposa: </author> <title> A New Architecture for Distributed Data". </title> <type> Technical Report UCB//S2K-93-31, </type> <institution> Univ. of California at Berkeley, </institution> <year> 1993. </year> <month> 130 </month>
Reference: [Sar87] <author> V. Sarkar. </author> <title> "Partitioning and Scheduling for Execution on Multiprocessors". </title> <type> Technical report, </type> <institution> Stanford Univ., </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: We encounter it in every aspect of multiprocessor scheduling both within the operating system and in the run-time environment. Although the tradeoffs of the problem sound simple, the problem has been shown to be NP-complete. Thus, most research has focused on finding suboptimal solutions <ref> [Sar87, HACL89, LHCA88] </ref> on placement of tasks on processors.
Reference: [SB89] <author> S. Sastry and M. Bodson. </author> <title> "Adaptive Control Stability, Convergence, and Robustness". </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: For this approach, assertions on the macroscopic dynamics of the system (such as convergence behaviour, impulse reaction, sensitivity) are obtained from an analysis of the microscopic behaviour of the system, which is specified via a differential equation for suitable system states (cf. e. g. [BC85], [Bar93], [FC93], [Isi89], [KK85], <ref> [SB89] </ref>). Nevertheless our considerations are based to a large extend on the setup from Sect. 5.3.3 and 5.3.4.2, considering load balancing in a distributed system as a dynamic flow of units of work among service stations.
Reference: [SBY88] <author> R. E. Strom, D. F. Bacon, and S. A. Yemini. </author> <title> "Volatile logging in n-fault-tolerant distributed systems". </title> <booktitle> In Proceedings of the 18th IEEE Symposium on Fault Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <year> 1988. </year>
Reference-contexts: The second algorithm only requires adding extra information of size O (1) on each message, but requires O (n 3 ) messages per failure. The second algorithm essentially determines the maximal consistent global state and then each recovery process restores its application to this state. In <ref> [SBY88] </ref>, two enhancements to optimistic recovery are presented which allow volatile logging of messages without performing any I/O to stable storage; only those messages received from the outside world and a very small number of additional messages are stored on stable storage. [CJ91] presents a timestamp-based periodic checkpointing protocol in an
Reference: [Sch91] <author> K. E. Schauser. </author> <title> "Compiling Dataflow into Threads:Efficient Compiler-Controlled Multithreading for Lenient Parallel Languages". </title> <type> Technical report, </type> <institution> Univ. of California Berkeley, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: They assume a known probability mass function for the number of cycles in the data-dependent iteration and show how a compile-time decision about assignment and/or ordering as well as timing can be made. Powerful non-strict parallel languages require fast dynamic scheduling. Schauser, in <ref> [Sch91] </ref>, explores how the need for multithreaded execution can be addressed as a compilation problem, to achieve switching rates approaching what hardware mechanisms might provide. Compiler-controlled multithreading is examined through compilation of a lenient parallel language, for a threaded abstract machine, (TAM).
Reference: [SCvE91a] <author> K. E. Schauser, D. E. Culler, and T. von Eicken. </author> <title> "Compiler-Controlled Multithreading for Lenient Parallel Languages". </title> <booktitle> In Proceedings of the '5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, MA, </address> <pages> pages 50-72, </pages> <address> New York, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Tolerance to communication latency and inexpensive synchronization are critical for general-purpose computing on large multiprocessors. Fast dynamic scheduling is required for powerful non-strict parallel languages. However, machines that support rapid switching between multiple execution threads remain a design challenge. Schauser et al, in Page 86 6.2. COMPILER LAYER <ref> [SCvE91a] </ref> [SCvE91b], explore how multithreaded execution can be addressed as a compilation problem, to achieve switching rates approaching what hardware mechanisms might provide. Compiler-controlled multithreading is examined through compilation of a lenient parallel language, Id90, for a threaded abstract machine, TAM.
Reference: [SCvE91b] <author> K. E. Schauser, D. E. Culler, and T. von Eicken. </author> <title> "Compiler-Controlled Multithreading for Lenient Parallel Languages". </title> <type> Technical report, </type> <institution> Univ. of California Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Tolerance to communication latency and inexpensive synchronization are critical for general-purpose computing on large multiprocessors. Fast dynamic scheduling is required for powerful non-strict parallel languages. However, machines that support rapid switching between multiple execution threads remain a design challenge. Schauser et al, in Page 86 6.2. COMPILER LAYER [SCvE91a] <ref> [SCvE91b] </ref>, explore how multithreaded execution can be addressed as a compilation problem, to achieve switching rates approaching what hardware mechanisms might provide. Compiler-controlled multithreading is examined through compilation of a lenient parallel language, Id90, for a threaded abstract machine, TAM.
Reference: [SDK + 94] <author> M. Stonebraker, R. Devine, M. Kornacker, W. Litwin, A. Pfeffer, A. Sah, and C. Staelin. </author> <title> "An Economic Paradigm for Query Processing and Data Migration in Mariposa". </title> <type> Technical Report UCB//S2K-94-49, </type> <institution> Univ. of Cal-ifornia at Berkeley, </institution> <year> 1994. </year>
Reference: [Seq85] <institution> Sequent Computer Systems Inc. "Balance 8000 System", </institution> <year> 1985. </year>
Reference-contexts: Thus, process migration is much less expensive: the load can be done as efficiently as migrating a thread. 3 The first scheduling and load balancing policies in shared-memory multiprocessors were based on their uniprocessor predecessors. Small scale bus-based shared-memory multiprocessors like the Sequent Symmetry and Balance <ref> [Seq85, Seq91] </ref>, the Encore [Enc87], and the Firefly workstation use a central ready queue. All ready processes created by all parallel and sequential applications are put in the same workqueue (ready queue). Idle processors take the first process from the queue and start executing it.
Reference: [Seq91] <institution> Sequent Computer Systems Inc. "Symmetry Multiprocessor Architecture Overview", </institution> <year> 1991. </year>
Reference-contexts: Thus, process migration is much less expensive: the load can be done as efficiently as migrating a thread. 3 The first scheduling and load balancing policies in shared-memory multiprocessors were based on their uniprocessor predecessors. Small scale bus-based shared-memory multiprocessors like the Sequent Symmetry and Balance <ref> [Seq85, Seq91] </ref>, the Encore [Enc87], and the Firefly workstation use a central ready queue. All ready processes created by all parallel and sequential applications are put in the same workqueue (ready queue). Idle processors take the first process from the queue and start executing it.
Reference: [SF93] <author> P. Sens and B. Folliot. </author> <title> "START: A fault-tolerant system for distributed applications". </title> <institution> Rapport de recherche 93-73, MASI, Paris, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: GATOS is a load balancing manager which automatically distributes parallel applications among heterogeneous hosts [Fol92]. STAR is a fault-tolerant manager which automatically recovers processes of faulty machines <ref> [SF93] </ref>. GATOSTAR relies on a UNIX BSD (SunOs 4.1.1) operating system and works on a set of workstations connected by a local area network (Ethernet). When a failure occurs, the faulty component stops immediately working in a detectable way. A network failure is seen as a site failure. <p> When a failure occurs, the faulty component stops immediately working in a detectable way. A network failure is seen as a site failure. Host crash detection is performed by periodically checking hosts' activities <ref> [SF93] </ref>. This checking message is also used for load balancing and contains informations on the load, the available memory, and the disk and network activity. Each host checks its immediate successor in a logical ring.
Reference: [SG91] <author> E. D. S. E. Silva and M. Gerla. </author> <title> "Queueing Network Models for Load Balancing in Distributed Systems". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(1) </volume> <pages> 24-38, </pages> <month> May </month> <year> 1991. </year>
Reference: [SGZ93] <author> H. Sandhu, B. Gamsa, and S. Zhou. </author> <title> "The shared regions approach to software cache coherence on multiprocessors". </title> <booktitle> In Proceedings of the 1993 ACM SIGPLAN Symposium on Principles and Pranctice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: They present the shared virtual memory code generation, compare both approaches and gives first performance results. Sandhu et al, in <ref> [SGZ93] </ref>, discuss a technique for software cache coherence that is based up on the integration of a program-level abstraction for shared data with software cache management, on shared-memory multiprocessors.
Reference: [SH86] <author> V. Sarkar and J. Hennessy. </author> <title> "Compile-time Partitioning and Scheduling of Parallel Programs". </title> <booktitle> In SIGPlan '86 Symposium on Compiler Construction, </booktitle> <pages> pages 17-26, </pages> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <journal> acm, SIGPlan. </journal> <volume> 131 </volume>
Reference-contexts: Significant work has been done on thread scheduling with emphasis on communication minimization: Partitioning and scheduling techniques are necessary to implement parallel languages on multiprocessors. Multiprocessor performance is maximized when parallelism between tasks is optimally traded off with communication and synchronization overhead. Sarkar and Hennessy, in <ref> [SH86] </ref>, present compile-time partitioning and scheduling techniques to achieve this trade-off. Lo, in [Lo89], also investigates the static task assignment problem in distributed computing systems, and proposes heuristic algorithms that take into account interference costs.
Reference: [SHT + 92] <author> J. P. Singh, C. Holt, T. Totsuka, A. Gupta, and J. L. Hennessy. </author> <title> "Load Balancing and Data Locality in Hierarchical n-body Methods". </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford Univ., </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Thus, some problems can have better parallelized solutions and are studied separately. Fisher et al, in [FERN84], have developed a new fine-grained parallel architecture and a compiler that together offer order-of-magnitude speedups for ordinary scientific code. Singh et al, in <ref> [SHT + 92] </ref>, study the partitioning and scheduling techniques required to obtain scalable parallel performance on a range of hierarchical N-body methods. These methods are important because they are based on a fundamental physical insight and efficiently solve large-scale problems in a wide range of application domains.
Reference: [Sil93] <author> M.G. Sillitoe. "Btoo: </author> <title> Compilation for Parallel Targets". </title> <type> Technical Report TCU/SARC/1993/7, </type> <institution> City University, </institution> <address> London, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: PARALLEL EXECUTION OF UNITS OF WORK methods facilitate the parallelization of the analysis phase of a software transformation system, by enabling deeper semantic analyses to be accomplished more efficiently than if performed sequentially. Object-oriented programming models have been presented too. Sillitoe, in <ref> [Sil93] </ref>, presents a parallelizing compiler which extracts the concurrency inherent in block structured imperative programs, using a simple object-oriented programming model. He introduces the programming model and a language and tries to determine the potential parallelism, expressed explicitly in the method-storage graph.
Reference: [Sim90] <author> D. H. Simon. </author> <title> "Partitioning of unstructured problems for parallel processing". </title> <type> Technical Report RNR-91-008, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA 94035, </address> <year> 1990. </year>
Reference-contexts: A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], [CHH90], <ref> [Sim90] </ref>.
Reference: [SK86] <author> M. Spezialetti and P. Kearns. </author> <title> "Efficient distributed snapshots". </title> <booktitle> In Proceedings 6th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 382-388, </pages> <year> 1986. </year>
Reference-contexts: Roll-back recovery guarantees that despite of the failure of some processes, the system stays in a coherent global state; a technique for determining global states in distributed systems is introduced in [CL85] and refined in <ref> [SK86] </ref>. Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic.
Reference: [SL93] <author> M. S. Squillante and E. D. Lazowska. </author> <title> "Using Processor-Cache Affinity Information in Shared-Memory Multiprocessor Scheduling". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In the (rather rare) case where the single system queue is empty as well, the processor snoops on the queues of other processors and takes processes to execute from there. Squillante simulated his proposed policies and showed that affinity-preserving policies outperform the others <ref> [Squ90, SL93] </ref>. In the last five years it was realized that most scheduling overheads are related to the fact that different applications time-share the same processor. Thus, several researchers start investigating space-sharing policies: processors are divided among the applications in the system, so that no two applications share a processor. <p> The existence of memory that is not equidistant from all processors (such as local memory or a processor cache) implies that some processors are closer to the data required by an iteration than others. Loop iterations frequently have an affinity <ref> [SL93] </ref> for a particular processor | the one whose local memory or cache contains the required data. By exploiting processor affinity, we can reduce the amount of communication required to execute a parallel loop, and thereby improve performance.
Reference: [SLM + 90] <author> M. L. Scott, T. J. LeBlanc, B. D. Marsh, T. G. Becker, C. Dubnicki, E. P. Markatos, and N. G. Smithline. </author> <title> "Implementation Issues for the Pcyche Multiprocessor Operating System". </title> <booktitle> Computing Systems, </booktitle> <address> 3,(1):101-137, </address> <month> winter </month> <year> 1990. </year>
Reference-contexts: Existing operating systems incorporate mechanisms to address the above problems. Psyche for example, <ref> [MSLM91, SLM + 90] </ref> incorporates a mechanism called the two-minute warning. When a process is about to be descheduled the kernel sends it an upcall called the two-minute warning. <p> All threads run within the same address space (within the same process). Creation and scheduling of threads is sometimes being done by the kernel (with user-hints) [Bla90b], or completely more frequently, by a library running in user-space <ref> [SLM + 90] </ref>. Thread creation, destruction and scheduling are inexpensive operations.
Reference: [SM75] <author> R. F. Sincovec and N. K. Madsen. </author> <title> "Software for nonlinear partial differential equations". </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 1(3) </volume> <pages> 232-260, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], [SWS92], <ref> [SM75] </ref>, [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings.
Reference: [Smi81] <author> B. Smith. </author> <title> "Architecture and Applications of the HEP Computer System". </title> <booktitle> In Proceedings of the SPIE, Real-Time Signal Processing IV, </booktitle> <year> 1981. </year>
Reference-contexts: The simplest dynamic algorithm for scheduling loop iterations is called self-scheduling <ref> [Smi81, TY86] </ref>. In this algorithm, each processor repeatedly executes one iteration of the loop until all iterations are executed.
Reference: [Soc91] <author> D. G. Socha. </author> <title> "Supporting fine-grain computation on distributed memory parallel computers". </title> <type> Technical Report 91-07-01, </type> <institution> Univ. of Washington, </institution> <year> 1991. </year>
Reference-contexts: PDEs are considered the fundamental tool for describing the physical behavior of many applications in science and engineering. The same methods can be easily extended to computations associated with numerical simulation and to more complicated mathematical models <ref> [Soc91] </ref>, [NZZ94], [FFL91]. Mesh/grid related problems can be classified as single grid or multigrid depending on the approximation of the domain of computation.
Reference: [SOG94a] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> "Generating communication for array statements: Design, implementation, and evaluation". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <year> 1994. </year>
Reference-contexts: When generating code for such a data parallel program for a private memory parallel system, the compiler must determine when array elements must be moved from one processor to another. Stichnoth et al, in <ref> [SOG94a] </ref>, describe a practical method to compute the set of array elements that are to be moved. Das et al, in [DSH93], examine the use of indirection arrays for indexing data arrays. Such irregular access patterns make it difficult for a compiler to generate efficient parallel code.
Reference: [SOG + 94b] <author> J. Subhlok, D. O'Hallaron, T. Gross, P. Dinda, and J. Webb. </author> <title> "Communication and memory requirements as the basis for mapping task and data parallel programs". </title> <booktitle> In Proc. Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: Next, the model is analyzed to obtain a good final mapping of the program onto the processors of the parallel machine. Subhlok et al, in <ref> [SOG + 94b] </ref>, present a framework to isolate and examine the specific characteristics of programs that determine the performance for different mappings and examine the tradeoffs between various mappings for them and show how the framework is used to obtain efficient mappings.
Reference: [Squ90] <author> M. S. Squillante. </author> <title> "Issues in Shared-Memory Multiprocessor Scheduling: A Performance Evaluation". </title> <type> Technical Report 90-10-04, </type> <institution> University of Wash-ington Computer Science Department, </institution> <month> October </month> <year> 1990. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: In the (rather rare) case where the single system queue is empty as well, the processor snoops on the queues of other processors and takes processes to execute from there. Squillante simulated his proposed policies and showed that affinity-preserving policies outperform the others <ref> [Squ90, SL93] </ref>. In the last five years it was realized that most scheduling overheads are related to the fact that different applications time-share the same processor. Thus, several researchers start investigating space-sharing policies: processors are divided among the applications in the system, so that no two applications share a processor.
Reference: [SR94] <author> V. Sgro and B. G. Ryder. </author> <title> "Differences in Algorithmic Parallelism in Control Flow and Call Multigraphs". </title> <type> Technical Report TR-224, </type> <institution> Rutgers University, Department of Computer Science, </institution> <year> 1994. </year> <month> 132 </month>
Reference-contexts: In particular, they use the design and implementation of their parallel hybrid algorithms to demonstrate the exploitation of algorithmic parallelism. They expect their approach to allow additional and more precise static analyses to be obtained in shorter analysis time than previous approaches. Sgro and Ryder, in <ref> [SR94] </ref>, try to calculate the modification side effects problem for Fortran programs using their parallel hybrid techniques. Their parallel hybrid analysis Page 77 CHAPTER 6.
Reference: [SS88] <author> D. Shasha and M. Snir. </author> <title> "Efficient and Correct Execution of Parallel Pro--grams that Share Memory". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 282-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: They conclude with important architectural principles required of parallel computers to support efficient, compiled programs. Their discussion is based on the language Modula-2*, an extension of Modula-2 for writing highly parallel programs in a machine-independent, problem-oriented way. Shasha and Snir, in <ref> [SS88] </ref>, consider an optimization problem that arises in the execution of parallel programs on shared-memory multiple-instruction-stream, multiple-data-stream (MIMD) computers. A program on such machines consists of many sequential program segments, each executed by a single processor. These segments interact as they access shared variables.
Reference: [SSM85] <author> W. Schoenauer, E. Schnepf, and H. Mueller. </author> <title> "The Fidisol Program Package". </title> <type> Technical Report 27/85, </type> <institution> Rechenzentrum der Universitat Karlsruhe, </institution> <month> December </month> <year> 1985. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications <ref> [SSM85] </ref>, [GRS93], [SWS92], [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes.
Reference: [SSSP94] <author> B. Steckel, F. Schon, and W. Schroder-Preikschat. </author> <title> "Written input to the ERCIM PPN Workshop by Partner GMD". </title> <booktitle> In "Proc. of ERCIM, Parallel Processing Network Workshop", </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: In general, multigrids are used for non smooth and complicated domains of computation, moreover, grid refinements may be necessary as an intermediate step to the PDE computation [MFL + 91], <ref> [SSSP94] </ref>. We shall be reviewing single mesh/grid problems. The parallelism and load balancing methods of such problems can easily be adopted for multigrid problems. Most of these problems use the mesh/grid as a static data structure of the geometry of the domain.
Reference: [ST85] <author> D. Sleator and R. Tarjan. </author> <title> "Amortized Efficiency of List Update and Paging Rules". </title> <journal> Communications of the ACM, </journal> <volume> 28(2) </volume> <pages> 202-208, </pages> <month> May </month> <year> 1985. </year>
Reference: [Ste94] <author> C. Stedman. </author> <title> "Parallel Developments". </title> <booktitle> Computerworld, </booktitle> <pages> page 83, </pages> <month> March </month> <year> 1994. </year>
Reference: [Sti93] <author> J. Stichnoth. </author> <title> "Efficient compilation of array statements for private memory multicomputers". </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Their static analysis give information about the extent of a recursively defined data field, that can be used to preallocate the data fields and map them efficiently to distributed memory Stichnoth, in <ref> [Sti93] </ref>, derives the communication and the memory management required to evaluate array statements similar to those in Fortran 90. The data distribution is given in terms similar to those in Fortran D; he assumes that each dimension of each array is distributed in block-cyclic fashion.
Reference: [Sto77] <author> H. S. Stone. </author> <title> "Multiprocessor Scheduling with the aid of Network Flow Algorithms". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 32 </volume> <pages> 85-93, </pages> <month> January </month> <year> 1977. </year>
Reference: [Sub93] <author> J. Subhlok. </author> <title> "Automatic mapping of task and data parallel programs for efficient execution on multicomputers". </title> <type> Technical Report CMU-CS-93-21, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: They also recognize that it is possible to reuse previously computed results from inspectors. They present performance results for these mechanisms from a Fortran 90D compiler implementation. Subhlok, in <ref> [Sub93] </ref>, isolates and examines specific characteristics of executing programs that determine the performance for different mappings on a parallel machine, and present an automatic system to obtain good mappings.
Reference: [Sur93] <author> R. Surati. </author> <title> "A Parallelizing Compiler Based on Partial Evaluation". </title> <type> Technical Report 1377, </type> <institution> MIT, AI Lab, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Their approach to compilation is specifically tailored to produce efficient statically scheduled code for parallel architectures which suffer from serious interprocessor communication latency and bandwidth limitations. Surati, in <ref> [Sur93] </ref>, constructed a parallelizing compiler that utilizes partial evaluation to achieve efficient parallel object code from very high-level data independent source programs. New static scheduling techniques are used to utilize the fine-grained parallelism of the computations. The compiler maps the computation graph resulting from partial evaluation onto the Supercomputer Toolkit.
Reference: [SW89] <author> A. P. Sistla and J. L. Welch. </author> <title> "Efficient distributed recovery using message logging". </title> <booktitle> In Proceedings of the 8th ACM Symposium on the Principles of Distributed Computing, </booktitle> <pages> pages 223-238, </pages> <year> 1989. </year>
Reference-contexts: When a processor fails, the processes have to find a set of the previous checkpoints representing a consistent system state. In order to avoid the domino effect, all the messages received since the last checkpoint was created must be logged. These messages are then replayed during the recovery <ref> [SY85, JZ90, SW89] </ref>; this involves that processes must be deterministic. Because there is no synchronization among computation, communication and check-pointing, optimistic recovery can tolerate the failure of an arbitrary number of processors. <p> These checkpoints form the set of latest globally consistent checkpoints. [WF92] proposes an independent checkpointing scheme using optimistic message logging allowing nondeterministic execution. Moreover a checkpoint space reclamation algorithm is presented to reclaim all checkpoints which are not useful for any possible future recovery. Page 62 5.4. SURVIVABILITY In <ref> [SW89] </ref>, two algorithms are proposed. The first one requires adding extra information of size O (n) to each message, where n is the number of nodes. For each failure, O (n 2 ) messages are exchanged but no node rolls back more than once.
Reference: [SW90] <author> Q. Stout and B. Wagar. </author> <title> "Intensive hypercube communication". Parallel Distribut. </title> <journal> Comput., </journal> <volume> 10 </volume> <pages> 167-181, </pages> <year> 1990. </year>
Reference-contexts: The high performance of these computations on distributed memory MIMD machines, depends on the minimization of the local and global communication time and on the synchronization delays. Global communication/synchronization depends on the efficient hardware/software implementation of broadcast operations of parallel machines [JH89], <ref> [SW90] </ref>. The main issue in most studies has been the minimization of local communication time per iteration. The local communication time depends both on data partitioning characteristics such as, interface length, degree of connectivity of the subdomains, and machine characteristics such as the interconnection network and routing.
Reference: [SWS92] <author> M. Schmauder, R. Weiss, and W. Schoenauer. </author> <title> "The Cadsol Program Package". </title> <type> Technical Report 46/92, </type> <institution> Rechenzentrum der Universitat Karlsruhe, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: In [CHH90] a list of partitioning algorithms have been developed for the parallel ELLPACK system [HRC + 90]. In parallel ELLPACK all the existing public domain software for PDE solvers is included that support well defined mathematical models of multiple applications [SSM85], [GRS93], <ref> [SWS92] </ref>, [SM75], [MS79], [MS81], [Mit91]. The analysis and performance evaluation of these partitioning algorithms is reported in [CHH + 89], [CHH90], [Chr91]. Four basic types of heuristics have been implemented for partitioning meshes. They include 1 fi P strips, P fi Q lattices, and 2-way recursive bisection and P-way partitionings.
Reference: [SY85] <author> R. E. Strom and S. A. Yemini. </author> <title> "Optimistic recovery in distributed systems". </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: When a processor fails, the processes have to find a set of the previous checkpoints representing a consistent system state. In order to avoid the domino effect, all the messages received since the last checkpoint was created must be logged. These messages are then replayed during the recovery <ref> [SY85, JZ90, SW89] </ref>; this involves that processes must be deterministic. Because there is no synchronization among computation, communication and check-pointing, optimistic recovery can tolerate the failure of an arbitrary number of processors. <p> Because there is no synchronization among computation, communication and check-pointing, optimistic recovery can tolerate the failure of an arbitrary number of processors. When failures are very rare, this technique yields better throughput and response time than other general recovery techniques <ref> [SY85] </ref>. In [JZ90], the authors prove the existence of a unique maximal consistent global state requiring to undo the minimal amount of the computation performed before a failure, but this state is not easy to determine. In [BL88], processes take checkpoints independently.
Reference: [SY92] <author> L. Shu and M. Young. </author> <title> "Correctness Criteria and Concurrency Control for Real-Time Systems: A Survey". </title> <type> Technical Report SERC-TR-131-P, </type> <institution> Software Engineering Research Center, Department of Computer Science, Purdue Univ., </institution> <month> November </month> <year> 1992. </year> <month> 133 </month>
Reference: [SZ88] <author> J. Stankovic and W. Zhao. </author> <title> "On Real-Time Transactions". </title> <journal> ACM, SIGMOD Record, </journal> <volume> 17(1) </volume> <pages> 4-18, </pages> <year> 1988. </year>
Reference: [SZ94] <author> N. Shavit and A. Zemach. </author> <title> "Diffracting Trees". </title> <booktitle> In Proceedings of the 6th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1994. </year>
Reference: [SZM94] <author> C. Steketee, W. P. Zhu, and P. Moseley. </author> <title> "Implementation of Process Migration in Amoeba". </title> <booktitle> Proc. 14-th Int. Conf. on Distr. Comp. Syst., </booktitle> <month> June </month> <year> 1994. </year>
Reference: [TA94] <author> K. Tomko and S. Abraham. </author> <title> "Partitioning Regular Applications for Cache-Coherent Multiprocessors". </title> <type> Technical Report CSE-TR-206-94, </type> <institution> University of Michigan, EECS, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: The compiler will trade off extra degrees of parallelism to reduce or eliminate communication. Finally, the compiler generates code to manage the multiple address spaces and to communicate data across processors. Tomko and Abraham, in <ref> [TA94] </ref>, focus on a partitioning methodology to optimize application performance on cache-coherent multiprocessors. They give an algorithm for choosing block-cyclic partitions for scientific programs with regular data structures such as dense linear algebra applications and PDE solvers.
Reference: [Tak86] <author> H. Takagi. </author> <title> "Analysis of Polling Systems". </title> <booktitle> MIT-Press Series in Computer Systems, </booktitle> <year> 1986. </year>
Reference-contexts: Then the load requesting nodes polls among these queues according to a predefined order, and takes one unit of work from the first non-empty queue according a specified selection rule. For introductory concepts of polling systems, see e. g. <ref> [Tak86] </ref>. Page 55 CHAPTER 5.
Reference: [TB86] <author> A. Thomasian and P. </author> <title> Bay. "Analytic Queueing Network Models for Parallel Processing of Task Systems". </title> <journal> IEEE trans. on comp., </journal> <volume> C-35, 12 </volume> <pages> 1045-1054, </pages> <year> 1986. </year>
Reference: [TC88] <author> R. H. Thomas and W. Crowther. </author> <title> "The Uniform System: An Approach to Runtime Support for Large Scale Shared Memory Parallel Processors". </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this section we survey user-level schedulers for thread libraries and run-time environments of parallelizing compilers. 6.4.1 Thread Libraries Most work on thread scheduling within an application has focused on the goal of load balancing alone. For example, in the process control scheme [TG89], Uniform System <ref> [TC88] </ref>, Brown Threads [Doe87], and Presto [BLLW88], all threads of the same application are placed in a FIFO central work queue. Processors take threads from this queue and run them to completion.
Reference: [TG89] <author> A. Tucker and A. Gupta. </author> <title> "Process Control and Scheduling Issues for Mul-tiprogrammed Shared-Memory Multiprocessors". </title> <booktitle> Proceedings of the Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: If the application uses more processes, various overheads (including preemption inside a critical region) hurt performance. They implemented their approach on top of an Encore shared-memory multiprocessor and showed that it outperformed other time-sharing approaches <ref> [TG89, TTG92] </ref>. Subsequent work by the same authors [GTU91] verifies via simulation that space-sharing outperforms coscheduling and naive time-sharing. Crovella et. al. implemented a semi- dynamic space sharing policy on top of a BBN Butterfly plus parallel processor, and compare it to coscheduling and naive time-sharing [CDD + 91]. <p> In this section we survey user-level schedulers for thread libraries and run-time environments of parallelizing compilers. 6.4.1 Thread Libraries Most work on thread scheduling within an application has focused on the goal of load balancing alone. For example, in the process control scheme <ref> [TG89] </ref>, Uniform System [TC88], Brown Threads [Doe87], and Presto [BLLW88], all threads of the same application are placed in a FIFO central work queue. Processors take threads from this queue and run them to completion.
Reference: [The86] <author> M. Theimer. </author> <title> "Preemtable Remote Execution Facilities for Loosely-Coupled Distributed Systems". </title> <type> PhD thesis, </type> <institution> Stanford Univ., </institution> <year> 1986. </year>
Reference: [TKT89] <author> Z. Tong, R. Y. Kain, and W. T. Tsai. </author> <title> "A low overhead checkpointing and rollback recovery scheme for distributed recovery". </title> <booktitle> In Proceedings of the 8th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <year> 1989. </year>
Reference-contexts: Page 61 CHAPTER 5. SEQUENTIAL EXECUTION OF UNITS OF WORK Three different approaches may be distinguished [LMP93]. 1. Processes create checkpoints synchronously representing a consistent system state. This basically avoids the domino effect [Ran75] and works even if processes are not deterministic. In <ref> [CL85, KT87, TKT89] </ref>, the nodes must periodically cooperate in computing a consistent global checkpoint. In [KT87], the checkpoint and roll-back recovery algorithms tolerate failures that occur during their execution.
Reference: [TLC87] <author> M. Theimer, K. Lantz, and D. Cheriton. </author> <title> "Preemptable Remote Execution Facilities for the V-System". </title> <booktitle> In "Proceedings of the Eleventh ACM Symposium on Operating System Principles", </booktitle> <pages> pages 13-22, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: In these cases, load balancing is necessary to provide efficient use of idle cycles. Statistical measurements suggest that between 30% <ref> [TLC87] </ref> and 80% [DO89b] of all workstations in a typical environments are idle within a day, and almost 100% of workstations are idle during the night. During the time when several workstations are idle, some other workstations are overloaded running CPU-intensive applications like simulations, and compilations.
Reference: [TN93] <author> T. H. Tzen and L. M. Ni. </author> <title> "Trapezoid Self-Scheduling: A Practical Scheduling Scheme for Parallel Computers". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 87-98, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Although guided-self scheduling minimizes the number of synchronization operations needed to achieve perfect load balancing, the overhead of synchronization can become significant in large-scale systems with very expensive synchronization primitives. Trapezoid self-scheduling <ref> [TN93] </ref> tries to reduce the need for synchronization, while still maintaining a reasonable balance in load. This algorithm allocates large chunks of iterations to the first few processors, and successively smaller chunks to the last few processors.
Reference: [Tri82] <author> K. S. Trivedi. </author> <title> "Probability and Statistics with Reliability, Queueing, </title> <booktitle> and Computer Science Applications". </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference: [TT85] <author> A. N. Tantawi and D. Towsley. </author> <title> "Optimal Static Load Balancing In Dis--tributed Computer Systems". </title> <journal> Journal of the ACM, </journal> <volume> 32 </volume> <pages> 445-465, </pages> <month> April </month> <year> 1985. </year>
Reference: [TTG92] <author> J. Torrellas, A. Tucker, and A. Gupta. </author> <title> "Evaluating the Benefits of Cache-Affinity Scheduling in Shared-Memory Multiprocessors". </title> <type> Technical report, </type> <institution> Computer Systems Laboratory, Stanford Univ., </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: If the application uses more processes, various overheads (including preemption inside a critical region) hurt performance. They implemented their approach on top of an Encore shared-memory multiprocessor and showed that it outperformed other time-sharing approaches <ref> [TG89, TTG92] </ref>. Subsequent work by the same authors [GTU91] verifies via simulation that space-sharing outperforms coscheduling and naive time-sharing. Crovella et. al. implemented a semi- dynamic space sharing policy on top of a BBN Butterfly plus parallel processor, and compare it to coscheduling and naive time-sharing [CDD + 91].
Reference: [TY86] <author> P. Tang and P.-C. Yew. </author> <title> "Processor Self-Scheduling for Multiple Nested Parallel Loops". </title> <booktitle> In Proceedings 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The simplest dynamic algorithm for scheduling loop iterations is called self-scheduling <ref> [Smi81, TY86] </ref>. In this algorithm, each processor repeatedly executes one iteration of the loop until all iterations are executed.
Reference: [VH93] <author> R. Varadarajan and I. Hwang. </author> <title> "An efficient dynamic processor allocation algorithm for adaptive mesh applications". </title> <type> Technical Report ???, Univ. </type> <institution> of Florida, </institution> <year> 1993. </year>
Reference-contexts: When mesh/grid refinements are necessary because of the discretization error convergence during the PDE computation, dynamic (adaptive) methodologies become more appropriate. Such dynamic methodologies are the subject of recent research in this area [WH93], <ref> [VH93] </ref>.
Reference: [Wal88] <author> J. Walrand. </author> <title> "Introduction to Queueing Networks". </title> <publisher> Prentice Hall: </publisher> <address> Engle-wood Cliffs NJ, </address> <year> 1988. </year>
Reference: [Wan94] <author> M. Wand. </author> <title> "Compiler Correctness for Parallel Languages". </title> <type> Technical report, </type> <institution> College of Computer Science, Northeastern University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The test can also be used for debugging parallel programs. They discuss how the test can be inserted automatically by the compiler and outline a cost/performance analysis that can be performed to decide when to use the test. Wand, in <ref> [Wan94] </ref>, presents a paradigm for proving the correctness of compilers for languages with parallelism.
Reference: [Wei71] <author> R. Weil. </author> <title> "An Extension of the Munkres Algorithm for the Assignment Problem to Rectangular Matrices". </title> <journal> Communications of ACM, </journal> <volume> 14 </volume> <pages> 802-804, </pages> <year> 1971. </year>
Reference-contexts: The naive approach includes the RANDOM algorithm where the assignment is done at random and the SHIFT algorithm which assigns each subdomain D i to processor (i 1) of the G A graph. For the explicit solution of the optimization problem, several algorithms have been tested [HaN72], <ref> [Wei71] </ref>, [CT80], and [Wes83], the one selected in [CHH90] is called EXPLICIT H. In this heuristic c (D i ; D j ) models the interface length of the subdomains.
Reference: [Wes83] <author> D. H. West. </author> <title> "Approximate Solution of the Quadratic Assignment Problem". </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 9 </volume> <pages> 461-466, </pages> <year> 1983. </year>
Reference-contexts: For the explicit solution of the optimization problem, several algorithms have been tested [HaN72], [Wei71], [CT80], and <ref> [Wes83] </ref>, the one selected in [CHH90] is called EXPLICIT H. In this heuristic c (D i ; D j ) models the interface length of the subdomains. Two implicit approaches based on subdomain exchange among processors and greedy procedures to achieve these goal are used in [Got81], [CHH90].
Reference: [WF92] <author> Y-M. Wang and W. K. Fuchs. </author> <title> "Optimistic message logging for independent checkpointing in message-passing systems". </title> <booktitle> In Proceedings of the 11th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 147-154, </pages> <year> 1992. </year>
Reference-contexts: It first collects information about relevant message exchanges and uses it in the second phase to determine both the set of processes that must roll back and the set of checkpoints up to which rollback occur. These checkpoints form the set of latest globally consistent checkpoints. <ref> [WF92] </ref> proposes an independent checkpointing scheme using optimistic message logging allowing nondeterministic execution. Moreover a checkpoint space reclamation algorithm is presented to reclaim all checkpoints which are not useful for any possible future recovery. Page 62 5.4. SURVIVABILITY In [SW89], two algorithms are proposed.
Reference: [WH93] <author> P. Wu and E. N. Houstis. </author> <title> "Parallel dynamic mesh generation and domain decomposition". </title> <type> Technical Report CSD-TR-93-075, </type> <institution> Purdue Univ., W. Laf. </institution> <note> IN 47907, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: When mesh/grid refinements are necessary because of the discretization error convergence during the PDE computation, dynamic (adaptive) methodologies become more appropriate. Such dynamic methodologies are the subject of recent research in this area <ref> [WH93] </ref>, [VH93].
Reference: [WH94] <author> H. Wabnig and G. Haring. "PAPS: </author> <title> The Parallel Program Performance Prediction Toolset". </title> <editor> In R. Marie, G. Haring, and G. Kotsis, editors, </editor> <booktitle> Proc. of the 7th International Conference on Modeling Techniques and Tools for Computer Perfomance Evaluation, </booktitle> <volume> volume 794, </volume> <pages> pages 283-304, </pages> <address> Vienna, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [WHMZ93a] <author> G. Weikum, C. Hasse, A. Monkeberg, and P. Zabback. </author> <title> "The COMFORT Automatic Tuning Project". </title> <type> Technical report, </type> <institution> Department of Computer Science of the Swiss Federal Institute of Technology, </institution> <address> Zurich, Switzerland, </address> <year> 1993. </year>
Reference-contexts: In this context, Page 104 unified resource management paradigms, like the feedback loop model of <ref> [WHMZ93a] </ref> [WHMZ93b] and the microeconomic model of [FNY89] [FNY92], may prove useful. On TP workload characterization In the area of workload characterization for OLTP systems, one could create a library of clustering algorithms for affinity clustering.
Reference: [WHMZ93b] <author> G. Weikum, C. Hasse, A. Monkeberg, and P. Zabback. </author> <title> "The COMFORT Automatic Tuning Project". </title> <booktitle> In Proc. th Intl. Conf. on Management of Data. ACM, </booktitle> <year> 1993. </year> <month> 135 </month>
Reference-contexts: In this context, Page 104 unified resource management paradigms, like the feedback loop model of [WHMZ93a] <ref> [WHMZ93b] </ref> and the microeconomic model of [FNY89] [FNY92], may prove useful. On TP workload characterization In the area of workload characterization for OLTP systems, one could create a library of clustering algorithms for affinity clustering.
Reference: [Wil90] <author> R. D. Williams. </author> <title> "Performance of Dynamic Load Balancing Algorithms for Unstructured Mesh Calculations". </title> <type> Technical Report C3P913, </type> <institution> Concurrent Supercomputing Facility, California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: In general, it can be included in the W (m (D i )). One approach to solve the optimization problem is to approximate its objective function (6.1) by another function which is smoother, more robust and suitable for the existing optimization methods [Fox86] [FOS88], <ref> [Wil90] </ref>, [Man92]. A second approach is to split the optimization problem into two distinct phases corresponding to the partitioning and allocation of the mesh [CHH + 89], [CHH90], [Sim90]. <p> A recursive variation of this algorithm based on a modified 2-way Kernighan-Lin algorithm has been also developed [CHH + 89] and named GGP-rec; this heuristic is also Page 71 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK called orthogonal recursive bisection [Fox86], <ref> [Wil90] </ref>. These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in [FOS88], [Fox86] and [Wil90]. <p> <ref> [Wil90] </ref>. These algorithms require an appropriate initial feasible solution, which is selected by the user out of the set of predetermined initializations. Stochastic Optimization Techniques A significant advance in optimization was made in 1983 with the invention of simulated annealing (SA) [KGV83]. SA has been used in [FOS88], [Fox86] and [Wil90]. Hopfield neural networks [Hop82] constitute another avenue for solving discrete combinatorial problems. These networks involve many simple computing units (or artificial neurons) whose objective is to minimize an energy function associated with the optimization problem. <p> The Page 72 6.1. APPLICATION LAYER Table 6.1: Mesh Partitioning Algorithms Name Description 1 fi P 1-D strips P fi Q 2-D strips ORB-E Eigenvalue Ortho. Rec. Bisection [Bop87] ORB-M Mass Center ORB <ref> [Wil90] </ref> ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). <p> Rec. Bisection [Bop87] ORB-M Mass Center ORB <ref> [Wil90] </ref> ORB-I Inertia Axis ORB [1] ORB-KL Kernighan-Lin ORB [KL70] ORB-GGP Modified K-L ORB [CHH + 89] ORB-ANN Neural Net [Hou90] ORB-SA Simulated Annealing GGP P -way Geometric Graph Part [CHH + 89] SA P -way SA [Wil90] CM-Clustering Cuthill-Mckee [Far88] allocation phase follows the partition phase which satisfies objectives (i)-(iv). c in (6.2) is replaced by the interface length between the two subdomains D i , D j or the communication requirements between them.
Reference: [WL91] <author> M. E. Wolf and M. S. Lam. </author> <title> "A Loop Transformation Theory and An Algorithm to Maximize Parallelism". </title> <booktitle> In IEEE Transactions on Parallel and Distributed Systems. The Stanford SUIF Compiler Group, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Kumar et al, in [KKB92], present a computationally efficient method for deriving the most appropriate transformation and mapping of a nested loop for a given hierarchical parallel machine. Page 81 CHAPTER 6. PARALLEL EXECUTION OF UNITS OF WORK Wolf and Lam, in <ref> [WL91] </ref>, deal with the problem of which loop transformations, and in what order, should be applied to achieve a particular goal, such as maximizing parallelism or data locality. They present an efficient loop transformation algorithm based on a new approach to maximize the degree of parallelism in a loop nest.
Reference: [WM85] <author> Y. T. Wang and J. T. Morris. </author> <title> "Load Sharing in Distributed Systems". </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(3):204-217, </volume> <month> March </month> <year> 1985. </year>
Reference: [Wol89] <author> R. W. Wolff. </author> <title> "Stochastic Modeling and the Theory of Queues". </title> <address> Englewood Cliffs NJ, </address> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference: [Wol92] <author> M. E. Wolf. </author> <title> "Improving Locality and Parallelism in Nested Loops". </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: They present the algorithm and give an extensive example of loop bounds determination, hinting at a simplification possibility. Wolf, in <ref> [Wol92] </ref>, explores compiler improvement of memory hierarchy utilization and parallelism for array-based loop nests by combining the elegance of the matrix theory with the generality of general dependence vectors into a new theory of loop transformation. This theory make it possible to apply an algorithmic approach to solving optimization goals.
Reference: [Yan93] <author> T. Yang. </author> <title> "Scheduling and Code Generation for Parallel Architectures (Ph.D. </title> <type> Dissertation)". Technical Report DCS-TR-299, </type> <institution> Rutgers Univ., Laboratory for Computer Science Research, </institution> <address> New Brunswick, NJ, </address> <year> 1993. </year>
Reference-contexts: Sarkar and Hennessy, in [SH86], present compile-time partitioning and scheduling techniques to achieve this trade-off. Lo, in [Lo89], also investigates the static task assignment problem in distributed computing systems, and proposes heuristic algorithms that take into account interference costs. Yang, in <ref> [Yan93] </ref>, proposes efficient algorithms and analyze their performances for automatic partitioning, scheduling and code generation. He considers compile-time static scheduling when communication overhead is not negligible and introduce a new task or Page 83 CHAPTER 6.
Reference: [YBL85] <author> P. S. Yu, S. Balsamo, and Y. H. Lee. </author> <title> "Notes on Dynamic Load Sharing and Transaction Routing". </title> <type> Technical Report Research Report RC11537, </type> <institution> IBM, </institution> <year> 1985. </year>
Reference: [YBL88] <author> P. S. Yu, S. Balsamo, and Y. H. Lee. </author> <title> "Dynamic transactions routing in distributed database systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(9) </volume> <pages> 1307-1318, </pages> <month> September </month> <year> 1988. </year>
Reference: [YCDI85] <author> P. S. Yu, D. W. Cornell, D. M. Dias, and B. R. Iyer. </author> <title> "Analysis of Affinity-Based Routing in Multisystem Data Sharing". </title> <type> Technical Report Research Report RC11424, </type> <institution> IBM, </institution> <year> 1985. </year>
Reference: [YCDI86] <author> P. S. Yu, D. W. Cornell, D. M. Dias, and B. R. Iyer. </author> <title> "On Affinity Based Routing in Multi-System Data Sharing". </title> <booktitle> In Proc. Int. Conf. on Very Large Data Bases, </booktitle> <address> Kyoto, Japan, </address> <year> 1986. </year>
Reference: [YCDT85] <author> P. S. Yu, D. W. Cornell, D. M. Dias, and A. Thomasian. </author> <title> "On Coupling Partitioned Database Systems". </title> <type> Technical Report Research Report RC11410, </type> <institution> IBM, </institution> <year> 1985. </year>
Reference: [YCDT89] <author> P. S. Yu, D. W. Cornell, D. M. Dias, and A. Thomasian. </author> <title> "Performance Comparison of I/O Shipping and Database Call Shipping: Schemes in Mul-tisystem Partitioned Databases". </title> <journal> Perfomance Evaluation, </journal> <volume> 10(1), </volume> <month> October </month> <year> 1989. </year>
Reference: [YCHL92] <author> P. S. Yu, M. S. Chen, H. U. Heiss, and S. Lee. </author> <title> "On Workload Characterization of Relational Database Environments". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(4) </volume> <pages> 347-355, </pages> <month> April </month> <year> 1992. </year> <month> 136 </month>
Reference: [YD91a] <author> P. S. Yu and A. Dan. </author> <title> "Effect of System Dynamics on Coupling Archi--tectures for Transaction Processing". </title> <type> Technical Report Research Report RC16606, </type> <institution> IBM, </institution> <year> 1991. </year>
Reference: [YD91b] <author> P. S. Yu and A. Dan. </author> <title> "Impact of Affinity on the Performance of Coupling Architectures for Transaction Processing". </title> <type> Technical Report Research Report RC16431, </type> <institution> IBM, </institution> <year> 1991. </year>
Reference: [YD92] <author> P. S. Yu and A. Dan. </author> <title> "Impact of Wokload Partitionability on the Performance Coupling Architectures for Transaction Processing". </title> <booktitle> In Proc. of the IEEE Int. Conf. on Parallel and Distributed Processing, </booktitle> <pages> pages 40-49. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference: [YD94] <author> P. S. Yu and A. Dan. </author> <title> "Performance Evaluation of Transaction Processing Coupling Architectures for Handling System Dynamics". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(2), </volume> <month> February </month> <year> 1994. </year>
Reference: [YDR + 87] <author> P. S. Yu, D. M. Dias, J. T. Robinson, B. R. Iyer, and D. W. Cornell. </author> <title> "On Coupling Multi-Systems Through Data Sharing". </title> <journal> Proc. IEEE, </journal> <volume> 75(5), </volume> <month> May </month> <year> 1987. </year>
Reference: [YL84] <author> T. Yaun and H. Lin. </author> <title> "Adaptive load balancing for parallel queues". </title> <booktitle> In Proc IEEE International Conf. on Communications, </booktitle> <address> Amsterdam, </address> <year> 1984. </year>
Reference: [YL89] <author> P. S. Yu and A. Leff. </author> <title> "On Robust Transaction Routing and Load Sharing". </title> <type> Technical Report Research Report RC15135, </type> <institution> IBM, </institution> <year> 1989. </year>
Reference: [YL91] <author> P. S. Yu and A. Leff. </author> <title> "On Robust Transaction Routing and Load Sharing". </title> <journal> ACM Trans. Database Syst., </journal> <volume> 16(3) </volume> <pages> 476-512, </pages> <month> September </month> <year> 1991. </year>
Reference: [YWS + 93] <author> B. Yang, J. Webb, J. Stichnoth, D. O'Hallaron, and T. Gross. "Do&Merge: </author> <title> Integrating parallel loops and reductions". </title> <booktitle> In Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 169-183, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <booktitle> Lecture Notes in Computer Science volume 768, </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: The loop iterations are independent of each other and can be done in parallel. Second, a reduction operation combines Page 76 6.2. COMPILER LAYER the elements of the partial result array to produce the single final result (Merge computation). Yang et al, in <ref> [YWS + 93] </ref>, show that combining the Do phase and the Merge phase into a single Do&Merge computation can lead to improved execution time and memory usage. They describe a simple and efficient construct (called the Pdo loop) that is included in an experimental HPF-like compiler for private-memory parallel systems.
Reference: [Zay87a] <author> E. Zayas. </author> <title> "The Use of Copy-on-Reference in a Process Migration System". </title> <type> PhD thesis, </type> <institution> Carnegie Mellon Univ., </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1987. </year>
Reference: [Zay87b] <author> E. R. Zayas. </author> <title> "Attacking the Process Migration Bottleneck". </title> <booktitle> In Proc. 11-th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 13-24, </pages> <year> 1987. </year>
Reference: [Zho88] <author> S. Zhou. </author> <title> "A Trace-Driven Simulation Study of Dynamic Load Balancing". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(9) </volume> <pages> 1327-1341, </pages> <month> September </month> <year> 1988. </year>
Reference: [ZLE91] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager. </author> <title> "The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Systems". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2) </volume> <pages> 180-198, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, processes of the same application communicate via critical sections, and synchronize using barriers. It has been shown that synchronization primitives interact with the scheduling policy used, and sometimes this interaction results in significant overhead <ref> [ZLE91, LG87a, LG87b] </ref>. For example, if a process is preemted while holding a lock, all other processes that want to get the lock will be forced to spin-wait for it, or suspend, and in any case, they will not be able to make forward progress.
Reference: [ZM90] <author> J Zahorjan and C. McCann. </author> <title> "Processor Scheduling in Shared Memory Multiprocessors". </title> <journal> Performance Evaluation Review, </journal> <volume> 18(1) </volume> <pages> 214-225, </pages> <month> May </month> <year> 1990. </year> <month> 137 </month>
Reference-contexts: In such cases, allocating a fixed number of processors to each application would result in significant load imbalance, Page 98 6.4. RUN TIME LAYER because applications with decreasing parallelism will underutilize their processors, while applications with increasing parallelism could use some extra processors <ref> [ZM90, MVZ90] </ref>. 6.4 Run Time Layer Most process-based parallel systems are expensive, because they use the process as the unit of parallelism. However, the process as implemented by most operating systems is both the unit of parallelism and the unit of protection.
References-found: 397

