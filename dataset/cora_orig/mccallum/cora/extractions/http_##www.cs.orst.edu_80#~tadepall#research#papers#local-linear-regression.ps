URL: http://www.cs.orst.edu:80/~tadepall/research/papers/local-linear-regression.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: tadepall,okd@research.cs.orst.edu  
Title: Scaling Up Average Reward Reinforcement Learning by Approximating the Domain Models and the Value Function  
Author: Prasad Tadepalli and DoKyeong Ok 
Keyword: ing function approximation to ARL.  
Address: Corvallis, Oregon 97331-3202  
Affiliation: Computer Science Department Oregon State University  
Abstract: Almost all the work in Average-reward Re- inforcement Learning (ARL) so far has focused on table-based methods which do not scale to domains with large state spaces. In this paper, we propose two extensions to a model-based ARL method called H-learning to address the scale-up problem. We extend H-learning to learn action models and reward functions in the form of Bayesian networks, and approximate its value function using local linear regression. We test our algorithms on several scheduling tasks for a simulated Automatic Guided Vehicle (AGV) and show that they are effective in significantly reducing the space requirement of H-learning and making it converge faster. To the best of our knowledge, our results are the first in apply 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic pro-gramming. </title> <journal> Artificial Intelligence, </journal> <volume> 73(1), </volume> <pages> 81-138. </pages>
Reference-contexts: In this paper, we study two extensions to a model-based ARL method called H-learning, an undis- counted version of Adaptive Real-Time Dynamic Pro- gramming (ARTDP) <ref> (Barto et al., 1995) </ref>. The study of ARL so far has been restricted to table- based methods in which the value function is stored as a table. In the worst case, this table can become as large as the number of states in the state space. <p> It is very similar to "Algorithm B" of Jalali and Ferguson (Jalali & Ferguson, 1989), and is an Average-reward version of the discounted RL method, Adaptive Real- time Dynamic Programming (ARTDP) <ref> (Barto et al., 1995) </ref>. It can also be seen as a model-based version of R-learning (Schwartz, 1993). The right hand side of Equation (1) also involves , which is estimated as the gain of the current greedy policy.
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Boutilier, C., Dearden, R., & Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Boutilier, C., & Puterman, M. L. </author> <year> (1995). </year> <title> Processoriented planning and average reward optimal-ity. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Boyan, J., & Moore, A. W. </author> <year> (1994). </year> <title> Generalizing reinforcement learning:safely approximating the value function. </title> <booktitle> In Proceedings of Neural Information Processing Systems. </booktitle>
Reference: <author> Canavos, G. C. </author> <year> (1984). </year> <title> Applied Probability and Statistical Methods. Little, </title> <publisher> Brown and Company, </publisher> <address> Boston, MA. </address>
Reference-contexts: = 6 6 fi 0 . . . 3 7 5 2 6 4 e 2 e m 7 7 : The values for parameters fi 0 ; : : :; fi k that minimize the squared error can be obtained as the (k + 1) fi 1 vector B <ref> (Canavos, 1984) </ref>. B = (X T X) 1 X T Y: (2) 4.2 Local Linear Regression Let us assume that the state is represented by a set of k "linear" features and n k "nonlinear" features.
Reference: <author> Dean, T., & Kanazawa, K. </author> <year> (1989). </year> <title> A model for rea-soning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3), </volume> <pages> 142-150. </pages>
Reference: <author> Jalali, A., & Ferguson, M. </author> <year> (1989). </year> <title> Computationally efficient adaptive control algorithms for markov chains. </title> <booktitle> In IEEE Proceedings of the 28'th Conference on Decision and Control Tampa, </booktitle> <address> FL. </address>
Reference-contexts: It initializes all the h values to 0, and in each current state i, updates h (i) with the right hand side of Equation (1). It is very similar to "Algorithm B" of Jalali and Ferguson <ref> (Jalali & Ferguson, 1989) </ref>, and is an Average-reward version of the discounted RL method, Adaptive Real- time Dynamic Programming (ARTDP) (Barto et al., 1995). It can also be seen as a model-based version of R-learning (Schwartz, 1993).
Reference: <author> Kjaerulff, U. </author> <year> (1992). </year> <title> A computational scheme for rea-soning in dynamic probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 121-129. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, </title> <journal> and teach-ing. Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empiri-cal results. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 159-195. </pages>
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1992). </year> <title> Automatic pro-gramming of behavior-based robots using rein-forcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <pages> 311-365. </pages>
Reference: <author> Moore, A. W. </author> <year> (1990). </year> <title> Efficient Memory-based Learning for Robot Control. </title> <type> Ph.D. thesis. </type> <institution> Trinity hall, University of Cambridge, </institution> <address> England. </address>
Reference: <author> Nicholson, A. E., & Brady, J. M. </author> <year> (1992). </year> <title> The data as-sociation problem when monitoring robot vehi-cles using dynamic belief networks. </title> <booktitle> In ECAI 92: 10th European Conference on Artificial Intelligence Proceedings, </booktitle> <pages> pp. </pages> <address> 689-693 Vienna, Austria. </address> <publisher> Wiley. </publisher>
Reference: <author> Ok, D., & Tadepalli, P. </author> <year> (1996). </year> <title> Auto-exploratory av-erage reward reinforcement learning. </title> <booktitle> In Proceedings of AAAI-96. </booktitle>
Reference-contexts: H-learning improves its ability to automatically explore the state space by initializing the value of to a value 0 higher than the expected optimum gain and adjusting it using a decreasing learning rate ff, initialized to a small value, ff 0 <ref> (Ok & Tadepalli, 1996) </ref>. Our experiments are based on this version of H-learning, denoted by H 0 ;ff 0 . 3 Model Generalization using Bayesian Networks One of the disadvantages of model-based methods like H-learning is that explicitly storing its action and re ward models consumes a lot of space.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes: Discrete Dynamic Stochastic Programming. </title> <publisher> John Wiley. </publisher>
Reference: <author> Russell, S., & Norvig, P. </author> <year> (1995). </year> <journal> Artificial Intelligence: </journal>
Reference-contexts: Model-based methods like H-learning also have the additional problem of having to explicitly store their action models and the reward functions, which we call the "domain models." Dynamic Bayesian networks have been successfully used in the past to represent the domain models <ref> (Russell & Norvig, 1995) </ref>. In many cases, it is possible to design these networks in such a way that a small number of parameters are sufficient to fully specify the domain models.
References-found: 17

