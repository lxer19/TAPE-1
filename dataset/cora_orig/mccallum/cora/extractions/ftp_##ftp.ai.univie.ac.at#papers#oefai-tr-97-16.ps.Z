URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-16.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Title: Mining for Causes of Cancer: Machine Learning Experiments at Various Levels of Detail  
Author: Stefan Kramer and Bernhard Pfahringer and Christoph Helma 
Abstract: This paper presents, from a methodological point of view, first results of an interdisciplinary project in scientific data mining. We analyze data about the carcinogenicity of chemicals derived from the carcinogenesis bioassay program, a long-term research study performed by the US National Institute of Environmental Health Sciences. The database contains detailed descriptions of 6823 tests performed with more than 330 compounds and animals of different species, strains and sexes. The chemical structures are described at the atom and bond level, and in terms of various relevant structural properties. The goal of this paper is to investigate the effects that various levels of detail and amounts of information have on the resulting hypotheses, both quantitatively and qualitatively. We apply relational and propositional machine learning algorithms to learning problems formulated as regression or as classification tasks. In addition, these experiments have been conducted with two learning problems which are at different levels of detail. Quantitatively, our experiments indicate that additional information not necessarily improves accuracy. Qualitatively, a number of potential discoveries have been made by the algorithm for Relational Regression, because it is not forced to abstract from the details contained in the relations of the database.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Auer, W. Maass, and R. Holte. </author> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we use C4.5 [13], the well-known decision-tree package, and T2 <ref> [1] </ref>, which induces 2-level decision trees. FOIL [11] and PROGOL [10] 2 are state-of-the-art ILP algorithms. M5 [12] learns regression trees with linear regression models in the leaves.
Reference: [2] <author> S. Dzeroski. </author> <title> Numerical Constraints and Learnability in Inductive Logic Programming. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, Ljubljana, Slovenija, </institution> <year> 1995. </year>
Reference-contexts: SARs are models that predict the activity of chemicals in organisms from the molecular structure. Formally, the problem is to predict numbers from "relational structures" (such as labeled graphs), a problem also known as Relational Regression <ref> [2] </ref>. 1 In the following, we present results from our analysis of carcinogenicity data derived from the carcinogenesis bioassay program, a long-term research study performed by the US National Institute of Environmental Health Sciences (NIEHS). In the next section, we present previous work.
Reference: [3] <author> U. Fayyad, D. Haussler, and P. Stolorz. </author> <title> KDD for science data analysis: Issues and examples. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pages 50-56, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction In science data analysis <ref> [3] </ref>, we benefit from the luxury of precision of the data and the availability of domain knowledge, but the basic problems are the same as in other areas.
Reference: [4] <author> L.S. </author> <title> Gold. </title> <booktitle> Sixth plot of the carcinogenicity potency in the general literature 1989 to 1990 and by the national toxicology program 1990 to 1993. Environmental Health Perspectives, </booktitle> <volume> 103 (Suppl7):1-122, </volume> <year> 1995. </year>
Reference: [5] <author> J.D. Hirst, R.D. King, and M.J.E. Sternberg. </author> <title> Quantitative structure-activity relationships by neural networks and inductive logic programming. the inhibition of di-hydrofolate reductase by pyrimidines. </title> <journal> Journal of Computer-Aided Molecular Design, </journal> <volume> 8 </volume> <pages> 405-420, </pages> <year> 1994. </year>
Reference: [6] <author> J.D. Hirst, R.D. King, and M.J.E. Sternberg. </author> <title> Quantitative structure-activity relationships by neural networks and inductive logic programming: The inhibition of dihydrofolate reductase by triazines. </title> <journal> Journal of Computer-Aided Molecular Design, </journal> <volume> 8 </volume> <pages> 421-432, </pages> <year> 1994. </year>
Reference: [7] <author> R.D. King and A. Srinivasan. </author> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives, </title> <year> 1997. </year>
Reference-contexts: This work is also much in the spirit of studies comparing various methods (FOIL vs. PROGOL [14], propositional learning vs. relational learning [15]) in the domain of muta-genicity. However, the closest work in the literature is <ref> [7] </ref>, which reports the application of the ILP algorithm PROGOL to one of the databases also used in this study. 3 Description of the Data In this section we describe the datasets used in our experiments "as is", without the data engineering steps to define the learning problems. <p> Comparing classification and regression, we observed a small improvement of the regression results over the classification results. However, the biggest difference was between using all the information, and using only part of the information. 2 The experiment with PROGOL has been described in <ref> [7] </ref>. 6 Examples Formalism Learning task Algorithm (s) Chemicals Propositional Classification C4.5, T2 Chemicals Propositional Regression (f1; 0; 1g) M5 Chemicals Relational Classification FOIL, PROGOL Chemicals Relational Regression (f1; 0; 1g) SRT Tests Propositional Classification C4.5, T2 Tests Propositional Regression M5 Tests Relational Classification FOIL Tests Relational Regression SRT Table 3:
Reference: [8] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: For propositional classification, we use C4.5 [13], the well-known decision-tree package, and T2 [1], which induces 2-level decision trees. FOIL [11] and PROGOL [10] 2 are state-of-the-art ILP algorithms. M5 [12] learns regression trees with linear regression models in the leaves. SRT <ref> [8] </ref> learns relational regression trees. 5 Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5). For the chemicals, we did not observe big differences in accuracy except for Relational Regression, i.e., when all the available information is provided.
Reference: [9] <editor> S. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> London, U.K., </address> <year> 1992. </year>
Reference: [10] <author> S. Muggleton. </author> <title> Inverse Entailment and Progol. New Generation Computing, </title> <address> 13:245--286, </address> <year> 1995. </year>
Reference-contexts: Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we use C4.5 [13], the well-known decision-tree package, and T2 [1], which induces 2-level decision trees. FOIL [11] and PROGOL <ref> [10] </ref> 2 are state-of-the-art ILP algorithms. M5 [12] learns regression trees with linear regression models in the leaves. SRT [8] learns relational regression trees. 5 Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: [11] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we use C4.5 [13], the well-known decision-tree package, and T2 [1], which induces 2-level decision trees. FOIL <ref> [11] </ref> and PROGOL [10] 2 are state-of-the-art ILP algorithms. M5 [12] learns regression trees with linear regression models in the leaves. SRT [8] learns relational regression trees. 5 Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: [12] <author> J.R. Quinlan. </author> <title> Learning with continuous classes. </title> <editor> In Sterling Adams, editor, </editor> <booktitle> Proceedings AI'92, </booktitle> <pages> pages 343-348, </pages> <address> Singapore, 1992. </address> <publisher> World Scientific. </publisher>
Reference-contexts: Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we use C4.5 [13], the well-known decision-tree package, and T2 [1], which induces 2-level decision trees. FOIL [11] and PROGOL [10] 2 are state-of-the-art ILP algorithms. M5 <ref> [12] </ref> learns regression trees with linear regression models in the leaves. SRT [8] learns relational regression trees. 5 Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: [13] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we use C4.5 <ref> [13] </ref>, the well-known decision-tree package, and T2 [1], which induces 2-level decision trees. FOIL [11] and PROGOL [10] 2 are state-of-the-art ILP algorithms. M5 [12] learns regression trees with linear regression models in the leaves.
Reference: [14] <author> A. Srinivasan, S. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by Inductive Logic Programming systems. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 199-230. </pages> <address> Katholieke Uni-versiteit Leuven, </address> <year> 1995. </year>
Reference-contexts: So the experiments demonstrated that the advantage afforded by comprehensible theories is not gained at the expense of predictive accuracy. This work is also much in the spirit of studies comparing various methods (FOIL vs. PROGOL <ref> [14] </ref>, propositional learning vs. relational learning [15]) in the domain of muta-genicity.
Reference: [15] <author> A. Srinivasan, S. Muggleton, R.D. King, and M. Sternberg. </author> <title> Theories for mutagenicity: a study of first-order and feature based induction. </title> <journal> Artificial Intelligence, </journal> <volume> 85(1-2):277-299, </volume> <year> 1996. </year>
Reference-contexts: So the experiments demonstrated that the advantage afforded by comprehensible theories is not gained at the expense of predictive accuracy. This work is also much in the spirit of studies comparing various methods (FOIL vs. PROGOL [14], propositional learning vs. relational learning <ref> [15] </ref>) in the domain of muta-genicity.
References-found: 15

