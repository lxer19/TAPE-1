URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1996/UM-CS-1996-070.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Author: Doina Precup, Rich Sutton 
Date: September 19, 1996  
Abstract: Empirical Comparison of Gradient Descent and Exponentiated Gradient Descent in Supervised and Reinforcement Learning Technical Report 96-70 
Abstract-found: 1
Intro-found: 1
Reference: <author> Moore A.W. </author> <year> (1991). </year> <title> Variable Resolution Dynamic Programming: Efficiently learning action mapsin multivariate real-valued state-spaces. </title> <booktitle> In Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann: </publisher> <pages> 333-337. </pages>
Reference: <author> Rummery G.A., Niranjan M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Dept. </institution>
Reference-contexts: This is a minimum time control problem, so the reward is -1 for all time steps until the goal is reached, without any discounting. The three possible actions at each step are: accelerate forward, accelerate backward and no acceleration. The algorithm applied to the problem is Sarsa <ref> (Rummery and Niranjan, 14 1994) </ref> with replacing eligibility traces. The algorithm with gradient descent additive updates is described in detail in (Singh and Sutton, 1996). The EG update algorithm is analogous, but uses the additive update in log weight space.
Reference: <author> Singh, S.P., Sutton, R.S. </author> <year> (1996). </year> <title> Reinforcement Learning with Replacing Eligibility Traces. </title> <booktitle> Machine Learning 22: </booktitle> <pages> 123-158. 18 </pages>
Reference-contexts: becomes: log w i := log w i + ffffi t e t For state value computation, ffi t := r + flV t (s t+1 ) V t (s t ) In order to prevent the weights from getting too big, replace traces are used instead of accumulating traces <ref> (Singh and Sutton, 1996) </ref>: e t (s) = 1 if s = s t fle t (s t ) otherwise The first task taken into consideration is a prediction task in a bounded continuous random walk problem. The walking interval is [0,1]. <p> The objective of the car is to pass the top of the mountain. Because the gravity is stronger than the engine, the solution is to accelerate backwards first and then thrust forwards towards the goal. The detailed physics of the problem are given in <ref> (Singh and Sutton, 1996) </ref>. This is a minimum time control problem, so the reward is -1 for all time steps until the goal is reached, without any discounting. The three possible actions at each step are: accelerate forward, accelerate backward and no acceleration. <p> The three possible actions at each step are: accelerate forward, accelerate backward and no acceleration. The algorithm applied to the problem is Sarsa (Rummery and Niranjan, 14 1994) with replacing eligibility traces. The algorithm with gradient descent additive updates is described in detail in <ref> (Singh and Sutton, 1996) </ref>. The EG update algorithm is analogous, but uses the additive update in log weight space. For the implementation, we used Mahadevan's code as a strating point. The mountain-car task has a continuous state space, with two state variables: position and velocity.
References-found: 3

