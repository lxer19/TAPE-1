URL: http://www.cs.ualberta.ca/~upal/plan/2/2.ps
Refering-URL: http://www.cs.ualberta.ca/~upal/pub.html
Root-URL: http://www.cs.ualberta.ca/~upal/pub.html
Title: Learning Rationales to Improve Plan Quality for Partial Order Planners that uses explanation-based learning to
Note: DerSNLP+EBL (IK97) is a partial-order planner  
Abstract: Plan rationale has been variously defined as "why the plan is the way it is", as "the reason as to why the planning decisions were taken." The usefulness of storing plan rationale to help future planning has been demonstrated by several types of case-based planners. However, the existing techniques are unable to distinguish between planning decisions that, while leading to successful plans, may produce plans that differ in overall quality, as defined by some quality metric. We outline a planning and learning system, PIPP, that applies analytic techniques to learn plan-refinement control rules that partial-order planners can use to produce better quality plans. Quality metrics are assumed to be variant- whether a particular plan refinement decision contributes to a better plan is a function of contextual factors that PIPP identifies. Preliminary evaluation results indicate that the overhead for applying these techniques to store and then use planning-refinement rules is not very large, and the outcome is the ability to produce better quality plans within a domain. Techniques like these are useful in those domains in which knowledge about how to measure quality is available and the quality of the final plan is more important than the time taken to produce it. According to Hammond (Ham90), case-based planning involves remembering past planning solutions so that they can be reused, remembering past planning failures so that they can be avoided, and remembering repairs to plans that once failed but got 'fixed' so that they can be re-applied. Early work on derivational analogy (Car83a; Car83b) proposed that each planning decision within a plan be annotated with the rationale for making that planning decision. An example of a rationale for taking a particular action a might be "it achieves goal g." Both state-space planners (Vel94) and partial-order planners (IK97) make use of such annotations in building a case library. State-space planners move through a search space of possible worlds, where each operator is a possible action to take in the world. Partial-order planners move through a space of possible plans, where each operator is a plan-refinement decision that typically adds additional steps or constraints to an evolving plan description. In the partial-order planning paradigm, a refinement decision such as "add-step a to partial plan p" might be annotated with the rationale "a's effects match an open (unsatisfied) condition of partial plan p." The idea behind storing rationales is that a previously-made and retrieved planning decision, be it in a state-space or partial-order framework, will only be applied in context of the current planning problem just in the case that the rationale for it also holds in the current problem. 
Abstract-found: 1
Intro-found: 1
Reference: <author> J. Ambite and C. Knoblock. </author> <title> Planning by rewriting: Efficiently generating high-quality plans. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA, 1997. </address> <publisher> AAAI Press. </publisher>
Reference: <author> A. Barett and D. Weld. </author> <title> Partial order planning: evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 71-112, </pages> <year> 1994. </year>
Reference: <author> J. Carbonell. </author> <title> Derivational analogy and its role in prob lem solving. </title> <booktitle> In Proceedings of the Third National Con--ference on Artificial Intelligence, </booktitle> <address> Los Altos, CA, 1983. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> J. Carbonell. </author> <title> Learning by analogy: Formulating and generalizing plans from past experience. </title> <editor> In R. Michal-ski, editor, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 131-161. </pages> <publisher> Tioga Pub. Co., </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference: <author> T. Estlin and R. Mooney. </author> <title> Learning to improve both efficiency and quality of planning. </title> <booktitle> In Proceedings of the IJCAI. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference: <author> P. Fishburn. </author> <title> Utility Theory for Decision Making. </title> <address> Wi-ley, New York, </address> <year> 1970. </year>
Reference: <author> K. Hammond. </author> <title> Case-based planning: A framework for planning from experience. </title> <journal> Cognitive Science, </journal> <volume> 14(3), </volume> <year> 1990. </year>
Reference: <author> L. Ihrig and S. Kambhampati. </author> <title> Storing and indexing plan derivations through explanation-based analysis of retrieval failures. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 7 </volume> <pages> 161-198, </pages> <year> 1997. </year>
Reference: <author> R. Keeney and H. Raiffa. </author> <title> Decisions With Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Cambridge University Press, </publisher> <address> New York, 2nd edition, </address> <year> 1993. </year>
Reference: <author> D. McAllester and D. Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 634-639, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> A. Perez. </author> <title> Representing and learning quality-improving search control knowledge. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <address> Los Altos, CA, 1996. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> M. Pazzani and D. Kibler. </author> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference: <author> R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-2666, </pages> <year> 1990. </year>
Reference: <author> M. Veloso, J. Carbonell, M. Perez, E. Borrajo, D. amd Fink, and J. Blythe. </author> <title> Integrating planning and learning: The PRODIGY architecture. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1), </volume> <year> 1995. </year>
Reference: <author> M. Veloso. </author> <title> Learning by Analogical Reasoning. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference: <author> M. Williamson. </author> <title> A value-directed approach to planning. </title> <type> Technical Report TR-96-06-03, PhD thesis, </type> <institution> University of Washington, </institution> <year> 1996. </year>
References-found: 16

