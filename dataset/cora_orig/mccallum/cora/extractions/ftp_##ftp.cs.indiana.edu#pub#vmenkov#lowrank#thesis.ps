URL: ftp://ftp.cs.indiana.edu/pub/vmenkov/lowrank/thesis.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/vmenkov/abstract.html
Root-URL: http://www.cs.indiana.edu
Title: LOW-RANK APPROXIMATION IN PRECONDITIONER DESIGN  
Author: Vladimir Me~nkov 
Degree: Submitted to the faculty of the Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: May 1998  
Affiliation: Department of Computer Science Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Edward Charles Anderson. </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations. </title> <type> Master's thesis, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year>
Reference-contexts: When a parallel computer is used for solving the linear system, the ability to compute the preconditioner and to apply it in parallel is important as well. Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU <ref> [1, 13, 12, 19, 32] </ref>, efficient parallel implementation of preconditioned iterative methods remains difficult [37]. 1.1.4 Types of preconditioners. <p> However, we still choose to compute ^u and ^v even in this case, even though they don't correspond to any ordinary matrix. For example, for the input block A = 6 1 0 3 5 the values ^u = 6 1 3 5 ; ^v T = <ref> [ 1; 1 ] </ref> ; and ^ fi = 0 will be computed and stored. This implies that there are some uv T -type LOWRANK blocks that cannot be thought of as a conventional rank-one matrix. We call them degenerate blocks.
Reference: [2] <author> Owe Axelsson. </author> <title> Iterative Solution Methods, 1 edition. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: Its coefficient matrix A may contain some 10n = 10 5 non-zeros. The inverse of A, however, will contain n 2 = 10 8 non-zeros. Even though computing A 1 explicitly is not needed in most direct methods, the 1 Or fill-in, as some authors <ref> [2] </ref> prefer to call them. 3 number of non-zeros in the factors L and U will still be unacceptably large: O (n 3=2 )| around 10 6 |non-zeros in the case of a matrix resulting from the discretization of a typical two-dimensional problem, or O (n 5=3 ) in a three-dimensional <p> While a basic stationary one-step iterative method uses only the last residual to obtain the update s k , accelerated iterative methods such as Chebyshev, conjugate gradients (CG), or GMRES <ref> [24, 2, 44] </ref> use a longer sequence of the residuals and/or updates. Iterative techniques are attractive for solving large sparse systems because one iteration is much cheaper than solving the Ax = y directly. <p> The ability to make the absolute value of the polynomial small everywhere on (AC 1 ) implies that the method makes the residual small as well <ref> [27, 2] </ref>. Convergence rate estimates [2, 5] are easy to obtain when the spectrum of AC 1 is positive real, or when it is contained in an ellipse within the Re &gt; 0 part of the complex plain. <p> The ability to make the absolute value of the polynomial small everywhere on (AC 1 ) implies that the method makes the residual small as well [27, 2]. Convergence rate estimates <ref> [2, 5] </ref> are easy to obtain when the spectrum of AC 1 is positive real, or when it is contained in an ellipse within the Re &gt; 0 part of the complex plain. <p> This auxiliary system should be much easier to solve than the original system: otherwise, why not use a direct method? ILU: A commonly used class of implicit preconditioners are ILU (incomplete LU factorization) preconditioners <ref> [2, Chapter 7] </ref>. If C is such a preconditioner, it is represented as a product of two sparse triangular matrices: the lower triangular L and the upper triangular U . <p> It has been proposed that the quality of the preconditioning may be improved if, instead of being discarded, fill elements that don't fit into the target sparsity pattern or are too small are added to the diagonal elements of the Schur complement. Although better spectral bounds have been proved <ref> [2] </ref> for this modified ILU (MILU) method than for the standard ILU, the latter continues to be more commonly used in practical applications. Explicit preconditioners: An explicit preconditioning method constructs a matrix M directly approximating the inverse of A. <p> It is shown how this method can be implemented on a parallel computer. A well-known preconditioner used with systems partitioned into blocks is block SSOR (BSSOR) [51], <ref> [2, Chapter 10] </ref>. In Chapter 3, the low-rank approximation techniques are also used to construct the SLRA preconditioner|an alternative to BSSOR more amenable to parallel implementation. <p> The cost of storing a BLOB matrix, as well as the cost of performing arithmetic operations with such matrices, is analyzed. Despite the large number of implicit preconditioning techniques proposed over the recent decades <ref> [2] </ref>, the incomplete LU factorization (ILU), in its various versions, remains the technique most commonly used in practice. <p> In Section 3.1, two such preconditioners are proposed: DLRA, whose preconditioner C DIR is obtained from A by replacing its ODBs with their low-rank approximations (LRA) introduced in Chapter 2; and SLRA preconditioner C SOR , which is one step of block SSOR <ref> [2, Chapter 10] </ref> [51] applied to C DIR z = r. In Section 3.2, spectral properties of the DLRA preconditioner for a sample problem are studied. <p> Results: In [15, Section 5] we have shown that for r = 1 and the low-rank approximation using lumping (2.3) the spectrum of C 1 A is contained in <ref> [c=L sd ; 2] </ref> = [c p=L; 2]. The analysis sketched here will extend that result to the case of r &gt; 1, as follows: (AC 1 ) 2 1 p ; 2 ; (3.4) and cond (AC 1 ) = O (1 + L=(r p p)). <p> Results: In [15, Section 5] we have shown that for r = 1 and the low-rank approximation using lumping (2.3) the spectrum of C 1 A is contained in [c=L sd ; 2] = <ref> [c p=L; 2] </ref>. The analysis sketched here will extend that result to the case of r &gt; 1, as follows: (AC 1 ) 2 1 p ; 2 ; (3.4) and cond (AC 1 ) = O (1 + L=(r p p)). <p> O, then instead of (3.30) we will have t B = O (t D + t m ): (3.36) 3.4 Relation between Methods Based on the SMW Formula and on the Schur Complement The SMW-formula based algorithm for solving (3.15) described in Section 3.3 is similar to Schur complement methods <ref> [2, Chapter 9] </ref>, where the solution of the system 2 4 A 21 A 22 7 2 4 x 2 7 2 4 y 2 7 is calculated by a process that involves matrix-vector multiplications with A 12 and A 21 and solving linear systems with A 11 and S =
Reference: [3] <author> Owe Axelsson and Gunhild Lindskog. </author> <title> On the eigenvalue distribution of a class of preconditioning methods. </title> <journal> Numer. Math., </journal> <volume> 48(5) </volume> <pages> 479-498, </pages> <year> 1986. </year>
Reference-contexts: However, an 9 iterative method may often converge faster than (1.4) indicates, if the eigenvalues are not spread out uniformly over the range [min (AC 1 ); max (AC 1 )], but rather are clustered in some smaller parts of it <ref> [3] </ref>. Viewing the entire spectral portrait of AC 1 often provides a better idea of how good the preconditioner is, especially when the spectrum is complex.
Reference: [4] <author> S.T. Barnard and H.D. Simon. </author> <title> Fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(2) </volume> <pages> 101-117, </pages> <year> 1994. </year>
Reference-contexts: The expression "a mesh node" will be used to 16 refer to a vertex of the matrix graph. The preconditioned techniques introduced in this dissertation make use of a partitioning of the set of unknowns into groups, normally derived from applying some graph partitioning algorithm <ref> [4, 30, 26] </ref> to the matrix graph. <p> A few more examples of domain partitioning trees: * is the set of nodes of some arbitrary mesh. The hierarchical domain parti tioning is obtained by a recursive spectral bisection <ref> [4] </ref>. 114 * is the set of nodes of some arbitrary mesh. The hierarchical domain parti-tioning is obtained by METIS [30], each subdomain being partitioned into 2, 3 or 5 parts at each level.
Reference: [5] <author> R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Ei-jkhout, R. Pozo, C. Romine, and H. Van der Vorst. </author> <title> Templates for the Solution 215 of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year> <note> http://www.netlib.org/templates/templates.ps. </note>
Reference-contexts: The ability to make the absolute value of the polynomial small everywhere on (AC 1 ) implies that the method makes the residual small as well [27, 2]. Convergence rate estimates <ref> [2, 5] </ref> are easy to obtain when the spectrum of AC 1 is positive real, or when it is contained in an ellipse within the Re &gt; 0 part of the complex plain. <p> The disadvantage is that the linear update operation (commonly referred to as DAXPY or SAXPY <ref> [5] </ref>) in 4 0 is done with dense vectors, while that in 5 uses sparse ones. In the 76 remainder of this chapter, we use 4 and 5 instead of 4 0 . Modifications for the SLRA Preconditioner: The SMW formula can also be used for applying the SLRA preconditioner. <p> A^s x i = x i1 + ff i ^p + ! i ^s check convergence; continue if necessary for continuation it is necessary that ! i 6= 0 endfor 91 3.5 Numerical Results for DLRA and SLRA The numerical experiments use the preconditioners within a biconjugate gradients stabilized (Bi-CGSTAB) <ref> [48, 5] </ref> iterative method (Figure 3.2). The program is written in pC++, a parallel extension [20] of C++, and run on a ten-processor SGI Power Challenge with 75 MHz R8000 chips running the 64 bit IRIX 6.1 operating system.
Reference: [6] <author> Peter Beckman. </author> <title> Parallel LU Decomposition for Sparse Matrices Using Quadtrees on a Shared-Heap Multiprocessor. </title> <type> PhD dissertation, </type> <institution> Indiana University, Bloom-ington, </institution> <year> 1993. </year>
Reference-contexts: This dissertation paid no attention to the parallelism aspect of BLOB precondi-tioners. However, it seems likely that there are ways to parallelize generation and application of such preconditioners, possibly using techniques proposed elsewhere <ref> [50, 6] </ref> to parallelize multiplication of quadtree matrices and LU factorization using such matrices. It is also highly desirable to find out the reason of disappointingly poor performance of BLOB approximate inverse preconditioners, and find a way to improve it.
Reference: [7] <author> M.W. Benson. </author> <title> Iterative solution of large scale linear systems. </title> <type> Master's thesis, </type> <institution> Lakehead University, Thunder Bay, Canada, </institution> <year> 1973. </year>
Reference-contexts: As discussed in Section 4.5, the BLOB framework can yield not only approximate LU factorization, but also an approximate inverse of A|an explicit preconditioner. 20 However, just like the classic sparsity-pattern-constrained approximate inverses <ref> [43, 7] </ref>, this approach appears not to be competitive with its implicit counterpart: for a given storage cost, the quality of an approximate-inverse preconditioner is worse than that of an ILU preconditioner with the same sparsity pattern (in the classic case) or the same BLOB structure (in the BLOB case). 1.4
Reference: [8] <author> Michele Benzi, Carl D. Meyer, and Miroslav Tuma. </author> <title> A sparse approximate inverse preconditioner for the conjugate gradient method. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 17(5) </volume> <pages> 1135-1149, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Despite these advantages, explicit preconditioners are not commonly used, since the numerical quality of an explicit preconditioner, as measured by the convergence rate of the iterative process, is usually worse than that of a comparable-cost implicit preconditioner [9, see also bibliography in it], <ref> [8, 10] </ref>. 1.2 Mesh and Mesh Decomposition Notation The preconditioners introduced in this dissertation rely on partitioning of the discretized domain where the problem (1.1) is solved, and the partitioning of vectors and matrices induced by it. This section reviews some of the related terminological and notational conventions.
Reference: [9] <author> Michele Benzi and Miroslav Tuma. </author> <title> A comparative study of sparse approximate inverse preconditioners. </title> <type> Technical Report LA-UR-98-0024, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1998. </year> <note> http://www.c3.lanl.gov/~benzi/Web_papers/final.ps.gz. </note>
Reference-contexts: Despite these advantages, explicit preconditioners are not commonly used, since the numerical quality of an explicit preconditioner, as measured by the convergence rate of the iterative process, is usually worse than that of a comparable-cost implicit preconditioner <ref> [9, see also bibliography in it] </ref>, [8, 10]. 1.2 Mesh and Mesh Decomposition Notation The preconditioners introduced in this dissertation rely on partitioning of the discretized domain where the problem (1.1) is solved, and the partitioning of vectors and matrices induced by it. <p> This matrix block notation is similar to that commonly used by researchers working with approximate inverses <ref> [23, 9] </ref>, who would say that r and c are "sets of indices", and would write A ( r ; c ) instead of A r ; c .
Reference: [10] <author> Michele Benzi and Miroslav Tuma. </author> <title> Numerical experiments with two approximate inverse preconditioners. </title> <journal> BIT, </journal> <volume> 38(2) </volume> <pages> 234-241, </pages> <year> 1998. </year> <note> http://www.c3.lanl.gov/~benzi/Web_papers/bit.ps.gz. </note>
Reference-contexts: Despite these advantages, explicit preconditioners are not commonly used, since the numerical quality of an explicit preconditioner, as measured by the convergence rate of the iterative process, is usually worse than that of a comparable-cost implicit preconditioner [9, see also bibliography in it], <ref> [8, 10] </ref>. 1.2 Mesh and Mesh Decomposition Notation The preconditioners introduced in this dissertation rely on partitioning of the discretized domain where the problem (1.1) is solved, and the partitioning of vectors and matrices induced by it. This section reviews some of the related terminological and notational conventions.
Reference: [11] <author> R. Bramley. </author> <title> Row Projection Methods for Linear Systems. </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year> <type> Technical Report 881, </type> <institution> Center 216 for Supercomputing Research and Development, University of Illinois at Urbana--Champaign. </institution>
Reference-contexts: u yy = f: * Problem 4: e xy u xx +e xy u yy +(ye xy 200 (x+y +1))u x xe xy u y (100+1=(x+y +2))u = f: * Problem 8: u xx + u yy 100x 2 u x + 100u = f: The problem numbers come from <ref> [11] </ref>, where similar problems are used.
Reference: [12] <author> R. Bramley, H-C Chen, U. Meier, and A. Sameh. </author> <title> On some parallel preconditioned conjugate gradient methods. </title> <editor> In O. Axelsson, editor, </editor> <booktitle> Proc. of Conf. on Preconditioned Conjugate Gradient Methods, </booktitle> <institution> University of Nijmegen, </institution> <address> The Netherlands, </address> <year> 1989. </year>
Reference-contexts: When a parallel computer is used for solving the linear system, the ability to compute the preconditioner and to apply it in parallel is important as well. Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU <ref> [1, 13, 12, 19, 32] </ref>, efficient parallel implementation of preconditioned iterative methods remains difficult [37]. 1.1.4 Types of preconditioners.
Reference: [13] <author> R. Bramley and A. Sameh. </author> <title> Domain decomposition for parallel row projection algorithms. </title> <journal> Appl. Numer. Math., </journal> <volume> 8(4-5):303-315, </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: When a parallel computer is used for solving the linear system, the ability to compute the preconditioner and to apply it in parallel is important as well. Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU <ref> [1, 13, 12, 19, 32] </ref>, efficient parallel implementation of preconditioned iterative methods remains difficult [37]. 1.1.4 Types of preconditioners.
Reference: [14] <author> Randall Bramley and Vladimir Me~nkov. </author> <title> Parallel block preconditioners with low rank off-diagonal blocks. </title> <note> To appear in Parallel Comput.. </note>
Reference-contexts: Some experiments here also use a singular value decomposition of the off-diagonal blocks to define the low-rank 3 This section, in a slightly different form, was submitted for publication as part of <ref> [14] </ref>. 92 approximations [15, Section 4.1]. In Table 3.3, DLRA and SLRA stand for the preconditioners of Section 3.1 applied using the parallel methd introduced in Section 3.3.4.
Reference: [15] <author> Randall Bramley and Vladimir Me~nkov. </author> <title> Low rank off-diagonal block precondi-tioners for solving sparse linear systems on parallel computers. </title> <type> Technical Report 446, </type> <institution> Department of Computer Science, Indiana University, Bloomington, </institution> <year> 1996. </year> <note> http://www.cs.indiana.edu/ftp/techreports/TR446.ps.Z. </note>
Reference-contexts: For any low-rank approximation method, each non-zero off-diagonal low-rank block B kl can be represented as a sum of rank-one terms B kl = s=1 kl;s ; (2.1) where r kl is the rank of the block. 1 An earlier form of this chapter appeared as a section of <ref> [15] </ref>. 23 Here we examine three methods for defining the sets of vectors u kl;s and v kl;s , so that the blocks B kl approximate, in some way, off-diagonal blocks (ODBs) A kl of A: singular value decomposition, lumping, and a projection method. 2.1 Singular Value Decomposition For any real <p> This and related techniques can reduce the complexity of the matrix-vector 1 Parts of this chapter, in their earlier forms, appeared in [35] and <ref> [15] </ref>. 49 product needed by preconditioned iterative methods from O (n 2 ) to O (n log n). In this chapter we examine using low-rank approximations of off-diagonal blocks for sparse matrices, to define preconditioners for iterative methods. <p> Several suitable low-rank approximation schemes will be discussed later, in Sub section 3.2.2. Block Jacobi. For comparison, it is natural to consider the block-diagonal, also known as block Jacobi, preconditioner C J = D as the r = 0 case. Results: In <ref> [15, Section 5] </ref> we have shown that for r = 1 and the low-rank approximation using lumping (2.3) the spectrum of C 1 A is contained in [c=L sd ; 2] = [c p=L; 2]. <p> Therefore the fraction in (3.14) exists for any non-zero u not parallel to e. It may be estimated using the Fourier decomposition technique similar to that used in <ref> [15] </ref>, with the term for the constant function (zero harmonic) absent in both the numerator and the denominator. <p> The preconditioners used either no off-diagonal blocks (block-diagonal preconditioner), or the original off-diagonal blocks of A, or a low-rank approximation of the off-diagonal blocks. The low-rank approximation in most reported experiments were obtained using a projection method <ref> [15, Section 4.3] </ref> with the subspace X based on a low degree polynomial. <p> Some experiments here also use a singular value decomposition of the off-diagonal blocks to define the low-rank 3 This section, in a slightly different form, was submitted for publication as part of [14]. 92 approximations <ref> [15, Section 4.1] </ref>. In Table 3.3, DLRA and SLRA stand for the preconditioners of Section 3.1 applied using the parallel methd introduced in Section 3.3.4.
Reference: [16] <author> H.G. Cragon. </author> <title> A historical note on binary tree. </title> <institution> SIGARCH Comput. Archit. News, 18(4):3, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Multilevel domain partitioning makes this notation inconvenient, since numbering all subdomains so that it is possible to refer uniquely to submatrices without using multiple superscripts and subscripts, for example by extending the Ahnentafel index system <ref> [16] </ref> to the case of non-binary trees, would require complicated additional arithmetic.
Reference: [17] <author> James Demmel, Stanley Eisenstat, John Gilbert, Xiaoye Li, and Joseph Liu. </author> <title> A supernodal approach to sparse partial pivoting. </title> <type> Technical Report UCB//CSD-95-883, </type> <institution> Computer Science Division, University of California-Berkeley, </institution> <year> 1995. </year> <note> 217 http://www.nersc.gov/research/SCG/Sherry/simax95.ps. </note>
Reference-contexts: A nave implementation of direct solve requires storing O (n 2 ) values and performing O (n 3 ) operations. Although more sophisticated direct solving methods reduce these numbers, by utilizing the sparsity pattern of A <ref> [22, 18, 17] </ref>, the amount of storage and the number of operations required still grow as O (n ff ) with ff &gt; 1. The non-zero elements of L and U in positions where A has zeros are known as fill 1 .
Reference: [18] <author> I. Duff, A. Erisman, and J. Reid. </author> <title> Direct Methods for Sparse Matrices, 1 edition. </title> <publisher> Oxford Science Publications, </publisher> <year> 1986. </year>
Reference-contexts: A nave implementation of direct solve requires storing O (n 2 ) values and performing O (n 3 ) operations. Although more sophisticated direct solving methods reduce these numbers, by utilizing the sparsity pattern of A <ref> [22, 18, 17] </ref>, the amount of storage and the number of operations required still grow as O (n ff ) with ff &gt; 1. The non-zero elements of L and U in positions where A has zeros are known as fill 1 . <p> The cost of computing the L and U factors grows fast too: as O (n 2 ) or O (n 7=3 ), respectively. Reordering methods designed to reduce the amount of fill, such as minimum degree ordering, reverse Cuthill-McKee ordering, or the ordering resulting from spectral nested dissection <ref> [18, 40] </ref>, may reduce the storage and computation costs, but not in a radical way.
Reference: [19] <author> K. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations, 1 edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: When a parallel computer is used for solving the linear system, the ability to compute the preconditioner and to apply it in parallel is important as well. Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU <ref> [1, 13, 12, 19, 32] </ref>, efficient parallel implementation of preconditioned iterative methods remains difficult [37]. 1.1.4 Types of preconditioners.
Reference: [20] <author> Dennis Gannon, Shelby X. Yang, and Peter Beckman. </author> <title> User Guide for a Portable Parallel C++ Programming System, pC++. </title> <institution> Department of Computer Science and CICA, Indiana University, Bloomington, </institution> <note> IN, 1994. http://www.extreme.indiana.edu/sage/pcxx ug/pcxx ug.html. </note>
Reference-contexts: The program is written in pC++, a parallel extension <ref> [20] </ref> of C++, and run on a ten-processor SGI Power Challenge with 75 MHz R8000 chips running the 64 bit IRIX 6.1 operating system. This is a shared-memory machine with 2Gbytes of main memory; each processor accesses memory through a 16 Kbyte primary and 4 Mbyte secondary cache.
Reference: [21] <author> A. George and W-H Liu. </author> <title> The Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Engelwood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Since we are discussing only structurally symmetric matrices, this is an undirected graph. It has n vertices (one per unknown) and (nnz (A) n)=2 edges: an edge (i; j) for each non-zero matrix element a ij <ref> [40, 21] </ref>. The element a ij of the system matrix A is a coefficient representing contribution of the solution component x j to the right-hand-side component y i . <p> The equation for H jkl shows that the block G kl can be non-zero only if U kl is non-zero, which, in its turn, can be non-zero only if Q kl is non-zero. 2 Here we have made the no-cancellation assumption <ref> [21, Section 2.2.2] </ref>: exact can cellation in floating-point computation is disregarded. It follows then from Proposi tion 3.2 that G has the same non-zero block structure as Q. Corollary 3.3 If Q is strictly block upper or lower triangular, then so is G.
Reference: [22] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations, 2 edition. </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: A nave implementation of direct solve requires storing O (n 2 ) values and performing O (n 3 ) operations. Although more sophisticated direct solving methods reduce these numbers, by utilizing the sparsity pattern of A <ref> [22, 18, 17] </ref>, the amount of storage and the number of operations required still grow as O (n ff ) with ff &gt; 1. The non-zero elements of L and U in positions where A has zeros are known as fill 1 . <p> matrix A of rank r there are two sets of orthonormal real vectors fu s g s=1;:::;r and fv s g s=1;:::;r of appropriate dimensions, and a set of real positive numbers 1 2 : : : r such that A = s=1 s ; constituting its singular value decomposition <ref> [22] </ref>.
Reference: [23] <author> M. Grote and T. Huckle. </author> <title> Parallel preconditioning with sparse approximate inverses. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18(3) </volume> <pages> 838-853, </pages> <year> 1997. </year>
Reference-contexts: Applying an explicit preconditioner can usually be implemented by more straightforward and efficient (in the sense of flops-per-second performance) code than an implicit preconditioner, and it is more amenable to parallel implementation <ref> [43, 23] </ref>. <p> This matrix block notation is similar to that commonly used by researchers working with approximate inverses <ref> [23, 9] </ref>, who would say that r and c are "sets of indices", and would write A ( r ; c ) instead of A r ; c . <p> Techniques commonly used for generating approximate inverses allow M to have non-zeros only in certain positions. The sparsity pattern of M may be determined either statically, e.g. based on the sparsity pattern of A or A d , or dynamically <ref> [23] </ref>. Usually the approximate inverse is designed to minimize a certain function, such as the Frobenius norm kAM Ik F . Since the BLOB framework provides a limited-storage approximate matrix representation format, it can be used to obtain not only an approximate LU factorization, but also an approximate inverse.
Reference: [24] <author> L. Hageman and D. Young. </author> <title> Applied Iterative Methods, 1 edition. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year> <month> 218 </month>
Reference-contexts: Such systems commonly appear as the result of discretizing a boundary problem for a partial differential equation (PDE) in a two- or three-dimensional domain by a mesh method such as finite elements or finite differences <ref> [24, 39] </ref>, as well as in other applications. This dissertation will be limited to the case of a single PDE, where the discretization results in one unknown per mesh node. <p> While a basic stationary one-step iterative method uses only the last residual to obtain the update s k , accelerated iterative methods such as Chebyshev, conjugate gradients (CG), or GMRES <ref> [24, 2, 44] </ref> use a longer sequence of the residuals and/or updates. Iterative techniques are attractive for solving large sparse systems because one iteration is much cheaper than solving the Ax = y directly.
Reference: [25] <author> B. Hendrickson and R. Leland. </author> <title> The Chaco User's Guide Version 1.0. </title> <type> Techni--cal Report SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, N.M., </address> <month> October </month> <year> 1993. </year>
Reference-contexts: This test problem was chosen because simple block diagonal (Jacobi) preconditioning allows convergence, and one purpose of our testing is to compare the parallelization of the low-rank approximation method with Jacobi preconditioning. The system was partitioned into p = 8 subdomains using a spectral method from the Chaco <ref> [25, 26] </ref> package for domain decomposition. Tests were run using p = 8 and 1,2,4, and 8 processors.
Reference: [26] <author> B. Hendrickson and R. Leland. </author> <title> An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 16(2) </volume> <pages> 452-469, </pages> <year> 1995. </year>
Reference-contexts: The expression "a mesh node" will be used to 16 refer to a vertex of the matrix graph. The preconditioned techniques introduced in this dissertation make use of a partitioning of the set of unknowns into groups, normally derived from applying some graph partitioning algorithm <ref> [4, 30, 26] </ref> to the matrix graph. <p> This test problem was chosen because simple block diagonal (Jacobi) preconditioning allows convergence, and one purpose of our testing is to compare the parallelization of the low-rank approximation method with Jacobi preconditioning. The system was partitioned into p = 8 subdomains using a spectral method from the Chaco <ref> [25, 26] </ref> package for domain decomposition. Tests were run using p = 8 and 1,2,4, and 8 processors.
Reference: [27] <author> M. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <institution> J. Res. Nat. Bur. Standards, B-49:409-436, </institution> <year> 1952. </year>
Reference-contexts: The ability to make the absolute value of the polynomial small everywhere on (AC 1 ) implies that the method makes the residual small as well <ref> [27, 2] </ref>. Convergence rate estimates [2, 5] are easy to obtain when the spectrum of AC 1 is positive real, or when it is contained in an ellipse within the Re &gt; 0 part of the complex plain.
Reference: [28] <author> Jean-Francois Hetu and Dominque Pelletier. </author> <title> Fast, adaptive finite element scheme for viscous incompressible flow. </title> <journal> AIAA Journal, </journal> <volume> 30(11) </volume> <pages> 2677-2681, </pages> <year> 1992. </year>
Reference-contexts: Then BLAS routines were applied in parallel to all the blocks of I + G in a given block column. Test problem: The results presented in this paper are obtained on a test problem from the fluid dynamics steady state modeling of a backward facing step in 2D <ref> [28] </ref>. The resulting matrix is of order n = 20284 and has 452752 non-zeros. This test problem was chosen because simple block diagonal (Jacobi) preconditioning allows convergence, and one purpose of our testing is to compare the parallelization of the low-rank approximation method with Jacobi preconditioning.
Reference: [29] <author> R. Horn and C. Johnson. </author> <title> Matrix Analysis, 1 edition. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference: [30] <author> George Karypis and Vipin Kumar. METIS. </author> <title> A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices. </title> <type> Version 3.0.3. </type> <institution> Dept. of Computer Science / Army HPC Research Center, University of Minnesota, </institution> <year> 1997. </year> <note> http://www-users.cs.umn.edu/~karypis/metis/metis/files/manual.ps. </note>
Reference-contexts: The expression "a mesh node" will be used to 16 refer to a vertex of the matrix graph. The preconditioned techniques introduced in this dissertation make use of a partitioning of the set of unknowns into groups, normally derived from applying some graph partitioning algorithm <ref> [4, 30, 26] </ref> to the matrix graph. <p> The hierarchical domain parti tioning is obtained by a recursive spectral bisection [4]. 114 * is the set of nodes of some arbitrary mesh. The hierarchical domain parti-tioning is obtained by METIS <ref> [30] </ref>, each subdomain being partitioned into 2, 3 or 5 parts at each level. <p> Using a general-purpose mesh partitioner: Domain partitioning trees T-I and T-II can only be created for a regular rectangular grid. How will storage requirements change if a general mesh partitioning algorithm, such as METIS <ref> [30] </ref> is used? We have used pmetis|the recursive binary partitioning mode of METIS|to 186 partition the same n-node mesh into n=n 0 parts.
Reference: [31] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming I, Fundamental Algorithms, 3 edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1997. </year> <month> 219 </month>
Reference-contexts: This means that if all leaves of the DPT are at the same level, in the new ordering the data associated with the mesh nodes in the leaves are arranged the same way they would be if the DPT were stored using the level-order sequential representation <ref> [31, p. 350] </ref>.
Reference: [32] <author> B. Kumar, K. Eswar, P. Sadayappan, and C.-H. Huang. </author> <title> A clustering algorithm for parallel sparse Cholesky factorization. Parallel Process. </title> <journal> Lett., </journal> <volume> 5(4) </volume> <pages> 685-696, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: When a parallel computer is used for solving the linear system, the ability to compute the preconditioner and to apply it in parallel is important as well. Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU <ref> [1, 13, 12, 19, 32] </ref>, efficient parallel implementation of preconditioned iterative methods remains difficult [37]. 1.1.4 Types of preconditioners.
Reference: [33] <author> Harry R. Lewis and Larry Denenberg. </author> <title> Data Structures and Their Algorithms. </title> <publisher> HarperCollins Publishers, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [34] <author> Thomas J. Loos. </author> <title> A New Model for Solving the Data Distribution Problem. </title> <type> PhD dissertation, </type> <institution> Indiana University, Bloomington, </institution> <year> 1996. </year> <type> Technical Report 471, </type> <institution> Computer Science Department, Indiana University, Bloomington. </institution> <note> http://www.cs.indiana.edu/ftp/techreports/index.html. </note>
Reference-contexts: Although a BLOB tree node - = ( r ; c ; t) may have children only if r or c have children, it is not required to. This is similar to the quadtree structure or the hypermatrix storage scheme ([18, Section 12.10]; <ref> [34] </ref>), where an all-zero block is not partitioned into parts any further either. Our structure is more complex than the standard quadtree though, since it uses an additional leaf type (LOWRANK), and a given matrix block may be partitioned into a number of parts not equal to 4.
Reference: [35] <author> Vladimir Me~nkov. </author> <title> Solving block linear systems with low-rank off-diagonal blocks is easily parallelizable. </title> <booktitle> In Proc. Copper Mountain Conference on Iterative Methods, </booktitle> <address> Copper Mountain, CO, </address> <note> volume I, 1996. http://ftp.cs.indiana.edu/pub/vmenkov/lowrank/cm96.dvi. </note>
Reference-contexts: This and related techniques can reduce the complexity of the matrix-vector 1 Parts of this chapter, in their earlier forms, appeared in <ref> [35] </ref> and [15]. 49 product needed by preconditioned iterative methods from O (n 2 ) to O (n log n). In this chapter we examine using low-rank approximations of off-diagonal blocks for sparse matrices, to define preconditioners for iterative methods.
Reference: [36] <author> Eric Michielssen, Amir Boag, and W.C.Chew. </author> <title> Scattering from elongated objects direct solution in O(N log 2 N ) operations. </title> <journal> IEEE Proc. Microwaves Antennas Propagat., </journal> <volume> 143(4) </volume> <pages> 277-283, </pages> <year> 1996. </year>
Reference-contexts: partitioning the domain into a small number of parts comparable to the number of CPUs in a parallel computer and treating all non-zero off-diagonal blocks of the resulting partitioned system the same way, more subtle hierarchical 19 domain decomposition techniques are also used in some applications, such as compu-tational electromagnetics <ref> [36] </ref>. In Chapter 4, we describe a general framework (BLOB: Bigger Low-rank Off-diagonal Blocks) for using different levels of approximation for different parts of the matrix, based on the matrix non-zero structure, i.e. ultimately on the geometry of the underlying mesh. <p> In its design we try to exploit an observation that has long been used by integral equations researchers <ref> [42, 36] </ref> to reduce storage and computation costs: When the points ~a 1 and ~a 2 in the physical domain are close to each other but far from the point ~ b, the contribution of the sources located at ~a 1 and ~a 2 to the solution at ~ b can <p> In matrix terms this approximation means using some kind of multilevel matrix decomposition, like the one proposed by Michielssen and Boag <ref> [36] </ref>, where a block corresponding to interaction between "well-separated" domains is approximated by a low-rank expression.
Reference: [37] <author> Vijay K. Naik and Vladimir Me~nkov. </author> <title> Distribution and scheduling strategies for parallel sparse system solvers in device simulation applications. </title> <type> Technical Report RC 20667, </type> <institution> IBM Research Division, T. 220 J. Watson Research Center, Yorktown Heights, </institution> <year> 1997. </year> <note> Available via http://www.watson.ibm.com:8080/search_paper.shtml. </note>
Reference-contexts: What is even more important, unpreconditioned iterative methods simply fail to converge for many problems, especially when A is not symmetric <ref> [43, 37] </ref>. To improve the convergence rate of iterative methods, they are commonly used with preconditioners. <p> Despite the significant efforts made to develop parallel versions of such popular preconditioning methods as block SSOR or ILU [1, 13, 12, 19, 32], efficient parallel implementation of preconditioned iterative methods remains difficult <ref> [37] </ref>. 1.1.4 Types of preconditioners.
Reference: [38] <author> Roy Nicolaides and Noel Walkington. </author> <title> MAPLE: a comprehensive introduction. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: Substituting -d from (4.17) and solving the recurrences, using techniques described in any good textbook of mathematics for algorithm analysis such as [41] and implemented by a computer algebra system such as Maple <ref> [38] </ref> (function rsolve), gives -c (n) = 8log n uv 0 10n 0 ; ee 15 log n 0 : (4.19) (b) Side-adjacent domains: For the leaf-sized blocks with n 0 mesh nodes, the matrix is dense, and the base-case values are the same as in (4.18).
Reference: [39] <author> J. M. Ortega and W. C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Such systems commonly appear as the result of discretizing a boundary problem for a partial differential equation (PDE) in a two- or three-dimensional domain by a mesh method such as finite elements or finite differences <ref> [24, 39] </ref>, as well as in other applications. This dissertation will be limited to the case of a single PDE, where the discretization results in one unknown per mesh node. <p> In Section 3.2, spectral properties of the DLRA preconditioner for a sample problem are studied. A new well-parallelizable method for solving C DIR z = r or C SOR z = r, using the Sherman-Morrison-Woodbury formula <ref> [39] </ref> and having some of the advantages of direct inverse approximations in its application is described in Section 3.3. <p> Block columns in (3.19) and (3.20) corresponding to zero blocks Q kl are absent (have no columns). Since B = D + U V T , the Sherman-Morrison-Woodbury (SMW) formula <ref> [39] </ref> gives B 1 = (D + U V T ) 1 = D 1 D 1 U (I + G) 1 V T D 1 ; (3.21) with G = V T D 1 U of order M .
Reference: [40] <author> S. Pissanetzky. </author> <title> Sparse Matrix Technology. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1984. </year>
Reference-contexts: The coefficient in the big O is determined by the mesh geometry; common values are 5 to 10 in a two-dimensional problem or 7 to 30 in a three-dimensional problem. Special data structures <ref> [40] </ref> are normally used to store a sparse matrix, so that only non-zero elements, or potential non-zeros, are stored. This section gives a brief overview of the methods commonly used for solving such linear systems. <p> The cost of computing the L and U factors grows fast too: as O (n 2 ) or O (n 7=3 ), respectively. Reordering methods designed to reduce the amount of fill, such as minimum degree ordering, reverse Cuthill-McKee ordering, or the ordering resulting from spectral nested dissection <ref> [18, 40] </ref>, may reduce the storage and computation costs, but not in a radical way. <p> Since we are discussing only structurally symmetric matrices, this is an undirected graph. It has n vertices (one per unknown) and (nnz (A) n)=2 edges: an edge (i; j) for each non-zero matrix element a ij <ref> [40, 21] </ref>. The element a ij of the system matrix A is a coefficient representing contribution of the solution component x j to the right-hand-side component y i . <p> BLOB matrix addition: Similarly to the addition of conventional sparse matrices <ref> [40] </ref>, adding two BLOB matrices requires the number of operations not exceeding the sum of non-zero counts for the matrices being added.
Reference: [41] <author> Paul Walton Purdom and Cynthia A. Brown. </author> <title> The Analysis of Algorithms. </title> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Substituting -d from (4.17) and solving the recurrences, using techniques described in any good textbook of mathematics for algorithm analysis such as <ref> [41] </ref> and implemented by a computer algebra system such as Maple [38] (function rsolve), gives -c (n) = 8log n uv 0 10n 0 ; ee 15 log n 0 : (4.19) (b) Side-adjacent domains: For the leaf-sized blocks with n 0 mesh nodes, the matrix is dense, and the base-case
Reference: [42] <author> V. Rokhlin. </author> <title> Rapid solution of integral equations of classical potential theory. </title> <journal> Journal of Computational Physics, </journal> <volume> 60 </volume> <pages> 187-207, </pages> <year> 1985. </year>
Reference-contexts: In its design we try to exploit an observation that has long been used by integral equations researchers <ref> [42, 36] </ref> to reduce storage and computation costs: When the points ~a 1 and ~a 2 in the physical domain are close to each other but far from the point ~ b, the contribution of the sources located at ~a 1 and ~a 2 to the solution at ~ b can
Reference: [43] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS Publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: What is even more important, unpreconditioned iterative methods simply fail to converge for many problems, especially when A is not symmetric <ref> [43, 37] </ref>. To improve the convergence rate of iterative methods, they are commonly used with preconditioners. <p> This approach is known as ILU (l): ILU with l levels of fill <ref> [43] </ref>. The amount of non-zeros in the approximate L and U factors produced by ILU (l) depends on l. <p> Levels of fill higher than l = 2 or 3 are rarely employed in practice. Another ILU approach, incomplete factorization by value, takes the values of the fill elements into accounts, and discards those which are smaller than the specified threshold <ref> [43] </ref>. It has been proposed that the quality of the preconditioning may be improved if, instead of being discarded, fill elements that don't fit into the target sparsity pattern or are too small are added to the diagonal elements of the Schur complement. <p> Applying an explicit preconditioner can usually be implemented by more straightforward and efficient (in the sense of flops-per-second performance) code than an implicit preconditioner, and it is more amenable to parallel implementation <ref> [43, 23] </ref>. <p> As discussed in Section 4.5, the BLOB framework can yield not only approximate LU factorization, but also an approximate inverse of A|an explicit preconditioner. 20 However, just like the classic sparsity-pattern-constrained approximate inverses <ref> [43, 7] </ref>, this approach appears not to be competitive with its implicit counterpart: for a given storage cost, the quality of an approximate-inverse preconditioner is worse than that of an ILU preconditioner with the same sparsity pattern (in the classic case) or the same BLOB structure (in the BLOB case). 1.4 <p> This similarity is not accidental: the Sherman-Morrison-Woodbury formula in (3.21) can be derived with the help of Schur complement <ref> [43] </ref>. Consider the system (3.15,3.16) (D + Q)x = y with Q = U V T . <p> Implementation details: The matrix D is obtained from an ILU factorization of the corresponding diagonal blocks with 3 levels of fill <ref> [43] </ref>, computed in parallel.
Reference: [44] <author> Y. Saad and M. Schultz. </author> <title> Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7(3) </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: While a basic stationary one-step iterative method uses only the last residual to obtain the update s k , accelerated iterative methods such as Chebyshev, conjugate gradients (CG), or GMRES <ref> [24, 2, 44] </ref> use a longer sequence of the residuals and/or updates. Iterative techniques are attractive for solving large sparse systems because one iteration is much cheaper than solving the Ax = y directly. <p> The problems were solved using the GMRES iterative method <ref> [44] </ref> with the restart value (dimension of the Krylov space) 20. In all experiments, the initial guess x 0 was zero, and the right-hand side y a random vector with the element values uniformly distributed between 0 and 1.
Reference: [45] <author> The MathWorks Inc. </author> <title> Application Program Interface Guide, </title> <type> Version 5, </type> <month> January </month> <year> 1997. </year> <title> MEX and Matlab Engine. </title> <type> 221 </type>
Reference-contexts: To improve the performance of the Matlab prototype we converted the computation-intensive parts of the code, such as the basic BLOB arithmetic operations and BLOB 198 factorization, into MEX files <ref> [45] </ref>: C code that can be invoked from Matlab. C++ code: Later the entire code for the most important preconditioner|BLOB LU|was converted into a separate C++ program. It still used the Matlab Engine, but only for getting the input parameters.
Reference: [46] <author> The MathWorks Inc. </author> <title> Matlab User's Guide, </title> <type> Version 5, </type> <month> January </month> <year> 1997. </year> <title> General purpose guide to Matlab. </title>
Reference-contexts: Table 4.2 summarizes possible combinations of nonzero argument types, and shows the type the result will have unless constrained by t result = LOWRANK or ZERO. In a practical implementation of the BLOB multiplication, such as described by the pseudocode below based on the Matlab <ref> [46] </ref> prototype of our program, it would be inefficient to obtain the multiplication result of a superior type and then convert it to an inferior (LOWRANK) type. <p> Thus it is O ( uv (n) log n) = O (n log 2 n). Computing a BLOB approximate inverse is more costly then merely a BLOB LU factorization, but is of the same order. 4.8 Implementation Details Matlab prototype: We developed first a flexible Matlab <ref> [46] </ref> prototype program, which could only work with problems on a rectangular mesh and DPTs T-I and T-II.
Reference: [47] <author> Alle-Jan Van der Veen. </author> <title> A Schur method for low-rank matrix approximation. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 17(1) </volume> <pages> 139-160, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: This approximation is not practical, because calculating singular vectors is com-putationally expensive, even though less costly approximate SVD methods exist <ref> [47] </ref>. Furthermore, the non-zero off-diagonal blocks of matrices appearing in many typical discretized PDE systems don't have a few large singular values and many small ones. The majority of the non-zero singular values often have similar values.
Reference: [48] <author> H. van der Vorst. </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13(2) </volume> <pages> 631-644, </pages> <year> 1992. </year>
Reference-contexts: A^s x i = x i1 + ff i ^p + ! i ^s check convergence; continue if necessary for continuation it is necessary that ! i 6= 0 endfor 91 3.5 Numerical Results for DLRA and SLRA The numerical experiments use the preconditioners within a biconjugate gradients stabilized (Bi-CGSTAB) <ref> [48, 5] </ref> iterative method (Figure 3.2). The program is written in pC++, a parallel extension [20] of C++, and run on a ten-processor SGI Power Challenge with 75 MHz R8000 chips running the 64 bit IRIX 6.1 operating system.
Reference: [49] <author> David S. Wise. </author> <title> Undulant-block elimination and integer-preserving matrix inversion. </title> <journal> Sci. Comput. Programming, </journal> <volume> 32, </volume> <month> July </month> <year> 1998. </year> <note> In press. http://www.cs.indiana.edu/ftp/techreports/TR418.ps.Z. </note>
Reference-contexts: Our structure is more complex than the standard quadtree though, since it uses an additional leaf type (LOWRANK), and a given matrix block may be partitioned into a number of parts not equal to 4. A BLOB tree is equivalent to a quadtree of <ref> [49] </ref> if it is based on a perfectly balanced binary domain partitioning tree and it has no LOWRANK leaves.
Reference: [50] <author> David S. Wise and Jeremy D. Frens. </author> <title> Square blocking for matrix-multiplication. </title> <type> Technical Report 449, </type> <institution> Department of Computer Science, Indiana University, Bloomington, </institution> <note> IN, 1996. http://www.cs.indiana.edu/ftp/techreports/TR418.ps.Z. </note>
Reference-contexts: This dissertation paid no attention to the parallelism aspect of BLOB precondi-tioners. However, it seems likely that there are ways to parallelize generation and application of such preconditioners, possibly using techniques proposed elsewhere <ref> [50, 6] </ref> to parallelize multiplication of quadtree matrices and LU factorization using such matrices. It is also highly desirable to find out the reason of disappointingly poor performance of BLOB approximate inverse preconditioners, and find a way to improve it.

References-found: 50

