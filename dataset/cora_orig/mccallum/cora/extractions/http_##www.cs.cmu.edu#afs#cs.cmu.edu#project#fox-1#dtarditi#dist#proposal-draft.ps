URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/fox-1/dtarditi/dist/proposal-draft.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/dtarditi/web/dtarditi-home.html
Root-URL: 
Title: Using program structure to guide optimization in the presence of first-class functions.  
Author: David Tarditi 
Degree: (Thesis proposal)  
Date: November 29, 1994  
Abstract: Compilers for functional languages such as Standard ML can do a good job compiling programs, especially programs that perform symbolic computation. However, they often do a poor job on programs in a wide range of real-world application domains, such as systems programming and scientific computing. One reason for this is that these compilers are not sensitive to program structure, that is, recursions (loops), the code executed during the evaluation of a recursive function (loop bodies), and recursive-function nesting (loop nesting). In part, this is because determining the code executed during evaluation of a recursive function and determining recursive-function nesting is difficult when functions are first-class values. The body of a recursive function may simply be a call to an unknown function. Recursive-function nesting can be difficult to determine because functions may be passed off to be used elsewhere. However, even in situations where the program structure is easy to determine, the traditional emphasis on compiling function calls well instead of compiling recursions well has led to compilers that simply ignore recursions. In my thesis, I propose to demonstrate that it is practical for a realistic compiler to determine the structure of functional programs and to show that compilers for functional languages need to be sensitive to this information to do a good job compiling systems and scientific programs. Furthermore, I will show that being sensitive to program structure improves compilation of symbolic programs as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </institution> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year> <note> ACM. </note>
Reference: [2] <institution> San Francisco, California, </institution> <month> January </month> <year> 1995. </year> <note> ACM. </note>
Reference: [3] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: In some sense, this is a compilation strategy for functional languages organized around program structure: take a higher-order program, remove all higher-order features by closure converting it, compile it to 3-address code, and then use traditional optimization techniques, such as those described in <ref> [3] </ref>. However, Bartlett does not argue that being sensitive to program structure is essential to compiling functional languages well. He shows that Scheme!C produces code which is competitive with a native code compiler for Lisp. LIAR [45] makes use of an analysis similar to Shivers' control-flow 0 analysis.
Reference: [4] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Functional languages like Standard ML [37] are a good thing. They have many features which offer significant programming benefits, such as module systems, static typing, garbage collection, and first-class functions. Compilers for these languages, such as Standard ML of New Jersey (SML/NJ) <ref> [4, 51] </ref> and Orbit [31, 30] can do a good job compiling programs, especially programs that perform symbolic computations. However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation [44, 59, 23, 39]. <p> program points with higher nesting depths. 3.2 Traditional optimizations for functional programs This section describes how traditional optimizations for functional programs, such as inlining, uncurrying, and closure conversion, can be improved by using information about program structure. 3.2.1 Inlining Appel has developed an effective inlining strategy for the SML/NJ compiler <ref> [4] </ref>. It makes programs about 25% faster on average (compared to doing no inlining), while expanding program sizes by about 20% [4, p. 191]. The basic idea is to inline a function if that does not increase the size of the program too much. <p> It makes programs about 25% faster on average (compared to doing no inlining), while expanding program sizes by about 20% <ref> [4, p. 191] </ref>. The basic idea is to inline a function if that does not increase the size of the program too much. The optimizer makes several passes over the entire program, inlining functions in each pass. <p> In addition, they will also benefit symbolic programs. Profiling shows that most of the benchmarks used by Appel <ref> [4] </ref> make significant use of arrays. Table 1 lists the most expensive routine for each of these benchmarks. For four out of the six benchmarks, the most expensive routine was an array-intensive recursive function or part of an array-intensive recursive function. <p> I decided to use Standard ML because it best met these criteria. First, it has a good compiler, the SML/NJ compiler, which is a state-of-the-art compiler for functional languages. The SML/NJ compiler is a real system with many man-years of effort behind it <ref> [4, 51] </ref>. It is competitive with and often better than compilers for other functional languages [59, 23]. Second, Standard ML has an interesting set of benchmarks, including the SML/NJ compiler and an implementation of TCP/IP. Third, Standard ML is statically-typed and call-by-value. <p> They make use of information about program structure in limited ways, and do not appear to be organized around the principle of focusing compilation on recursive functions or loops. The SML/NJ compiler <ref> [4, 51] </ref> is a state-of-the-art compiler for functional languages. It uses a type-based data representation analysis, which attempts to keep values unboxed when possible [32, 51]. It has a sophisticated global optimization phase, which does inlining, - reduction, uncurrying, constant-folding, hoisting, and dead-code elimination.
Reference: [5] <author> Andrew W. Appel, James S. Mattson, and David Tarditi. </author> <title> A lexical analyzer generator for Standard ML. Distributed with Standard ML of New Jersey, </title> <year> 1989. </year>
Reference-contexts: Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple mented by Gerard Huet, processing some axioms of geometry. Lawrence Livermore Loops Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi <ref> [5] </ref>, processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade [40], running 50 generations of a glider gun. It is implemented using lists. Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule [23].
Reference: [6] <author> Jonathan M. Asuru. </author> <title> Optimization of array subscript range checks. </title> <journal> Letters on Programming Langauges and Systems, </journal> <volume> 1(2) </volume> <pages> 109-118, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Hence, we must check whether an array index is in-bounds before doing an array access. Chow [11] has shown that these checks greatly interfere with optimization, and have a significant performance penalty. It is crucial that these checks be eliminated. A number of papers have addressed bounds-check elimination <ref> [36, 20, 6] </ref>. However, it is unclear how well the techniques they describe perform on realistic programs. The following example, integer matrix multiply, demonstrates the usefulness of these optimizations. Figures 19 presents a straightforward implementation of 2-dimensional integer arrays with matrix multiplication, written in terms of 1-dimensional arrays 4 .
Reference: [7] <author> Joel F. Bartlett. SCHEME!C: </author> <title> A Portable Scheme-to-C Compiler. </title> <type> Technical Report 89/1, </type> <institution> DEC Western Research Laboratory, </institution> <address> 100 Hamilton Avenue, Palo Alto, CA 94301 USA, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: This avoids saving and restoring register values across the function calls. Orbit also demonstrated that a continuation-passing-style language was a practical intermediate language for compilers. Bartlett's Scheme!C compiler <ref> [7] </ref> compiles Scheme to C. It produces readable C code which is suitable for optimization by a C compiler. Thus, it gets all the traditional loop optimizations done by C compilers for free.
Reference: [8] <author> Edoardo Biagioni, Robert Harper, Peter Lee, and Brian Milnes. </author> <title> Signatures for a network protocol stack: A systems application of Standard ML. </title> <booktitle> In LFP '94 [34], </booktitle> <pages> pages 55-64. </pages>
Reference-contexts: However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming <ref> [8] </ref> and scientific computation [44, 59, 23, 39]. <p> The input is the sample session from Section 7.5 of [12]. FoxNet The FoxNet <ref> [8] </ref> is an implementation of TCP/IP in an extended version of Standard ML. Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple mented by Gerard Huet, processing some axioms of geometry. Lawrence Livermore Loops Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R.
Reference: [9] <author> Rodney A. Brooks, Richard P. Gabriel, and Guy L. Steele,Jr. </author> <title> An optimizing compiler for lexically scoped LISP. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 261-275, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1982. </year> <note> ACM. </note>
Reference-contexts: It also does tail-recursion optimization: it reuses stack frames for tail calls, and self-tail calls turn into jumps. Epic [28] compiled a dynamically-scoped version of Lisp. It did loop-invariant removal, but does not appear to make any other use of information about program structure. The S-1 compiler <ref> [9] </ref> was an optimizing compiler for a statically-scoped version of Lisp targeted at a vector processor. It was intended to compete with the S-1 PASCAL and FORTRAN compilers for quality of compiled numerical code. However, the paper describes no loop-based optimizations.
Reference: [10] <author> Rodney A. Brooks, David B. Posner, James L. McDonald, Jon L. White, Eric Benson, and Richard P. Gabriel. </author> <title> Design of an optimizing dynamically retargetable compiler for common LISP. </title> <booktitle> In Proceedings of the 1986 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 67-86, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1986. </year> <note> ACM. </note>
Reference-contexts: The CMU Python Common Lisp compiler [35] tries to avoid unnecessary unconditional branches when generating code for loops. However, no other optimizations for recursive functions or loops are described. The Lucid Portable LISP compiler <ref> [10] </ref> does the following global optimizations: constant folding, dead-code elimination, tail-recursion optimization, and inlining of lambda expressions used only once. It also does tail-recursion optimization: it reuses stack frames for tail calls, and self-tail calls turn into jumps. Epic [28] compiled a dynamically-scoped version of Lisp.
Reference: [11] <author> Frederick C. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> Decem-ber </month> <year> 1983. </year> <type> Technical Report No. </type> <pages> 83-254. </pages>
Reference-contexts: If the induction variable is an array index, we can expect range analysis to take care of this. 5. Bounds-check elimination. Standard ML is a type-safe language. Hence, we must check whether an array index is in-bounds before doing an array access. Chow <ref> [11] </ref> has shown that these checks greatly interfere with optimization, and have a significant performance penalty. It is crucial that these checks be eliminated. A number of papers have addressed bounds-check elimination [36, 20, 6]. However, it is unclear how well the techniques they describe perform on realistic programs.
Reference: [12] <author> Rance Cleaveland, Joachim Parrow, and Bernhard Steffen. </author> <title> The Concurrency Workbench: A semantics-based tool for the verification of concurrent systems. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 15(1) </volume> <pages> 36-72, </pages> <month> January </month> <year> 1993. </year> <month> 59 </month>
Reference-contexts: Disabling is better, because it shows how much slower programs would run without that optimization. 7 For example, the virtual-to-physical page-mapping made by the operating system can change the memory-system performance of a program. 27 Program Description CW The Concurrency Workbench <ref> [12] </ref> is a tool for analyzing networks of finite state processes expressed in Milner's Calculus of Communicating Systems. The input is the sample session from Section 7.5 of [12]. FoxNet The FoxNet [8] is an implementation of TCP/IP in an extended version of Standard ML. <p> virtual-to-physical page-mapping made by the operating system can change the memory-system performance of a program. 27 Program Description CW The Concurrency Workbench <ref> [12] </ref> is a tool for analyzing networks of finite state processes expressed in Milner's Calculus of Communicating Systems. The input is the sample session from Section 7.5 of [12]. FoxNet The FoxNet [8] is an implementation of TCP/IP in an extended version of Standard ML. Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple mented by Gerard Huet, processing some axioms of geometry. Lawrence Livermore Loops Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R.
Reference: [13] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, Livermore, </institution> <address> CA, </address> <month> February </month> <year> 1978. </year>
Reference-contexts: It is implemented using lists. Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule [23]. PIA A perspective inversion algorithm [62], deciding the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a realistic FORTRAN benchmark <ref> [13] </ref>, translated into ID [17], and then translated into Standard ML by Lal George. SML/NJ SML/NJ is a compiler for Standard ML. Sparse Gaussian eimination on sparse matrices [59]. The matrices were 1000x1000 in size. Only non-zero elements are stored, with roughly 2 elements per row.
Reference: [14] <author> Olivier Danvy and Julia L. Lawall. </author> <title> Back to direct style II: First-class continuations. </title> <booktitle> In LFP '92 [33], </booktitle> <pages> pages 299-310. </pages>
Reference-contexts: I will convert programs to continuation-passing-style (CPS) immediately before passing them to the closure conversion phase. The choice of whether to use a direct-style or CPS intermediate language is largely an engineering choice. Theoretical studies have shown that the two representations are inter-convertible <ref> [14, 47, 48] </ref>. Thus, in principle, neither representation is more powerful than the other. I have chosen to use a direct-style intermediate language because I believe it is more amenable to analyzing program structure. However, using a direct-style language complicates inlining, that is, fi-reduction, [48].
Reference: [15] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs with intensive heap allocation. </title> <type> Technical Report CMU-CS-93-227, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: It uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap rather than on a call stack. This leads to poor memory-system performance on many machines <ref> [15, 16] </ref>. This is not a problem for demonstrating the thesis, though, since the end result will be to understate the relative performance benefits of better global optimization. The optimizations I am proposing will reduce mostly instruction counts, and not eliminate large numbers of function calls.
Reference: [16] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs with copying garbage collection. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Principles of Programming languages, </booktitle> <address> Portland, Oregon, </address> <month> January </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: It uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap rather than on a call stack. This leads to poor memory-system performance on many machines <ref> [15, 16] </ref>. This is not a problem for demonstrating the thesis, though, since the end result will be to understate the relative performance benefits of better global optimization. The optimizations I am proposing will reduce mostly instruction counts, and not eliminate large numbers of function calls.
Reference: [17] <author> K. Ekanadham and Arvind. </author> <title> SIMPLE: An exercise in future scientific programming. Technical Report Computation Structures Group Memo 273, </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <month> July </month> <year> 1987. </year> <note> Simultaneously published as IBM/T. J. </note> <institution> Watson Research Center Research Report 12686, Yorktown Heights, NY. </institution>
Reference-contexts: Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule [23]. PIA A perspective inversion algorithm [62], deciding the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a realistic FORTRAN benchmark [13], translated into ID <ref> [17] </ref>, and then translated into Standard ML by Lal George. SML/NJ SML/NJ is a compiler for Standard ML. Sparse Gaussian eimination on sparse matrices [59]. The matrices were 1000x1000 in size. Only non-zero elements are stored, with roughly 2 elements per row.
Reference: [18] <author> Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias Felleisen. </author> <title> The essence of compiling with continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 237-247, </pages> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Its values are integers, booleans, tuples, lists, and functions. It does not have exceptions, references, datatype declarations, or modules. Without loss of generality, we will analyze a restricted subset of Flow ML, called A-Normal-Form <ref> [18] </ref> ML (ANF ML) where every intermediate value is named and pattern matching has also been desugared. Like Flow ML, ANF ML is polymorphically typed. ANF ML is not a continuation-passing-style language. Figures 2 and 3 present concrete and abstract syntax for ANF ML, respectively. <p> Also, we require that all bound variables be unique. It is straightforward to convert Flow ML programs to semantically equivalent ANF ML programs <ref> [18] </ref>. An example of a Flow ML program and an equivalent ANF ML program is given in Figures 4 and 5, respectively. A definitional interpreter for ANF ML is given in Appendix C.
Reference: [19] <author> Martin L. Griss and Anthony C. Hearn. </author> <title> Notes on a portable LISP compiler. </title> <journal> Software Practice and Experience, </journal> <volume> 11 </volume> <pages> 541-605, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: It was intended to compete with the S-1 PASCAL and FORTRAN compilers for quality of compiled numerical code. However, the paper describes no loop-based optimizations. The Portable Standard Lisp (PSL) compiler <ref> [19] </ref> translated LISP programs to instructions for an abstract Standard LISP machine. It then optimized these instructions.
Reference: [20] <author> Rajiv Gupta. </author> <title> A fresh look at optimizing array bound checking. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 272-282, </pages> <address> White Plains, New York, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Hence, we must check whether an array index is in-bounds before doing an array access. Chow [11] has shown that these checks greatly interfere with optimization, and have a significant performance penalty. It is crucial that these checks be eliminated. A number of papers have addressed bounds-check elimination <ref> [36, 20, 6] </ref>. However, it is unclear how well the techniques they describe perform on realistic programs. The following example, integer matrix multiply, demonstrates the usefulness of these optimizations. Figures 19 presents a straightforward implementation of 2-dimensional integer arrays with matrix multiplication, written in terms of 1-dimensional arrays 4 .
Reference: [21] <author> Mary W. Hall and Ken Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(3) </volume> <pages> 227-242, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> He describes a family of analyses, called control-flow n, which is described in more detail in Appendix A. He also describes a number of optimizations to speed up the analysis. Shivers' control-flow 0 is a special case of Heintze's set-based analysis [24]. Hall <ref> [21] </ref> is concerned primarily with improving the computation of a call graph analysis that is very similar to Shivers' control-flow 0. Jagannathan and Weeks [26] present a general framework for control-flow analysis. Unlike Shivers' analyses, their analyses directly compute the control-flow graph. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways.
Reference: [22] <author> Robert Harper and Greg Morrisett. </author> <title> Compiling polymorphism using intensional type analysis. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Principles of Programming Languages [2]. to appear. </booktitle>
Reference-contexts: Most importantly, we need to be careful about data representation. The boxing of array values such as floating-point numbers can degrade the performance of array-processing code seriously. Data representation can be dealt with by using intensional polymorphism <ref> [22] </ref>, which can give programmers tight control over data representation using flat types. However, the implementation and design of languages using intensional polymorphism is the subject of active research [38], so I will not use this approach in my thesis.
Reference: [23] <author> Pieter H. Hartel et al. Pseudoknot: </author> <title> A float-intensive benchmark for functional compilers. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation <ref> [44, 59, 23, 39] </ref>. <p> First, it has a good compiler, the SML/NJ compiler, which is a state-of-the-art compiler for functional languages. The SML/NJ compiler is a real system with many man-years of effort behind it [4, 51]. It is competitive with and often better than compilers for other functional languages <ref> [59, 23] </ref>. Second, Standard ML has an interesting set of benchmarks, including the SML/NJ compiler and an implementation of TCP/IP. Third, Standard ML is statically-typed and call-by-value. I did not use CAML because the benchmarks for CAML were unacceptable. The benchmarks for CAML compilers consist of mostly toy programs. <p> Mattson and David R. Tarditi [5], processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade [40], running 50 generations of a glider gun. It is implemented using lists. Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule <ref> [23] </ref>. PIA A perspective inversion algorithm [62], deciding the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a realistic FORTRAN benchmark [13], translated into ID [17], and then translated into Standard ML by Lal George.
Reference: [24] <author> Nevin Heintze. </author> <title> Set Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> He describes a family of analyses, called control-flow n, which is described in more detail in Appendix A. He also describes a number of optimizations to speed up the analysis. Shivers' control-flow 0 is a special case of Heintze's set-based analysis <ref> [24] </ref>. Hall [21] is concerned primarily with improving the computation of a call graph analysis that is very similar to Shivers' control-flow 0. Jagannathan and Weeks [26] present a general framework for control-flow analysis. Unlike Shivers' analyses, their analyses directly compute the control-flow graph. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways.
Reference: [25] <editor> Paul Hudak, Simon Peyton Jones, and Philip Wadler (editors). </editor> <title> Report on the programming language haskell, a non-strict purely functional language (version 1.2). </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The evaluation section describes the measurements I plan to make to demonstrate the utility of the optimization strategy. It also describes the benchmarks I plan to use. 4.1 Using Standard ML There are many functional languages to choose from: Standard ML [37], CAML, Scheme [41], Common Lisp, and Haskell <ref> [25] </ref>. I used three criteria to choose which functional language to use to demonstrate the thesis. First, the language needed to have a good compiler that I could modify. Second, the language need to have a good set of benchmarks.
Reference: [26] <author> Suresh Jagannathan and Stephen Weeks. </author> <title> A unified treatment of flow analysis in higher-order languages. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Principles of Programming Languages [2]. to appear. </booktitle>
Reference-contexts: He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> He also describes a number of optimizations to speed up the analysis. Shivers' control-flow 0 is a special case of Heintze's set-based analysis [24]. Hall [21] is concerned primarily with improving the computation of a call graph analysis that is very similar to Shivers' control-flow 0. Jagannathan and Weeks <ref> [26] </ref> present a general framework for control-flow analysis. Unlike Shivers' analyses, their analyses directly compute the control-flow graph. My work differs from previous works in that my focus is on applying control-flow analysis and using it to improve optimization. First, previous works focused on constructing control-flow graphs and call graphs.
Reference: [27] <author> Neil D. Jones. </author> <title> Flow analysis of lambda expressions. </title>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways.
Reference: [28] <author> R.R. Kessler, J.C. Peterson, H. Carr, G.P. Duggan, J. Knell, and J. J. Krohnfeldt. </author> <title> Epic a retargetable, highly optimizing LISP compiler. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 118-130, </pages> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <journal> ACM. </journal> <volume> 60 </volume>
Reference-contexts: The Lucid Portable LISP compiler [10] does the following global optimizations: constant folding, dead-code elimination, tail-recursion optimization, and inlining of lambda expressions used only once. It also does tail-recursion optimization: it reuses stack frames for tail calls, and self-tail calls turn into jumps. Epic <ref> [28] </ref> compiled a dynamically-scoped version of Lisp. It did loop-invariant removal, but does not appear to make any other use of information about program structure. The S-1 compiler [9] was an optimizing compiler for a statically-scoped version of Lisp targeted at a vector processor.
Reference: [29] <author> Jens Knoop and Bernhard Steffen. </author> <title> The interprocedural coincidence theorem. </title> <booktitle> In Compiler Construction: 4th International Conference Proceedings, </booktitle> <pages> pages 125-140, </pages> <address> Paderborn, FRG, </address> <month> October </month> <year> 1992. </year> <note> Spring Verlag. </note>
Reference-contexts: Most works on interprocedural analysis do not deal with languages with first-class functions. Sharir and Pneuli [53] present a framework for interprocedural analysis, which is extended by Knoop and Steffen <ref> [29] </ref>. The language which the framework covers is unclear. However, it is clear that this language does not include lexically-scoped functions or first-class functions, and thus the framework is not strong enough for languages based on the call-by-value - calculus.
Reference: [30] <author> David Kranz. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Functional languages like Standard ML [37] are a good thing. They have many features which offer significant programming benefits, such as module systems, static typing, garbage collection, and first-class functions. Compilers for these languages, such as Standard ML of New Jersey (SML/NJ) [4, 51] and Orbit <ref> [31, 30] </ref> can do a good job compiling programs, especially programs that perform symbolic computations. However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation [44, 59, 23, 39]. <p> It has a sophisticated closure-conversion phase, which does an excellent job of keeping closures in registers [52]. This phase uses some simple information about program structure, as described in Section 3.2.3. The Orbit compiler <ref> [31, 30] </ref> showed that Scheme programs can be compiled to be as efficient as comparable Pascal programs. Orbit does only a few global optimizations [30, p. 27], such as constant-folding, simple inlining, and dead-code elimination. It employs information about program structure when making decisions about how to represent closures. <p> This phase uses some simple information about program structure, as described in Section 3.2.3. The Orbit compiler [31, 30] showed that Scheme programs can be compiled to be as efficient as comparable Pascal programs. Orbit does only a few global optimizations <ref> [30, p. 27] </ref>, such as constant-folding, simple inlining, and dead-code elimination. It employs information about program structure when making decisions about how to represent closures. For example, if a function is tail-recursive but makes some function calls, its closure might be allocated on the stack instead of in registers.
Reference: [31] <author> David Kranz, Richard Kelsey, Jonathan Rees, Paul Hudak, James Philbin, and Norman Adams. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <booktitle> In Proceedings of the SIG-PLAN '86 Conference Symposium on Compiler Construction, </booktitle> <pages> pages 219-233, </pages> <address> Palo Alto, California, </address> <month> June </month> <year> 1986. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction Functional languages like Standard ML [37] are a good thing. They have many features which offer significant programming benefits, such as module systems, static typing, garbage collection, and first-class functions. Compilers for these languages, such as Standard ML of New Jersey (SML/NJ) [4, 51] and Orbit <ref> [31, 30] </ref> can do a good job compiling programs, especially programs that perform symbolic computations. However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation [44, 59, 23, 39]. <p> It has a sophisticated closure-conversion phase, which does an excellent job of keeping closures in registers [52]. This phase uses some simple information about program structure, as described in Section 3.2.3. The Orbit compiler <ref> [31, 30] </ref> showed that Scheme programs can be compiled to be as efficient as comparable Pascal programs. Orbit does only a few global optimizations [30, p. 27], such as constant-folding, simple inlining, and dead-code elimination. It employs information about program structure when making decisions about how to represent closures.
Reference: [32] <author> Xavier Leroy. </author> <title> Unboxed objects and polymorphic typing. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-188, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The SML/NJ compiler [4, 51] is a state-of-the-art compiler for functional languages. It uses a type-based data representation analysis, which attempts to keep values unboxed when possible <ref> [32, 51] </ref>. It has a sophisticated global optimization phase, which does inlining, - reduction, uncurrying, constant-folding, hoisting, and dead-code elimination. However, this phase does not use information on program structure, such as nesting of recursive functions.
Reference: [33] <institution> Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </institution> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference: [34] <institution> Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, Or-lando, Florida, </institution> <month> June </month> <year> 1994. </year> <note> ACM. </note>
Reference: [35] <author> Robert A. MacLachlan. </author> <title> The Python compiler for CMU Common Lisp. </title> <booktitle> In LFP '92 [33], </booktitle> <pages> pages 235-246. </pages>
Reference-contexts: Like the Scheme!C compiler, the Bigloo compiler tries to produce readable C code which is suitable for optimization by a C compiler. However, Serrano also does not argue that being sensitive to program structure is essential to compiling 29 functional languages well. The CMU Python Common Lisp compiler <ref> [35] </ref> tries to avoid unnecessary unconditional branches when generating code for loops. However, no other optimizations for recursive functions or loops are described. The Lucid Portable LISP compiler [10] does the following global optimizations: constant folding, dead-code elimination, tail-recursion optimization, and inlining of lambda expressions used only once.
Reference: [36] <author> Victoria Markstein, John Cocke, and Peter Markstein. </author> <title> Optimization of range checking. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 114-119. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1982. </year>
Reference-contexts: Hence, we must check whether an array index is in-bounds before doing an array access. Chow [11] has shown that these checks greatly interfere with optimization, and have a significant performance penalty. It is crucial that these checks be eliminated. A number of papers have addressed bounds-check elimination <ref> [36, 20, 6] </ref>. However, it is unclear how well the techniques they describe perform on realistic programs. The following example, integer matrix multiply, demonstrates the usefulness of these optimizations. Figures 19 presents a straightforward implementation of 2-dimensional integer arrays with matrix multiplication, written in terms of 1-dimensional arrays 4 .
Reference: [37] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Functional languages like Standard ML <ref> [37] </ref> are a good thing. They have many features which offer significant programming benefits, such as module systems, static typing, garbage collection, and first-class functions. <p> The evaluation section describes the measurements I plan to make to demonstrate the utility of the optimization strategy. It also describes the benchmarks I plan to use. 4.1 Using Standard ML There are many functional languages to choose from: Standard ML <ref> [37] </ref>, CAML, Scheme [41], Common Lisp, and Haskell [25]. I used three criteria to choose which functional language to use to demonstrate the thesis. First, the language needed to have a good compiler that I could modify. Second, the language need to have a good set of benchmarks.
Reference: [38] <author> J. Gregory Morrisett. </author> <title> Data representations and polymorphic languages. </title> <type> PhD thesis proposal, </type> <month> December </month> <year> 1993. </year>
Reference-contexts: Data representation can be dealt with by using intensional polymorphism [22], which can give programmers tight control over data representation using flat types. However, the implementation and design of languages using intensional polymorphism is the subject of active research <ref> [38] </ref>, so I will not use this approach in my thesis. Instead, I will use ad-hoc approaches to ensure that integer and floating-point arrays are flattened. Specifically, there will be three kinds of arrays: flattened integer arrays, flattened floating-point arrays, and polymorphic arrays.
Reference: [39] <author> George C. Necula and Lal George. </author> <title> Accounting for the performance of Standard ML on the DEC Alpha. </title> <type> Technical report, </type> <note> in preparation, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation <ref> [44, 59, 23, 39] </ref>.
Reference: [40] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1989. </year>
Reference-contexts: Lawrence Livermore Loops Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi [5], processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade <ref> [40] </ref>, running 50 generations of a glider gun. It is implemented using lists. Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule [23]. PIA A perspective inversion algorithm [62], deciding the location of an object in a perspective video image.
Reference: [41] <author> Jonathan Rees and William Clinger. </author> <title> Revised report on the algorithmic language Scheme. </title> <journal> SIGPLAN Notices, </journal> <volume> 21(12) </volume> <pages> 37-79, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The evaluation section describes the measurements I plan to make to demonstrate the utility of the optimization strategy. It also describes the benchmarks I plan to use. 4.1 Using Standard ML There are many functional languages to choose from: Standard ML [37], CAML, Scheme <ref> [41] </ref>, Common Lisp, and Haskell [25]. I used three criteria to choose which functional language to use to demonstrate the thesis. First, the language needed to have a good compiler that I could modify. Second, the language need to have a good set of benchmarks.
Reference: [42] <author> Thomas Reps, Mooly Sagiv, and Susan Horwitz. </author> <title> Interpocedural dataflow analysis via graph reachability. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Principles of Programming Languages [2]. to appear. </booktitle>
Reference-contexts: The language which the framework covers is unclear. However, it is clear that this language does not include lexically-scoped functions or first-class functions, and thus the framework is not strong enough for languages based on the call-by-value - calculus. Reps, Sagiv, and Horwitz <ref> [42] </ref> show how a number of interprocedural dataflow analysis problems can be solved in polynomial time within this framework. Many works study computing interprocedural summary information. Interprocedural summary information tells what variables are used or modified by functions, or what variables are aliased to other variables.
Reference: [43] <author> Stephen Richardson and Mahadevan Ganapathi. </author> <title> Interprocedural optimization: experimental results. </title> <journal> Software Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 149-168, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In the absence of this information, optimizers must make worst-case assumptions at function calls, assuming all global or possibly aliased variables are used or changed by the functions. Richardson and Ganapathi <ref> [43] </ref> found that for Pascal programs this information did not improve optimization. This kind of information is not useful for most functional languages languages, excluding Scheme. In most functional languages, variables cannot be altered once they are bound. Thus, compilers are guaranteed that function calls cannot change variable bindings.
Reference: [44] <author> Gerald Roylance. </author> <title> Expressing mathematical subroutines constructively. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 8-13, </pages> <address> Snowbird, Utah, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation <ref> [44, 59, 23, 39] </ref>.
Reference: [45] <author> Guillermo Juan Rozas. </author> <title> Taming the Y operator. </title> <booktitle> In LFP '92 [33], </booktitle> <pages> pages 226-234. </pages>
Reference-contexts: However, Bartlett does not argue that being sensitive to program structure is essential to compiling functional languages well. He shows that Scheme!C produces code which is competitive with a native code compiler for Lisp. LIAR <ref> [45] </ref> makes use of an analysis similar to Shivers' control-flow 0 analysis. It attempts to find variables which are bound to only one closed function. These variables can then be replaced by these closed functions.
Reference: [46] <author> Barbara G. Ryder. </author> <title> Constructing the call graph of a program. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(3) </volume> <pages> 216-226, </pages> <month> May </month> <year> 1979. </year> <month> 61 </month>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs [46, 63, 27, 54, 55, 24, 21, 26]. Ryder <ref> [46] </ref> investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. The source language is unclear, and the algorithms are presented in an informal style. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways.
Reference: [47] <author> Amr Sabry and Matthias Felleisen. </author> <title> Reasoning about programs in continuations-passing style. </title> <booktitle> In LFP '92 [33], </booktitle> <pages> pages 288-298. </pages>
Reference-contexts: I will convert programs to continuation-passing-style (CPS) immediately before passing them to the closure conversion phase. The choice of whether to use a direct-style or CPS intermediate language is largely an engineering choice. Theoretical studies have shown that the two representations are inter-convertible <ref> [14, 47, 48] </ref>. Thus, in principle, neither representation is more powerful than the other. I have chosen to use a direct-style intermediate language because I believe it is more amenable to analyzing program structure. However, using a direct-style language complicates inlining, that is, fi-reduction, [48].
Reference: [48] <author> Amr Sabry and Matthias Felleisen. </author> <title> Reasoning about programs in continuation-passing style. </title> <journal> Lisp and Symbolic Computation, </journal> 6(3/4):289-360, November 1993. 
Reference-contexts: I will convert programs to continuation-passing-style (CPS) immediately before passing them to the closure conversion phase. The choice of whether to use a direct-style or CPS intermediate language is largely an engineering choice. Theoretical studies have shown that the two representations are inter-convertible <ref> [14, 47, 48] </ref>. Thus, in principle, neither representation is more powerful than the other. I have chosen to use a direct-style intermediate language because I believe it is more amenable to analyzing program structure. However, using a direct-style language complicates inlining, that is, fi-reduction, [48]. <p> Thus, in principle, neither representation is more powerful than the other. I have chosen to use a direct-style intermediate language because I believe it is more amenable to analyzing program structure. However, using a direct-style language complicates inlining, that is, fi-reduction, <ref> [48] </ref>.
Reference: [49] <author> Amr Sabry and Matthias Felleisen. </author> <booktitle> Is continuation-passing useful for data flow analysis ? In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 1-12. </pages>
Reference-contexts: The semantic domains for the interpeter are given as ML datatypes in Figure 53, and the interpreter is given as an ML program in Figures 54 and 55. The interpreter is based on the operational semantics given by Sabry and Felleisen <ref> [49] </ref>, which in turn is based on the denotational semantics by Shivers [55]. It uses explicit closures to represent functions. Environments are factored: environments map variables to locations, and a store maps locations to values. <p> We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways. First, following Sabry and Felleisen <ref> [49] </ref>, the analyses are for a direct-style intermediate language, not a CPS intermediate language. I claim that that CPS causes problems when analyzing program structure. Second, the analyses directly compute approximations of control-flow and call graphs.
Reference: [50] <author> Manual Serrano and Pierre Weis. </author> <title> 1+1 = 1: an optimizing CAML compiler. </title> <type> Technical Report 2264, </type> <institution> INRIA, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: LIAR also does a side-effect analysis, which eliminates function calls that return known values. The analysis may change the termination behavior (and correctness) of programs. Serrano's Bigloo Scheme compiler <ref> [50] </ref> compiles Scheme to C. Like LIAR, it uses an analysis similar to Shivers' control-flow 0 analysis to find variables within a module which are bound to just one closed function. The Bigloo compiler also removes invariant recursive-function arguments.
Reference: [51] <author> Zhong Shao. </author> <title> Compiling Standard ML for Efficient Execution on Modern Machines. </title> <type> PhD thesis, </type> <institution> Princeton University, Princeton, </institution> <address> New Jersey, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Functional languages like Standard ML [37] are a good thing. They have many features which offer significant programming benefits, such as module systems, static typing, garbage collection, and first-class functions. Compilers for these languages, such as Standard ML of New Jersey (SML/NJ) <ref> [4, 51] </ref> and Orbit [31, 30] can do a good job compiling programs, especially programs that perform symbolic computations. However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation [44, 59, 23, 39]. <p> Consider the recursive function given in Figure 18. The control-flow graph can be used to determine whether f is always bound to functions of the form x.y.e, and whether all calls to these functions can be uncurried also. 3.2.3 Closure conversion Shao and Appel <ref> [52, 51] </ref> present a closure-conversion algorithm which makes use of some simple information about program structure. <p> I decided to use Standard ML because it best met these criteria. First, it has a good compiler, the SML/NJ compiler, which is a state-of-the-art compiler for functional languages. The SML/NJ compiler is a real system with many man-years of effort behind it <ref> [4, 51] </ref>. It is competitive with and often better than compilers for other functional languages [59, 23]. Second, Standard ML has an interesting set of benchmarks, including the SML/NJ compiler and an implementation of TCP/IP. Third, Standard ML is statically-typed and call-by-value. <p> They make use of information about program structure in limited ways, and do not appear to be organized around the principle of focusing compilation on recursive functions or loops. The SML/NJ compiler <ref> [4, 51] </ref> is a state-of-the-art compiler for functional languages. It uses a type-based data representation analysis, which attempts to keep values unboxed when possible [32, 51]. It has a sophisticated global optimization phase, which does inlining, - reduction, uncurrying, constant-folding, hoisting, and dead-code elimination. <p> The SML/NJ compiler [4, 51] is a state-of-the-art compiler for functional languages. It uses a type-based data representation analysis, which attempts to keep values unboxed when possible <ref> [32, 51] </ref>. It has a sophisticated global optimization phase, which does inlining, - reduction, uncurrying, constant-folding, hoisting, and dead-code elimination. However, this phase does not use information on program structure, such as nesting of recursive functions.
Reference: [52] <author> Zhong Shao and Andrew W. Appel. </author> <title> Space-efficient closure representations. </title> <booktitle> In LFP '94 [34], </booktitle> <pages> pages 150-161. </pages>
Reference-contexts: Consider the recursive function given in Figure 18. The control-flow graph can be used to determine whether f is always bound to functions of the form x.y.e, and whether all calls to these functions can be uncurried also. 3.2.3 Closure conversion Shao and Appel <ref> [52, 51] </ref> present a closure-conversion algorithm which makes use of some simple information about program structure. <p> However, this phase does not use information on program structure, such as nesting of recursive functions. It has a sophisticated closure-conversion phase, which does an excellent job of keeping closures in registers <ref> [52] </ref>. This phase uses some simple information about program structure, as described in Section 3.2.3. The Orbit compiler [31, 30] showed that Scheme programs can be compiled to be as efficient as comparable Pascal programs.
Reference: [53] <author> Micha Sharir and Amir Pnueli. </author> <title> Two approaches to interprocedural data flow analysis. </title> <editor> In Steven S. Muchnick and Neil D. Jones, editors, </editor> <title> Program Flow Analysis: </title> <journal> Theory and Applications, </journal> <volume> chapter 7, </volume> <pages> pages 189-234. </pages> <publisher> Prentice-Hall, Inc., </publisher> <year> 1981. </year>
Reference-contexts: Most works on interprocedural analysis do not deal with languages with first-class functions. Sharir and Pneuli <ref> [53] </ref> present a framework for interprocedural analysis, which is extended by Knoop and Steffen [29]. The language which the framework covers is unclear.
Reference: [54] <author> Olin Shivers. </author> <title> Control Flow Analysis in Scheme. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs [46, 63, 27, 54, 55, 24, 21]. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers <ref> [54, 55] </ref>. The analyses we present differ from Shivers' analyses in two important ways. First, following Sabry and Felleisen [49], the analyses are for a direct-style intermediate language, not a CPS intermediate language. I claim that that CPS causes problems when analyzing program structure.
Reference: [55] <author> Olin Shivers. </author> <title> Control-Flow Analysis of Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> The interpreter is based on the operational semantics given by Sabry and Felleisen [49], which in turn is based on the denotational semantics by Shivers <ref> [55] </ref>. It uses explicit closures to represent functions. Environments are factored: environments map variables to locations, and a store maps locations to values. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. The source language is unclear, and the algorithms are presented in an informal style. Shivers <ref> [55] </ref> studies constructing the control-flow graph of programs in a subset of Scheme. His primary concern is formalizing and showing the correctness of the analysis using abstract interpretation. He describes a family of analyses, called control-flow n, which is described in more detail in Appendix A. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs [46, 63, 27, 54, 55, 24, 21]. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers <ref> [54, 55] </ref>. The analyses we present differ from Shivers' analyses in two important ways. First, following Sabry and Felleisen [49], the analyses are for a direct-style intermediate language, not a CPS intermediate language. I claim that that CPS causes problems when analyzing program structure.
Reference: [56] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 196-205. </pages>
Reference-contexts: Making it the core of a language design means that the compiler must work hard to eliminate a feature that programmers rarely need. 4.2 The SML/NJ compiler There are some disadvantages to using the SML/NJ compiler. It is hard to measure performance because vendor-supplied performance-analysis tools, such as ATOM <ref> [56] </ref>, cannot be used with programs produced by the SML/NJ compiler. These tools usually require standard executables, and the SML/NJ compiler does not produce standard executables. The inability to use standard 25 performance analysis tools is a serious drawback of using the SML/NJ compiler in research.
Reference: [57] <author> Amitabh Srivastava and David W. Wall. </author> <title> A practical system for intermodule code optimization at link-time. </title> <type> Technical Report 92/6, </type> <institution> Digital Equipment Western Research Laboratory, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This kind of information is not useful for most functional languages languages, excluding Scheme. In most functional languages, variables cannot be altered once they are bound. Thus, compilers are guaranteed that function calls cannot change variable bindings. Srivastava and Wall's OM <ref> [57] </ref> does interprocedural loop-invariant removal and dead-code elimination at link-time. Like my proposed work, OM analyzes the call graph and look for loops induced by recursion and loops that may span procedure boundaries. Unlike my proposed work, OM works at the level of machine code.
Reference: [58] <author> Peter Steenkiste. </author> <title> LISP on a Reduced-Instruction-Set Processor: Characterization and Optimization. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <address> Stan-ford,CA 94305, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: The optimizations were peephole optimizations, elimination of common chains of code (which reduces program size but not execution time), and a simple form of common subexpression elimination (which tracked the contents of registers to avoid unnecessary loads). Finally, these abstract machine instruction were translated to actual machine instructions. Steenkiste <ref> [58] </ref> ported the PSL compiler to a RISC machine to evaluate the effectiveness of RISC processors at supporting LISP. He extended the compiler to optimize tag handling, do interprocedural register allocation, and inlining based on the size of functions.
Reference: [59] <author> Stephen J. Sullivan and Benjamin G. Zorn. </author> <title> Numerical analysis using non-procedural paradigms. </title> <journal> ACM Transactions on Mathematical Software. </journal> <note> To appear, late 1995. </note>
Reference-contexts: However, they often do a poor job compiling programs in many other real-world application domains, such as systems programming [8] and scientific computation <ref> [44, 59, 23, 39] </ref>. <p> First, it has a good compiler, the SML/NJ compiler, which is a state-of-the-art compiler for functional languages. The SML/NJ compiler is a real system with many man-years of effort behind it [4, 51]. It is competitive with and often better than compilers for other functional languages <ref> [59, 23] </ref>. Second, Standard ML has an interesting set of benchmarks, including the SML/NJ compiler and an implementation of TCP/IP. Third, Standard ML is statically-typed and call-by-value. I did not use CAML because the benchmarks for CAML were unacceptable. The benchmarks for CAML compilers consist of mostly toy programs. <p> Simple A spherical fluid-dynamics program, developed as a realistic FORTRAN benchmark [13], translated into ID [17], and then translated into Standard ML by Lal George. SML/NJ SML/NJ is a compiler for Standard ML. Sparse Gaussian eimination on sparse matrices <ref> [59] </ref>. The matrices were 1000x1000 in size. Only non-zero elements are stored, with roughly 2 elements per row. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan skin. YACC An LALR (1) parser generator, implemented by David R. Tarditi [60], processing the grammar of Standard ML.
Reference: [60] <author> David Tarditi and Andrew W. Appel. ML-YACC, </author> <title> version 2.0. Distributed with Standard ML of New Jersey, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: Sparse Gaussian eimination on sparse matrices [59]. The matrices were 1000x1000 in size. Only non-zero elements are stored, with roughly 2 elements per row. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan skin. YACC An LALR (1) parser generator, implemented by David R. Tarditi <ref> [60] </ref>, processing the grammar of Standard ML. Table 3: Benchmarks to be used to evaluate the quality of optimization. I plan to use the benchmarks described in Figure 3 to measure performance of the optimiz-ers.
Reference: [61] <author> Tim A. Wagner, Vance Maverick, Susan L. Graham, and Michael A. Harrison. </author> <title> Accurate static estimators for progream optimization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 85-96. </pages>
Reference-contexts: Is this premise true ? Put another way, can we use program structure to predict at compile-time where programs spend their time at run-time ? A study along these lines for C programs was done by Wagner and others <ref> [61] </ref>. I would like to conduct a similar 40 study for functional programs. It is not clear whether this is possible.
Reference: [62] <author> Kevin G. Waugh, Patrick McAndrew, and Greg Michaelson. </author> <title> Parallel implementations from function prototypes: a case study. </title> <institution> Technical Report Computer Science 90/4, Heriot-Watt University, Edinburgh, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Tarditi [5], processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade [40], running 50 generations of a glider gun. It is implemented using lists. Pseudoknot Computes the three-dimensional structure of part of a nucleic acid molecule [23]. PIA A perspective inversion algorithm <ref> [62] </ref>, deciding the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a realistic FORTRAN benchmark [13], translated into ID [17], and then translated into Standard ML by Lal George. SML/NJ SML/NJ is a compiler for Standard ML.
Reference: [63] <author> William E. Weihl. </author> <title> Interprocedural data flow analysis in the presence of pointers, procedure variables, and label variables. </title> <booktitle> In Conference Record of the Ninth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 83-94, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year> <month> 62 </month>
Reference-contexts: My thesis differs from previous work on understanding the structure of functional programs (control-flow analysis) in that my focus is on applying control-flow analysis and using it to improve optimization. Previous works <ref> [46, 63, 27, 54, 55, 24, 21] </ref> focused on analyzing the structure of program, and not on using the information. <p> He used program structure when doing interprocedural register allocation by making use of the call graph. He also considered limiting inlining to only structured loops. This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs <ref> [46, 63, 27, 54, 55, 24, 21, 26] </ref>. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl [63] studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. <p> This had worse performance than the normal inlining strategy. 5.2 Constructing control-flow and call graphs Many researchers have investigated constructing control-flow and call graphs [46, 63, 27, 54, 55, 24, 21, 26]. Ryder [46] investigated the construction of call graphs for Fortran programs, which are recursion-free. Weihl <ref> [63] </ref> studied the effect of pointers and procedure variables on constructing call graphs and control-flow graphs. The source language is unclear, and the algorithms are presented in an informal style. Shivers [55] studies constructing the control-flow graph of programs in a subset of Scheme. <p> This topic has been studied extensively, and numerous analyses have been proposed for constructing approximations of control-flow graphs and call graphs <ref> [46, 63, 27, 54, 55, 24, 21] </ref>. We present analyses for approximating control-flow graphs based on the factored environment approach of Shivers [54, 55]. The analyses we present differ from Shivers' analyses in two important ways.
References-found: 63

