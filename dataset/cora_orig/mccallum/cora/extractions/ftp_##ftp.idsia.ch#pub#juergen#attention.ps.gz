URL: ftp://ftp.idsia.ch/pub/juergen/attention.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Email: schmidhu@tumult.informatik.tu-muenchen.de  
Title: LEARNING TO GENERATE ARTIFICIAL FOVEA TRAJECTORIES FOR TARGET DETECTION  
Author: J URGEN SCHMIDHUBER RUDOLF HUBER 
Address: Arcisstr. 21, 8000 Munchen 2, Germany  
Affiliation: Institut fur Informatik Technische Universitat Munchen  
Note: International Journal of Neural Systems, 2(1 2):135-141, 1991. Figures not included!  
Abstract: It is shown how `static' neural approaches to adaptive target detection can be replaced by a more efficient and more sequential alternative. The latter is inspired by the observation that biological systems employ sequential eye-movements for pattern recognition. A system is described which builds an adaptive model of the time-varying inputs of an artificial fovea controlled by an adaptive neural controller. The controller uses the adaptive model for learning the sequential generation of fovea trajectories causing the fovea to move to a target in a visual scene. The system also learns to track moving targets. No teacher provides the desired activations of `eye-muscles' at various times. The only goal information is the shape of the target. Since the task is a `reward-only-at-goal' task , it involves a complex temporal credit assignment problem. Some implications for adaptive attentive systems in general are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Huber. Selektive visuelle Aufmerksamkeit: Untersuchungen zum Erlernen von Fokustrajekto-rien durch neuronale Netze, </author> <year> 1990. </year> <institution> Diplomarbeit, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: The experiments show that the learning of successful fovea trajectories involving translations and rotations is possible, although M ususally makes erroneous predictions. See <ref> [1] </ref> for additional experiments. <p> For such situations, additional experiments (not reported here) with a recurrent M and a recurrent C were conducted. It turned out that internal feedback within M and C sometimes can lead to success in cases where the simple approach fails <ref> [1] </ref>. However, there is an approach which in the long run might prove to be even more promising: The adaptive on-line generation of appropriate sub-goals. Some first work in this direction already has been done [13].
Reference: [2] <author> M. I. Jordan. </author> <title> Supervised learning and systems with excess degrees of freedom. </title> <type> Technical Report COINS TR 88-27, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan <ref> [2] </ref>, Munro [5], Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber [10] [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20]. <p> The reason is: It suffices if the inner products of the approximated gradients (based on an inaccurate model) for C and the true gradients (according to a hypothetical perfect model) are always positive (see also <ref> [2] </ref>). Even if these inner products are not always positive but only `in most cases', performance improvement can be expected. 2.2 Formal Details In the comparatively simple case considered here, the controller C is a standard back-propagation network. There are discrete time steps. <p> It was found that two interacting conventional deterministic networks in the style of <ref> [2] </ref> and [6] were not appropriate. Usually a deterministic system soon became trapped in a state where the controller never shifted the fovea towards regions which allowed the model network to collect new relevant information about the external world. This is called the deadlock problem. <p> This can be done by constructing a new error function by adding differences in successive fovea inputs to the final input error observed at the end of a fovea trajectory. (The approach is reminiscent of Jordan's work <ref> [2] </ref>, however, Jordan imposes temporal constraints on the output units.) The effect is that the system develops a preference for temporal invariances in input space. For attentive vision, such temporal invariances can be caused e.g. by fovea movements that follow edges.
Reference: [3] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: The control network should already start learning with an incomplete representation of the external dynamics in the model network. M should concentrate on those parts of the external dynamics that are necessary for achieving C's goals. Just like Kohonen's self organizing feature maps <ref> [3] </ref> dedicate more storage capacity for fine grained representation of common similar inputs, M should 7 dedicate more storage capacity and time for fine grained modeling of those aspects of the world that are likely to be relevant for the system's main goal. (See [11] for more reasons for parallel on-line
Reference: [4] <author> Y. LeCun. </author> <title> Une procedure d'apprentissage pour reseau a seuil asymetrique. </title> <booktitle> Proceedings of Cognitiva 85, Paris, </booktitle> <pages> pages 599-604, </pages> <year> 1985. </year>
Reference: [5] <author> P. W. Munro. </author> <title> A dual back-propagation scheme for scalar reinforcement learning. </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, WA, </address> <pages> pages 165-176, </pages> <year> 1987. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro <ref> [5] </ref>, Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber [10] [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20].
Reference: [6] <author> Nguyen and B. Widrow. </author> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In IEEE/INNS International Joint Conference on Neural Networks, Washington, D.C., </booktitle> <volume> volume 1, </volume> <pages> pages 357-364, </pages> <year> 1989. </year> <month> 9 </month>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro [5], Robinson & Fallside [8], Nguyen & Widrow <ref> [6] </ref>, and Schmidhuber [10] [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20]. <p> If objects in a visual scene may occupy random positions then it will be impossible for the model network to predict exactly the future fovea inputs from previous ones. Unlike with e.g. the `truck backer upper' <ref> [6] </ref> both C and M never `see' the complete state of the environment, but only some local details. <p> In that case a single network would be able to store all information about the environment. Thus for all interesting cases the model network necessarily has to remain imperfect. So unlike with the `truck backer upper' problem <ref> [6] </ref> it is not intended to make M a perfect predictor whose output could replace the input from the environment (in that case not much would be gained compared to the static approach to target detection). <p> It was found that two interacting conventional deterministic networks in the style of [2] and <ref> [6] </ref> were not appropriate. Usually a deterministic system soon became trapped in a state where the controller never shifted the fovea towards regions which allowed the model network to collect new relevant information about the external world. This is called the deadlock problem.
Reference: [7] <author> D. B. Parker. Learning-logic. </author> <type> Technical Report TR-47, </type> <institution> Center for Comp. Research in Economics and Management Sci., MIT, </institution> <year> 1985. </year>
Reference: [8] <author> T. Robinson and F. Fallside. </author> <title> Dynamic reinforcement driven error propagation networks with application to game playing. </title> <booktitle> In Proceedings of the 11th Conference of the Cognitive Science Society, </booktitle> <address> Ann Arbor, </address> <pages> pages 836-843, </pages> <year> 1989. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro [5], Robinson & Fallside <ref> [8] </ref>, Nguyen & Widrow [6], and Schmidhuber [10] [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20].
Reference: [9] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: See figure 3. Gradient information for the weights of the control network now can be back-propagated from M 's final prediction through the model network down into the control network and back through the model network etc. according to the `unfolding in time' algorithm [18] <ref> [9] </ref>. An important difference to conventional `back-propagation through time' (with a single recurrent network) is that the weights of the model network remain fixed during this procedure.
Reference: [10] <author> J. H. Schmidhuber. </author> <title> Learning algorithms for networks with internal and external feedback. </title> <editor> In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, editors, </editor> <booktitle> Proc. of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 52-61. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro [5], Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber <ref> [10] </ref> [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20].
Reference: [11] <author> J. H. Schmidhuber. </author> <title> Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. </title> <type> Technical Report FKI-126-90 (revised), </type> <institution> Institut fur Informatik, Technische Universitat Munchen, </institution> <note> Novem-ber 1990. (Revised and extended version of an earlier report from February.). </note>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro [5], Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber [10] <ref> [11] </ref> [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20]. Their system uses adaptive actions that can bind `markers' to certain features of an environmental state. <p> like Kohonen's self organizing feature maps [3] dedicate more storage capacity for fine grained representation of common similar inputs, M should 7 dedicate more storage capacity and time for fine grained modeling of those aspects of the world that are likely to be relevant for the system's main goal. (See <ref> [11] </ref> for more reasons for parallel on-line learning of M and C.) We conducted some experiments with on-line learning. It was found that two interacting conventional deterministic networks in the style of [2] and [6] were not appropriate.
Reference: [12] <author> J. H. Schmidhuber. </author> <title> An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. </title> <booktitle> In Proc. IEEE/INNS International Joint Conference on Neural Networks, San Diego, </booktitle> <volume> volume 2, </volume> <pages> pages 253-258, </pages> <year> 1990. </year>
Reference: [13] <author> J. H. Schmidhuber. </author> <title> Towards compositional learning with dynamic neural networks. </title> <type> Technical Report FKI-129-90, </type> <institution> Institut fur Informatik, Technische Universitat Munchen, </institution> <year> 1990. </year>
Reference-contexts: However, there is an approach which in the long run might prove to be even more promising: The adaptive on-line generation of appropriate sub-goals. Some first work in this direction already has been done <ref> [13] </ref>. By using the above-mentioned concept of goal-defining input units with time-invariant activations, we intend to apply adaptive sub-goal generators to the problems of `local minima' that can arise during the target detection process. 4.2 Methods of Temporal Invariances.
Reference: [14] <author> J. H. Schmidhuber. </author> <title> A possibility for implementing curiosity and boredom in model-building neural controllers. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <pages> pages 222-227. </pages> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: The system described in <ref> [14] </ref>, which implements `curiosity' and `boredom' by means of adaptive dynamic attention depending on the amount of a model network's ignorance about the external dynamics, also is based on external feedback. A generalization of the method described above would work as follows.
Reference: [15] <author> J. H. Schmidhuber. </author> <title> Reinforcement learning in markovian and non-markovian environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos [19], Jordan [2], Munro [5], Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber [10] [11] <ref> [15] </ref>. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20]. Their system uses adaptive actions that can bind `markers' to certain features of an environmental state.
Reference: [16] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <year> 1989. </year>
Reference-contexts: This is analogous to our fovea-guiding actions which dynamically change the input such that certain environmental details become visible, while others disappear. With Whitehead and Ballard's system the learning of active perception is based on an adaptive control technique for delayed reinforcement learning called `Q-learning' <ref> [16] </ref>. Our approach implements an adaptive control technique for `reward-only-at-goal' tasks which is quite different from those reinforcement learning control architectures used by Whitehead and Ballard. Our approach is gradient-based.
Reference: [17] <author> P. J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference: [18] <author> P. J. Werbos. </author> <title> Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, </title> <type> 1, </type> <year> 1988. </year>
Reference-contexts: See figure 3. Gradient information for the weights of the control network now can be back-propagated from M 's final prediction through the model network down into the control network and back through the model network etc. according to the `unfolding in time' algorithm <ref> [18] </ref> [9]. An important difference to conventional `back-propagation through time' (with a single recurrent network) is that the weights of the model network remain fixed during this procedure.
Reference: [19] <author> P. J. Werbos. </author> <title> Backpropagation and neurocontrol: A review and prospectus. </title> <booktitle> In IEEE/INNS International Joint Conference on Neural Networks, Washington, D.C., </booktitle> <volume> volume 1, </volume> <pages> pages 209-216, </pages> <year> 1989. </year>
Reference-contexts: In different contexts and with different degrees of generality the basic principle for credit assignment by system realization and `gradient descent through a frozen model network' has been previously described by Werbos <ref> [19] </ref>, Jordan [2], Munro [5], Robinson & Fallside [8], Nguyen & Widrow [6], and Schmidhuber [10] [11] [15]. The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard [20].
Reference: [20] <author> S.D. Whitehead and D. H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <type> Technical Report 331, </type> <institution> University of Rochester, Dept. of Comp. Sci., </institution> <year> 1990. </year>
Reference-contexts: The only work by other authors that also addresses the problem of learning active perception in reactive environments (and that we are aware of) is the work of Whitehead and Ballard <ref> [20] </ref>. Their system uses adaptive actions that can bind `markers' to certain features of an environmental state. Markers dynamically mask or emphasize inputs from the visible environment. This is analogous to our fovea-guiding actions which dynamically change the input such that certain environmental details become visible, while others disappear.

References-found: 20

