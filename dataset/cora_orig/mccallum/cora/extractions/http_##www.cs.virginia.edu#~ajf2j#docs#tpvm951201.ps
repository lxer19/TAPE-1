URL: http://www.cs.virginia.edu/~ajf2j/docs/tpvm951201.ps
Refering-URL: http://www.cs.virginia.edu/~ajf2j/research.html
Root-URL: http://www.cs.virginia.edu
Email: ferrari@virginia.edu  vss@mathcs.emory.edu  
Title: Multiparadigm Distributed Computing with TPVM  
Author: Adam Ferrari and V. S. Sunderam 
Note: Research supported in part by NASA, DoE, and NSF under grants NAG 2-828, DE-FG05-91ER25105, ASC-9214149  
Address: Charlottesville, VA 22903, USA  Atlanta, GA 30322, USA  Atlanta, GA 30322, USA  
Affiliation: Department of Computer Science University of Virginia,  Department of Mathematics and Computer Science Emory University,  Department of Math and Computer Science Emory University,  
Pubnum: Computer Science Technical Report CSTR-951201  
Abstract-found: 0
Intro-found: 1
Reference: [Bai91] <author> D. Bailey, E. Barszcz, et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-67, </pages> <year> 1991. </year>
Reference-contexts: To demonstrate that reasonable performance can be delivered by network-based systems when granularity is not excessively fine, especially when increasingly popular high-speed networks are used, we present in Table 1 results of recent experiments with the NAS Parallel benchmarks <ref> [Bai91] </ref>. This table shows excerpted performance numbers from a detailed benchmarking effort [WAS95] aimed at analyzing efficiency issues in network-based high-performance computing, using different types of environments, for a widely accepted and representative class of scientific applications. Platform/ Cray i860 PVM Benchmark Y-MP 128 Comm. <p> Elements 512x512 512x512 1024x1024 1024x1024 4 39.3 36.1 358.3 363.4 16 41.1 18.8 336.2 173.1 36 31.2 17.9 216.0 144.7 Table 8: Matrix Multiply on a heterogeneous network (times in seconds). 6.3 Multigrid Solver Application As a second test application, we implemented the Multigrid Solver Kernel described as described in <ref> [Bai91] </ref>. The application executes four iterations of the V-cycle multigrid algorithm to obtain an approximate solution to the discrete Poisson problem r 2 u = v on a three dimensional grid with periodic boundary conditions.
Reference: [BL94] <author> R.M. Butler and E.L. Lusk. </author> <title> Monitors, Messages, and Clusters: The P4 Parallel Programming System. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 417-444, </pages> <year> 1994. </year>
Reference-contexts: Numerous software systems are in existence that support network based parallel computing [Tur93, Che91], although the majority of use is probably based on a few popular ones, such as PVM [GBD + 94], MPI [GLS94], and P4/Parmacs <ref> [BL94] </ref>. However, despite their widespread adoption and abundance of success stories [PVM95, Cluop], network-based concurrent computing systems suffer from several critical shortcomings. Principal among them are factors influencing, or in some way related to, performance.
Reference: [Che91] <author> Doreen Y. Cheng. </author> <title> A Survey of Parallel Programming Tools. </title> <type> Technical Report RND-91-005, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: For many classes of applications, loosely coupled concurrent computing in such environments has proven to be straightforward in terms of paralleliza-tion effort, satisfactory to very good, in terms of performance, and highly effective in terms of cost. Numerous software systems are in existence that support network based parallel computing <ref> [Tur93, Che91] </ref>, although the majority of use is probably based on a few popular ones, such as PVM [GBD + 94], MPI [GLS94], and P4/Parmacs [BL94]. However, despite their widespread adoption and abundance of success stories [PVM95, Cluop], network-based concurrent computing systems suffer from several critical shortcomings.
Reference: [Cluop] <institution> Workshop on Cluster Computing, Florida State University. Tallahassee, </institution> <address> FL, 1992,1993 ftp://ftp.scri.fsu.edu/pub/cluster-workshop/. </address>
Reference-contexts: Numerous software systems are in existence that support network based parallel computing [Tur93, Che91], although the majority of use is probably based on a few popular ones, such as PVM [GBD + 94], MPI [GLS94], and P4/Parmacs [BL94]. However, despite their widespread adoption and abundance of success stories <ref> [PVM95, Cluop] </ref>, network-based concurrent computing systems suffer from several critical shortcomings. Principal among them are factors influencing, or in some way related to, performance. The relatively slow networks and communications software typical in heterogeneous net-worked environments usually preclude high efficiencies.
Reference: [CR95] <author> Michel Christaller and Martha-Rosa Castaneda Retiz. </author> <title> Control Parallelism on top of PVM. </title> <booktitle> In Proceedings of the Second European PVM User's Group Meeting, </booktitle> <year> 1995. </year>
Reference-contexts: This system differs from TPVM in its "control parallel" programming model, and also lack of support for concurrent thread execution on multiprocessors. As the authors mention in <ref> [CR95] </ref>, an asynchronous RPC programming model such as this might be implemented over TPVM. It is worth mentioning another effort to introduce multithreading into the PVM system: the LPVM project at Oak Ridge National Laboratory [ZG95].
Reference: [Cra93] <author> S. Crane. </author> <title> The Rex Lightweight Process Library. </title> <type> Technical report, </type> <institution> Imperial College of Science and Technology, </institution> <address> London, England, </address> <year> 1993. </year>
Reference-contexts: It seems likely that almost any implementation of "threads" or "lightweight processes" could satisfy this loose set of requirements. To illustrate this point, we implemented our prototype design implemented over three significantly dissimilar thread systems: * For use with SunOS 4.x, we implemented the thread interface over the Rex <ref> [Cra93] </ref> thread system, which operates in user space, is non-preemptive, does not support true concurrent thread execution, and performs scheduling only at the request of the user (via a yield () operation). * We also implemented the thread interface over the Solaris threads library [Sun93], a POSIX compliant package which provides
Reference: [FS94] <author> A.J. Ferrari and V.S. Sunderam. TPVM: </author> <title> A Threads-Based Interface and Subsystem for PVM. </title> <type> Technical Report CSTR-940802, </type> <institution> Emory University Department of Mathematics and Computer Science, </institution> <address> Atlanta, GA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: tpvm_recv (MessageTag,-1); tpvm_pkint (&VecSize,1,1); tpvm_bufinfo (Buf,&Source,&Size,&Tag); 10 tpvm_pkdouble (Vector,Vecsize,1); printf ("Message from id %d"n",Source); tpvm_send (MessageTag,ttids [dest]); tpvm_upkint (&VecSize,1,1); tpvm_upkdouble (Vector,VecSize,1); Complete discussion of the details of PVM style message passing and buffer management primitives can be found in the PVM Users' Guide [GBD + 94] and the TPVM Manual <ref> [FS94] </ref>. 3.2 Data-driven TPVM Programming The TPVM data-driven programming model is intended to mask some of of the inherent complexity of the message passing model, and facilitate the construction of composable program elements.
Reference: [FS95] <author> A.J. Ferrari and V.S. Sunderam. TPVM: </author> <title> Distributed Concurrent Computing with Lightweight Processes. </title> <booktitle> In Proceedings of the 4th IEEE International Symposium on High Performance Distributed Computing, </booktitle> <address> Washington, D.C., </address> <month> August </month> <year> 1995. </year> <note> IEEE. </note>
Reference: [GBD + 94] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidyalingam S. Sunderam. </author> <title> PVM (Parallel Virtual Machine): A Users' Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Numerous software systems are in existence that support network based parallel computing [Tur93, Che91], although the majority of use is probably based on a few popular ones, such as PVM <ref> [GBD + 94] </ref>, MPI [GLS94], and P4/Parmacs [BL94]. However, despite their widespread adoption and abundance of success stories [PVM95, Cluop], network-based concurrent computing systems suffer from several critical shortcomings. Principal among them are factors influencing, or in some way related to, performance. <p> /* Receiver... */ tpvm_initsend (PvmDataDefault); Buf = tpvm_recv (MessageTag,-1); tpvm_pkint (&VecSize,1,1); tpvm_bufinfo (Buf,&Source,&Size,&Tag); 10 tpvm_pkdouble (Vector,Vecsize,1); printf ("Message from id %d"n",Source); tpvm_send (MessageTag,ttids [dest]); tpvm_upkint (&VecSize,1,1); tpvm_upkdouble (Vector,VecSize,1); Complete discussion of the details of PVM style message passing and buffer management primitives can be found in the PVM Users' Guide <ref> [GBD + 94] </ref> and the TPVM Manual [FS94]. 3.2 Data-driven TPVM Programming The TPVM data-driven programming model is intended to mask some of of the inherent complexity of the message passing model, and facilitate the construction of composable program elements.
Reference: [Geo88] <author> Geoffrey C. Fox et al. </author> <title> Solving Problems On Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: As a first example application for performance evaluation, we implemented 27 the matrix multiplication algorithm described in <ref> [Geo88] </ref>. This algorithm employs square subblock decomposition of the operand and result matrices, where each processing entity is initially is distributed one block of each operand and computes a single result subblock. Communication is regular and occurs in a mesh pattern.
Reference: [GLS94] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Numerous software systems are in existence that support network based parallel computing [Tur93, Che91], although the majority of use is probably based on a few popular ones, such as PVM [GBD + 94], MPI <ref> [GLS94] </ref>, and P4/Parmacs [BL94]. However, despite their widespread adoption and abundance of success stories [PVM95, Cluop], network-based concurrent computing systems suffer from several critical shortcomings. Principal among them are factors influencing, or in some way related to, performance.
Reference: [HCP94] <author> Matthew Haines, David Cronk, and Mehrotra Piyush. </author> <booktitle> On the Design of Chant: </booktitle>
Reference-contexts: An example of a similar project is Chant <ref> [HCP94] </ref>. This package combines the POSIX pthreads interface with the MPI communications interface, supporting both of these interfaces and allowing them to interoperate.
References-found: 12

