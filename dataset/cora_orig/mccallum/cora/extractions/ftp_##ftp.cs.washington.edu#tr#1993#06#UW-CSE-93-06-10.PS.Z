URL: ftp://ftp.cs.washington.edu/tr/1993/06/UW-CSE-93-06-10.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Modeling a Dynamic and Uncertain World I: Symbolic and Probabilistic Reasoning about Change 1  
Author: Steve Hanks, Drew McDermott 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  2 Department of Computer Science, Yale University  
Abstract: Technical Report 93-06-10 June 15, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Berzuini and Quaglini 1989] <author> Riccardo Bellazzi Berzuini, Carlo and Silvana Quaglini. </author> <title> Temporal Reasoning with Probabilities. </title> <booktitle> In Proceedings of the Uncertainty in Artificial Intelligence Workshop, </booktitle> <year> 1989. </year>
Reference: [Berzuini 1990] <author> Carlo Berzuini. </author> <title> Representing Time in Causal Probabilistic Networks. </title> <editor> In Max Henrion, editor, </editor> <booktitle> Uncertainty in Artificial Intelligence 5, </booktitle> <pages> pages 15-28. </pages> <publisher> Elsevier North Holland, </publisher> <year> 1990. </year>
Reference-contexts: Berzuini's work|[Berzuini and Quaglini 1989], <ref> [Berzuini 1990] </ref>|likewise provides a discussion of how an expert might build a graphical model that explicitly describes the time course of the system. The (human) expert describes the domain's causal structure by providing the model's state space and transition probabilities directly.
Reference: [Cooper et al. 1988] <author> Gregory F. Cooper, Eric J. Horvitz, and David E. Heckerman. </author> <title> A Method for Temporal Probabilistic Reasoning. </title> <type> Technical Report KSL 88-30, </type> <institution> Knowledge Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1988. </year> <note> Working paper. </note>
Reference-contexts: Our rule-based representation is simply more appropriate for the larger task of maintaining a world model that will be used for plan generation, refinement, and repair. 9.3 Expert-system approaches <ref> [Cooper et al. 1988] </ref> confronts the problem of probabilistic diagnostic reasoning, in particular determining the probability that a patient has a certain disease given that he has displayed a certain set of observable symptoms (evidence).
Reference: [de Kleer and Williams 1987] <author> Johan de Kleer and Brian Williams. </author> <title> Diagnosing Multiple Faults. </title> <journal> Artificial Intelligence, </journal> <volume> 32(1) </volume> <pages> 97-130, </pages> <year> 1987. </year> <month> 40 </month>
Reference-contexts: This model|the reliability probability parameter r coupled with a minimum-information assumption about the sensor's failure-mode behavior|is essentially the same as that adopted for electronic components in the literature on circuit diagnosis, e.g. <ref> [de Kleer and Williams 1987] </ref>. For the remainder of the paper we will assume a model consisting of the reliability parameter r only.
Reference: [Dean and Boddy 1988] <author> Thomas Dean and Mark Boddy. </author> <title> Reasoning about Partially Ordered Events. </title> <journal> Arti--ficial Intelligence, </journal> <volume> 36(3) </volume> <pages> 375-400, </pages> <year> 1988. </year>
Reference-contexts: This assumption indicates that a rule's precondition propositions could be expected to change state between potential firings of the rule, and allows us to consider rule firings in isolation. We noted that 17 See, for example, <ref> [Dean and Boddy 1988] </ref>. 35 rules representing an agent's intended actions, strung together in a plan, would typically violate this assumption, and this violation forces us to engage in explicit reasoning by cases. We confront this problem in [Hanks and McDermott 1992]. * Events occur dispassionately and predictably.
Reference: [Dean and Kanazawa 1988] <author> Thomas Dean and Keiji Kanazawa. </author> <title> Probabilistic Temporal Reasoning. </title> <booktitle> In Proceedings AAAI, </booktitle> <year> 1988. </year>
Reference-contexts: A promising area for future research is to establish the relationship between these semi-Markov models and symbolic rule-based theories in which a rule's consequent has an indeterminate delay and an event's time of occurrence is uncertain. 9.4 Rule-based approaches <ref> [Dean and Kanazawa 1988] </ref> presents a probabilistic model essentially identical to the one that we built in Section 3.6: they too take the idea that causal rules are triggered by events and effect changes in facts.
Reference: [Dean and Kanazawa 1989] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A Model for Reasoning about Persistence and Causation. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: We then provide probabilities for the facts at the initial time point t 0 and propagate that information through the graph. This method is equivalent to the usual method for reasoning about Markov random processes, and is similar to the approach suggested by <ref> [Dean and Kanazawa 1989] </ref>. 13 We call this approach a "time-based" approach to solving the system. While this is an adequate formal model for solving the system it can be quite inefficient, for several reasons.
Reference: [Dean 1985] <author> Thomas Dean. </author> <title> Temporal Imagery: An Approach to Reasoning about Time for Planning and Problem Solving. </title> <type> Technical Report 433, </type> <institution> Yale University, Department of Computer Science, </institution> <month> October </month> <year> 1985. </year> <type> Ph.D. thesis. </type>
Reference-contexts: This work describes a "world-model manager"| a program module that stores and operates on an agent's model of its world. 1 Such a program is similar in spirit to temporal database managers, e.g. in <ref> [Dean 1985] </ref>, though we will introduce significant differences both in representation and functionality, the most significant of which is our explicit treatment of uncertainty.
Reference: [Haddawy and Hanks 1990] <author> Peter Haddawy and Steve Hanks. </author> <title> Issues in Decision-Theoretic Planning: Symbolic Goals and Numeric Utilities. In Workshop on Innovative Approaches to Planning, Scheduling, </title> <journal> and Control, </journal> <month> November </month> <year> 1990. </year>
Reference-contexts: Weighing the discomfort of getting wet against the inconvenience of having to find a parking place, I arrive at some threshold for the probability of rain, say 0.6, above which I will decide to drive. The threshold presumably comes from an analysis of the planner's current goals|see <ref> [Haddawy and Hanks 1990] </ref>. At that point the exact probability of rain is of no interest to me; all that matters is what side of the threshold the true probability lies. <p> the graphical network from a set of rules. 20 Things are a little more complicated than that because the updates are defined in terms of logarithms. 38 respect to a threshold t amounts to asking which of the intervals [1; t ] and [t; 1] the exact probability lies in. <ref> [Haddawy and Hanks 1990] </ref> shows a decision-theoretic justification for queries of this form.
Reference: [Haddawy and Hanks 1992] <author> Peter Haddawy and Steve Hanks. </author> <title> Representations for Decision-Theoretic Panning: Utility Functions for Deadline Goals. </title> <booktitle> In Third International Conference on Knowledge Representation and Reasoning, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Planning, replanning, and learning We have yet to integrate the model manager into a planning system. Interesting questions arise as to how a planner might use decision-theoretic methods to guide the process of plan selection <ref> [Haddawy and Hanks 1992] </ref>, and how that process affects the interface between planner and world model.
Reference: [Hanks and Firby 1990] <author> Steve Hanks and R. James Firby. </author> <title> Issues and Architectures for Planning and Execution. In Workshop on Innovative Approaches to Planning, Scheduling, </title> <journal> and Control, </journal> <month> November </month> <year> 1990. </year>
Reference-contexts: Context should make clear the word's sense. 2 "Projected" means that the probability that a proposition will be true at time t is computed on the basis only of the world's state at time prior to t. See Section 3.6.1 for more detail. 3 See <ref> [Hanks and Firby 1990] </ref> for more details. 4 The summary takes the form of a probability, so each belief consists of a probability distribution over the proposition's state 4 along with the time point at which the proposition's state was queried.
Reference: [Hanks and McDermott 1987] <author> Steven Hanks and Drew McDermott. </author> <title> Nonmonotonic Logic and Temporal Projection. </title> <journal> Artificial Intelligence, </journal> <volume> 33(3) </volume> <pages> 379-412, </pages> <year> 1987. </year>
Reference-contexts: The agent's world model consists of a set of causal rules that dictate what event types cause what changes to what fact types under what circumstances. One such rule (a variant of which appears in <ref> [Hanks and McDermott 1987] </ref>) represents the information that shooting somebody with a loaded gun usually causes that person to become dead (actually, "not alive"): 11 (DEFINE-RULE-SCHEMA 'GUN-SHOOTING '(PRECONDITIONS (LOADED ?G)) '(TRIGGER (SHOOT ?P ?G)) '(CONSEQUENT (ALIVE ?P)) '(SIGN 0) '(EFFECTIVENESS 0.9)) A SIGN of 0 means the same thing as negating
Reference: [Hanks and McDermott 1992] <author> Steven Hanks and Drew McDermott. </author> <title> Modeling a Dynamic and Uncertain World II: Projecting Courses of Action. </title> <journal> Artificial Intelligence, </journal> <note> 1992. Submitted. </note>
Reference-contexts: Computing states of the world based on plan commitments is the problem typically known as "plan projection," and is the topic of <ref> [Hanks and McDermott 1992] </ref>. Here we will concentrate on the process of computing probabilities based on sensory evidence and the causal theory. <p> The joint event (', ) corresponds to the conjunction (' ^ ). We will discuss below the problem of computing the probabilities of joint events. We allow quantification only in special domain-dependent cases, which we will discuss in <ref> [Hanks and McDermott 1992] </ref>. When we speak of propositions such as ', however, we are talking about atomic propositions|no connectives. <p> approximations that (1) can be arrived at quickly, and (2) provide the agent with the information necessary to make decisions about its future actions. 8 The final point, that a single probability number does not suffice to represent concepts related to the agent's state of information, will be addressed in <ref> [Hanks and McDermott 1992] </ref>. 3 Uncertainty, Evidence, and Probabilistic Updates Estimating a proposition's probability can be considered in two steps: 1. choosing an appropriate body of evidence E, and 2. computing the probability P (' t jE). <p> The reflects two factors: a fixed cost associated with setting up and executing a fetch in the temporal database, and a variable cost associated with how far back in time the fetch covers. The magnitudes of these numbers reflect the actual implementation, of course|we find in <ref> [Hanks and McDermott 1992] </ref> that most of the time is spent in the temporal database manager module computing distances between time points, so efficiency gains in that module would improve these results considerably. 7 Assessing joint events The assessment algorithm estimates the probability of simple propositions. <p> In the first case we rely on a "scenario" or "hypothetical future" structure generated by the projection algorithm to resolve the dependencies. We will formally introduce the scenario structure in <ref> [Hanks and McDermott 1992] </ref> 16 ; here we will just hint at how such a structure can be used to avoid the over- or under-estimation of joint probabilities in those cases where the propositions are interrelated by sharing causal rule components. <p> We noted that 17 See, for example, [Dean and Boddy 1988]. 35 rules representing an agent's intended actions, strung together in a plan, would typically violate this assumption, and this violation forces us to engage in explicit reasoning by cases. We confront this problem in <ref> [Hanks and McDermott 1992] </ref>. * Events occur dispassionately and predictably. We assumed that the probability with which an event occurs does not depend on the propositions that figure in the rule for which that event is a trigger. <p> Relaxing these assumptions involves more reasoning by cases, which tends to be computationally expensive. Additionally, planning involves reasoning about a richer collection of phenomena than can be conveniently modeled using our restriction of rule consequents to binary random variables. We report on this work in <ref> [Hanks and McDermott 1992] </ref>. 2. Relaxing structural assumptions Relaxing the "projection" assumption|that all evidence bearing on ''s state at t occurs prior to t|complicates the probability computation, but makes the algorithm amenable to a wider variety of tasks, such as failure diagnosis and replanning.
Reference: [Hanks 1987] <author> Steven Hanks. </author> <title> Temporal Reasoning About Uncertain Worlds. </title> <booktitle> In Proceedings of the Uncertainty in Artificial Intelligence Workshop, </booktitle> <pages> pages 114-122, </pages> <year> 1987. </year>
Reference-contexts: We will also discuss briefly the temporal database manager module, which is used by the rest of the system. This work is covered more thoroughly in <ref> [Hanks 1987] </ref> and [Hanks 1990b, Chapter 2]. 2.1 Temporal ontology First of all we will adopt a point-based representation of time, meaning that the temporal individuals will denote instants in time. We will use variables like t and t i to refer to these individuals.
Reference: [Hanks 1990a] <author> Steven Hanks. </author> <title> Practical Temporal Projection. </title> <booktitle> In Proceedings AAAI, </booktitle> <year> 1990. </year>
Reference-contexts: correctly computes P (P; Q j E) = P (P; Q j E; C)P (C j E) + P (P; Q j E; C)P (C j E) (where P (C j E) = P (C) by assumption), thus avoiding the problem of double-counting the effect of C. 16 Also see <ref> [Hanks 1990a] </ref>. 34 8 Assumptions The assessment algorithm embodies several assumptions both stated and unstated. They divide naturally into three groups: those concerning the temporal model, those concerning the causal rules, and those concerning the probabilistic model and its related calculations.
Reference: [Hanks 1990b] <author> Steven Hanks. </author> <title> Projecting Plans for Uncertain Worlds. </title> <type> Technical Report 756, </type> <institution> Yale University, Department of Computer Science, </institution> <month> January </month> <year> 1990. </year> <type> Ph.D. thesis. </type>
Reference-contexts: We will also discuss briefly the temporal database manager module, which is used by the rest of the system. This work is covered more thoroughly in [Hanks 1987] and <ref> [Hanks 1990b, Chapter 2] </ref>. 2.1 Temporal ontology First of all we will adopt a point-based representation of time, meaning that the temporal individuals will denote instants in time. We will use variables like t and t i to refer to these individuals. <p> That our assumption about the failure-mode probabilities should add the least possible amount of information to the distribution. This is in effect the "maximum-entropy" solution ([Jaynes 1979] [Hunter 1989]): choose the (unique) distribution consistent with the constraints that exhibits maximum entropy (minimum information). We show in <ref> [Hanks 1990b, Section 3.4] </ref> that the three assumptions are equivalent: that the maximum entropy distribution implies P (O j R; ') = P (O j R; ') = 0:5; which in turn implies P (' j O; R) = P (' j O; R) = P ('): In other words, the
Reference: [Howard 1971] <author> Ronald A. Howard. </author> <title> Dynamic Probabilistic Systems|Volume I: Markov Models. </title> <publisher> John Wiley & Sons, </publisher> <year> 1971. </year>
Reference-contexts: A state change between t i &gt; and t i+1 is an unpredicted state change. The usual approach to reasoning about probabilistic dynamic systems, e.g. in <ref> [Howard 1971] </ref>, is to assume the probabilities P (' i+1 j' i ) and P (' i+1 j' i ) (called "transition probabilities") are available directly as inputs to the system. <p> Our algorithm for computing probability given a fixed set of information corresponds to Shoham's definition of a chronologicallly minimal model. 18 9.2 Markov processes Before discussing alternative proposals in the AI literature we should mention briefly the literature on Markov processes, e.g. <ref> [Howard 1971] </ref>. The idea is to represent the domain by a state space, in which every state represents a complete static description of the world (or that portion of it under study).
Reference: [Hunter 1989] <author> D. Hunter. </author> <title> Causality and Maximum Entropy Updating. </title> <journal> International Journal of Approximate Reasoning, </journal> <year> 1989. </year>
Reference-contexts: That our assumption about the failure-mode probabilities should add the least possible amount of information to the distribution. This is in effect the "maximum-entropy" solution ([Jaynes 1979] <ref> [Hunter 1989] </ref>): choose the (unique) distribution consistent with the constraints that exhibits maximum entropy (minimum information).
Reference: [Jaynes 1979] <author> E.T. Jaynes. </author> <title> Where Do We Stand on Maximum Entropy? In R.D Levine and M. </title> <editor> Tribus, editors, </editor> <title> The Maximum Entropy Formalism. </title> <publisher> MIT Press, </publisher> <year> 1979. </year>
Reference: [Kanazawa 1991] <author> Keiji Kanazawa. </author> <title> A Logic and Time Nets for Probabilistic Inference. </title> <booktitle> In Proceedings AAAI, </booktitle> <year> 1991. </year>
Reference: [Lifschitz 1987] <author> Vladimir Lifschitz. </author> <title> Formal Theories of Action. </title> <editor> In Frank Brown, editor, </editor> <booktitle> The Frame Problem in Artificial Intelligence: Proceedings of the 1987 Workshop. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: instant t &gt; . (If we were to assume a discrete temporal model we would set t &gt; = t + 1.) 3.3.1 The structure of causal rules Many proposals have appeared for reasoning about the dynamics of the world, variously called theories of time [McDermott 1982], theories of action <ref> [Lifschitz 1987] </ref> or theories of change [Shoham 1988]. The idea in all cases is the same: we identify events as the propositions that initiate change in the world; events change the state of facts.
Reference: [McCarthy 1987] <author> John McCarthy. </author> <title> Applications of Circumscription to Formalizing Common Sense Knowledge. </title> <editor> In Matthew L. Ginsberg, editor, </editor> <booktitle> Readings in Nonmonotonic Reasoning, </booktitle> <pages> pages 153-166. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1987. </year> <month> 41 </month>
Reference-contexts: This system represents an advance over deterministic theories of change ([McDermott 1982], <ref> [McCarthy 1987] </ref>, [Shoham 1988]) both due to its greater expressive power and due to its ability to limit its inference according to the demands of the task at hand.
Reference: [McDermott 1982] <author> Drew McDermott. </author> <title> A Temporal Logic for Reasoning About Processes and Plans. </title> <journal> Cognitive Science, </journal> <volume> 6 </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: We allow quantification only in special domain-dependent cases, which we will discuss in [Hanks and McDermott 1992]. When we speak of propositions such as ', however, we are talking about atomic propositions|no connectives. Our causal model, inspired by McDermott's concept of the "pcause" rule <ref> [McDermott 1982] </ref>, grants special semantic status to a class of propositions known as "events." Events are the propositions that effect changes in the world. The agent's actions are events. <p> time t and the next instant t &gt; . (If we were to assume a discrete temporal model we would set t &gt; = t + 1.) 3.3.1 The structure of causal rules Many proposals have appeared for reasoning about the dynamics of the world, variously called theories of time <ref> [McDermott 1982] </ref>, theories of action [Lifschitz 1987] or theories of change [Shoham 1988]. The idea in all cases is the same: we identify events as the propositions that initiate change in the world; events change the state of facts.
Reference: [Nunez 1989] <author> Linda I. Mensinger Nunez. </author> <title> Relationship Between Temporal Bayes Networks and Markov Random Process Transition Tables. </title> <type> Technical Report CS-89-M2, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: While the representational power of these systems is the same as for our symbolic graphical models (see <ref> [Nunez 1989] </ref>, for example), the problem with the standard Markov-process representation is that it is inconvenient.
Reference: [Pearl 1988] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The philosophical issues begin a debate we have no wish to enter at this point; there is an excellent summary of the issues in <ref> [Pearl 1988, Chapter 1] </ref>. <p> A more traditional update technique (see <ref> [Pearl 1988, Chapter 2] </ref>, for example) uses two likelihood ratios instead, o = P (O j ') P (O j ') 10 Note that we no longer need to condition on E. Prior information is summarized in the parameter p. <p> We can also describe the model in terms of a graph <ref> [Pearl 1988] </ref>: each node in the graph represents a quantity that can be assigned a probability and the arcs connecting nodes indicate a probabilistic dependency. <p> true before and whether an unpredicted state change took place over the interval. 3.6.1 Projected probabilities We should note that although Figure 3 illustrates the probability space and how elements of the space are related, our update algorithm is not the same as the Bayesian propagation algorithm (as described in <ref> [Pearl 1988] </ref> for example). The difference is that our algorithm computes probabilities only from earlier to later time points, so a proposition's state at time t can only have an effect on the system's state at times subsequent to t.
Reference: [Shoham and Goyal 1988] <author> Yoav Shoham and Nita Goyal. </author> <title> Temporal Reasoning in Artificial Intelligence. </title> <editor> In Howard Shrobe, editor, </editor> <booktitle> Exploring Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [Shoham 1988] <author> Yoav Shoham. </author> <title> Reasoning about Change and Time from the Standpoint of Aritificial Intelligence. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: were to assume a discrete temporal model we would set t &gt; = t + 1.) 3.3.1 The structure of causal rules Many proposals have appeared for reasoning about the dynamics of the world, variously called theories of time [McDermott 1982], theories of action [Lifschitz 1987] or theories of change <ref> [Shoham 1988] </ref>. The idea in all cases is the same: we identify events as the propositions that initiate change in the world; events change the state of facts. <p> This assumption allowed us to compute ''s probability at t taking into account the world's state prior to t only. All of these assumptions are common in the work on causal reasoning within a logical framework (e.g. <ref> [Shoham 1988] </ref>). Finally we made the following assumptions about the structure of the probability space: * Sensor reliability not systematically predictable. <p> This system represents an advance over deterministic theories of change ([McDermott 1982], [McCarthy 1987], <ref> [Shoham 1988] </ref>) both due to its greater expressive power and due to its ability to limit its inference according to the demands of the task at hand.
Reference: [Tatman and Shachter 1990] <author> Joseph A. Tatman and Ross D. Shachter. </author> <title> Dynamic Programming and Influence Diagrams. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 20(2) </volume> <pages> 365-379, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: They suggest using approximate simulation methods to solve the network, but provide no insight into the time required to guarantee convergence. 19 <ref> [Tatman and Shachter 1990] </ref> propose an alternative approach that involves applying dynamic-programming techniques.
Reference: [Weber 1989a] <author> Jay Weber. </author> <title> A Parallel Algorithm for Statistical Belief Refinement and its use in Causal Reasoning. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> pages 900-905, </pages> <year> 1989. </year>
Reference: [Weber 1989b] <author> Jay Weber. </author> <title> Principles and Algorithms for Causal Reasoning under Uncertainty. </title> <type> Technical Report 287, </type> <institution> University of Rochester, Department of Computer Science, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We try to reason only about those parts of the network that are of interest to the problem solver and only to the degree of accuracy it requires. 9.5 Belief updating by assessing impact Weber's work ([Weber 1989a], <ref> [Weber 1989b] </ref>) is a computational theory of how to form beliefs based on statistical information. The idea is that estimating the likelihood of some proposition (say whether my car will start) suggests the problem of what information is relevant and significant to the prediction. <p> Weber's algorithm would continue incorporating positive evidence, even though it would only continue to confirm the current hypothesis. <ref> [Weber 1989b] </ref> also provides classes of "statistical causal rules" that correspond to the causal information we code in causal rules, observations, and background probabilities.
Reference: [Wellman 1988] <author> Michael P. Wellman. </author> <title> Formulation of Tradeoffs in Planning under Uncertainty. </title> <type> Technical Report MIT/LCS/TR-427, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> August </month> <year> 1988. </year> <type> Ph.D. thesis. 42 </type>
Reference-contexts: Nonetheless, what we are doing is computing probabilities, and the reader is free to interpret them any way he or she wants. 8 <ref> [Wellman 1988] </ref> advances this position as well. 10 We will assume that we will get observational evidence about ' only in the form of direct (but possibly inaccurate) testimony.
References-found: 31

