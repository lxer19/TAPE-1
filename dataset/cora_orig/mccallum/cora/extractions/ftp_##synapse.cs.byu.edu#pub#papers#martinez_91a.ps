URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_91a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: A Self-Adjusting Dynamic Logic Module  
Author: Tony R. Martinez and Douglas M. Campbell 
Keyword: ASOCS, Self-organization, Neural Networks, Connectionist Computing, Parallel, Rule-based, Machine Learning.  
Address: Provo, Utah 84602  
Affiliation: Computer Science Department Brigham Young University  
Note: In Journal of Parallel and Distributed Computing, vol. 11, No. 4, pp. 303-313, 1991.  
Abstract: This paper presents an ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic, robotics, logical inference, and dynamic control. An ASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. This paper focuses on Adaptive Algorithm 2 (AA2) and details its architecture and learning algorithm. AA2 has significant memory and knowledge maintenance advantages over previous ASOCS models. An ASOCS can operate in either a data processing mode or a learning mode. During learning mode, the ASOCS is given a new rule expressed as a boolean conjunction. The AA2 learning algorithm incorporates the new rule in a distributed fashion in a short, bounded time. During data processing mode, the ASOCS acts as a parallel hardware circuit. 
Abstract-found: 1
Intro-found: 1
Reference: 10. <author> Martinez, T. R., </author> <title> Adaptive Self-Organizing Concurrent Systems, </title> <booktitle> in Progress in Neural Networks, </booktitle> <publisher> Ablex Publishing, </publisher> <year> 1989. </year> <month> 20 </month>
Reference: 11. <author> Martinez, T. R. and Campbell, </author> <title> D.M., A Self-Organizing Binary Decision Tree for Arbitrary Functions, </title> <note> Submitted. </note>
Reference-contexts: Details for AA1 can be found in [8]; details for AA3 can be found in <ref> [6, 11] </ref>. These three algorithms vary dramatically, although AA3 shares some important aspects of AA2. ASOCS arose from reexamining perceptron [13] related ideas. The basic building block, however, is that of digital programmable nodes, an idea spawned by the notion of a universal logic module (ULM) [16].
Reference: 12. <editor> Michalski, R., J. Carbonell, and T. Mitchell, Eds., </editor> <booktitle> Machine Learning, </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Rather, distributed, parallel, and self-organizing paradigms are used in order to attain an improved computational mechanism, offering speed, fault tolerance, and ease of use. The model also differs from standard AI machine learning techniques, such as the A q algorithm <ref> [12] </ref>, since AA2 does not do heuristic search over an inductive space. This paper seeks to overview the AA2 processing and learning algorithms. Discussion of specific implementations is beyond that scope and can be found elsewhere [1, 6]. The outline of the paper follows.
Reference: 13. <author> Rosenblatt, F., </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan Books, </publisher> <address> Washington, D.C. </address> <year> (1962). </year>
Reference-contexts: Details for AA1 can be found in [8]; details for AA3 can be found in [6, 11]. These three algorithms vary dramatically, although AA3 shares some important aspects of AA2. ASOCS arose from reexamining perceptron <ref> [13] </ref> related ideas. The basic building block, however, is that of digital programmable nodes, an idea spawned by the notion of a universal logic module (ULM) [16]. Verstraete [15] sought methods of programming fixed ULM structures to solve arbitrary boolean mappings.
Reference: 14. <author> Rumelhart, D. and McClelland, J., </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. I, </title> <publisher> MIT Press, </publisher> <year> (1986). </year>
Reference: 15. <author> Verstraete, R.A., </author> <title> Assignment of Functional Responsibility in Perceptrons, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, CA, </address> <month> (June </month> <year> 1986). </year>
Reference-contexts: These three algorithms vary dramatically, although AA3 shares some important aspects of AA2. ASOCS arose from reexamining perceptron [13] related ideas. The basic building block, however, is that of digital programmable nodes, an idea spawned by the notion of a universal logic module (ULM) [16]. Verstraete <ref> [15] </ref> sought methods of programming fixed ULM structures to solve arbitrary boolean mappings. ASOCS departs drastically from these efforts by having a nonpassive network which adapts in a self-organizing fashion [6, 9]. ASOCS models offer parallel inference, high speed adaptation, and internal consistency control [6,7,8].
Reference: 16. <author> Yau, S. S. and C. K. Tang, </author> <title> Universal Logic Circuits and their Modular Realizations, </title> <booktitle> AFIPS Conference Proceedings, </booktitle> <volume> vol. 32, </volume> <pages> pp. 297-305, </pages> <year> (1968). </year>
Reference-contexts: These three algorithms vary dramatically, although AA3 shares some important aspects of AA2. ASOCS arose from reexamining perceptron [13] related ideas. The basic building block, however, is that of digital programmable nodes, an idea spawned by the notion of a universal logic module (ULM) <ref> [16] </ref>. Verstraete [15] sought methods of programming fixed ULM structures to solve arbitrary boolean mappings. ASOCS departs drastically from these efforts by having a nonpassive network which adapts in a self-organizing fashion [6, 9]. ASOCS models offer parallel inference, high speed adaptation, and internal consistency control [6,7,8].
References-found: 7

