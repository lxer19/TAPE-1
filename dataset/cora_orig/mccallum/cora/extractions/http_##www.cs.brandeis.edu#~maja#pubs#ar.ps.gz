URL: http://www.cs.brandeis.edu/~maja/pubs/ar.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~maja/publications.html
Root-URL: http://www.cs.brandeis.edu
Email: maja@cs.brandeis.edu  
Phone: tel: (617) 736-2708 fax: (617) 736-2741  
Title: Reinforcement Learning in the Multi-Robot Domain  
Author: Maja J Mataric 
Address: Waltham, MA 02254  
Affiliation: Volen Center for Complex Systems Computer Science Department Brandeis University  
Abstract: This paper describes a formulation of reinforcement learning that enables learning in noisy, dynamic environemnts such as in the complex concurrent multi-robot learning domain. The methodology involves minimizing the learning space through the use behaviors and conditions, and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators. We experimentally validate the ap proach on a group of four mobile robots learning a foraging task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S. & Hosoda, K. </author> <year> (1994), </year> <title> Coordination of Multiple Behaviors Acquired by A Avision-Based Reinforcement Learning, </title> <booktitle> in `Proceedings, IEEE/RSJ/GI International Conference on In telligent Robots and Systems', </booktitle> <address> Munich, Germany. </address>
Reference: <author> Atkeson, C. G. </author> <year> (1989), </year> <title> Using Local Models to Control Movement, </title> <booktitle> in `Proceedings, Neural Information Processing Systems Conference'. </booktitle>
Reference: <author> Atkeson, C. G., Aboaf, E. W., McIntyre, J. & Reinkensmeyer, D. J. </author> <year> (1988), </year> <title> Model-Based Robot Learning, </title> <type> Technical Report AIM-1024, </type> <institution> MIT. </institution>
Reference: <author> Barto, A. G., Bradtke, S. J. & Singh, S. P. </author> <year> (1993), </year> <title> `Learning to Act using Real-Time Dynamic Programming', </title> <journal> AI Journal. </journal>
Reference: <author> Brooks, R. A. </author> <year> (1986), </year> <title> `A Robust Layered Control System for a Mobile Robot', </title> <journal> IEEE Journal of Robotics and Automation RA-2, </journal> <pages> 14-23. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1990), </year> <title> The Behavior Language; User's Guide, </title> <type> Technical Report AIM-1227, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: Inter-robot communication consists of broadcasting 6-byte messages at the rate of 1 Hz. In the experiments described here, the radios are used to determine the presence of other nearby robots. The robots are programmed in the Behavior Language <ref> (Brooks 1990) </ref> and tested in the workspace shown in Figure 2. 5 The Learning Task The learning task consists of finding a mapping from conditions to behaviors into the most effective policy for group foraging.
Reference: <author> Brooks, R. A. </author> <year> (1991), </year> <title> Intelligence Without Reason, </title> <booktitle> in `Proceedings, IJCAI-91'. </booktitle>
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> PhD thesis, </type> <institution> Stan-ford University. </institution>
Reference: <author> Lin, L.-J. </author> <year> (1991a), </year> <title> Programming Robots Using Reinforcement Learning and Teaching, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 781-786. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1991b), </year> <title> Self-improving Reactive Agents: Case Studies of Reinforcement Learning Frameworks, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to Coordinate Behaviors, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Boston, MA, </address> <pages> pp. 796-802. </pages> <note> 20 Mahadevan, </note> <author> S. & Connell, J. </author> <year> (1990), </year> <title> Automatic Programming of Behavior--based Robots using Reinforcement Learning, </title> <type> Technical report, </type> <institution> IBM T. </institution>
Reference: <institution> J. Watson Research Center Research Report. </institution>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991a), </year> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference-contexts: Lin (1991b) studied reinforcement learning in a group of simulated agents. The formulation we described is a direct extension of behavior-based control (Mataric 1992a, Brooks 1991, Brooks 1986). The presented heterogeneous reward functions are related to subgoals <ref> (Mahadevan & Connell 1991a) </ref> as well as subtasks (Whitehead, Karlsson & Tenenberg 1993). 9 Summary This paper described an approach to formulating reinforcement learning for applying it in noisy, dynamic domains.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991b), </year> <title> Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture, </title> <booktitle> in `Eighth International Workshop on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 328-337. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1992a), </year> <title> Behavior-Based Systems: Key Properties and Implications, </title> <booktitle> in `IEEE International Conference on Robotics and Automation, Workshop on Architectures for Intelligent Control Systems', Nice, France, </booktitle> <pages> pp. 46-54. </pages>
Reference-contexts: Tan (1993) explored reinforcement learning in a situated multi-agent domain utilizing communication to share learned information. Lin (1991b) studied reinforcement learning in a group of simulated agents. The formulation we described is a direct extension of behavior-based control <ref> (Mataric 1992a, Brooks 1991, Brooks 1986) </ref>. The presented heterogeneous reward functions are related to subgoals (Mahadevan & Connell 1991a) as well as subtasks (Whitehead, Karlsson & Tenenberg 1993). 9 Summary This paper described an approach to formulating reinforcement learning for applying it in noisy, dynamic domains.
Reference: <author> Mataric, M. J. </author> <year> (1992b), </year> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Foraging was chosen because it is a complex and biologically inspired task, because it serves as a canonical abstraction of a variety of real-world applications (such as demining and toxic waste clean up), and because our previous group behavior work <ref> (Mataric 1992b, Mataric 1993) </ref> provided the basic behavior repertoire from which to learn behavior selection. That repertoire, given to the robots a priori, consisted of the following fixed behavior set: 7 iment.
Reference: <author> Mataric, M. J. </author> <year> (1993), </year> <title> Kin Recognition, Similarity, and Group Behavior, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 705-710. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1994a), </year> <title> Interaction and Intelligent Behavior, </title> <type> Technical Report AI-TR-1495, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: This section describes how we addressed each one. 3.1 Behaviors and Conditions Our work can be classified as behavior-based since it involves the use of behaviors as the basic representation level for control and learning. Behaviors are goal-driven control laws that achieve and/or maintain particular goals <ref> (Mataric 1994a) </ref>. Behaviors with achievement goals, such as homing, terminate when the goal is reached, while behaviors with maintenance goals, such as wall-following, continue execution as long as their conditions are satisfied. Both kinds of behaviors can be externally terminated. <p> They are designed (or learned) so as to provide the desired outputs while abstracting away the low level details of control. Well-designed behaviors utilize the dynamics of the system and its interaction with the world in order to achieve robust and repeatable performance <ref> (Mataric 1994a) </ref>. Behaviors are triggered by conditions, predicates on sensor readings that 4 map into a proper subset of the state space. Each condition is defined as the part of the state that is necessary and sufficient for activating a particular behavior. <p> The truth value of a condition determines when a behavior can be executed and when it should be terminated, thus providing a set of events for a learner's control algorithm. In general, conditions for execution of any behavior are given by the formal specification of that behavior <ref> (Mataric 1994a) </ref>. Thus, if a fixed behavior set is used by a learning robot, its condition set can be computed off-line. This set is typically much smaller than the robot's complete state space. <p> The table is initialized to the average of the minimum and maximum possible A (c; b) values. The desired policy, shown in Table 1, was derived by hand, based on empirical data from the hard-coded foraging experiments in which it was independently tested and compared to alternative solutions <ref> (Mataric 1994a) </ref>. 7 Experimental Results and Evaluation The effectiveness of the proposed reinforcement functions was evaluated by testing three different types of reinforcement. The following three approaches were compared: 1.
Reference: <author> Mataric, M. J. </author> <year> (1994b), </year> <note> Learning to Behave Socially, </note> <editor> in D. Cliff, P. Husbands, J.-A. Meyer & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <pages> pp. 453-462. </pages>
Reference-contexts: Learning such rules is a challenging problem since they may not necessarily have immediate or even delayed payoff to the individual. Our more recent work has successfully demonstrated an extension of the described approaches to learning such social rules, including yielding, proceeding, and broadcasting <ref> (Mataric 1994b) </ref>. 8 Related Work Very few demonstrations of group behavior on physical robots have been performed to date. This section reviews the most relevant multi-robot learning work as well as the related work on applying reinforcement learning to a single robot.
Reference: <author> Millan, J. D. R. </author> <year> (1994), </year> <title> Learning Reactive Sequences from Basic Reflexes, </title> <booktitle> in `Proceedings, Simulation of Adaptive Behavior SAB-94', </booktitle> <publisher> The MIT Press, </publisher> <address> Brighton, England, </address> <pages> pp. 266-274. </pages> <note> 21 Moore, </note> <author> A. W. </author> <year> (1992), </year> <title> `Fast, Robust Adaptive Control by Learning only For--ward Models', </title> <booktitle> Advances in Neural Information Processing 4 pp. </booktitle> <pages> 571-579. </pages>
Reference: <author> Parker, L. E. </author> <year> (1994), </year> <title> Heterogeneous Multi-Robot Cooperation, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Pomerleau, D. A. </author> <year> (1992), </year> <title> Neural Network Perception for Mobile Robotic Guidance, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference: <author> Schaal, S. & Atkeson, C. C. </author> <year> (1994), </year> <title> `Robot Juggling: An Implementation of Memory-Bassed Learning', </title> <journal> Control Systems Magazine 14, </journal> <pages> 57-71. </pages>
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> `Learning to Predict by Method of Temporal Differences', </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference-contexts: This is of particular importance for making learning possible in the complex multi-robot domain. The next step is to make learning efficient, by using appropriate reinforcement. Simplifying and minimizing reinforcement, as practiced by some early RL algorithms <ref> (Sutton 1988) </ref>, diminishes programmer bias, but also greatly handicaps and in some domains completely debilitates the learner. We propose shaped reinforcement as a means of taking advantage of as much information as is available to the robot at any point.
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Thrun, S. B. & Mitchell, T. M. </author> <year> (1993), </year> <title> Integrating Inductive Neural Network Learning and Explanation-Based Learning, </title> <booktitle> in `Proceedings, IJCAI-93', </booktitle> <address> Chambery, France. </address>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992), </year> <title> `Q-Learning', </title> <booktitle> Machine Learning 8, </booktitle> <pages> 279-292. </pages>
Reference-contexts: In this paper we describe a method for state clustering through the use of behaviors and conditions. Regardless of the state representation used, reinforcement learning algorithms rely on their dynamic programming roots for clean theoretical convergence properties <ref> (Watkins & Dayan 1992, Barto, Bradtke & Singh 1993) </ref> which in turn require large numbers of learning trials that are prohibitive in physical robot domains.
Reference: <author> Whitehead, S. D., Karlsson, J. & Tenenberg, J. </author> <year> (1993), </year> <title> Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging, </title> <editor> in J. H. Connell & S. Mahadevan, eds, </editor> <title> `Robot Learning', </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 45-78. 22 </pages>
Reference-contexts: Lin (1991b) studied reinforcement learning in a group of simulated agents. The formulation we described is a direct extension of behavior-based control (Mataric 1992a, Brooks 1991, Brooks 1986). The presented heterogeneous reward functions are related to subgoals (Mahadevan & Connell 1991a) as well as subtasks <ref> (Whitehead, Karlsson & Tenenberg 1993) </ref>. 9 Summary This paper described an approach to formulating reinforcement learning for applying it in noisy, dynamic domains. The concurrent multi-robot learning domain was chosen as the validation environment for the task of learning to forage in a group of four robots.
References-found: 28

