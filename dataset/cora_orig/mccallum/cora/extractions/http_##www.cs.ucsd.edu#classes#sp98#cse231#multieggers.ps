URL: http://www.cs.ucsd.edu/classes/sp98/cse231/multieggers.ps
Refering-URL: http://www.cs.ucsd.edu/classes/sp98/cse231/
Root-URL: http://www.cs.ucsd.edu
Email: levy-@cs.washington.edu  tullsen@cs.ucsd.edu  
Title: Tuning Compiler Optimizations for Simultaneous Multithreading  
Author: Jack L. Lo, Susan J. Eggers, Henry M. Levy, Sujay S. Parekh, and Dean M. Tullsen* -jlo, sparekh, eggers, 
Address: Box 352350  Seattle, WA 98195-2350  9500 Gilman Drive La Jolla, CA 92093-0114  
Affiliation: Dept. of Computer Science and Engineering  University of Washington  *Dept. of Computer Science and Engineering University of California, San Diego  
Abstract: This paper reexamines several compiler optimizations in the context of simultaneous multithreading (SMT), a processor architecture that issues instructions from multiple threads to the functional units each cycle. Unlike shared-memory multiprocessors, SMT provides and benefits from fine-grained sharing of processor and memory system resources; unlike current uniprocessors, SMT exposes and benefits from inter-thread instruction-level parallelism when hiding latencies. Therefore, optimizations that are appropriate for these conventional machines may be inappropriate for SMT. We revisit three optimizations in this light: loop-iteration scheduling, software speculative execution, and loop tiling. Our results show that all three optimizations should be applied differently in the context of SMT architectures: threads should be paral-lelized with a cyclic, rather than a blocked algorithm; non-loop programs should not be software speculated, and compilers no longer need to be concerned about precisely sizing tiles to match cache sizes. By following these new guidelines, compilers can generate code that improves the performance of programs executing on SMT machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> In ACM SIG-PLAN 88 Conf. on Programming Language Design and Implementation, </booktitle> <address> p. 308317, </address> <month> June </month> <year> 1988. </year>
Reference: [2] <editor> J. Allen, et al. </editor> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conf. Record of the Tenth Ann. ACM Symp. on Principles of Programming Languages, </booktitle> <address> p. 177189, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: Predicated execution [23][16][28] is an architectural model in which instruction execution can be guarded by boolean predicates that determine whether an instruction should be executed or nullified. Compilers can then use if-conversion <ref> [2] </ref> to transform control dependences into data dependences, thereby exposing more ILP. Like software speculative execution, aggressive predication can incur additional instruction overhead by executing instructions that are either nullified or produce results that are never used.
Reference: [3] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Fifth ACM SIG-PLAN Symp. on Principles & Practice of Parallel Programming, </booktitle> <address> p. 166178, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Loop iteration scheduling for shared-memory multiprocessors has been evaluated by Wolf and Lam [34], Carr, McKinley, and Tseng [7], Anderson, Amarasinghe, and Lam <ref> [3] </ref>, and Cierniak and Li [10], among others. These studies focus on scheduling to minimize communication and synchronization overhead; all restructured loops and data layout to improve access locality for each processor.
Reference: [4] <author> J. Boyle, et al. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: The object files generated by Multiflow were linked with our versions of the ANL <ref> [4] </ref> and SUIF runtime libraries to create executables. Our SMT simulator processes these unmodified Alpha executables and uses emulation-based, instruction-level simulation to model in detail the processor pipelines, hardware support for out of-order execution, and the entire memory hierarchy, including TLB usage.
Reference: [5] <author> E. Bugnion, et al. </author> <title> Compiler-directed page coloring for multiprocessors. </title> <booktitle> In Seventh Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> p. 244255, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Implicitly-parallel applications (the SPEC suites) were first parallelized by the SUIF compiler [15]; SUIFs C output was then fed to Multiflow. A blocked loop distribution policy commonly used for multiprocessor execution has been implemented in SUIF; because we used applications compiled with the latest version of SUIF <ref> [5] </ref>, but did not have access to its source, we implemented an alternative algorithm (described in Section 5) by hand. SUIF also finds tileable loops, determines appropriate multiprocessor-oriented tile sizes for particular data sets and caches, and then generates tiled code; we experimented with other tile sizes with manual coding.
Reference: [6] <author> S. Carr and K. Kennedy. </author> <title> Compiler blockability of numerical algorithms. In Supercomputing 92, </title> <editor> p. </editor> <volume> 114124, </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: transformation for improved locality has proposed various frameworks and algorithms for selecting and applying a range of loop transformations [14][33]<ref> [6] </ref>[17][34][7]. These studies illustrate the effectiveness of tiling and also propose other loop transformations for enabling better tiling. Lam, Rothberg, and Wolf [20], Coleman and McKinley [11], and Carr et al., [6] show that application performance is sensitive to the tile size, and present techniques for selecting tile sizes based on problem-size and cache parameters, rather than targeting a fixed-size or fixed-cache occupancy. 10 Conclusions This paper has examined compiler optimizations in the context of a simultaneous multithreading architecture.
Reference: [7] <author> S. Carr, K. S. McKinley, and C. W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth Intl Conf. on Architectural Support for Programming Languages and Operating Systems, p. </booktitle> <volume> 252 262, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Loop iteration scheduling for shared-memory multiprocessors has been evaluated by Wolf and Lam [34], Carr, McKinley, and Tseng <ref> [7] </ref>, Anderson, Amarasinghe, and Lam [3], and Cierniak and Li [10], among others. These studies focus on scheduling to minimize communication and synchronization overhead; all restructured loops and data layout to improve access locality for each processor.
Reference: [8] <author> L. Carter, J. Ferrante, and S. F. Hummel. </author> <title> Hierarchical tiling for improved superscalar performance. </title> <booktitle> In Proceedings of the Ninth Intl Parallel Processing Symp., </booktitle> <address> p. 239245, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Only at the very smallest tile size did an increase in tiling overhead overwhelm SMTs ability to hide memory latency. Cyclic tiling is still appropriate for a multiprocessor of SMTs. A hierarchical <ref> [8] </ref> or hybrid tiling approach might be most effective.
Reference: [9] <author> A. E. Charlesworth. </author> <title> An approach to scientific array processing: </title> <booktitle> The architectural design of the AP-120B/FPS-164 family. IEEE Computer, </booktitle> <address> 14(9):1827, </address> <month> December </month> <year> 1981. </year>
Reference: [10] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In ACM SIGPLAN 95 Conf. on Programming Language Design and Implementation, </booktitle> <address> p. 205217, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Loop iteration scheduling for shared-memory multiprocessors has been evaluated by Wolf and Lam [34], Carr, McKinley, and Tseng [7], Anderson, Amarasinghe, and Lam [3], and Cierniak and Li <ref> [10] </ref>, among others. These studies focus on scheduling to minimize communication and synchronization overhead; all restructured loops and data layout to improve access locality for each processor. In particular, Anderson et al., discuss the blocked and cyclic mapping schemes, and present a heuristic for choosing between them.
Reference: [11] <author> S. Coleman and K. S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In ACM SIGPLAN 95 Conf. on Programming Language Design and Implementation, </booktitle> <address> p. 279290, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Previous work in code transformation for improved locality has proposed various frameworks and algorithms for selecting and applying a range of loop transformations [14][33][6][17][34][7]. These studies illustrate the effectiveness of tiling and also propose other loop transformations for enabling better tiling. Lam, Rothberg, and Wolf [20], Coleman and McKinley <ref> [11] </ref>, and Carr et al., [6] show that application performance is sensitive to the tile size, and present techniques for selecting tile sizes based on problem-size and cache parameters, rather than targeting a fixed-size or fixed-cache occupancy. 10 Conclusions This paper has examined compiler optimizations in the context of a simultaneous
Reference: [12] <author> K. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In COMPCON 92 digest of papers, </booktitle> <address> p. 305310, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: SMT, with its simultaneous multithreading capabilities, naturally tolerates high latencies without the additional instruction overhead. 4 Methodology Before examining the compiler optimizations, we describe the methodology used in the experiments. We chose applications from the SPEC 92 <ref> [12] </ref>, SPEC 95 [30] and SPLASH-2 [35] benchmark suites (Table 2). All programs were compiled with the Multiflow trace scheduling compiler [22] to generate DEC Alpha object files. Multiflow was chosen, because it generates high-quality code, using aggressive static scheduling for wide-issue, loop unrolling, and other ILP-exposing optimizations.
Reference: [13] <author> S. J. Eggers, et al. </author> <title> Simultaneous multithreading: A platform for next-generation processors. </title> <booktitle> In IEEE Micro, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: When new processing paradigms change these architectural assumptions, however, we must reevaluate machine-dependent compiler optimizations in order to maximize performance on the new machines. Simultaneous multithreading (SMT) [32][31][21] <ref> [13] </ref> is a multithreaded processor design that alters several architectural assumptions on which compilers have traditionally relied. On an SMT processor, instructions from multiple threads can issue to the functional units each cycle. <p> This single feature is responsible for performance gains of almost 2X over wide-issue superscalars and roughly 60% over single-chip, shared memory multiprocessors on both multi-programmed (SPEC92, SPECint95) and parallel (SPLASH-2, SPECfp95) workloads; SMT achieves this improvement while limiting the slowdown of a single executing thread to under 2% <ref> [13] </ref>. Simultaneous multithreading presents to the compiler a different model for hiding operation latencies and sharing code and data. Operation latencies are hidden by instructions from all executing threads, not just by those in the thread with the long-latency operation. <p> Because it can issue instructions from multiple threads, an SMT processor has fewer empty issue slots; in fact, sustained instruction throughput can be rather high, roughly 2 times greater than on a conventional superscalar <ref> [13] </ref>. Furthermore, SMT does a better job of hiding latencies than single-threaded processors, because it uses instructions from one thread to mask delays in another. In such an environment, the aforementioned optimizations may be less useful, or even detrimental, because the overhead instructions compete with useful instructions for hardware resources. <p> Even though it has few oating point computations, water-spatial had a high IPC without speculation (6.5). Therefore the speculative instructions bottlenecked the integer units, and execution without speculation was more profitable. (approximating execution on a superscalar <ref> [13] </ref>). The results indicate that tiling is profitable on an SMT, just as it is on conventional processors. Mxm may seem to be an exception, since tiling brings no improvement, but it is an exception that shows there is no harm in applying the optimization.
Reference: [14] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 5(5):587616, </volume> <month> October </month> <year> 1988. </year>
Reference: [15] <author> M. W. Hall, et al. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12):8489, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: All programs were compiled with the Multiflow trace scheduling compiler [22] to generate DEC Alpha object files. Multiflow was chosen, because it generates high-quality code, using aggressive static scheduling for wide-issue, loop unrolling, and other ILP-exposing optimizations. Implicitly-parallel applications (the SPEC suites) were first parallelized by the SUIF compiler <ref> [15] </ref>; SUIFs C output was then fed to Multiflow.
Reference: [16] <author> P. Hsu and E. Davidson. </author> <title> Highly concurrent scalar processing. </title> <booktitle> In 13th Ann. Intl Symp. on Computer Architecture, </booktitle> <address> p. 386395, </address> <month> June </month> <year> 1986. </year>
Reference: [17] <author> K. Kennedy and K. S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Languages and Compilers for Parallel Computing, 6th Intl Workshop, </booktitle> <address> p. 301319. </address> <month> August </month> <year> 1993. </year>
Reference: [18] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In ACM SIGPLAN 88 Conf. on Programming Language Design and Implementation, </booktitle> <address> p. 318328, </address> <month> June </month> <year> 1988. </year>
Reference: [19] <author> M. Lam and R. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In 19th Ann. Intl Symp. on Computer Architecture, </booktitle> <address> p. 4657, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Global scheduling optimizations, like trace scheduling [22], superblocks [25] and hyperblocks [23], allow code motion (including speculative motion) across basic blocks, thereby exposing more ILP for statically-scheduled VLIWs and wide-issue superscalars. In their study on ILP limits, Lam and Wilson <ref> [19] </ref> found that speculation provides greater speedups on loop-based numeric applications than on non-numeric codes, but their study did not include the effects of wrong-path instructions. Previous work in code transformation for improved locality has proposed various frameworks and algorithms for selecting and applying a range of loop transformations [14][33][6][17][34][7].
Reference: [20] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Fourth Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> p. 6374, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Previous work in code transformation for improved locality has proposed various frameworks and algorithms for selecting and applying a range of loop transformations [14][33][6][17][34][7]. These studies illustrate the effectiveness of tiling and also propose other loop transformations for enabling better tiling. Lam, Rothberg, and Wolf <ref> [20] </ref>, Coleman and McKinley [11], and Carr et al., [6] show that application performance is sensitive to the tile size, and present techniques for selecting tile sizes based on problem-size and cache parameters, rather than targeting a fixed-size or fixed-cache occupancy. 10 Conclusions This paper has examined compiler optimizations in the
Reference: [21] <author> J. L. Lo, et al. </author> <title> Converting thread-level parallelism to instruction-level parallelism via simultaneous multithreading. </title> <journal> ACM Trans. on Computer and Systems, </journal> <volume> 15(3), </volume> <month> August </month> <year> 1997. </year>
Reference-contexts: All large hardware data structures (caches, TLBs, and branch prediction tables) are shared among all threads. The additional cross-thread conflicts in the caches and branch prediction hardware are absorbed by SMTs enhanced latency-hiding capabilities <ref> [21] </ref>, while TLB interference can be addressed with a technique described in Section 5. 3 Rethinking compiler optimizations As explained above, simultaneous multithreading relies on a novel feature for attaining greater processor performance: the coupling of multithreading and wide-instruction issue by scheduling instructions from different threads in the same cycle.
Reference: [22] <author> P. G. Lowney, et al. </author> <title> The Multiflow trace scheduling compiler. </title> <editor> J. </editor> <booktitle> of Supercomputing, </booktitle> <address> 7(1/2):51142, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We chose applications from the SPEC 92 [12], SPEC 95 [30] and SPLASH-2 [35] benchmark suites (Table 2). All programs were compiled with the Multiflow trace scheduling compiler <ref> [22] </ref> to generate DEC Alpha object files. Multiflow was chosen, because it generates high-quality code, using aggressive static scheduling for wide-issue, loop unrolling, and other ILP-exposing optimizations. Implicitly-parallel applications (the SPEC suites) were first parallelized by the SUIF compiler [15]; SUIFs C output was then fed to Multiflow. <p> A hybrid parallelization policy might be desirable, though, with a blocked distribution across processors to minimize inter-processor communication. 6 Software speculative execution Todays optimizing compilers rely on aggressive code scheduling to hide instruction latencies. In global scheduling techniques, such as trace scheduling <ref> [22] </ref> or hyperblock scheduling [23], instructions from a predicted branch path may be moved above a conditional branch, so that their execution becomes speculative. If at runtime, the other branch path is taken, then the speculative instructions are useless and potentially waste processor resources. <p> In particular, Anderson et al., discuss the blocked and cyclic mapping schemes, and present a heuristic for choosing between them. Global scheduling optimizations, like trace scheduling <ref> [22] </ref>, superblocks [25] and hyperblocks [23], allow code motion (including speculative motion) across basic blocks, thereby exposing more ILP for statically-scheduled VLIWs and wide-issue superscalars.
Reference: [23] <author> S. A. Mahlke, et al. </author> <title> Effective compiler support for predicated execution using the hyperblock. </title> <booktitle> In 25th Intl Symp. on Microarchitecture, </booktitle> <address> p. 4554, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: A hybrid parallelization policy might be desirable, though, with a blocked distribution across processors to minimize inter-processor communication. 6 Software speculative execution Todays optimizing compilers rely on aggressive code scheduling to hide instruction latencies. In global scheduling techniques, such as trace scheduling [22] or hyperblock scheduling <ref> [23] </ref>, instructions from a predicted branch path may be moved above a conditional branch, so that their execution becomes speculative. If at runtime, the other branch path is taken, then the speculative instructions are useless and potentially waste processor resources. <p> In particular, Anderson et al., discuss the blocked and cyclic mapping schemes, and present a heuristic for choosing between them. Global scheduling optimizations, like trace scheduling [22], superblocks [25] and hyperblocks <ref> [23] </ref>, allow code motion (including speculative motion) across basic blocks, thereby exposing more ILP for statically-scheduled VLIWs and wide-issue superscalars.
Reference: [24] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> DEC-Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: For branch prediction, we use a McFarling-style hybrid predictor with a 256-entry, 4-way set-associative branch target buffer, and an 8K entry selector that chooses between a global history predictor (13 history bits) and a local predictor (a 2K-entry local history table that indexes into a 4K-entry, 2-bit local prediction table) <ref> [24] </ref>. Because of the length of the simulations, we limited our detailed simulation results to the parallel computation portion of the applications (the norm for simulating parallel applications).
Reference: [25] <author> W.W. Hwu, et al. </author> <title> The superblock: An effective technique for VLIW and superscalar compilation. </title> <editor> J. </editor> <booktitle> of Supercomputing, </booktitle> <address> 7(1/2):229248, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In particular, Anderson et al., discuss the blocked and cyclic mapping schemes, and present a heuristic for choosing between them. Global scheduling optimizations, like trace scheduling [22], superblocks <ref> [25] </ref> and hyperblocks [23], allow code motion (including speculative motion) across basic blocks, thereby exposing more ILP for statically-scheduled VLIWs and wide-issue superscalars.
Reference: [26] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> p. 6275, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: On a conventional processor, compiler-directed prefetching <ref> [26] </ref> can be useful for tolerating memory latencies, as long as prefetch overhead (due to prefetch instructions, additional memory bandwidth, and/or cache interference) is minimal. On an SMT, this overhead is more detrimental: it interferes not only with the thread doing the prefetching, but also competes with other threads.
Reference: [27] <author> B. R. Rau and C. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In 14th Ann. Workshop on Microprogramming, p. </booktitle> <volume> 183 197, </volume> <month> October </month> <year> 1981. </year>
Reference: [28] <author> B. R. Rau, et al. </author> <title> The Cydra 5 departmental supercomputer. </title> <journal> IEEE Computer, </journal> <volume> 22:1235, </volume> <month> January </month> <year> 1989. </year>
Reference: [29] <author> J. P. Singh, J. L. Hennessy, and A. Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <journal> IEEE Computer, </journal> <volume> 27(7):4250, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: The second set is more typical of todays memory subsystems and is used to emulate larger data set sizes <ref> [29] </ref>; it is used in the tiling studies only. Table 2. We model the cache behavior, as well as bank and bus contention.
Reference: [30] <author> SPEC. </author> <type> SPEC CPU 95 Technical Manual. </type> <month> August </month> <year> 1995. </year>
Reference-contexts: SMT, with its simultaneous multithreading capabilities, naturally tolerates high latencies without the additional instruction overhead. 4 Methodology Before examining the compiler optimizations, we describe the methodology used in the experiments. We chose applications from the SPEC 92 [12], SPEC 95 <ref> [30] </ref> and SPLASH-2 [35] benchmark suites (Table 2). All programs were compiled with the Multiflow trace scheduling compiler [22] to generate DEC Alpha object files. Multiflow was chosen, because it generates high-quality code, using aggressive static scheduling for wide-issue, loop unrolling, and other ILP-exposing optimizations.
Reference: [31] <author> D. M. Tullsen, et al. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23rd Ann. Intl Symp. on Computer Architecture, </booktitle> <address> p. 191202, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Instruction scheduling is done as in a conventional, out-of-order superscalar: instructions are issued after their operands have been calculated or loaded from memory, without regard to thread; the renaming hardware eliminates inter-thread register name conflicts by mapping thread-specific architectural registers onto the processors physical registers (see <ref> [31] </ref> for more details). All large hardware data structures (caches, TLBs, and branch prediction tables) are shared among all threads.
Reference: [32] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multi-threading: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Ann. Intl Symp. on Computer Architecture, </booktitle> <address> p. 392403, </address> <month> June </month> <year> 1995. </year>
Reference: [33] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN 91 Conf. on Programming Language Design and Implementation, </booktitle> <address> p. 3044, </address> <month> June </month> <year> 1991. </year>
Reference: [34] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4):452471, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Loop iteration scheduling for shared-memory multiprocessors has been evaluated by Wolf and Lam <ref> [34] </ref>, Carr, McKinley, and Tseng [7], Anderson, Amarasinghe, and Lam [3], and Cierniak and Li [10], among others. These studies focus on scheduling to minimize communication and synchronization overhead; all restructured loops and data layout to improve access locality for each processor.
Reference: [35] <author> S. C. Woo, et al. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In 22nd Ann. Intl Symp. on Computer Architecture, </booktitle> <address> p. 2436, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: SMT, with its simultaneous multithreading capabilities, naturally tolerates high latencies without the additional instruction overhead. 4 Methodology Before examining the compiler optimizations, we describe the methodology used in the experiments. We chose applications from the SPEC 92 [12], SPEC 95 [30] and SPLASH-2 <ref> [35] </ref> benchmark suites (Table 2). All programs were compiled with the Multiflow trace scheduling compiler [22] to generate DEC Alpha object files. Multiflow was chosen, because it generates high-quality code, using aggressive static scheduling for wide-issue, loop unrolling, and other ILP-exposing optimizations.
References-found: 35

