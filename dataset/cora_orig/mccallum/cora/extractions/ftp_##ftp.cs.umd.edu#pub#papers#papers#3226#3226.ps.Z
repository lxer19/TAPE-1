URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3226/3226.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Using Recurrent Neural Networks to Learn the Structure of Interconnection Networks  
Author: Mark W. Goudreau a and C. Lee Giles b;c 
Date: 15 February 1994  
Address: Orlando, FL 32816  4 Independence Way, Princeton, NJ 08540  College Park, MD 20742  
Affiliation: UNIVERSITY OF MARYLAND  a Department of Computer Science University of Central Florida,  b NEC Research Institute  c Institute for Advanced Computer Studies University of Maryland,  
Pubnum: TECHNICAL REPORT UMIACS-TR-94-20 AND CS-TR-3226  
Abstract: A modified Recurrent Neural Network (RNN) is used to learn a Self-Routing Interconnection Network (SRIN) from a set of routing examples. The RNN is modified so that it has several distinct initial states. This is equivalent to a single RNN learning multiple different synchronous sequential machines. We define such a sequential machine structure as augmented and show that a SRIN is essentially an Augmented Synchronous Sequential Machine (ASSM). As an example, we learn a small six-switch SRIN. After training we extract the net-work's internal representation of the ASSM and corresponding SRIN. fl This paper is adapted from ( Goudreau, 1993, Chapter 6 ) . A shortened version of this paper was published in ( Goudreau & Giles, 1993 ) . 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1978). </year> <title> On the complexity of minimum inference of regular sets. </title> <journal> Infor mation and Control, </journal> <volume> 39, </volume> <pages> 337-350. </pages>
Reference: <author> Brown, T. X. </author> <year> (1989). </year> <title> Neural networks for switching. </title> <journal> IEEE Communications Mag azine, </journal> <volume> 27 (11), </volume> <pages> 72-81. </pages>
Reference: <author> Brown, T. X. & Liu, K.-H. </author> <year> (1990). </year> <title> Neural network design of a Banyan network controller. </title> <journal> IEEE Journal on Selected Areas of Communication, </journal> <volume> 8 (8), </volume> <pages> 1428-1438. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., & McClelland, J. L. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 372-381. </pages>
Reference: <author> Das, S. & Mozer, M. </author> <year> (1994). </year> <title> A hybrid clustering/gradient descent architecture for finite state machine induction. </title> <editor> In Cowan, J., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Funabiki, N., Takefuji, Y., & Lee, K. C. </author> <year> (1991). </year> <title> A neural network model for traffic controls in multistage interconnection networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <pages> (pp. </pages> <note> A898). 22 Funabiki, </note> <author> N., Takefuji, Y., & Lee, K. C. </author> <year> (1993). </year> <title> Comparisons of seven neural net work models on traffic control problems in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42 (4), </volume> <pages> 497-501. </pages>
Reference: <author> Giles, C. L. & Horne, B. G. </author> <year> (1994). </year> <title> Some large DFA's are learnable with recurrent neural networks. </title> <institution> NEC Research Institute, Inc., Princeton, NJ. </institution>
Reference-contexts: It should be pointed out that the limited success of this approaches is due to the learning algorithms. Generally, the RNNs have rich representational capabilities. However, recent work has shown that certain types of large SSMs, with thousands of states, are learnable <ref> ( Giles & Horne, 1994 ) </ref> . Furthermore, the performance of the RNNs can sometimes be improved by using "hints" if partial information about the structure of the SSM is known ( Giles & Omlin, 1993 ) . <p> A sample problem was used to illustrate the methodology. This work clearly pointed out the need for further research into the use of RNNs to inference larger SSMs. To date, RNNs have had limited success for large problems in grammatical inference, but some recent results are promising <ref> ( Giles & Horne, 1994 ) </ref> . The concept of learning interconnection networks is an unusual one for the interconnection network community.
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Chen, H. H., Sun, G. Z., & Lee, Y. C. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 (3), </volume> <pages> 393-405. </pages>
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z., Chen, H. H., & Lee, Y. C. </author> <year> (1992). </year> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title>
Reference: <editor> In Moody, J., Hanson, S., & Lippmann, R. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> (pp. 317-324)., </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Giles, C. L. & Omlin, C. W. </author> <year> (1993). </year> <title> Extraction, insertion and refinement of symbolic rules in dynamically-driven recurrent neural networks. </title> <journal> Connection Science, </journal> <volume> 5 (3,4), </volume> <pages> 307-337. </pages> <booktitle> Special Issue on Architectures for Integrating Symbolic and Neural Processes. </booktitle>
Reference-contexts: However, recent work has shown that certain types of large SSMs, with thousands of states, are learnable ( Giles & Horne, 1994 ) . Furthermore, the performance of the RNNs can sometimes be improved by using "hints" if partial information about the structure of the SSM is known <ref> ( Giles & Omlin, 1993 ) </ref> . Other approaches that use neural networks for grammatical inference exist that will not be used in this paper. <p> One approach is to start with as many switch neurons as reasonably possible; if training is successful, then reduce the number of neurons using a destructive heuristic <ref> ( Omlin & Giles, 1993 ) </ref> . The SLRNN will work in the following manner. The input to the SLRNN will be an initial state vector and a series of input vectors.
Reference: <author> Gold, E. M. </author> <year> (1978). </year> <title> Complexity of automaton identification from given data. </title> <booktitle> In formation and Control, </booktitle> <volume> 37, </volume> <pages> 302-320. </pages>
Reference: <author> Goudreau, M. W. </author> <year> (1993). </year> <title> Neural Network Applications for Interconnection Net works. </title> <type> PhD thesis, </type> <institution> Princeton University, Princeton, NJ. </institution>
Reference: <author> Goudreau, M. W. & Giles, C. L. </author> <year> (1992). </year> <title> Routing in random multistage intercon nection networks: Comparing exhaustive search, greedy and neural network approaches. </title> <journal> International Journal of Neural Systems, </journal> <volume> 3 (2), </volume> <pages> 125-142. </pages>
Reference: <author> Goudreau, M. W. & Giles, C. L. </author> <year> (1993). </year> <title> Discovering the structure of a self-routing interconnection network with a recurrent neural network. </title> <editor> In Alspector, J., Goodman, R., & Brown, T. X. (Eds.), </editor> <booktitle> Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <pages> (pp. 52-59)., </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates, Inc. </publisher>
Reference: <author> Goudreau, M. W., Giles, C. L., Chakradhar, S. T., & Chen, D. </author> <year> (1994). </year> <title> First-order vs. second-order single layer recurrent neural networks. </title> <note> To be published in IEEE Transactions on Neural Networks. </note>
Reference-contexts: Additionally, the initial state, which depends on the source processor, will be a one-hot code. This structure is chosen because it is known that a solution will exist to map the SLRNN to the desired ASSM <ref> ( Goudreau et al., 1994 ) </ref> . The solution that is known to exist requires the use of one-hot codes for the states and the inputs. The representation that the SLRNN actually learns, however, can have states that are not in a one-hot code.
Reference: <author> Hakim, N. Z. & Meadows, H. E. </author> <year> (1990). </year> <title> A neural network approach to the setup of the Benes switch. </title> <booktitle> In Infocom 90, </booktitle> <pages> (pp. 397-402). </pages>
Reference: <author> Hillis, W. D. </author> <year> (1990). </year> <title> Co-evolving parasites improve simulated evolution as an opti mization procedure. </title> <journal> Physics D, </journal> <volume> 42, </volume> <pages> 228-234. </pages> <note> 23 Hopcroft, </note> <author> J. E. & Ullman, J. D. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Lan guages, and Computation. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference-contexts: The training data is a table of source processors, message headers, and destination processors. Once the training data has been learned, the structure of the SRIN can be extracted from the SLRNN. One topic that is related to the learning of INs was presented by Hillis <ref> ( Hillis, 1990 ) </ref> . In that paper, Hillis makes use of simulated evolution to construct sorting networks.
Reference: <author> Kearns, M. & Valiant, L. </author> <year> (1989). </year> <title> Cryptographic limitations on learning boolean for mulae and finite automata. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing. </booktitle> <publisher> ACM Press. </publisher>
Reference: <author> Kohavi, Z. </author> <year> (1978). </year> <title> Switching and Finite Automata Theory (second ed.). </title> <address> New York, NY: </address> <publisher> McGraw-Hill, Inc. </publisher>
Reference-contexts: FSAs and SSMs are thoroughly described in ( Hopcroft & Ullman, 1979; Kohavi, 1978 ) . We will use the definition of SSMs that is provided in <ref> ( Kohavi, 1978 ) </ref> . 1 Definition 2 A synchronous sequential machine is a quintuple, (O; S; I; ffi; ), where: * O is a finite, nonempty set of outputs symbols. * S is a finite, nonempty set of states. * I is a finite, nonempty set of inputs symbols. * <p> For example, just as one can minimize the size of an ASSM by merging equivalent states <ref> ( Kohavi, 1978 ) </ref> , one can minimize the size of a SRIN by merging equivalent switches. What we are interested in is the inference of a SRIN from examples. A great deal of work has been done on the problem of machine inference.
Reference: <author> Lang, K. </author> <year> (1992). </year> <title> Random DFA's can be approximately learned from sparse uni form examples. </title> <booktitle> In Proceedings of the Fifth ACM Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 45-52). </pages> <publisher> ACM Press. </publisher>
Reference-contexts: The algorithm produces a machine that is consistent with a sparsely labeled tree, but the machine that is produced is not necessarily the minimum machine that is consistent with the data. Lang <ref> ( Lang, 1992 ) </ref> performed several experiments using this algorithm for random finite automata with 1000 states and 2000 transitions.
Reference: <author> Lee, S.-L. & Chang, S. </author> <year> (1993). </year> <title> Neural networks for routing of communication net works with unreliable components. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4 (5), </volume> <pages> 854-863. </pages>
Reference: <author> Marrakchi, A. M. & Troudet, T. </author> <year> (1989). </year> <title> A neural net arbitrator for large crossbar packet-switches. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 36 (7), </volume> <pages> 1039-1041. </pages>
Reference: <author> Melsa, P. J. W., Kenney, J. B., & Rohrs, C. E. </author> <year> (1990a). </year> <title> A neural network solution for call routing with preferential call placement. </title> <booktitle> In Proceedings of the 1990 Global Telecommunications Conference, </booktitle> <pages> (pp. 1377-1382). </pages>
Reference: <author> Melsa, P. J. W., Kenney, J. B., & Rohrs, C. E. </author> <year> (1990b). </year> <title> A neural network solution for routing in three stage interconnection networks. </title> <booktitle> In Proceedings of the 1990 International Symposium on Circuits and Systems, </booktitle> <pages> (pp. 483-486). </pages>
Reference: <author> Mozer, M. C. & Bachrach, J. </author> <year> (1990). </year> <title> Discovering the structure of a reactive envi ronment by exploration. </title> <journal> Neural Computation, </journal> <volume> 2 (4), </volume> <pages> 447-457. </pages>
Reference: <author> Mozer, M. C. & Bachrach, J. </author> <year> (1991). </year> <title> SLUG: A connectionist architecture for in ferring the structure of finite-state environments. </title> <journal> Machine Learning, </journal> <volume> 7 (2/3), </volume> <pages> 139-160. </pages>
Reference: <author> Omlin, C. W. & Giles, C. L. </author> <year> (1993). </year> <title> Pruning recurrent neural networks for im proved generalization performance. </title> <type> Technical Report TR 93-6, </type> <institution> Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY. </institution>
Reference-contexts: However, recent work has shown that certain types of large SSMs, with thousands of states, are learnable ( Giles & Horne, 1994 ) . Furthermore, the performance of the RNNs can sometimes be improved by using "hints" if partial information about the structure of the SSM is known <ref> ( Giles & Omlin, 1993 ) </ref> . Other approaches that use neural networks for grammatical inference exist that will not be used in this paper. <p> One approach is to start with as many switch neurons as reasonably possible; if training is successful, then reduce the number of neurons using a destructive heuristic <ref> ( Omlin & Giles, 1993 ) </ref> . The SLRNN will work in the following manner. The input to the SLRNN will be an initial state vector and a series of input vectors.
Reference: <author> Pitt, L. & Warmuth, M. </author> <year> (1989). </year> <title> The minimum DFA consistency problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing. </booktitle> <publisher> ACM Press. 24 Pollack, </publisher> <editor> J. B. </editor> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 (2/3), </volume> <pages> 227-252. </pages>
Reference: <author> Rivest, R. L. & Schapire, R. E. </author> <year> (1987a). </year> <title> Diversity-based inference of finite au tomata. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> (pp. 78-87). </pages>
Reference: <author> Rivest, R. L. & Schapire, R. E. </author> <year> (1987b). </year> <title> A new approach to unsupervised learning in deterministic environments. In Langley, </title> <editor> P. (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning. </booktitle>
Reference: <author> Schapire, R. E. </author> <year> (1988). </year> <title> Diversity-based inference of finite automata. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Siegel, H. J. </author> <year> (1990). </year> <title> Interconnection Networks for Large Scale Parallel Processing. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Numerous different types of INs have been proposed. A detailed description of many of the INs that have been applied to parallel computing can be found in Siegel's book <ref> ( Siegel, 1990 ) </ref> . In this paper, the learning of Self-Routing Interconnection Networks (SRINs) is discussed. SRINs are described in detail in Section 2. They can be used to describe many commonly used INs.
Reference: <author> Takefuji, Y. & Lee, K. C. </author> <year> (1991). </year> <title> An artificial hysteresis binary neuron: A model suppressing the oscillatory behavior of neural dynamics. </title> <journal> Biological Cybernetics, </journal> <volume> 64, </volume> <pages> 353-356. </pages>
Reference: <author> Thomopoulos, S. C. A., Zhang, L., & Wann, C. D. </author> <year> (1991). </year> <title> Neural network imple mentation of the shortest path algorithm for traffic routing in communication networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <pages> (pp. 2693-2702)., </pages> <address> Singapore. </address>
Reference: <author> Trakhtenbrot, B. & Barzdin, Y. </author> <year> (1973). </year> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> Amsterdam: North-Holland Publishing Company. </publisher>
Reference-contexts: In fact, a polynomial time algorithm proposed by Trakhtenbrot and Barzdin <ref> ( Trakhtenbrot & Barzdin, 1973 ) </ref> has been shown to be able to infer some very large finite automata. The algorithm produces a machine that is consistent with a sparsely labeled tree, but the machine that is produced is not necessarily the minimum machine that is consistent with the data.
Reference: <author> Troudet, T. P. & Walters, S. M. </author> <year> (1991). </year> <title> Neural network architecture for crossbar switch control. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 38 (1), </volume> <pages> 42-56. </pages>
Reference: <author> Watrous, R. L. & Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4 (3), </volume> <pages> 406-414. </pages>
Reference: <author> Williams, R. J. & Zipser, D. </author> <year> (1989). </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 (2), </volume> <pages> 270-280. </pages>
Reference-contexts: The SLRNN will learn the training data, and the ASSM will be extracted from the SLRNN. A gradient descent algorithm that is similar to the one proposed by Williams and Zipser <ref> ( Williams & Zipser, 1989 ) </ref> will be used to train the SLRNN. If it were not possible to extract the ASSM from the SLRNN, then the SLRNN would not be useful for this problem.
Reference: <author> Zeng, Z., Goodman, R. M., & Smyth, P. </author> <year> (1993). </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5 (6), </volume> <pages> 976-990. 25 </pages>
References-found: 40

