URL: http://www.cs.umn.edu/Research/Agassiz/Paper/konas.jmcs94.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: konas@csrd.uiuc.edu  
Phone: phone: 1-217-244-0046  
Title: Parallel Simulations of Multiprocessors  
Author: Pavlos Konas and Pen-Chung Yew 
Address: 447 CSRL, 1308 West Main Street, Urbana, IL 61801.  
Affiliation: Center for Supercomputing Research and Development,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Agrawal. </author> <title> Circuit Partitioning for Hardware Simulation Engines. </title> <booktitle> In Proceedings of the 1985 International Conference on Computer Aided Design, </booktitle> <pages> pages 161-163, </pages> <year> 1985. </year>
Reference-contexts: First, the system is divided into sets of strings, defined as sets of connected components with as most one fanin and one fanout included. Second, a load leveling 18 19 20 scheme is used to insure that block sizes are kept fairly close <ref> [1, 22, 34] </ref>. SPaDES performs well when there is sufficient amount of parallelism inherent in the simulation. It achieves a (relative) speedup of more than 11 on 16 processors (e.g. random and nonconflicting accesses simulations).
Reference: [2] <author> R. Ayani. </author> <title> Parallel Simulation on Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Dept. of Telecommunication and Computer Systems, The Royal Institute of Technology, Sweden, </institution> <year> 1989. </year>
Reference-contexts: Ayani proposed a similar algorithm, called the three phase algorithm <ref> [2, 3] </ref>. In this algorithm, however, each object maintains its own local event queue and is responsible for deciding whether or not to evaluate an event from its input.
Reference: [3] <author> R. Ayani. </author> <title> A Parallel Simulation Scheme Based on Distances Between Objects. </title> <booktitle> In Proceedings of the SCS Western Multiconference on Distributed Simulation, </booktitle> <pages> pages 113-118, </pages> <month> March, </month> <year> 1989. </year>
Reference-contexts: Ayani proposed a similar algorithm, called the three phase algorithm <ref> [2, 3] </ref>. In this algorithm, however, each object maintains its own local event queue and is responsible for deciding whether or not to evaluate an event from its input.
Reference: [4] <author> R. Ayani and H. Rajaei. </author> <title> Parallel Simulation of a Generalized Cube Multistage Interconnection Network. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 60-63, </pages> <month> January, </month> <year> 1990. </year>
Reference-contexts: The speedups achieved by this algorithm on a 16-processor Sequent Symmetry range between 8 (for a 5-stage network), and 14 (for a 9-stage network). Ayani and Rajaei <ref> [4] </ref> use the three-phase algorithm to simulate a class of MINs called Generalized Cube Networks (GCMINs) on a 16-processor Sequent Symmetry. The speedup they obtain increases as the size of the MIN increases and becomes almost linear for MINs with more than 256 inputs.
Reference: [5] <author> B. Berkman and R. Ayani. </author> <title> Parallel Simulation of Multistage Interconnection Networks on a SIMD Computer. </title> <booktitle> In Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <pages> pages 133-140, </pages> <month> January, </month> <year> 1991. </year>
Reference-contexts: The relative speedups they present are lower than the absolute ones and range between 1 and 9 for the largest simulated MIN (512-input). In a follow-up paper, Ayani and Berkman <ref> [5] </ref> simulate GCMINs using the three-phase algorithm on a CM-2 with 8,192 processors. Even though they achieve speedups of more than 2,000 for networks with 14 stages, the efficiency of their implementation is considerably low.
Reference: [6] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. PROTEUS: </author> <title> A high-performance parallel-architecture simulator. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS and PERFORMANCE '92 Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Other characteristics of such systems include the large number of components of possibly variable load, and the large state size of those components. We study the performance of the simulation method on both a centralized shared-memory multiprocessor (an Alliant FX/80) and on a simulated NUMA (Nonuniform-Memory-Access) multiprocessor <ref> [6] </ref>. The centralized memory of the Alliant reduces the cost of synchronizing the processors, simplifies the load balancing during each time step, and decreases the cost of communication between the LPs. <p> The nonuniformity of the memory access delays makes an efficient implementation of a parallel synchronous method much harder. In order to examine the behavior and performance of our method on a NUMA machine, we implemented SPaDES on a simulated NUMA multiprocessor. We used Proteus <ref> [6] </ref> to simulate a multiprocessor consisting of processing modules (each module containing processor, cache, local memory) interconnected by a multistage interconnection network. <p> The accuracy of Proteus has been validated with several applications and multiprocessors <ref> [6] </ref> and, therefore, our experimental results are expected to closely match the executions of the simulations on real NUMA multiprocessors. We performed the same set of experiments as on the Alliant FX/80, and the results we obtained are shown in Figures 5 and 6.
Reference: [7] <author> J.V. Briner, Jr. </author> <title> Parallel Mixed-Level Simulation of Digital Circuits Using Virtual Time. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: As parallelism studies show <ref> [7, 36] </ref>, the most striking characteristic of synchronous simulations is the clock effect: steps with high activity, followed by long relatively inactive periods. If a PDES method is to be efficient on such simulations it should handle well these long periods with limited parallelism.
Reference: [8] <author> J. Bruner, C.J. Beckmann, P. Konas, D.K. Poulsen, and P.-C. Yew. </author> <title> An Environment for Parallel Execution-Driven and Event-Driven Architecture Simulation. </title> <booktitle> International Journal in Computer Simulation, </booktitle> <year> 1994. </year> <month> 23 </month>
Reference-contexts: Then, during the second phase, we simulate in parallel all the components whose inputs have changed by an event evaluation. Barrier synchronizations between the two phases guarantee the correctness of the simulation. An efficient implementation of this parallel centralized-time algorithm is Parsim, a simulation method developed at CSRD <ref> [8, 9] </ref>. Because of its efficiency, we use Parsim as the basis of our study of the proposed parallel simulation method. 2.2 Parallel Architectural Simulations As computer systems become increasingly more complex, studying their architectures and exploring design alternatives through simulation becomes increasingly more computation demanding.
Reference: [9] <author> J. Bruner, H. Cheong, A. Veidenbaum, and P.-C. Yew. </author> <title> Chief: A Parallel Simulation Environment for Parallel Systems. </title> <booktitle> In Proceedings of the 5th International Parallel Processing Symposium, </booktitle> <pages> pages 568-575, </pages> <year> 1991. </year>
Reference-contexts: However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm <ref> [9, 35, 36] </ref>. Under this algorithm, simulation evolves in steps which are divided into two phases. During the first phase, we concurrently evaluate all events occurring at the earliest simulation time. <p> Then, during the second phase, we simulate in parallel all the components whose inputs have changed by an event evaluation. Barrier synchronizations between the two phases guarantee the correctness of the simulation. An efficient implementation of this parallel centralized-time algorithm is Parsim, a simulation method developed at CSRD <ref> [8, 9] </ref>. Because of its efficiency, we use Parsim as the basis of our study of the proposed parallel simulation method. 2.2 Parallel Architectural Simulations As computer systems become increasingly more complex, studying their architectures and exploring design alternatives through simulation becomes increasingly more computation demanding.
Reference: [10] <author> K.M. Chandy and J. Misra. </author> <title> Distributed Simulation: A Case Study in Design and Verification of Distributed Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(5):440-452, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Even though they achieve speedups of more than 2,000 for networks with 14 stages, the efficiency of their implementation is considerably low. Konas and Yew [19] compare the performance of a parallel centralized-time event driven method to the Chandy-Misra <ref> [10] </ref> and Time Warp [17] methods in simulating a multiprocessor. The results they present show that the synchronous method is considerably faster than both the Chandy-Misra and the Time Warp approaches.
Reference: [11] <author> K.M. Chandy and R. Sherman. </author> <title> Space-Time and Simulation. </title> <booktitle> In Proceedings of the SCS Multicon-ference on Distributed Simulation, </booktitle> <pages> pages 53-57, </pages> <year> 1989. </year>
Reference-contexts: After the processing of the events has completed, ceiling becomes floor for the next window and the iteration is repeated. Other interesting synchronous methods proposed in the literature are Space-Time <ref> [11] </ref>, the use of conditional knowledge [12], both of which were proposed by Chandy and Sherman, and Breathing Time Buckets a hybrid method proposed by Steinman [37, 38].
Reference: [12] <author> K.M. Chandy and R. Sherman. </author> <title> The Conditional Event Approach to Distributed Simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 93-99, </pages> <year> 1989. </year>
Reference-contexts: After the processing of the events has completed, ceiling becomes floor for the next window and the iteration is repeated. Other interesting synchronous methods proposed in the literature are Space-Time [11], the use of conditional knowledge <ref> [12] </ref>, both of which were proposed by Chandy and Sherman, and Breathing Time Buckets a hybrid method proposed by Steinman [37, 38]. However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm [9, 35, 36].
Reference: [13] <author> R.M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 31-53, </pages> <month> October, </month> <year> 1990. </year>
Reference-contexts: One way to meet the processing requirements of these applications is to execute them on a multiprocessor. Most of the existing parallel methods were developed for simulating asynchronous systems <ref> [13] </ref>. In such systems only a few evaluations occur at any single point in simulated time; a parallel simulation method has to find and expose the parallelism for efficient parallel execution. <p> Finally, Section 6 contains our conclusions. 2 Related Work 2.1 Synchronous Parallel Simulation Methods In synchronous parallel simulation methods, one iteratively determines which events are safe to process and then processes them <ref> [13] </ref>. Barrier synchronizations are used to keep iterations (or components of a single iteration) from interfering with each other. The feature that separates different synchronous ap 3 proaches is the method used to determine which events are safe to process. <p> In order to avoid these potential problems and reduce the overhead introduced by the simulation method, we propose a conservative synchronous parallel simulation method (SPaDES) [20]. Our approach is based on the logical processes model <ref> [13] </ref>. The simulator consists of a set of communicating logical processes (LPs), each of which represents a component or a group of components of the simulated system. Each LP has a single, time-ordered, input queue which acts as its local event list.
Reference: [14] <author> D. Gajski, D.J. Kuck, D.H. Lawrie, and A.H. Sameh. </author> <title> CEDAR ALarge Scale Multiprocessor. </title> <booktitle> In Proceedings of the 1983 International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> August, </month> <year> 1983. </year>
Reference-contexts: This simulation models many parallel shared memory architectures found in practice <ref> [14, 16, 33, 31] </ref>. 11 During the simulation each processor calculates requests for the memories and transmits them to the appropriate memory modules through the forward Omega network.
Reference: [15] <author> P.K. Goli, P. Heidelberger, D.F. Towsley, and Q. Yu. </author> <title> Processor Assignment and Synchronization in Parallel Simulation of Multistage Interconnection Networks. </title> <booktitle> In Proceedings of the SCS Multi-conference on Distributed Simulation, </booktitle> <pages> pages 181-187, </pages> <year> 1990. </year>
Reference-contexts: The multibarrier approach achieves the best performance among the three 5 alternatives and delivers speedups between 4 (for a 4-stage network), and 7 (for a 9-stage network), on an 8-processor Sequent Symmetry. In a follow-up paper <ref> [15] </ref>, they present an improved processor allocation and synchronization algorithm, called the two-phase algorithm, which exploits the network's topological structure. The speedups achieved by this algorithm on a 16-processor Sequent Symmetry range between 8 (for a 5-stage network), and 14 (for a 9-stage network).
Reference: [16] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracom-puter Designing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-32(2):175-190, </volume> <month> February, </month> <year> 1983. </year>
Reference-contexts: This simulation models many parallel shared memory architectures found in practice <ref> [14, 16, 33, 31] </ref>. 11 During the simulation each processor calculates requests for the memories and transmits them to the appropriate memory modules through the forward Omega network.
Reference: [17] <author> D.R. Jefferson. </author> <title> Virtual Time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Even though they achieve speedups of more than 2,000 for networks with 14 stages, the efficiency of their implementation is considerably low. Konas and Yew [19] compare the performance of a parallel centralized-time event driven method to the Chandy-Misra [10] and Time Warp <ref> [17] </ref> methods in simulating a multiprocessor. The results they present show that the synchronous method is considerably faster than both the Chandy-Misra and the Time Warp approaches. The clock advancing overhead introduced by the deadlock avoidance mechanism in the Chandy-Misra method makes this approach inappropriate for architectural simulations.
Reference: [18] <author> P. Konas. </author> <title> Parallel Discrete Event Simulation on Shared Memory Multiprocessors. </title> <type> Technical Report CSRD, </type> <institution> Rpt. No. 1074, Center for Supercomputing Research and Development, </institution> <type> Master Thesis, </type> <institution> Computer Science Department, University of Illinois, </institution> <month> May, </month> <year> 1991. </year>
Reference-contexts: MIP 89-20891 and MIP 93-07910, NASA NCC 2-559, and a donation from Motorola. 1 simulations is to exploit the available parallelism with the least possible overhead. Unfortunately, both conservative and optimistic asynchronous methods introduce significant overhead in their attempt to find parallelism <ref> [18, 19, 36] </ref>. They are not appropriate for synchronous architectural simulations. Therefore, we focus our attention on synchronous parallel simulation methods. In this paper we present a synchronous, parallel, event-driven approach (SPaDES) to the execution of a single simulation program on a parallel machine and study its performance. <p> Time Warp, on the other hand, introduces the state saving overhead and is not expected to perform well in detailed architectural simulations because such simulators tend to have components with large state sizes. However, in an associated report <ref> [18] </ref> Konas observes that the efficiency of the synchronous method is considerably low.
Reference: [19] <author> P. Konas and P.-C. Yew. </author> <title> Parallel Discrete Event Simulation on Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 24th Annual Simulation Symposium, </booktitle> <pages> pages 134-148, </pages> <month> April, </month> <year> 1991. </year>
Reference-contexts: MIP 89-20891 and MIP 93-07910, NASA NCC 2-559, and a donation from Motorola. 1 simulations is to exploit the available parallelism with the least possible overhead. Unfortunately, both conservative and optimistic asynchronous methods introduce significant overhead in their attempt to find parallelism <ref> [18, 19, 36] </ref>. They are not appropriate for synchronous architectural simulations. Therefore, we focus our attention on synchronous parallel simulation methods. In this paper we present a synchronous, parallel, event-driven approach (SPaDES) to the execution of a single simulation program on a parallel machine and study its performance. <p> In a follow-up paper, Ayani and Berkman [5] simulate GCMINs using the three-phase algorithm on a CM-2 with 8,192 processors. Even though they achieve speedups of more than 2,000 for networks with 14 stages, the efficiency of their implementation is considerably low. Konas and Yew <ref> [19] </ref> compare the performance of a parallel centralized-time event driven method to the Chandy-Misra [10] and Time Warp [17] methods in simulating a multiprocessor. The results they present show that the synchronous method is considerably faster than both the Chandy-Misra and the Time Warp approaches.
Reference: [20] <author> P. Konas and P.-C. Yew. </author> <title> Synchronous Parallel Discrete Event Simulation on Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 6th Workshop on Parallel and Distributed Simulations, </booktitle> <pages> pages 20-32, </pages> <month> January, </month> <year> 1992. </year>
Reference-contexts: In order to avoid these potential problems and reduce the overhead introduced by the simulation method, we propose a conservative synchronous parallel simulation method (SPaDES) <ref> [20] </ref>. Our approach is based on the logical processes model [13]. The simulator consists of a set of communicating logical processes (LPs), each of which represents a component or a group of components of the simulated system.
Reference: [21] <author> D.H. Lawrie. </author> <title> Access and Alignment of Data in an Array Processor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-24(12):1145-1155, </volume> <month> December, </month> <year> 1975. </year>
Reference-contexts: of events that need to be evaluated during the course of the simulation, thus increasing the efficiency of its parallel execution. 4 The Simulated System In order to study the performance of SPaDES, we examine a behavioral model of a synchronous multiprocessor: a 16-processor/16-memory system with an Omega interconnection network <ref> [21] </ref>. This simulation models many parallel shared memory architectures found in practice [14, 16, 33, 31]. 11 During the simulation each processor calculates requests for the memories and transmits them to the appropriate memory modules through the forward Omega network.
Reference: [22] <author> Y.H. Levendel, P.R. Menon, and S.H. Patel. </author> <title> Special Purpose Computer for Logic Simulation using Distributed Processing. </title> <journal> Bell System Technical Journal, </journal> <volume> 61(10) </volume> <pages> 2873-2909, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: First, the system is divided into sets of strings, defined as sets of connected components with as most one fanin and one fanout included. Second, a load leveling 18 19 20 scheme is used to insure that block sizes are kept fairly close <ref> [1, 22, 34] </ref>. SPaDES performs well when there is sufficient amount of parallelism inherent in the simulation. It achieves a (relative) speedup of more than 11 on 16 processors (e.g. random and nonconflicting accesses simulations).
Reference: [23] <author> B.D. Lubachevsky. </author> <title> Bounded Lag Distributed Discrete Event Simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 183-191, </pages> <year> 1988. </year> <month> 24 </month>
Reference-contexts: Each iteration of the algorithm, first identifies the definite independent events from a set of potentially independent events, and then evaluates these events concurrently. Lubachevsky proposed the use of a moving time window to reduce the overhead associated with determining when it is safe to process an event <ref> [23, 24, 25] </ref>. The algorithm, called the bounded lag algorithm, determines which events occur within a statically defined window and executes them concurrently.
Reference: [24] <author> B.D. Lubachevsky. </author> <title> Scalability of the Bounded Lag Distributed Discrete Event Simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 100-107, </pages> <year> 1989. </year>
Reference-contexts: Each iteration of the algorithm, first identifies the definite independent events from a set of potentially independent events, and then evaluates these events concurrently. Lubachevsky proposed the use of a moving time window to reduce the overhead associated with determining when it is safe to process an event <ref> [23, 24, 25] </ref>. The algorithm, called the bounded lag algorithm, determines which events occur within a statically defined window and executes them concurrently.
Reference: [25] <author> B.D. Lubachevsky. </author> <title> Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks. </title> <journal> Communications of the ACM, </journal> <volume> 32(1) </volume> <pages> 111-123, </pages> <month> January, </month> <year> 1989. </year>
Reference-contexts: Each iteration of the algorithm, first identifies the definite independent events from a set of potentially independent events, and then evaluates these events concurrently. Lubachevsky proposed the use of a moving time window to reduce the overhead associated with determining when it is safe to process an event <ref> [23, 24, 25] </ref>. The algorithm, called the bounded lag algorithm, determines which events occur within a statically defined window and executes them concurrently.
Reference: [26] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Memory-Conscious Scheduling in Shared-Memory Multiprocessors. </title> <type> Technical report, </type> <institution> Computer Science Department, University of Rochester, </institution> <year> 1991. </year>
Reference-contexts: On NUMA multiprocessors, processor self-scheduling has the additional responsibility of addressing the tradeoff between load balancing and locality management <ref> [26, 27] </ref>. For this purpose we use a shared scheduling queue per processor and employ a two-level data structure that allows us to efficiently schedule conditionally active LPs for future execution and, at the same time, to easily exploit affinity information during the allocation of processors to unconditionally active LPs.
Reference: [27] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared-Memory Multiprocessors. </title> <type> Technical Report 399, </type> <institution> Computer Science Department, University of Rochester, Oc-tober, </institution> <year> 1991. </year>
Reference-contexts: On NUMA multiprocessors, processor self-scheduling has the additional responsibility of addressing the tradeoff between load balancing and locality management <ref> [26, 27] </ref>. For this purpose we use a shared scheduling queue per processor and employ a two-level data structure that allows us to efficiently schedule conditionally active LPs for future execution and, at the same time, to easily exploit affinity information during the allocation of processors to unconditionally active LPs.
Reference: [28] <author> W. Najjar, J.-L. Jezouin, and J.-L. Gaudiot. </author> <title> Parallel Discrete-Event Simulation. </title> <booktitle> IEEE Design and Test of Computers, </booktitle> <pages> pages 41-44, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The feature that separates different synchronous ap 3 proaches is the method used to determine which events are safe to process. Najjar et al. proposed the use of directional distances between objects to identify independent events on a centralized event queue <ref> [29, 28] </ref>. Two events are independent if the interval of time between their respective occurrences is smaller than the minimum information propagation delay between the two objects to which they belong. Each iteration of the algorithm is divided into three phases.
Reference: [29] <author> W. Najjar, J.-L. Jezouin, and J.-L. Gaudiot. </author> <title> Parallel Execution of Discrete-Event Simulation. </title> <booktitle> In Proceedings of the 1987 International Conference on Computer Design, </booktitle> <pages> pages 668-671, </pages> <year> 1987. </year>
Reference-contexts: The feature that separates different synchronous ap 3 proaches is the method used to determine which events are safe to process. Najjar et al. proposed the use of directional distances between objects to identify independent events on a centralized event queue <ref> [29, 28] </ref>. Two events are independent if the interval of time between their respective occurrences is smaller than the minimum information propagation delay between the two objects to which they belong. Each iteration of the algorithm is divided into three phases.
Reference: [30] <author> D.M. Nicol. </author> <title> The Cost of Concervative Synchronization in Parallel Discrete Event Simulations. </title> <type> Technical Report 90-20, </type> <institution> ICASE, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The simulation terminates when the lower edge of the window reaches the end of simulated time. Nicol proposed an algorithm similar to Lubachevsky's bounded lag, but in which the width of the execution window is dynamically calculated <ref> [30] </ref>. At the beginning of each iteration, the processors cooperatively determine the upper window edge (ceiling). This value is chosen in such a way that all events 4 within the time interval [f loor; ceiling) can safely be processed in parallel.
Reference: [31] <author> J. Palmer and Jr. G.L. Steele. </author> <title> Connection Machine Model CM-5 System Overview. </title> <booktitle> In Proceedings of Frontiers '92: Fourth Symposium on Massively Parallel Computation, </booktitle> <pages> pages 474-483, </pages> <year> 1992. </year>
Reference-contexts: This simulation models many parallel shared memory architectures found in practice <ref> [14, 16, 33, 31] </ref>. 11 During the simulation each processor calculates requests for the memories and transmits them to the appropriate memory modules through the forward Omega network.
Reference: [32] <author> R. Razdan, G. Bischoff, and E. Ulrich. </author> <title> Exploitation of Periodicity in Logic Simulation of Synchronous Systems. </title> <booktitle> In Proceedings of 1990 IEEE International Conference on Computer Aided Design, </booktitle> <pages> pages 62-65, </pages> <year> 1990. </year>
Reference-contexts: In contrast, architectural simulations are inherently synchronous; the activity during the simulation follows the changes of the clock signal (s) (a behavior known as the clock effect <ref> [32, 39, 40] </ref>). Furthermore, such simulations contain significant amounts of inherent parallelism [36]. Hence, the main problem in architectural parallel fl This work was supported in part by the National Science Foundation under Grant Nos.
Reference: [33] <author> J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report TR-9202001, </type> <institution> Kendall Square Research, </institution> <year> 1992. </year>
Reference-contexts: This simulation models many parallel shared memory architectures found in practice <ref> [14, 16, 33, 31] </ref>. 11 During the simulation each processor calculates requests for the memories and transmits them to the appropriate memory modules through the forward Omega network.
Reference: [34] <author> S.P. Smith, B. Underwood, </author> <title> and M.R. Mercer. An Analysis of Several Approaches to Circuit Simulation for Parallel Logic Simulation. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <pages> pages 664-667, </pages> <year> 1987. </year>
Reference-contexts: First, the system is divided into sets of strings, defined as sets of connected components with as most one fanin and one fanout included. Second, a load leveling 18 19 20 scheme is used to insure that block sizes are kept fairly close <ref> [1, 22, 34] </ref>. SPaDES performs well when there is sufficient amount of parallelism inherent in the simulation. It achieves a (relative) speedup of more than 11 on 16 processors (e.g. random and nonconflicting accesses simulations).
Reference: [35] <author> L. Soule and T. Blank. </author> <title> Parallel Logic Simulation on General Purpose Machines. </title> <booktitle> In Proceedings of the 25th ACM/IEEE Design Automation Conference, </booktitle> <pages> pages 166-171, </pages> <year> 1988. </year>
Reference-contexts: However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm <ref> [9, 35, 36] </ref>. Under this algorithm, simulation evolves in steps which are divided into two phases. During the first phase, we concurrently evaluate all events occurring at the earliest simulation time.
Reference: [36] <author> L.P. Soule. </author> <title> Parallel Logic Simulation: An Evaluation of Centralized-Time and Distributed-Time Algorithms. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: In contrast, architectural simulations are inherently synchronous; the activity during the simulation follows the changes of the clock signal (s) (a behavior known as the clock effect [32, 39, 40]). Furthermore, such simulations contain significant amounts of inherent parallelism <ref> [36] </ref>. Hence, the main problem in architectural parallel fl This work was supported in part by the National Science Foundation under Grant Nos. MIP 89-20891 and MIP 93-07910, NASA NCC 2-559, and a donation from Motorola. 1 simulations is to exploit the available parallelism with the least possible overhead. <p> MIP 89-20891 and MIP 93-07910, NASA NCC 2-559, and a donation from Motorola. 1 simulations is to exploit the available parallelism with the least possible overhead. Unfortunately, both conservative and optimistic asynchronous methods introduce significant overhead in their attempt to find parallelism <ref> [18, 19, 36] </ref>. They are not appropriate for synchronous architectural simulations. Therefore, we focus our attention on synchronous parallel simulation methods. In this paper we present a synchronous, parallel, event-driven approach (SPaDES) to the execution of a single simulation program on a parallel machine and study its performance. <p> However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm <ref> [9, 35, 36] </ref>. Under this algorithm, simulation evolves in steps which are divided into two phases. During the first phase, we concurrently evaluate all events occurring at the earliest simulation time. <p> As parallelism studies show <ref> [7, 36] </ref>, the most striking characteristic of synchronous simulations is the clock effect: steps with high activity, followed by long relatively inactive periods. If a PDES method is to be efficient on such simulations it should handle well these long periods with limited parallelism.
Reference: [37] <author> J.S. Steinman. </author> <title> Interactive SPEEDES. </title> <booktitle> In Proceedings of the 24rd Annual Simulation Symposium, 1991 Eastern Multiconference, </booktitle> <pages> pages 149-158, </pages> <year> 1991. </year>
Reference-contexts: Other interesting synchronous methods proposed in the literature are Space-Time [11], the use of conditional knowledge [12], both of which were proposed by Chandy and Sherman, and Breathing Time Buckets a hybrid method proposed by Steinman <ref> [37, 38] </ref>. However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm [9, 35, 36]. Under this algorithm, simulation evolves in steps which are divided into two phases.
Reference: [38] <author> J.S. Steinman. SPEEDES: </author> <title> Synchronous Parallel Environment for Emulation and Discrete. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation Event Simulation, </booktitle> <pages> pages 95-103, </pages> <year> 1991. </year>
Reference-contexts: Other interesting synchronous methods proposed in the literature are Space-Time [11], the use of conditional knowledge [12], both of which were proposed by Chandy and Sherman, and Breathing Time Buckets a hybrid method proposed by Steinman <ref> [37, 38] </ref>. However, the most obvious and commonly used parallel synchronous simulation algorithm is the straightforward parallel implementation of the traditional event driven algorithm [9, 35, 36]. Under this algorithm, simulation evolves in steps which are divided into two phases.
Reference: [39] <author> E. Ulrich. </author> <title> A Design Verification Methodology Based on Concurrent Simulation and Clock Suppression. </title> <booktitle> In Proceedings of 20th Design Automation Conference, </booktitle> <pages> pages 709-712, </pages> <year> 1983. </year> <month> 25 </month>
Reference-contexts: In contrast, architectural simulations are inherently synchronous; the activity during the simulation follows the changes of the clock signal (s) (a behavior known as the clock effect <ref> [32, 39, 40] </ref>). Furthermore, such simulations contain significant amounts of inherent parallelism [36]. Hence, the main problem in architectural parallel fl This work was supported in part by the National Science Foundation under Grant Nos.
Reference: [40] <author> E.G. Ulrich. </author> <title> Exclusive Simulation of Activity in Digital Circuits. </title> <journal> Communications of the ACM, </journal> <volume> 12(2) </volume> <pages> 102-110, </pages> <month> February, </month> <year> 1969. </year>
Reference-contexts: In contrast, architectural simulations are inherently synchronous; the activity during the simulation follows the changes of the clock signal (s) (a behavior known as the clock effect <ref> [32, 39, 40] </ref>). Furthermore, such simulations contain significant amounts of inherent parallelism [36]. Hence, the main problem in architectural parallel fl This work was supported in part by the National Science Foundation under Grant Nos.
Reference: [41] <author> Q. Yu, D. Towsley, and P. Heidelberger. </author> <title> Time-Driven Parallel Simulation of Multistage Interconnection Networks. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 191-196, </pages> <year> 1989. </year>
Reference-contexts: Such systems are naturally discrete and inherently parallel. Thus, they are excellent candidates for parallel simulation. Several parallel simulation methods have been used with various degrees of success. Yu et al. <ref> [41] </ref> use a time-driven algorithm to simulate a class of Multistage Interconnection Networks (MINs) called Buffered Delta Networks. In their work, they focus on the synchronization of the processors within and between time steps and present three approaches for handling the problem: multibarriers, locks, and temporary storage.
References-found: 41

