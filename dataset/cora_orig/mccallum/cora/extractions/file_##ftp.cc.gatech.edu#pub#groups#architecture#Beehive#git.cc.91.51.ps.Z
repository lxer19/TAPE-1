URL: file://ftp.cc.gatech.edu/pub/groups/architecture/Beehive/git.cc.91.51.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/s/Gautam.Shah/homepage.html
Root-URL: 
Title: Towards Exploiting the Architectural Features of  
Author: Beehive Gautam Shah Umakishore Ramachandran 
Keyword: Key Words: Multiprocessor caches, shared memory consistency models, reader-initiated coherence, false shar ing, compiler issues.  
Note: This work is supported in part by the NSF PYI Award MIP-9058430.  
Address: Atlanta, GA 30332 USA  
Affiliation: College of Computing Georgia Institute of Technology  
Pubnum: GIT-CC-91/51  
Email: e-mail: rama@cc.gatech.edu  
Phone: Ph: (404) 894-5136  
Abstract: Beehive" is a project that investigates the software and hardware issues in the design of scalable shared memory multiprocessors. The architecture is designed to support a form of weakly consistent memory model in a cache-based multiprocessor environment. The novel features of the architecture are decoupling the notion of the type of data (private or shared) from the domain (local or global) over which the access should be performed; eliminating false sharing; providing for software assisted consistency maintenance using reader-initiated coherence; and supporting queue-based shared and exclusive locks in hardware. We identify how the architectural features supporting a weak memory model can be exploited by the system software such as the compiler and the runtime. In particular, marking algorithms are developed for specifying the domain of loads and stores that occur between synchronization points in a parallel program. 
Abstract-found: 1
Intro-found: 1
Reference: [AH90] <author> S. Adve and M. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: At about the same time that RC was developed, we proposed weak coherence [LR90a] and Adve and Hill proposed DRF0 <ref> [AH90] </ref> both of which have very similar ordering constraint as RC.
Reference: [CFKA90] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-based cache coherence in large-scale multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: CACHE D R CACHE CONTROLLER WRITE BUFFER NETWORK CONTROLLER NETWORK MEMORY D R machine. Each node has a private cache and an associated directory. A piece of the global shared memory resides on each node with its associated directory. Beehive uses a directory-based protocol <ref> [CFKA90] </ref> for maintaining cache coherence. The write-buffer is provided for buffering global writes. Each node communicates with the rest of the system through the network controller.
Reference: [CMM88] <author> R. Cytron, S. Marlovsky, and K. P. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In 17th International Conference on Parallel Processing, </booktitle> <pages> pages II-229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In all of the discussion above relating to concurrency we assume that the language runtime is responsible for maintaining the task dependence relationship that is inherent in the parallel program. The work presented in this section is inspired by the kind of work done by Cytron et al. <ref> [CMM88] </ref>. While their work deals with analyzing and exploiting loop-level parallelism for programs using the SC memory model, our work is applicable to arbitrary synchronization and concurrency scenarios in the context of Beehive for the BC memory model.
Reference: [DSB86] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: In such a case, there need not be a single global order that has to be observed by all the streams since the relative order is constrained by the explicit synchronization. Therefore, SC over-specifies the ordering when explicit synchronization is used. Recognizing this fact, Dubois et al. <ref> [DSB86] </ref> have proposed the Weak Consistency (WC) model that relaxes the ordering constraint of SC by distinguishing between accesses to synchronization variables and ordinary data. <p> In the rest of this paper we use the terms performed and globally performed in the same sense as defined by Dubois et al. <ref> [DSB86, SD87] </ref>. The Release Consistency (RC) model [GLL + 90] goes one step further to distinguish between two kinds of synchronization accesses, namely, acquire and release.
Reference: [EJ91] <author> Susan J. Eggers and Tor E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In 20th International Conference on Parallel Processing, </booktitle> <pages> pages I-377-381, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: It is upto the software to minimize the performance penalty that may result due to false sharing in such architectures. However, it has been recognized that false sharing is quite a difficult problem to solve in software in the general case <ref> [EJ91] </ref>. It turns out that with the consistency preserving mechanisms of Beehive it is not just a performance penalty but a correctness issue if false sharing is not eliminated. <p> y do f if i is the matching SEstart then f check [i] = true; g if i has a store of x f mark the store as write-global; check [i] = true; g else f mark (i, x); check [i] = true; g g 16 by careful data placement <ref> [EJ91] </ref>. Beehive provides implicit queue-based locking primitives that are associated with each cache line. To implement the language recognized synchronization constructs, the compiler or runtime may choose to use these primitives. However, care has to be taken in allocating shared variables when these implicit locks are in use.
Reference: [FOW87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The list of locks on shared data mentioned in the side-effect specification of the construct is maintained with these nodes. The rest of the dependence graph is constructed in the normal way <ref> [FOW87] </ref>. The main difference between the PDG constructed in [FOW87] and the one we construct is that in our graph there are additional nodes that explicitly signify all the loads and stores in the program. <p> The list of locks on shared data mentioned in the side-effect specification of the construct is maintained with these nodes. The rest of the dependence graph is constructed in the normal way <ref> [FOW87] </ref>. The main difference between the PDG constructed in [FOW87] and the one we construct is that in our graph there are additional nodes that explicitly signify all the loads and stores in the program. Once the PDG has been constructed the mapping of loads and stores in the PDG to the read/write primitives of Beehive proceeds as follows.
Reference: [GHPR88] <author> Thomas R. Gross, John L. Hennessy, Steven A. Przybylski, and Christopher Rowen. </author> <title> Measurement and evaluation of the MIPS architecture and processor. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(3) </volume> <pages> 229-257, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This property is really something the compiler can exploit to reorder loads that occur after a CP-Synch operation in the program to further help in latency hiding. This situation is similar to the one in pipelined uniprocessors such as MIPS <ref> [GHPR88] </ref>, wherein there are no hardware interlocks, and the compiler is entrusted with the responsibility of ensuring that the dependencies in the sequential program are respected.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In the rest of this paper we use the terms performed and globally performed in the same sense as defined by Dubois et al. [DSB86, SD87]. The Release Consistency (RC) model <ref> [GLL + 90] </ref> goes one step further to distinguish between two kinds of synchronization accesses, namely, acquire and release. <p> Correspondingly, the programming model guarantees that coherent values of shared data become visible to concurrent threads that comprise a parallel program only after the completion of synchronization operations that fall into the CP-Synch category. Programs that fall into this class have been referred to as properly labeled (PL) programs <ref> [GLL + 90] </ref>. The language used for writing programs for Beehive should support the above programming model. Further, to exploit the multiple processors in the architecture the language should have constructs to explicitly specify concurrency. Using such constructs it is possible to construct a task graph that represents the program.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multipro-cess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year> <month> 20 </month>
Reference-contexts: The programming model is defined by the memory consistency model, which specifies the order of execution of the memory accesses from independent processors. 2.1 A Chronology Sequential Consistency (SC) has been proposed by Lamport <ref> [Lam79] </ref> as the ordering constraint for the correct execution of multiprocess computations. This memory model requires the order of execution of the independent access streams to be any arbitrary interleaving of the streams which preserves the relative order in each access stream.
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory--based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In Beehive the type of operation (local or global) is decoupled from the type of data (private or shared). Thus it is possible for the compiler to exploit this architectural feature to track the BC memory model more closely than other architectures <ref> [LLG + 90] </ref>. For example, loads and stores outside of the SEs are marked as reads and writes, respectively, which are local operations if the data is already in the cache (see Section 4). <p> This decoupling may be exploited by the compiler (as detailed above) to reduce the number of global accesses. Note that consistency is maintained only when accesses are performed globally (i.e. write-global, read-update). This is in contrast to other shared memory architectures such as <ref> [LLG + 90] </ref> where there is a concept of ownership of a cache line at all times regardless of whether the line contains shared or private data. The absence of ownership in Beehive is the key aspect that eliminates false sharing.
Reference: [LR90a] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Locks, directories, and weak coherence - A recipe for scalable shared memory multiprocessors. </title> <booktitle> In 1990 ISCA Workshop on Scalable Shared-Memory Multiprocessors, </booktitle> <month> May </month> <year> 1990. </year> <title> To appear in Scalable Shared Memory Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The main difference in the ordering constraint of RC with respect to WC, is that RC requires that constraint b above apply only for a release access, and constraint c apply only for an acquire access. At about the same time that RC was developed, we proposed weak coherence <ref> [LR90a] </ref> and Adve and Hill proposed DRF0 [AH90] both of which have very similar ordering constraint as RC.
Reference: [LR90b] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Synchronization with multiprocessor caches. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 27-37, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Performance implications (through analytical and simulation studies) of reader-initiated coherence, the BC memory model, and queue-based locks may be found in [LR91b]. The protocols and performance implications of incorporating synchronization in multiprocessor caches is discussed in <ref> [LR90b] </ref>. 5 Exploiting the Features of Beehive The programming model that corresponds to the BC memory model assumes that the data is divided into three classes: private variables, shared variables, and synchronization variables.
Reference: [LR91a] <author> Monica S. Lam and Martin C. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Third ACM SIGPLAN Symposium on the Principles and Practices of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The task graph generated from the explicit concurrency constructs in the language may restrict the true concurrency that may exist in the program. The true concurrency is really dictated by the inter-relationship between the SEs that comprise the tasks. The language Jade <ref> [LR91a] </ref> has constructs that match well with our intuition of synchronization epochs. Jade separates the notions of synchronization and concurrency thus allowing the exploitation of SE-level concurrency.
Reference: [LR91b] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Architectural primitives for a scalable shared memory multiprocessor. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 103-114, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Further, distinguishing between normal read/write accesses and synchronization accesses in hardware enables efficient implementation of weaker notions of memory consistency <ref> [LR91b] </ref>. "Beehive" is a project that addresses some of the hardware and software issues in the design of scalable shared memory multiprocessors. The rest of the paper is organized as follows: We trace the evolution of memory consistency models and their chronology in Section 2. <p> The name weak coherence has since been changed to Buffered Consistency (BC) <ref> [LR91b] </ref> to reflect the architectural features that we propose to support this model (see Section 4). 2.2 Buffered Consistency The BC memory model recognizes two types of accesses: data, and synchronization. Data accesses (reads and writes) may be to private or to shared data. <p> We show in the next section that with minimal overhead we can solve this problem in hardware. 4 Architecture of Beehive A summary of the architectural features of Beehive were presented in an earlier paper <ref> [LR91b] </ref>, which focussed on the performance implications of these features. <p> Performance implications (through analytical and simulation studies) of reader-initiated coherence, the BC memory model, and queue-based locks may be found in <ref> [LR91b] </ref>. The protocols and performance implications of incorporating synchronization in multiprocessor caches is discussed in [LR90b]. 5 Exploiting the Features of Beehive The programming model that corresponds to the BC memory model assumes that the data is divided into three classes: private variables, shared variables, and synchronization variables.
Reference: [MCS91] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The second is the intrinsic overhead for implementing the synchronization. An atomic read-modify-write type of operation is sufficient to implement higher level synchronization primitives but this could lead to bursty traffic on the release of a mutual exclusion lock. Recent work <ref> [MCS91] </ref> has shown that it is possible to implement locks and barriers with minimal network traffic. In spite of reducing the network traffic, these software algorithms still have to pay the latency penalty for synchronization operations. Therefore, we have provided synchronization support in hardware. <p> Therefore, only one shared data object can be allocated per cache line when used in conjunction with an implicit lock. Note that any number of private variables may be colocated in the same cache line. However, if the synchronization constructs are implemented using software mutual exclusion algorithms <ref> [MCS91, PS85] </ref>, then colocation of shared data objects in the same cache line is not a problem. As we mentioned in the earlier section, (implicit) hardware locks that can be active in a processor at a given time are limited by the size of the fully-associative lock cache.
Reference: [PN85] <author> G. F. Pfister and V. A. Norton. </author> <title> Hotspot contention and combining in multistage interconnection network. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-8, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: It turns out that the latter has more potential for causing network contention than the former due to the possibility for simultaneous access to the same synchronization variable from several processors <ref> [PN85] </ref>. Network contention may be considered a second order effect induced by the shared memory accesses, and an efficient solution to combat the latency problem can go a long way in reducing the network contention.
Reference: [PS85] <author> James L. Peterson and Abraham Silberschatz. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley Publishing Coompany, </publisher> <year> 1985. </year>
Reference-contexts: Therefore, only one shared data object can be allocated per cache line when used in conjunction with an implicit lock. Note that any number of private variables may be colocated in the same cache line. However, if the synchronization constructs are implemented using software mutual exclusion algorithms <ref> [MCS91, PS85] </ref>, then colocation of shared data objects in the same cache line is not a problem. As we mentioned in the earlier section, (implicit) hardware locks that can be active in a processor at a given time are limited by the size of the fully-associative lock cache.
Reference: [SD87] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operations of cache-based multiprocessors. </title> <booktitle> In 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: In the rest of this paper we use the terms performed and globally performed in the same sense as defined by Dubois et al. <ref> [DSB86, SD87] </ref>. The Release Consistency (RC) model [GLL + 90] goes one step further to distinguish between two kinds of synchronization accesses, namely, acquire and release.
Reference: [Ste90] <author> Per Stenstrom. </author> <title> A survey of cache coherence schemes for multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 12-24, </pages> <month> June </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: As a nice side benefit of solving this potential correctness problem in hardware, we also eliminate the ill-effects of false sharing. We have chosen a pointer-based directory structure since it is more scalable than either a full-map or a limited map directory structure from the point of memory requirement <ref> [Ste90] </ref>. A criticism against such a structure is the latency for serial propagation of invalidations or updates, and the inability to use multicast.
References-found: 19

