URL: http://www.isi.edu/soar/galk/Publications/98/aaai98fn.ps.gz
Refering-URL: http://www.isi.edu/soar/galk/Publications/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: galk, tambe-@isi.edu  
Title: What is Wrong With Us? Improving Robustness Through Social Diagnosis  
Author: Gal A. Kaminka and Milind Tambe 
Address: 4676 Admiralty Way, Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Note: In Proceedings of the 15 th National Conference on Artificial Intelligence (AAAI-98)  
Abstract: 1 Robust behavior in complex, dynamic environments mandates that intelligent agents autonomously monitor their own runtime behavior, detect and diagnose failures, and attempt recovery. This challenge is intensified in multi-agent settings, where the coordinated and competitive behaviors of other agents affect an agents own performance. Previous approaches to this problem have often focused on single agent domains and have failed to address or exploit key facets of multi-agent domains, such as handling team failures. We present SAM, a complementary approach to monitoring and d iagnosis for multi-agent domains that is particularly wellsuited for collaborative settings. SAM i ncludes the following key novel concepts: First, SAMs fai lure detection technique, inspired by social psychology, utilizes other agents as information sources and detects failures both in an agent and in its teammates. Second, SAM performs social diagnosis, reasoning about the failures in its team using an explicit model of teamwork (previously, teamwork models have been employed only in prescribing agent behaviors in teamwork). Third, SAM employs model sharing to alleviate the inherent inefficiencies associated with representing multiple agent models. We have impl emented SAM in a complex, realistic multi-agent domain, and provide detailed empirical results assessing its benefits. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkins, E. M.; Durfee, E. H.; and Shin, K. G. </author> <year> 1997. </year> <title> Detecting and reacting to unplanned-for world states, </title> <booktitle> in Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97). </booktitle> <pages> pp. 571-576. </pages>
Reference-contexts: The inherent e xplosion of state space complexity in these dynamic enviro nments inhibits the ability of any designer, human or machine (i.e., planners), to specify the correct response in each po ssible state in advance <ref> (Atkins et al. 1997) </ref>. For instance, it is generally difficult to predict when sensors will return unrel iable answers, communication message s get lost, etc. The agents are therefore presented with countless opportunities for 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). <p> Indeed. SAM is useful as a diagnosis component for general teamwork models, allowing a general teamwork r eplanner to take over the recovery process. Atkins et al. <ref> (Atkins et al. 1997) </ref> attack a similar problem of detecting states for which the agent does not have a plan ready. They offer a classification of these states, and provide planning algorithms that build tests for these states. However, their approach considers only the individual agents and not teams.
Reference: <author> Bakker, P.; and Kuniyoshi, Y. </author> <year> 1996. </year> <title> Robot see, robot do : An overview of robot imitation. </title> <booktitle> AISB Workshop on Learning in Robots and Animals, </booktitle> <address> Brighton, UK. </address>
Reference-contexts: This is to be expected, as SAM is a complementary technique, and does not consider the task-specific sensors that the condition monitors do. SAMs results should also be contrasted with those achieved with imitation <ref> (Bakker and Kuniyoshi 1996) </ref>. Imitation is a very special case of the general SAM method - by choosing to always adapt the others view, SAM leads to imitation. However, imitation works only in the presence of a correct role-model.
Reference: <author> Doyle R. J., Atkinson D. J., Doshi R. S., </author> <title> Generating perception requests and expectations to verify the execution of plans, </title> <booktitle> in Proceedings of AAAI-86. </booktitle>
Reference: <author> Festinger, L. </author> <year> 1954. </year> <title> A theory of social comparison processes. </title> <booktitle> Human Relations, </booktitle> <volume> 7, </volume> <pages> pp. 117-140. </pages>
Reference-contexts: It is particularly relevant for collaborative (teamwork) settings, that are ubiquitous in multi-agent env ironments. SAM allows detection of failures in the monitoring agent and its peers by a technique inspired by Social Comparison Theory <ref> (Festinger 1954) </ref>. The key idea is that agents compare their own behavior, beliefs, goals, and plans to those of other agents, reason about the differences in b elief and behavior (not necessarily imitating the others), and draw useful conclusions regarding the correctness of their own actions or their peers. <p> We begin in this section by describing SAMs failure detection process, which is inspired by Social Comparison Theory <ref> (Festinger 1954) </ref> from social psychology.
Reference: <author> Firby, J. </author> <year> 1987. </year> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-87). </booktitle>
Reference-contexts: Our agents design is based on reactive plans (operators) <ref> (Firby 1987, Newell 1990) </ref>, which form a decomposition hierarchy that controls each agent. Operator hierarchies provide a good tradeoff between the criteria considered above, as they are both compact enough to be reasoned about efficiently, while being central to the agent behavior (capturing it s decision process).
Reference: <author> Grosz, B.; and Kraus, S. </author> <year> 1996. </year> <title> Collaborative Plans for Complex Group Actions. </title> <booktitle> In Artificial Intelligence . Vol. </booktitle> <volume> 86, </volume> <pages> pp. 269-358. </pages>
Reference-contexts: In particular, teamwork models contain domain-independent axioms that prescribe general responsibilities for team members and the team. These axioms in turn have been derived from teamwork theories, such as joint intentions (Levesque et al. 1990) and SharedPlans <ref> (Grosz and Kraus, 1996) </ref>. Our basic idea is to backchain through these axioms to diagnose the failure. For instance, one axiom of the joint intentions framework mandates that a persistent (committed) team goal cannot be abandoned unless there exists mutual belief in the team goal being irrelevant, achieved, or unachievable.
Reference: <author> Halpern, J. Y. and Moses, Y. </author> <year> 1990. </year> <title> Knowledge and Common Knowledge in a Distributed Environment ., in Distributed Computing 37(3), </title> <journal> pp. </journal> <pages> 549-587. </pages>
Reference-contexts: In theory, team operators must therefore always be identical for all team members. However, given the well recognized difficulty of establishing mutual belief in practice <ref> (Halpern and Moses 1990) </ref>, differences in team operators unfortunately do occur. Furthermore, for security or efficiency, team members sometimes deliberately reduce communication, and inadve rtently contribute to such team-operator differences.
Reference: <author> Huber, M. J.; and Durfee, E. H. </author> <year> 1996. </year> <title> An Initial Assessment of Plan-Recognition-Based Coordination for Multi-Agent Teams. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems.. </booktitle>
Reference: <author> Jennings, N. </author> <year> 1995. </year> <title> Controlling Cooperative Problem Solving in Industrial Multi-Agent System Using Joint Intentions. </title> <journal> Artificial Intelligence. </journal> <volume> Vol. </volume> <pages> 75 pp. 195-240. </pages>
Reference-contexts: These agents do not have the guarantees of maximal social similarity at the team level, and while they possibly will find the detected differences useful, they cannot be certain of failures, nor facilitate team recovery (since the other agent may simply have left the team opportunistically). Work on teamwork <ref> (Jennings 1995, Tambe 1997) </ref> concentrates on maintaining identical joint goals to prevent miscoordination, while the focus of SAM is on detecting when the goals do differ. Indeed. SAM is useful as a diagnosis component for general teamwork models, allowing a general teamwork r eplanner to take over the recovery process.
Reference: <author> Kitano, H; Asada, M.; Kuniyoshi, Y.; Noda, I.; and Osawa, E. </author> <year> 1995. </year> <title> RoboCup: The Robot World Cup Initiative. </title> <booktitle> In Proceedings of IJCAI-95 Workshop on Entertainment and AI/Alife. </booktitle>
Reference-contexts: This problem is exacerbated in complex multi-agent environments due to the added r equirements for communication and coordination. Example domains include virtual environments for training (Tambe et al. 1995), robotic soccer <ref> (Kitano et al. 95) </ref>, p otential multi-robotic space missions, etc. The inherent e xplosion of state space complexity in these dynamic enviro nments inhibits the ability of any designer, human or machine (i.e., planners), to specify the correct response in each po ssible state in advance (Atkins et al. 1997).
Reference: <author> Levesque, H. J.; Cohen, P. R.; Nunes, J. </author> <year> 1990. </year> <title> On acting together, </title> <booktitle> in Proceedings of the National Conference on Artificial Intelligence (AAAI-1990) , Menlo Park, </booktitle> <address> California, </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus, tracing back through such a model can help confirm and diagnose team failures. In particular, teamwork models contain domain-independent axioms that prescribe general responsibilities for team members and the team. These axioms in turn have been derived from teamwork theories, such as joint intentions <ref> (Levesque et al. 1990) </ref> and SharedPlans (Grosz and Kraus, 1996). Our basic idea is to backchain through these axioms to diagnose the failure.
Reference: <author> Newell A., </author> <year> 1990. </year> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press. </publisher>
Reference-contexts: Ther efore, the monitoring agent communicates with teammates and their commander to possibly resolve this problem. Results and Evaluation Our agent, including SAM, is implemented completely in the Soar integrated AI architecture <ref> (Newell 1990) </ref>. About 1200 rules are used to implement the agent, including the military procedures, teamwork capabilities (STEAM), and plan-recognition capabilities (RESC team ). Approximately 60 additional rules implement SAM, forming an add-on layer on top of the procedures making up the agent.
Reference: <author> Tambe, M.; Johnson W. L.; Jones, R.; Koss, F.; Laird, </author> <note> J. </note>
Reference: <author> E.; Rosenbloom, P. S.; and Schwamb, K. </author> <year> 1995. </year> <title> Intelligent Agents for interactive simulation environments. </title> <journal> AI Magazine, </journal> <note> 16(1) (Spring). </note>
Reference: <author> Tambe, M. </author> <year> 1996. </year> <title> Tracking Dynamic Team Activity, </title> <booktitle> in Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96). </booktitle>
Reference-contexts: Our estimates of reliability and the cost make plan recognition an attractive choice for acquiring knowledge of others. We use the RESC team <ref> (Tambe 1996) </ref> method for plan recognition, but different techniques may be used interchangeably, as long as they provide the needed information and representation. RESC team provides real-time plan re cognition capabilities, constructing operator hierarchies (in the recognizing agents memory) that correspond to the other agents currently executing reactive operators. <p> To alleviate this inefficiency, we rely on modelsharing-the use of shared hierarchies for team operators <ref> (Tambe 1996) </ref>. Team operators at equal depths (the agents own, and those i nferred of others), which should be identical for all team-members, are shared in the agent memory. Thus, only ind ividual operators are maintained separately.
Reference: <author> Tambe, M. </author> <booktitle> 1997 . Towards Flexible Teamwork , in Journal of Artificial Intelligence Research, </booktitle> <volume> Vol. 7. </volume> <pages> pp. 83-124. </pages>
Reference-contexts: We therefore chose operator hierarchies for our comparison purpose. Figure 1 presents a small portion of such a hierarchy. Each operator in the hierarchy has pr econditions for selecting it, application conditions to apply it, and termination conditions. The design of the hierarchical plans uses the STEAM framework <ref> (Tambe 1997) </ref> for mai ntaining an explicit model of teamwork. Following this framework, operators may be team operators (that explicitly represent the joint activities of the team) or individual (specific to one agent). In Figure 1, boxed operators are team operators, while other operators are individual. <p> Thus, if the agent discovers a goal has been abandoned by others, it can infer they believe mutual belief has been established in the goals irrelevancy, achievement, or unachievability. In our implementation, the agents utilize one such explicit model of teamwork, STEAM <ref> (Tambe 1997) </ref>, for their co llaborative execution of team operators. STEAM ensures that team operators are established jointly by the team via attai nment of mutual bel ief in their preconditions, and terminated jointly by atta ining mutual belief in the team-operators termination conditions (either achievement, unachievability, or irrelevancy conditions).
Reference: <author> Toyama, K.; and Hager, G. D. </author> <year> 1997. </year> <title> If at First You Dont Succeed..., </title> <booktitle> in Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97). </booktitle> <pages> pp. 3-9. </pages>
Reference-contexts: Introduction Attaining robustness in face of uncertainty in complex, dynamic environment s is a key challenge for intelligent agents <ref> (Toyama and Hager 1997) </ref>. This problem is exacerbated in complex multi-agent environments due to the added r equirements for communication and coordination. Example domains include virtual environments for training (Tambe et al. 1995), robotic soccer (Kitano et al. 95), p otential multi-robotic space missions, etc. <p> The agents are therefore presented with countless opportunities for 1 Copyright 1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. failure, and must autonomously monitor and detect fai lures in their runtime behavior, then diagnose and recover from them, i.e., agents must display post-failure robustness <ref> (Toyama and Hager 1997) </ref>. Previous approaches to monitoring and diagnosis (e.g., Doyle et al. 1986, Williams and Nayak 1996) have often focused on a single agent that utilizes designersupplied information, either in the form of explicit execution-monitoring cond itions, or a model of the agent itself.
Reference: <author> Williams, B. C.; and Nayak, P. P. </author> <year> 1996. </year> <title> A Model-Based Approach to Reactive Self-Configuring Systems. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96). </booktitle>
References-found: 18

