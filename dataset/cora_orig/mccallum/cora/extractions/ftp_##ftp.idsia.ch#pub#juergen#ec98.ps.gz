URL: ftp://ftp.idsia.ch/pub/juergen/ec98.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Email: juergen@idsia.ch  
Title: A GENERAL METHOD FOR INCREMENTAL SELF-IMPROVEMENT AND MULTI-AGENT LEARNING  
Author: Jurgen Schmidhuber 
Note: To appear in X. Yao, editor, Evolutionary Computation: Theory and Applications. Scientific Publ. Co., Singapore, 1998.  
Web: http://www.idsia.ch/~juergen  
Address: Corso Elvezia 36 CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> L. Adleman. </author> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1979. </year>
Reference-contexts: In general, this makes it impossible to take the entire learning history into account. Algorithmic probability / Universal search. Levin's universal search algorithm is theoretically optimal for certain "non-incremental" search tasks with exactly repeatable initial conditions. See Levin [19, 20]; see also Adleman <ref> [1] </ref>. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. [46, 22, 31].
Reference: [2] <author> A. G. Barto. </author> <title> Connectionist approaches for control. </title> <type> Technical Report COINS 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003, </address> <year> 1989. </year>
Reference-contexts: 1 Theoretical Considerations Previous work on reinforcement learning (e.g., <ref> [16, 2, 49, 51] </ref>) requires strong assumptions about the environment. In many realistic settings, however, these assumptions do not hold. In particular, any event/action/experiment occurring early in the life of a learning system may influence events/actions/experiments at any later time. <p> In general environments, events/actions/experiments occurring early in system life may influence events/actions/experiments at any later time. In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is not addressed by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [16, 2, 49, 51] </ref>), and not even by naive, inefficient, but more general and supposedly infallible exhaustive search among all possible policies, as will be seen next.
Reference: [3] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: Throughout its lifetime, the system's goal is to maximize R (T ), the cumulative reinforcement at (initially unknown) "death" T . There is only one life. Time flows in one direction (no resets to zero). Related, but less general scenarios were studied in papers on "bandit problems" (e.g., <ref> [3, 9] </ref> and references therein), which also require to wisely use limited resources to perform experiments.
Reference: [4] <author> M. Boddy and T. L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: Scenario. Consider a learning system executing a lifelong action sequence in an unknown environment. For now, it won't even be necessary to introduce a detailed formal model of the environment. Different system actions may require different amounts of execution time (like in scenarios studied in <ref> [25, 4] </ref>, and in references given therein). Occasionally the environment provides real-valued "reinforcement". The sum of all reinforcements obtained between "system birth" (at time 0) and time t &gt; 0 is denoted by R (t).
Reference: [5] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: This lack is shared by almost all other reinforcement learning approaches, though. Note, however, that unlike previous, less general systems, the novel system in principle can exploit almost arbitrary environmental regularities <ref> [14, 5, 45, 21] </ref> (if there are any) to speed up performance improvement, simply because it can run almost arbitrary learning algorithms. <p> Somehow, a training set is chosen. In almost all cases, the shortest algorithm computing a (non-overlapping) test set essentially has the same size as the whole test set. This is because most computable objects are irregular and incompressible <ref> [14, 5] </ref>. The shortest algorithm computing the test set, given the training set, isn't any shorter. <p> In other words, the relative algorithmic complexity of the test set, given the training set, is maximal, and the mutual algorithmic information between test set and training set is zero (ignoring an additive constant independent of the problem | see <ref> [14, 5, 45, 21] </ref>). Therefore, in almost all cases, (1) knowledge of the training set does not provide any clues about the test set, (2) there is no hope for generalization, and (3) inductive inference does not make any sense.
Reference: [6] <author> N. L. Cramer. </author> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In J.J. Grefenstette, editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale NJ, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Today, this approach would be classified as "Genetic Programming", e.g. [15]. [7] was in fact one of the first two papers on using GP-like algorithms to evolve assembler-like computer programs (but Cramer's work preceded ours <ref> [6] </ref>). We applied our system to simple tasks, including the "lawnmower problem" (later also studied by Koza, 1994).
Reference: [7] <author> D. Dickmanns, J. Schmidhuber, and A. Winklhofer. </author> <title> Der genetische Algorithmus: Eine Implementierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen, </institution> <year> 1987. </year>
Reference-contexts: Meta-evolution. My first attempts to come up with schemes for "true" 2 self-referential learning based on universal languages date back to 1987. They were partly inspired by a collaboration with Dickmanns and Winklhofer <ref> [7] </ref>. We used a genetic algorithm (GA) to evolve variable length programs for solving simple tasks (the system was implemented in Prolog). Today, this approach would be classified as "Genetic Programming", e.g. [15]. [7] was in fact one of the first two papers on using GP-like algorithms to evolve assembler-like computer <p> They were partly inspired by a collaboration with Dickmanns and Winklhofer <ref> [7] </ref>. We used a genetic algorithm (GA) to evolve variable length programs for solving simple tasks (the system was implemented in Prolog). Today, this approach would be classified as "Genetic Programming", e.g. [15]. [7] was in fact one of the first two papers on using GP-like algorithms to evolve assembler-like computer programs (but Cramer's work preceded ours [6]). We applied our system to simple tasks, including the "lawnmower problem" (later also studied by Koza, 1994).
Reference: [8] <author> T. G. Dietterich. </author> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, </address> <pages> pages 124-128. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen.
Reference: [9] <author> J. C. Gittins. </author> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY, </address> <year> 1989. </year>
Reference-contexts: Throughout its lifetime, the system's goal is to maximize R (T ), the cumulative reinforcement at (initially unknown) "death" T . There is only one life. Time flows in one direction (no resets to zero). Related, but less general scenarios were studied in papers on "bandit problems" (e.g., <ref> [3, 9] </ref> and references therein), which also require to wisely use limited resources to perform experiments.
Reference: [10] <author> R. Greiner. </author> <title> PALO: A probabilistic hill-climbing algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2), </volume> <year> 1996. </year>
Reference-contexts: There is only one life. Time flows in one direction (no resets to zero). Related, but less general scenarios were studied in papers on "bandit problems" (e.g., [3, 9] and references therein), which also require to wisely use limited resources to perform experiments. See also <ref> [10] </ref> and references therein for work on finding search space elements with optimal expected utility, subject to the resource constraint of spending only a feasible amount of time on finding such an element. Policy.
Reference: [11] <author> S. Heil. </author> <title> Universelle Suche und inkrementelles Lernen, </title> <type> diploma thesis, </type> <institution> 1995. Fakultat fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution> <month> 27 </month>
Reference-contexts: The content of this work cell, however, does not have essential limits, and may point to any address in storage. See <ref> [11, 53, 42, 40, 39] </ref>, however, for alternative implementations without double indexed addressing. Current policy. The set of all current P -values defines the system's current policy. Instruction cycle. <p> Inserting prior bias. The few experiments above were designed to illustrate basic principles of the paradigm. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments using such low-level instructions can be found in <ref> [11, 53, 40, 42] </ref>).
Reference: [12] <author> F. Hoffmeister and T. </author> <title> Back. Genetic algorithms and evolution strategies: Similarities and differences. </title> <editor> In R. Manner and H. P. Schwefel, editors, </editor> <booktitle> Proc. of 1st International Conference on Parallel Problem Solving from Nature, </booktitle> <address> Berlin. </address> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: For instance, in principle, unlike previous evolutionary and genetic algorithms <ref> [23, 43, 13, 12, 15] </ref>, the system can learn to focus its modifications on interfaces between useful "subprograms" ("divide and conquer"), instead of mutating the subprograms themselves (if this proves to be beneficial in a given environment), thus creating a higher-level, more abstract search 14 space (! directed mutations as opposed
Reference: [13] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: For instance, in principle, unlike previous evolutionary and genetic algorithms <ref> [23, 43, 13, 12, 15] </ref>, the system can learn to focus its modifications on interfaces between useful "subprograms" ("divide and conquer"), instead of mutating the subprograms themselves (if this proves to be beneficial in a given environment), thus creating a higher-level, more abstract search 14 space (! directed mutations as opposed
Reference: [14] <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11, </pages> <year> 1965. </year>
Reference-contexts: This lack is shared by almost all other reinforcement learning approaches, though. Note, however, that unlike previous, less general systems, the novel system in principle can exploit almost arbitrary environmental regularities <ref> [14, 5, 45, 21] </ref> (if there are any) to speed up performance improvement, simply because it can run almost arbitrary learning algorithms. <p> Somehow, a training set is chosen. In almost all cases, the shortest algorithm computing a (non-overlapping) test set essentially has the same size as the whole test set. This is because most computable objects are irregular and incompressible <ref> [14, 5] </ref>. The shortest algorithm computing the test set, given the training set, isn't any shorter. <p> In other words, the relative algorithmic complexity of the test set, given the training set, is maximal, and the mutual algorithmic information between test set and training set is zero (ignoring an additive constant independent of the problem | see <ref> [14, 5, 45, 21] </ref>). Therefore, in almost all cases, (1) knowledge of the training set does not provide any clues about the test set, (2) there is no hope for generalization, and (3) inductive inference does not make any sense.
Reference: [15] <author> J. R. Koza. </author> <title> Genetic Programming II Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: For instance, in principle, unlike previous evolutionary and genetic algorithms <ref> [23, 43, 13, 12, 15] </ref>, the system can learn to focus its modifications on interfaces between useful "subprograms" ("divide and conquer"), instead of mutating the subprograms themselves (if this proves to be beneficial in a given environment), thus creating a higher-level, more abstract search 14 space (! directed mutations as opposed <p> They were partly inspired by a collaboration with Dickmanns and Winklhofer [7]. We used a genetic algorithm (GA) to evolve variable length programs for solving simple tasks (the system was implemented in Prolog). Today, this approach would be classified as "Genetic Programming", e.g. <ref> [15] </ref>. [7] was in fact one of the first two papers on using GP-like algorithms to evolve assembler-like computer programs (but Cramer's work preceded ours [6]). We applied our system to simple tasks, including the "lawnmower problem" (later also studied by Koza, 1994).
Reference: [16] <author> P. R. Kumar and P. Varaiya. </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: 1 Theoretical Considerations Previous work on reinforcement learning (e.g., <ref> [16, 2, 49, 51] </ref>) requires strong assumptions about the environment. In many realistic settings, however, these assumptions do not hold. In particular, any event/action/experiment occurring early in the life of a learning system may influence events/actions/experiments at any later time. <p> In general environments, events/actions/experiments occurring early in system life may influence events/actions/experiments at any later time. In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is not addressed by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [16, 2, 49, 51] </ref>), and not even by naive, inefficient, but more general and supposedly infallible exhaustive search among all possible policies, as will be seen next.
Reference: [17] <author> D. Lenat. </author> <title> Theory formation by heuristic search. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <year> 1983. </year>
Reference-contexts: Although such approaches sometimes may have their merits, they do not deserve the attribute "self-referential" | the additional level typically just defers the credit assignment problem. However, there were a few apparently more general approaches. For instance, Lenat <ref> [17] </ref> reports that his Eurisko system was able to discover certain heuristics for discovering heuristics.
Reference: [18] <author> L. A. Levin. </author> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference-contexts: The second implementation leads to a general reinforcement learning algorithm for recurrent nets. Alternatively, however, the PMP from section 1 may be designed to execute arbitrary, conventional or non-conventional learning or search algorithms. Other SSA applications. In recent work [50, 42], we combine SSA and Levin search (LS) <ref> [18, 20] </ref> to solve partially observable Markov decision problems (POMDPs). POMDPs received a lot of attention in the reinforcement learning community. LS is theoretically optimal for a wide variety of search problems including many POMDPs.
Reference: [19] <author> L. A. Levin. </author> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210, </pages> <year> 1974. </year>
Reference-contexts: In general, this makes it impossible to take the entire learning history into account. Algorithmic probability / Universal search. Levin's universal search algorithm is theoretically optimal for certain "non-incremental" search tasks with exactly repeatable initial conditions. See Levin <ref> [19, 20] </ref>; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. [46, 22, 31]. <p> Meta-version of universal search. Without going into details, Solomonoff [46] mentions that self-improvement may be formulated as a time-limited optimization problem, thus being solvable by universal search. However, the straight-forward meta-version of universal search (generating and evaluating probability distributions in order of their Levin complexities <ref> [19] </ref>) just defers the credit assignment problem to the meta-level, and does not necessarily make optimal incremental use of computational resources and previous experience 3 . In fact, just like exhaustive search, but unlike SSA, universal search by itself cannot properly deal with changing environments.
Reference: [20] <author> L. A. Levin. </author> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37, </pages> <year> 1984. </year>
Reference-contexts: In general, this makes it impossible to take the entire learning history into account. Algorithmic probability / Universal search. Levin's universal search algorithm is theoretically optimal for certain "non-incremental" search tasks with exactly repeatable initial conditions. See Levin <ref> [19, 20] </ref>; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. [46, 22, 31]. <p> The second implementation leads to a general reinforcement learning algorithm for recurrent nets. Alternatively, however, the PMP from section 1 may be designed to execute arbitrary, conventional or non-conventional learning or search algorithms. Other SSA applications. In recent work [50, 42], we combine SSA and Levin search (LS) <ref> [18, 20] </ref> to solve partially observable Markov decision problems (POMDPs). POMDPs received a lot of attention in the reinforcement learning community. LS is theoretically optimal for a wide variety of search problems including many POMDPs.
Reference: [21] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: This lack is shared by almost all other reinforcement learning approaches, though. Note, however, that unlike previous, less general systems, the novel system in principle can exploit almost arbitrary environmental regularities <ref> [14, 5, 45, 21] </ref> (if there are any) to speed up performance improvement, simply because it can run almost arbitrary learning algorithms. <p> In other words, the relative algorithmic complexity of the test set, given the training set, is maximal, and the mutual algorithmic information between test set and training set is zero (ignoring an additive constant independent of the problem | see <ref> [14, 5, 45, 21] </ref>). Therefore, in almost all cases, (1) knowledge of the training set does not provide any clues about the test set, (2) there is no hope for generalization, and (3) inductive inference does not make any sense.
Reference: [22] <author> W. Paul and R. J. Solomonoff. </author> <title> Autonomous theory building systems, 1991. </title> <type> Manuscript, </type> <note> revised 1994. </note>
Reference-contexts: See Levin [19, 20]; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. <ref> [46, 22, 31] </ref>. For instance, to improve future performance, Solomonoff [45, 46] describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in [31] simply keeps successful code in its program area.
Reference: [23] <author> I. Rechenberg. </author> <title> Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. </title> <type> Dissertation, </type> <year> 1971. </year> <note> Published 1973 by Fromman-Holzboog. </note>
Reference-contexts: For instance, in principle, unlike previous evolutionary and genetic algorithms <ref> [23, 43, 13, 12, 15] </ref>, the system can learn to focus its modifications on interfaces between useful "subprograms" ("divide and conquer"), instead of mutating the subprograms themselves (if this proves to be beneficial in a given environment), thus creating a higher-level, more abstract search 14 space (! directed mutations as opposed
Reference: [24] <author> M. B. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: The success of an SSM recursively depends on the success of all later SSMs. SSA automatically takes care of this, thus recursively encouraging "learning how to learn how to learn ...". This represents an essential difference to previous approaches to "continual" learning, see <ref> [24] </ref>. Improvement speed? Due to the generality of the approach, no reasonable statements can be made about improvement speed, which indeed highly depends on the nature of the environment and the choice of initial primitive instructions. This lack is shared by almost all other reinforcement learning approaches, though.
Reference: [25] <author> S. Russell and E. </author> <title> Wefald. </title> <booktitle> Principles of Metareasoning. Artificial Intelligence, </booktitle> <volume> 49 </volume> <pages> 361-395, </pages> <year> 1991. </year>
Reference-contexts: Scenario. Consider a learning system executing a lifelong action sequence in an unknown environment. For now, it won't even be necessary to introduce a detailed formal model of the environment. Different system actions may require different amounts of execution time (like in scenarios studied in <ref> [25, 4] </ref>, and in references given therein). Occasionally the environment provides real-valued "reinforcement". The sum of all reinforcements obtained between "system birth" (at time 0) and time t &gt; 0 is denoted by R (t).
Reference: [26] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen.
Reference: [27] <author> J. Schmidhuber. </author> <title> Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... </title> <type> hook. </type> <institution> Institut fur Informatik, Technische Universitat Munchen, </institution> <year> 1987. </year> <month> 28 </month>
Reference-contexts: Meta-evolution recursively creates a growing hierarchy of pools of programs | higher-level pools containing program modifying programs being applied to lower-level programs and being rewarded based on lower-level performance. Details in <ref> [27] </ref>. Collapsing meta-levels. The explicit creation of "meta-levels" and "meta-meta-levels" seemed unnatural, however. For this reason, alternative systems based on "self-referential" languages were explored, the goal being to collapse all meta-levels into one [27]. At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. <p> Details in <ref> [27] </ref>. Collapsing meta-levels. The explicit creation of "meta-levels" and "meta-meta-levels" seemed unnatural, however. For this reason, alternative systems based on "self-referential" languages were explored, the goal being to collapse all meta-levels into one [27]. At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. Later work presented a neural network with the potential to run its own weight change algorithm [30, 29].
Reference: [28] <author> J. Schmidhuber. </author> <title> Reinforcement learning in Markovian and non-Markovian environ-ments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Unlike with Sutton's original set-up, the system does not see a built-in unique representation of its current position on the grid. Its interface to the environment is non-Markovian <ref> [28] </ref>. This represents one reason why most traditional reinforcement learning systems do not have a sound strategy for solving this task. Another is the changing policy environment. See next paragraph. 19 which also includes storage cells with changing contents. See figure 1. Changing policy environment.
Reference: [29] <author> J. Schmidhuber. </author> <title> A neural network that embeds its own meta-levels. </title> <booktitle> In Proc. of the International Conference on Neural Networks '93, </booktitle> <address> San Francisco. </address> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. Later work presented a neural network with the potential to run its own weight change algorithm <ref> [30, 29] </ref>. With this system, top-level credit 2 I am not talking about fixed learning algorithms for adjusting the parameters of others. For instance, GAs are sometimes used to adjust learning rates of gradient based neural nets, etc.
Reference: [30] <author> J. Schmidhuber. </author> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: In the end, no weight stack had more than 7 entries (each followed by faster reinforcement intake than all the previous ones). Ongoing work. It is intended to replace the random weight modification process by a process that may be strongly influenced by the current weights themselves. Following <ref> [30] </ref>, the idea is to use activation patterns across special output units to address and modify the network's own weights. <p> At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. Later work presented a neural network with the potential to run its own weight change algorithm <ref> [30, 29] </ref>. With this system, top-level credit 2 I am not talking about fixed learning algorithms for adjusting the parameters of others. For instance, GAs are sometimes used to adjust learning rates of gradient based neural nets, etc.
Reference: [31] <author> J. Schmidhuber. </author> <title> Discovering problem solutions with low Kolmogorov complexity and high generalization capability. </title> <type> Technical Report FKI-194-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, </institution> <year> 1994. </year> <note> Short version in A. </note> <editor> Prieditis and S. Russell, eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pages 488-496, </pages> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. <p> See Levin [19, 20]; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. <ref> [46, 22, 31] </ref>. For instance, to improve future performance, Solomonoff [45, 46] describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in [31] simply keeps successful code in its program area. <p> For instance, to improve future performance, Solomonoff [45, 46] describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in <ref> [31] </ref> simply keeps successful code in its program area. This system was a conceptual starting point for the one in the current paper. With first attempts (in September 1994), the probability distributions underlying the Turing machine equivalent language required for universal search were modified heuristically.
Reference: [32] <author> J. Schmidhuber. </author> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, </institution> <year> 1994. </year> <note> Revised 1995. </note>
Reference-contexts: Again, an application to a variant of Sutton's maze task will serve to illustrate the operation of the system. In this application, the environment of each connection's policy continually changes, because the policies of all the other connections keep changing. Note. This paper is based on <ref> [32] </ref>. In the meantime we have published several additional papers on SSA, e.g., [50, 53, 40, 42]. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". <p> Note. This paper is based on <ref> [32] </ref>. In the meantime we have published several additional papers on SSA, e.g., [50, 53, 40, 42]. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". To improve/speed up its own (initially very dumb, highly random) learning strategy, the system policy makes use of an assembler-like programming language suitable to modify the policy itself. <p> To find out whether the incremental self-improvement paradigm did indeed lead to incremental self-improvement, let us have a look at the learning history (the results are slightly different from those reported in <ref> [32] </ref>, where a slightly different implementation led to different calls of the random number generator). Self-generated reduction of numbers of probability modifications. In the beginning, the system computed a lot of probability modifications but later preferred to decrease the number of probability modifications per time interval. <p> Comparable results were obtained with additional runs. As with the previous task, performance did not improve smoothly. The history of broken records reflects the history of performance improvements (the results from the run reported below are slightly different from those reported in <ref> [32] </ref>, where a slightly different implementation led to different calls of the random number generator). First, there was a rather quick sequence of improvements which lasted until time 4:6 fl 10 7 . By then (after 155,741 payoff events), the shortest trial so far had taken 44 time steps. <p> Additional applications of SSA to quite challenging, complex multiagent tasks are described in [41, 40]. 7 Acknowledgements I am grateful to Ray Solomonoff, Peter Dayan, Mike Mozer, Don Matthis, Clayton McMil-lan, and various NIPS*94 participants, for valuable comments/discussions on the first version of <ref> [32] </ref>. Many thanks to Sepp Hochreiter, Gerhard Wei, Martin Eldracher, Margit 26 Kinder, and Daniel Prelinger, for critical remarks on earlier drafts, and to Leslie Kaelbling, David Cohn, Tommi Jaakkola, and Andy Barto, for useful comments on later versions.
Reference: [33] <author> J. Schmidhuber. </author> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: Both register area and input area are subsets of the work area. Environmental inputs. At every time step, new inputs from the environment may be written into the input cells. Primitives. The assembler-like programming language is partly inspired by the one in <ref> [33, 38] </ref>. There are n ops primitive instructions (n ops &lt;< M axint). Each "primitive" is represented by a unique number in the set f0; 1; : : : ; n ops 1g (due to the code being written in C).
Reference: [34] <author> J. Schmidhuber. </author> <title> Environment-independent reinforcement acceleration. </title> <note> Technical Report Note IDSIA-59-95, IDSIA, </note> <month> June </month> <year> 1995. </year> <institution> Invited talk at Hongkong University of Science and Technology. </institution>
Reference-contexts: Also, thanks to Marco Dorigo, Luca Gambardella, Rafal Salustowicz, Cristina Versino, and Marco Wiering, for hepful remarks on <ref> [34] </ref>. I am particularly indebted to Mark Ring for extensive and constructive criticism. The recent collaboration with Jieyu Zhao was supported by SNF grant 21-43'417.95 "Incremental Self-Improvement" and SNF grant 2100-49'144.96 "Long Short-Term Memory".
Reference: [35] <author> J. Schmidhuber. </author> <title> A general method for multi-agent learning in unrestricted environments. In Adaptation, Co-evolution and Learning in Multiagent Systems, </title> <type> Technical Report SS-96-01, </type> <pages> pages 84-87. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <address> Menlo Park, Calif., </address> <year> 1996. </year>
Reference-contexts: In cases where all agents try to speed up the same reinforcement signals, and where no agent can speed up reinforcement intake by itself, this automatically enforces "learning to cooperate" <ref> [35, 36, 40] </ref>. Section 2.3 will illustrate this with an application of a system consisting of multiple agents, where each agent is in fact just a connection in a neural net. (**) Learning how to learn / Incremental self-improvement.
Reference: [36] <author> J. Schmidhuber. </author> <title> Realistic multi-agent reinforcement learning. </title> <editor> In G. Weiss, editor, </editor> <booktitle> Learning in Distributed Artificial Intelligence Systems. Working Notes of the 1996 ECAI Workshop. </booktitle> <year> 1996. </year>
Reference-contexts: In cases where all agents try to speed up the same reinforcement signals, and where no agent can speed up reinforcement intake by itself, this automatically enforces "learning to cooperate" <ref> [35, 36, 40] </ref>. Section 2.3 will illustrate this with an application of a system consisting of multiple agents, where each agent is in fact just a connection in a neural net. (**) Learning how to learn / Incremental self-improvement.
Reference: [37] <author> J. Schmidhuber. </author> <title> A computer scientist's view of life, the universe, and everything. </title> <editor> In C. Freksa, M. Jantzen, and R. Valk, editors, </editor> <booktitle> Foundations of Computer Science: Theory, Cognition, Applications, </booktitle> <volume> volume 1337, </volume> <pages> pages 201-208. </pages> <booktitle> Lecture Notes in Computer Science, </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. <p> See, e.g., [8, 26, 52, 31, 38, 37]. Paraphrasing from a previous argument <ref> [38, 37] </ref>: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. In almost all cases, the shortest algorithm computing a (non-overlapping) test set essentially has the same size as the whole test set. <p> This may seem discouraging. Atypical real world. Apparently, however, generalization and inductive inference do make sense in the real world! One reason for this may be that the real world is run by a short algorithm. See, e.g., <ref> [37] </ref>. Anyway, problems that humans consider to be typical are atypical when compared to the general set of all well-defined problems.
Reference: [38] <author> J. Schmidhuber. </author> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873, </pages> <year> 1997. </year> <month> 29 </month>
Reference-contexts: Both register area and input area are subsets of the work area. Environmental inputs. At every time step, new inputs from the environment may be written into the input cells. Primitives. The assembler-like programming language is partly inspired by the one in <ref> [33, 38] </ref>. There are n ops primitive instructions (n ops &lt;< M axint). Each "primitive" is represented by a unique number in the set f0; 1; : : : ; n ops 1g (due to the code being written in C). <p> See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. <p> See, e.g., [8, 26, 52, 31, 38, 37]. Paraphrasing from a previous argument <ref> [38, 37] </ref>: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. In almost all cases, the shortest algorithm computing a (non-overlapping) test set essentially has the same size as the whole test set.
Reference: [39] <author> J. Schmidhuber. </author> <note> What's interesting? Technical Report IDSIA-35-97, IDSIA, </note> <year> 1997. </year>
Reference-contexts: The content of this work cell, however, does not have essential limits, and may point to any address in storage. See <ref> [11, 53, 42, 40, 39] </ref>, however, for alternative implementations without double indexed addressing. Current policy. The set of all current P -values defines the system's current policy. Instruction cycle.
Reference: [40] <author> J. Schmidhuber, J. Zhao, and N. Schraudolph. </author> <title> Reinforcement learning with self-modifying policies. </title> <editor> In S. Thrun and L. Pratt, editors, </editor> <booktitle> Learning to learn, </booktitle> <pages> pages 293-309. </pages> <publisher> Kluwer, </publisher> <year> 1997. </year>
Reference-contexts: In cases where all agents try to speed up the same reinforcement signals, and where no agent can speed up reinforcement intake by itself, this automatically enforces "learning to cooperate" <ref> [35, 36, 40] </ref>. Section 2.3 will illustrate this with an application of a system consisting of multiple agents, where each agent is in fact just a connection in a neural net. (**) Learning how to learn / Incremental self-improvement. <p> In this application, the environment of each connection's policy continually changes, because the policies of all the other connections keep changing. Note. This paper is based on [32]. In the meantime we have published several additional papers on SSA, e.g., <ref> [50, 53, 40, 42] </ref>. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". <p> The content of this work cell, however, does not have essential limits, and may point to any address in storage. See <ref> [11, 53, 42, 40, 39] </ref>, however, for alternative implementations without double indexed addressing. Current policy. The set of all current P -values defines the system's current policy. Instruction cycle. <p> To illustrate basic aspects of the principle, the remainder of this paper will also present a few experiments. These, how 15 ever, in no way represent a systematic experimental analysis. In fact, many much more complex experiments are described in other recent papers on this subject, e.g., <ref> [50, 42, 40] </ref>. The experiments in the current section demonstrate that the system from section 2 indeed can learn to compute SSMs leading to faster and faster reinforcement intake. The system uses low-level problem-specific instructions in addition to the 17 general, assembler-like instructions mentioned in section 2. <p> Instead, this section's purpose is to illustrate typical aspects of the system's basic (bias independent) mode of operation. See, e.g., <ref> [42, 40] </ref> for more complex applications. 3.1 Writing Variable Sequences Task. The external environment consists of an array of 30 variables V 0 ; V 1 ; : : : ; V 29 . The i-th variable is denoted by V i . <p> Inserting prior bias. The few experiments above were designed to illustrate basic principles of the paradigm. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments using such low-level instructions can be found in <ref> [11, 53, 40, 42] </ref>). <p> Experiments demonstrate the multi-agent system's effectiveness. For instance, a system consisting of three co-evolving agents chasing each other learns rather sophisticated, stochastic predator and prey strategies. Additional applications of SSA to quite challenging, complex multiagent tasks are described in <ref> [41, 40] </ref>. 7 Acknowledgements I am grateful to Ray Solomonoff, Peter Dayan, Mike Mozer, Don Matthis, Clayton McMil-lan, and various NIPS*94 participants, for valuable comments/discussions on the first version of [32].
Reference: [41] <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Simple principles of metalearning. </title> <type> Technical Report IDSIA-69-96, </type> <institution> IDSIA, </institution> <year> 1996. </year>
Reference-contexts: Experiments demonstrate the multi-agent system's effectiveness. For instance, a system consisting of three co-evolving agents chasing each other learns rather sophisticated, stochastic predator and prey strategies. Additional applications of SSA to quite challenging, complex multiagent tasks are described in <ref> [41, 40] </ref>. 7 Acknowledgements I am grateful to Ray Solomonoff, Peter Dayan, Mike Mozer, Don Matthis, Clayton McMil-lan, and various NIPS*94 participants, for valuable comments/discussions on the first version of [32].
Reference: [42] <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130, </pages> <year> 1997. </year>
Reference-contexts: In this application, the environment of each connection's policy continually changes, because the policies of all the other connections keep changing. Note. This paper is based on [32]. In the meantime we have published several additional papers on SSA, e.g., <ref> [50, 53, 40, 42] </ref>. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". <p> The content of this work cell, however, does not have essential limits, and may point to any address in storage. See <ref> [11, 53, 42, 40, 39] </ref>, however, for alternative implementations without double indexed addressing. Current policy. The set of all current P -values defines the system's current policy. Instruction cycle. <p> To illustrate basic aspects of the principle, the remainder of this paper will also present a few experiments. These, how 15 ever, in no way represent a systematic experimental analysis. In fact, many much more complex experiments are described in other recent papers on this subject, e.g., <ref> [50, 42, 40] </ref>. The experiments in the current section demonstrate that the system from section 2 indeed can learn to compute SSMs leading to faster and faster reinforcement intake. The system uses low-level problem-specific instructions in addition to the 17 general, assembler-like instructions mentioned in section 2. <p> Instead, this section's purpose is to illustrate typical aspects of the system's basic (bias independent) mode of operation. See, e.g., <ref> [42, 40] </ref> for more complex applications. 3.1 Writing Variable Sequences Task. The external environment consists of an array of 30 variables V 0 ; V 1 ; : : : ; V 29 . The i-th variable is denoted by V i . <p> Inserting prior bias. The few experiments above were designed to illustrate basic principles of the paradigm. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments using such low-level instructions can be found in <ref> [11, 53, 40, 42] </ref>). <p> In fact, just like exhaustive search, but unlike SSA, universal search by itself cannot properly deal with changing environments. However, variants of universal search may be used as the parameter modification algorithms executed by PMPs (see section 1 and recent work with Marco Wiering <ref> [50, 42] </ref>). 6 Conclusion It is easy to show that there can be no algorithm for general, unknown environments that is guaranteed to continually increase reinforcement intake per fixed time interval. <p> The second implementation leads to a general reinforcement learning algorithm for recurrent nets. Alternatively, however, the PMP from section 1 may be designed to execute arbitrary, conventional or non-conventional learning or search algorithms. Other SSA applications. In recent work <ref> [50, 42] </ref>, we combine SSA and Levin search (LS) [18, 20] to solve partially observable Markov decision problems (POMDPs). POMDPs received a lot of attention in the reinforcement learning community. LS is theoretically optimal for a wide variety of search problems including many POMDPs.
Reference: [43] <author> H. P. Schwefel. </author> <title> Numerische Optimierung von Computer-Modellen. </title> <type> Dissertation, </type> <year> 1974. </year> <note> Published 1977 by Birkhauser, Basel. </note>
Reference-contexts: For instance, in principle, unlike previous evolutionary and genetic algorithms <ref> [23, 43, 13, 12, 15] </ref>, the system can learn to focus its modifications on interfaces between useful "subprograms" ("divide and conquer"), instead of mutating the subprograms themselves (if this proves to be beneficial in a given environment), thus creating a higher-level, more abstract search 14 space (! directed mutations as opposed
Reference: [44] <author> C. E. Shannon. </author> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, </journal> <volume> XXVII:379-423, </volume> <year> 1948. </year>
Reference-contexts: If the instruction is syntactically incorrect, IP is reset to the first program cell. This instruction cycle represents the basic operation of the system. System life. At time step 0, storage is initialized with zeros. The probability distributions of all program cells are initialized with maximum entropy distributions <ref> [44] </ref>. That is, all P ij values are initialized to the same value, so that there is no bias for a particular value in any cell. After initialization, the instruction cycle is repeated over and over again until system death at time T .
Reference: [45] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: This lack is shared by almost all other reinforcement learning approaches, though. Note, however, that unlike previous, less general systems, the novel system in principle can exploit almost arbitrary environmental regularities <ref> [14, 5, 45, 21] </ref> (if there are any) to speed up performance improvement, simply because it can run almost arbitrary learning algorithms. <p> In other words, the relative algorithmic complexity of the test set, given the training set, is maximal, and the mutual algorithmic information between test set and training set is zero (ignoring an additive constant independent of the problem | see <ref> [14, 5, 45, 21] </ref>). Therefore, in almost all cases, (1) knowledge of the training set does not provide any clues about the test set, (2) there is no hope for generalization, and (3) inductive inference does not make any sense. <p> See Levin [19, 20]; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. [46, 22, 31]. For instance, to improve future performance, Solomonoff <ref> [45, 46] </ref> describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in [31] simply keeps successful code in its program area. This system was a conceptual starting point for the one in the current paper.
Reference: [46] <author> R.J. Solomonoff. </author> <title> A system for incremental learning based on algorithmic probability. </title> <editor> In E. P. D. Pednault, editor, </editor> <booktitle> The Theory and Application of Minimal-Length Encoding (Preprint of Symposium papers of AAAI 1990 Spring Symposium), </booktitle> <year> 1990. </year>
Reference-contexts: This is analoguous to the history of science itself. Informally, a "revolution" corresponds to a self-improvement with high "conceptual jump size" (an expression coined by Solomonoff <ref> [46] </ref>). One nice thing about open-ended incremental self-improvement is that there is no significant theoretical limit to the nature of the revolutions and to what the system may learn. This is, of course, due to the general nature of the underlying programming language. Inserting prior bias. <p> See Levin [19, 20]; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. <ref> [46, 22, 31] </ref>. For instance, to improve future performance, Solomonoff [45, 46] describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in [31] simply keeps successful code in its program area. <p> See Levin [19, 20]; see also Adleman [1]. There were a few attempts to extend universal search to incremental learning situations, where previous "trials" may provide information about how to speed up further learning, see e.g. [46, 22, 31]. For instance, to improve future performance, Solomonoff <ref> [45, 46] </ref> describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in [31] simply keeps successful code in its program area. This system was a conceptual starting point for the one in the current paper. <p> The system, however, was unsatisfactory, precisely because there was no principled way of adjusting probability distributions. This criticism led to the ideas expressed in the current paper. Meta-version of universal search. Without going into details, Solomonoff <ref> [46] </ref> mentions that self-improvement may be formulated as a time-limited optimization problem, thus being solvable by universal search.
Reference: [47] <author> R. S. Sutton. </author> <title> Integrated modeling and control based on reinforcement learning and dynamic programming. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 471-478. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In section 2, I will exemplify the ideas in paragraph (**) on incremental self-improvement and describe a particular, concrete, working, "evolutionary" system that implements them. Section 3 will then apply this system to various tasks, including a variant of Sutton's maze task <ref> [47] </ref>. One difference to Sutton's original task is that the policy environment continually changes because of actions generated by the system itself. Section 4 will then exemplify the ideas in paragraph (*) on multi-agent learning and also describe a particular, concrete, working, "evolutionary" system that implements them. <p> Such stack entries may be interpreted as results of "adjusting the prior on the space of solution candidates" or "fine-tuning search space structure" or "learning to create directed mutations" or "learning how to learn". 3.2 A Navigation Task Non-Markovian variant of Sutton's Markovian maze task <ref> [47] </ref>, with changing policy environment. The external environment consists of a two-dimensional grid with 9 by 6 fields. F i;j denotes the field in the i-th row and the j-th column.
Reference: [48] <author> P. Utgoff. </author> <title> Shift of bias for inductive concept learning. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 2, </volume> <pages> pages 163-190. </pages> <publisher> Morgan Kauf-mann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: analogy", "learning by chunking", "incremental learning", "continual learning", "learning from invariances", "learning by knowledge transfer" etc. would not be possible, and experience with previous problems could not help to sensibly adjust the prior distribution of solution candidates in the search space for a new problem (shift of inductive bias, e.g. <ref> [48] </ref>).
Reference: [49] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: 1 Theoretical Considerations Previous work on reinforcement learning (e.g., <ref> [16, 2, 49, 51] </ref>) requires strong assumptions about the environment. In many realistic settings, however, these assumptions do not hold. In particular, any event/action/experiment occurring early in the life of a learning system may influence events/actions/experiments at any later time. <p> In general environments, events/actions/experiments occurring early in system life may influence events/actions/experiments at any later time. In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is not addressed by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [16, 2, 49, 51] </ref>), and not even by naive, inefficient, but more general and supposedly infallible exhaustive search among all possible policies, as will be seen next.
Reference: [50] <author> M.A. Wiering and J. Schmidhuber. </author> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: In this application, the environment of each connection's policy continually changes, because the policies of all the other connections keep changing. Note. This paper is based on [32]. In the meantime we have published several additional papers on SSA, e.g., <ref> [50, 53, 40, 42] </ref>. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". <p> To illustrate basic aspects of the principle, the remainder of this paper will also present a few experiments. These, how 15 ever, in no way represent a systematic experimental analysis. In fact, many much more complex experiments are described in other recent papers on this subject, e.g., <ref> [50, 42, 40] </ref>. The experiments in the current section demonstrate that the system from section 2 indeed can learn to compute SSMs leading to faster and faster reinforcement intake. The system uses low-level problem-specific instructions in addition to the 17 general, assembler-like instructions mentioned in section 2. <p> In fact, just like exhaustive search, but unlike SSA, universal search by itself cannot properly deal with changing environments. However, variants of universal search may be used as the parameter modification algorithms executed by PMPs (see section 1 and recent work with Marco Wiering <ref> [50, 42] </ref>). 6 Conclusion It is easy to show that there can be no algorithm for general, unknown environments that is guaranteed to continually increase reinforcement intake per fixed time interval. <p> The second implementation leads to a general reinforcement learning algorithm for recurrent nets. Alternatively, however, the PMP from section 1 may be designed to execute arbitrary, conventional or non-conventional learning or search algorithms. Other SSA applications. In recent work <ref> [50, 42] </ref>, we combine SSA and Levin search (LS) [18, 20] to solve partially observable Markov decision problems (POMDPs). POMDPs received a lot of attention in the reinforcement learning community. LS is theoretically optimal for a wide variety of search problems including many POMDPs.
Reference: [51] <author> R. J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256, </pages> <year> 1992. </year>
Reference-contexts: 1 Theoretical Considerations Previous work on reinforcement learning (e.g., <ref> [16, 2, 49, 51] </ref>) requires strong assumptions about the environment. In many realistic settings, however, these assumptions do not hold. In particular, any event/action/experiment occurring early in the life of a learning system may influence events/actions/experiments at any later time. <p> In general environments, events/actions/experiments occurring early in system life may influence events/actions/experiments at any later time. In particular, PMP i may affect the environmental conditions for PMP k ; k &gt; i. This is not addressed by existing algorithms for adaptive control and reinforcement learning (see, e.g., <ref> [16, 2, 49, 51] </ref>), and not even by naive, inefficient, but more general and supposedly infallible exhaustive search among all possible policies, as will be seen next.
Reference: [52] <author> D. H. Wolpert. </author> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1341-1390, </pages> <year> 1996. </year> <month> 30 </month>
Reference-contexts: See, e.g., <ref> [8, 26, 52, 31, 38, 37] </ref>. Paraphrasing from a previous argument [38, 37]: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen.
Reference: [53] <author> J. Zhao and J. Schmidhuber. </author> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Pattie Maes, Maja Mataric, Jean-Arcady Meyer, Jordan Pollack, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books, </publisher> <year> 1996. </year> <month> 31 </month>
Reference-contexts: In this application, the environment of each connection's policy continually changes, because the policies of all the other connections keep changing. Note. This paper is based on [32]. In the meantime we have published several additional papers on SSA, e.g., <ref> [50, 53, 40, 42] </ref>. 2 SSA for Incremental Self-Improvement Outline. The "evolutionary" system in this section (see also [32]) implements the ideas from section 1, in particular those in paragraph (**) on "incremental self-improvement". <p> The content of this work cell, however, does not have essential limits, and may point to any address in storage. See <ref> [11, 53, 42, 40, 39] </ref>, however, for alternative implementations without double indexed addressing. Current policy. The set of all current P -values defines the system's current policy. Instruction cycle. <p> Inserting prior bias. The few experiments above were designed to illustrate basic principles of the paradigm. They were based on low-level, assembler-like instructions (making even apparently simple tasks difficult | additional experiments using such low-level instructions can be found in <ref> [11, 53, 40, 42] </ref>). <p> Experiments with additional POMs demonstrate: (a) ALS can dramatically reduce the search time consumed by successive calls of LS. (b) Additional significant speed-ups can be obtained by combining ALS and SSA. In other recent work <ref> [53] </ref>, we use SSA for a multi-agent system with agents much more complex than the ones in section 4. In fact, each agent uses incremental self-improvement as described in section 2. Experiments demonstrate the multi-agent system's effectiveness.
References-found: 53

