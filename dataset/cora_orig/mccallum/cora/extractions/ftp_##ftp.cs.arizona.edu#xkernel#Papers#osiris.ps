URL: ftp://ftp.cs.arizona.edu/xkernel/Papers/osiris.ps
Refering-URL: http://www.cs.rice.edu/~druschel/
Root-URL: 
Title: Experiences with a High-Speed Network Adaptor: A Software Perspective  
Author: Peter Druschel and Larry L. Peterson Bruce S. Davie 
Address: Tucson, AZ 85721 Morristown, NJ 07963  
Affiliation: Department of Computer Science Computer Networking Research Department University of Arizona Bell Communications Research  
Abstract: This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for Unix development. </title> <booktitle> In Proceedings of the USENIX Summer '86 Conference, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: The other relevant piece of software, of course, is the OS running on the host. In our case, it is the Mach 3.0 operating system <ref> [1] </ref>, retrofitted with a network subsystem based on the x-kernel [9, 12]. For the purpose of this paper, there are two relevant things to note about the OS. First, because the x-kernel supports arbitrary protocols, our approach is protocol-independent; it is not tailored to TCP/IP.
Reference: [2] <author> D. Banks and M. Prudence. </author> <title> A high-performance network architecture for a PA-RISC workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors <ref> [5, 2, 3, 16, 20] </ref>. This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed [4, 8]. <p> Both the literature on the subject (e.g. <ref> [16, 2, 6] </ref>) and our own experience have led us to the conclusion that the preferable technique is highly machine-dependent.
Reference: [3] <author> G. Blair, et al. </author> <title> A network interface unit to support continuous media. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 264-275, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors <ref> [5, 2, 3, 16, 20] </ref>. This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed [4, 8].
Reference: [4] <author> D. Clark, et al. </author> <title> The AURORA gigabit testbed. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 25 </volume> <pages> 599-621, </pages> <year> 1992. </year>
Reference-contexts: This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed <ref> [4, 8] </ref>. We consider the network adaptor from a software perspective, identifying the subtle interactions between the adaptor and the operating system software that drives it. Others have looked at this hardware/software interaction as well [18, 6, 16].
Reference: [5] <author> Eric Cooper, et al. </author> <title> Host interface design for ATM LANs. </title> <booktitle> In Proc. 16th Conf. on Local Computer Networks, </booktitle> <address> Minneapolis, MN, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors <ref> [5, 2, 3, 16, 20] </ref>. This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed [4, 8].
Reference: [6] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: We consider the network adaptor from a software perspective, identifying the subtle interactions between the adaptor and the operating system software that drives it. Others have looked at this hardware/software interaction as well <ref> [18, 6, 16] </ref>. In our case, fl This work supported in part by National Science Foundation Grant CCR-9102040 and ARPA Contract DABT63-91-C-0030. the flexibility built into the OSIRIS board makes this interaction an especially interesting one to study. The OSIRIS network adaptor was designed specifically to support software experimentation. <p> Both the literature on the subject (e.g. <ref> [16, 2, 6] </ref>) and our own experience have led us to the conclusion that the preferable technique is highly machine-dependent. <p> In the PIO case, with carefully designed software, data can be read from the adaptor and written directly to the application's buffer in main memory, leaving the data in the cache <ref> [13, 6] </ref>. If the application reads the data soon after the PIO transfer, the data may still be in the cache.
Reference: [7] <author> B. S. Davie. </author> <title> A host-network interface architecture for ATM. </title> <booktitle> In Proc. ACM SIGCOMM '91, </booktitle> <address> Zurich, </address> <month> Septem-ber </month> <year> 1991. </year>
Reference-contexts: Clearly, it would be advantageous to increase the length of DMA transfers. In the transmit direction, the only penalty for increasing DMA length is an increase in the granularity of multiplexing. We argued previously that fine-grained multiplexing is advantageous for latency and switch performance reasons <ref> [7] </ref>. However, when the adaptor is used in a mode where the goal is to maximize throughput to a single application, neither of these reasons is relevant. It is therefore reasonable, and straightforward, to modify the DMA controller so that it can perform DMA transactions longer than one ATM cell.
Reference: [8] <author> B. S. Davie. </author> <title> The architecture and implementation of a high-speed host interface. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 228-239, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed <ref> [4, 8] </ref>. We consider the network adaptor from a software perspective, identifying the subtle interactions between the adaptor and the operating system software that drives it. Others have looked at this hardware/software interaction as well [18, 6, 16]. <p> We now discuss each of these problems, in turn. 2.5.1 DMA Overhead As reported previously <ref> [8] </ref>, the maximum data transfer speed that can be sustained with 44 byte transfers over the TURBOchannel on a DECstation 5000/200 is 367 Mbps in the transmit direction and 463 Mbps in the receive direction.
Reference: [9] <author> P. Druschel, M. B. Abbott, M. Pagels, and L. L. Pe-terson. </author> <title> Network subsystem design. </title> <journal> IEEE Network (Special Issue on End-System Support for High Speed Networks), </journal> <volume> 7(4) </volume> <pages> 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The other relevant piece of software, of course, is the OS running on the host. In our case, it is the Mach 3.0 operating system [1], retrofitted with a network subsystem based on the x-kernel <ref> [9, 12] </ref>. For the purpose of this paper, there are two relevant things to note about the OS. First, because the x-kernel supports arbitrary protocols, our approach is protocol-independent; it is not tailored to TCP/IP. <p> In traditional operating systems, where network data is copied between application memory and kernel buffers, this can be achieved by statically allocating contiguous physical pages to the fixed set of kernel buffers. Unfortunately, this approach does not readily generalize to a copy-free data path <ref> [9] </ref>, since applications generally cannot be allowed to hold buffers from a statically allocated pool. We are currently experimenting with OS support for dynamic allocation of contiguous physical pages on a best-effort basis.
Reference: [10] <author> P. Druschel and L. L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Since fbufs are described in detail elsewhere <ref> [10] </ref>, this section concentrates on the OSIRIS features that we were able to exploit. The effectiveness of fbufs depends on the ability of the adaptor to make an early demultiplexing decision. <p> This is significant, since it implies that there is no penalty for crossing the protection domain boundary between OS kernel and unprivileged user processes. The effectiveness of fbufs, independent of ADCs, is reported elsewhere <ref> [10] </ref>.
Reference: [11] <author> D. C. Feldmeier. </author> <title> Multiplexing issues in communication system design. </title> <booktitle> In Proc. ACM SIGCOMM '90, </booktitle> <pages> pages 209-219, </pages> <address> Philadelphia, PA, </address> <month> Spetember </month> <year> 1990. </year>
Reference-contexts: Early demultiplexing has advantages beyond that of enabling efficient delivery of data to applications. It is also the basis for the appropriate processing of prioritized network traffic under high receiver load <ref> [11] </ref>. The threads that de-queue buffers from the various receive queues may be assigned priorities corresponding to the traffic priorities of the network stream they handle.
Reference: [12] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The other relevant piece of software, of course, is the OS running on the host. In our case, it is the Mach 3.0 operating system [1], retrofitted with a network subsystem based on the x-kernel <ref> [9, 12] </ref>. For the purpose of this paper, there are two relevant things to note about the OS. First, because the x-kernel supports arbitrary protocols, our approach is protocol-independent; it is not tailored to TCP/IP.
Reference: [13] <author> V. Jacobson. </author> <title> Efficient protocol implementation. </title> <booktitle> ACM SIGCOMM '90 tutorial, </booktitle> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In the PIO case, with carefully designed software, data can be read from the adaptor and written directly to the application's buffer in main memory, leaving the data in the cache <ref> [13, 6] </ref>. If the application reads the data soon after the PIO transfer, the data may still be in the cache.
Reference: [14] <author> C. Maeda and B. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Linked with the application is an ADC channel driver, which performs essentially the same functions as the in-kernel OSIRIS device driver. Also linked with the application is a replicated implementation of the network protocol stack. The technology of application-linked network protocols has been demonstrated elsewhere in the literature <ref> [19, 14] </ref>, and is also supported by the x-kernel. The operating system assigns a set of VCIs, a priority, and a list of physical pages to the ADC.
Reference: [15] <author> M. Pagels, P. Druschel, and L. L. Peterson. </author> <title> Cache and TLB effectiveness in the processing of network data. </title> <type> Technical Report 93-4, </type> <institution> Department of Computer Science, University of Arizona, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: According to one study, the PIO transfer from adaptor to application buffer must be delayed until the application is scheduled for execution, in order to ensure sufficient proximity of data accesses for the data to remain cached under realistic system load conditions <ref> [15] </ref>. Loading data into the cache too early is not only ineffective, but can actually decrease overall system performance by evicting live data from the cache.
Reference: [16] <author> K. K. Ramakrishnan. </author> <title> Performance considerations in designing network interfaces. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 203-219, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors <ref> [5, 2, 3, 16, 20] </ref>. This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed [4, 8]. <p> We consider the network adaptor from a software perspective, identifying the subtle interactions between the adaptor and the operating system software that drives it. Others have looked at this hardware/software interaction as well <ref> [18, 6, 16] </ref>. In our case, fl This work supported in part by National Science Foundation Grant CCR-9102040 and ARPA Contract DABT63-91-C-0030. the flexibility built into the OSIRIS board makes this interaction an especially interesting one to study. The OSIRIS network adaptor was designed specifically to support software experimentation. <p> Both the literature on the subject (e.g. <ref> [16, 2, 6] </ref>) and our own experience have led us to the conclusion that the preferable technique is highly machine-dependent.
Reference: [17] <author> F. Reynolds and J. Heller. </author> <title> Kernel support for network protocol servers. </title> <booktitle> In Proceedings of the USENIX Mach Symposium, </booktitle> <pages> pages 149-162, </pages> <address> Monterey, Calif., </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: At first glance, ADCs may appear similar to the mapped device drivers used in Mach <ref> [17] </ref> and other microkernel-based systems. In these systems, the user-level UNIX server is granted direct access to, and control of, the network device. However, application device channels are different from mapped device drivers in two important ways.
Reference: [18] <author> J. M. Smith and C. B. S. Traw. </author> <title> Giving applications access to Gb/s networking. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 44-52, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: We consider the network adaptor from a software perspective, identifying the subtle interactions between the adaptor and the operating system software that drives it. Others have looked at this hardware/software interaction as well <ref> [18, 6, 16] </ref>. In our case, fl This work supported in part by National Science Foundation Grant CCR-9102040 and ARPA Contract DABT63-91-C-0030. the flexibility built into the OSIRIS board makes this interaction an especially interesting one to study. The OSIRIS network adaptor was designed specifically to support software experimentation.
Reference: [19] <author> C. Thekkath, T. Nguyen, E. Moy, and E. Lazowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In Proceedings of the SIGCOMM '93 Symposium, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Linked with the application is an ADC channel driver, which performs essentially the same functions as the in-kernel OSIRIS device driver. Also linked with the application is a replicated implementation of the network protocol stack. The technology of application-linked network protocols has been demonstrated elsewhere in the literature <ref> [19, 14] </ref>, and is also supported by the x-kernel. The operating system assigns a set of VCIs, a priority, and a list of physical pages to the ADC.
Reference: [20] <author> C. B. S. Traw and J. M. Smith. </author> <title> Hardware/software organization of a high-performance atm host interface. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 240-253, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction With the emergence of high-speed network facilities, several research efforts are focusing on the design and implementation of network adaptors <ref> [5, 2, 3, 16, 20] </ref>. This paper takes the next step in the evolution of adaptors for high-speed networks by reporting our experiences with one particular adaptorthe OSIRIS ATM board built for the AURORA Gigabit Testbed [4, 8].
References-found: 20

