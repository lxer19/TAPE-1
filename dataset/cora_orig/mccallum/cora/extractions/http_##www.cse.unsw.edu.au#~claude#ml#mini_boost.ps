URL: http://www.cse.unsw.edu.au/~claude/ml/mini_boost.ps
Refering-URL: http://www.cse.unsw.edu.au/~claude/
Root-URL: 
Email: quinlan@cse.unsw.edu.au  
Title: MiniBoosting Decision Trees  
Author: J. R. Quinlan 
Date: 2052  
Address: Sydney Australia  
Affiliation: School of Computer Science and Engineering University of New South Wales  
Note: Journal of Artificial Intelligence Research X (1998) XX-XX Submitted XX/98; published XX/98  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: 1. Introduction Learning to predict the categories to which objects belong by analysing a collection of training cases is one of the most studied areas of Machine Learning. Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches <ref> (Aha, Kibler, & Albert, 1991) </ref>, neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1991), naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets (Pearl, 1988).
Reference: <author> Bauer, E., & Kohavi, R. </author> <year> (1998). </year> <title> An empirical comparison of voting classification algorithms: bagging, boosting, and variants. </title> <journal> Machine Learning, </journal> <volume> 0, </volume> <month> XX-XX. </month>
Reference-contexts: The number of leaves of the decision tree (or, equivalently, the number of separate regions) is a useful measure of the complexity of the classifier. 4. MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, <ref> (Bauer & Kohavi, 1998) </ref> uses 25 classifiers, (Freund & Schapire, 1996b) and (Breiman, 1996b) use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials.
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <note> Bagging predictors. Machine Learning, (to appear). </note>
Reference-contexts: The sample is enlarged by including some training cases on which the classifier performs poorly, a new classifier is generated from the enlarged sample, and the process repeats. * Bagging <ref> (Breiman, 1996a) </ref>: Repeated bootstrap samples are drawn with replacement from the training cases, and a classifier constructed from each. * Boosting (Freund & Schapire, 1996a): Each training case has an associated weight. <p> Experiments and Results The effectiveness of MiniBoost has been evaluated using the same datasets and experimental procedure as a previous study (Quinlan, 1996a) comparing AdaBoost with bagging <ref> (Breiman, 1996a) </ref>. Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable.
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <journal> Machine Learning, </journal> <volume> 0, </volume> <pages> 00-00. </pages>
Reference-contexts: MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, (Freund & Schapire, 1996b) and <ref> (Breiman, 1996b) </ref> use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> Xxxx. </title> <journal> AI Magazine, </journal> <volume> 0, </volume> <month> XXX-XXX. </month>
Reference: <author> Domingos, P., & Pazzani, M. J. </author> <year> (1996). </year> <title> Beyond independence: conditions for the optimality of the simple bayesian classifier. </title> <booktitle> In Proceedings Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 105-112. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1991), naive Bayesian learners <ref> (Domingos & Pazzani, 1996) </ref>, and inference nets (Pearl, 1988). The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1998).
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996a). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Unpublished manuscript, </type> <note> available from the authors' home pages ("http://www.research.att.com/orgs/ssr/people/fyoav,schapireg"). </note>
Reference-contexts: sample is enlarged by including some training cases on which the classifier performs poorly, a new classifier is generated from the enlarged sample, and the process repeats. * Bagging (Breiman, 1996a): Repeated bootstrap samples are drawn with replacement from the training cases, and a classifier constructed from each. * Boosting <ref> (Freund & Schapire, 1996a) </ref>: Each training case has an associated weight. At every iteration a classifier is built from the weighted training cases and each case is then reweighted according to whether or not it is misclassified. <p> Boosting has so far proved to be the most effective of these because it focuses attention on the difficult cases using a multiplicative reweighting strategy derived from Winnow (Littlestone & Warmuth, 1994). This description of boosting follows Freund and Schapire's AdaBoost.M1 <ref> (Freund & Schapire, 1996a) </ref>. We assume a set of training cases i = 1,2,...,N . At each repetition or trial t = 1,2,...,T , case i has weight w t [i], where w 1 [i] = 1/N for all i.
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996b). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 148-156. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The number of leaves of the decision tree (or, equivalently, the number of separate regions) is a useful measure of the complexity of the classifier. 4. MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, <ref> (Freund & Schapire, 1996b) </ref> and (Breiman, 1996b) use 100 classifiers, and (Schapire, Freund, Bartlett, & Lee, 1998) extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials.
Reference: <author> Littlestone, N., & Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108, </volume> <pages> 212-261. </pages>
Reference-contexts: Boosting has so far proved to be the most effective of these because it focuses attention on the difficult cases using a multiplicative reweighting strategy derived from Winnow <ref> (Littlestone & Warmuth, 1994) </ref>. This description of boosting follows Freund and Schapire's AdaBoost.M1 (Freund & Schapire, 1996a). We assume a set of training cases i = 1,2,...,N .
Reference: <author> McClelland, J. L., & Rumelhart, D. E. </author> <year> (1988). </year> <title> Experiments in Parallel Distributed Pro cessing. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists (Rivest, 1991), naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets <ref> (Pearl, 1988) </ref>. The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1998).
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The new descriptions are then analysed to yield further classifiers, and so on. * Windowing <ref> (Quinlan, 1993) </ref>: A classifier is formed from an initial sample of the train ing cases. <p> Learning methods sidestep this difficulty in various ways. If an attribute appearing in a decision node has an unknown value, C4.5 follows all branches from that node, yielding a posterior probability distribution, then selects the most probable class <ref> (Quinlan, 1993) </ref>. In situations like this, each of the trees C 1 , C 2 , and C 3 will collapse a distribution to a selected class, and the categorical results are then voted.
Reference: <author> Quinlan, J. R. </author> <year> (1996a). </year> <title> Bagging, boosting, </title> <booktitle> and c4.5. In Proceedings Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 725-730. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: The procedure incorporating these small changes and using uniform voting weights will be referred to as MiniBoost to distinguish it from three-trial AdaBoost. 4 MiniBoosting Decision Trees 5. Experiments and Results The effectiveness of MiniBoost has been evaluated using the same datasets and experimental procedure as a previous study <ref> (Quinlan, 1996a) </ref> comparing AdaBoost with bagging (Breiman, 1996a). Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable.
Reference: <author> Quinlan, J. R. </author> <year> (1996b). </year> <title> Improved use of continuous attributes in c4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 0, </volume> <month> XX-XX. </month>
Reference-contexts: Twenty-seven classification tasks, whose characteristics are summarized in the Appendix, cover a broad spectrum of properties such as size, numbers and types of attributes, numbers of classes, and accuracy achievable. Ten repetitions of ten-fold cross-validation were carried out with each dataset and C4.5 Release 8 <ref> (Quinlan, 1996b) </ref> was used as the underlying decision tree generator. A summary of the results on unseen test cases appears in Table 1.
Reference: <author> Rivest, R. </author> <year> (1991). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <month> xx-XX. </month>
Reference-contexts: Well-tried methods for constructing a classifier or function that maps from object descriptions to class names include instance-based approaches (Aha, Kibler, & Albert, 1991), neural networks (Mc-Clelland & Rumelhart, 1988), decision trees (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1993), decision lists <ref> (Rivest, 1991) </ref>, naive Bayesian learners (Domingos & Pazzani, 1996), and inference nets (Pearl, 1988). The "standard" application of one of these methods results in a single classifier, but there has recently been considerable interest in ensembles of classifiers (Bauer & Kohavi, 1998; Dietterich, 1998).
Reference: <author> Schapire, R. E., Freund, Y., Bartlett, P., & Lee, W. S. </author> <year> (1998). </year> <title> Boosting the margin: a new explanation for the effectiveness of voting methods. </title> <journal> XXXXX, </journal> <note> 0, </note> <author> XXX-XXX. 13 Quinlan Wallace, C. S., & Patrick, J. D. </author> <year> (1993). </year> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 7-22. </pages>
Reference-contexts: MiniBoosting Most studies of boosting use a substantial ensemble of classifiers. For example, (Bauer & Kohavi, 1998) uses 25 classifiers, (Freund & Schapire, 1996b) and (Breiman, 1996b) use 100 classifiers, and <ref> (Schapire, Freund, Bartlett, & Lee, 1998) </ref> extends this to 1000. In contrast to this assumption, the intention here is to explore boosting with a minimal number of trials. <p> Therefore, offered a choice between them, MML would always choose C 0 1:2:3 . This is unfortunate because the predictive accuracy of C 1:2:3 is almost invariably superior. * <ref> (Schapire et al., 1998) </ref> ascribes the power of boosting and other voting methods to improving the distribution of margin for the training cases. Roughly speaking, the margin of a case is determined by the proportion of votes amassed by the correct class.
Reference: <author> Webb, G. I. </author> <year> (1996). </year> <title> Further experimental evidence against the utility of occam's razor. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 397-417. </pages>
Reference-contexts: The class label assigned to such regions is decided by assembling evidence from enclosing regions (i.e., by voting the enclosing regions from C 1 , C 2 , and C 3 ). This is similar in spirit if not in detail to Webb's procedure for labeling empty regions <ref> (Webb, 1996) </ref>. * The empty regions are important! If they are eliminated by removing unpopulated leaves, the predictive performance of the reduced tree is much impaired. 7. Conclusion For decision trees at least, MiniBoosting has been found to be a practical method for increasing classification accuracy at an affordable cost.
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages>
Reference-contexts: Multiple classifiers have been shown to lead to improved predictive accuracy when classifying objects that are not among the training cases. Once again, there is considerable diversity in the methods used to assemble the ensembles, including: * Stacking <ref> (Wolpert, 1992) </ref>: The descriptions of the training cases are extended to include the results of classifying the cases with an initial selection of classifiers.
References-found: 19

