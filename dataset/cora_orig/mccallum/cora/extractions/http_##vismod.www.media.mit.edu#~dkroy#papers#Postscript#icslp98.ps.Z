URL: http://vismod.www.media.mit.edu/~dkroy/papers/Postscript/icslp98.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/~dkroy/toco.html
Root-URL: http://www.media.mit.edu
Email: fdkroy, sandyg@media.mit.edu  
Title: LEARNING WORDS FROM NATURAL AUDIO-VISUAL INPUT  
Author: Deb Roy Alex Pentland 
Web: http://www.media.mit.edu/ dkroy/toco.html  
Address: Ames Street, Rm. E15-388, Cambridge, MA 01239, USA  
Affiliation: 20  
Abstract: We present a model of early word learning which learns from natural audio and visual input. The model has been successfully implemented to learn words and their audio-visual grounding from camera and microphone input. Although simple in its current form, this model is a first step towards a more complete, fully-grounded model of language acquisition. Practical applications include adaptive human-machine interfaces for information browsing, assistive technologies, education, and entertainment. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Feldman, G. Lakoff, D. Bailey, S. Narayanan, T. Regier, and A. Stolcke. Lzero: </author> <title> The first five years. </title> <journal> Artificial Intelligence Review, </journal> <volume> 10 </volume> <pages> 103-129, </pages> <year> 1996. </year>
Reference-contexts: Similarly shape terms (ex. "car", "ball", "bottle") form a separate word class. Many interesting computational models of learning word semantics have been proposed including [3], <ref> [1] </ref>, [10]. Each of these models relies on either human-generated text, phoneme transcripts, or assumptions about pre-existing discrete semantic classes. In contrast, our model learns surface-form and semantic models of words from only audio and visual sensory input.
Reference: 2. <author> G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais. </author> <title> The vocabulary problem in human-system communications. </title> <journal> Communications of the Association for Compuring Machinery, </journal> <volume> 30 </volume> <pages> 964-972, </pages> <year> 1987. </year>
Reference-contexts: The word learning model has applications in the design of human-machine interfaces that use spoken language. A significant problem in designing effective speech interfaces is the difficulty in anticipating a person's word choice and associated intent <ref> [2] </ref>. Our system addresses this problem by learning the vocabulary of each user together with its visual grounding. This approach enables a new type of human-machine interface which can adapt to the preferences and abilities of individual users over time. 2.
Reference: 3. <author> Allen L. Gorin. </author> <title> On automated language acquisition. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 97(6) </volume> <pages> 3441-3461, </pages> <year> 1995. </year>
Reference-contexts: Similarly shape terms (ex. "car", "ball", "bottle") form a separate word class. Many interesting computational models of learning word semantics have been proposed including <ref> [3] </ref>, [1], [10]. Each of these models relies on either human-generated text, phoneme transcripts, or assumptions about pre-existing discrete semantic classes. In contrast, our model learns surface-form and semantic models of words from only audio and visual sensory input. <p> Pairs of segments which have a symmetric log likelihood below a predefined threshold are considered an acoustic match. For the next stage of processing, we need to define the salience of each speech segment similar to <ref> [3] </ref>.
Reference: 4. <author> Richard Rose. </author> <title> Word Spotting from Continuous Speech Utterances, </title> <booktitle> chapter 13, </booktitle> <pages> pages 303-329. </pages> <publisher> Kluwer Academic, </publisher> <year> 1996. </year>
Reference-contexts: Similar to many word spotting algorithms (for example see <ref> [4] </ref>) we normalize this score by an all-phone garbage model. 3.2. Visual Processing Objects are segmented from a controlled background using a statistical background color model. The resulting object mask is used to compute a set of local shape features based on spatial relations between derivatives.
Reference: 5. <author> Deb Roy, Michal Hlavac, Marina Umaschi, Tony Je-bara, Justine Cassell, and Alex Pentland. </author> <title> Toco the toucan: A synthetic character guided by perception, emotion, and story. </title> <booktitle> In Visual Proceedings of Siggraph, </booktitle> <address> Los Angeles, CA, </address> <month> August </month> <year> 1997. </year> <note> ACM Siggraph. </note>
Reference-contexts: We believe that further exploration of this theme may lead to powerful new algorithms for syntax acquisition. This work has practical applications for adaptive human-machine interfaces in numerous domains including information browsing (for example browsing and searching catalogs), command-and-control, entertainment <ref> [5] </ref>, and disability aids [6].
Reference: 6. <author> Deb Roy and Rupal Patel. </author> <title> Adaptive user interfaces for individuals with speech and physical impairments. </title> <type> Technical Report (upcoming), </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1998. </year>
Reference-contexts: We believe that further exploration of this theme may lead to powerful new algorithms for syntax acquisition. This work has practical applications for adaptive human-machine interfaces in numerous domains including information browsing (for example browsing and searching catalogs), command-and-control, entertainment [5], and disability aids <ref> [6] </ref>.
Reference: 7. <author> Deb Roy and Alex Pentland. </author> <title> Multimodal adaptive interfaces. </title> <type> Technical Report 438, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1997. </year>
Reference-contexts: The sequence of phoneme probability estimates associated with the utterance is referred to as the phoneme trace. The HMM and the associated phoneme trace forms our underlying representation of a spoken utterance. For more details on the speech processing and segmentation algorithms see <ref> [7] </ref> and [8]. Speech segments are compared by using forced viterbi alignment as follows. Consider two speech segments A and B. For each segment we will have a phoneme trace and corresponding HMM.
Reference: 8. <author> Deb Roy and Alex Pentland. </author> <title> Word learning in a multimodal environment. </title> <booktitle> In Proceedings of ICASSP, </booktitle> <address> Seattle, Washington, May 1998. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The sequence of phoneme probability estimates associated with the utterance is referred to as the phoneme trace. The HMM and the associated phoneme trace forms our underlying representation of a spoken utterance. For more details on the speech processing and segmentation algorithms see [7] and <ref> [8] </ref>. Speech segments are compared by using forced viterbi alignment as follows. Consider two speech segments A and B. For each segment we will have a phoneme trace and corresponding HMM.
Reference: 9. <author> B. Schiele and J.L. Crowley. </author> <title> Object recognition using multidimensional receptive field histograms. </title> <booktitle> In ECCV96-I, </booktitle> <pages> pages 610-619, </pages> <year> 1996. </year>
Reference-contexts: Color features are computed by sampling pixel values within the object mask region. The shape and color features are accumulated in separate histograms which can later be compared using 2 divergence. For further details on the visual processing algorithms see <ref> [9] </ref>. 3.3. CREATING AND USING AN AUDIO-VISUAL LEXICON Several segmentation and clustering algorithms are used in the learning processes described in Section 2.1. Initial visual clustering is performed by computing pairwise distances between each pair of AV-events using the 2 divergence on shape and color histograms.
Reference: 10. <author> Jeffrey Siskind. </author> <title> A computational study of cross-situational techniques for learning word-to-meaning mapping. </title> <journal> Cognition, </journal> <volume> 61(1-2):39-91, </volume> <year> 1996. </year>
Reference-contexts: Similarly shape terms (ex. "car", "ball", "bottle") form a separate word class. Many interesting computational models of learning word semantics have been proposed including [3], [1], <ref> [10] </ref>. Each of these models relies on either human-generated text, phoneme transcripts, or assumptions about pre-existing discrete semantic classes. In contrast, our model learns surface-form and semantic models of words from only audio and visual sensory input.
References-found: 10

