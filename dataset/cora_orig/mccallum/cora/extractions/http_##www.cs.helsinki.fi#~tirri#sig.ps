URL: http://www.cs.helsinki.fi/~tirri/sig.ps
Refering-URL: http://www.cs.Helsinki.FI/research/cosco/Projects/NONE/
Root-URL: 
Phone: 26,  
Title: Using Neural Networks for Descriptive Statistical Analysis of Educational Data  
Author: Henry Tirri and Tomi Silander 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland Kirsi Tirri P.O. Box 38,Department  Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  University of  
Date: March 1997)  
Note: A paper presented at the 1997 Annual Meeting of the American Educational Research Association (Chicago, IL, USA,  of Teacher Education FIN-00014  
Abstract: In this paper we discuss the methodological issues of using a class of neural networks called Mixture Density Networks (MDN) for discriminant analysis. MDN models have the advantage of having a rigorous probabilistic interpretation, and they have proven to be a viable alternative as a classification procedure in discrete domains. We will address both the classification and interpretive aspects of discriminant analysis, and compare the approach to the traditional method of linear discrimin- ants as implemented in standard statistical packages. We show that the MDN approach adopted performs well in both aspects. Many of the observations made are not restricted to the particular case at hand, and are applicable to most applications of discriminant analysis in educational research. fl URL: http://www.cs.Helsinki.FI/research/cosco/ 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baestaens, D., Van den Bergh, W., and Wood, D. </author> <year> (1994). </year> <title> Neural Network Solutions for Trading in Financial Markets. </title> <publisher> Financial Times / Pitman Publishing. </publisher>
Reference-contexts: Such networks have been successfully used in various fields for nonlinear modeling and approximation, for example in speech recognition (Kohonen, 1995), expert systems (Gallant, 1993), and machine vision (Hinton and Sejnowski, 1983). More recently they have also been applied for data analysis in various financial domains <ref> (Baestaens et al., 1994) </ref>. The increasing importance of neural networks as nonlinear models is witnessed by the fact that currently many of the standard statistical software packages include feed-forward neural network modeling in their tool box.
Reference: <author> Bernardo, J. and Smith, A. </author> <year> (1994). </year> <title> Bayesian theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: Since our estimation of the network parameters will be Bayesian <ref> (Bernardo and Smith, 1994) </ref> we need to fix the prior distributions for the parameters. <p> Now for each possible value x mi , x mi 2 fx m1 ; : : : ; x mn m g we wish to compute the probabilities p (X m = x mi j (x 1 ; : : : ; x m1 ); D): From the Bayes' theorem <ref> (Bernardo and Smith, 1994) </ref> we know that p (X m = x mi j (x 1 ; : : : ; x m1 ); D) p ( ~ d [x mi ]jD) k=1 p ( ~ d [x mk ]jD) where ~ d [x mi ] denotes the vector (X 1
Reference: <author> Bezdek, J. </author> <year> (1994). </year> <title> What is computational intelligence? In Zurada, </title> <editor> J., II, R. M., and Robinson, C., editors, </editor> <booktitle> Computational Intelligence Imitating Life. </booktitle> <publisher> IEEE Press. </publisher>
Reference-contexts: Rather we would like to discuss methodological issues for both constructing the classifiers and for evaluating their quality, if one moves from the linear discriminant framework to neural network approaches. Many of the concerns raised are well-known in the computational intelligence community <ref> (Bezdek, 1994) </ref>, but seem to be very seldom discussed in the educational quantitative methodology literature. Finally we will address the interpretive side of the discriminant analysis.
Reference: <author> Bishop, C. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: Thus the MDN models are actually a combined neural network structure and a density model (for more details see the discussion in <ref> (Bishop, 1995) </ref>). Provided we consider a sufficiently flexible network, and a sufficiently general density model, we have a framework for approximating arbitrary conditional distributions. 4 Typical choices for a parametric model are a single Gaussian or a linear combination of fixed set of kernel functions.
Reference: <author> Bishop, C. M. </author> <year> (1994). </year> <title> Mixture density networks. </title> <type> Technical Report NCGR/4288, </type> <institution> Neural Computing Research Group, Department of Computer Science, Aston University. </institution> <note> 17 Cheng, </note> <author> B. and Titterington, D. </author> <year> (1994). </year> <title> Neural networks: a review from a statistical perspective (with discussion). </title> <journal> Statist. Sci., </journal> <volume> 9 </volume> <pages> 2-54. </pages>
Reference-contexts: Since many neural network models assume continuous input variables (outputs), educational data sets, such as questionnaire data, have not been modeled due to their discrete nature. In this paper we focus on the use of a particular class of neural networks called the Mixture Density Networks <ref> (Bishop, 1994) </ref> in the analysis of educational data. This intuitively very appealing neural network model family is introduced in Section 2, and can be understood as an implementation of a particular subclass of finite mixture models (Titterington et al., 1985). <p> Readers not interested in the technical details of the MDN network models can browse Sections 2 and 3, and go directly to the description of the problem domains (Section 4), from which the data samples for the experiments were taken. 2 Mixture Density Networks Mixture Density Networks <ref> (Bishop, 1994) </ref> is a neural network class which can be used to represent general conditional probability densities p ( ~ tj ~ d) by considering a (semi)parametric model for the distribution of ~ t, whose parameters are determined by the outputs of a feed-forward neural network, which takes ~ d as
Reference: <author> Cover, T. and Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY. </address>
Reference-contexts: MDN approach the corresponding notion would be the Kullback-Leibler distance of the unconditional and conditional marginal likelihood of X i , i.e., D KL (p (X i jX m = k; fi); p (X i jfi)); where where D KL (p; q) is the relative entropy between p and q <ref> (Cover and Thomas, 1991) </ref>.
Reference: <author> Everitt, B. and Hand, D. </author> <year> (1981). </year> <title> Finite Mixture Distributions. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <editor> Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors (1996). </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The increasing importance of neural networks as nonlinear models is witnessed by the fact that currently many of the standard statistical software packages include feed-forward neural network modeling in their tool box. Similarly the recent multidisciplinary research efforts in the field of "Knowledge Discovery in Databases" <ref> (Fayyad et al., 1996) </ref> use quite frequently neural network techniques. Neural network models are composed of a large number of individual computational elements called nodes, which are linked together to form a structure (called the architecture).
Reference: <author> Gallant, S., </author> <title> editor (1993). Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: 1 Introduction Artificial neural networks (Haykin, 1994) can be viewed as a family of nonlinear models used for empirical regression and classification modeling. Such networks have been successfully used in various fields for nonlinear modeling and approximation, for example in speech recognition (Kohonen, 1995), expert systems <ref> (Gallant, 1993) </ref>, and machine vision (Hinton and Sejnowski, 1983). More recently they have also been applied for data analysis in various financial domains (Baestaens et al., 1994).
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> IEEE Press/Macmillan College Publishing Company, </publisher> <address> New York. </address>
Reference-contexts: 1 Introduction Artificial neural networks <ref> (Haykin, 1994) </ref> can be viewed as a family of nonlinear models used for empirical regression and classification modeling. Such networks have been successfully used in various fields for nonlinear modeling and approximation, for example in speech recognition (Kohonen, 1995), expert systems (Gallant, 1993), and machine vision (Hinton and Sejnowski, 1983). <p> The more parameters the model used in the discriminant analysis has, the more severe this overfitting phenomenon is. With the exception of the simple model class of perceptrons <ref> (Haykin, 1994) </ref>, all neural network model families are highly parameterized nonlinear function estimators, and would perform extremely poorly, if the models were selected based on their training sample performance.
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243. </pages>
Reference: <author> Hinton, G. </author> <year> (1989). </year> <title> Connectionist learning procedures. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 40(1-3). </pages>
Reference: <author> Hinton, G. </author> <year> (1992). </year> <title> How neural networks learn from experience. </title> <publisher> Scientific American, </publisher> <pages> 267(104-109). </pages>
Reference: <author> Hinton, G. and Sejnowski, T. </author> <year> (1983). </year> <title> Optimal perceptual inference. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 448-453, </pages> <address> Washington DC. </address> <publisher> IEEE, </publisher> <address> New York, NY. </address>
Reference-contexts: Such networks have been successfully used in various fields for nonlinear modeling and approximation, for example in speech recognition (Kohonen, 1995), expert systems (Gallant, 1993), and machine vision <ref> (Hinton and Sejnowski, 1983) </ref>. More recently they have also been applied for data analysis in various financial domains (Baestaens et al., 1994).
Reference: <author> Jordan, M. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference: <author> Klecka, W. </author> <year> (1981). </year> <title> Discriminant analysis. </title> <publisher> Sage Publications, </publisher> <address> Beverly Hills, CA. </address>
Reference-contexts: This intuitively very appealing neural network model family is introduced in Section 2, and can be understood as an implementation of a particular subclass of finite mixture models (Titterington et al., 1985). Klecka <ref> (Klecka, 1981) </ref> defines discriminant analysis to be a set of statistical techniques to study the differences between two or more groups of objects with respect to several variables simultaneously. <p> analysis is performed at the primary variable level, all discussion is naturally valid for discriminant analysis with factor scores also. 5.1 Testing with sample vs. cross validation The traditional classification procedures in linear discriminant analysis typically use either the discriminating variables or the canonical discriminant functions constructed from the data <ref> (Klecka, 1981) </ref>. We assume that the reader is familiar with the standard approach as implemented in the SPSS statistical software package (Norusis, 1990), and do not repeat the principles here. <p> Although many textbooks include a warning about the fact that testing a model with the same data sample from which it is constructed (see e.g., <ref> (Klecka, 1981) </ref>, pp. 51-52) gives inflated estimates of the classification performance, this seems to be the standard practice, unless the size of the data sample is large and sometimes independent samples are used.
Reference: <author> Kohonen, T. </author> <year> (1995). </year> <title> Self-Organizing Maps. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <note> 18 Kontkanen, </note> <author> P., Myllymaki, P., Silander, T., Tirri, H., and Grunwald, P. </author> <year> (1997). </year> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida. </address>
Reference-contexts: 1 Introduction Artificial neural networks (Haykin, 1994) can be viewed as a family of nonlinear models used for empirical regression and classification modeling. Such networks have been successfully used in various fields for nonlinear modeling and approximation, for example in speech recognition <ref> (Kohonen, 1995) </ref>, expert systems (Gallant, 1993), and machine vision (Hinton and Sejnowski, 1983). More recently they have also been applied for data analysis in various financial domains (Baestaens et al., 1994).
Reference: <author> Mackay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation net-works. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992b). </year> <title> The evidence framework applied to classification networks. </title> <journal> Neural Computation, </journal> <volume> 4(5) </volume> <pages> 698-714. </pages>
Reference: <author> McLachlan, G., </author> <title> editor (1992). Discriminant Analysis and Statistical Pattern Recognition. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: In Section 5 we will discuss the use of Mixture Density Networks for the classification problem formulated in Section 3. Instead of just presenting classification accuracy information, we want to put the results in perspective, and compare them to the ones achieved by the traditional linear discriminant analysis <ref> (McLachlan, 1992) </ref>. We would like to point out that the purpose here is 3 not to demonstrate the superiority of the Mixture Density Network approach in classification accuracy (although, due the power of the underlying mixture model language, this in many cases is the case).
Reference: <author> Niemi, H. and Tirri, K. </author> <year> (1996). </year> <title> Effectiveness of teacher education. new chal-lenges and approaches to evaluation. </title> <type> Technical Report A 6/1996, </type> <institution> Department of Teacher Education in Tampere University. </institution>
Reference-contexts: A more detailed description of the framework and research conducted in the project is discussed in <ref> (Niemi and Tirri, 1996) </ref>. The data adopted to this study was gathered to investigate how well teacher education had been able to achieve the goals set to it. These goals were selected from school-law, programs of teacher education and other documents describing teachers' work at school.
Reference: <author> Niemi, H. and Tirri, K. </author> <year> (1997). </year> <title> Readiness for teaching profession evaluated by teachers and teacher educators. </title> <publisher> In Press. </publisher> <month> Norusis </month> <year> (1990). </year> <title> SPSS Advanced Statistics User's Guide. </title> <publisher> SPSS Inc, Chigago. </publisher>
Reference-contexts: The evaluation instrument consisted of 41 behavior statements (and information about the teacher education department), and used a Likert scale from 1 to 5 for the assertions. The results of this evaluation study are reported in the forthcoming study <ref> (Niemi and Tirri, 1997) </ref>. The data sample used for our comparison is derived from the teachers' data in the study described above. This data consists of ratings of 204 Finnish teachers.
Reference: <author> Ripley, B. </author> <year> (1993). </year> <title> Statistical aspects of neural networks. </title> <editor> In O.E. BarndorffNielsen, J. J. a. W. K., editor, </editor> <booktitle> Networks and Chaos Statistical and Probabilistic Aspects, </booktitle> <pages> pages 40-123. </pages> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Ripley, B. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predic-tions. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: In particular, use of k-fold cross validation <ref> (Stone, 1974) </ref>, sometimes known as the "jackknife", tends to be very rare. This is quite concerning, as it is well known that most parameter learning procedures have a tendency to overfit, i.e., form classification functions that are more accurate for the sample than they would be for the full population.
Reference: <author> Tirri, H., Kontkanen, P., and Myllymaki, P. </author> <year> (1996). </year> <title> Probabilistic instancebased learning. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers. 19 Titterington, </publisher> <editor> D., Smith, A., and Makov, U. </editor> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address> <month> 20 </month>
Reference-contexts: A more detailed description of the framework and research conducted in the project is discussed in <ref> (Niemi and Tirri, 1996) </ref>. The data adopted to this study was gathered to investigate how well teacher education had been able to achieve the goals set to it. These goals were selected from school-law, programs of teacher education and other documents describing teachers' work at school.
References-found: 27

