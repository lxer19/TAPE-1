URL: http://www.iscs.nus.sg/~plong/papers/rects.ps
Refering-URL: 
Root-URL: 
Title: Approximating Hyper-Rectangles: Learning and Pseudo-random Sets  
Author: Peter Auer Philip M. Long Aravind Srinivasan 
Keyword: Key words and phrases. Rectangles, machine learning, PAC learning, derandomization, pseudorandomness, multiple-instance learning, explicit constructions, Ramsey graphs, random graphs, sample complexity, approximations of distributions.  
Abstract: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for several applied learning problems. Also, pseudorandom sets for rectangles have been actively studied recently because (i) they are a subprob-lem common to the derandomization of depth-2 (DNF) circuits and derandomizing Randomized Logspace, and (ii) they approximate the distribution of n independent multivalued random variables. We present improved upper bounds for a class of such problems of approximating high-dimensional rectangles that arise in PAC learning and pseudorandomness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. L. Abbott. </author> <title> A note on Ramsey's theorem. </title> <journal> Canad. Math. Bull., </journal> <volume> 15:910, </volume> <year> 1972. </year>
Reference-contexts: We tackle the first problem by a graph product result of <ref> [1] </ref>, and the second via *-approximations for axis-parallel rectangles. Graph products. <p> Graph products. Given undirected graphs G 1 = (V 1 ; E 1 ) and G 2 = (V 2 ; E 2 ), the following product graph G 1 fi G 2 = (V 1 fi V 2 ; H) is considered in <ref> [1] </ref>: f (u 1 ; v 1 ); (u 2 ; v 2 )g 2 H iff either (i) u 1 6= u 2 and (u 1 ; u 2 ) 2 E 1 , or (ii) u 1 = u 2 and (v 1 ; v 2 ) 2 E <p> The following fact is shown in <ref> [1] </ref>: if G 1 is an (s 1 ; t 1 ; n 1 )-graph and G 2 is an (s 2 ; t 2 ; n 2 )-graph, then G 1 fi G 2 is an (s 1 s 2 ; t 1 t 2 ; n 1 n 2
Reference: [2] <author> N. Alon. </author> <title> Explicit Ramsey graphs and orthonormal labelings. </title> <journal> The Electronic Journal of Combinatorics, </journal> <volume> 1, </volume> <year> 1994, </year> <note> R12, 8pp. </note>
Reference-contexts: Similarly, while nonconstructive progress has been made on the (off-diagonal or Ramsey-type) case of s 6= t [5], where we assume w.l.o.g. that s &lt; t, very few constructive results are known; see, e.g., <ref> [2] </ref> for a construction of (2; O (n 2=3 ); n)-graphs. (It is known that (2; fi ( p n log n); n)-graphs exist.) We present a family of improved deterministic sequential and parallel (EREW PRAM) constructions for Ramsey-type graphs here, using *-approximations for axis-aligned rectangles in f0; 1; : :
Reference: [3] <author> N. Alon, L. Babai, and A. Itai. </author> <title> A fast and simple randomized parallel algorithm for the maximal independent set problem. </title> <journal> Journal of Algorithms, </journal> <volume> 7:567583, </volume> <year> 1986. </year>
Reference: [4] <author> N. Alon, O. Goldreich, J. Hastad, and R. Peralta. </author> <title> Simple constructions of almost kwise independent random variables. Random Structures and Algorithms, </title> <address> 3(3):289303, </address> <year> 1992. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables.
Reference: [5] <author> N. Alon, J. H. Spencer, and P. Erdos. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: Similarly, while nonconstructive progress has been made on the (off-diagonal or Ramsey-type) case of s 6= t <ref> [5] </ref>, where we assume w.l.o.g. that s &lt; t, very few constructive results are known; see, e.g., [2] for a construction of (2; O (n 2=3 ); n)-graphs. (It is known that (2; fi ( p n log n); n)-graphs exist.) We present a family of improved deterministic sequential and parallel <p> There are more involved probabilistic approaches than this to show the existence of (s; t; n)- graphs, e.g., using the deletion method <ref> [5] </ref> or, even stronger, the Lovasz Local Lemma [33] or certain large-deviation inequalities [22].
Reference: [6] <author> R. Armoni, M. Saks, A. Wigderson, and S. Zhou. </author> <title> Discrepancy sets and pseudorandom generators for combinatorial rectangles. </title> <booktitle> In Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 412421, </pages> <year> 1996. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables. <p> The major open question here is to explicitly construct an indexible *-sample S such that log jSj = O (log m + log n + log (1=*)). Progress on this has been made very recently in Armoni, Saks, Wigderson & Zhou <ref> [6] </ref>, constructing an indexible S with log jSj = O (log m + log n + log 2 (1=*)). Hence, the key problem is to improve the O (log 2 (1=*)) term, to the eventual target of O (log (1=*)). <p> This would be optimal to within a constant factor, and by setting k = n, we would also get the required construction for R n We first show how to convert the construction of <ref> [6] </ref> to get an in-dexible *-approximation S for R n m;k with log jSj = O (log log n + log k+log m+log 2 (1=*)). <p> We then use some previous results with new ideas to construct an indexible *-approximation S where log jSj = O (log log n + (log m) log (1=*) To parse this, we note that this is better than the above-seen construction of <ref> [6] </ref> iff * k c 1 and * m c 2 , for certain absolute constants c 1 ; c 2 &gt; 0, i.e., if * is sufficiently small. Often, * is in fact as small as exp (fi (k)), e.g., in many applications of almost k-wise independent random bits. <p> Often, * is in fact as small as exp (fi (k)), e.g., in many applications of almost k-wise independent random bits. Thus, the main determining factor for comparing the construction of <ref> [6] </ref> with ours seems to be whether * m c 2 or not. If * is small, e.g., if m is O (polylog (1=*)), then we get good improvements. <p> Even if k is, say, O (polylog (1=*)), we would have log jS 00 j = O (log log n + log m + log (1=*) log log (1=*)), a significant improvement over the O (log log n+log m+log 2 (1=*)) that we derive above from <ref> [6] </ref>. (d) Pseudorandom sets for axis-aligned rectangles, and constructions of Ramsey-type graphs. Call an undirected graph G an (s; t; n)-graph iff it has n vertices, has clique number !(G) s, and independence number ff (G) t. <p> Recall that an indexible *-approximation S 0 for R n 0 m , with log jS 0 j = O (log m + log n 0 + log 2 (1=*)), is presented in <ref> [6] </ref>. Using this with Theorem 11 gives the following.
Reference: [7] <author> P. Auer. </author> <title> On learning from multi-instance examples: empirical evaluation of a theoretical approach. </title> <note> Submitted, </note> <year> 1997. </year>
Reference-contexts: Our algorithm is substantially different from those previously proposed for this problem [11, 25]. We believe that a variant of our algorithm will prove useful in practice. Initial empirical results <ref> [7] </ref> support this belief: a straightforward implementation of a variant of our algorithm performs competitively on datasets used in [11]. Our analysis still requires that all instances are drawn indepen dently.
Reference: [8] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, </author> <title> and M.K. War-muth. Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4):929965, </volume> <year> 1989. </year>
Reference-contexts: Our bound improves on the O n log 1 ffi bound that follows from the general results of Blumer, Ehrenfeucht, Haussler and Warmuth <ref> [8] </ref>, and on the bound of O n log 1 * that follows from the general results of Haussler, Littlestone and Warmuth [18]. <p> Then, for any positive integer s t, j Pr ( V V at most 2 s + e 1s=(2e) + P s P V 1)) Pr ( i2A (Z i = 1))j: We also need a simple proposition from [10] (see also <ref> [8] </ref>): Proposition 16 For any positive integers r; t, r t, we have P r i=0 i (te=r) r .
Reference: [9] <author> J. L. Carter and M. N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 18:143154, </volume> <year> 1979. </year>
Reference-contexts: One major goal in derandomization is to efficiently construct a discrete structure (e.g., constant-degree expanders (Lubotzky, Phillips and Sarnak [26]), dispersers and extractors (Nisan [29]), hash function families (Carter and Wegman <ref> [9] </ref>)) that is usually easily shown to exist by a probabilistic argument. Converting randomized algorithms to deterministic ones is one of the many applications of such results.
Reference: [10] <author> S. Chari, P. Rohatgi, and A. Srinivasan. </author> <title> Improved Algorithms via Approximations of Probability Distributions. </title> <booktitle> In Proc. ACM Symposium on the Theory of Computing, </booktitle> <volume> 584 592, </volume> <year> 1994. </year> <note> Full version available as Technical Report TR 10/96, </note> <institution> Dept. of Information Systems and Computer Science, National University of Singapore, </institution> <year> 1996. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables. <p> 00 j = O (log log n + log k + log m + log (1=*) We see this as particularly promising, since: (i) log jS 0 j is allowed to be quite high as a function of k, and, indeed, such a construction has been achieved for axis-aligned rectangles <ref> [13, 10] </ref>, and (ii) log jS 00 j would be optimal for the common situation of k = O (log (1=*)). <p> An explicit family fS m;k;n;* [0; m 1] n : k; m; n 2 N; k ng, where S m;k;n;* is an indexible *-approximation for BOXES k;n;m , was presented in the full version of <ref> [10] </ref> with log jS m;k;n;* j = O (log log n + log k + log (1=*) (12) This builds on some ideas from [13], and improves on all three constructions of [13]. (Though it may look surprising that the bound of (12) is independent of m, it is shown in <p> We may then invoke Lemma 14. Next, a useful lemma from the full version of <ref> [10] </ref>: Lemma 15 Let Y 1 ; : : : ; Y t be arbitrary binary r.v.s, and let Z 1 ; : : : ; Z t be independent binary r.v.s. <p> Then, for any positive integer s t, j Pr ( V V at most 2 s + e 1s=(2e) + P s P V 1)) Pr ( i2A (Z i = 1))j: We also need a simple proposition from <ref> [10] </ref> (see also [8]): Proposition 16 For any positive integers r; t, r t, we have P r i=0 i (te=r) r . <p> Thus, our goal is to show that j Pr ( i2 [k] ^ (Z i = 0))j *: (15) The proof technique now borrows largely from <ref> [10] </ref>. For each i 2 [k], let T i = f0; 1; : : : ; m 1g S i . <p> In fact, the sample space S m;k;n;* (see (12)) of <ref> [10] </ref> can be constructed in polylog (n + jS m;k;n;* j) time using poly (n; jS m;k;n;* j) processors in an EREW PRAM. Thus we have, in particular, Lemma 19 Let X 1 ; X 2 ; : : : ; X a be i.i.d. binary random variables.
Reference: [11] <author> T.G. Dietterich, R.H. Lathrop, and T. Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, 89(1-2):3171,1997. </journal>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see <ref> [36, 11] </ref>). Also, pseudorandom sets for rectangles have been actively studied recently [28, 4, 13, 24, 10, 19, 6] because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables. <p> We describe and analyze a new algorithm for a practical learning problem, motivated by drug discovery, introduced by Dietterich, Lathrop and Lozano-Perez <ref> [11] </ref>. Their problem boils down to that of learning an axis-parallel rectangle B in R n from multi-instance examples. An r-instance example consists of r elements of R n , together with a label indicating whether any of the instances of this example are in B. <p> This al gorithm can be modified slightly to achieve similar results in the statistical query model; applying the results of Kearns [20], this implies that it can be made robust against classification noise. Our algorithm is substantially different from those previously proposed for this problem <ref> [11, 25] </ref>. We believe that a variant of our algorithm will prove useful in practice. Initial empirical results [7] support this belief: a straightforward implementation of a variant of our algorithm performs competitively on datasets used in [11]. Our analysis still requires that all instances are drawn indepen dently. <p> Our algorithm is substantially different from those previously proposed for this problem [11, 25]. We believe that a variant of our algorithm will prove useful in practice. Initial empirical results [7] support this belief: a straightforward implementation of a variant of our algorithm performs competitively on datasets used in <ref> [11] </ref>. Our analysis still requires that all instances are drawn indepen dently. We point out that r-instance examples generated by an arbi-trary distribution on (R n ) r yield a much harder learning problem.
Reference: [12] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. Information and Computation, </title> <address> 82(3):247251, </address> <year> 1989. </year>
Reference-contexts: We show that the closure algorithm [18], which takes time linear in the size of the sample, PAC learns axis-aligned rectangles in R n in the origi nal (one-instance) PAC model from O ffi examples. This matches the lower bound of <ref> [12] </ref> to within a constant factor. This is the first example we know of an infinite concept class used in practice whose PAC learning sample complexity has been determined to within a constant factor.
Reference: [13] <author> G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic. </author> <title> Approximations of general independent distributions. </title> <booktitle> In Proc. ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 1016, </pages> <year> 1992. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables. <p> 00 j = O (log log n + log k + log m + log (1=*) We see this as particularly promising, since: (i) log jS 0 j is allowed to be quite high as a function of k, and, indeed, such a construction has been achieved for axis-aligned rectangles <ref> [13, 10] </ref>, and (ii) log jS 00 j would be optimal for the common situation of k = O (log (1=*)). <p> : k; m; n 2 N; k ng, where S m;k;n;* is an indexible *-approximation for BOXES k;n;m , was presented in the full version of [10] with log jS m;k;n;* j = O (log log n + log k + log (1=*) (12) This builds on some ideas from <ref> [13] </ref>, and improves on all three constructions of [13]. (Though it may look surprising that the bound of (12) is independent of m, it is shown in [13] that in the case of axis-parallel rectangles, we can effectively reduce the problem to the case where m d4k=*e and where * is <p> where S m;k;n;* is an indexible *-approximation for BOXES k;n;m , was presented in the full version of [10] with log jS m;k;n;* j = O (log log n + log k + log (1=*) (12) This builds on some ideas from <ref> [13] </ref>, and improves on all three constructions of [13]. (Though it may look surprising that the bound of (12) is independent of m, it is shown in [13] that in the case of axis-parallel rectangles, we can effectively reduce the problem to the case where m d4k=*e and where * is replaced by *=2; hence the independence from m.) <p> with log jS m;k;n;* j = O (log log n + log k + log (1=*) (12) This builds on some ideas from <ref> [13] </ref>, and improves on all three constructions of [13]. (Though it may look surprising that the bound of (12) is independent of m, it is shown in [13] that in the case of axis-parallel rectangles, we can effectively reduce the problem to the case where m d4k=*e and where * is replaced by *=2; hence the independence from m.) In the following we first show how *-approximations for R n 0 m easily lead to *-approximations for R <p> Then, it is shown in <ref> [13] </ref> that there exists a (k; *)-approximation S 2 for (X 1 ; X 2 ; : : : ; X n ), such that: (a) jS 2 j = jS 1 j, and (b) given a uniformly random sample from S 1 , a uniformly random sample from S 2
Reference: [14] <author> P. Erdos. </author> <title> Some remarks on the theory of graphs. </title> <journal> Bulletin of the American Mathematics Society, </journal> <volume> 53:292294, </volume> <year> 1947. </year>
Reference-contexts: One of the first applications of the probabilistic method was the proof by Erdos that (2 log 2 n; 2 log 2 n; n)-graphs (also known as Ramsey graphs since they provide lower bounds for the graph Ramsey function) exist <ref> [14] </ref>. It is still an outstanding open question to explicitly construct such graphs; the current-best are the breakthroughs of Frankl and Wilson, who constructed (2 O ( log n log log n) ; 2 O ( log n log log n) ; n)-graphs [16, 17].
Reference: [15] <author> W. Feller. </author> <title> An Introduction to Probability and its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: Then rewriting (11), we get E 2D ` ((er A;B;D ()) p ) R R Applying Fubini's Theorem (see <ref> [15, volume 2, page 120] </ref>) completes the proof. The proof of Lemma 8 did not use anything specific about A or OBOXES n ; therefore, the lemma can trivially be generalized to any algorithm and concept class.
Reference: [16] <author> P. Frankl. </author> <title> A constructive lower bound for Ramsey numbers. </title> <address> Ars Combinatoria, 3:297302, </address> <year> 1977. </year>
Reference-contexts: It is still an outstanding open question to explicitly construct such graphs; the current-best are the breakthroughs of Frankl and Wilson, who constructed (2 O ( log n log log n) ; 2 O ( log n log log n) ; n)-graphs <ref> [16, 17] </ref>.
Reference: [17] <author> P. Frankl and R. M. Wilson. </author> <title> Intersection theorems with geometric consequences. </title> <journal> Combinatorica, </journal> <volume> 1:357368, </volume> <year> 1981. </year>
Reference-contexts: It is still an outstanding open question to explicitly construct such graphs; the current-best are the breakthroughs of Frankl and Wilson, who constructed (2 O ( log n log log n) ; 2 O ( log n log log n) ; n)-graphs <ref> [16, 17] </ref>.
Reference: [18] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. Information and Computation, </title> <address> 115(2):129161, </address> <year> 1994. </year>
Reference-contexts: Furthermore, we show that a polynomial-time learning algorithm which outputs a rectangle as its hypothesis only exists if N P = RP. (b) Learning from single-instance examples. We show that the closure algorithm <ref> [18] </ref>, which takes time linear in the size of the sample, PAC learns axis-aligned rectangles in R n in the origi nal (one-instance) PAC model from O ffi examples. This matches the lower bound of [12] to within a constant factor. <p> Our bound improves on the O n log 1 ffi bound that follows from the general results of Blumer, Ehrenfeucht, Haussler and Warmuth [8], and on the bound of O n log 1 * that follows from the general results of Haussler, Littlestone and Warmuth <ref> [18] </ref>. In our analysis, we bound the p-norm of the error of the algorithm's hypothesis as a function of the random sample it receives, where p = ln (1=ffi). <p> A side effect of our analysis is that for all p, after m examples, this norm is at most n+p m . The bound of <ref> [18] </ref> was obtained by analyzing the expected error (i.e. the 1-norm). (c) Pseudorandom sets for combinatorial rectangles. <p> The proof of Lemma 8 did not use anything specific about A or OBOXES n ; therefore, the lemma can trivially be generalized to any algorithm and concept class. Next, we record a well-known lemma whose application is commonly known as the permutation trick. Lemma 9 (see <ref> [18] </ref>) Choose a set X, m 2 N, a distribution D on X, and a random variable ' defined on X m . Let U be the uniform distribution on the permutations of f1; :::; mg.
Reference: [19] <author> R. Impagliazzo, N. Nisan, and A. Wigderson. </author> <title> Pseudorandom-ness for network algorithms. </title> <booktitle> In Proc. ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 356364, </pages> <year> 1994. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables.
Reference: [20] <author> M.J. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392401, </pages> <year> 1993. </year>
Reference-contexts: This al gorithm can be modified slightly to achieve similar results in the statistical query model; applying the results of Kearns <ref> [20] </ref>, this implies that it can be made robust against classification noise. Our algorithm is substantially different from those previously proposed for this problem [11, 25]. We believe that a variant of our algorithm will prove useful in practice.
Reference: [21] <author> M. Kearns, M. Li, L. Pitt, and L.G. Valiant. </author> <title> On the learn-ability of Boolean formulae. </title> <booktitle> Proceedings of the 19th Annual Symposium on the Theory of Computation, </booktitle> <pages> pages 285295, </pages> <year> 1987. </year>
Reference-contexts: Thus BOXES n can be learned if OBOXES 2n can be learned. (A similar trick was employed in <ref> [21, 23] </ref>.) We therefore give an algorithm for learning OBOXES n for each n. The algorithm for learning OBOXES n learns the faces b 1 ; :::; b n of the target B = Q n k=1 (1; b k ] independently.
Reference: [22] <author> M. Krivelevich. </author> <title> Bounding Ramsey numbers through large deviation inequalities. Random Structures and Algorithms, </title> <address> 7:145155, </address> <year> 1995. </year>
Reference-contexts: There are more involved probabilistic approaches than this to show the existence of (s; t; n)- graphs, e.g., using the deletion method [5] or, even stronger, the Lovasz Local Lemma [33] or certain large-deviation inequalities <ref> [22] </ref>. If the basic probabilistic method (the usage of (17)) shows that an (s; t; n)-graph exists, these more refined methods usually help show that an (s; t ff ; n)-graph exists, where ff &lt; 1 is some constant.
Reference: [23] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <booktitle> Machine Learning, </booktitle> <address> 2:285318, </address> <year> 1988. </year>
Reference-contexts: Thus BOXES n can be learned if OBOXES 2n can be learned. (A similar trick was employed in <ref> [21, 23] </ref>.) We therefore give an algorithm for learning OBOXES n for each n. The algorithm for learning OBOXES n learns the faces b 1 ; :::; b n of the target B = Q n k=1 (1; b k ] independently.
Reference: [24] <author> N. Linial, M. Luby, M. Saks, and D. Zuckerman. </author> <title> Efficient Construction of a Small Hitting Set for Combinatorial Rectangles in High Dimension. </title> <booktitle> In Proc. ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 258267, </pages> <year> 1993. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables.
Reference: [25] <author> P.M. Long and L. Tan. </author> <title> PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. </title> <booktitle> The 1996 Conference on Computational Learning Theory, </booktitle> <pages> pages 228234, </pages> <year> 1996. </year>
Reference-contexts: This problem has previously been studied in Valiant's PAC framework [34]. In <ref> [25] </ref>, it was proved that if all instances are drawn independently from a product distribution on R n , the target rectan gle can be learned from r-instance examples in O * 20 log 2 nr time, where * and ffi are accuracy and confidence parameters. <p> This al gorithm can be modified slightly to achieve similar results in the statistical query model; applying the results of Kearns [20], this implies that it can be made robust against classification noise. Our algorithm is substantially different from those previously proposed for this problem <ref> [11, 25] </ref>. We believe that a variant of our algorithm will prove useful in practice. Initial empirical results [7] support this belief: a straightforward implementation of a variant of our algorithm performs competitively on datasets used in [11]. Our analysis still requires that all instances are drawn indepen dently.
Reference: [26] <author> A. Lubotzky, R. Phillips and P. Sarnak. </author> <title> Ramanujan graphs. </title> <journal> Combinatorica, </journal> <volume> 8:261277, </volume> <year> 1988. </year>
Reference-contexts: The bound of [18] was obtained by analyzing the expected error (i.e. the 1-norm). (c) Pseudorandom sets for combinatorial rectangles. One major goal in derandomization is to efficiently construct a discrete structure (e.g., constant-degree expanders (Lubotzky, Phillips and Sarnak <ref> [26] </ref>), dispersers and extractors (Nisan [29]), hash function families (Carter and Wegman [9])) that is usually easily shown to exist by a probabilistic argument. Converting randomized algorithms to deterministic ones is one of the many applications of such results.
Reference: [27] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM J. Comput., </journal> <volume> 15(4):10361053, </volume> <year> 1986. </year>
Reference: [28] <author> J. Naor and M. Naor. </author> <title> Smallbias probability spaces: efficient constructions and applications. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4):838 856, </volume> <year> 1993. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see [36, 11]). Also, pseudorandom sets for rectangles have been actively studied recently <ref> [28, 4, 13, 24, 10, 19, 6] </ref> because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables. <p> This area was enriched by the key observation of Naor and Naor that randomized algorithms are usually robust to approximating the distribution of n i.i.d. unbiased random bits X 1 ; X 2 ; : : : ; X n <ref> [28] </ref>. A natural generalization of this is to allow the X i to have arbitrary independent distributions on any finite set, and is formalized as the following problem of pseudorandom sets for combinatorial rectangles. <p> ( log n) ; 2 O ( log n) ; n)-graphs using n O ((log log n) 2 ) processors and polylog (n) time. (If we are willing to expend n O (log n) processors, it is known that we can use almost (2 log 2 n)-wise independent random bits <ref> [28] </ref> to construct (2 log 2 n; 2 log 2 n; n)- graphs in polylog (n) time.) At the other end of the spectrum, we show, e.g., that for arbitrarily small constants ff; fi &gt; 0, we can construct (c; n ff ; n)-graphs using exp (n fi ) processors and
Reference: [29] <author> N. Nisan. </author> <title> Extracting Randomness: How and Why. </title> <booktitle> In Proc. IEEE Conference on Computational Complexity (formerly Structure in Complexity Theory), </booktitle> <pages> pages 4458, </pages> <year> 1996. </year>
Reference-contexts: The bound of [18] was obtained by analyzing the expected error (i.e. the 1-norm). (c) Pseudorandom sets for combinatorial rectangles. One major goal in derandomization is to efficiently construct a discrete structure (e.g., constant-degree expanders (Lubotzky, Phillips and Sarnak [26]), dispersers and extractors (Nisan <ref> [29] </ref>), hash function families (Carter and Wegman [9])) that is usually easily shown to exist by a probabilistic argument. Converting randomized algorithms to deterministic ones is one of the many applications of such results.
Reference: [30] <author> L. Pitt and L.G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4):965984, </volume> <year> 1988 </year>
Reference-contexts: Next, we claim that if H A labels collections of r instances according to an axis-aligned hyperrectangle, then H A 0 can be expressed as an r-term DNF. Applying the result of Pitt and Valiant <ref> [30] </ref>, that r-term DNF are not learnable using r-term DNF as hypotheses in polynomial time unless RP = N P, will complete the proof of the second statement.
Reference: [31] <author> L. Pitt and M.K. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3), </volume> <year> 1990. </year>
Reference-contexts: Consider the DNF learning algorithm A 0 that, for each example (~v; y), gives ('(~v); y) to A, and, given the hypothesis H A output by A, constructs H A 0 by letting ~v 2 H A 0 , '(~v) 2 H A . It is easily verified (see <ref> [31] </ref>), that A 0 learns r-term DNF formulas over n variables in poly (n,r,1=*,1=ffi) time. Next, we claim that if H A labels collections of r instances according to an axis-aligned hyperrectangle, then H A 0 can be expressed as an r-term DNF.
Reference: [32] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference: [33] <author> J. H. Spencer. </author> <title> Asymptotic lower bounds for Ramsey functions. </title> <journal> Discrete Math., </journal> <volume> 20:6976, </volume> <year> 1977. </year>
Reference-contexts: There are more involved probabilistic approaches than this to show the existence of (s; t; n)- graphs, e.g., using the deletion method [5] or, even stronger, the Lovasz Local Lemma <ref> [33] </ref> or certain large-deviation inequalities [22]. If the basic probabilistic method (the usage of (17)) shows that an (s; t; n)-graph exists, these more refined methods usually help show that an (s; t ff ; n)-graph exists, where ff &lt; 1 is some constant.
Reference: [34] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):11341142, </volume> <year> 1984. </year>
Reference-contexts: This problem has previously been studied in Valiant's PAC framework <ref> [34] </ref>. In [25], it was proved that if all instances are drawn independently from a product distribution on R n , the target rectan gle can be learned from r-instance examples in O * 20 log 2 nr time, where * and ffi are accuracy and confidence parameters.
Reference: [35] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> 16(2):264280, </address> <year> 1971. </year>
Reference: [36] <author> S.M. Weiss and C.A. </author> <title> Kulikowski. Computer systems that learn. </title> <publisher> Morgan Kauffman, </publisher> <year> 1991. </year>
Reference-contexts: The PAC learning of rectangles has been studied because they have been found experimentally to yield excellent hypotheses for a variety of applied learning problems (see <ref> [36, 11] </ref>). Also, pseudorandom sets for rectangles have been actively studied recently [28, 4, 13, 24, 10, 19, 6] because (i) they are a subproblem common to the derandomization of depth-2 (DNF) circuits and deran-domizing Randomized Logspace (RL), and (ii) they approximate the distribution of n independent multivalued random variables.
References-found: 36

