URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR91185-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: ADIFOR Generating Derivative Codes from Fortran Programs  
Author: Christian Bischof Alan Carle George Corliss Andreas Griewank Paul Hovland 
Keyword: Key words. Large-scale problems, derivative, gradient, Jacobian, automatic differentiation, optimization, stiff ordinary differential equations, chain rule, parallel, ParaScope Parallel Programming Environment, source transformation and optimization.  
Note: Published in Scientific Programming, 1(1), pp.  
Address: Preprint MCS-P263-0991  
Affiliation: Argonne  
Pubnum: CRPC Technical Report CRPC-TR91185  
Date: 1-29, 1992.  
Abstract: The numerical methods employed in the solution of many scientific computing problems require the computation of derivatives of a function f : R n ! R m . Both the accuracy and the computational requirements of the derivative computation are usually of critical importance for the robustness and speed of the numerical solution. ADIFOR (Automatic Differentiation In FORtran) is a source transformation tool that accepts Fortran 77 code for the computation of a function and writes portable Fortran 77 code for the computation of the derivatives. In contrast to previous approaches, ADIFOR views automatic differentiation as a source transformation problem. ADIFOR employs the data analysis capabilities of the ParaScope Parallel Programming Environment, which enable us to handle arbitrary Fortran 77 codes and to exploit the computational context in the computation of derivatives. Experimental results show that ADIFOR can handle real-life codes and that ADIFOR-generated codes are competitive with divided-difference approximations of derivatives. In addition, studies suggest that the source-transformation approach to automatic differentation may improve the time to compute derivatives by orders of magnitude. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brett Averick, Richard G. Carter, and Jorge J. </author> <title> More. The MINPACK-2 test problem collection (preliminary version). </title> <type> Technical Memorandum MCS-TM-150, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, May </month> <year> 1991. </year>
Reference-contexts: The same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well. As an example, consider the swirling flow problem, which comes from Parter [42] and is part of the MINPACK-2 test problem collection <ref> [1] </ref>. The problem is a coupled system of boundary value problems describing the steady flow of a viscous, incompressible, axisymmetric fluid between two rotating, infinite coaxial disks. The number of variables in the resulting optimization problem depends on the discretization.
Reference: [2] <author> Christian Bischof. </author> <title> Issues in parallel automatic differentiation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 100 - 113. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: of context: Since all computation is performed as a by-product of an elementary operation, it is very difficult, if not impossible, to perform optimizations that transcend one elementary operation (such as the constant-folding techniques that simplified the reverse mode shown in Figure 5 into that shown in are discussed in <ref> [2] </ref>. Loss of Efficiency: The overwhelming majority of codes for which computational scientists want derivatives are written in Fortran, which does not support operator overloading.
Reference: [3] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> ADIFOR exception handling. </title> <type> Technical Memorandum ANL/MCS-TM-159, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, </month> <year> 1991. </year>
Reference-contexts: ADIFOR knows when Fortran intrinsics are nondifferentiable, and traps to an error handler if we wish to compute derivatives at a point where the derivatives do not exist. The current error-handling mechanism of ADIFOR is described in <ref> [3] </ref>. 5 Experimental Results In this section, we report on the execution time of ADIFOR-generated derivative codes in comparison with divided-difference approximations of first derivatives.
Reference: [4] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <type> Technical Memorandum ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, October </month> <year> 1991. </year>
Reference-contexts: The resulting code generated by ADIFOR can be called by users' programs in a flexible manner to be used in conjunction with standard software tools for optimization, solving nonlinear equations, or for stiff ordinary differential equations. A discussion of calling the ADIFOR-generated code from users' programs is included in <ref> [4] </ref>. 4 The Functionality of ADIFOR-Generated Derivative Codes The functionality provided by ADIFOR is best understood through an example. Our example is adapted from problem C2 in the STDTST set of test problems for stiff ODE solvers [25]. <p> Here every circle denotes a nonzero entry. Now, instead of g$p$ = 56, a size of g$p$ = 14 is sufficient, a sizeable reduction in cost. The proper and efficient initialization of ADIFOR-generated derivative codes is described in detail in <ref> [4] </ref>. T Right: Structure of g$Fval T One issue that deserves some attention is that of error handling. Exceptional conditions arise because of branches in the code or because subexpressions may be defined but not be differentiable ( p for example).
Reference: [5] <author> Christian Bischof and James Hu. </author> <title> Utilities for building and optimizing a computational graph for algorithmic decomposition. </title> <type> Technical Memorandum ANL/MCS-TM-148, </type> <institution> Mathematics and Computer Sciences Division, Argonne National Laboratory, 9700 South Cass Ave., Argonne, Ill. </institution> <month> 60439, April </month> <year> 1991. </year>
Reference-contexts: by-product of the computation of f we can store a record of every computation performed and then have an interpreter perform a backward pass on this "tape." The only drawback is that for straightforward implementations, 8 the length of the tape is proportional to the number of arithmetic operations performed <ref> [33, 5] </ref>. Recently, Griewank [31] suggested an approach to overcome this limitation through clever checkpointing.
Reference: [6] <author> Paul T. Boggs and Janet E. Rogers. </author> <title> Orthogonal distance regression. </title> <journal> Contemporary Mathematics, </journal> <volume> 112:183 - 193, </volume> <year> 1990. </year>
Reference-contexts: The code submitted to ADIFOR computes elementary Jacobian matrices which are then assembled to a large sparse Jacobian matrix used in an orthogonal-distance regression fit <ref> [6] </ref>. The code named "shock" was given to us by Greg Shubin, Boeing Computer Services, Seattle, Washington. This code implements the steady shock tracking method for the axisymmetric blunt body problem [46]. The Jacobian has a banded structure.
Reference: [7] <author> Preston Briggs, Keith D. Cooper, Mary W. Hall, and Linda Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> CRPC Report CRPC-TR90102, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Tex., </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The decision to do cloning based on active/passive variable context will eventually be based on an assessment of the savings made possible by introducing the cloned procedures, in accordance with the goal-directed interprocedural transformation approach being adopted within ParaScope <ref> [7] </ref>. Another advantage of a compiler-based approach is that we have the mechanism in place for simplifying the derivative code that has been generated by application of the simple statement-by-statement rules. For example, consider the reverse mode code shown in Figure 5.
Reference: [8] <author> J. C. Butcher. </author> <title> Implicit Runge-Kutta processes. </title> <journal> Math. Comp., </journal> <volume> 18:50 - 64, </volume> <year> 1964. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably best known are gradient methods for optimization [22], Newton's method for the solution of nonlinear systems [15, 22], and the numerical solution of stiff ordinary differential equations <ref> [8, 21] </ref>. Other examples can be found in [17].
Reference: [9] <author> J. C. Butcher. </author> <title> The Numerical Analysis of Ordinary Differential Equations (Runge-Kutta and General Linear Methods). </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1987. </year> <month> 19 </month>
Reference-contexts: Methods such as implicit Runge-Kutta <ref> [9] </ref> and backward differentiation formula (BDF) [35] methods require a Jacobian which is either provided by the user or approximated by divided differences.
Reference: [10] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. I. Nonlinear functional analysis approach. </title> <journal> J. Math. Phys., </journal> <volume> 22(12):2794 - 2802, </volume> <year> 1981. </year>
Reference-contexts: These quantities are usually called adjoints. They measure the sensitivity of the final result with respect to some intermediate quantity. This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering <ref> [10, 11] </ref>, in weather forecasting [41], and even in neural networks [50]. In the reverse mode, let tbar denote the adjoint object corresponding to t. The goal is for tbar to contain the derivative @ w @ t . We know that wbar = @ w @ w = 1:0.
Reference: [11] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. II. Extension to additional classes of responses. </title> <journal> J. Math. Phys., </journal> <volume> 22(12):2803 - 2812, </volume> <year> 1981. </year>
Reference-contexts: These quantities are usually called adjoints. They measure the sensitivity of the final result with respect to some intermediate quantity. This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering <ref> [10, 11] </ref>, in weather forecasting [41], and even in neural networks [50]. In the reverse mode, let tbar denote the adjoint object corresponding to t. The goal is for tbar to contain the derivative @ w @ t . We know that wbar = @ w @ w = 1:0.
Reference: [12] <author> D. Callahan, K. Cooper, R. T. Hood, Ken Kennedy, and Linda M. Torczon. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: By taking a source translator view, we can bring the many man-years of effort of the compiler community to bear on this problem. ADIFOR is based on the ParaScope programming environment <ref> [12] </ref> which combines dependence analysis with interprocedural analysis to support the semi-automatic parallelization of Fortran programs. While our primary goal is not the parallelization of Fortran programs, the ParaScope environment provides us with a Fortran parser, data abstractions for representing Fortran programs, and tools for constructing and manipulating those representations.
Reference: [13] <author> Bruce W. Char. </author> <title> Computer algebra as a toolbox for program generation and manipulation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 53 - 60. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The rationale underlying this code will be given in Section 2. Symbolic differentiation uses the rules of calculus in a more or less mechanical way, although some efficiency can be recouped by back-end optimization techniques <ref> [13, 28] </ref>. In contrast, automatic differentiation is intimately related to the program for the computation of the function to be differentiated.
Reference: [14] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or multivariate higher-order derivatives <ref> [14, 45, 30] </ref>. The derivatives computed by automatic differentiation are highly accurate, unlike those computed by divided differences.
Reference: [15] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10:329 - 345, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably best known are gradient methods for optimization [22], Newton's method for the solution of nonlinear systems <ref> [15, 22] </ref>, and the numerical solution of stiff ordinary differential equations [8, 21]. Other examples can be found in [17].
Reference: [16] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20:187 - 209, </volume> <year> 1984. </year>
Reference-contexts: Thus, if we compute a Jacobian J with n columns by setting g$p$ = n, its computation will require roughly n times as many operations as the original function evaluation, independent of whether J is dense or sparse. However, it is well known <ref> [16, 27] </ref> that the number of function evaluations that are required to compute an approximation to the Jacobian by divided differences can be much less than n if J is sparse. The same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well. <p> The number of variables in the resulting optimization problem depends on the discretization. For example, for n = 56 the Jacobian of F has the structure shown in Figure 10. By using a graph coloring algorithm designed to identify structurally orthogonal columns (we used the one described in <ref> [16] </ref>), we can determine that this Jacobian can be grouped into 14 sets of structurally 14 orthogonal columns, independent of the size of the problem. As a result, we initialize a 56 fi 14 matrix g$x T to the structure shown in Figure 11.
Reference: [17] <author> George F. Corliss. </author> <title> Applications of differentiation arithmetic. </title> <editor> In Ramon E. Moore, editor, </editor> <booktitle> Reliability in Computing, </booktitle> <pages> pages 127 - 148. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Probably best known are gradient methods for optimization [22], Newton's method for the solution of nonlinear systems [15, 22], and the numerical solution of stiff ordinary differential equations [8, 21]. Other examples can be found in <ref> [17] </ref>.
Reference: [18] <author> George F. Corliss. </author> <title> Overloading point and interval Taylor operators. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 139 - 146. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: As a result, a variety of implementations of automatic differentiation have been developed over the years (see [39] for a survey). Most of these implementations implement automatic differentiation by means of operator overloading, which is a language feature in C++, Ada, Pascal-XSC, and Fortran 90 <ref> [18] </ref>. Operator overloading provides the possibility of associating side-effects with arithmetic operations. For example, with an addition "+" we now could associate the addition of the derivative vectors that is required in the forward mode.
Reference: [19] <author> R. Courant, K. Friedrichs, and H. Lewy. </author> <title> Uber die partiellen Differenzengleichungen der mathematischen Physik. </title> <journal> Mathematische Annalen, </journal> <volume> 100:32 - 74, </volume> <year> 1928. </year>
Reference-contexts: If explicit methods (multistep, Runge-Kutta, Taylor, or extrapolation) are applied, the step size must be very small in order to retain desirable stability properties of the method. That is why authors as early as the 1920's <ref> [19] </ref> and again in the late 1940's [20] were led to consider implicit methods in which the approximate solution y i+1 at t = t i+1 is given by the solution to some nonlinear system (y i+1 ) = 0; which is solved by a Newton-type iteration requiring the Jacobian @
Reference: [20] <author> J. Crank and P. Nicholson. </author> <title> A practical method for numerical integration of solutions of partial differential equations of heat conduction type. </title> <booktitle> Proc. </booktitle> <address> Cambridge Philos. </address> <publisher> Soc., </publisher> <address> 43:50 ff, </address> <year> 1947. </year>
Reference-contexts: If explicit methods (multistep, Runge-Kutta, Taylor, or extrapolation) are applied, the step size must be very small in order to retain desirable stability properties of the method. That is why authors as early as the 1920's [19] and again in the late 1940's <ref> [20] </ref> were led to consider implicit methods in which the approximate solution y i+1 at t = t i+1 is given by the solution to some nonlinear system (y i+1 ) = 0; which is solved by a Newton-type iteration requiring the Jacobian @ @y .
Reference: [21] <author> G. Dahlquist. </author> <title> A special stability problem for linear multistep methods. </title> <journal> BIT, </journal> <volume> 3:27 - 43, </volume> <year> 1963. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably best known are gradient methods for optimization [22], Newton's method for the solution of nonlinear systems [15, 22], and the numerical solution of stiff ordinary differential equations <ref> [8, 21] </ref>. Other examples can be found in [17].
Reference: [22] <author> John Dennis and R. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably best known are gradient methods for optimization <ref> [22] </ref>, Newton's method for the solution of nonlinear systems [15, 22], and the numerical solution of stiff ordinary differential equations [8, 21]. Other examples can be found in [17]. <p> 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function. Probably best known are gradient methods for optimization [22], Newton's method for the solution of nonlinear systems <ref> [15, 22] </ref>, and the numerical solution of stiff ordinary differential equations [8, 21]. Other examples can be found in [17].
Reference: [23] <author> Lawrence C. W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimisation. </title> <type> Technical Report No. 180, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1987. </year>
Reference-contexts: Current tools (see [39]) achieve this by storing a record of every computation performed. Then an interpreter performs a backward pass on this "tape." The resulting overhead often annihilates the complexity advantage of the reverse mode in an actual implementation (see <ref> [23, 24] </ref>). ADIFOR uses a hybrid approach. It is generally based on the forward mode, but uses the reverse mode to compute the gradients of assignment statements, since for this restricted case the reverse mode can easily be implemented by a source-to-source translation.
Reference: [24] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 114 - 125. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Current tools (see [39]) achieve this by storing a record of every computation performed. Then an interpreter performs a backward pass on this "tape." The resulting overhead often annihilates the complexity advantage of the reverse mode in an actual implementation (see <ref> [23, 24] </ref>). ADIFOR uses a hybrid approach. It is generally based on the forward mode, but uses the reverse mode to compute the gradients of assignment statements, since for this restricted case the reverse mode can easily be implemented by a source-to-source translation.
Reference: [25] <author> Wayne H. Enright and John D. Pryce. </author> <title> Two FORTRAN packages for assessing initial value methods. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 13(1):1 - 22, </volume> <year> 1987. </year>
Reference-contexts: Our example is adapted from problem C2 in the STDTST set of test problems for stiff ODE solvers <ref> [25] </ref>. The routine FCN2 shown in Figure 7 that computes the right-hand side of a system of ordinary differential equations y 0 = f (x; y) by calling a subordinate routine FCN.
Reference: [26] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M.-Y. Wu. </author> <title> Fortran d language specification. </title> <type> CRPC Report CRPC-TR90079, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Tex., </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The most important are * second- and higher-order derivatives, * automatic detection of sparsity, * increased use of the reverse mode for better performance, and * integration with Fortran parallel programming environments such as Fortran-D <ref> [26] </ref>. Second-order derivatives are a natural extension, and this functionality is required for many applications in numerical optimization. In addition, for sensitivity analysis applications, second derivatives reveal correlations between various parameters.
Reference: [27] <author> D. Goldfarb and P. Toint. </author> <title> Optimal estimation of Jacobian and Hessian matrices that arise in finite difference calculations. </title> <booktitle> Mathematics of Computation, </booktitle> <pages> pages 69 - 88, </pages> <year> 1984. </year>
Reference-contexts: Thus, if we compute a Jacobian J with n columns by setting g$p$ = n, its computation will require roughly n times as many operations as the original function evaluation, independent of whether J is dense or sparse. However, it is well known <ref> [16, 27] </ref> that the number of function evaluations that are required to compute an approximation to the Jacobian by divided differences can be much less than n if J is sparse. The same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well.
Reference: [28] <author> Victor V. Goldman, J. Molenkamp, and J. A. van Hulzen. </author> <title> Efficient numerical program generation and computer algebra environments. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 74 - 83. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The rationale underlying this code will be given in Section 2. Symbolic differentiation uses the rules of calculus in a more or less mechanical way, although some efficiency can be recouped by back-end optimization techniques <ref> [13, 28] </ref>. In contrast, automatic differentiation is intimately related to the program for the computation of the function to be differentiated.
Reference: [29] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83 - 108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: We call x the independent variable and y the dependent variable. While the terms "dependent", "independent", and "variable" are used in many different contexts, this terminology corresponds to the mathematical use of derivatives. There are four approaches to computing derivatives (these issues are discussed in more detail in <ref> [29] </ref>): By Hand: Hand coding is increasingly difficult and error-prone, especially as the problem complexity in creases. 2 Divided Differences: The derivative of f with respect to the ith component of x at a particular point x 0 is approximated by either one-sided differences @ f (x) fi fi h or <p> following simple rule to the statements executed in computing w, but in reverse order: if s = f (t), then tbar += sbar * (df / dt) if s = f (t,u), then tbar += sbar * (df /dt) ubar += sbar * (df /du) Using this simple recipe (see <ref> [29, 45] </ref>), we generate the code shown in Figure 5 for computing w and its gradient. /* Compute function values */ t1 = - y t3 = t2 * z /* Initialize adjoint quantities */ wbar = 1.0; t3bar = 0.0; t2bar = 0.0; t1bar = 0.0; zbar = 0.0; ybar <p> The reverse mode requires fewer operations if the number of independent variables is larger than the number of dependent variables. This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [29, 31, 33] </ref>. Despite the advantages of the reverse mode with regard to complexity, the implementation of the reverse mode for the general case is quite complicated.
Reference: [30] <author> Andreas Griewank. </author> <title> Automatic evaluation of first- and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135 - 148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or multivariate higher-order derivatives <ref> [14, 45, 30] </ref>. The derivatives computed by automatic differentiation are highly accurate, unlike those computed by divided differences. <p> In addition, for sensitivity analysis applications, second derivatives reveal correlations between various parameters. While we currently can just process the ADIFOR-generated code for first derivatives, much can be gained by computing both first- and second-order derivatives at the same time <ref> [30, 44] </ref>. The automatic detection of sparsity is a functionality that is unique to automatic differentiation. Here we exploit the fact that in automatic differentiation, the computation of derivatives is intimately related to the computation of the function itself.
Reference: [31] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, </title> <note> to appear. Also appeared as Preprint MCS-P228-0491, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, </month> <year> 1991. </year>
Reference-contexts: The reverse mode requires fewer operations if the number of independent variables is larger than the number of dependent variables. This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [29, 31, 33] </ref>. Despite the advantages of the reverse mode with regard to complexity, the implementation of the reverse mode for the general case is quite complicated. <p> Recently, Griewank <ref> [31] </ref> suggested an approach to overcome this limitation through clever checkpointing.
Reference: [32] <author> Andreas Griewank and George F. Corliss, </author> <title> editors. Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: of previously existing tools has prevented automatic differentiation from becoming a standard tool for mainstream high-performance computing, even though there are numerous applications where the need for accurate first- and higher-order derivatives essentially mandated the use of automatic differentiation techniques and prompted the development of custom-tailored automatic differentiation systems (see <ref> [32] </ref>). For the majority of applications, however, automatic differentiation techniques were substantially slower than divided-difference approximations, discouraging potential users. The issues of ease of use and portability have received scant attention in software for automatic differentiation as well.
Reference: [33] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 S. Cass Ave., Argonne, Ill. </institution> <month> 60439, </month> <year> 1990. </year>
Reference-contexts: The reverse mode requires fewer operations if the number of independent variables is larger than the number of dependent variables. This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [29, 31, 33] </ref>. Despite the advantages of the reverse mode with regard to complexity, the implementation of the reverse mode for the general case is quite complicated. <p> by-product of the computation of f we can store a record of every computation performed and then have an interpreter perform a backward pass on this "tape." The only drawback is that for straightforward implementations, 8 the length of the tape is proportional to the number of arithmetic operations performed <ref> [33, 5] </ref>. Recently, Griewank [31] suggested an approach to overcome this limitation through clever checkpointing.
Reference: [34] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126 - 135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The derivatives computed by automatic differentiation are highly accurate, unlike those computed by divided differences. Griewank and Reese <ref> [34] </ref> showed that the derivative objects computed in the presence of round-off corresponds to the exact result of a nonlinear system whose partial derivatives have been perturbed by factors of at most (1 + " i;j ) 2 , where j" i;j j ", the relative machine precision. 3 ADIFOR Design
Reference: [35] <author> E. Hairer and G. Wanner. </author> <title> Solving Ordinary Differential Equations II (Stiff and Differential-Algebraic Problems), </title> <booktitle> volume 14 of Springer Series in Computational Mathematics. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Methods such as implicit Runge-Kutta [9] and backward differentiation formula (BDF) <ref> [35] </ref> methods require a Jacobian which is either provided by the user or approximated by divided differences.
Reference: [36] <author> Kenneth E. Hillstrom. </author> <title> JAKEF a portable symbolic differentiator of functions given by algorithms. </title> <type> Technical Report ANL-82-48, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Ave., Argonne, Ill. </institution> <month> 60439, </month> <year> 1982. </year>
Reference-contexts: Examples of this approach are DAPRE [43, 49], GRESS/ADGEN [37, 38], and JAKEF <ref> [36] </ref>. Experiments with some of those systems are described in [48].
Reference: [37] <author> Jim E. Horwedel. GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 243 - 250. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE [43, 49], GRESS/ADGEN <ref> [37, 38] </ref>, and JAKEF [36]. Experiments with some of those systems are described in [48].
Reference: [38] <author> Jim E. Horwedel, Brian A. Worley, E. M. Oblow, and F. G. Pin. </author> <note> GRESS version 1.0 users manual. Technical Memorandum ORNL/TM 10835, </note> <institution> Martin Marietta Energy Systems, Inc., Oak Ridge National Laboratory, Oak Ridge, Tenn. </institution> <month> 37830, </month> <year> 1988. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE [43, 49], GRESS/ADGEN <ref> [37, 38] </ref>, and JAKEF [36]. Experiments with some of those systems are described in [48].
Reference: [39] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 315 - 329. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: ADIFOR is a tool to provide automatic differentiation for programs written in Fortran 77. Given a Fortran subroutine (or collection of subroutines) for a function f, ADIFOR produces Fortran 77 subroutines for the computation of the derivatives of this function. ADIFOR differs from other approaches to automatic differentiation (see <ref> [39] </ref> for a survey) by being based on a source translator paradigm and by having been designed from the outset with large-scale codes in mind. ADIFOR provides several advantages: Portability: ADIFOR produces vanilla Fortran 77 code. <p> It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. Current tools (see <ref> [39] </ref>) achieve this by storing a record of every computation performed. Then an interpreter performs a backward pass on this "tape." The resulting overhead often annihilates the complexity advantage of the reverse mode in an actual implementation (see [23, 24]). ADIFOR uses a hybrid approach. <p> As a result, a variety of implementations of automatic differentiation have been developed over the years (see <ref> [39] </ref> for a survey). Most of these implementations implement automatic differentiation by means of operator overloading, which is a language feature in C++, Ada, Pascal-XSC, and Fortran 90 [18]. Operator overloading provides the possibility of associating side-effects with arithmetic operations.
Reference: [40] <author> Jorge J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 32 - 45. </pages> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference: [41] <author> I. Michael Navon and U. Muller. </author> <title> FESW | A finite-element Fortran IV program for solving the shallow water equations. </title> <booktitle> Advances in Engineering Software, </booktitle> <volume> 1:77 - 84, </volume> <year> 1970. </year>
Reference-contexts: They measure the sensitivity of the final result with respect to some intermediate quantity. This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering [10, 11], in weather forecasting <ref> [41] </ref>, and even in neural networks [50]. In the reverse mode, let tbar denote the adjoint object corresponding to t. The goal is for tbar to contain the derivative @ w @ t . We know that wbar = @ w @ w = 1:0.
Reference: [42] <author> Seymour V. Parter. </author> <title> On the swirling flow between rotating coaxial disks: A survey. </title> <editor> In W. Eckhaus and E. M. de Jager, editors, </editor> <title> Theory and Applications of Singular Perturbations, </title> <booktitle> volume 942 of Lecture Notes in Mathematics, </booktitle> <pages> pages 258 - 280. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The same idea can be applied to greatly reduce the running time of ADIFOR-generated derivative code as well. As an example, consider the swirling flow problem, which comes from Parter <ref> [42] </ref> and is part of the MINPACK-2 test problem collection [1]. The problem is a coupled system of boundary value problems describing the steady flow of a viscous, incompressible, axisymmetric fluid between two rotating, infinite coaxial disks. The number of variables in the resulting optimization problem depends on the discretization.
Reference: [43] <author> John D. Pryce and Paul H. Davis. </author> <title> A new implementation of automatic differentiation for use with numerical software. </title> <type> Technical Report TR AM-87-11, </type> <institution> Mathematics Department, Bristol University, </institution> <year> 1987. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE <ref> [43, 49] </ref>, GRESS/ADGEN [37, 38], and JAKEF [36]. Experiments with some of those systems are described in [48].
Reference: [44] <author> Louis B. Rall. </author> <title> Applications of software for automatic differentiation in numerical computation. </title> <editor> In G. Alefeld and R. D. Grigorieff, editors, </editor> <title> Fundamentals of Numerical Computation (Computer Oriented Numerical Analysis), </title> <journal> Computing Supplement No. </journal> <volume> 2, </volume> <pages> pages 141 - 156. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1980. </year>
Reference-contexts: In addition, for sensitivity analysis applications, second derivatives reveal correlations between various parameters. While we currently can just process the ADIFOR-generated code for first derivatives, much can be gained by computing both first- and second-order derivatives at the same time <ref> [30, 44] </ref>. The automatic detection of sparsity is a functionality that is unique to automatic differentiation. Here we exploit the fact that in automatic differentiation, the computation of derivatives is intimately related to the computation of the function itself.
Reference: [45] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @ t @ t ! We can propagate those derivatives by using elementary differentiation arithmetic based on the chain rule (see <ref> [45] </ref> for more details). <p> following simple rule to the statements executed in computing w, but in reverse order: if s = f (t), then tbar += sbar * (df / dt) if s = f (t,u), then tbar += sbar * (df /dt) ubar += sbar * (df /du) Using this simple recipe (see <ref> [29, 45] </ref>), we generate the code shown in Figure 5 for computing w and its gradient. /* Compute function values */ t1 = - y t3 = t2 * z /* Initialize adjoint quantities */ wbar = 1.0; t3bar = 0.0; t2bar = 0.0; t1bar = 0.0; zbar = 0.0; ybar <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or multivariate higher-order derivatives <ref> [14, 45, 30] </ref>. The derivatives computed by automatic differentiation are highly accurate, unlike those computed by divided differences.
Reference: [46] <author> G. R. Shubin, A. B. Stephens, H. M. Glaz, A. B. Wardlaw, and L. B. Hackerman. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM J. on Sci. and Stat. Computing, </journal> <volume> 3(2):127 - 144, </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: The code named "shock" was given to us by Greg Shubin, Boeing Computer Services, Seattle, Washington. This code implements the steady shock tracking method for the axisymmetric blunt body problem <ref> [46] </ref>. The Jacobian has a banded structure. The compressed Jacobian has 28 columns, compared to 190 for the "normal" Jacobian. The code named "adiabatic" is from Larry Biegler, Chemical Engineering, Carnegie-Mellon University and implements adiabatic flow, a common module in chemical engineering [47].
Reference: [47] <author> J. M. Smith and H. C. Van Ness. </author> <title> Introduction to Chemical Engineering. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The Jacobian has a banded structure. The compressed Jacobian has 28 columns, compared to 190 for the "normal" Jacobian. The code named "adiabatic" is from Larry Biegler, Chemical Engineering, Carnegie-Mellon University and implements adiabatic flow, a common module in chemical engineering <ref> [47] </ref>. Lastly, the code named "reactor" was given to us by Hussein Khalil, Reactor Analysis and Safety Division, Argonne National Laboratory.
Reference: [48] <author> Edgar J Soulie. </author> <title> User's experience with Fortran precompilers for least squares optimization problems. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 297 - 306. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Examples of this approach are DAPRE [43, 49], GRESS/ADGEN [37, 38], and JAKEF [36]. Experiments with some of those systems are described in <ref> [48] </ref>.
Reference: [49] <author> Bruce R. Stephens and John D. Pryce. </author> <title> The DAPRE/UNIX Preprocessor Users' Guide v1.2. </title> <institution> Royal Military College of Science at Shrivenham, </institution> <year> 1990. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE <ref> [43, 49] </ref>, GRESS/ADGEN [37, 38], and JAKEF [36]. Experiments with some of those systems are described in [48].
Reference: [50] <author> P. Werbos. </author> <title> Applications of advances in nonlinear sensitivity analysis. </title> <booktitle> In Systems Modeling and Optimization, </booktitle> <pages> pages 762 - 777, </pages> <address> New York, 1982. </address> <publisher> Springer Verlag. </publisher> <pages> 22 </pages>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering [10, 11], in weather forecasting [41], and even in neural networks <ref> [50] </ref>. In the reverse mode, let tbar denote the adjoint object corresponding to t. The goal is for tbar to contain the derivative @ w @ t . We know that wbar = @ w @ w = 1:0.
References-found: 50

