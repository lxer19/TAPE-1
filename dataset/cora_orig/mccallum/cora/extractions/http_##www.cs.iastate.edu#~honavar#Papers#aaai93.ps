URL: http://www.cs.iastate.edu/~honavar/Papers/aaai93.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: Symbolic and Subsymbolic Learning for Vision: Some Possibilities  
Author: Vasant Honavar 
Date: September 1, 1993  
Address: Ames, IA 50011-1040, USA  
Affiliation: Department of Computer Science Iowa State University  
Abstract: Robust, flexible and sufficiently general vision systems such as those for recognition and description of complex 3-dimensional objects require an adequate armamentarium of representations and learning mechanisms. This paper briefly analyzes the strengths and weaknesses of different learning paradigms such as symbol processing systems, connectionist networks, and statistical and syntactic pattern recognition systems as possible candidates for providing such capabilities and points out several promising directions for integrating multiple such paradigms in a synergistic fashion towards that goal. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arbib, M. A. </author> <year> (1993). </year> <title> Schema Theory: Cooperative Computation for Brain Theory and Distributed AI. In: Symbol Processors and Connectionist Networks in Artificial Intelligence and Cognitive Modelling. Honavar, </title> <editor> V. & Uhr, L. (ed). </editor> <address> New York: </address> <note> Academic Press. To appear. </note>
Reference-contexts: With the exception of some recent work in connectionist architectures for variable binding and unification in slot and filler type representations, very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames (Minsky, 1975), schemas <ref> (Arbib, 1993) </ref> and conceptual graphs (Sowa, 1984) in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <author> Buchanan, B. G., & Wilkins, D. C. </author> <year> (1993). </year> <title> Readings in Knowledge Acquisition and Learning. </title> <address> San Mateo, Cal-ifornia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Carpenter, G., & Grossberg, S. </author> <year> (1991). </year> <title> (Ed). Pattern Recognition by Self-Organizing Neural Networks. </title> <address> Cam-bridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classifica 4 tion and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Fu, K. S. </author> <year> (1982). </year> <title> Syntactic Pattern Recognition and Applications. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: In practice however, some patterns are more likely to appear than others. This is easily modeled using stochastic grammars which associate a probablity with each of the rules of the grammar. These probabilities can be estimated from samples using standard statistical estimation methods <ref> (Fu, 1982) </ref>. Stochastic extensions of attributed grammars present additional possibilities that remain to be explored. 6 Summary and Discussion Learning structures and processes are essential components of adaptive, flexible, robust, and creative intelligent systems. Knowledge representation mechanisms play a central role in problem solving and learning.
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, </title> <booktitle> and Machine Learning. </booktitle> <address> New York: </address> <publisher> Addi-son Wesley. </publisher>
Reference: <author> Goldfarb, L. </author> <year> (1990). </year> <title> An Evolving Model for Pattern Learning. </title> <booktitle> Pattern Recognition 23, </booktitle> <pages> 595-616. </pages>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <institution> Ann Arbor, Michigan: University of Michigan Press. </institution>
Reference: <author> Honavar, V. </author> <year> (1992a). </year> <title> Inductive Learning Using Generalized Distance Measures. </title> <booktitle> In: Proceedings of the SPIE Conference on Adaptive and Learning Systems. </booktitle> <address> Orlando, Florida. </address>
Reference-contexts: Such systems extract, abstract, and encode instances of complex structured objects as necessary and use parameter modification algorithms to fine-tune the representations thus constructed <ref> (Honavar, 1992a) </ref>. 4 Statistical pattern recognition Learning in statistical pattern recognition systems (Duda & Hart, 1973; Fukunaga, 1990) is a form of inductive learning. It is typically assumed that the objects in the domain of interest can be distinguished among each other based on a set of measurements. <p> constructive learning algorithms developed in the context of connectionist networks (see above) to add new structured templates through a feedback-guided process of extraction and abstraction of patterns and subpatterns from the environmental input once suitable similarity or distance measures are defined and efficient algorithms for computing such distances are specified <ref> (Honavar, 1992a) </ref>. Many of the generalization and abstraction algorithms 3 used in symbol processing approaches can find use in the modification of acquired templates.
Reference: <author> Honavar, V. </author> <year> (1992b). </year> <title> Some Biases for Efficient Learning of Spatial, Temporal, and Spatio-Temporal Patterns. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Beijing, China. </address>
Reference-contexts: But recently, several researchers have begun to investigate the use of prior knowledge in the form of rules (Shavlik, 1993), specific inductive biases that can be captured in terms of constraints on network connectivity <ref> (Honavar, 1992b) </ref>, and the organization of a task into more or less independent sub-tasks (Jacobs, Jordan, & Barto, 1990) to facilitate learning. The applicability of connectionist learning methods to complex tasks such as 3-dimensional object recognition and description is limited by the representation of input patterns as unstructured numeric vectors.
Reference: <author> Honavar, V. </author> <year> (1992c). </year> <title> Learning Parsimonious epresen-tations of Three-Dimensional Shapes. </title> <booktitle> In: NATO Advanced Research Workshop on Mathematical Representations of Shape, </booktitle> <address> Driebergen, Netherlands. </address> <note> To appear. </note>
Reference: <author> Honavar, V. & Uhr, L. </author> <title> (1993a) Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <journal> Information Sciences (Special Issue on Artificial Intelligence and Neural Networks) 70 75-108. </journal>
Reference-contexts: Except in some restricted cases, there is no way to guarantee that an ad hoc choice of network architecture would be adequate for learning a mapping implicitly specified in terms of a set of sample input-output pairs. Generative or constructive algorithms <ref> (Honavar & Uhr, 1993a) </ref> attempt to discover an adequate network architecture for a given task by extending the search to the space of network topologies (which, unless suitably constrained, can easily get out of hand). <p> The primitives so added have an effect analogous to extending the vocabulary of natural language by defining new terms or concepts. Similar chunking mechanisms have been explored in symbol processing systems (Laird, Rosenbloom, & Newell, 1986; Uhr & Vossler, 1963), generative learning algorithms for connectionist systems <ref> (Honavar & Uhr, 1993a) </ref> and in some evolutionary learning systems (Koza, 1992). It is possible to combine the template-based and grammar-based approaches to syntactic pattern recognition.
Reference: <author> Honavar, V. </author> <year> (1993). </year> <title> Learning Systems That Use Multiple Strategies and Multiple Representations. In: Symbol Processors and Connectionist Networks in Artificial Intelligence and Cognitive Modelling. Honavar, </title> <editor> V. & Uhr, L. (ed). </editor> <address> New York: </address> <note> Academic Press. To appear. </note>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1993b). </year> <title> Symbol Processing Systems, Connectionist Networks, and Beyond. In: Intelligent Hybrid Systems. </title> <editor> Goonatilake, S. & Khebbal, S. (Ed). </editor> <address> London: </address> <note> Wiley. To appear. </note>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1993c). </year> <title> Integration of Symbol Processing and Connectionist Systems in Artificial Intelligence and Cognitive Modelling. In: Computational Architectures for Integrating Symbolic and Neural Processes. </title>
Reference: <author> Sun, R., </author> & <title> Bookman, </title> <editor> L. (Ed). </editor> <address> New York: </address> <note> Kluwer. To appear. </note>
Reference: <author> Jacobs, R. A., Jordan, M. I., & Barto, A. G. </author> <title> Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks. </title> <type> Tech. Rep. 90-27. </type> <institution> Amherst, Massachusetts: Department of Computer and Information Science, University of Mas-sachusetts. </institution>
Reference: <editor> Koza, J. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Boston, Mas-sachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Similar chunking mechanisms have been explored in symbol processing systems (Laird, Rosenbloom, & Newell, 1986; Uhr & Vossler, 1963), generative learning algorithms for connectionist systems (Honavar & Uhr, 1993a) and in some evolutionary learning systems <ref> (Koza, 1992) </ref>. It is possible to combine the template-based and grammar-based approaches to syntactic pattern recognition. For example, a pre-specified or learned grammar G i for a class of patterns C i can be treated in a manner analogous to a template for the class.
Reference: <author> Laird, J. E., Rosenbloom, P. S. </author> & <title> Newell (1986). Chunking in SOAR: The Anatomy of a General Learning Mechanism. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 11-46. </pages>
Reference: <author> Levenshtein, A. </author> <year> (1966). </year> <title> Binary Codes Capable of Correcting Deletions, </title> <journal> Insertions, and Reversals. Soviet Physics - Doklady 10, </journal> <pages> 703-710. </pages>
Reference: <author> Michalski, R. S. </author> <year> (1993). </year> <title> Toward a Unified Theory of Learning: </title> <booktitle> Multi-Strategy Task-Adaptive Learning. In: Readings in Knowledge Acquisition and Learning. </booktitle>
Reference: <author> Buchanan, B. G., </author> & <title> Wilkins, </title> <address> D. C. San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Miclet, L. </author> <year> (1986). </year> <title> Structural Methods in Pattern Recognition. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Minsky, M. </author> <year> (1975). </year> <title> A Framework for Representing Knowledge. </title> <booktitle> In: The Psychology of Computer Vision. </booktitle>
Reference-contexts: With the exception of some recent work in connectionist architectures for variable binding and unification in slot and filler type representations, very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames <ref> (Minsky, 1975) </ref>, schemas (Arbib, 1993) and conceptual graphs (Sowa, 1984) in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <editor> Winston, P. H. (Ed). </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Parekh, R. G., & Honavar, V. </author> <year> (1993). </year> <title> Grammar Induction for Machine Learning. </title> <note> In preparation. </note>
Reference: <author> Pavlidis, T. </author> <year> (1977). </year> <title> Structural Pattern Recognition. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Rosenfeld, A. </author> <year> (1981). </year> <title> Picture Languages. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Rosenfeld, A. </author> <year> (1984). </year> <title> (Ed). Multi-Resolution Image Processing and Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <editor> Rumelhart, D. E., & McClelland, J. L. (1986) (Ed). </editor> <booktitle> Parallel Distributed Processing, Vol. </booktitle> <address> I-II, Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Shavlik, J. W., & Dietterich, T. G. </author> <year> (1990). </year> <editor> (Ed). </editor> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo, California: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Shavlik, J. W. </author> <year> (1993). </year> <title> A Framework for Combining Symbolic and Neural Learning. In: Symbol Processors and Connectionist Networks in Artificial Intelligence and Cognitive Modelling. Honavar, </title> <editor> V. & Uhr, L. (ed). </editor> <address> New York: </address> <note> Academic Press. To appear. </note>
Reference-contexts: Connectionist approaches generally fail to use prior knowledge that can potentially ease the burden on the learner. But recently, several researchers have begun to investigate the use of prior knowledge in the form of rules <ref> (Shavlik, 1993) </ref>, specific inductive biases that can be captured in terms of constraints on network connectivity (Honavar, 1992b), and the organization of a task into more or less independent sub-tasks (Jacobs, Jordan, & Barto, 1990) to facilitate learning.
Reference: <author> Sowa, J. F. </author> <year> (1984). </year> <title> Conceptual Structures: </title> <booktitle> Information Processing in Mind and Machine. </booktitle> <address> Reading, Mas-sachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: With the exception of some recent work in connectionist architectures for variable binding and unification in slot and filler type representations, very little systematic study has been done on the representation and use (especially in learning) of structured objects such as frames (Minsky, 1975), schemas (Arbib, 1993) and conceptual graphs <ref> (Sowa, 1984) </ref> in connectionist networks. One possibility is to use structured object representations of the sort developed in the context of syntactic pattern recognition (see below) along with suitably defined similarity metrics with generative learning algorithms.
Reference: <author> Tanimoto, S. L., & Klinger, A. </author> <year> (1980). </year> <title> Structured Computer Vision: Machine Perception Through Hierarchical Computation Structures. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Uhr, L., & Vossler, C. </author> <year> (1963). </year> <title> A Pattern Recognition Program that Generates, Evaluates, and Adjusts its Own Pperators. In: Computers and Thought. </title> <editor> Feigenbaum, E., & Feldman, J. (Ed). </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Uhr, L. </author> <year> (1973). </year> <title> Pattern Recognition, Learning, </title> <booktitle> and Thought. </booktitle> <address> New York: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Uhr, L. </author> <title> (1987) (Ed). </title> <booktitle> Parallel Computer Vision. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Winston, P. H. </author> <title> (1975) (Ed). </title> <booktitle> The Psychology of Computer Vision. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Yang, J. & Honavar, V. </author> <year> (1993). </year> <title> A Hierarchical Representation Scheme for Three-Dimensional Object Recognition and Description. </title> <booktitle> In: Proceedings of FLAIRS-93, </booktitle> <address> Fort Lauderdale, Florida. </address> <month> 5 </month>
References-found: 40

