URL: http://www.ri.cmu.edu/afs/cs/project/sensor-9/ftp/papers/jmcv_phd.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs/project/sensor-9/ftp/papers/
Root-URL: 
Title: EFFICIENT COMPRESSION OF ARBITRARY MULTI-VIEW VIDEO SIGNALS  
Author: Jeffrey Scott McVeigh 
Degree: A DISSERTATION SUBMITTED TO THE GRADUATE SCHOOL IN PARTIAL FULFILLMENT OF THE REQUIREMENTS for the degree DOCTOR OF PHILOSOPHY in ELECTRICAL AND COMPUTER ENGINEERING by  
Date: June, 1996  
Affiliation: CARNEGIE MELLON UNIVERSITY  Pittsburgh, Pennsylvania  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> CCITT Study Group XV - Report R95, </author> <title> Recommendation H.261, Video Codec for Audiovisual Services at p x 64 kbits, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: Interframe redundancy is due to the predictable displacement of objects between frames, while intraframe redundancy is due to the spatial correlation of pixels. Popular single-view implementations of hybrid coders are found in the MPEG-1, MPEG-2, H.261, and H.263 video compression standards <ref> [1, 2, 3, 4] </ref>. Frame-based hybrid encoders view a video signal as a set of image frames. The signal is compressed by performing intraframe coding on marker frames, and by encoding all other frames using information from previously decoded frames. <p> This is the case for the common fixed-sized, block-based displacement estimation techniques, where a single displacement vector, estimated to a specified level of accuracy, is transmitted for each identically-sized, nonoverlapping block in the current frame <ref> [1, 2, 3, 4] </ref>. <p> Slight variations in the prediction bit rate are possible due to the entropy coding of the displacement vectors. It is also customary to use a spatially-invariant prediction coefficient that approximates the correlation coefficient in Eq. (5.1) over an entire image or a class of images <ref> [1, 2, 3, 4, 71] </ref>. For either case, the overhead in terms of the complexity to estimate the prediction coefficient and the number of bits to transmit this quantity are inconsequential with respect to the displacement estimation and residual encoding operations. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients. <p> The optimal (minimum-occlusion) reference frame was always used for the prediction of frames within the dependent view; View 0 was specified as the independent view for all sequences. To perform a direct comparison with current standards, which perform DCT/scalar-quantiza tion for the coding of the residual image <ref> [1, 2, 3, 4] </ref>, image blocks were used in the vector coding techniques. These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. <p> auxiliary information (see Fig. 6.1) is readily extracted from the coded bit-stream, which alleviates the need to perform a displacement estimation procedure at the decoder. 5 To facilitate the application of this work to standardized compression algorithms, we assume that the displacement field was estimated using a fixed-size, block-based technique <ref> [1, 2, 3, 4, 60] </ref>. While block-based techniques provide an adequate prediction and a compact representation of the displacement field, they are prone to serious estimation errors.
Reference: [2] <author> ISO/IEC JTC1/SG29/WG11, </author> <month> ISO/IEC 11172-2, </month> <title> Information Technology - Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbits/s - Part 2: Video, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: Interframe redundancy is due to the predictable displacement of objects between frames, while intraframe redundancy is due to the spatial correlation of pixels. Popular single-view implementations of hybrid coders are found in the MPEG-1, MPEG-2, H.261, and H.263 video compression standards <ref> [1, 2, 3, 4] </ref>. Frame-based hybrid encoders view a video signal as a set of image frames. The signal is compressed by performing intraframe coding on marker frames, and by encoding all other frames using information from previously decoded frames. <p> We note a few observations on techniques used to obtain a compact representation for occluded regions. The noncausal model lends itself to transform-based techniques [43]. This is the approach taken by the MPEG class of video compression standards <ref> [2] </ref>, where intraframe DCT-based compression is performed on poorly predicted (read occluded) blocks within a predictively-coded frame. The use of a fixed transform implies the assumption of a stationary image model. <p> This is the case for the common fixed-sized, block-based displacement estimation techniques, where a single displacement vector, estimated to a specified level of accuracy, is transmitted for each identically-sized, nonoverlapping block in the current frame <ref> [1, 2, 3, 4] </ref>. <p> Slight variations in the prediction bit rate are possible due to the entropy coding of the displacement vectors. It is also customary to use a spatially-invariant prediction coefficient that approximates the correlation coefficient in Eq. (5.1) over an entire image or a class of images <ref> [1, 2, 3, 4, 71] </ref>. For either case, the overhead in terms of the complexity to estimate the prediction coefficient and the number of bits to transmit this quantity are inconsequential with respect to the displacement estimation and residual encoding operations. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients. <p> The optimal (minimum-occlusion) reference frame was always used for the prediction of frames within the dependent view; View 0 was specified as the independent view for all sequences. To perform a direct comparison with current standards, which perform DCT/scalar-quantiza tion for the coding of the residual image <ref> [1, 2, 3, 4] </ref>, image blocks were used in the vector coding techniques. These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. <p> auxiliary information (see Fig. 6.1) is readily extracted from the coded bit-stream, which alleviates the need to perform a displacement estimation procedure at the decoder. 5 To facilitate the application of this work to standardized compression algorithms, we assume that the displacement field was estimated using a fixed-size, block-based technique <ref> [1, 2, 3, 4, 60] </ref>. While block-based techniques provide an adequate prediction and a compact representation of the displacement field, they are prone to serious estimation errors.
Reference: [3] <author> ISO/IEC JTC1/SC29/WG11 Test Model Editing Committee, </author> <title> MPEG-2 Video Test Model 5", </title> <editor> ISO/IEC JTC1/SC29/WG11 Doc. N0400, </editor> <month> April </month> <year> 1993. </year>
Reference-contexts: Interframe redundancy is due to the predictable displacement of objects between frames, while intraframe redundancy is due to the spatial correlation of pixels. Popular single-view implementations of hybrid coders are found in the MPEG-1, MPEG-2, H.261, and H.263 video compression standards <ref> [1, 2, 3, 4] </ref>. Frame-based hybrid encoders view a video signal as a set of image frames. The signal is compressed by performing intraframe coding on marker frames, and by encoding all other frames using information from previously decoded frames. <p> This is the case for the common fixed-sized, block-based displacement estimation techniques, where a single displacement vector, estimated to a specified level of accuracy, is transmitted for each identically-sized, nonoverlapping block in the current frame <ref> [1, 2, 3, 4] </ref>. <p> Slight variations in the prediction bit rate are possible due to the entropy coding of the displacement vectors. It is also customary to use a spatially-invariant prediction coefficient that approximates the correlation coefficient in Eq. (5.1) over an entire image or a class of images <ref> [1, 2, 3, 4, 71] </ref>. For either case, the overhead in terms of the complexity to estimate the prediction coefficient and the number of bits to transmit this quantity are inconsequential with respect to the displacement estimation and residual encoding operations. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients. <p> The optimal (minimum-occlusion) reference frame was always used for the prediction of frames within the dependent view; View 0 was specified as the independent view for all sequences. To perform a direct comparison with current standards, which perform DCT/scalar-quantiza tion for the coding of the residual image <ref> [1, 2, 3, 4] </ref>, image blocks were used in the vector coding techniques. These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. <p> auxiliary information (see Fig. 6.1) is readily extracted from the coded bit-stream, which alleviates the need to perform a displacement estimation procedure at the decoder. 5 To facilitate the application of this work to standardized compression algorithms, we assume that the displacement field was estimated using a fixed-size, block-based technique <ref> [1, 2, 3, 4, 60] </ref>. While block-based techniques provide an adequate prediction and a compact representation of the displacement field, they are prone to serious estimation errors.
Reference: [4] <author> ISO/IEC JTC1/SC29/WG11, </author> <title> Information Technology - Generic Coding of Moving Pictures and Associated Audio, Recommendation H.262, ISO/IEC 13818-2, Draft International Standard, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: Interframe redundancy is due to the predictable displacement of objects between frames, while intraframe redundancy is due to the spatial correlation of pixels. Popular single-view implementations of hybrid coders are found in the MPEG-1, MPEG-2, H.261, and H.263 video compression standards <ref> [1, 2, 3, 4] </ref>. Frame-based hybrid encoders view a video signal as a set of image frames. The signal is compressed by performing intraframe coding on marker frames, and by encoding all other frames using information from previously decoded frames. <p> This is the case for the common fixed-sized, block-based displacement estimation techniques, where a single displacement vector, estimated to a specified level of accuracy, is transmitted for each identically-sized, nonoverlapping block in the current frame <ref> [1, 2, 3, 4] </ref>. <p> Slight variations in the prediction bit rate are possible due to the entropy coding of the displacement vectors. It is also customary to use a spatially-invariant prediction coefficient that approximates the correlation coefficient in Eq. (5.1) over an entire image or a class of images <ref> [1, 2, 3, 4, 71] </ref>. For either case, the overhead in terms of the complexity to estimate the prediction coefficient and the number of bits to transmit this quantity are inconsequential with respect to the displacement estimation and residual encoding operations. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients. <p> The optimal (minimum-occlusion) reference frame was always used for the prediction of frames within the dependent view; View 0 was specified as the independent view for all sequences. To perform a direct comparison with current standards, which perform DCT/scalar-quantiza tion for the coding of the residual image <ref> [1, 2, 3, 4] </ref>, image blocks were used in the vector coding techniques. These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. <p> auxiliary information (see Fig. 6.1) is readily extracted from the coded bit-stream, which alleviates the need to perform a displacement estimation procedure at the decoder. 5 To facilitate the application of this work to standardized compression algorithms, we assume that the displacement field was estimated using a fixed-size, block-based technique <ref> [1, 2, 3, 4, 60] </ref>. While block-based techniques provide an adequate prediction and a compact representation of the displacement field, they are prone to serious estimation errors.
Reference: [5] <author> G. P. Abousleman, M. W. Marcellin and B. R. Hunt, </author> <title> Compression of hyperspectral imagery using the 3-D DCT and hybrid DPCM/DCT, </title> <journal> IEEE Trans. Geoscience and Remote Sensing, </journal> <volume> vol. 33, no. 1, </volume> <pages> pp. 26-34, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Additional applications may use cameras that vary not only in their viewpoint, but also have vary The cameras (C ij ) are arranged with parallel axes and coplanar image sensors. C 11 Dc y C 21 C I2 C 1J 4 ing scale [101] and spectral bandwidth selectivity parameters <ref> [5, 35] </ref>. Regardless of their ultimate use, it is likely that these signals will need to be transmitted and/or stored in order to be displayed at a remote location, re-displayed at a future time, or processed off-line. <p> Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients.
Reference: [6] <author> A. Asif and J. M. F. Moura, </author> <title> Image codec by noncausal prediction, residual mean removal, and cascaded VQ, </title> <journal> IEEE Trans. on Circuits and Systems for Video Tech., </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 42-55, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: This approach can accommodate a finite number of correlation variations. Noncausal prediction, as described in <ref> [6] </ref>, also can be used for the intraframe coding of occlusions.
Reference: [7] <author> H. Aydinoglu and M. H. Hayes, </author> <title> Compression of multi-view images, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 385-389, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We define the two exterior views of periodic MVSs as the views where is at an extremum; all other views are classified as interior views. The majority of prior research on the compression of more than two views has dealt with nonarbitrary viewpoints with zero phase offset <ref> [7, 30, 40, 47] </ref>. These periodic signals require a very high camera complexity to generate the multitude of views, and they possess a simple relationship between the interior and exterior views. <p> structures is non-trivial, we feel that the high compression ratios often reported for non-arbitrary q m ( ) Dc x 0 0 0 23 v a. m 19 plane image (EPI) for middle row of stacked images. u Views EPI row 24 viewpoints are obtained by essentially oversampling the scene <ref> [7, 30, 40, 47] </ref>. To avoid the generation and compression of the inherently redundant interior views, we wish to capture and encode only the exterior views and to accurately interpolate the interior views. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>.
Reference: [8] <author> N. Balram and J. M. F. Moura, </author> <title> Noncausal Gauss Markov random fields: Parameter structure and estimation, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 334-354, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: A predictive coder attempts to exploit this memory to provide a more concise represen 1. See <ref> [8] </ref> and [43] for a complete description of noncausal models.
Reference: [9] <author> H. H. Baker and R. C. Bolles, </author> <title> Generalizing epipolar plane image analysis on the spatiotem-poral surface, </title> <journal> Int. J. of Computer Vision, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 33-49, </pages> <year> 1989. </year>
Reference-contexts: The displacement range of corresponding points between two views obtained at the same time instant is constrained to one-dimension, which is referred to as the epipolar line <ref> [9, 11, 56, 92] </ref>. 4 Since the camera configuration is rarely known to 3. Due to their overwhelming use, we will consider only block-based (fixed and variable sized) displacement compensated prediction schemes in this thesis. As such, we shall interchangeably use the terms prediction process and displacement estimation/compensation. 4.
Reference: [10] <author> S. T. Barnar and M. A. Fischler, </author> <title> Disparity analysis of images, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 2, </volume> <pages> pp. 333-340, </pages> <month> July </month> <year> 1980. </year>
Reference: [11] <author> R. C. Bolles, H. H. Baker and D. H. Marimont, </author> <title> Epipolar-plane image analysis: An approach to determining structure from motion, </title> <journal> Int. J. of Computer Vision, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 176-185, </pages> <year> 1987. </year> <month> 185 </month>
Reference-contexts: The displacement range of corresponding points between two views obtained at the same time instant is constrained to one-dimension, which is referred to as the epipolar line <ref> [9, 11, 56, 92] </ref>. 4 Since the camera configuration is rarely known to 3. Due to their overwhelming use, we will consider only block-based (fixed and variable sized) displacement compensated prediction schemes in this thesis. As such, we shall interchangeably use the terms prediction process and displacement estimation/compensation. 4.
Reference: [12] <author> L. Breiman, J. H. Friedman, R. A. Olshen and C. J. Stone, </author> <title> Classification and Regression Trees. The Wadsworth Statistics / Probability Series, </title> <address> Belmont CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: The number of bits to encode an index then will depend on the particular reproduction vector selected. Optimal pruning of the balanced tree is performed using the generalized BFOS algorithm <ref> [12] </ref>. The two variations of this algorithm trade either average rate (length-pruned tree-structure, PTSVQ) or average entropy (entropy-pruned tree-structure, EPTSVQ) for average distortion.
Reference: [13] <author> C. Cafforio, F. Rocca and S. Tubaro, </author> <title> Motion compensated image interpolation, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 215-222, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views. <p> Previously reported techniques for the detection of occluded regions typically use a measure of symmetry between estimated, bidirectional displacement vectors fields as the decision threshold <ref> [13, 83] </ref>. The problem with using these occlusion estimation techniques for our reference frame selection task is that the estimation of a displacement vector field is equivalent to performing a frame-based prediction; i.e., we have merely reformulated the exhaustive search method. <p> Motion-compensated interpolation is used for the applications of frame rate conversion, de-interlacing and frame skipping <ref> [13, 48, 83, 93] </ref>. Since constant, translational object motion is often assumed, interpolation of unoccluded regions is performed in the same manner as described in Section 6.1.2. Cafforio, et. al., achieve a highly accurate motion field by using bidirectional motion estimates obtained from a pel-recursive algorithm [13]. <p> Since constant, translational object motion is often assumed, interpolation of unoccluded regions is performed in the same manner as described in Section 6.1.2. Cafforio, et. al., achieve a highly accurate motion field by using bidirectional motion estimates obtained from a pel-recursive algorithm <ref> [13] </ref>. To reduce streaking effects at object boundaries, a total of eight recursions must be performed and combined to yield the final vector field. <p> The total decoder complexity, including the mapping of unoccluded displacements, is given by, 11 In summary, our technique is substantially less complex than the EPI-based algorithms. While our decoder requires slightly more operations than that of <ref> [13] </ref> or [83], our block-based displacement field is encoded considerably more efficiently than their multiple, pixel-based fields; hence, our approach is more applicable to viewpoint interpolation within a complete encoder/decoder system. 6.4.2 Storage At the encoder, our technique must store the two reference views and a sparse displacement field. <p> Our storage requirement is approximately equal to that of the various viewpoint interpolation schemes that use only two views. Only our method requires SNR values for each valid vector. Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields <ref> [13, 83] </ref> or a change detection operator [93].
Reference: [14] <author> E. Chalom, E. and V. M. Bove, </author> <title> V.M, Segmentation of frames in a video sequence using motion and other attributes, </title> <booktitle> in Proc. Digital Video Compression: Algorithms and Technologies 1995, SPIE, </booktitle> <volume> vol. 2419, </volume> <pages> pp. 230-241, </pages> <year> 1995. </year>
Reference-contexts: A substantial body of work has been devoted to improving the prediction performance for multi-view signals through segmentation-based schemes, which are applicable to both single and multi-view signals <ref> [14, 87, 100] </ref>. These techniques begin to approach the concept of content-based coding (see Section 1.3.3) by predicting homogeneous regions within each frame from a reference frame, as opposed to purely pixel-based schemes, such as the ubiquitous block-based techniques. <p> The improved prediction has been shown to significantly reduce the number of bits needed in the residual encoding stage <ref> [14, 87, 100] </ref>. The more accurate displacement estimation afforded by these techniques also can be used to improve displacement-compensated interpolation schemes (see Section 1.3.2). However, segmentation-based methods are not without their faults. If the scene contains numerous, high-detail regions, the coders bit rate actually may increase over pixel-based prediction schemes.
Reference: [15] <author> R. Chassaing, B. Choquet and D. Pele, </author> <title> A stereoscopic television system (3D-TV) and compatible transmission on a MAC channel (3D-MAC), Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 33-43, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Additional views are easily incorporated into this methodology due to the frame-based approach, where the number of frames increases linearly with the number of views. This extension of the hybrid coder structure has been reported in numerous systems dealing with multi-view signals <ref> [15, 59, 78, 86, 87] </ref>. The frames from each view may be treated equally or an implicit ranking may be established between the individual views. <p> Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>.
Reference: [16] <author> P. A. Chou, T. Lookabaugh and R. M. Gray, </author> <title> Optimal pruning with application to tree-structured source coding and modeling, </title> <journal> IEEE Tran. Inform. Theory, </journal> <volume> vol. 35, no. 2, </volume> <pages> pp. 299-315, </pages> <year> 1989. </year>
Reference-contexts: Prior to entropy coding, the indices of a balanced TSVQ have fixed rate. A variable rate code can be obtained if the balanced tree is replaced by a pruned tree, where the terminal nodes lie at varying depths <ref> [16] </ref>. The number of bits to encode an index then will depend on the particular reproduction vector selected. Optimal pruning of the balanced tree is performed using the generalized BFOS algorithm [12]. <p> We speculate that the gain achieved by TSVR_8+ for rates over 0.125 bpp are due to over-training of the codebook. 12 These codebooks were then pruned to various average rates using the generalized BFOS algorithm <ref> [16] </ref>. The performance plots for the pruned TSVR (PTSVR) and pruned TSVQ (PTSVQ) codebooks are shown in Fig. 5.5b. Again, we note that using the 4+ neighborhood provides the best compromise between rate-distortion performance and complexity/storage costs. For the remainder of the experimental results, we will only consider this neighborhood.
Reference: [17] <author> P. A. Chou, T. Lookabaugh and R. M. Gray, </author> <title> Entropy-constrained vector quantization, </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol. 37, no. 1, </volume> <pages> pp. 31-42, </pages> <year> 1989. </year>
Reference-contexts: A PVQ scheme 4. Another approach to achieving a variable rate vector quantizer, termed entropy-constrained vector quantization (ECVQ), jointly optimizes an unstructured vector quantizer and a noiseless variable rate coder <ref> [17] </ref>.
Reference: [18] <author> R. J. Clarke, </author> <title> Transform Coding of Images, </title> <publisher> London: Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: The residual encoder may include numerous operations, such as: the transformation of the residual image data to a generalized frequency domain (DCT <ref> [18] </ref> and wavelet transformations [49]), and the application of both lossy (scalar and vector quantization [32]) and lossless (run-length and entropy coding [42]) compression techniques. <p> The Karhunen-Love Transform (KLT) is known to achieve optimal decorrelation and energy compaction for a class of image sources <ref> [18] </ref>. However, the KLT has seen limited use in residual coding applications since the statistics of the image source often are not known a priori, and the computation and subsequent transmission of the KLT for a particular image is prohibitive.
Reference: [19] <author> L. Corte-Real and A. P. Alves, </author> <title> A very low bit rate video coder based on vector quantization, </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 263-273, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. While this block size is larger than that typically used in vector quantization implementations for image sequence coding <ref> [19, 39, 58] </ref>, we justify this selection with our requirement of low bit rate coding in the region of less than 0.25 bits-per-pixel, and the consistency with the macroblock/block partitioning of image frames in the MPEG and H.26x standards.
Reference: [20] <author> J. L. Crowley, P. Bobet and C. Schmid, </author> <title> Auto-calibration by direct observation of objects, </title> <journal> Image and Vision Computing, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 67-81, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Only two views of the scene are required to calculate displacement information for unoccluded regions. Additional views are useful for autocalibration <ref> [20, 107] </ref>, reducing ambiguity in the displacement estimation process [37, 89], and minimizing the number and area of occluded regions [30]. 1.1.4 Multi-view Bit Rate The described applications are but a sampling of the possible uses for multi-view video signals.
Reference: [21] <author> G. Demoment, </author> <title> Image reconstruction and restoration: Overview of common estimation structures and problems, </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol. 37, no. 12, </volume> <pages> pp. 2024-2036, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: We call this approach restoration-based residual coding. From this description, this process obviously is closely related to the problem of restoring an image from an observed, degraded versions of the original <ref> [21, 84] </ref>.
Reference: [22] <author> I. Dinstein, M. G. Kim, A. Henik and J. Tzelgov, </author> <title> Compression of stereo images using sub-sampling and transform coding, </title> <journal> Optical Engineering, </journal> <volume> vol. 30, no. 9, </volume> <pages> pp. 1359-1364, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: These methods may be applied to multi-view signals intended for viewing by a human observer. Also, binocular systems often exploit the singularity-of-vision property of the HVS to remove irrelevant information <ref> [22] </ref>. In binocular vision, one eye is dominant over the other eye artifacts viewed by the non-dominant eye will be masked by the dominant eye. This feature allows for a non-symmetric allocation of bits between the two views; i.e., one view is compressed much more coarsely than the other view. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients.
Reference: [23] <author> I. Dinstein, G. Guy, J. Rabany, J. Tzelgov and A. Henik, </author> <title> On the compression of stereo images: Preliminary results, </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 17, no. 4, </volume> <pages> pp. 373-382, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration.
Reference: [24] <author> M. Effros and P. A. Chou, </author> <title> Weighted universal transform coding: Universal image compression with the Karhunen-Love Transform, </title> <booktitle> in Proc. IEEE Internat. Conf. Image Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 61-64, </pages> <year> 1995. </year>
Reference-contexts: This is the approach taken by the MPEG class of video compression standards [2], where intraframe DCT-based compression is performed on poorly predicted (read occluded) blocks within a predictively-coded frame. The use of a fixed transform implies the assumption of a stationary image model. This concept was extended in <ref> [24] </ref> to include image dependent statistics; an image block is coded using a transform selected from a codebook of transforms designed off-line, each of which approximates the KLT for a specific class of image sources. This approach can accommodate a finite number of correlation variations. <p> Also, fast implementations of the DCT exist [57, 80], and it is possible to piggy-back off of existing hardware used to perform DCT-based intraframe compression [60]. 94 A solution to some of the difficulties associated with using the KLT recently was presented in <ref> [24] </ref>, where a finite set of transforms were designed that approximate the optimal transform for a broad class of image sources, i.e., it is universal in nature. While this technique was applied to intraframe coding, it should be easily extended to the residual coding problem. <p> for the residual and predicted images, in that only nonoverlapping blocks are extracted from the residual image, and the predicted blocks may overlap; and the restoration and minimize blocks in the encoder constitutes the nearest-neighboring encoding procedure. 5.4.3 Interpretation and Comparison Restoration-based coding is both adaptive and universal in nature <ref> [24, 85, 105] </ref>. Its adaptive feature is based on the ability to adaptively select the best restoration operator for a region in the residual image; basically, it is able to track non-stationarities of the source signal.
Reference: [25] <author> D. Eggert and K. Bowyer, </author> <title> Computing the perspective projection aspect graph of solids of revolution, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 15, no. 2, </volume> <pages> pp. 109-128, </pages> <year> 1993. </year>
Reference-contexts: Both EPI-based techniques reported by Katayama, et. al, and Hsu, et. al, handle holes in the interpolated image by simple region filling with adjacent pixel data. In [40], ambiguously-referenced image regions are addressed through the use of aspect graphs <ref> [25] </ref> to layer scene objects based on range; hidden surface removal then is performed to ensure that foreground objects are mapped in front of background objects. A more cautious approach to interpolating image regions with more than one possible displacement trajectory is taken in [47].
Reference: [26] <author> N. Farvardin and J. W. Modestino, </author> <title> Rate-distortion performance of DPCM schemes for autoregressive sources, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol., 31, no. 3, </volume> <pages> pp. 402-418, </pages> <month> May </month> <year> 1985. </year> <month> 186 </month>
Reference-contexts: universal characteristic of all residual encoding schemes known to the author is that the residual image is operated upon independently of any knowledge of the predicted image. 10 This approach is based on the model that the original image consists of both a predictable portion and an unpredictable, white-noise component <ref> [26] </ref>. The prediction operation is assumed to exactly yield the predictable information of the original image and the residual image merely contains the white-noise process. Therefore, the residual and predicted images are uncorrelated and no coding gain is possible through the use of information available in the predicted image.
Reference: [27] <author> J. D. Fooley, </author> <title> Introduction to Computer Graphics, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: In the simplest case, and the 4. A complete derivation of the conversion between coordinate systems and the transformation from 3-D world- to 2-D image-coordinates can be found in <ref> [27] </ref>. Dt m MVS F DF m nT Dt m +,( )= m 0 M, , -" n 0 N 1, , -" F 1 23.5,( ) DF m q m ( )DF= q m ( ) m= 22 viewpoints possess a strict periodicity.
Reference: [28] <author> M. Foodeei and E. Dubois, </author> <title> Coding image sequence intensities along motion trajectories using EC-CELP quantization, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <pages> pp. 720-724, </pages> <year> 1994. </year>
Reference-contexts: Due to the varying statistical characteristics of each of these regions, we present two distinct, yet related, models for pixel intensities. 3.1.1 Unoccluded Regions The discrete-time, Gauss-Markov process has been shown to adequately model image sequence intensities along motion trajectories <ref> [28, 29] </ref>. Since the displacement of an image region between two frames within a multi-view signal is analogous to an objects motion trajectory between two temporally-offset frames, we adopt this model for our purposes.
Reference: [29] <author> M. Foodeei and E. Dubois, </author> <title> Rate-distortion performance of source coders in the low bit-rate region for highly correlated Gauss-Markov source, </title> <booktitle> in Proc. GLOBECOM, </booktitle> <pages> pp. 123-127, </pages> <year> 1993. </year>
Reference-contexts: However, due to the bit rate and complexity constraints imposed on the prediction process and the non-stationarity of the image data, this model becomes invalid: substantial correlation exists between the predicted and residual images. This is particularly true when the coder operates in low bit rate regions <ref> [29] </ref>. A substantial contribution of this thesis is the development of a novel coder model that exploits this correlation, due to the practical constraints on the prediction process, to achieve superior rate-distortion performance over traditional residual encoders. <p> Due to the varying statistical characteristics of each of these regions, we present two distinct, yet related, models for pixel intensities. 3.1.1 Unoccluded Regions The discrete-time, Gauss-Markov process has been shown to adequately model image sequence intensities along motion trajectories <ref> [28, 29] </ref>. Since the displacement of an image region between two frames within a multi-view signal is analogous to an objects motion trajectory between two temporally-offset frames, we adopt this model for our purposes. <p> The innovations sequence is a white-noise random process independent of previous intensity values along the displacement trajectory. This model reduces to the stationary case when the correlation coefficient is fixed. Typically, video sources are assumed to be highly correlated with coefficient values in the range 0.9 to 1.0 <ref> [29] </ref>. Due to the possibility of pixel-by-pixel intensity uctuations, we make no assumptions on the rate of change of the correlation coefficient. <p> The effect of this discrepancy is particularly troublesome in very low bit rate regions, where coarse quantization of the 89 residual image results in a violation of the additive noise model often assumed by recursive, predictive coding schemes <ref> [29] </ref>. Each of these practical limitations of the prediction process result in the same conclusion: optimal prediction of unoccluded pixel intensities, in the sense that the residual exactly equals the innovations sequence, is impossible.
Reference: [30] <author> T. Fujii and H. Harashima, </author> <title> Data compression of an autostereoscopic 3-D image, </title> <institution> University of Tokyo Tech. </institution> <type> Rep., </type> <year> 1994. </year>
Reference-contexts: Only two views of the scene are required to calculate displacement information for unoccluded regions. Additional views are useful for autocalibration [20, 107], reducing ambiguity in the displacement estimation process [37, 89], and minimizing the number and area of occluded regions <ref> [30] </ref>. 1.1.4 Multi-view Bit Rate The described applications are but a sampling of the possible uses for multi-view video signals. Additional applications may use cameras that vary not only in their viewpoint, but also have vary The cameras (C ij ) are arranged with parallel axes and coplanar image sensors. <p> Numerous techniques for the synthesis of intermediate views have been reported in the literature. A common approach is through the generation of epipolar plane images (EPI) <ref> [30, 40, 47] </ref>. <p> We define the two exterior views of periodic MVSs as the views where is at an extremum; all other views are classified as interior views. The majority of prior research on the compression of more than two views has dealt with nonarbitrary viewpoints with zero phase offset <ref> [7, 30, 40, 47] </ref>. These periodic signals require a very high camera complexity to generate the multitude of views, and they possess a simple relationship between the interior and exterior views. <p> structures is non-trivial, we feel that the high compression ratios often reported for non-arbitrary q m ( ) Dc x 0 0 0 23 v a. m 19 plane image (EPI) for middle row of stacked images. u Views EPI row 24 viewpoints are obtained by essentially oversampling the scene <ref> [7, 30, 40, 47] </ref>. To avoid the generation and compression of the inherently redundant interior views, we wish to capture and encode only the exterior views and to accurately interpolate the interior views. <p> View 1 View 0 (blackened) and ambiguously-referenced (lightly-shaded) image regions. View 1 View 0 b 1 b. 132 A common interpolation approach is through the generation of epipolar plane images <ref> [30, 40, 47] </ref>. We have discussed the various advantages and disadvantages of these non-arbitrary multi-view signals in detail in Section 2.2. Essentially, numerous views of the scene are captured and processed at the encoder to estimate the displacement variation of objects through the set of image frames. <p> A slightly different EPI-based approach was described by Fujii and Harashima <ref> [30] </ref>. This work was presented in the context of both compressing the multiple views and synthesizing virtual viewpoints. A more compact representation of the pixel displacements, compared to trace lines, was obtained using triangular patches of textured regions. <p> directly calculate the number of operations-per-pixel; however, they did indicate the processing time required to perform their algorithm on a Silicon Graphics Iris Indigo Elan. 10 Without the Var-space segmentation enhancement, fields for CIF-sized images were obtained in a few minutes; with segmentation, the computational time was about 10 minutes <ref> [30] </ref>. Obviously, this approach has excessive complexity. The three-camera technique developed by Skerjanc and Liu generates the interpolated field through: feature extraction, feature correspondence matching, and correspondence compatibility. Due to the object-oriented (i.e., feature-based) approach, it is difficult to compute per-pixel complexity quantities. <p> Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields [13, 83] or a change detection operator [93]. The other techniques that process more than two views at the encoder require proportionally more storage based on the total number of views <ref> [30, 40, 47, 89] </ref>. 6.5 Experimental Results The improved interpolation technique was used to synthesize intermediate viewpoints between numerous views in the compressed multi-view sequences described in Section 2.4. <p> Fujii and Harashima reported only 17 dB for the interpolation of a similar frame in this sequence <ref> [30] </ref>. 13 The absolute frame difference from using the repiti-tion of frame 0 and our interpolation technique for this image are shown in Fig. 6.11. The frame repitition results indicate that a majority of the image is displaced between frames.
Reference: [31] <author> B. Gerod, </author> <title> The efficiency of motion-compensating prediction for hybrid coding of video sequences, </title> <journal> IEEE J. Select. Areas Commun., </journal> <volume> vol. 5, </volume> <pages> pp. 1140-1154, </pages> <month> Aug. </month> <year> 1987 </year>
Reference-contexts: For relatively high sampling rates and moderate camera and object motion, the compression gain achievable from the prediction process is quite impressive (an order of magnitude increase in the prediction peak signal-to-noise ratio over simple frame repetition is common <ref> [31, 44] </ref>). Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint [5, 15, 23, 59, 86, 102].
Reference: [32] <author> A. Gersho and R. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <address> Boston: </address> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: The residual encoder may include numerous operations, such as: the transformation of the residual image data to a generalized frequency domain (DCT [18] and wavelet transformations [49]), and the application of both lossy (scalar and vector quantization <ref> [32] </ref>) and lossless (run-length and entropy coding [42]) compression techniques. The choice of which technique (s) to use depends on the bit rate, complexity and reconstruction fidelity constraints of the system along with the signal content. <p> Finite-state vector quantization (FSVQ) and predictive vector quantization (PVQ) are capable of exploiting inter-vector correlation <ref> [32, 39, 69] </ref>. As the name implies, FSVQ consists a finite number of states, where a distinct and relatively small codebook is associated with each state.
Reference: [33] <author> V. S. Grinberg, G. Podnar and M. W. Siegel, </author> <title> Geometry of binocular imaging, in Proc. Stereoscopic Displays and Virtual Reality Systems, </title> <booktitle> SPIE, </booktitle> <volume> vol. 2177, </volume> <pages> pp. 56-65, </pages> <year> 1994. </year>
Reference-contexts: Binocular imagery, which, on a suitable display, precisely replicates the geometry of human vision, is obtained from two identical cameras separated by a horizontal distance, which is equal to the inter-ocular separation (typically 65 mm), and oriented with parallel camera axes and coplanar image sensors <ref> [33] </ref>. By presenting the appropriate views to the corresponding left and right-eyes of a viewer, two slightly different perspective views of the scene are imaged on each retina. <p> It should be noted that the point p need not, and sometimes should not, lie at the center of the image sensor plane. Offsets from the center are useful in binocular imaging applications to provide geo metrically-correct stereo-vision <ref> [33] </ref>. I u v nT, ,( ) u v,( ) U V 19 e Camera axis x z location. <p> To achieve geometrically correct stereo-vision, in the sense that the viewer sees what would be seen by the naked-eye, the cameras capturing the binocular image pair must be separated by the viewers inter-ocular separation <ref> [33] </ref>. However, this quantity is viewer dependent and individuals often prefer varying degrees of depth perception based on individual stereoscopic viewing ability and the range of depth present in the scene [97].
Reference: [34] <author> E. Grosso and M. Tistarelli, </author> <title> Active/dynamic stereo vision, </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 17, no. 9, </volume> <pages> pp. 868-879, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems <ref> [34, 52, 91] </ref>, scene modeling [46], and object-based compression [61, 72, 94]. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [35] <author> S. Gupta and A. Gersho, </author> <title> Feature predictive vector quantization of multispectral images, </title> <journal> IEEE Trans. Geoscience and Remote Sensing, </journal> <volume> vol. 30, no. 3, </volume> <pages> pp. 491-501, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Additional applications may use cameras that vary not only in their viewpoint, but also have vary The cameras (C ij ) are arranged with parallel axes and coplanar image sensors. C 11 Dc y C 21 C I2 C 1J 4 ing scale [101] and spectral bandwidth selectivity parameters <ref> [5, 35] </ref>. Regardless of their ultimate use, it is likely that these signals will need to be transmitted and/or stored in order to be displayed at a remote location, re-displayed at a future time, or processed off-line. <p> we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>.
Reference: [36] <author> S. Gupta and A. Gersho, </author> <title> Nonlinear predictive vector quantization of multispectral imagery, </title> <booktitle> in Proc. Twenty-Fourth Asilomar Conf. Signals, Systems and Computers, </booktitle> <pages> pp. 331-335, </pages> <year> 1990. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> The design of a finite-state or a predictive vector quantizer respectively involves the joint optimization of either the next-state/state codebook or the predictor/residual-codebook. An interesting, nonlinear predictive VQ (NLPVQ) technique was presented in <ref> [36] </ref> for the compression of multispectral imagery. A single spectral band is extracted from the multi-spectral imagery, and is defined as the feature band. Vectors within the feature band are coded using a standard vector quantizer. <p> This coupling also allows for the predictor and quantizer to be jointly optimized in the codebook design phase. While techniques for adaptive coding without side information have appeared in the literature <ref> [36, 70, 85, 105] </ref>, these methods base their adaptation on previously decoded signal; our approach utilizes the current signal. The decoder is able to follow this adaptation since it is explicitly described in the transmitted index.
Reference: [37] <author> C. Hansen, N. Ayache and F. Lustman, </author> <title> Efficient depth estimation using trinocular stereo, in Proc. Sensor Fusion: Spatial Reasoning and Scene Interpretation, </title> <address> Cambridge, MA, </address> <booktitle> SPIE, </booktitle> <volume> vol. 1003, </volume> <pages> pp. 124-131, </pages> <year> 1988. </year>
Reference-contexts: Only two views of the scene are required to calculate displacement information for unoccluded regions. Additional views are useful for autocalibration [20, 107], reducing ambiguity in the displacement estimation process <ref> [37, 89] </ref>, and minimizing the number and area of occluded regions [30]. 1.1.4 Multi-view Bit Rate The described applications are but a sampling of the possible uses for multi-view video signals.
Reference: [38] <author> R. Hopkins, </author> <title> Digital terrestrial HDTV for North America: The Grand Alliance HDTV system,, </title> <journal> IEEE Trans. Consumer Electronics, </journal> <volume> vol. 40, no. 3, </volume> <pages> pp. 185-198, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Using the digital video transmitter described in <ref> [38] </ref>, a compression factor of at least 33:1 is required to broadcast this multi-view signal in a 6 MHz channel, with a service area comparable to the NTSC service area.
Reference: [39] <author> C.-H. Hsieh and J.-S. Shue, </author> <title> Frame adaptive finite-state vector quantization for image sequence coding, Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 7, </volume> <pages> pp. 13-26, </pages> <year> 1995. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> Finite-state vector quantization (FSVQ) and predictive vector quantization (PVQ) are capable of exploiting inter-vector correlation <ref> [32, 39, 69] </ref>. As the name implies, FSVQ consists a finite number of states, where a distinct and relatively small codebook is associated with each state. <p> These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. While this block size is larger than that typically used in vector quantization implementations for image sequence coding <ref> [19, 39, 58] </ref>, we justify this selection with our requirement of low bit rate coding in the region of less than 0.25 bits-per-pixel, and the consistency with the macroblock/block partitioning of image frames in the MPEG and H.26x standards. <p> We speculate that a method to adapt the codebook will alleviate some of the problems associated codebook generation, in particular the codebook design time and over-training. For example, we plan to apply the concepts of super-codebooks [106], codebook replenishment <ref> [39] </ref>, and adaptive filter techniques [85] to our vector restoration coder. 124 Chapter 6 Interpolation from a Noisy Displacement Vector Field This chapter considers the problem of interpolating intermediate views 1 from two decoded image frames captured by spatially-offset cameras.
Reference: [40] <author> R. Hsu, K. Kodama and H. Harashima, </author> <title> View interpolation using epipolar plane images, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 745-749, </pages> <year> 1994. </year>
Reference-contexts: Numerous techniques for the synthesis of intermediate views have been reported in the literature. A common approach is through the generation of epipolar plane images (EPI) <ref> [30, 40, 47] </ref>. <p> We define the two exterior views of periodic MVSs as the views where is at an extremum; all other views are classified as interior views. The majority of prior research on the compression of more than two views has dealt with nonarbitrary viewpoints with zero phase offset <ref> [7, 30, 40, 47] </ref>. These periodic signals require a very high camera complexity to generate the multitude of views, and they possess a simple relationship between the interior and exterior views. <p> structures is non-trivial, we feel that the high compression ratios often reported for non-arbitrary q m ( ) Dc x 0 0 0 23 v a. m 19 plane image (EPI) for middle row of stacked images. u Views EPI row 24 viewpoints are obtained by essentially oversampling the scene <ref> [7, 30, 40, 47] </ref>. To avoid the generation and compression of the inherently redundant interior views, we wish to capture and encode only the exterior views and to accurately interpolate the interior views. <p> View 1 View 0 (blackened) and ambiguously-referenced (lightly-shaded) image regions. View 1 View 0 b 1 b. 132 A common interpolation approach is through the generation of epipolar plane images <ref> [30, 40, 47] </ref>. We have discussed the various advantages and disadvantages of these non-arbitrary multi-view signals in detail in Section 2.2. Essentially, numerous views of the scene are captured and processed at the encoder to estimate the displacement variation of objects through the set of image frames. <p> Both EPI-based techniques reported by Katayama, et. al, and Hsu, et. al, handle holes in the interpolated image by simple region filling with adjacent pixel data. In <ref> [40] </ref>, ambiguously-referenced image regions are addressed through the use of aspect graphs [25] to layer scene objects based on range; hidden surface removal then is performed to ensure that foreground objects are mapped in front of background objects. <p> Our technique, therefore, is applicable to systems with a low-complexity decoder constraint. The EPI-based techniques generate the dense interpolated field using numerous, closely-spaced views of the scene; the individual techniques used vary from nine [47] to 180 views <ref> [40] </ref>. Hence, we assume that the displacement field relating corresponding points within the extreme views is generated at the encoder. The interpolated field is calculated at the decoder based on the relative location of the desired viewpoint. <p> Hsu, et. al., use an edge detection and region growing procedure to estimate the displacement field. Although the exact method was not described, region growing is typically considered a difficult (read computationally costly) procedure <ref> [40] </ref>. In [47], the interpolated field is generated by searching for the minimum variance trace line for each pixel. To constrain the search, minimum and maximum trace line slopes are specified. <p> Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields [13, 83] or a change detection operator [93]. The other techniques that process more than two views at the encoder require proportionally more storage based on the total number of views <ref> [30, 40, 47, 89] </ref>. 6.5 Experimental Results The improved interpolation technique was used to synthesize intermediate viewpoints between numerous views in the compressed multi-view sequences described in Section 2.4.
Reference: [41] <author> C.-L. Huang and T.-T. Chao, </author> <title> Motion-compensated interpolation for scan rate up-conversion, </title> <journal> Optical Engineering, </journal> <volume> vol. 35, no. 1, </volume> <pages> pp. 166-176, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views.
Reference: [42] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <address> Englewood Cliffs NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1989. </year> <month> 187 </month>
Reference-contexts: The residual encoder may include numerous operations, such as: the transformation of the residual image data to a generalized frequency domain (DCT [18] and wavelet transformations [49]), and the application of both lossy (scalar and vector quantization [32]) and lossless (run-length and entropy coding <ref> [42] </ref>) compression techniques. The choice of which technique (s) to use depends on the bit rate, complexity and reconstruction fidelity constraints of the system along with the signal content. <p> However, we know of no widely-accepted distortion measure that provides a quantitative result that coincides with subjective image quality. We will thus use the often-criticized class of distortion measures that calculate the L p -norm between the intensity values of the original and reconstructed images <ref> [42] </ref>. These measures are easily calculated and allow for direct comparisons with other research results. In its most basic form, we define distortion as, (2.9) 5. Since the relative values of these weights are system dependent, we leave their assignment to sys tem designers. <p> The common approach then has been to use a non-optimal, data-independent transform that is related to the statistics of all image sources. For example, the Discrete Cosine Transform (DCT) approximates the KLT for highly correlated sources <ref> [42, 80] </ref>. This is the transform used in the residual coding stage of the MPEG and H.26x video compression standards. Although the residual image often contains considerable contrast and abrupt edges (i.e., it has relatively low spatial correlation), the DCT repeatedly has been shown to achieve adequate, mid-bit rate performance. <p> Specifically, we seek the predictor that minimizes, (5.9) This is a standard least squares estimation problem, with a modification incorporating the known partition, and the solution is <ref> [42] </ref>, (5.10) 6. The overall goal of the encoding process obviously is the minimization of the distortion between the original and reconstructed images. We may reformulate Eq. (5.8) to reect this goal by substituting for z, which yields for the bracketed-term. <p> Also, since H i is in general a shift-variant filter, it is not necessarily a block-Toeplitz matrix <ref> [42] </ref>. If (i.e., each partial predicted vector is identical), the auto-covariance matrix and its inverse need to be computed only once for the generation of H i in Eq. (5.15).
Reference: [43] <author> A. K. Jain, </author> <title> Advances in mathematical models for image processing, </title> <journal> Proc. IEEE, </journal> <volume> vol. 69, no. 5, </volume> <pages> pp. 502-528, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: A predictive coder attempts to exploit this memory to provide a more concise represen 1. See [8] and <ref> [43] </ref> for a complete description of noncausal models. I u v m n, , ,( ) a ij u v,( ) I u i v j m n, , ,( ) w occ u v,( )+= I u v m n, , ,( ) 44 tation of the multi-view signal. <p> We note a few observations on techniques used to obtain a compact representation for occluded regions. The noncausal model lends itself to transform-based techniques <ref> [43] </ref>. This is the approach taken by the MPEG class of video compression standards [2], where intraframe DCT-based compression is performed on poorly predicted (read occluded) blocks within a predictively-coded frame. The use of a fixed transform implies the assumption of a stationary image model.
Reference: [44] <author> J. R. Jain and A. K. Jain, </author> <title> Displacement measurement and its application in interframe image coding, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 29, </volume> <pages> pp. 1799-1808, </pages> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: For relatively high sampling rates and moderate camera and object motion, the compression gain achievable from the prediction process is quite impressive (an order of magnitude increase in the prediction peak signal-to-noise ratio over simple frame repetition is common <ref> [31, 44] </ref>). Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint [5, 15, 23, 59, 86, 102].
Reference: [45] <author> B. Julesz, </author> <title> Foundations of Cyclopean Perception, </title> <publisher> Chicago: The University of Chicago Press, </publisher> <year> 1971. </year>
Reference-contexts: The brain then fuses these images into one view and the viewer experiences the sensation of stereopsis, which provides added realism through improved depth perception <ref> [45, 95] </ref>. 1.1.2 Motion Parallax For further improvements in realism, the binocular imaging system can be extended to provide the viewer with the depth cue of motion parallax [74, 99]. 1 Motion parallax, which provides the distinction between binocular and three-dimensional imagery, can be simulated by obtaining multiple, closely-spaced views of
Reference: [46] <author> T. Kanade and M. Okutomi, </author> <title> A stereo matching algorithm with an adaptive window: Theory and experiment, </title> <booktitle> in Proc. Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 1088-1095, </pages> <year> 1991. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems [34, 52, 91], scene modeling <ref> [46] </ref>, and object-based compression [61, 72, 94]. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [47] <author> A. Katayama, K. Tanaka, T. Oshino and H. Tamura, </author> <title> A viewpoint dependent stereoscopic display using interpolation of multi-viewpoint images, in Proc. Stereoscopic Displays and Virtual Reality Systems II, </title> <booktitle> SPIE, </booktitle> <volume> vol. 2409, </volume> <pages> pp. 11-20, </pages> <year> 1995. </year>
Reference-contexts: Numerous techniques for the synthesis of intermediate views have been reported in the literature. A common approach is through the generation of epipolar plane images (EPI) <ref> [30, 40, 47] </ref>. <p> The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views. <p> We define the two exterior views of periodic MVSs as the views where is at an extremum; all other views are classified as interior views. The majority of prior research on the compression of more than two views has dealt with nonarbitrary viewpoints with zero phase offset <ref> [7, 30, 40, 47] </ref>. These periodic signals require a very high camera complexity to generate the multitude of views, and they possess a simple relationship between the interior and exterior views. <p> structures is non-trivial, we feel that the high compression ratios often reported for non-arbitrary q m ( ) Dc x 0 0 0 23 v a. m 19 plane image (EPI) for middle row of stacked images. u Views EPI row 24 viewpoints are obtained by essentially oversampling the scene <ref> [7, 30, 40, 47] </ref>. To avoid the generation and compression of the inherently redundant interior views, we wish to capture and encode only the exterior views and to accurately interpolate the interior views. <p> View 1 View 0 (blackened) and ambiguously-referenced (lightly-shaded) image regions. View 1 View 0 b 1 b. 132 A common interpolation approach is through the generation of epipolar plane images <ref> [30, 40, 47] </ref>. We have discussed the various advantages and disadvantages of these non-arbitrary multi-view signals in detail in Section 2.2. Essentially, numerous views of the scene are captured and processed at the encoder to estimate the displacement variation of objects through the set of image frames. <p> A more cautious approach to interpolating image regions with more than one possible displacement trajectory is taken in <ref> [47] </ref>. Here, the selection is based on a measure of the reliability of the intersecting displacement estimates; the displacement estimates from regions that have fewer valid trace lines through the EPI are assumed to be more accurate than those that have numerous valid traces. <p> This is particularly impressive since the search range for spatially-offset views is one-dimensional. Our technique, therefore, is applicable to systems with a low-complexity decoder constraint. The EPI-based techniques generate the dense interpolated field using numerous, closely-spaced views of the scene; the individual techniques used vary from nine <ref> [47] </ref> to 180 views [40]. Hence, we assume that the displacement field relating corresponding points within the extreme views is generated at the encoder. The interpolated field is calculated at the decoder based on the relative location of the desired viewpoint. <p> Hsu, et. al., use an edge detection and region growing procedure to estimate the displacement field. Although the exact method was not described, region growing is typically considered a difficult (read computationally costly) procedure [40]. In <ref> [47] </ref>, the interpolated field is generated by searching for the minimum variance trace line for each pixel. To constrain the search, minimum and maximum trace line slopes are specified. <p> Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields [13, 83] or a change detection operator [93]. The other techniques that process more than two views at the encoder require proportionally more storage based on the total number of views <ref> [30, 40, 47, 89] </ref>. 6.5 Experimental Results The improved interpolation technique was used to synthesize intermediate viewpoints between numerous views in the compressed multi-view sequences described in Section 2.4.
Reference: [48] <author> J.-S. Kim and R.-H. Park, </author> <title> Local motion-adaptive interpolation technique based on block matching algorithms, Signal Processing: </title> <journal> Image Communications, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 519-528, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views. <p> Motion-compensated interpolation is used for the applications of frame rate conversion, de-interlacing and frame skipping <ref> [13, 48, 83, 93] </ref>. Since constant, translational object motion is often assumed, interpolation of unoccluded regions is performed in the same manner as described in Section 6.1.2. Cafforio, et. al., achieve a highly accurate motion field by using bidirectional motion estimates obtained from a pel-recursive algorithm [13].
Reference: [49] <author> Y. Kim, I. Choi, I. Lee, T. Yun, and K. T. Park, </author> <title> Wavelet transform image compression using human visual characteristics and a tree structure with a height attribute, </title> <journal> Optical Engineering, </journal> <volume> vol. 35, no. 1, </volume> <pages> pp. 204-212, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The residual encoder may include numerous operations, such as: the transformation of the residual image data to a generalized frequency domain (DCT [18] and wavelet transformations <ref> [49] </ref>), and the application of both lossy (scalar and vector quantization [32]) and lossless (run-length and entropy coding [42]) compression techniques. The choice of which technique (s) to use depends on the bit rate, complexity and reconstruction fidelity constraints of the system along with the signal content.
Reference: [50] <author> Y. H. Kim and J. W. Modestino, </author> <title> Adaptive entropy-coded pruned tree-structured predictive vector quantization of images, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 41, no. 1, </volume> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> We briey describe a sampling of these variants, which will prove useful in the latter sections of this chapter. A tree-structured vector quantizer (TSVQ) constrains the codebook to consist of a tree of reproduction vectors <ref> [50, 58] </ref>. A balanced binary tree typically is employed (i.e., a tree with all of its terminal nodes at the same layer), and only a sequence of binary searches is per formed, greatly reducing the encoding process compared to the full-search VQ. <p> The restoration-based coder is essentially a locally linear (affine) globally nonlinear, adaptive, shift-variant filter (cluster-wise linear). Scalar and vector-restoration are, respectively, related to Adaptive Differential Pulse Code Modulation (ADPCM) and adaptive predictive vector quantization (APVQ) <ref> [50, 108] </ref>. In these techniques, the prediction and quantization steps are decoupled. The major contribution common to both SR and VR is that the adaptation does not require the transmission of any side information.
Reference: [51] <author> B. Kost and S. Pastoor, </author> <title> Visibility thresholds for disparity quantization errors in stereoscopic displays, </title> <journal> Proc. SID, </journal> <volume> vol. 32, no. 2, </volume> <pages> pp. 165-170, </pages> <year> 1991. </year>
Reference-contexts: Psychovisual studies of the capabilities of the human visual system to discern variations in object displacement indicate that over ten distinct views per inter-ocular separation are needed to ensure smooth and realistic motion parallax <ref> [51, 75] </ref>. 2 The camera configuration of a system capable of providing both horizontal and vertical motion parallax is shown in Fig. 1.1.
Reference: [52] <author> E. Krotkov, M. Hebert and Simmons, </author> <title> Stereo perception and dead reckoning for a prototype lunar rover, </title> <booktitle> Autonomous Robots, </booktitle> <volume> vol. 2, no. 4, </volume> <pages> pp. 313-331, </pages> <year> 1995. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems <ref> [34, 52, 91] </ref>, scene modeling [46], and object-based compression [61, 72, 94]. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [53] <author> D. Kundur and D. Hatzinakos, </author> <title> Blind image deconvolution, </title> <journal> IEEE Signal Processing Mag., </journal> <volume> vol. 13, no. 3, </volume> <pages> pp. 43-64, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Consider a pixel at in the residual image, 5. For the task of blind deconvolution, only limited knowledge of the degradation mechanism is avail able. See <ref> [53] </ref> for a tutorial on this subject. U V ( ) Z Y X= u v,( ) 100 given by z, that is to be encoded using information in the displacement-compensated predicted image.
Reference: [54] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> An algorithm for vector quantization design, </title> <journal> IEEE Tran. Comm., </journal> <volume> vol. 28, no. 1, </volume> <pages> pp. 84-95, </pages> <year> 1980. </year>
Reference-contexts: The vec tor quantization codebook frequently is designed from a set of representative training vectors using a clustering algorithm, such as the generalized Lloyd algorithm (GLA) <ref> [54] </ref>. In its most basic form (full-search vector quantization), the codebook is unstructured, and the encoding process entails a full-search for the minimum-distortion reproduction vector. For high bit rates, this search is prohibitive. <p> We next examine the design of the restoration code book. An iterative, clustering algorithm can be applied to design the codebook of restoration operations from a representative set of pairs, similar to the well-known Generalized Lloyd Algorithm (GLA) for vector quantization codebook design <ref> [54] </ref>. In each iteration, the training pairs are first partitioned by encoding with the current codebook. The restoration operation associated with partition i, (i.e., S i ), then is updated by computing the function that minimizes the partial mean squared error over the partition. <p> These values can be numerically estimated by computing time-averages over the partitioned training data. Higher rate codebooks also are obtained in a manner analogous to that of vector quantization codebook splitting <ref> [54] </ref>. At initialization, we set and define the initial codebook, , as the solution of Eq. (5.12) for the entire training set. Given the codebook , we split each codebook entry by perturbing c i into the two close values and .
Reference: [55] <author> L. Lipton, </author> <booktitle> The CrystalEyes Handbook, </booktitle> <address> San Rafael CA: </address> <publisher> StereoGraphics Corporation, </publisher> <year> 1991. </year>
Reference-contexts: Intermediate views also can be used to ensure that stereoscopic imagery is comfortable to view. Discomfort is often experienced when viewing stereoscopic images on a two-dimensional display device. The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort <ref> [55, 62, 65, 74, 103] </ref>. An additional source of eye strain is related to the viewers ability to fuse binocular information [97].
Reference: [56] <author> J. Liu and R. Skerjanc, </author> <title> Stereo and motion correspondence in a sequence of stereo images, Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 5, </volume> <pages> pp. 305-318, </pages> <year> 1993. </year>
Reference-contexts: The displacement range of corresponding points between two views obtained at the same time instant is constrained to one-dimension, which is referred to as the epipolar line <ref> [9, 11, 56, 92] </ref>. 4 Since the camera configuration is rarely known to 3. Due to their overwhelming use, we will consider only block-based (fixed and variable sized) displacement compensated prediction schemes in this thesis. As such, we shall interchangeably use the terms prediction process and displacement estimation/compensation. 4.
Reference: [57] <author> C. Loefer, A. Ligtenberg and G. Moschytz, </author> <title> Practical fast 1-D DCT algorithms with 11 multiplications, </title> <booktitle> in Proc. Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 988-991, </pages> <year> 1989. </year>
Reference-contexts: Although the residual image often contains considerable contrast and abrupt edges (i.e., it has relatively low spatial correlation), the DCT repeatedly has been shown to achieve adequate, mid-bit rate performance. Also, fast implementations of the DCT exist <ref> [57, 80] </ref>, and it is possible to piggy-back off of existing hardware used to perform DCT-based intraframe compression [60]. 94 A solution to some of the difficulties associated with using the KLT recently was presented in [24], where a finite set of transforms were designed that approximate the optimal transform for <p> The two-dimensional DCT is a separable transform, which can be performing by a applying series of 1-D DCTs on the rows and columns of the image block. Numerous fast implementations exist; for our simulations, we used the alternative method described in <ref> [57] </ref>, which requires 12 multiplications and 32 additions per 1-D DCT. For an image block, this amounts to 11 operations-per-pixel.
Reference: [58] <author> T. Lookabaugh, E. A. Riskin, P. A. Chou and R. M. Gray, </author> <title> Variable rate vector quantization for speech, image, and video compression, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 41, no. 1, </volume> <pages> pp. 186-199, </pages> <month> Jan. </month> <year> 1993. </year> <month> 188 </month>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> We briey describe a sampling of these variants, which will prove useful in the latter sections of this chapter. A tree-structured vector quantizer (TSVQ) constrains the codebook to consist of a tree of reproduction vectors <ref> [50, 58] </ref>. A balanced binary tree typically is employed (i.e., a tree with all of its terminal nodes at the same layer), and only a sequence of binary searches is per formed, greatly reducing the encoding process compared to the full-search VQ. <p> These blocks were obtained by sectioning the luminance and two chrominance components of the residual image into nonoverlapping blocks. While this block size is larger than that typically used in vector quantization implementations for image sequence coding <ref> [19, 39, 58] </ref>, we justify this selection with our requirement of low bit rate coding in the region of less than 0.25 bits-per-pixel, and the consistency with the macroblock/block partitioning of image frames in the MPEG and H.26x standards.
Reference: [59] <author> M. E. Lukacs, </author> <title> Predictive coding of multi-viewpoint image sets, </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 521-524, </pages> <year> 1986. </year>
Reference-contexts: Additional views are easily incorporated into this methodology due to the frame-based approach, where the number of frames increases linearly with the number of views. This extension of the hybrid coder structure has been reported in numerous systems dealing with multi-view signals <ref> [15, 59, 78, 86, 87] </ref>. The frames from each view may be treated equally or an implicit ranking may be established between the individual views. <p> Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration.
Reference: [60] <author> J. N. Mailhot and H. Derovanessian, </author> <title> Grand Alliance HDTV video encoder, </title> <journal> IEEE Trans. Consumer Electronics, </journal> <volume> vol. 41, no. 4, </volume> <pages> pp. 1014-1019, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: For single-view systems, irrelevancy reduction typically amounts to performing spatial and temporal-frequency dependent quantization. Example techniques include devoting more bits to lower spatial-frequency components of the signal, subsampling the chrominance information, and relaxing fidelity requirements for high-motion regions <ref> [60] </ref>. These methods may be applied to multi-view signals intended for viewing by a human observer. Also, binocular systems often exploit the singularity-of-vision property of the HVS to remove irrelevant information [22]. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients. <p> Also, fast implementations of the DCT exist [57, 80], and it is possible to piggy-back off of existing hardware used to perform DCT-based intraframe compression <ref> [60] </ref>. 94 A solution to some of the difficulties associated with using the KLT recently was presented in [24], where a finite set of transforms were designed that approximate the optimal transform for a broad class of image sources, i.e., it is universal in nature. <p> auxiliary information (see Fig. 6.1) is readily extracted from the coded bit-stream, which alleviates the need to perform a displacement estimation procedure at the decoder. 5 To facilitate the application of this work to standardized compression algorithms, we assume that the displacement field was estimated using a fixed-size, block-based technique <ref> [1, 2, 3, 4, 60] </ref>. While block-based techniques provide an adequate prediction and a compact representation of the displacement field, they are prone to serious estimation errors.
Reference: [61] <author> F. C. M. Martins and J. M. F. Moura, </author> <title> 3-D video compositing: Towards a compact representation for video sequences, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <pages> pp. 550-553, </pages> <year> 1995. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems [34, 52, 91], scene modeling [46], and object-based compression <ref> [61, 72, 94] </ref>. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [62] <author> T. Miyashita and T. Uchida, </author> <title> Cause of fatigue and its improvement in stereoscopic displays, </title> <journal> Proc. SID, </journal> <volume> vol. 31, no. 3, </volume> <pages> pp. 249-254, </pages> <year> 1990. </year>
Reference-contexts: Intermediate views also can be used to ensure that stereoscopic imagery is comfortable to view. Discomfort is often experienced when viewing stereoscopic images on a two-dimensional display device. The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort <ref> [55, 62, 65, 74, 103] </ref>. An additional source of eye strain is related to the viewers ability to fuse binocular information [97].
Reference: [63] <author> J. S. McVeigh, M. W. Siegel and A. G. Jordan, </author> <title> Intermediate view synthesis considering occluded and ambiguously referenced image regions, Signal Processing: </title> <journal> Image Communications, </journal> <note> accepted. </note>
Reference-contexts: This operation is straightforward and accurate for unoccluded regions where the exact displacement is known. The performance of displacement-compensated interpolation schemes degrades abruptly when a region is visible in only one view (occluded) and when displacement estimation errors occur <ref> [63] </ref>. Techniques for handling occlusions in single-view signals are provided in [83] and [93]. However, these methods are not applicable to multi-view imagery, and they do not achieve an adequate delineation of occlusion borders to avoid a subsequent residual encoding stage for image pairs with relatively large displacement ranges. <p> If a continuum of views between two extreme viewpoints are available, a viewer can dynamically select the inter-camera separation for comfort and preferred sense of depth in a manner similar to the adjustments of brightness and contrast found on most display devices <ref> [63, 67, 97] </ref>. Decreasing the camera separation also reduces the breakdown between the accommodation/convergence relationship. 126 inter-camera separation, (a) functional block diagram, (b) virtual camera positions of desired intermediate views. <p> The interpolated view is virtually indistinguishable from the original (Fig. 6.10). Accurate synthesis of the image borders and sides of the foreground tree were obtained, and a PSNR of over 28 dB was measured. 12 This represents an improvement of over 3.5 dB from our earlier work <ref> [63] </ref>, which did not include the preprocessing and self-synthesis steps.
Reference: [64] <author> J. S. McVeigh, M. W. Siegel and A. G. Jordan, </author> <title> Adaptive reference frame selection for generalized video signal coding, </title> <booktitle> in Proc. Digital Video Compression: Algorithms and Technologies 1996, SPIE, </booktitle> <volume> vol. 2668, </volume> <pages> pp. 441-449, </pages> <year> 1996. </year>
Reference-contexts: The gain that would be achieved through the use of the 9 most similar reference frame is independent of the type of frame-based prediction used <ref> [64] </ref>. Puri, et. al, allude to the effect of reference frame configuration on coding performance for stereoscopic video in [78]; yet, they do not discuss how the optimal reference should be selected.
Reference: [65] <author> J. S. McVeigh, M. W. Siegel and A. G. Jordan, </author> <title> Algorithm for automated eye-strain reduction in real stereoscopic images and sequences, </title> <booktitle> in Proc. Human Vision and Electronic Imaging, SPIE, </booktitle> <volume> vol. 2657, </volume> <pages> pp. 307-316, </pages> <year> 1996. </year>
Reference-contexts: Intermediate views also can be used to ensure that stereoscopic imagery is comfortable to view. Discomfort is often experienced when viewing stereoscopic images on a two-dimensional display device. The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort <ref> [55, 62, 65, 74, 103] </ref>. An additional source of eye strain is related to the viewers ability to fuse binocular information [97].
Reference: [66] <author> J. S. McVeigh, S.-W. Wu, M. W. Siegel and A. G. Jordan, </author> <title> Vector restoration for video coding, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 93-96, </pages> <year> 1995. </year>
Reference-contexts: For tree-structured vector restoration (TSVR), only a sequence of binary searches are performed, and the complexity is reduced considerably to: (5.22) 10. Vector restoration can be used to directly restore the original image, thereby avoiding the generation of the residual image (see <ref> [66] </ref> and footnote 6).
Reference: [67] <author> J. S. McVeigh, V. S. Grinberg and M. W. Siegel, </author> <title> Double buffering technique for binocular imaging in a window, in Proc. Stereoscopic Displays and Virtual Reality Systems II, </title> <booktitle> SPIE, </booktitle> <volume> vol. 2409, </volume> <pages> pp. 168-175, </pages> <year> 1995. </year>
Reference-contexts: If a continuum of views between two extreme viewpoints are available, a viewer can dynamically select the inter-camera separation for comfort and preferred sense of depth in a manner similar to the adjustments of brightness and contrast found on most display devices <ref> [63, 67, 97] </ref>. Decreasing the camera separation also reduces the breakdown between the accommodation/convergence relationship. 126 inter-camera separation, (a) functional block diagram, (b) virtual camera positions of desired intermediate views.
Reference: [68] <author> J. S. McVeigh and S.-W. Wu, </author> <title> Partial closed loop versus open loop motion estimation for HDTV compression, </title> <journal> Int. J. Imaging Systems and Technology, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 268-275, </pages> <year> 1994. </year>
Reference-contexts: The initial displacement vector fields were extracted from the coded bit-stream that described the predictive coding of one of the reference views from the other. The reference views were predicted using a full-search, block-based technique with full-pixel accuracy, where the displacement vectors were estimated using an open-loop structure <ref> [68] </ref>. Block sizes of either or pixels were used, depending on whether the view was a complete image frame or only a single field, i.e., sampled at twice the frame rate. The reconstructed quality of the reference views was such that they were indistinguishable from the original views.
Reference: [69] <author> N. M. Nasrabadi, C. Y. Choo and Y. S. Feng, </author> <title> Finite-state vector quantization of digital images, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 32, no. 5, </volume> <pages> pp. 2145-2154, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> Finite-state vector quantization (FSVQ) and predictive vector quantization (PVQ) are capable of exploiting inter-vector correlation <ref> [32, 39, 69] </ref>. As the name implies, FSVQ consists a finite number of states, where a distinct and relatively small codebook is associated with each state.
Reference: [70] <author> A. Ortega and M. Vetterli, </author> <title> Adaptive quantization without side information, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 3, </volume> <pages> pp. 856-860, </pages> <year> 1994. </year>
Reference-contexts: This coupling also allows for the predictor and quantizer to be jointly optimized in the codebook design phase. While techniques for adaptive coding without side information have appeared in the literature <ref> [36, 70, 85, 105] </ref>, these methods base their adaptation on previously decoded signal; our approach utilizes the current signal. The decoder is able to follow this adaptation since it is explicitly described in the transmitted index.
Reference: [71] <author> K. K. Pang and T. K. Tan, </author> <title> Optimum loop filter in hybrid coders, </title> <journal> IEEE Trans. Circuits and Systems for Video Tech., </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 158-167, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Slight variations in the prediction bit rate are possible due to the entropy coding of the displacement vectors. It is also customary to use a spatially-invariant prediction coefficient that approximates the correlation coefficient in Eq. (5.1) over an entire image or a class of images <ref> [1, 2, 3, 4, 71] </ref>. For either case, the overhead in terms of the complexity to estimate the prediction coefficient and the number of bits to transmit this quantity are inconsequential with respect to the displacement estimation and residual encoding operations.
Reference: [72] <author> S. Panis and M. Ziegler, </author> <title> Object-based coding using motion and stereo information, </title> <booktitle> in Proc. Picture Coding Symposium, </booktitle> <pages> pp. 308-312, </pages> <year> 1994. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems [34, 52, 91], scene modeling [46], and object-based compression <ref> [61, 72, 94] </ref>. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [73] <author> A. Papoulis, </author> <title> Probability, random variables, and stochastic processes, </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1991, </year> <journal> ch. </journal> <volume> 12. </volume> <pages> 189 </pages>
Reference-contexts: The displacement discontinuity then is a series of Kronecker delta functions located at object boundaries. 2 2. In statistical terms, the displacement along an image row (column) can be modeled as a generalized Poisson process, and the displacement discontinuity as a generalized Poisson impulse process <ref> [73] </ref>, where the location of the Poisson points are correlated between adjacent image rows (columns).
Reference: [74] <author> S. Pastoor, 3D-television: </author> <title> A survey of recent research results on subjective requirements, Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 21-32, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: fuses these images into one view and the viewer experiences the sensation of stereopsis, which provides added realism through improved depth perception [45, 95]. 1.1.2 Motion Parallax For further improvements in realism, the binocular imaging system can be extended to provide the viewer with the depth cue of motion parallax <ref> [74, 99] </ref>. 1 Motion parallax, which provides the distinction between binocular and three-dimensional imagery, can be simulated by obtaining multiple, closely-spaced views of the scene and then presenting the appropriate binocular image pair based on the viewers position. <p> Intermediate views also can be used to ensure that stereoscopic imagery is comfortable to view. Discomfort is often experienced when viewing stereoscopic images on a two-dimensional display device. The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort <ref> [55, 62, 65, 74, 103] </ref>. An additional source of eye strain is related to the viewers ability to fuse binocular information [97].
Reference: [75] <author> S. Pastoor and K. Schenke, </author> <title> Subjective assessments of the resolution of viewing directions in a multi-viewpoint 3D TV system, </title> <journal> Proc. SID, </journal> <volume> vol. 30, no. 3, </volume> <pages> pp. 217-223, </pages> <year> 1989. </year>
Reference-contexts: Psychovisual studies of the capabilities of the human visual system to discern variations in object displacement indicate that over ten distinct views per inter-ocular separation are needed to ensure smooth and realistic motion parallax <ref> [51, 75] </ref>. 2 The camera configuration of a system capable of providing both horizontal and vertical motion parallax is shown in Fig. 1.1. <p> Specifically, the required number of views depends on the depth range of the scene and the visibility threshold for parallax shifts. A threshold of 1.15 min of arc was estimated experimentally in <ref> [75] </ref>. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems [34, 52, 91], scene modeling [46], and object-based compression [61, 72, 94].
Reference: [76] <author> M. G. Perkins, </author> <title> Data compression of stereopairs, </title> <journal> IEEE Trans. Comm, </journal> <volume> vol. 40, no. 4, </volume> <pages> pp. 684-696, </pages> <month> Apr. </month> <year> 1992. </year>
Reference: [77] <author> A. Puri and B. </author> <title> Haskell, Straw man proposal for multi-view profile, </title> <address> ISO/IEC JTC1/SC29/ WG11 MPEG95/485, </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: The latter approach is taken by the recently proposed multi-view extension to the MPEG-2 standard, which uses the temporal-scalability option of the standard to accommodate multiple views of the scene <ref> [77] </ref>. The MPEG-2 Multi-View Profile (MVP) defines one view as the base layer and all additional views as enhancement layers.
Reference: [78] <author> A. Puri, R. V. Kollarits and B. G. </author> <title> Haskell, Stereoscopic video compression using temporal scalability, </title> <booktitle> in Proc. Visual Communications and Image Processing, SPIE, </booktitle> <volume> vol. 2501, </volume> <pages> pp. 745-756, </pages> <year> 1995. </year>
Reference-contexts: Additional views are easily incorporated into this methodology due to the frame-based approach, where the number of frames increases linearly with the number of views. This extension of the hybrid coder structure has been reported in numerous systems dealing with multi-view signals <ref> [15, 59, 78, 86, 87] </ref>. The frames from each view may be treated equally or an implicit ranking may be established between the individual views. <p> The gain that would be achieved through the use of the 9 most similar reference frame is independent of the type of frame-based prediction used [64]. Puri, et. al, allude to the effect of reference frame configuration on coding performance for stereoscopic video in <ref> [78] </ref>; yet, they do not discuss how the optimal reference should be selected. The development of an algorithm to overcome this shortcoming is one of the major contributions of this thesis. 1.3.1.2 Residual Encoding Ideally, the difference (residual) between the original and predicted images would be spatially-uncorrelated. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>. <p> In the following discussion, we highlight concepts that will be potentially useful for our task. 5.3.1 Transform-based with Scalar Quantization A popular method for compressing the residual image is a transform-based technique <ref> [1, 2, 3, 4, 5, 22, 60, 78] </ref>. Here, the residual image data is transformed to a generalized frequency domain, with the goal of decorrelating the signal and compacting most of the energy in the signal into a small number of transform coefficients.
Reference: [79] <author> R. L. de Queiroz and K. R. Rao, </author> <title> Variable block size lapped transforms, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 290-293, </pages> <year> 1995. </year>
Reference-contexts: A combination of both scalar quantization and entropy encoding then is applied to these coefficients. To reduce the computational complexity of the transformation, these techniques typically are applied to relatively small-sized image blocks, where the size of the block used is a trade-off between spatial and frequency resolution <ref> [79] </ref>. The Karhunen-Love Transform (KLT) is known to achieve optimal decorrelation and energy compaction for a class of image sources [18].
Reference: [80] <author> K. R. Rao and P. Yip, </author> <title> Discrete Cosine Transform - Algorithms, Advantages, Applications, </title> <publisher> London: Academic Press, </publisher> <year> 1990, </year> <note> appendix A.2. </note>
Reference-contexts: The common approach then has been to use a non-optimal, data-independent transform that is related to the statistics of all image sources. For example, the Discrete Cosine Transform (DCT) approximates the KLT for highly correlated sources <ref> [42, 80] </ref>. This is the transform used in the residual coding stage of the MPEG and H.26x video compression standards. Although the residual image often contains considerable contrast and abrupt edges (i.e., it has relatively low spatial correlation), the DCT repeatedly has been shown to achieve adequate, mid-bit rate performance. <p> Although the residual image often contains considerable contrast and abrupt edges (i.e., it has relatively low spatial correlation), the DCT repeatedly has been shown to achieve adequate, mid-bit rate performance. Also, fast implementations of the DCT exist <ref> [57, 80] </ref>, and it is possible to piggy-back off of existing hardware used to perform DCT-based intraframe compression [60]. 94 A solution to some of the difficulties associated with using the KLT recently was presented in [24], where a finite set of transforms were designed that approximate the optimal transform for
Reference: [81] <author> C. </author> <title> Reader, MPEG4: coding for content, interactively, and universal accessibility, </title> <journal> Optical Engineering, </journal> <volume> vol. 35, no. 1, </volume> <pages> pp. 104-108, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Content-based coding is at the core of the MPEG-4 video compression standard currently under development, which contains a provision for the compression of multiple, concurrent views of the scene <ref> [81] </ref>. This object-based methodology efficiently handles multi-view signals since an object visible in separate views can be represented compactly as a single object, and the various perspective views can by synthesized from the object model and knowledge of the camera viewpoints. <p> This capability also can be used to generate views from virtual viewpoints, perform content-based retrieval, and actually alter the content and evolution of the video signal <ref> [81] </ref>.
Reference: [82] <author> J. Ribas-Corbera and D. L. Neuhoff, </author> <title> Optimal bit allocations for lossless video coders: Motion vectors vs. difference frames, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 3, </volume> <pages> pp. 180-183, </pages> <year> 1995. </year>
Reference-contexts: Ideally, a joint rate-distortion minimization would be performed to calculate the optimal bit allocation between these two processes. A technique to perform this optimization was presented in <ref> [82] </ref>, for the case of lossless video coders. Here, an approximation of the total rate for encoding the current frame was related to the resolution of the estimated motion vectors. This optimization is decidedly more complex for lossy encoders.
Reference: [83] <author> J. Ribas-Corbera and J. Sklansky, </author> <title> Interframe interpolation of cinematic sequences, </title> <editor> J. </editor> <booktitle> Visual Communication and Image Representation, </booktitle> <volume> vol. 4, no. 4, </volume> <pages> pp. 392-406, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views. <p> The performance of displacement-compensated interpolation schemes degrades abruptly when a region is visible in only one view (occluded) and when displacement estimation errors occur [63]. Techniques for handling occlusions in single-view signals are provided in <ref> [83] </ref> and [93]. However, these methods are not applicable to multi-view imagery, and they do not achieve an adequate delineation of occlusion borders to avoid a subsequent residual encoding stage for image pairs with relatively large displacement ranges. <p> Previously reported techniques for the detection of occluded regions typically use a measure of symmetry between estimated, bidirectional displacement vectors fields as the decision threshold <ref> [13, 83] </ref>. The problem with using these occlusion estimation techniques for our reference frame selection task is that the estimation of a displacement vector field is equivalent to performing a frame-based prediction; i.e., we have merely reformulated the exhaustive search method. <p> Motion-compensated interpolation is used for the applications of frame rate conversion, de-interlacing and frame skipping <ref> [13, 48, 83, 93] </ref>. Since constant, translational object motion is often assumed, interpolation of unoccluded regions is performed in the same manner as described in Section 6.1.2. Cafforio, et. al., achieve a highly accurate motion field by using bidirectional motion estimates obtained from a pel-recursive algorithm [13]. <p> While this assumption may be valid for image sequences, it is not valid for spatially-offset views only a single-depth plane will exhibit zero displacement. A motion-compensated interpolation scheme that considers object scaling was presented by Ribas-Corbera and Sklansky <ref> [83] </ref>. They used a bidirectional optical ow analysis and Cafforios occlusion detection algorithm to generate the displacement field containing only likely unoccluded regions. Again, holes were filled using a zero displacement mapping from the reference frame that contained the occlusion. <p> The total decoder complexity, including the mapping of unoccluded displacements, is given by, 11 In summary, our technique is substantially less complex than the EPI-based algorithms. While our decoder requires slightly more operations than that of [13] or <ref> [83] </ref>, our block-based displacement field is encoded considerably more efficiently than their multiple, pixel-based fields; hence, our approach is more applicable to viewpoint interpolation within a complete encoder/decoder system. 6.4.2 Storage At the encoder, our technique must store the two reference views and a sparse displacement field. <p> The second displacement field is the actual interpolated field used to predict the 11. The authors of this technique felt that the procedure presented by Thoma and Bierling was of much higher complexity than [their] algorithm and Cafforios <ref> [83] </ref>. In light of this discussion, we believe this characterization is unjustified, unless the elimination of small regions in [93] consti tutes a significant computational burden. C Ribas-Corbera-encoder 2C flow = C Ribas-Corbera-decoder 16= K L 153 pixel intensities in the desired view. <p> Our storage requirement is approximately equal to that of the various viewpoint interpolation schemes that use only two views. Only our method requires SNR values for each valid vector. Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields <ref> [13, 83] </ref> or a change detection operator [93]. <p> The basic interpolator uses the unprocessed, block-based displacement field, selects the displacement for ambiguous regions using range ordering, and assigns zero displacement to holes in the interpolated field using the technique described in <ref> [83] </ref>. Both methods achieve subjectively-pleasing interpolated views for this sequence. These results indicate that the basic interpolator is sufficient for scenes with relatively simple structures and limited occlusion.
Reference: [84] <author> P. Richardson, </author> <title> Image restoration using vector classified adaptive filtering, </title> <booktitle> in Proc. Visual Communication and Image Processing, SPIE, </booktitle> <volume> vol. </volume> <year> 2094, </year> <pages> pp. 1581-1591, </pages> <year> 1993. </year>
Reference-contexts: A substantial contribution of this thesis is the development of a novel coder model that exploits this correlation, due to the practical constraints on the prediction process, to achieve superior rate-distortion performance over traditional residual encoders. A technique similar to our solution was reported independently by Richardson in <ref> [84] </ref>; however, this work was concerned with the problem of restoring a degraded image using only a model of the degradation process and did not address the coding problem. 1.3.1.3 Irrelevancy Reduction Irrelevancy reduction is the final form of information reduction in hybrid coders that will be examined. <p> We call this approach restoration-based residual coding. From this description, this process obviously is closely related to the problem of restoring an image from an observed, degraded versions of the original <ref> [21, 84] </ref>. <p> In image restoration, only the 99 degraded image and a model of the degradation mechanism are available. 5 A technique similar to our approach of applying a set of restoration operators to the degraded image in a piecewise manner, termed Vector Classified Adaptive Filtering (VCAF), was presented in <ref> [84] </ref>. Here, the selection of the restoration operator was based on a priori knowledge of typical image statistics in the form of a classification codebook.
Reference: [85] <author> S. A. Rizvi and N. M. Nasrabadi, </author> <title> Predictive residual vector quantization, </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 4, no. 11, </volume> <pages> pp. 1482-1495, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: we will not explore this thought further, we will revisit the concept of universal coders in our novel class of restoration-based residual encoders (see Section 5.4). 5.3.2 Vector Quantization Another common set of techniques for the compression of images and video are known as vector codes or vector quantizers (VQ) <ref> [19, 35, 36, 39, 50, 58, 69, 85] </ref>. <p> for the residual and predicted images, in that only nonoverlapping blocks are extracted from the residual image, and the predicted blocks may overlap; and the restoration and minimize blocks in the encoder constitutes the nearest-neighboring encoding procedure. 5.4.3 Interpretation and Comparison Restoration-based coding is both adaptive and universal in nature <ref> [24, 85, 105] </ref>. Its adaptive feature is based on the ability to adaptively select the best restoration operator for a region in the residual image; basically, it is able to track non-stationarities of the source signal. <p> This coupling also allows for the predictor and quantizer to be jointly optimized in the codebook design phase. While techniques for adaptive coding without side information have appeared in the literature <ref> [36, 70, 85, 105] </ref>, these methods base their adaptation on previously decoded signal; our approach utilizes the current signal. The decoder is able to follow this adaptation since it is explicitly described in the transmitted index. <p> We speculate that a method to adapt the codebook will alleviate some of the problems associated codebook generation, in particular the codebook design time and over-training. For example, we plan to apply the concepts of super-codebooks [106], codebook replenishment [39], and adaptive filter techniques <ref> [85] </ref> to our vector restoration coder. 124 Chapter 6 Interpolation from a Noisy Displacement Vector Field This chapter considers the problem of interpolating intermediate views 1 from two decoded image frames captured by spatially-offset cameras.
Reference: [86] <author> A. Schertz, </author> <title> Source coding of stereoscopic television pictures, </title> <booktitle> in Proc. IEE Int. Conf. Image Processing and its Applications, </booktitle> <pages> pp. 462-464, </pages> <year> 1992. </year>
Reference-contexts: Additional views are easily incorporated into this methodology due to the frame-based approach, where the number of frames increases linearly with the number of views. This extension of the hybrid coder structure has been reported in numerous systems dealing with multi-view signals <ref> [15, 59, 78, 86, 87] </ref>. The frames from each view may be treated equally or an implicit ranking may be established between the individual views. <p> Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>.
Reference: [87] <author> S. Sethuraman, M. W. Siegel, and A. G. Jordan, </author> <title> A multiresolution framework for stereoscopic image sequence compression, </title> <booktitle> in Proc. IEEE Int. Conf. Image Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 361-365, </pages> <year> 1994. </year>
Reference-contexts: Additional views are easily incorporated into this methodology due to the frame-based approach, where the number of frames increases linearly with the number of views. This extension of the hybrid coder structure has been reported in numerous systems dealing with multi-view signals <ref> [15, 59, 78, 86, 87] </ref>. The frames from each view may be treated equally or an implicit ranking may be established between the individual views. <p> A substantial body of work has been devoted to improving the prediction performance for multi-view signals through segmentation-based schemes, which are applicable to both single and multi-view signals <ref> [14, 87, 100] </ref>. These techniques begin to approach the concept of content-based coding (see Section 1.3.3) by predicting homogeneous regions within each frame from a reference frame, as opposed to purely pixel-based schemes, such as the ubiquitous block-based techniques. <p> The improved prediction has been shown to significantly reduce the number of bits needed in the residual encoding stage <ref> [14, 87, 100] </ref>. The more accurate displacement estimation afforded by these techniques also can be used to improve displacement-compensated interpolation schemes (see Section 1.3.2). However, segmentation-based methods are not without their faults. If the scene contains numerous, high-detail regions, the coders bit rate actually may increase over pixel-based prediction schemes. <p> From these scenarios, it is evident that the relative location of the optimum reference frame is time-varying and signal-dependent. However, in all prior work known to the author on the predictive coding of multi-view signals, the reference frames have been fixed and heuristically chosen <ref> [5, 7, 15, 78, 86, 87] </ref>.
Reference: [88] <author> C. E. Shannon, </author> <title> Coding theorems for a discrete source with a fidelity criteria, </title> <journal> in IRE Nat. Conv. Rec., </journal> <volume> vol. 4, </volume> <pages> pp. 142-163, </pages> <year> 1959. </year> <month> 190 </month>
Reference-contexts: From the classic result of Shan-non, by coding blocks or vectors of source samples as a unit rather than individually, the theoretical limit on compression performance can be approached arbitrarily closely, in the sense of minimizing the average distortion for a specified bit rate <ref> [88] </ref>. The bound is approached as the dimensionality of the vector goes to infinity. This result holds even if the source is memoryless. <p> We set the perturbation constant equal to one-half the standard deviation of the difference between the actual and restored residual pixels, i.e., , where: 7 5.4.2 Vector Restoration (VR) To realize the gains afforded by dimensionality <ref> [88] </ref>, we develop the vector restoration-based coder, which is merely a generalization of the scalar case. Here, the residual image is sectioned into small, nonoverlapping subimages containing N pixels, and the pixel values of the subimages are listed into the column vector denoted by z.
Reference: [89] <author> R. Skerjanc and J. Liu, </author> <title> A three camera approach for calculating disparity and synthesizing intermediate pictures, Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 55-64, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Only two views of the scene are required to calculate displacement information for unoccluded regions. Additional views are useful for autocalibration [20, 107], reducing ambiguity in the displacement estimation process <ref> [37, 89] </ref>, and minimizing the number and area of occluded regions [30]. 1.1.4 Multi-view Bit Rate The described applications are but a sampling of the possible uses for multi-view video signals. <p> The initial algorithm was observed to have difficulty when occlusions were present in the scene; this was addressed through the segmentation of the Var-space based on depth. Skerjanc and Liu presented a technique for synthesizing intermediate views using three cameras positioned at the corner-points of a right-angled isosceles triangle <ref> [89] </ref>. An object-oriented, feature matching approach was used to analyze the range information of the scene. Displacement correlation between epipolar-lines is exploited through the use of feature points, as opposed to the single-line EPI displacement estimation methods. <p> Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields [13, 83] or a change detection operator [93]. The other techniques that process more than two views at the encoder require proportionally more storage based on the total number of views <ref> [30, 40, 47, 89] </ref>. 6.5 Experimental Results The improved interpolation technique was used to synthesize intermediate viewpoints between numerous views in the compressed multi-view sequences described in Section 2.4.
Reference: [90] <author> P. W. Smith and N. Nandhakumar, </author> <title> An improved power cepstrum based stereo correspondence method for textured scenes, </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> vol. 18, no. 3, </volume> <pages> pp. 338-348, </pages> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: This represents a significant computational reduction compared with applications that require a solution to the ill-posed problem of generating an exact, dense range map of the scene <ref> [90] </ref>. View 1 View 0 View b b 0 130 inaccurate boundary between unoccluded regions with differing displacements is estimated, or 4) the meaningless displacement estimates for occlusions are used in the calculation of the interpolated field.
Reference: [91] <author> J. Takeno and U. Rembold, </author> <title> Stereovision systems for autonomous mobile robots, </title> <booktitle> in Proc. Int. Conf. Intelligent Autonomous Systems, </booktitle> <pages> pp. 26-41, </pages> <year> 1995. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems <ref> [34, 52, 91] </ref>, scene modeling [46], and object-based compression [61, 72, 94]. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [92] <author> A. Tamtaoui and C. Labit, </author> <title> Constrained disparity and motion estimators for 3DTV image sequence coding, Signal Processing: </title> <journal> Image Communication, </journal> <volume> vol. 4, </volume> <pages> pp. 45-54, </pages> <year> 1991. </year>
Reference-contexts: If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views <ref> [92] </ref>. Only two views of the scene are required to calculate displacement information for unoccluded regions. <p> The displacement range of corresponding points between two views obtained at the same time instant is constrained to one-dimension, which is referred to as the epipolar line <ref> [9, 11, 56, 92] </ref>. 4 Since the camera configuration is rarely known to 3. Due to their overwhelming use, we will consider only block-based (fixed and variable sized) displacement compensated prediction schemes in this thesis. As such, we shall interchangeably use the terms prediction process and displacement estimation/compensation. 4.
Reference: [93] <author> R. Thoma and M. Bierling, </author> <title> Motion compensating interpolation considering covered and uncovered background, Signal Processing: </title> <journal> Image Communications, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 191-212, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: The other class of techniques for the synthesis of intermediate views is that of displacement-compensated view interpolation, which is closely related to motion-compensated interpolation of single-view signals for frame rate conversion, de-interlacing and frame skipping <ref> [13, 41, 47, 48, 83, 93] </ref>. Here, the intermediate view is synthesized by estimating the displacement of regions between two views and linearly scaling this displacement based on the relative viewpoint of the desired view between two given views. <p> The performance of displacement-compensated interpolation schemes degrades abruptly when a region is visible in only one view (occluded) and when displacement estimation errors occur [63]. Techniques for handling occlusions in single-view signals are provided in [83] and <ref> [93] </ref>. However, these methods are not applicable to multi-view imagery, and they do not achieve an adequate delineation of occlusion borders to avoid a subsequent residual encoding stage for image pairs with relatively large displacement ranges. <p> Motion-compensated interpolation is used for the applications of frame rate conversion, de-interlacing and frame skipping <ref> [13, 48, 83, 93] </ref>. Since constant, translational object motion is often assumed, interpolation of unoccluded regions is performed in the same manner as described in Section 6.1.2. Cafforio, et. al., achieve a highly accurate motion field by using bidirectional motion estimates obtained from a pel-recursive algorithm [13]. <p> No mention of ambiguously-referenced regions was provided. Thoma and Bierling explicitly consider the effects of occluded regions in their motion-compensated interpolation algorithm <ref> [93] </ref>. They combined a hierarchical displacement estimator with a change detector to improve the accuracy of the displacement vector field and to classify occlusions as being covered or uncovered between the two reference frames in the image sequence. <p> The authors of this technique felt that the procedure presented by Thoma and Bierling was of much higher complexity than [their] algorithm and Cafforios [83]. In light of this discussion, we believe this characterization is unjustified, unless the elimination of small regions in <ref> [93] </ref> consti tutes a significant computational burden. C Ribas-Corbera-encoder 2C flow = C Ribas-Corbera-decoder 16= K L 153 pixel intensities in the desired view. <p> Only our method requires SNR values for each valid vector. Appropriate increases to the total storage cost are experienced if a technique uses bidirectional fields [13, 83] or a change detection operator <ref> [93] </ref>.
Reference: [94] <author> D. Tzovaras, N. Grammalidis, M. G. Strintzis, </author> <title> Joint three-dimensional motion/disparity segmentation for object-based stereo image sequence coding, </title> <journal> Optical Engineering, </journal> <volume> vol. 35, no. 1, </volume> <pages> pp. 137-144, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: A threshold of 1.15 min of arc was estimated experimentally in [75]. 3 1.1.3 Scene Analysis Multiple views of the scene are also useful in applications that analyze the three-dimensional structure of the scene. Typical applications include vision-based navigation systems [34, 52, 91], scene modeling [46], and object-based compression <ref> [61, 72, 94] </ref>. If the configuration of the cameras capturing the multiple views are known, a straightforward relationship can be used to estimate the range of objects from the displacement of corresponding points within the views [92].
Reference: [95] <author> N. A. Valyus, </author> <title> Stereoscopy, </title> <publisher> London: The Focal Press, </publisher> <year> 1966. </year>
Reference-contexts: The brain then fuses these images into one view and the viewer experiences the sensation of stereopsis, which provides added realism through improved depth perception <ref> [45, 95] </ref>. 1.1.2 Motion Parallax For further improvements in realism, the binocular imaging system can be extended to provide the viewer with the depth cue of motion parallax [74, 99]. 1 Motion parallax, which provides the distinction between binocular and three-dimensional imagery, can be simulated by obtaining multiple, closely-spaced views of
Reference: [96] <author> H. </author> <title> Van Trees, Detection, Estimation, and Modulation Theory, </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1968, </year> <note> vol. 1, ch. 2. </note>
Reference-contexts: The single-step displacement vector estimate associated with a region whose prediction PSNR falls below a user-defined threshold is assumed to be erroneous, and is discarded from any further consideration. The thresholding process can be viewed in terms of a classical, binary detection problem <ref> [96] </ref>. We observe the prediction PSNR for a particular displacement estimate, where the statistics of the PSNR are governed by whether the estimate is either correct or incorrect.
Reference: [97] <author> C. Ware, C. Gobrecht and M. Paton, </author> <title> Algorithm for dynamic disparity adjustment, in Proc. Stereoscopic Displays and Virtual Reality Systems II, </title> <booktitle> SPIE, </booktitle> <volume> vol. 2409, </volume> <pages> pp. 150-156, </pages> <year> 1995. </year>
Reference-contexts: The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort [55, 62, 65, 74, 103]. An additional source of eye strain is related to the viewers ability to fuse binocular information <ref> [97] </ref>. To achieve geometrically correct stereo-vision, in the sense that the viewer sees what would be seen by the naked-eye, the cameras capturing the binocular image pair must be separated by the viewers inter-ocular separation [33]. <p> However, this quantity is viewer dependent and individuals often prefer varying degrees of depth perception based on individual stereoscopic viewing ability and the range of depth present in the scene <ref> [97] </ref>. A greater sense of depth is provided by a relatively large inter-camera separation, but the larger the separation the more difficulty marginal viewers have in fusing the images. <p> If a continuum of views between two extreme viewpoints are available, a viewer can dynamically select the inter-camera separation for comfort and preferred sense of depth in a manner similar to the adjustments of brightness and contrast found on most display devices <ref> [63, 67, 97] </ref>. Decreasing the camera separation also reduces the breakdown between the accommodation/convergence relationship. 126 inter-camera separation, (a) functional block diagram, (b) virtual camera positions of desired intermediate views.
Reference: [98] <author> N. Wiener, </author> <title> Extrapolation, Interpolation, and Smoothing of Stationary Time Series, </title> <address> Cam-bridge MA: </address> <publisher> MIT Press, </publisher> <year> 1949. </year>
Reference-contexts: The optimal affine restoration transform for the i th partition then is given by the Wiener-Hopf equation for non-zero mean sources <ref> [98] </ref>: and (5.12) where, , denotes the conditional auto-covariance matrix of x, and each of the quantities in Eq. (5.12) is with respect to (i.e., conditioned on) the i th partition. These values can be numerically estimated by computing time-averages over the partitioned training data.
Reference: [99] <author> C. D. Wilkens, </author> <title> Three-dimensional stereoscopic display implementation: Guidelines derived from human visual capabilities, </title> <booktitle> in Proc. Stereoscopic Displays and Applications, SPIE, </booktitle> <volume> vol. 1256, </volume> <pages> pp. 2-11, </pages> <year> 1990. </year>
Reference-contexts: fuses these images into one view and the viewer experiences the sensation of stereopsis, which provides added realism through improved depth perception [45, 95]. 1.1.2 Motion Parallax For further improvements in realism, the binocular imaging system can be extended to provide the viewer with the depth cue of motion parallax <ref> [74, 99] </ref>. 1 Motion parallax, which provides the distinction between binocular and three-dimensional imagery, can be simulated by obtaining multiple, closely-spaced views of the scene and then presenting the appropriate binocular image pair based on the viewers position.
Reference: [100] <author> X. Wu and Y. Fang, </author> <title> A segmentation-based predictive multiresolution image coder, </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 34-47, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: A substantial body of work has been devoted to improving the prediction performance for multi-view signals through segmentation-based schemes, which are applicable to both single and multi-view signals <ref> [14, 87, 100] </ref>. These techniques begin to approach the concept of content-based coding (see Section 1.3.3) by predicting homogeneous regions within each frame from a reference frame, as opposed to purely pixel-based schemes, such as the ubiquitous block-based techniques. <p> The improved prediction has been shown to significantly reduce the number of bits needed in the residual encoding stage <ref> [14, 87, 100] </ref>. The more accurate displacement estimation afforded by these techniques also can be used to improve displacement-compensated interpolation schemes (see Section 1.3.2). However, segmentation-based methods are not without their faults. If the scene contains numerous, high-detail regions, the coders bit rate actually may increase over pixel-based prediction schemes.
Reference: [101] <author> H. Yamaguchi, </author> <title> Multifocus synthesis and its application to 3-D image capturing, </title> <booktitle> in Proc. Visual Communications and Image Processing, SPIE, </booktitle> <volume> vol. </volume> <year> 2094, </year> <pages> pp. 281-291, </pages> <year> 1993. </year>
Reference-contexts: Additional applications may use cameras that vary not only in their viewpoint, but also have vary The cameras (C ij ) are arranged with parallel axes and coplanar image sensors. C 11 Dc y C 21 C I2 C 1J 4 ing scale <ref> [101] </ref> and spectral bandwidth selectivity parameters [5, 35]. Regardless of their ultimate use, it is likely that these signals will need to be transmitted and/or stored in order to be displayed at a remote location, re-displayed at a future time, or processed off-line.
Reference: [102] <author> H. Yamaguchi, Y. Tatehira, K. Akiyama and Y. Kobayashi, </author> <title> Stereoscopic images disparity for predictive coding, </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 1976-1979, </pages> <year> 1989. </year>
Reference-contexts: Similar gains have been reported for multi-view signals, where the frame-based prediction is generated from a reference frame offset in time and/or viewpoint <ref> [5, 15, 23, 59, 86, 102] </ref>. For these systems, interframe redundancy is related not only to the temporal sampling rate and object motion but also to the scene structure and camera configuration.
Reference: [103] <author> T. Yamazaki, K. Kamijo and S. Fukuzumi, </author> <title> Quantitative evaluation of visual fatigue encountered in viewing stereoscopic 3D displays: Near-point distance and visual evoked potential study, </title> <journal> Proc. SID, </journal> <volume> vol. 31, no. 3, </volume> <pages> pp. 245-247, </pages> <year> 1990. </year> <month> 191 </month>
Reference-contexts: Intermediate views also can be used to ensure that stereoscopic imagery is comfortable to view. Discomfort is often experienced when viewing stereoscopic images on a two-dimensional display device. The breakdown of the accommodation/convergence relationship has been widely reported as a cause of this discomfort <ref> [55, 62, 65, 74, 103] </ref>. An additional source of eye strain is related to the viewers ability to fuse binocular information [97].
Reference: [104] <author> A. Zakhor and F. Lari, </author> <title> Edge-based 3-D camera motion estimation with application to video coding, </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 481-498, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The focal length of the camera is . The roll vector, given by , provides the rotation of the camera about its axis. The relationship between these points and the mapping between 3-D world- and 2-D image-coordinates are illustrated in Fig. 2.1. 1. See <ref> [104] </ref> for a description of techniques that consider the effect of global camera motion. 2. It should be noted that the point p need not, and sometimes should not, lie at the center of the image sensor plane.
Reference: [105] <author> K. Zeger and A. </author> <title> Bist, Universal source coding with codebook transmission, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 42, no. </volume> <pages> 2-4, pp. 336-346, </pages> <year> 1994. </year>
Reference-contexts: for the residual and predicted images, in that only nonoverlapping blocks are extracted from the residual image, and the predicted blocks may overlap; and the restoration and minimize blocks in the encoder constitutes the nearest-neighboring encoding procedure. 5.4.3 Interpretation and Comparison Restoration-based coding is both adaptive and universal in nature <ref> [24, 85, 105] </ref>. Its adaptive feature is based on the ability to adaptively select the best restoration operator for a region in the residual image; basically, it is able to track non-stationarities of the source signal. <p> This coupling also allows for the predictor and quantizer to be jointly optimized in the codebook design phase. While techniques for adaptive coding without side information have appeared in the literature <ref> [36, 70, 85, 105] </ref>, these methods base their adaptation on previously decoded signal; our approach utilizes the current signal. The decoder is able to follow this adaptation since it is explicitly described in the transmitted index.
Reference: [106] <author> K. Zeger and A. </author> <title> Bist, Universal adaptive vector quantization using codebook quantization with application to image compression, </title> <booktitle> in Proc. Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 5, </volume> <pages> pp. 381-384, </pages> <year> 1992. </year>
Reference-contexts: We speculate that a method to adapt the codebook will alleviate some of the problems associated codebook generation, in particular the codebook design time and over-training. For example, we plan to apply the concepts of super-codebooks <ref> [106] </ref>, codebook replenishment [39], and adaptive filter techniques [85] to our vector restoration coder. 124 Chapter 6 Interpolation from a Noisy Displacement Vector Field This chapter considers the problem of interpolating intermediate views 1 from two decoded image frames captured by spatially-offset cameras.
Reference: [107] <author> Q. Zheng and R. Chellappa, </author> <title> A computational vision approach to image registration, </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 311-325, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Only two views of the scene are required to calculate displacement information for unoccluded regions. Additional views are useful for autocalibration <ref> [20, 107] </ref>, reducing ambiguity in the displacement estimation process [37, 89], and minimizing the number and area of occluded regions [30]. 1.1.4 Multi-view Bit Rate The described applications are but a sampling of the possible uses for multi-view video signals.
Reference: [108] <author> W. Zschunke, </author> <title> DPCM picture coding with adaptive prediction, </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 25, no. 11, </volume> <pages> pp. 1295-1302, </pages> <month> Nov. </month> <year> 1977. </year>
Reference-contexts: The restoration-based coder is essentially a locally linear (affine) globally nonlinear, adaptive, shift-variant filter (cluster-wise linear). Scalar and vector-restoration are, respectively, related to Adaptive Differential Pulse Code Modulation (ADPCM) and adaptive predictive vector quantization (APVQ) <ref> [50, 108] </ref>. In these techniques, the prediction and quantization steps are decoupled. The major contribution common to both SR and VR is that the adaptation does not require the transmission of any side information.
References-found: 108

