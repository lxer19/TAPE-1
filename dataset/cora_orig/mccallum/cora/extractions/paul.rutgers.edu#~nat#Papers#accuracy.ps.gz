URL: paul.rutgers.edu/~nat/Papers/accuracy.ps.gz
Refering-URL: 
Root-URL: 
Email: e-mail: nat@cis.ohio-state.edu  
Keyword: Nathalie Japkowicz  
Address: 395 Dreese Lab, 2015 Neil Avenue Columbus, OH 43210, USA  
Affiliation: Computer and Information Science The Ohio State University  
Abstract: Are we better off without Counter-Examples? Abstract Concept-learning is commonly implemented using discrimination-based techniques which rely on both examples and counter-examples of the concept. Recently, however, a recognition-based approach that learns a concept in the absence of counter-examples was shown to be more accurate than its discrimination counterpart on two real-world domains and as accurate on the third. The purpose of this paper is to find out whether this recognition-based approach is generally more accurate than its discrimination counterpart or whether the results it obtained previously are purely coincidental. The analysis conducted in this paper concludes that the results obtained on the real-world domains were not coincidental, and this suggests that recognition-based approaches are promising techniques worth studying in greater depth. 
Abstract-found: 1
Intro-found: 1
Reference: [Cottrell et al.1987] <author> Garrison W. Cottrell, Paul Munro, and David Zipser. </author> <title> Learning internal representations from gray-scale images: An example of extensional programming. </title> <booktitle> In Proceedings of the ninth AnnualConference of the Cognitive Science Society, </booktitle> <pages> pages 462-473, </pages> <address> Seattle, WA, </address> <year> 1987. </year>
Reference-contexts: The network of figure 1 (b) is not as universal as the MLP network, but it has been used previously to implement a compression scheme (e.g., <ref> [Cottrell et al.1987] </ref>). As well, and this is the application of interest here, it can be used to implement a recognition approach to concept learning as proposed by [Hanson and Kegl1987].
Reference: [Hanson and Kegl1987] <author> Stephen J. Hanson and Judy Kegl. Parsnip: </author> <title> A connectionist network that learns natural language grammar from exposure to natural language sentences. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Cognitive Science, </booktitle> <year> 1987. </year>
Reference-contexts: As well, and this is the application of interest here, it can be used to implement a recognition approach to concept learning as proposed by <ref> [Hanson and Kegl1987] </ref>. In this scheme, each response vector y i of y is set to x i , the corresponding input vector. In other words, the network is trained to reproduce the input at the output layer. Such a network is called an autoassociator.
Reference: [Japkowicz et al.1995] <author> Nathalie Japkowicz, Catherine Myers, and Mark Gluck. </author> <title> A novelty detection approach to classification. </title> <booktitle> In Proceedings of the Fourteenth Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 518-523, </pages> <year> 1995. </year>
Reference-contexts: Details of these experiments can be found in <ref> [Japkowicz et al.1995] </ref>, but what is of interest here is the classification error rates obtained by the two systems on 5-fold cross-validation experiments on the three domains. These results are reported in Table 1. <p> As shown in Figure 2 (a), each mean actually stands for a cluster of data points normally distributed around that mean and of variance oe 2 = 0:01. This domain was inspired from the real-world domains used in <ref> [Japkowicz et al.1995] </ref> as described in [Japkowicz1999]. <p> Conclusion This paper originated from the observation that autoassociation-based classification was reported to be more accurate than the MLP discrimination network on two out of three real-world domains and as accurate as MLP on the third, despite the fact that the autoassociator learns a concept in the absence of counter-examples <ref> [Japkowicz et al.1995] </ref>. The purpose of this paper was to find out whether this result was only coincidental or whether it could be expected in other domains.
Reference: [Japkowicz et al.1999] <author> Nathalie Japkowicz, Stephen J. Hanson, and Mark A. Gluck. </author> <title> Nonlinear autoassociation is not equivalent to pca. </title> <note> In Submitted to Neural Computation (Revise and Resubmit), </note> <year> 1999. </year>
Reference: [Japkowicz1999] <author> Nathalie Japkowicz. </author> <title> Concept-Learning in the Absence of Counter-Examples: An Autoassociation-Based Approach to Classification (in progress). </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <year> 1999. </year>
Reference-contexts: As shown in Figure 2 (a), each mean actually stands for a cluster of data points normally distributed around that mean and of variance oe 2 = 0:01. This domain was inspired from the real-world domains used in [Japkowicz et al.1995] as described in <ref> [Japkowicz1999] </ref>. <p> In particular, the capacities considered are those of 1, 2, 4, 8, 16, 32 and 64 hidden units (for a list of the capacities retained for each domain and each system, see <ref> [Japkowicz1999] </ref>). <p> The results obtained in this section thus demonstrate that, at least on synthetic domains, autoassociation-based classification is more accurate than MLP in multi-modal domains presenting certain well-defined characteristics. As discussed in <ref> [Japkowicz1999] </ref> these results also extend to practical domains, and in particular, can be used to explain the accuracy results obtained on the real-world domains of Section 2.2. 5 Conclusion This paper originated from the observation that autoassociation-based classification was reported to be more accurate than the MLP discrimination network on two
Reference: [Mitchell1982] <author> Tom Michael Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: In more detail, we expect that the MLP network should be better suited than the autoassociator to learning domains that require a strong specialization bias caused by the conceptual class, provided that the counter-conceptual class contains meaningful specialization information. 2 These ideas are formalized in <ref> [Mitchell1982] </ref> using the notion of "Version Spaces". This is because, the MLP network has the possibility to rely on the counter-examples during the inductive process, whereas the autoassociator does not.
References-found: 6

