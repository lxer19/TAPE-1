URL: http://www.cs.wisc.edu/~cao/papers/gd-size.ps.Z
Refering-URL: http://www.cs.wisc.edu/~cao/papers/proxy/proxy.html
Root-URL: 
Email: cao@cs.wisc.edu irani@ics.uci.edu  
Title: Cost-Aware WWW Proxy Caching Algorithms  
Author: Pei Cao Sandy Irani 
Address: California-Irvine.  
Affiliation: Department of Computer Science, Information and Computer Science Department, University of Wisconsin-Madison. University of  
Abstract: Web caches can not only reduce network traffic and downloading latency, but can also affect the distribution of web traffic over the network through cost-aware caching. This paper introduces GreedyDual-Size, which incorporates locality with cost and size concerns in a simple and non-parameterized fashion for high performance. Trace-driven simulations show that with the appropriate cost definition, GreedyDual-Size outperforms existing web cache replacement algorithms in many aspects, including hit ratios, latency reduction and network cost reduction. In addition, GreedyDual-Size can potentially improve the performance of main-memory caching of Web documents. 
Abstract-found: 1
Intro-found: 1
Reference: [ASAWF95] <author> M. Abrams, C.R. Standbridge, G.Abdulla, S. Williams and E.A. Fox. </author> <title> Caching Proxies: Limitations and Potentials. </title> <address> WWW-4, Boston Conference, </address> <month> December, </month> <year> 1995. </year>
Reference-contexts: Page 2 * Least-Frequently-Used (LFU) evicts the document which is accessed least frequently. * Size [WASAF96] evicts the largest document. * LRU-Threshold <ref> [ASAWF95] </ref> is the same as LRU, except documents larger than a certain threshold size are never cached; * Log (Size)+LRU [ASAWF95] evicts the document who has the largest log (size) and is the least recently used document among all doc uments with the same log (size). * Hyper-G [WASAF96] is a <p> Page 2 * Least-Frequently-Used (LFU) evicts the document which is accessed least frequently. * Size [WASAF96] evicts the largest document. * LRU-Threshold <ref> [ASAWF95] </ref> is the same as LRU, except documents larger than a certain threshold size are never cached; * Log (Size)+LRU [ASAWF95] evicts the document who has the largest log (size) and is the least recently used document among all doc uments with the same log (size). * Hyper-G [WASAF96] is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the least-recently-used document, except if all <p> Thus, LRV takes into account locality, cost and size of a document. Existing studies using actual Web proxy traces narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in <ref> [WASAF96, ASAWF95] </ref> show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate.
Reference: [Bel66] <author> L.A. Belady. </author> <title> A study of replacement algorithms for virtual storage computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5 </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: If one is given a sequence of requests to uniform size blocks of memory, it is well known that the simple rule of evicting the block whose next request is farthest in the future will yield the optimal performance <ref> [Bel66] </ref>. In the variable-size case, no such o*ine algorithm is known. In fact, it is known that determining the optimal performance is NP-hard [Ho97], although there is an algorithm which can approximate the optimal to within a logarithmic factor [Ir97].
Reference: [CD73] <author> G. Coffman, Jr., Edward and Peter J. Den-ning, </author> <title> Operating Systems Theory, </title> <publisher> Prentice-Hall, Inc. </publisher> <year> 1973. </year>
Reference-contexts: The median hit ratios show an almost linear increase as the group size doubles. In the absence of cost and size concerns, LRU is the optimal online algorithm for reference streams exhibiting good locality <ref> [CD73] </ref> (strictly speaking, those conforming to the LRU-stack model). However, in the Web context, replacing a more recently used but large file can yield a higher hit ratio than replacing a less recently used but small file.
Reference: [CKPV91] <author> M. Chrobak, H. Karloff, T. H. Payne and S. Vishwanathan. </author> <title> New results on server problems. </title> <journal> newblock SIAM Journal on Discrete Mathematics, </journal> <volume> 4 </volume> <pages> 172-181, </pages> <year> 1991. </year>
Reference-contexts: The proof below is based on a proof that another algorithm called BALANCE which also solves the multi-cost uniform-size paging problem is k-competitive <ref> [CKPV91] </ref>. All of the above bounds are tight, since we can always assume that all pages are as small as possible and have the same cost and invoke the lower bound of k on the competitive ratio for the uniform-size uniform-cost paging problem found in [ST85].
Reference: [DEC96] <institution> Digital Equipment Cooperation, </institution> <note> Digital's Web Proxy Traces ftp://ftp.digital.com/pub/DEC/traces/proxy /webtraces.html. </note>
Reference: [FKIP96] <author> A. Feldman, A. Karlin, S. Irani, S. Phillips. </author> <title> Private Communication. </title>
Reference-contexts: Interestingly, it is also known that LRU has an optimal competitive ratio when the page size can vary and the cost of fetching a document is the same for all documents or proportional to the size of a document <ref> [FKIP96] </ref>. 2.2 Existing Document Replacement Algorithms We describe nine cache replacement algorithms proposed in recent studies, which attempt to minimize various cost metrics, such as miss ratio, byte miss ratio, average latency, and total cost. Below we give a brief description of all of them.
Reference: [Ho97] <author> Hosseini, Saied, </author> <title> Private Communication. </title>
Reference-contexts: In the variable-size case, no such o*ine algorithm is known. In fact, it is known that determining the optimal performance is NP-hard <ref> [Ho97] </ref>, although there is an algorithm which can approximate the optimal to within a logarithmic factor [Ir97]. The approximation factor is logarithmic in the maximum number of bytes that can fit in the cache, which we will call k.
Reference: [LC97] <author> Chengjie Liu, Pei Cao. </author> <title> Maintaining Strong Cache Consistency in the World-Wide Web. </title> <booktitle> In Proceedings of the 1997 International Conferences on Distributed Computing Systems, </booktitle> <month> May, </month> <year> 1997. </year>
Reference-contexts: proxy systems also consider the time-to-live fields of the documents and replace expired documents first, studies have found that time-to-live fields rarely correspond exactly to the actual life time of the document and it is better to keep expired-but-recently-used documents in the cache and validate them by querying the server <ref> [LC97] </ref>. The advantage of LRU is its simplicity; the disadvantage is that it does not take into account file sizes or latency and might not give the best hit ratio. Many Web caching algorithms have been proposed to address the size and latency concerns.
Reference: [Ir97] <author> S. Irani. </author> <title> Page replacement with multi-size pages and applications to web caching. </title> <booktitle> In the Proceedings for the 29th Symposium on the Theory of Computing, </booktitle> <year> 1997, </year> <pages> pages 701-710. </pages>
Reference-contexts: In the variable-size case, no such o*ine algorithm is known. In fact, it is known that determining the optimal performance is NP-hard [Ho97], although there is an algorithm which can approximate the optimal to within a logarithmic factor <ref> [Ir97] </ref>. The approximation factor is logarithmic in the maximum number of bytes that can fit in the cache, which we will call k. For the cost consideration, there have been several algorithms developed for the uniform-size variable-cost paging problem.
Reference: [LRV97] <author> P. Lorenzetti, L. Rizzo and L. Vi-cisano. </author> <title> Replacement Policies for a Proxy Cache. </title> <address> http://www.iet.unipi.it/ luigi/research.html. </address>
Reference-contexts: Estimates for c s and b s are based on the the times to fetch documents from server s in the recent past. * Lowest Relative Value (LRV), introduced in <ref> [LRV97] </ref>, includes the cost and size of a document in the calculation of a value that estimates the utility of keeping a document in the cache. The algorithm evicts the document with the lowest value. The calculation of the value is based on extensive empirical analysis of trace data. <p> Results in [WASAF96, ASAWF95] show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study <ref> [LRV97] </ref> shows that LRU outperforms SIZE in terms of byte hit rate. Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. <p> Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. In terms of minimizing latency, [WA97] show that Hybrid performs better than Lowest-Latency-First. Finally, <ref> [LRV97] </ref> shows that LRV outperforms both LRU and SIZE in terms of hit ratio and byte hit ratio. One disadvantage of both Hybrid and LRV is their heavy parameterization, which leaves one uncertain about their performance across access streams. <p> That is, the probability of a document being referenced again within t minutes is proportionally to log (t), indicating that the probability of re-reference to documents referenced exactly t minutes ago can be modeled as k=t, where k is a constant. A different study <ref> [LRV97] </ref> reached very similar conclusions on a different set of traces. Indeed, it is this observation that promoted the design of the function D (t) in LRV. <p> Though one might expect that browsers' caches absorb the locality among the same user's accesses seen by the proxy, the results seems to indicate that this is not necessarily the case, and users are using proxy caches as an extension to the browser cache. <ref> [LRV97] </ref> observes the same phenomenon. The other reason is that users' interests overlap in time | comparing figures 2 and 1, we can see that for the same t, the percentage in figure 1 is higher than that in figure 2. <p> Size, Hybrid, and LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> The relative comparison of LRU and Size differs from the results in [WASAF96], but agrees with those in <ref> [LRV97] </ref>. In summary, for proxy designers that seek to maximize hit ratio, GD-Size (1) is the appropriate algorithm.
Reference: [CBC95] <author> Carlos R. Cunba, Azer Bestavros, Mark E. </author> <title> Crovella Characteristics of WWW Client-based Traces BU-CS-96-010, </title> <address> Boston University. </address>
Reference-contexts: For each trace, we first calculate the benefit obtained if the cache size is infinite. The values for all traces are shown in Table 1. In the table, BU-272 and BU-B19 are two sets of traces from Boston University <ref> [CBC95] </ref>, VT-BL, VT-C, VT-G, VT-U are four sets of traces from Virginia Tech [WASAF96], DEC-U1:8/29-9/4 through DEC-U1:9/19-9/22 are the requests made by users 0-512 (user group 1) for each week in the three and half week period, and DEC-U2:8/29-9/4 through DEC-U2:9/19-9/22 are the traces for users 1024-2048 (user group 2).
Reference: [LM96] <author> Paul Leach and Jeff Mogul. </author> <title> The Hit Metering Protocol. </title> <type> Manuscript. </type>
Reference: [HT97] <institution> IETF The HTTP 1.1 Protocol Draft. </institution> <note> http://www.ietf.org. </note>
Reference-contexts: We perform some necessary pre-processing over the traces. For the DEC traces, we simulated only those requests whose replies are cacheable as specified in HTTP 1.1 <ref> [HT97] </ref> (i.e. GET or HEAD requests with status 200, 203, 206, 300, or 301, and not a "cgi bin" request).
Reference: [ST85] <author> D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communications of the ACM, </journal> <volume> 28 </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: The competitive ratio is essentially the maximum ratio of the algorithms cost to the optimal o*ine algorithm's cost over all possible request sequences. (For an introduction to competitive analysis, see <ref> [ST85] </ref>). We have generalized the result in [You91b] to show that our algorithm GreedyDual-Size, which handles documents of differing sizes and differing cost (described in Section 4), also has an optimal competitive ratio. <p> All of the above bounds are tight, since we can always assume that all pages are as small as possible and have the same cost and invoke the lower bound of k on the competitive ratio for the uniform-size uniform-cost paging problem found in <ref> [ST85] </ref>. It should also be noted that the same bound can be proven for the version of the algorithm which uses c (p) instead of c (p)=s (p) in the description of the algorithm in Figure 5. Young has independently proven a generalization of the result below [You97].
Reference: [Tufte] <institution> Edward Tufte The Visual Display of Quantitative Information. Graphics Printers, </institution> <month> Feburary </month> <year> 1992. </year>
Reference-contexts: Note that the y-axis is in linear scale and the x-axis is in log scale. and byte hit ratio as a function of the size of the user group sharing the cache. The figures are quartile graphs <ref> [Tufte] </ref>, the middle curve showing the median of the hit ratios of individual groups of clients in the DEC traces, and the other four points for each group size showing the minimum, the 25% percentile, the 75% percentile, and the maximum of the hit ratios of individual groups.
Reference: [W3C] <institution> The Notification Protocol. </institution> <note> http://www.w3c.org. </note>
Reference: [WASAF96] <author> S. Williams, M. Abrams, C.R. Stand-bridge, G.Abdulla and E.A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proceedings of the ACM Sigcomm96, </booktitle> <month> August, </month> <year> 1996, </year> <institution> Stanford University. </institution>
Reference-contexts: Since documents are stored at the proxy cache, many HTTP requests can be satisfied directly from the cache instead of generating traffic to and from the 1 Navigator is a trademark of Netscape Inc. Web server. Numerous studies <ref> [WASAF96] </ref> have shown that the hit ratio for Web proxy caches can be as high as over 50%. This means that if proxy caching is utilized extensively, the network traffic can be reduced significantly. <p> Page 2 * Least-Frequently-Used (LFU) evicts the document which is accessed least frequently. * Size <ref> [WASAF96] </ref> evicts the largest document. * LRU-Threshold [ASAWF95] is the same as LRU, except documents larger than a certain threshold size are never cached; * Log (Size)+LRU [ASAWF95] evicts the document who has the largest log (size) and is the least recently used document among all doc uments with the same <p> * LRU-Threshold [ASAWF95] is the same as LRU, except documents larger than a certain threshold size are never cached; * Log (Size)+LRU [ASAWF95] evicts the document who has the largest log (size) and is the least recently used document among all doc uments with the same log (size). * Hyper-G <ref> [WASAF96] </ref> is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the least-recently-used document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First [WA97] tries to minimize average latency by removing the document with the lowest download <p> size are never cached; * Log (Size)+LRU [ASAWF95] evicts the document who has the largest log (size) and is the least recently used document among all doc uments with the same log (size). * Hyper-G <ref> [WASAF96] </ref> is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the least-recently-used document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First [WA97] tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. <p> Thus, LRV takes into account locality, cost and size of a document. Existing studies using actual Web proxy traces narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in <ref> [WASAF96, ASAWF95] </ref> show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate. <p> Existing studies using actual Web proxy traces narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in [WASAF96, ASAWF95] show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in <ref> [WASAF96] </ref> also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate. <p> successful in obtaining the following traces of HTTP requests going through Web proxies: * Digital Equipment Corporation Web Proxy server traces [DEC96](Aug-Sep 1996), servicing about 17,000 workstations, for a period of 25 days, containing a total of about 24,000,000 ac cesses; * University of Virginia proxy server and client traces <ref> [WASAF96] </ref> (Feb-Oct 1995), containing four sets of traces, each servicing from 25 to 61 workstations, containing from 13,127 to 227,210 accesses; * Boston University client traces [CBC95](Nov 1994 May 1995), containing two sets of traces, one servicing 5 workstations (17,008 accesses), the other 32 workstations (118,105 accesses); We are in the <p> For Virginia Tech traces, we simulated only the "GET" requests with reply status 200 and a known reply size. Thus, our numbers differ from what are reported in <ref> [WASAF96] </ref>. The Virginia Tech traces unfortunately do not come with latency information. <p> Size, Hybrid, and LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> The values for all traces are shown in Table 1. In the table, BU-272 and BU-B19 are two sets of traces from Boston University [CBC95], VT-BL, VT-C, VT-G, VT-U are four sets of traces from Virginia Tech <ref> [WASAF96] </ref>, DEC-U1:8/29-9/4 through DEC-U1:9/19-9/22 are the requests made by users 0-512 (user group 1) for each week in the three and half week period, and DEC-U2:8/29-9/4 through DEC-U2:9/19-9/22 are the traces for users 1024-2048 (user group 2). <p> LRU performs better than SIZE in terms of hit ratio when the cache size is small (less or equal than 5% of the total date set size), but performs slightly worse when the cache size is large. The relative comparison of LRU and Size differs from the results in <ref> [WASAF96] </ref>, but agrees with those in [LRV97]. In summary, for proxy designers that seek to maximize hit ratio, GD-Size (1) is the appropriate algorithm.
Reference: [WA97] <author> R. Wooster and M. Abrams. </author> <title> Proxy Caching the Estimates Page Load Delays. </title> <booktitle> In the 6th International World Wide Web Conference, </booktitle> <address> April 7-11, 1997, Santa Clara, CA. http://www6.nttlabs.com/HyperNews/get/ PA-PER250.html. </address>
Reference-contexts: used document among all doc uments with the same log (size). * Hyper-G [WASAF96] is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the least-recently-used document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First <ref> [WA97] </ref> tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. A function is computed for each document which is designed to capture the utility of retaining a given document in the cache. <p> last access time and size considerations; * Pitkow/Recker [WASAF96] removes the least-recently-used document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First <ref> [WA97] </ref> tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. A function is computed for each document which is designed to capture the utility of retaining a given document in the cache. The document with the smallest function value is then evicted. <p> Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. In terms of minimizing latency, <ref> [WA97] </ref> show that Hybrid performs better than Lowest-Latency-First. Finally, [LRV97] shows that LRV outperforms both LRU and SIZE in terms of hit ratio and byte hit ratio. One disadvantage of both Hybrid and LRV is their heavy parameterization, which leaves one uncertain about their performance across access streams. <p> Another concern about both Hybrid and LRV is that they employ constants which might have to be tuned to the patterns in the request stream. For Hybrid, we use the values which were used in <ref> [WA97] </ref> in our simulations. We did not experiment with tuning those constants to improve the performance of Hybrid. Though LRV can incorporate arbitrary network costs associated with documents, the O (k) computational complexity of finding a replacement can be prohibitively expensive. <p> Size, Hybrid, and LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> One study <ref> [WA97] </ref> introduced a proxy replacement algorithm called Hybrid, which takes into account the different latencies incurred to load different web pages, and attempts to minimize the average latency. The study [WA97] further showed that in general the algorithm has a lower average latency than LRU, LFU and SIZE. <p> One study <ref> [WA97] </ref> introduced a proxy replacement algorithm called Hybrid, which takes into account the different latencies incurred to load different web pages, and attempts to minimize the average latency. The study [WA97] further showed that in general the algorithm has a lower average latency than LRU, LFU and SIZE. We also designed two versions of GreedyDual-Size that take latency into account. One, called GD-Size (latency), sets the cost of a document to the latency that was required to download the document. <p> One, called GD-Size (latency), sets the cost of a document to the latency that was required to download the document. The other, called GD-Size (avg latency), sets the cost to the estimated download latency of a document, using the same method of estimating latency as in Hybrid <ref> [WA97] </ref>. LRU, Hybrid, GD-Size (1), GD-Size (latency) and GD-Size (avg latency). The graphs, from left to right, show the results for Boston University traces, DEC-U1 traces and DEC-U2 traces. <p> For all DEC traces, Hybrid's hit ratio is much lower than LRU's, under all cache sizes. Hybrid has a low hit ratio because it does not consider how recently a document has been accessed during replacement. Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. <p> Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. It is a surprise to us that GD-Size (1), which does not take latency into account, performs better than GD-Size (latency) and GD-Size (avg latency). <p> Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. It is a surprise to us that GD-Size (1), which does not take latency into account, performs better than GD-Size (latency) and GD-Size (avg latency). Detailed examination of the traces shows that the latency of loading the same document varies significantly.
Reference: [WPB] <author> Jussara Almeida and Pei Cao. </author> <title> The Wisconsin Proxy Benchmark (WPB). </title> <address> http://www.cs.wisc.edu/~cao/wpb1.0.html. </address>
Reference-contexts: Since the studies find similar temporal locality patterns in the Web access traces, the time since last access by the same user. probability density function of k=t has been used to simulate temporal locality behavior in a recent Web proxy benchmark <ref> [WPB] </ref>. There are two reasons for the good locality in Web accesses seen by the proxy.
Reference: [You91b] <author> N. Young. </author> <title> The k-server dual and loose competitiveness for paging. </title> <journal> Algorithmica,June 1994, </journal> <volume> vol. </volume> <month> 11,(no.6):525-41. </month> <title> Rewritten version of "Online caching as cache size varies", </title> <booktitle> in The 2nd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 241-250, </pages> <year> 1991. </year>
Reference-contexts: In addition, none of the existing algorithms address the network cost concerns. In this paper, we introduce a new algorithm, called GreedyDual-Size, which combines locality, size and latency/cost concerns effectively to achieve the best overall performance. GreedyDual-Size is a variation on a simple and elegant algorithm called Greedy-Dual <ref> [You91b] </ref>, which handles uniform-size variable-cost cache replacement. <p> The approximation factor is logarithmic in the maximum number of bytes that can fit in the cache, which we will call k. For the cost consideration, there have been several algorithms developed for the uniform-size variable-cost paging problem. GreedyDual <ref> [You91b] </ref>, is actually a range of algorithms which include a generalization of LRU and a generalization of FIFO. The name GreedyDual comes from the technique used to prove that this entire range of algorithms is optimal according to its competitive ratio. <p> The competitive ratio is essentially the maximum ratio of the algorithms cost to the optimal o*ine algorithm's cost over all possible request sequences. (For an introduction to competitive analysis, see [ST85]). We have generalized the result in <ref> [You91b] </ref> to show that our algorithm GreedyDual-Size, which handles documents of differing sizes and differing cost (described in Section 4), also has an optimal competitive ratio. <p> Thus, we need an algorithm that combines locality, size and cost considerations in a simple, online way that does not require tuning paramters according to the particular traces, and yet maximizes the overall performance. 4 GreedyDual-Size Algorithm The original GreedyDual algorithm is proposed by Young <ref> [You91b] </ref>. It is actually a range of algorithms, but we focus one particular version which is a generalization of LRU. It is concerned with the case when pages in a cache have the same size, but incur different costs to fetch from a secondary storage. <p> Below is a proof of the online-optimality of GreedyDual-Size. Neal Young proved in <ref> [You91b] </ref> that Greedy Dual for pages of uniform size is k-competitive. We prove here that the version which handles pages of multiple size is also k-competitive. (In both cases, k is defined to be the ratio of the size of the cache to the size of the smallest page). <p> Young has independently proven a generalization of the result below [You97]. The generalization covers the whole range of algorithms described in his original paper <ref> [You91b] </ref> instead of the particular version covered here. Theorem 1 GreedyDual-Size is k-competitive, where k is the ratio of the size of the cache to the size of the smallest document. Proof. We will charge each algorithm for the documents they evict instead of the documents they bring in.
Reference: [You97] <author> N. Young. </author> <title> Online file caching. </title> <booktitle> To appear in the Proceedings for the 9th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1998. </year> <pages> Page 14 </pages>
Reference-contexts: It should also be noted that the same bound can be proven for the version of the algorithm which uses c (p) instead of c (p)=s (p) in the description of the algorithm in Figure 5. Young has independently proven a generalization of the result below <ref> [You97] </ref>. The generalization covers the whole range of algorithms described in his original paper [You91b] instead of the particular version covered here. Theorem 1 GreedyDual-Size is k-competitive, where k is the ratio of the size of the cache to the size of the smallest document. Proof.
References-found: 21

