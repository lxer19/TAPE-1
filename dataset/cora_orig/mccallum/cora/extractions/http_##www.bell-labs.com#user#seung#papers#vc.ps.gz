URL: http://www.bell-labs.com/user/seung/papers/vc.ps.gz
Refering-URL: http://www.bell-labs.com/user/seung/entropic.html
Root-URL: 
Title: Vapnik-Chervonenkis entropy of the spherical perceptron  
Author: P. Riegler and H. S. Seung 
Note: 2 permanent address:  
Date: revised December 2, 1996  
Address: Murray Hill, NJ 07974  Universtitat Wurzburg, D-97074 Wurzburg, Germany  
Affiliation: Bell Laboratories, Lucent Technologies  Institut fur Theoretische Physik,  
Abstract: Perceptron learning of randomly labeled patterns is analyzed using a Gibbs distribution on the set of realizable labelings of the patterns. The entropy of this distribution is an extension of the Vapnik-Chervonenkis (VC) entropy, reducing to it exactly in the limit of infinite temperature. The close relationship between the VC and Gardner entropies can be seen within the replica formalism. There has been recent progress towards understanding the relationship between the statistical physics and Vapnik-Chervonenkis (VC) approaches to learning theory[1, 2, 3, 4]. The two approaches can be unified in a statistical mechanics based on the VC entropy. This paper treats the case of learning randomly labeled patterns, or the capacity problem, and extends some of the results of previous work[5, 6] to finite temperature. As will be explained in a companion paper, this extension is important for treating the generalization problem, which occurs in the context of learning patterns labeled by a target rule. Our general framework is illustrated for the simple perceptron sgn(w x), which maps an N -dimensional real-valued input x to a 1-valued output. Given a sample X = (x 1 ; : : : ; x m ) of inputs, the weight vector w determines a labeling L = (l 1 ; : : : ; l m ) of the sample via l i = sgn(w x i ). The weight vector w defines a normal hyperplane that separates the positive from the negative examples. The training error of a labeling L with respect to a reference labeling L 0 is defined by 1 m X 1 l i l 0 and is just the fraction of different labels in the two labelings. We consider the case in which the reference labeling is chosen at random, and address the issue of 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> J. M. R. Parrondo and C. Van den Broeck. </author> <title> Vapnik-chervonenkis bounds for generalization. </title> <journal> J. Phys., </journal> <volume> A26:2211-2223, </volume> <year> 1993. </year>
Reference: [2] <author> A. Engel and C. Van den Broeck. </author> <title> Systems that can learn from examples: replica calculation of uniform convergence bounds for the perceptron. </title> <journal> Phys. Rev. Lett., </journal> <volume> 71 </volume> <pages> 1772-1775, </pages> <year> 1993. </year>
Reference: [3] <author> D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 76-87, </pages> <address> New York, 1994. </address> <publisher> ACM. </publisher>
Reference: [4] <author> M. Opper. </author> <title> Learning and generalization in a two-layer neural network: The role of the vapnik-chervonenkis dimension. </title> <journal> Phys. Rev. Lett., </journal> <volume> 72 </volume> <pages> 2113-2116, </pages> <year> 1994. </year>
Reference: [5] <author> A. Engel and M. Weigt. </author> <title> Multifractal analysis of the coupling space of feedforward neural networks. </title> <journal> Phys. Rev., </journal> <volume> E53:R2064-7, </volume> <year> 1996. </year>
Reference: [6] <author> R. Monasson and D. O'Kane. </author> <title> Domains of solutions and replica symmetry breaking in multilayer neural networks. </title> <journal> Europhys. Lett., </journal> <volume> 27 </volume> <pages> 85-90, </pages> <year> 1994. </year>
Reference: [7] <author> T. M. </author> <title> Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Trans. Electronic Comput., </journal> <volume> 14 </volume> <pages> 326-334, </pages> <year> 1965. </year>
Reference: [8] <author> E. Gardner. </author> <title> The space of interactions in neural network models. </title> <journal> J. Phys., </journal> <volume> A21:257-270, </volume> <year> 1988. </year>
Reference: [9] <author> V. N. Vapnik. </author> <title> Estimation of Dependences based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [10] <author> M. Opper and D. Haussler. </author> <title> Generalization performance of Bayes optimal classification algorithm for learning a perceptron. </title> <journal> Phys. Rev. Lett., </journal> <volume> 66 </volume> <pages> 2677-2680, </pages> <year> 1991. </year>
Reference: [11] <author> H. S. Seung. </author> <title> Annealed theories of learning. </title> <editor> In J.-H. Oh, C Kwon, and S. Cho, editors, </editor> <booktitle> Neural networks: the statistical mechanics perspective, </booktitle> <pages> pages 32-41, </pages> <address> Singapore, 1995. </address> <publisher> World Scientific. </publisher>
Reference: [12] <author> L. Schlafli. </author> <title> Theorie der vielfachen Kontinuitat (1852). </title> <journal> In Gesammelte Mathematische Abhandlungen, </journal> <volume> volume 1, </volume> <pages> pages 177-392. </pages> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1950. </year>
Reference: [13] <author> E. Gardner and B. Derrida. </author> <title> Optimal storage properties of neural network models. </title> <journal> J. Phys., </journal> <volume> A21:271-284, </volume> <year> 1988. </year>
Reference: [14] <author> R. Erichsen and W. K. Thuemann. </author> <title> Optimal storage of a neural network model: a replica symmetry breaking solution. </title> <journal> J. Phys., </journal> <volume> A26:L61-L68, </volume> <year> 1993. </year>
Reference: [15] <author> P. Majer, A. Engel, and A. Zippelius. </author> <title> Perceptrons above saturation. </title> <journal> J. Phys., </journal> <volume> A26:7405-16, </volume> <year> 1993. </year>
Reference: [16] <author> W. Krauth and M. Mezard. </author> <title> Storage capacity of memory networks with binary couplings. </title> <journal> J. Phys. (Paris), </journal> <volume> 50 </volume> <pages> 3057-3066, </pages> <year> 1989. </year>
Reference: [17] <author> W. Krauth and M. Opper. </author> <title> Critical storage capacity of the j = 1 neural network. </title> <journal> J. Phys., </journal> <volume> 22:L519-L523, </volume> <year> 1989. </year>

References-found: 17

