URL: ftp://ftp.cs.monash.edu.au/pub/annn/TR-96-249.ps
Refering-URL: http://www.cs.monash.edu.au/~annn/cv/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: njitnah,annn@cs.monash.edu.au.  
Phone: tel: (03) 9905 5211. fax: (03) 9905 5146  
Title: Belief Network Inference Algorithms: a Study of Performance Based on Domain Characterisation  
Author: N. Jitnah and A. E. Nicholson 
Address: 3168, Australia,  
Affiliation: Department of Computer Science, Monash University, Clayton, VIC  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S.K. Andersen, K.G. Olesen, F.V. Jensen, and F. Jensen. </author> <title> HUGIN | a shell for building Bayesian belief universes for expert systems. </title> <booktitle> In Proc. of IJCAI-89, </booktitle> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: The implementation is slow and hence for the purposes of algorithm comparison the results are given in terms of numbers of iterations rather than absolute performance times. Much faster computation speed and hence realistic computation time results would be obtained using a system such as the commercially available HUGIN <ref> [1] </ref>.
Reference: [2] <author> I. Beinlich, H. Suermondt, R. Chavez, and G. Cooper. </author> <title> The alarm monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> In Proc. of the 2nd European Conf. on AI in medicine, </booktitle> <pages> pages 689-693, </pages> <year> 1992. </year>
Reference-contexts: In [11] performance results compare LPE to LW, according to size (# of nodes) and connectivity (#arcs/node). In [10] performance results are given comparing exact and LPE evaluation of two example network: the ALARM network <ref> [2] </ref> a medical diagnosis network of 37 nodes (8 roots, 16 leaves and 13 intermediate nodes) and 46 arcs, where the nodes have between 2 and 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) [24] called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only <p> Much faster computation speed and hence realistic computation time results would be obtained using a system such as the commercially available HUGIN [1]. The following networks were used to produce the our experimental results: * the ALARM network <ref> [2] </ref>, * the CANCER network [22], * the M1 network which was obtained by breaking up ALARM along its length into two, then connecting the two pieces sideways, thus obtaining a fatter network to be used to investigate the effect of width, * the DBN network used in [17], * S3,
Reference: [3] <author> K-C. Chang and R. Fung. </author> <title> Refinement and coarsening of bayesian networks. </title> <booktitle> In Proc. of UAI-90, </booktitle> <pages> pages 475-482, </pages> <year> 1990. </year>
Reference-contexts: Thus the proportion of usable runs decreases exponentially with the number of evidence variables. The method of Likelihood Weighting (LW) <ref> [26, 3] </ref> overcomes this problem; the counts of each run are weighted according to how likely the evidence is, given the values 3 generated for the parents of the evidence variables. Hence no samples are wasted. <p> Chang and Fung <ref> [3] </ref> describe a method for abstracting the state space of a network by 4 abstracting the state spaces of selected nodes. The resulting reduction in total state space size leads to faster evaluation of the network.
Reference: [4] <author> Homer L. Chin and Gregory F. Cooper. </author> <title> Bayesian belief network inference using simulation. </title> <booktitle> In Uncertainty in Artificial Intelligence 3, </booktitle> <pages> pages 129-147, </pages> <year> 1989. </year>
Reference-contexts: LW and LS both belong to a class of stochastic simulation methods, called forward propagation, because values are first assigned to root nodes, then propagated towards the leaves along the direction of the arcs. Evidence Reversal <ref> [4] </ref> is a technique in which evidence nodes are turned into root nodes by reversing any incoming arcs and recomputing the appropriate conditional probabilities. The advantage of this process is that it produces a version of the network which is very suitable for forward propagation. <p> Node removal, node merging and network pruning are some other ways of obtaining an approximate model, simpler than the original network. These methods are described in <ref> [4] </ref>. 5 Domain Characterisation We characterise a problem by obtaining measurements for a set of features of the network, individual nodes, the set of instantiated nodes (evidence) and the set of queried nodes (targets). These measurements are taken prior to each experiment and are used later for algorithm performance comparison.
Reference: [5] <author> G.F. Cooper. </author> <title> The computational complexity of probabilistic inference using bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405, </pages> <year> 1990. </year>
Reference-contexts: Evidence of specific values of some nodes is incorporated into decision-making by belief updating of other nodes of interest, the query nodes. There are a number of exact and approximate inference algorithms available for belief updating. In general, the complexity of exact inference for multiply-connected networks is NP-hard <ref> [5] </ref>. Cooper [5] suggests that the complexity problem should be tackled by designing efficient special-case, average-case and approximation algorithms, rather than searching for a general, efficient exact inference algorithm. The approximation task is also NP-hard [6]. <p> There are a number of exact and approximate inference algorithms available for belief updating. In general, the complexity of exact inference for multiply-connected networks is NP-hard <ref> [5] </ref>. Cooper [5] suggests that the complexity problem should be tackled by designing efficient special-case, average-case and approximation algorithms, rather than searching for a general, efficient exact inference algorithm. The approximation task is also NP-hard [6].
Reference: [6] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in belief networks is NP-hard. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 141-153, </pages> <year> 1993. </year>
Reference-contexts: In general, the complexity of exact inference for multiply-connected networks is NP-hard [5]. Cooper [5] suggests that the complexity problem should be tackled by designing efficient special-case, average-case and approximation algorithms, rather than searching for a general, efficient exact inference algorithm. The approximation task is also NP-hard <ref> [6] </ref>. One class of approximate inference algorithms is based on stochastic simulation; a number of variations have been proposed. Another way to reduce the computational complexity is by approximating (i.e. simplifying) the model, for example by removing arcs [19], or abstracting the state space [28]. <p> Rather than using a percentage, the probabilities can also be estimated by counting the frequencies with which relevant events occur. Note that stochastic simulation is a special case of Gibbs Sampling as described in [14]. While stochastic simulation algorithms are quite effective if the network contains no evidence <ref> [6] </ref>, often events of interest occur only rarely, so when we have evidence for rare events, most simulation rounds have different values for the evidence variable and have to be discarded, as in the variant of stochastic simulation called Logic Sampling (LS) [12].
Reference: [7] <author> B. D'Ambrosio. </author> <title> Incremental probabilistic inference. </title> <booktitle> In Proc. of UAI-93, </booktitle> <pages> pages 301-308, </pages> <year> 1993. </year>
Reference-contexts: where the nodes have between 2 and 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) [24] called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only part of the problem and limit the potential effect of the rest include Bounded Conditioning [13], Incremental SPI <ref> [7] </ref> and Poole's search-based algorithms [23]; see [10] for a comparison to LPE. Approximate evaluation based on conditioning can be done by evaluating only some of the polytrees generated.
Reference: [8] <author> T. Dean and M. Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 49-54. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference-contexts: This version of state space abstraction is tested on a set of example networks and the relative error of the abstracted network is shown to decrease as the granularity of abstraction is refined; by using successive refinements of granularity they produce an "anytime" algorithm <ref> [8] </ref>. The time to exactly evaluate (using the Jensen algorithm) one network abstracted at different levels is also shown. Kjaerulff [19] describes a method based on the removal of weak dependencies, that is arcs, in the join tree.
Reference: [9] <author> T. Dean and M. P. Wellman. </author> <title> Planning and control. </title> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, Ca., </address> <year> 1991. </year>
Reference-contexts: The time for one pass is thus proportional to the length of the longest root-leaf path. In order to compare to the Jensen algorithm, we also record the join-tree cost, JTC, as proposed by Kanazawa <ref> [9] </ref> for assessing the computational expense of evaluating a belief network. A measure of the skewness of the CPDs for each nodes is computed as follows.
Reference: [10] <author> D. Draper. </author> <title> Localized Partial Evaluation of Belief Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Washington, </institution> <year> 1995. </year>
Reference-contexts: Other algorithms reduce the complexity by computing probability intervals rather than exact probabilities, or do not update beliefs for all nodes in the network, only those of interest (e.g. <ref> [10] </ref>). When new belief updating algorithms, or variants on existing algorithms, are presented in the literature, usually a comparison is made only to one or two other algorithms, on one or two example networks. <p> Note also, that with the simple structure within each time slice, the network as a whole is in fact a tree, and hence performance results on this network may not be indicative of results on more complex structures. Localized Partial Evaluation (LPE) <ref> [11, 10] </ref> computes interval bounds on the marginal probability of a specified query node by examining a subset of nodes in the entire network. Basically, LPE ignores parts of the network which are too far away from the query node to have much impact on its value. <p> Basically, LPE ignores parts of the network which are too far away from the query node to have much impact on its value. In [11] performance results compare LPE to LW, according to size (# of nodes) and connectivity (#arcs/node). In <ref> [10] </ref> performance results are given comparing exact and LPE evaluation of two example network: the ALARM network [2] a medical diagnosis network of 37 nodes (8 roots, 16 leaves and 13 intermediate nodes) and 46 arcs, where the nodes have between 2 and 4 states; and a subnetwork of CPCS-BN (364 <p> 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) [24] called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only part of the problem and limit the potential effect of the rest include Bounded Conditioning [13], Incremental SPI [7] and Poole's search-based algorithms [23]; see <ref> [10] </ref> for a comparison to LPE. Approximate evaluation based on conditioning can be done by evaluating only some of the polytrees generated.
Reference: [11] <author> D. L. Draper and S. Hanks. </author> <title> Localized partial evaluation of a belief network. </title> <booktitle> In Proc. of UAI-94, </booktitle> <pages> pages 170-177, </pages> <year> 1994. </year>
Reference-contexts: Note also, that with the simple structure within each time slice, the network as a whole is in fact a tree, and hence performance results on this network may not be indicative of results on more complex structures. Localized Partial Evaluation (LPE) <ref> [11, 10] </ref> computes interval bounds on the marginal probability of a specified query node by examining a subset of nodes in the entire network. Basically, LPE ignores parts of the network which are too far away from the query node to have much impact on its value. <p> Basically, LPE ignores parts of the network which are too far away from the query node to have much impact on its value. In <ref> [11] </ref> performance results compare LPE to LW, according to size (# of nodes) and connectivity (#arcs/node).
Reference: [12] <author> M. Henrion. </author> <title> Propagating uncertainty in bayesian networks by logic sampling. </title> <editor> In J. Lemmer and L. Kanal, editors, </editor> <booktitle> Proc. of UAI-88, </booktitle> <pages> pages 149-163. </pages> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: quite effective if the network contains no evidence [6], often events of interest occur only rarely, so when we have evidence for rare events, most simulation rounds have different values for the evidence variable and have to be discarded, as in the variant of stochastic simulation called Logic Sampling (LS) <ref> [12] </ref>. Thus the proportion of usable runs decreases exponentially with the number of evidence variables.
Reference: [13] <author> E.J. Horvitz, H.J. Suermondt, and G.F. Cooper. </author> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proc. of UAI-89, </booktitle> <pages> pages 182-193, </pages> <year> 1989. </year>
Reference-contexts: and 46 arcs, where the nodes have between 2 and 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) [24] called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only part of the problem and limit the potential effect of the rest include Bounded Conditioning <ref> [13] </ref>, Incremental SPI [7] and Poole's search-based algorithms [23]; see [10] for a comparison to LPE. Approximate evaluation based on conditioning can be done by evaluating only some of the polytrees generated.
Reference: [14] <author> Thomas Hrycej. </author> <title> Gibbs sampling in bayesian networks. </title> <booktitle> In Artificial Intelligence, </booktitle> <pages> pages 351-363, </pages> <year> 1990. </year>
Reference-contexts: Rather than using a percentage, the probabilities can also be estimated by counting the frequencies with which relevant events occur. Note that stochastic simulation is a special case of Gibbs Sampling as described in <ref> [14] </ref>.
Reference: [15] <author> M. Hulme. </author> <title> Improved sampling for diagnostic reasoning in bayesian networks. </title> <booktitle> In Proc. of UAI-95, </booktitle> <pages> pages 315-322, </pages> <year> 1995. </year>
Reference-contexts: Hence no samples are wasted. Pradham et al. [24] report that LW converges considerably faster than LS and can also handle very large networks There are a number of techniques for reducing the error that can be used with either LW or LS <ref> [25, 15] </ref>. LW and LS both belong to a class of stochastic simulation methods, called forward propagation, because values are first assigned to root nodes, then propagated towards the leaves along the direction of the arcs.
Reference: [16] <author> F. Jensen and S.K. Andersen. </author> <title> Approximations in Bayesian belief universes for knowledge based systems. </title> <booktitle> In Proc. of UAI-90, </booktitle> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Kjaerulff discusses how link removal compares with a method for simplifying the computation by replacing small probabilities with zero, and using strategies for zero compression <ref> [16] </ref>, and outlines cases in which each technique should be preferred. Node removal, node merging and network pruning are some other ways of obtaining an approximate model, simpler than the original network.
Reference: [17] <author> K. Kanazawa, D. Koller, and S. Russell. </author> <title> Stochastic simulation algorithms for dynamic prob-abilistic networks. </title> <booktitle> In Proc. of UAI-95, </booktitle> <pages> pages 346-351, </pages> <year> 1995. </year>
Reference-contexts: Unfortunately, if the network is highly connected, arc reversal is often too computationally intensive and hence has not been very popular. It is well known that algorithm performance depends on network structure. For example, Kanazawa et al. <ref> [17] </ref> have looked at methods for stochastic simulation which take advantage of the specific structure of Dynamic Belief Networks, but their results were obtained using a very simple network: a 50 time slice network, where each time slice contains a single state variable node, a single evidence node, and with all <p> ALARM network [2], * the CANCER network [22], * the M1 network which was obtained by breaking up ALARM along its length into two, then connecting the two pieces sideways, thus obtaining a fatter network to be used to investigate the effect of width, * the DBN network used in <ref> [17] </ref>, * S3, S6, S6-var, S7 and L6 which were randomly generated. S6 and S6-var are have the same number of nodes and structure, but different state space size. 7 See Table 1 for the profiles of each of these networks. <p> Acknowledgements: Thanks to Mark Peot for electronic discussions and pointers to the literature and to Stuart Russell for providing us with a copy of the DBN used in <ref> [17] </ref>. 15
Reference: [18] <author> J. Kirman. </author> <title> Predicting real-time planner performance by domain characterization. </title> <type> PhD thesis, </type> <institution> Brown Univ., </institution> <year> 1994. </year>
Reference-contexts: The idea that classification of problems can provide an effective way to determine when a particular algorithm is appropriate for a given problem, is developed for planning systems in <ref> [18] </ref>. The particular characteristics of a belief network affect the performance of belief updating algorithms. The domain characterisation presented in this paper gives a structured way to describe network features and provide a framework for predicting algorithm performance.
Reference: [19] <author> U. Kjaerulff. </author> <title> Reduction of computation complexity in bayesian networks through removal of weak dependencies. </title> <booktitle> In Proc. of UAI-94, </booktitle> <pages> pages 374-382, </pages> <year> 1994. </year>
Reference-contexts: The approximation task is also NP-hard [6]. One class of approximate inference algorithms is based on stochastic simulation; a number of variations have been proposed. Another way to reduce the computational complexity is by approximating (i.e. simplifying) the model, for example by removing arcs <ref> [19] </ref>, or abstracting the state space [28]. Other algorithms reduce the complexity by computing probability intervals rather than exact probabilities, or do not update beliefs for all nodes in the network, only those of interest (e.g. [10]). <p> The time to exactly evaluate (using the Jensen algorithm) one network abstracted at different levels is also shown. Kjaerulff <ref> [19] </ref> describes a method based on the removal of weak dependencies, that is arcs, in the join tree.
Reference: [20] <author> S.L. Lauritzen and D.J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 50(2) </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: Rather than transforming the network into a complex polytree, Conditioning algorithms transform the network into many simpler subtrees; conditioning involves breaking the communication pathways along the loops by instantiating a select group of variables. The Jensen version of the Lau-ritzen and Spiegelhalter clustering algorithm <ref> [20] </ref> is the fastest exact clustering algorithm currently available. However, it is computationally intensive and sometimes impractical for evaluating large real-world networks.
Reference: [21] <author> C. Liu and M. Wellman. </author> <title> On state-space abstraction for anytime evaluation of bayesian networks. </title> <booktitle> In IJCAI 95: Anytime Algorithms and Deliberation Scheduling Workshop, </booktitle> <pages> pages 91-98, </pages> <year> 1995. </year>
Reference-contexts: For each state of a child of an abstracted node, the new conditional probability is assigned as an sum of the original conditional probabilities weighted by the prior probability of the abstracted states. No empirical performance results are given. In <ref> [28, 21] </ref>, Wellman and Liu use the same technique of state space abstraction, but instead they set the new conditional probabilities of children of an abstracted node as an average of the original conditional probabilities.
Reference: [22] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Belief (or Bayesian) networks <ref> [22] </ref> have become a popular representation for reasoning under uncertainty, as they integrate a graphical representation of causal relationships with a sound Bayesian foundation. They have been used in a variety of applications including medical diagnosis, model-based vision, robot-vehicle monitoring, traffic monitoring, and automated vehicle control. <p> Any node whose posterior probabilities must be computed for decision-making is called a query node. 3 Algorithms for updating beliefs Belief propagation for singly-connected networks, also called polytrees, can be done efficiently using a simple message passing procedure <ref> [22] </ref>. However, most interesting real-world problems involve multiply-connected networks and then simple belief propagation is not possible; informally, this is because we can no longer be sure that evidence has not already been counted at a node having arrived via another route. <p> Beliefs are then computed by recording the percentage of times that each processor selects a given value." <ref> [22, p. 195] </ref>. Rather than using a percentage, the probabilities can also be estimated by counting the frequencies with which relevant events occur. Note that stochastic simulation is a special case of Gibbs Sampling as described in [14]. <p> Much faster computation speed and hence realistic computation time results would be obtained using a system such as the commercially available HUGIN [1]. The following networks were used to produce the our experimental results: * the ALARM network [2], * the CANCER network <ref> [22] </ref>, * the M1 network which was obtained by breaking up ALARM along its length into two, then connecting the two pieces sideways, thus obtaining a fatter network to be used to investigate the effect of width, * the DBN network used in [17], * S3, S6, S6-var, S7 and L6 <p> The error measure used is the average Kullback-Leibler (KL) distance <ref> [22] </ref> of the approximate posteriors of the query nodes from their true posteriors.
Reference: [23] <author> D. Poole. </author> <title> Average-case analysis of a search algorithm for estimating prior and posterior probabilities in bayesian networks with extreme probabilities. </title> <booktitle> In Proc. of IJCAI-93, </booktitle> <pages> pages 606-612, </pages> <year> 1993. </year>
Reference-contexts: 2 and 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) [24] called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only part of the problem and limit the potential effect of the rest include Bounded Conditioning [13], Incremental SPI [7] and Poole's search-based algorithms <ref> [23] </ref>; see [10] for a comparison to LPE. Approximate evaluation based on conditioning can be done by evaluating only some of the polytrees generated.
Reference: [24] <author> M. Pradham, G. Provan, B. Middleton, and M. Henrion. </author> <title> Knowledge engineering for large belief networks. </title> <booktitle> In Proc. of UAI-94, </booktitle> <pages> pages 484-490, </pages> <year> 1994. </year>
Reference-contexts: The method of Likelihood Weighting (LW) [26, 3] overcomes this problem; the counts of each run are weighted according to how likely the evidence is, given the values 3 generated for the parents of the evidence variables. Hence no samples are wasted. Pradham et al. <ref> [24] </ref> report that LW converges considerably faster than LS and can also handle very large networks There are a number of techniques for reducing the error that can be used with either LW or LS [25, 15]. <p> given comparing exact and LPE evaluation of two example network: the ALARM network [2] a medical diagnosis network of 37 nodes (8 roots, 16 leaves and 13 intermediate nodes) and 46 arcs, where the nodes have between 2 and 4 states; and a subnetwork of CPCS-BN (364 nodes, 732 arcs) <ref> [24] </ref> called BN4 (245 nodes, 356 arcs), Other algorithms which, like LPE, evaluate only part of the problem and limit the potential effect of the rest include Bounded Conditioning [13], Incremental SPI [7] and Poole's search-based algorithms [23]; see [10] for a comparison to LPE.
Reference: [25] <author> R. Shachter and M. Peot. </author> <title> Evidential reasoning using likelihood weighting. </title> <type> Personal Communication, </type> <year> 1989. </year>
Reference-contexts: Hence no samples are wasted. Pradham et al. [24] report that LW converges considerably faster than LS and can also handle very large networks There are a number of techniques for reducing the error that can be used with either LW or LS <ref> [25, 15] </ref>. LW and LS both belong to a class of stochastic simulation methods, called forward propagation, because values are first assigned to root nodes, then propagated towards the leaves along the direction of the arcs.
Reference: [26] <author> R. Shachter and M. Peot. </author> <title> Simulation approaches to general probabilistic inference on belief networks. </title> <booktitle> In Proc. of UAI-89, </booktitle> <pages> pages 311-318, </pages> <year> 1989. </year>
Reference-contexts: Thus the proportion of usable runs decreases exponentially with the number of evidence variables. The method of Likelihood Weighting (LW) <ref> [26, 3] </ref> overcomes this problem; the counts of each run are weighted according to how likely the evidence is, given the values 3 generated for the parents of the evidence variables. Hence no samples are wasted.
Reference: [27] <author> Sampath Srinivas and Jack Breese. </author> <title> Ideal: Influence diagram evaluation and analysis in lisp. </title> <type> Technical Report No. 23, </type> <institution> Rockwell International Science Center, </institution> <year> 1989. </year>
Reference-contexts: When nodes closer to the roots are instantiated, forward simulation performs better because propagating evidence along the arc directions will lead to faster convergence of node values in the generated samples. 6 Results The results given in this section were obtained using the Lisp-based IDEAL belief network development environment <ref> [27] </ref> on a GNU Common Lisp platform. The implementation is slow and hence for the purposes of algorithm comparison the results are given in terms of numbers of iterations rather than absolute performance times.

References-found: 27

