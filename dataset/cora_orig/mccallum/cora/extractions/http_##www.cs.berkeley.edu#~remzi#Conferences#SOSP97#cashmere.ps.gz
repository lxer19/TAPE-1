URL: http://www.cs.berkeley.edu/~remzi/Conferences/SOSP97/cashmere.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~remzi/Conferences/SOSP97/
Root-URL: 
Email: cashmere@cs.rochester.edu  
Title: Cashmere-2L: Software Coherent Shared Memory on a Clustered Remote-Write Network  
Author: Robert Stets, Sandhya Dwarkadas, Nikolaos Hardavellas, Galen Hunt, Leonidas Kontothanassis Srinivasan Parthasarathy, and Michael Scott 
Date: October 1997.  
Note: To appear in the Proceedings of the Sixteenth ACM Symposium on Operating Systems Prin ciples, Saint-Malo, France,  
Address: Rochester One Kendall Sq., Bldg. 700 Rochester, NY 14627-0226 Cambridge, MA 02139  
Affiliation: Department of Computer Science DEC Cambridge Research Lab University of  
Abstract: Low-latency remote-write networks, such as DEC's Memory Channel, provide the possibility of transparent, inexpensive, large-scale shared-memory parallel computing on clusters of shared memory multiprocessors (SMPs). The challenge is to take advantage of hardware shared memory for sharing within an SMP, and to ensure that software overhead is incurred only when actively sharing data across SMPs in the cluster. In this paper, we describe a two-level software coherent shared memory systemCashmere-2L that meets this challenge. Cashmere-2L uses hardware to share memory within a node, while exploiting the Memory Channel's remote-write capabilities to implement moderately lazy release consistency with multiple concurrent writers, directories, home nodes, and page-size coherence blocks across nodes. Cashmere-2L employs a novel coherence protocol that allows a high level of asynchrony by eliminating global directory locks and the need for TLB shootdown. Remote interrupts are minimized by exploiting the remote-write capabilities of the Memory Channel network. Cashmere-2L currently runs on an 8-node, 32-processor DEC AlphaServer system. Speedups range from 8 to 31 on 32 processors for our benchmark suite, depending on the application's characteristics. We quantify the importance of our protocol optimizations by comparing performance to that of several alternative protocols that do not share memory in hardware within an SMP, and require more synchronization. In comparison to a one-level protocol that does not share memory in hardware within an SMP, Cashmere-2L improves performance by up to 46%. fl This work was supported in part by NSF grants CDA-9401142, CCR-9319445, CCR-9409120, CCR-9702466, CCR-9705594, and CCR-9510173; ARPA contract F19628-94-C-0057; an external research grant from Digital Equipment Corporation; and a graduate fellowship from Mi-crosoft Research (Galen Hunt). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A Unified Formulation of Four Shared-Memory Models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The protocol is also designed to exploit the intra-node hardware coherence to reduce the demands on the inter-node level. Moderately Lazy Release Consistency Implementation: Cashmere-2L implements a multiple-writer, release consistent protocol. This design decision is enabled by the requirement that 2 applications adhere to the data-race-free programming model <ref> [1] </ref>. Simply stated, shared memory accesses must be protected by locks and barriers that are explicitly visible to the run-time system. The consistency model implementation lies in between TreadMarks [3] and Munin [6]. Invalidations in Munin take effect at the time of a release.
Reference: [2] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the Twenty-Second International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The protocol also includes an optimization that avoids unnecessary diff computation in the case of a single writer. The MGS results indicate that performance improves with larger numbers of processors per node in the cluster. MGS is implemented on the Alewife <ref> [2] </ref> hardware shared memory multiprocessor with a mesh interconnect. In contrast, commercial multiprocessors currently provide a single connection (usually through the I/O bus) that all processors within a node have to share, thus limiting the benefits of larger nodes.
Reference: [3] <author> C. Amza, A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Unfortunately, while small-scale hardware cache-coherent symmetric multiprocessors (SMPs) are now widely available in the market, larger hardware-coherent machines are typically very expensive. Software techniques based on virtual memory have been used to support a shared memory programming model on a network of commodity workstations <ref> [3, 6, 12, 14, 17] </ref>. In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation. <p> This design decision is enabled by the requirement that 2 applications adhere to the data-race-free programming model [1]. Simply stated, shared memory accesses must be protected by locks and barriers that are explicitly visible to the run-time system. The consistency model implementation lies in between TreadMarks <ref> [3] </ref> and Munin [6]. Invalidations in Munin take effect at the time of a release. Invalidations in TreadMarks take effect at the time of a causally related acquire (consistency information is communicated only among synchronizing processes at the time of an acquire). <p> They simulate a system of 8-way, bus-based multiprocessors connected by an ATM network, using a protocol derived from TreadMarks <ref> [3] </ref>, and show that for clustering to provide significant benefits, reduction in inter-node messages and bandwidth requirements must be proportional to the degree of clustering.
Reference: [4] <author> A. Bilas, L. Iftode, D. Martin, and J. P. Singh. </author> <title> Shared Virtual Memory Across SMP Nodes Using Automatic Update: Protocols and Performance. </title> <type> Technical Report TR-517-96, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: At 4 processors, the two-level protocols also benefit from sharing memory in hardware thus avoiding software overheads and extra traffic on the bus. These results only appear to contradict those in <ref> [4, 7, 10] </ref>, which report that bandwidth plays a major role in the performance of clustered systems. Our results compare one and two-level protocols on the same clustered hardware, as opposed to two-level protocols on clustered hardware versus one-level protocols on non-clustered hardware (with consequently higher network bandwidth per processor). <p> Karlsson and Stenstrom [13] examined a similar system in simulation and found that the limiting factor in performance was the latency rather than the bandwidth of the message-level interconnect. Bilas et al. <ref> [4] </ref> present a simulation study of the automatic update release consistent (AURC) protocol on SMP nodes.
Reference: [5] <author> D. L. Black, R. F. Rashid, D. B. Golub, C. R. Hill, and R. V. Baron. </author> <title> Translation Lookaside Buffer Consistency: A Software Approach. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 113-122, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: a description 1 When a process reduces access permissions on a page in a shared address space, it must generally interrupt the execution of any processes executing in that address space on other processors, in order to force them to flush their TLBs, and to update the page table atomically <ref> [5, 16, 20] </ref>.
Reference: [6] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Unfortunately, while small-scale hardware cache-coherent symmetric multiprocessors (SMPs) are now widely available in the market, larger hardware-coherent machines are typically very expensive. Software techniques based on virtual memory have been used to support a shared memory programming model on a network of commodity workstations <ref> [3, 6, 12, 14, 17] </ref>. In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation. <p> All processors on a node share the same physical frame for a shared data page. We employ a moderately lazy VM-based implementation of release consistency, with multiple concurrent writers, directories, home nodes, and page-size coherence blocks. Updates by multiple writers are propagated to the home node using diffs <ref> [6] </ref>. Cashmere-2L exploits the capabilities of a low-latency remote-write network 1 to apply these outgoing diffs without remote assistance, and to implement low-cost directories, notification queues, and application locks and barriers. <p> This design decision is enabled by the requirement that 2 applications adhere to the data-race-free programming model [1]. Simply stated, shared memory accesses must be protected by locks and barriers that are explicitly visible to the run-time system. The consistency model implementation lies in between TreadMarks [3] and Munin <ref> [6] </ref>. Invalidations in Munin take effect at the time of a release. Invalidations in TreadMarks take effect at the time of a causally related acquire (consistency information is communicated only among synchronizing processes at the time of an acquire).
Reference: [7] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: a Case Study. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: A low-latency network reduces the time that the program must wait for those operations to complete. While software shared memory has been an active area of research for many years, it is only recently that protocols for clustered systems have begun to be developed <ref> [7, 10, 13, 22] </ref>. The challenge for such a system is to take advantage of hardware shared memory for sharing within an SMP, and to ensure that software overhead is incurred only when actively sharing data across SMPs in the cluster. <p> Running the application on an SMP without linking it to any protocol code takes 51.8 seconds, a 21% drop in performance from the 4:1 1LD protocol case. Cox et al. <ref> [7] </ref> report similar results in their comparison of hardware and software shared memory systems. LU exhibits negative clustering effects only for the one-level protocols. The 4:4, 8:4, and 16:4 configurations experience a significant performance drop, which is not present in either of the two-level protocols. <p> At 4 processors, the two-level protocols also benefit from sharing memory in hardware thus avoiding software overheads and extra traffic on the bus. These results only appear to contradict those in <ref> [4, 7, 10] </ref>, which report that bandwidth plays a major role in the performance of clustered systems. Our results compare one and two-level protocols on the same clustered hardware, as opposed to two-level protocols on clustered hardware versus one-level protocols on non-clustered hardware (with consequently higher network bandwidth per processor). <p> The remaining applications have relatively few accesses to these structures and show no significant differences between the lock-free or lock-based approaches. 4 Related Work Cox et al. were among the first to study layered hardware/software coherence protocols <ref> [7] </ref>. They simulate a system of 8-way, bus-based multiprocessors connected by an ATM network, using a protocol derived from TreadMarks [3], and show that for clustering to provide significant benefits, reduction in inter-node messages and bandwidth requirements must be proportional to the degree of clustering.
Reference: [8] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings, Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The Barnes-Hut tree construction is performed sequentially, while all other phases are parallelized and dynamically load balanced. Synchronization consists of barriers between phases. Em3d: a program to simulate electromagnetic wave propagation through 3D objects <ref> [8] </ref>. The major data structure is an array that contains the set of magnetic and electric nodes. These are equally distributed among the processors in the system.
Reference: [9] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of General Linkage Analysis Problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: A synchronization flag for each row indicates when it is available to other rows for use as a pivot. Ilink: a widely used genetic linkage analysis program from the FASTLINK 2.3P package that locates disease genes on chromosomes. We use the parallel algorithm described in <ref> [9] </ref>. The main shared data is a pool of sparse arrays of genotype probabilities. For load balance, non-zero elements are assigned to processors in a round-robin fashion. The computation is master-slave, with one-to-all and all-to-one data communication. Barriers are used for synchronization.
Reference: [10] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. Soft-FLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-220, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: A low-latency network reduces the time that the program must wait for those operations to complete. While software shared memory has been an active area of research for many years, it is only recently that protocols for clustered systems have begun to be developed <ref> [7, 10, 13, 22] </ref>. The challenge for such a system is to take advantage of hardware shared memory for sharing within an SMP, and to ensure that software overhead is incurred only when actively sharing data across SMPs in the cluster. <p> The use of two-way diffing does not provide significant performance advantages over the more common, TLB-shootdown implementation of the two-level protocol. There are three reasons for this contradiction of previous findings by other researchers <ref> [10] </ref> that have characterized TLB shootdown as a significant source of overhead for software DSM on clustered architectures. <p> In all other respects, the shootdown protocol is the same as Cashmere-2L. It should be noted that this shootdown protocol is significantly less synchronousthan single-writer alternatives <ref> [10, 22] </ref>. It allows writable copies of a page to exist on multiple nodes concurrently. Single-writer protocols must shoot down write permission on all processors of all other nodes when one processor takes a write fault. One-level protocols We also present results for two one-level protocols. <p> At 4 processors, the two-level protocols also benefit from sharing memory in hardware thus avoiding software overheads and extra traffic on the bus. These results only appear to contradict those in <ref> [4, 7, 10] </ref>, which report that bandwidth plays a major role in the performance of clustered systems. Our results compare one and two-level protocols on the same clustered hardware, as opposed to two-level protocols on clustered hardware versus one-level protocols on non-clustered hardware (with consequently higher network bandwidth per processor). <p> This result stands in sharp contrast to results reported by others (e.g. SoftFLASH <ref> [10] </ref>), and is explained by our use of a multi-writer protocol and our implementation of the shootdown mechanism. Shootdown happens in SoftFLASH mainly when a page is stolen by a remote processor, and all local mappings must be eliminated. In Cashmere-2L, pages are never stolen. <p> MGS is implemented on the Alewife [2] hardware shared memory multiprocessor with a mesh interconnect. In contrast, commercial multiprocessors currently provide a single connection (usually through the I/O bus) that all processors within a node have to share, thus limiting the benefits of larger nodes. SoftFLASH <ref> [10] </ref> is a kernel-level implementation of a two-level coherence protocol, on a cluster of SMPs. The protocol is based on the hardware coherent FLASH protocol. Shared data is tracked via TLB faults, and thus TLB shootdown with costly inter-processor interrupts is required to avoid consistency problems.
Reference: [11] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 12-18, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation. Low-latency remote-write networks, such as DEC's Memory Channel <ref> [11] </ref>, provide the possibility of transparent and inexpensive shared memory. These networks allow processors in one node to modify the memory of another node safely from user space, with very low (microsecond) latency.
Reference: [12] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving Release-Consistent Shared Virtual Memory Using Automatic Update. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Unfortunately, while small-scale hardware cache-coherent symmetric multiprocessors (SMPs) are now widely available in the market, larger hardware-coherent machines are typically very expensive. Software techniques based on virtual memory have been used to support a shared memory programming model on a network of commodity workstations <ref> [3, 6, 12, 14, 17] </ref>. In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation.
Reference: [13] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance Evaluation of a Cluster-Based Multiprocessor Built from ATM Switches and Bus-Based Multiprocessor Servers. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 4-13, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: A low-latency network reduces the time that the program must wait for those operations to complete. While software shared memory has been an active area of research for many years, it is only recently that protocols for clustered systems have begun to be developed <ref> [7, 10, 13, 22] </ref>. The challenge for such a system is to take advantage of hardware shared memory for sharing within an SMP, and to ensure that software overhead is incurred only when actively sharing data across SMPs in the cluster. <p> They simulate a system of 8-way, bus-based multiprocessors connected by an ATM network, using a protocol derived from TreadMarks [3], and show that for clustering to provide significant benefits, reduction in inter-node messages and bandwidth requirements must be proportional to the degree of clustering. Karlsson and Stenstrom <ref> [13] </ref> examined a similar system in simulation and found that the limiting factor in performance was the latency rather than the bandwidth of the message-level interconnect. Bilas et al. [4] present a simulation study of the automatic update release consistent (AURC) protocol on SMP nodes.
Reference: [14] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, and M. L. Scott. </author> <title> VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In Proceedings of the Twenty-Fourth International Symposium on Computer Architecture, </booktitle> <pages> pages 157-169, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Unfortunately, while small-scale hardware cache-coherent symmetric multiprocessors (SMPs) are now widely available in the market, larger hardware-coherent machines are typically very expensive. Software techniques based on virtual memory have been used to support a shared memory programming model on a network of commodity workstations <ref> [3, 6, 12, 14, 17] </ref>. In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation. <p> We have experimented with both interrupt- and polling-based approaches to handling explicit requests. Polling provides better performance in almost every case (TSP is the only exception in our application suite see <ref> [14] </ref> for more details on a comparison of polling versus interrupts on our platform). Polling requires that processors check for messages frequently, and branch to a handler if one has arrived. <p> Subsequent release operations within the node will then realize that the local modifications have already been flushed, and will avoid overwriting more recent changes to the home by other nodes. 2.5.1 Prior Work In earlier work on a one-level protocol <ref> [14] </ref>, we used write-through to the home node to propagate local changes on the fly. On the current Memory Channel, which has only modest cross-sectional bandwidth, the results in Section 3 indicate that twins and diffs perform better. <p> Single-writer protocols must shoot down write permission on all processors of all other nodes when one processor takes a write fault. One-level protocols We also present results for two one-level protocols. The first of these (Cashmere-1L) is described in more detail in a previous paper <ref> [14] </ref>. In addition to treating each processor as a separate node, it doubles its writes to shared memory on the fly using extra in-line instructions. Each write is sent both to the local copy of the page and to the home node copy. <p> On a remote node, the write to the home node copy goes to I/O space which bypasses all caches.) See <ref> [14] </ref> for more details. In addition, for all applications, write-through on the Memory Channel often incurs the full latency and overhead of Memory Channel access for each word written (in addition to contention). <p> We present the results of 1L mainly as a base point for comparison to our earlier paper describing the one-level protocol with write-through <ref> [14] </ref>. (Note that these latest results have been executed on a new version of the Memory Channel which offers slightly higher bandwidth.) We will use 1LD (without the home-node optimization) as the basis for comparison of the two-level protocols in the rest of this section since it has the best over-all
Reference: [15] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Home node selection. Home nodes are initially assigned in a round robin manner, and then are re-assigned dynamically after program initialization to the processor that first touches a page. <ref> [15] </ref>. To relocate a page a processor must acquire a global lock and explicitly request a remapping from the initial home node. Because we only relocate once, the use of locks does not impact performance.
Reference: [16] <author> B. Rosenburg. </author> <title> Low-Synchronization Translation Lookaside Buffer Consistency in Large-Scale Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 137-146, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: a description 1 When a process reduces access permissions on a page in a shared address space, it must generally interrupt the execution of any processes executing in that address space on other processors, in order to force them to flush their TLBs, and to update the page table atomically <ref> [5, 16, 20] </ref>.
Reference: [17] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <note> Shasta: </note>
Reference-contexts: Unfortunately, while small-scale hardware cache-coherent symmetric multiprocessors (SMPs) are now widely available in the market, larger hardware-coherent machines are typically very expensive. Software techniques based on virtual memory have been used to support a shared memory programming model on a network of commodity workstations <ref> [3, 6, 12, 14, 17] </ref>. In general, however, the high latencies of traditional networks have resulted in poor performance relative to hardware shared memory for applications requiring frequent communication. Recent technological advances are changing the equation.
References-found: 17

