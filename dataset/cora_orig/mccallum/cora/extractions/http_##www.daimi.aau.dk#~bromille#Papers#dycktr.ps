URL: http://www.daimi.aau.dk/~bromille/Papers/dycktr.ps
Refering-URL: http://www.daimi.aau.dk/~bromille/Papers/index.html
Root-URL: http://www.daimi.aau.dk
Title: DYNAMIC ALGORITHMS FOR THE DYCK LANGUAGES  
Author: GUDMUND SKOVBJERG FRANDSEN, THORE HUSFELDT, PETER BRO MILTERSEN, THEIS RAUHE, AND SOREN SKYUM 
Date: January 1995  
Note: 11th  
Address: Ny Munkegade, DK-8000 Arhus C, Denmark  
Affiliation: BRICS Department of Computer Science, University of Aarhus,  
Abstract: We study dynamic membershipproblems for the Dyck languages, the class of strings of properly balanced parentheses. We also study the Dynamic Word problem for the free group. We present deterministic algorithms and data structures which maintain a string under replacements of symbols, insertions, and deletions of symbols, and language membership queries. Updates and queries are handled in polylogarithmic time. We also give both Las Vegas- and Monte Carlo-type randomised algorithms to achieve better running times, and present lower bounds on the complexity for variants of the problems. fl Basic Research in Computer Science, Centre of the Danish National Research Foundation. This work was partially supported by the ESPRIT II Basic Research Actions Program of the EC under contract no. 7141 (project ALCOM II). Gudmund Frandsen was partially supported by CCI-Europe. Peter Bro Miltersen was partially supported by a grant from the Danish Natural Science Research Council, part of his research was done done at the Department of Computer Science, University of Toronto. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Paul F. Dietz, </author> <title> Optimal algorithms for list indexing and subset rank, </title> <booktitle> Proc. First Workshop on Algorithms and Data Structures (WADS) (F. </booktitle> <editor> Dehne, J.-R. Sack, and N. Santoro, eds.), </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 382, </volume> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989, </year> <pages> pp. 39-46. </pages>
Reference-contexts: The solution does not extend to the extended set of operations, in fact a larger lower bound is proved below. We leave it to the reader to prove a logarithmic upper bound. Using am algorithm by Dietz <ref> [1] </ref> a solution with an O (log n= log log n) upper bound on the amortised complexity for the extended set of operations can also be found. Proposition 2.2. The Dynamic Membership problem for D 1 can be done in O (log n) time per operation. 5 Proof.
Reference: 2. <author> Gudmund Skovbjerg Frandsen, Peter Bro Miltersen, and Sven Skyum, </author> <title> Dynamic word problems, </title> <booktitle> Proc 34th Ann. Symp. on Foundations of Computer Science (FOCS), </booktitle> <year> 1993, </year> <pages> pp. 470-479. </pages>
Reference-contexts: indeed separate the complexity of the two problems: for D 1 , we prove a lower bound of (log n= log log n) by a technique of Fredman [3], while we can bound the complexity of D 0 1 from above by O (log log n) using a construction of <ref> [2] </ref>. The latter bound is shown to be tight by a reduction from the Dynamic Word problem for the monoid (f0; 1g; _), for which a lower bound is given in [2]. <p> can bound the complexity of D 0 1 from above by O (log log n) using a construction of <ref> [2] </ref>. The latter bound is shown to be tight by a reduction from the Dynamic Word problem for the monoid (f0; 1g; _), for which a lower bound is given in [2]. The upper bound for D 1 is O (log n log log n), not quite matching the lower bound. These results are only valid for the restricted set of operations (and are only of theoretical interest anyway). The table below summarises these results. Table 3: Bit probe complexity. <p> One can phrase this even stronger in terms of circuit complexity: all Dyck languages are complete for TC 0 under AC 0 -reductions, (this appears to be folklore). Dynamic Word and Prefix problems for finite monoids are studied in <ref> [2, 11] </ref>. The free group of k generators studied in the present paper is infinite. Turning from context-free to regular languages, it is easy to find logarithmic time algorithms for the Dynamic Membership problem for the latter class. <p> The free group of k generators studied in the present paper is infinite. Turning from context-free to regular languages, it is easy to find logarithmic time algorithms for the Dynamic Membership problem for the latter class. The results from <ref> [2] </ref> give better upper bounds depending on the language's syntactic monoid M (L). 1.5. Preliminaries and notation. <p> A fast one-bit counter. Since both Propositions 2.2 and 2.1 were based on counting, we will briefly review a construction from <ref> [2] </ref> that will be useful to us: a fast counter for cell size one. The counter maintains a value c from some interval fl; : : : ; hg, for integers l; h. <p> It can be implemented such that incrementing and decrementing can be performed in time log log (h l) + O (1) and the test operations takes constant time, see <ref> [2] </ref>. 5.3. The two-sided case. The next two results give tight bounds on the bit probe complexity complexity of the language D 0 1 . Proposition 5.1. <p> Proposition 5.2. The Dynamic Membership problem for D 0 1 requires (log log n) time per operation in the bit probe model. Proof. We use a result from <ref> [2] </ref>, that the Dynamic Word problem for any commutative non-group requires time (log log n) in the bit probe model.
Reference: 3. <author> Michael L. Fredman, </author> <title> The complexity of maintaining an array and computing its partial sums, </title> <journal> Journal of the ACM 29 (1982), </journal> <pages> 250-260. </pages>
Reference-contexts: more restricted model for dynamic algorithms, namely the cell probe model with cell size 1 (the bit probe model ), we can indeed separate the complexity of the two problems: for D 1 , we prove a lower bound of (log n= log log n) by a technique of Fredman <ref> [3] </ref>, while we can bound the complexity of D 0 1 from above by O (log log n) using a construction of [2]. <p> The technique is due to Fredman. Proposition 5.4. The Dynamic Membership problem for D 1 requires log log n time par operation in the bit probe model. Proof. Fredman <ref> [3] </ref> essentially shows the stated lower bound on the complexity of the Which Side? problem, which is to maintain a single value t 2 f1; : : :; ng under the following operations: change (i): set t = i, which side (i)?: return `left' if t &lt; i and `right' if
Reference: 4. <author> Michael L. Fredman and Michael E. Saks, </author> <title> The cell probe complexity of dynamic data structures, </title> <booktitle> Proc. 21st Ann. Symp. on Theory of Computing (STOC), </booktitle> <year> 1989, </year> <pages> pp. 345-354. </pages>
Reference-contexts: If instead we allow insertion and deletion, a lower bound of (log n= log log n) can be derived from a result of Fredman and Saks <ref> [4] </ref>. The same lower bound holds if we replace member by match in the restricted of operations (for one-sided languages). 3 Table 2: Lower bounds for logarithmic word size. <p> First, we will modify the operations by replacing member with match and show a lower bound that is valid for all one-sided Dyck languages. The first two lower bounds use a result of Fredman and Saks <ref> [4] </ref> that gives a lower bound of (log n= log log n) on the (amortised) complexity of the Dynamic Parity Prefix problem: given a vector x 1 ; : : : ; x n of bits, maintain a data structure that is able to react to the following operations for all <p> maintain a list L 2 f0; 1g fl under the following operations: insert (i): insert `1' between the (i 1)th and the ith element, delete (i): delete the ith element, value (i): return L i . 13 The lower bound is stated without proof for a slightly different problem in <ref> [4] </ref>, we give the proof here for completeness. Lemma 6.1. List Representation requires log log n time per operation with logarithmic cell size. Proof. Let x 2 f0; 1g n be an instance of the Parity Prefix problem and assume without loss of generality that n is even.
Reference: 5. <author> Leo J. Guibas and Robert Sedgewick, </author> <title> A dichromatic framework for balanced trees, </title> <booktitle> Proc. 19th Ann. Symp. on Foundations of Computer Science (FOCS), IEEE Computer Society, </booktitle> <year> 1978, </year> <pages> pp. 8-21. </pages>
Reference-contexts: The data structure is easily generalised to the extended set of operations. Most complicated are the insert and delete operations. To accommodate these, we have to maintain balance in the tree using any scheme for balancing dynamic search trees, e.g. red-black trees <ref> [5] </ref>. We will not comment on such extensions any further; the reader can check that they are also possible for all the algorithms in Sections 3 and 4.
Reference: 6. <author> Michael A. Harrison, </author> <title> Introduction to formal language theory, </title> <publisher> Addison-Wesley, </publisher> <year> 1978. </year>
Reference-contexts: For example, a 1 a 2 a 2 a 1 2 D 0 2 because a 1 1 a 2 a 1 2 a 1 evaluates to unity. The Dyck languages are covered in detail in Harrison's classical treatment <ref> [6] </ref>. 1.2. The Dynamic Membership problem. In this paper we consider the problem of maintaining membership in D k or D 0 k of a string from (A [ A) n dynamically. <p> Related results. It is interesting that all Dyck languages seem to be equally hard in most non-dynamic computational models. Ritchie and Spring-steel [13] showed that the one-sided Dyck languages are in deterministic logspace, Lipton and Zalcstein [8] extended this to the two-sided case (see also <ref> [6, Exercises 22 and 23] </ref>). One can phrase this even stronger in terms of circuit complexity: all Dyck languages are complete for TC 0 under AC 0 -reductions, (this appears to be folklore). Dynamic Word and Prefix problems for finite monoids are studied in [2, 11]. <p> All logarithms are base two. We call a string reduced if it contains no neighbouring pair of matching parentheses. So, for the one-sided case, ([ ]) is not reduced but [ )( is. In the two-sided case, the latter is not reduced. To formalise this (following Harrison <ref> [6] </ref>), we introduce two mappings 1 ; 2 : (A [ A) fl ! (A [ A) fl : We want 1 (u) and 2 (u) to be the reduced form of u using one- and two-sided cancellation, respectively.
Reference: 7. <author> Richard M. Karp and Michael O. Rabin, </author> <title> Efficient randomised pattern-matching algorithms, </title> <journal> IBM J. Res. Develop. </journal> <volume> 31 (1987), no. 2, </volume> <pages> 249-260. </pages>
Reference-contexts: We achieve better bounds for Monte Carlo-style algorithms. Using the fingerprint method of Karp and Rabin <ref> [7] </ref>, where strings are represented by (nonunique) fingerprints in the form of a matrix product modulo a small randomly chosen prime, D k can be done in time O (log 2 n) and D 0 k in time O (log n). <p> Monte Carlo Algorithms 4.1. The two-sided case. We begin with D 0 k , which is quite simple. We use the well-known fingerprint string matching technique of Karp and Rabin <ref> [7] </ref>. Proposition 4.1. The Dynamic Membership problem for D 0 k can be done in O (log n) time per operation such that the probability of an erroneous answer in any sequence of n updates is O ( 1 n ). Proof. <p> This suggests a randomised algorithms in the spirit of <ref> [7] </ref>: compute h (x) modulo a randomly chosen prime p and check whether the result is the identity matrix. For the dynamic version we need to maintain h (x) mod p under updates to x, we write n for jxj.
Reference: 8. <author> Richard J. Lipton and Yechezkel Zalcstein, </author> <title> Word problems solvable in logspace, </title> <journal> Journal of the ACM 24 (1977), </journal> <volume> no. 3, </volume> <pages> 522-526. </pages>
Reference-contexts: Related results. It is interesting that all Dyck languages seem to be equally hard in most non-dynamic computational models. Ritchie and Spring-steel [13] showed that the one-sided Dyck languages are in deterministic logspace, Lipton and Zalcstein <ref> [8] </ref> extended this to the two-sided case (see also [6, Exercises 22 and 23]). One can phrase this even stronger in terms of circuit complexity: all Dyck languages are complete for TC 0 under AC 0 -reductions, (this appears to be folklore). <p> Following Lipton and Zalcstein <ref> [8] </ref> (see also [9, Problem 2.3.13]), we represent (A [ A) fl = as a group of 2 fi 2 integer matrices using the group homomorphism h : (A [ A) fl = ! M 2 (Z); i 1 2 j i 1 0 j In this terminology, x is in
Reference: 9. <author> Wilhelm Magnus, Abraham Karrass, and Donald Solitar, </author> <title> Combinatorial group theory, </title> <journal> Pure and Applied Mathematics, </journal> <volume> vol. 13, </volume> <publisher> Interscience Publishers, </publisher> <year> 1966. </year>
Reference-contexts: Following Lipton and Zalcstein [8] (see also <ref> [9, Problem 2.3.13] </ref>), we represent (A [ A) fl = as a group of 2 fi 2 integer matrices using the group homomorphism h : (A [ A) fl = ! M 2 (Z); i 1 2 j i 1 0 j In this terminology, x is in D 0 2 <p> Indeed, if for 1 i k we put c i = g i 1 g i 2 then c 1 ; : : : ; c k generate a free group, see <ref> [9, Problem 1.4.12] </ref>. 4.2. The one-sided case. The algorithm for D k is somewhat more difficult. We will combine the tree-structure we used for the deterministic algorithm for D k (Proposition 3.2) with the Monte Carlo algorithm for D 0 k from the last proposition.
Reference: 10. <author> K. Mehlhorn, R. Sundar, and C. Uhrig, </author> <title> Maintaining dynamic sequences under equality-tests in polylogarithmic time, </title> <booktitle> Proc. 5th Ann. Symp. on Discrete Algorithms (SODA), ACM-SIAM, </booktitle> <year> 1994, </year> <pages> pp. 213-222. </pages>
Reference-contexts: Our main result is that the Dynamic Membership problem for all Dyck languages can be solved in polylogarithmic time per operation, the bound exact is O (log 3 n log fl n). We use a technique for maintaining dynamic sequences under equality tests by Mehlhorn, Sundar, and Uhrig <ref> [10] </ref>, which also gives (Las Vegas-style) randomised algorithms that run in slightly better expected time: O (log 3 n). We achieve better bounds for Monte Carlo-style algorithms. <p> At each node, we store entire sequences (rather than just a tuple as above) that are formed from the sequences stored at its children. To this end we first need a recent surprising construction for dynamically maintaining sequences. 3.1. A data structure for strings equality. Mehlhorn, Sundar, and Uhrig <ref> [10] </ref> present a data structure for dynamically maintaining a family of strings under equality tests. We use a slightly modified set of updates that is better suited to our problem. <p> = s i+1 s n , and add them to S, equal (s; s 0 ): return `yes' if and only if s = s 0 , lcp (s; s 0 ): return the length of the longest common prefix of s and s 0 . 6 The techniques from <ref> [10] </ref> can easily be modified to cope with the above updates. The time bounds are summarised in the following lemma: Lemma 3.1 ([10]). <p> This completes the proof. The upper bounds from the last two propositions can be improved to expected time O (log 3 n) by using the Las Vegas variant of the algorithm described in Sec tion 3.1, see <ref> [10] </ref>. 8 4. Monte Carlo Algorithms 4.1. The two-sided case. We begin with D 0 k , which is quite simple. We use the well-known fingerprint string matching technique of Karp and Rabin [7]. Proposition 4.1.
Reference: 11. <author> Peter Bro Miltersen, </author> <title> Lower bounds for union-split-find related problems on random access machines, </title> <booktitle> Proc. 26th Ann. Symp. on Theory of Computing (STOC), ACM, </booktitle> <year> 1994, </year> <pages> pp. 625-634. </pages>
Reference-contexts: However, if the prefix-operation is added, we can get a weak lower bound of (log log n= log log log n) using a technique from <ref> [11] </ref>, obviously, the same bound holds with the mismatch query instead. If instead we allow insertion and deletion, a lower bound of (log n= log log n) can be derived from a result of Fredman and Saks [4]. <p> One can phrase this even stronger in terms of circuit complexity: all Dyck languages are complete for TC 0 under AC 0 -reductions, (this appears to be folklore). Dynamic Word and Prefix problems for finite monoids are studied in <ref> [2, 11] </ref>. The free group of k generators studied in the present paper is infinite. Turning from context-free to regular languages, it is easy to find logarithmic time algorithms for the Dynamic Membership problem for the latter class. <p> Note that by storing S in an ordered list, we achieve a size jSj data structure that makes all queries answerable in time O (jSj). Beame and Fich (personal communication) improving Miltersen <ref> [11] </ref>, have shown that for any scheme (with the stated size bound) there exists a set S for which there is a lower bound of t = log log log n (6.1) on the time t needed for a query.
Reference: 12. <author> Mark H. Overmars, </author> <title> The design of dynamic data structures, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 156, </volume> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1983. </year>
Reference-contexts: Choosing p n 4 randomly and choosing a new p for every n operations by the global rebuilding technique of Overmars <ref> [12] </ref> we guarantee that the probability of an erroneous answer in a sequence of n consecutive queries is bounded by O ( 1 n ).
Reference: 13. <author> R. W. Ritchie and F. N. Springsteel, </author> <title> Language recognition by marking automata, </title> <booktitle> Information and Control 20 (1972), </booktitle> <pages> 313-330. 16 </pages>
Reference-contexts: Language Operations Upper bound Lower bound D 1 change member O (log n log log n) log n 1 change member fi (log log n) 1.4. Related results. It is interesting that all Dyck languages seem to be equally hard in most non-dynamic computational models. Ritchie and Spring-steel <ref> [13] </ref> showed that the one-sided Dyck languages are in deterministic logspace, Lipton and Zalcstein [8] extended this to the two-sided case (see also [6, Exercises 22 and 23]).
References-found: 13

