URL: http://www.cs.unc.edu/~marshall/WWW/PAPERS/exclu-alloc9801.ps.gz
Refering-URL: http://www.cs.unc.edu/~marshall/WWW/index.html
Root-URL: http://www.cs.unc.edu
Title: Generalization and Exclusive Allocation of Credit in Unsupervised Category Learning  
Author: Jonathan A. Marshall Vinay S. Gupta Jonathan A. Marshall, 
Keyword: Running title: Generalization and Exclusive Allocation.  
Note: Send requests for reprints to:  [January 1998, to appear in Network: Computation in Neural Systems.]  
Address: 3175, Sitterson Hall  Chapel Hill, NC 27599-3175, U.S.A.  3175, Sitterson Hall,  Chapel Hill, NC 27599-3175, U.S.A.  919-962-1799.  
Affiliation: Department of Computer Science, CB  University of North Carolina,  Department of Computer Science, CB  University of North Carolina,  
Email: E-mail marshall@cs.unc.edu,  
Phone: telephone 919-962-1887, fax  
Abstract: Acknowledgements: This research was supported in part by the Office of Naval Research (Cognitive and Neural Sciences, N00014-93-1-0208) and by the Whitaker Foundation (Special Opportunity Grant). We thank George Kalarickal, Charles Schmitt, William Ross, and Douglas Kelly for valuable discussions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J.A., Silverstein, J.W., Ritz, S.A. & Jones, R.S. </author> <year> (1977). </year> <title> Distinctive features, categorical perception, and probability learning: Some applications of a neural model. </title> <journal> Psychological Review , 84, </journal> <volume> 5, </volume> <pages> 413-451. </pages>
Reference: <author> Bell, A.J. & Sejnowski, T.J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 1129-1159. </pages>
Reference: <author> Bregman, A.S. </author> <year> (1990). </year> <title> Auditory Scene Analysis: The Perceptual Organization of Sound . Cambridge, </title> <address> MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Carpenter, G.A. & Grossberg, S. </author> <year> (1987). </year> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37, </volume> <pages> 54-115. </pages>
Reference-contexts: First, each network will be described briefly. 3.1.1 Winner-take-all competitive learning Among the simplest unsupervised learning procedures is the winner-take-all (WTA) competitive learning rule, which divides the space of input patterns into hyper-polyhedral decision regions, each centered around a "prototype" pattern. The ART-1 network <ref> (Carpenter & Grossberg, 1987) </ref> and the Kohonen network (Kohonen, 1982) are examples of essentially WTA neural networks. When an input pattern first arrives, it is assigned to the one category whose prototype pattern best matches it. The activation of neurons encoding other categories is suppressed (e.g., through strong inhibition).
Reference: <author> Cohen, M.A. & Grossberg, S. </author> <year> (1986). </year> <title> Neural dynamics of speech and language coding: Developmental programs, perceptual grouping, and competition for short term memory. </title> <journal> Human Neurobiology, </journal> <volume> 5, </volume> <pages> 1-22. </pages>
Reference-contexts: As in EXIN networks, the excitatory learning rule involves prototype modification of output layer competition winners, and the inhibitory learning rule is based on coactivation of the competing neurons; hence SONNET-2 displays the global context-sensitive constraint satisfaction property (abc versus abcd ) and the sequence masking property <ref> (Cohen & Grossberg, 1986, 1987) </ref> (abc versus c) (Nigrin, 1993) displayed by EXIN networks. 3.2.2 Example patterns ab and bc, which are assumed to occur with equal training probability.
Reference: <author> Cohen, M.A. & Grossberg, S. </author> <year> (1987). </year> <title> Masking fields: A massively parallel neural architecture for learning, recognizing and predicting multiple groupings of patterned data. </title> <journal> Applied Optics, </journal> <volume> 26, </volume> <pages> 1866-1891. </pages>
Reference: <author> Comon, P., Jutten, C., & Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part II: Problems statement. </title> <booktitle> Signal Processing, </booktitle> <volume> 24, </volume> <pages> 11-21. </pages>
Reference: <author> Craven, M.W. & Shavlik, J.W. </author> <year> (1994). </year> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 37-45. </pages>
Reference-contexts: Hence, to formalize the two exclusive allocation conditions, a precise way to describe the concepts or category prototypes learned by the network must be provided. Deriving such a description is analogous to the rule-extraction task <ref> (Craven & Shavlik, 1994) </ref>: "Given a trained neural network and the examples used to train it, produce a concise and accurate symbolic description of the network" (p. 38). <p> However, because of the possible presence of lateral interactions, feedback, etc., connection weights may not provide an accurate picture of the patterns learned by the neuron. Another approach would be to use symbolic if-then rules <ref> (Craven & Shavlik, 1994) </ref>. Such a description can be quite comprehensive and elaborate; however, the number of rules required to describe a network can grow exponentially with the number of input features.
Reference: <author> Desimone, R. </author> <year> (1992). </year> <title> Neural circuits for visual attention in the primate brain. In Carpenter, G.A. & Grossberg, </title> <editor> S. (Eds.), </editor> <booktitle> Neural Networks for Vision and Image Processing (pp. </booktitle> <pages> 343-364). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Foldiak, P. </author> <year> (1989). </year> <title> Adaptive network for optimal linear feature extraction. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, </address> <publisher> I, </publisher> <pages> 401-405. </pages>
Reference: <author> Hubbard, R.S. & Marshall, J.A. </author> <year> (1994). </year> <title> Self-organizing neural network model of the visual inertia phenomenon in motion perception. </title> <type> Technical Report 94-001, </type> <institution> Department of Computer Science, University of North Carolina at Chapel Hill, </institution> <note> 26 pp. </note>
Reference: <author> Jutten, C. & Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24, </volume> <pages> 1-10. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> 59-69. </pages>
Reference-contexts: The ART-1 network (Carpenter & Grossberg, 1987) and the Kohonen network <ref> (Kohonen, 1982) </ref> are examples of essentially WTA neural networks. When an input pattern first arrives, it is assigned to the one category whose prototype pattern best matches it. The activation of neurons encoding other categories is suppressed (e.g., through strong inhibition).
Reference: <author> Marr, D. </author> <year> (1982). </year> <title> Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. </title> <address> San Fransisco: </address> <publisher> W.H. Freeman and Company. </publisher>
Reference: <author> Marr, D. & Poggio, T. </author> <year> (1976). </year> <title> Cooperative computation of stereo disparity. </title> <journal> Science, </journal> <volume> 194, </volume> <pages> 238-287. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1990a). </year> <title> Self-organizing neural networks for perception of visual motion. </title> <booktitle> Neural Networks 3 </booktitle> <pages> 45-74. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1990b). </year> <title> A self-organizing scale-sensitive neural network. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> San Diego, CA, </address> <booktitle> III., </booktitle> <pages> 649-654. </pages>
Reference-contexts: That this is correct can be verified by comparing the sum of the input signals, 1 + 1 + 1 = 3, with the sum of the "size-normalized" output signals. Each output neuron encodes a pattern of a certain preferred "size" (or "scale") <ref> (Marshall, 1990b, 1995) </ref>, which in Figure 1 is the sum of the weights of the neuron's input connections. The sum of the input weights to neuron ab is 1+1+0 = 2, and the sum of the input weights to neuron bc is 0 + 1 + 1 = 2. <p> However, a WTA network cannot activate multiple categories simultaneously. Hence, the network cannot parse the input in a way that satisfies the second exclusive allocation condition, so generalization performance suffers. 6 3.1.2 EXIN networks In the EXIN (EXcitatory + INhibitory learning) neural network model <ref> (Marshall, 1990b, 1995) </ref>, this problem is overcome by using an anti-Hebbian inhibitory learning rule in addition to a Hebbian excitatory learning rule.
Reference: <author> Marshall, J.A. </author> <year> (1990c). </year> <title> Adaptive neural methods for multiplexing oriented edges. Intelligent Robots and Computer Vision IX: Neural, Biological, and 3-D Methods, </title> <editor> Casasent DP, Ed. </editor> <booktitle> Proceedings of the SPIE 1382, </booktitle> <address> Boston, MA, 282-291. 23 Marshall, J.A. </address> <year> (1992). </year> <title> Development of perceptual context-sensitivity in unsupervised neural networks: Parsing, grouping, and segmentation. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, Baltimore, MD, III, </booktitle> <pages> 315-320. </pages>
Reference: <author> Marshall, J.A. </author> <year> (1995). </year> <title> Adaptive perceptual pattern recognition by self-organizing neural networks: Context, uncertainty, multiplicity, and scale. </title> <booktitle> Neural Networks, </booktitle> <volume> 8, </volume> <pages> 335-362. </pages>
Reference-contexts: This paper proposes such a general-purpose definition, based on unsupervised learning. The definition measures how well a system's internal representations correspond to the underlying structure of its input environment, under manipulations of context, uncertainty, multiplicity, and scale <ref> (Marshall, 1995) </ref>. For this definition, a good internal representation is the goal, rather than good performance on a particular task. In unsupervised learning, input patterns are assigned to output categories based on some internal standard, such as similarity to other classified patterns. <p> Consider, for instance, the network shown in Figure 1, which has been trained to recognize patterns ab and bc <ref> (Marshall, 1995) </ref>. Each output neuron is given a "label" (ab, bc) that reflects the familiar patterns to which the neuron responds. The parsings that the network generates are evaluated in terms of those labels. <p> This results in category scission between independent category groupings and allows the EXIN network to generate near-optimal parsings of multiple superimposed patterns, in terms of multiple simultaneous activations <ref> (Marshall, 1995) </ref>. 3.1.3 Linear decorrelator networks Linear decorrelator networks (Oja, 1982; Foldiak, 1989) also use an anti-Hebbian inhibitory learning rule that can cause the lateral inhibitory connections to vanish during learning. This allows simultaneous neural activations. <p> This allows simultaneous neural activations. However, the linear decorrelator network responds essentially to differences, or distinctive features (Anderson, Silverstein, Ritz, & Jones, 1977; Sattath & Tversky, 1987) among the patterns, rather than to the patterns themselves <ref> (Marshall, 1995) </ref>. 3.1.4 Example competitive learning networks, linear decorrelator networks, and EXIN networks and illustrates the intuitions on which the rest of this paper is based. The initial connectivity pattern in the three networks is identical (Figure 2A). <p> Thus, all input features are fully represented in the output, and the exclusive allocation conditions are met for input pattern abcd , in the linear decorrelator and EXIN networks. These two networks exhibit a global context-sensitive constraint satisfaction property <ref> (Marshall, 1995) </ref> in their parsing of abcd : the contextual presence or absence of small distinguishing features, or nuances, (like d ) dramatically alters the parsing. <p> The EXIN network was chosen as an example because it yields good but not perfect generalization; thus, it shows effectively how the criteria operate. <ref> (Marshall, 1995) </ref>. Figure 7 (bottom) shows the multiplexed, context-sensitive response of the network to a variety of familiar and unfamiliar input combinations. All 64 possible binary input patterns were tested, and reasonable results were produced in each case (Marshall, 1995); Figure 7 (bottom) shows 16 of the 64 tested parsings. <p> but not perfect generalization; thus, it shows effectively how the criteria operate. <ref> (Marshall, 1995) </ref>. Figure 7 (bottom) shows the multiplexed, context-sensitive response of the network to a variety of familiar and unfamiliar input combinations. All 64 possible binary input patterns were tested, and reasonable results were produced in each case (Marshall, 1995); Figure 7 (bottom) shows 16 of the 64 tested parsings. Given the four generalization conditions described in the preceding sections, the performance of this EXIN network (Marshall, 1995) will be summarized below. <p> All 64 possible binary input patterns were tested, and reasonable results were produced in each case <ref> (Marshall, 1995) </ref>; Figure 7 (bottom) shows 16 of the 64 tested parsings. Given the four generalization conditions described in the preceding sections, the performance of this EXIN network (Marshall, 1995) will be summarized below. A sample of the most illustrative parsings, and the degree to which they satisfy the conditions, will be discussed.
Reference: <author> Marshall, J.A., Kalarickal, G.J., Graves, E.B. </author> <year> (1996). </year> <title> Neural model of visual stereomatching: Slant, transparency, and clouds. Network: </title> <booktitle> Computation in Neural Systems 7 </booktitle> <pages> 635-670. </pages>
Reference-contexts: In stereo transparency (Prazdny, 1985), individual visual features should be assigned to the representation of only one of multiple superimposed surfaces <ref> (Marshall, Kalarickal, & Graves, 1996) </ref>. 2.1 Neural network classifiers A neural network categorizes an input pattern by activating some classifier output neurons. These activations constitute a representation of the input pattern, and the input features of that pattern are said to be assigned to that output representation.
Reference: <author> Marshall, J.A., Kalarickal, G.J., & Ross, W.D. </author> <year> (1997). </year> <title> Transparent surface segmentation and filling-in using local cortical interactions. </title> <institution> Investigative Ophthalmology & Visual Science 38(4):641. </institution>
Reference: <author> Marshall, J.A., Schmitt, C.P., Kalarickal, G.J., & Alley, R.K. </author> <year> (1997). </year> <title> Neural model of transfer-of-binding in visual relative motion perception. </title> <note> To appear in Computational Neuroscience: Trends in Research, 1998 , JM Bower (ed.), 5 pp. </note>
Reference: <author> Morse, B. </author> <year> (1994). </year> <title> Computation of Object Cores from Grey-Level Images. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of North Carolina at Chapel Hill. </institution>
Reference-contexts: This analysis is concerned nonconstructively with the existence of parsing coefficients that satisfy or minimize the equations. In practice, the minimization can be performed in a number of ways, e.g., using an iterative procedure <ref> (Morse, 1994) </ref> to find the coefficients. 13 4.4 Condition 2 is necessary but not sufficient The E 1 scores produced by Equation 5 can be used as a criterion to grade a network's generalization behavior on particular input pattern parsings.
Reference: <author> Nigrin, A. </author> <year> (1993). </year> <title> Neural Networks for Pattern Recognition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: of specifying how a network should respond to unfamiliar patterns and is therefore a valuable criterion for generalization. 8 3.2 Equality of total input and total output Condition 2 will be used to compare a linear decorrelator network, an EXIN network, and a SONNET-2 (Self-Organizing Neural NETwork - 2) network <ref> (Nigrin, 1993) </ref>. (Since a WTA network does not allow simultaneous activation of multiple category winners, it is not considered in this example.) 3.2.1 SONNET-2 SONNET-2 is a fairly complex network, involving the use of inhibition between connections (Desimone, 1992; Reggia, D'Autrechy, Sutton, & Weinrich, 1992; Yuille & Grzywacz, 1989), rather than <p> excitatory learning rule involves prototype modification of output layer competition winners, and the inhibitory learning rule is based on coactivation of the competing neurons; hence SONNET-2 displays the global context-sensitive constraint satisfaction property (abc versus abcd ) and the sequence masking property (Cohen & Grossberg, 1986, 1987) (abc versus c) <ref> (Nigrin, 1993) </ref> displayed by EXIN networks. 3.2.2 Example patterns ab and bc, which are assumed to occur with equal training probability. The linear decorrelator network can end up in one of many possible final configurations, subject to the constraint that the output neurons are maximally decorrelated.
Reference: <author> Oja, E. </author> <year> (1982). </year> <title> A simplified neuron model as a principal component analyzer. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 15, </volume> <pages> 267-273. </pages>
Reference: <author> Reggia, J.A., D'Autrechy, C.L., Sutton, G.G., & Weinrich, M. </author> <year> (1992). </year> <title> A competitive redistribution theory of neocortical dynamics. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 287-317. </pages>
Reference: <author> Rumelhart, D.E. & McClelland, J.L. </author> <year> (1986). </year> <title> On the learning of past tenses of English verbs. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <booktitle> Volume 2: Psychological and Biological Models, </booktitle> <pages> 216-271. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sattath, S. & Tversky, A. </author> <year> (1987). </year> <title> On the relation between common and distinctive feature models. </title> <journal> Psychological Review , 94, </journal> <volume> 1, </volume> <pages> 16-22. </pages>
Reference: <author> Schmitt, C.P. & Marshall, J.A. </author> <year> (1998). </year> <title> Grouping and disambiguation in visual motion perception: A self-organizing neural circuit model. </title> <booktitle> In preparation, </booktitle> <pages> 30 pp. </pages>

References-found: 29

