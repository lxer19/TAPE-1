URL: ftp://ftp.cs.huji.ac.il/users/clag/ms/ms.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~clag/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Emergent Coordination through the Use of Cooperative State-Changing Rules  
Author: Claudia V. Goldman supervised by Dr. Jeffrey S. Rosenschein 
Degree: A thesis submitted in fulfillment of the requirements for the degree of Master of Science by  
Date: 24 August 1993  
Address: Jerusalem, Israel  
Affiliation: Computer Science Department The Hebrew University of Jerusalem  
Abstract-found: 0
Intro-found: 1
Reference: [Arg91] <author> Michael Argyle. </author> <title> Cooperation The Basis for Sociability. </title> <publisher> Routledge, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: These can give us some perspective on building artificial systems of agents. It has been argued <ref> [Arg91] </ref> that individuals may be cooperative in three main ways: 1. cooperation towards material rewards 2. communal relationships 3. coordination Experiments showed that subjects cooperate when they can trust the others to behave like them. Threats also were found to be a way to increase cooperation.
Reference: [Axe84] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, Inc., </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: In addition, people might look for a third party that could help them to reach agreement. It can be an arbitrator whose decision will be acceptable to the conflicting parties, or a negotiator who will help the parties to reach a settlement. After having analyzed the Prisoner's dilemma <ref> [Axe84] </ref> played between computer programs, Axelrod argued that to encourage cooperation between individuals, they have to meet again in the future, they have to be able to recognize each other, and they have to remember the behavior of the other. <p> The solution to these problems often relies on penalty mechanisms of one sort or another. For example, it may be possible for agents to act uncooperatively in encounters with known uncooperative agents, while acting cooperatively with cooperative agents (a strategy similar to TIT-FOR-TAT <ref> [Axe84] </ref>). Alternatively, it may be possible to alter the cooperation rule itself so that it becomes stable.
Reference: [Bar67] <author> S.A. Barnett. </author> <booktitle> Instinct and Intelligence The science of behavior in animals and man. Macgibbon and Kee, </booktitle> <year> 1967. </year>
Reference-contexts: Examples of animal behavior coordination can be found in <ref> [Bar67] </ref>. Ants and bees use sun navigation. The work in a bee hive is regulated by secretions that the queen produces and are passed 5 among the workers. Signals are used to guide ants and bee workers towards food. <p> Signals are used to guide ants and bee workers towards food. In human societies, agreements have to be adopted to limit or eliminate conflicts since man has no fixed signals as animals do <ref> [Bar67] </ref>. It has also been argued [F.V89] that norms are necessary to achieve cooperation. 1.2.2 DAI When do agents cooperate? Based on previous DAI work, cooperation seems to occur between artificial agents when they help each other in achieving their goals.
Reference: [BG88] <editor> Alan H. Bond and Les Gasser, editors. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Introduction 1.1 DAI Distributed Artificial Intelligence (DAI) is the subfield of Artificial Intelligence that investigates the behavior of societies of artificial agents (high-level, interacting computer programs). Work in DAI has been divided into three areas: <ref> [BG88] </ref> * Distributed Problem Solving Agents who were created by the same designer work together to solve shared goals. They might cooperate by decomposing the main goal into sub-tasks and allocating them to different agents in the group.
Reference: [BR91] <author> J.J. Bull and W.R. Rice. </author> <title> Distinguishing mechanisms for the evolution of cooperation. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 149 </volume> <pages> 63-74, </pages> <year> 1991. </year>
Reference-contexts: Axelrod maintains that "an excellent way to promote cooperation in a society is to teach people to care about the welfare of others." In biology <ref> [BR91] </ref>, the magnitude of cooperativeness of one individual of species A with another of species B is measured by its influence on the fitness of the latter. Examples of animal behavior coordination can be found in [Bar67]. Ants and bees use sun navigation.
Reference: [CMS88] <author> S. Cammarata, D. McArthur, and R. Steeb. </author> <title> Strategies of cooperation in distributed solving problems. </title> <editor> In Alan H. Bond and Les Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 102-105. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: The mechanism of sub-contracting is evaluated according to the simplicity, Pareto-optimality, and stability criteria. 2.1.2 Air Traffic Control Several strategies of cooperation have been taken into consideration for avoiding collisions in air traffic control. In <ref> [CMS88] </ref>, those strategies were divided into two classes: * organizational policies These policies transform the group of agents into a network, fixing the communication paths among them. The policies also decompose the tasks into sub-tasks so that the agents can work in an individual manner. <p> This policy did rather well with tasks of low complexity and with those of high difficulty, "that is tasks with primarily 2- and 3-body interactions but having few potential solutions" <ref> [CMS88] </ref>. * task shared Conflicts are resolved in two rounds of negotiation. One determines how to modify a plan and the second one determines who is going to carry it out. The performance of the systems that were designed according to this policy is higher than the centralized task systems.
Reference: [Dav81] <author> Randall Davis. </author> <title> A model for planning in a multi-agent environment: steps toward principles for teamwork. </title> <type> Working Paper 217, </type> <institution> Massachusetts Institute of Technology AI Laboratory, </institution> <month> June </month> <year> 1981. </year>
Reference-contexts: The second consists of approaches to achieve coordination implicitly where the agents refrain from conflict situations by behaving according to laws that were designed into them or where the agents react to the world. I also present a list of principles for teamwork <ref> [Dav81] </ref>. 2.1 Explicit Coordination 2.1.1 The Economic Approach The Contract Net The Contract Net is a high-level communication protocol for a Distributed Problem Solving system. Its aim is to facilitate the distribution of the tasks among the processors (nodes) that operate in the system. <p> All the group members are committed to guarantee G k 's success in performing fi i . A partial Shared Plan differs from the full one, allowing the agents to have partial recipes for actions. 2.2 Principles for Teamwork According to <ref> [Dav81] </ref>, plans in a multi-agent world have the following characteristics: * robust | since the agents operate under a certain degree of uncertainty, the plans should not be highly sensitive to the world state * cautious | agents should interfere with each other as little as possible (e.g., agents should use <p> Agents should spend more time inferring about the others instead of communicating with them From the above characteristics, Davis specifies a list of principles that will determine well coordinated behavior in a multi-agent world without the need to duplicate the work of any agent: <ref> [Dav81] </ref> 22 1. predict other agents' behavior | by using the following methods: * resource analysis | each agent could know about other agents' behavior by analyzing its requirements for resources. It can be done by checking the other agent's goals, thus avoiding the need for duplicate planning.
Reference: [DD91] <author> Alexis Drogoul and Christophe Dubreuil. </author> <title> Eco-problem solving model:results of the n-puzzle. </title> <booktitle> In Pre Proceedings of the Third European Workshop on Modeling Autonomous Agents and Multi-Agent Worlds, </booktitle> <address> Germany, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Conventions developed as formerly explained could be used in artificial social systems to design or predict the behavior of the participating agents. 26 2.4 Multi-Agent Reactive systems Distributed Problem Solving has also been approached by constructing a solution dynamically by reactive agents (eco-agents) in a multi- agent environment <ref> [FJ91, DD91, FD92] </ref>. The solution evolves from the agents' behaviors. The agents interact with the others without any global knowledge about the world, based only on their local information. The agent's knowledge includes its satisfaction state, its dependencies, and its jailers. <p> An example of a problem that was solved by the eco-problem solving model, as a result of the interactions among the agents, is the N-puzzle <ref> [DD91] </ref>. 27 Chapter 3 Implicit Coordination 3.1 Cooperative State-Changing Rules We address the problem of coordinating agents in a multi-agent environment by analyzing the interactions between the agents and the world and we don't regard the interactions among the agents.
Reference: [DL88] <author> Edmund H. Durfee and Victor R. Lesser. </author> <title> Using partial global plans to coordinate distributed problem solvers. </title> <editor> In Alan H. Bond and Les Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 285-293. </pages> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: In this way, more abstract data climbs up through the hierarchy. The highest level processor decides upon strategic plans and sends commands to the lower level agents. 2.1.3 Partial Global Planning In <ref> [DL88] </ref>, the use of partial global plans was proposed to determine effective coordinated actions for agents that are members of a problem solving group. The authors maintain that the agents should exchange information, negotiate about task assignments, and plan interactions to cooperate with each other to achieve better global performance. <p> The meta-level organization (i.e., the communication topology and the coordination responsibilities of the nodes in the network) determines the consistency of the information with which a node deals. Since the agents can work asynchronously, the partial global plans may be out of date. In <ref> [DL88] </ref>, it was assumed that the meta-level organization is defined when the network is created and it cannot be modified. The nodes will send their node-plans to pre-determined agents, specified by the organization. In this way, agents cooperate explicitly with other agents knowing about their goals and plans. <p> There are two kinds of messages that the agents communicate. The PGP messages describe the global work of a group of agents. The node-plan messages provide information to decide how agents might cooperate. The principal planning activities of a node's planner, as they appeared in <ref> [DL88] </ref>, are: 1. receive network information 2. find the next problem solving action using the network model (a) update local abstract view with new data (b) update network model, including PGPs, using changed local and received infor mation (c) map through the PGPs whose local plans are active, for each: i.
Reference: [DL92] <author> Keith S. Decker and Victor R. Lesser. </author> <title> Generalizing the partial global planning algorithm. </title> <journal> International Journal of Intelligent Cooperative Information Systems, </journal> <volume> 1(2) </volume> <pages> 319-346, </pages> <year> 1992. </year> <month> 56 </month>
Reference-contexts: Therefore, they might not achieve the optimal solutions since the world might change rapidly and they might have limited resources. Instead, the agents have to use all the information available to them to cooperate in achieving acceptable solutions. More recently <ref> [DL92] </ref>, the Partial Global Planning technique has been extended to let the agents transmit more abstract information among themselves, and detect coordination relationships that can be useful to improve the performance of the agents.
Reference: [ER91] <author> Eithan Ephrati and Jeffrey S. Rosenschein. </author> <title> The Clarke Tax as a consensus mechanism among automated agents. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 173-178, </pages> <address> Anaheim, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Agents decide upon what actions they will do, after having voted and thus coordinated their plans with other agents. The coordination is explicit in the sense that all the agents participate in the decision procedure. The Clarke Tax voting mechanism has been used in <ref> [ER91] </ref> as a method for reaching consensus. It requires that the agents reveal their preferences about the choices to be voted for (and in fact induces them to truthfully reveal their preferences), but it reduces the need for explicit negotiation. <p> The tax that is collected must be wasted, because otherwise an agent could improve its own utility by lying. For example, he could underestimate the worth of a given tax in a way that the tax will increase and he could gain more from it. The solution presented in <ref> [ER91] </ref> is to divide the tax among the agents that have not participated in the voting group. The disadvantages of dealing with the tax waste problem are the complexity of forming the voting groups and the need to maintain a bookkeeping mechanism to record the agents' debts.
Reference: [ER92] <author> Eithan Ephrati and Jeffrey S. Rosenschein. </author> <title> Reaching agreement through partial revelation of preferences. </title> <booktitle> In Proceedings of the Tenth European Conference on Artificial Intelligence, </booktitle> <pages> pages 229-233, </pages> <address> Vienna, Austria, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: One of the disadvantages of the former procedure is that every agent should reveal all its preferences although it is not always necessary nor desirable. A new method is proposed in <ref> [ER92] </ref>. The agents vote iteratively, keeping their information private as long as their true preferences don't change the group outcome. They also can change the group outcome by a fixed amount denoted by ffi or opt out of the process. <p> Let's denote this sum by S. Then the state r that got the maximum sum value when agent i's votes are taken into consideration is chosen. Let's denote this sum by R. Then the tax for agent i is S R. In <ref> [ER92] </ref>, they suggest to compute ffi in a 1 step, where all agents specify what is their preferred ffi and the smallest one is chosen.
Reference: [ER93] <author> Eithan Ephrati and Jeffrey S. Rosenschein. </author> <title> Multi-agent planning as a dynamic search for social consensus. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, page to appear, </booktitle> <year> 1993. </year>
Reference-contexts: The iterative procedure can be changed so that the agents vote sequentially. In this case, only one agent can make a change to the outcome, and the procedure becomes immune to coalitions. In a newer paper <ref> [ER93] </ref>, they introduce a new technique to incrementally generate the alternatives among which the agents vote. The method is based on constraints that the 20 agents aggregate at each step of the generation.
Reference: [FD92] <author> J. Ferber and A. Drogoul. </author> <title> Using reactive multi-agent systems in simulation and problem solving. </title> <editor> In Nicholas M. Avouris and Les Gasser, editors, </editor> <booktitle> Distributed Artificial Intelligence: Theory and Praxis, </booktitle> <pages> pages 53-80. </pages> <publisher> Kluwer Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Conventions developed as formerly explained could be used in artificial social systems to design or predict the behavior of the participating agents. 26 2.4 Multi-Agent Reactive systems Distributed Problem Solving has also been approached by constructing a solution dynamically by reactive agents (eco-agents) in a multi- agent environment <ref> [FJ91, DD91, FD92] </ref>. The solution evolves from the agents' behaviors. The agents interact with the others without any global knowledge about the world, based only on their local information. The agent's knowledge includes its satisfaction state, its dependencies, and its jailers. <p> The agent's knowledge includes its satisfaction state, its dependencies, and its jailers. The satisfaction state is true when the agent has achieved its goal. The goal is interpreted as another agent with which the first agent has to be related to <ref> [FD92] </ref>. The dependencies are agents. They define a master-slave relation among the agents. A dependency agent is the goal agent of an agent [FD92]. Finally, the jailers are agents that prevent other agents from acting. <p> The goal is interpreted as another agent with which the first agent has to be related to <ref> [FD92] </ref>. The dependencies are agents. They define a master-slave relation among the agents. A dependency agent is the goal agent of an agent [FD92]. Finally, the jailers are agents that prevent other agents from acting. In [FJ91], they also present the possible behaviors of an agent; the will to be satisfied, the will to be free, and the obligation to flee (an agent's answer to aggression). <p> An eco-agent can be represented as a finite state automaton with four states for each possible internal state of the agent: satisfied, looking for satisfaction, looking for flight and fleeing <ref> [FD92] </ref>. <p> Assuming that the rule designer gets the information about the domain, he could be able to build the appropriate automaton for it and thus induce the cooperative rules 46 for that domain. The information needed can be obtained from the model of the multi--agent problem domain. As specified in <ref> [FD92] </ref>, a model of a multi-agent simulation is a quadruple: &lt; agents; objects; environment; communications &gt;. Agents are the acting individuals. They are defined by their ability to communicate and their skills.
Reference: [Fel66] <author> William Feller. </author> <title> An Introduction to Probability Theory and Its Applications, volume 2. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1966. </year>
Reference-contexts: Given the distribution of the amount of work done by the agents without cooperating, we can investigate what functions could transform this distribution into another distribution. This computation can be done by convolution. Following the definition in <ref> [Fel66] </ref> Definition 4.1 The convolution of a function ' with a probability distribution F is the function defined by u (x) = R 1 1 '(x y)F fdyg. It will be denoted by u = F ? '.
Reference: [FJ91] <author> Jacques Ferber and Eric Jacopin. </author> <title> The framework of ECO-problem solving. </title> <editor> In Yves Demazeau and Jean-Pierre Muller, editors, </editor> <booktitle> Decentralized A.I. 2 Proceedings of the Second European Workshop on Modelling Autonomous Agents in a Multi-Agent World, </booktitle> <pages> pages 181-193. </pages> <publisher> Elsevier Science, </publisher> <year> 1991. </year>
Reference-contexts: They act according to the social laws in the same way that they act to achieve their goals. From a global perspective, agents' actions might not conflict, and therefore their actions have been (implicitly) coordinated. Multi-Agent reactive systems have lately been built <ref> [FJ91] </ref>. The solution to a problem evolves (or "emerges") from the agents' behaviors. Our approach is to induce the agents to cooperate implicitly through the use of state-changing rules. <p> Conventions developed as formerly explained could be used in artificial social systems to design or predict the behavior of the participating agents. 26 2.4 Multi-Agent Reactive systems Distributed Problem Solving has also been approached by constructing a solution dynamically by reactive agents (eco-agents) in a multi- agent environment <ref> [FJ91, DD91, FD92] </ref>. The solution evolves from the agents' behaviors. The agents interact with the others without any global knowledge about the world, based only on their local information. The agent's knowledge includes its satisfaction state, its dependencies, and its jailers. <p> The dependencies are agents. They define a master-slave relation among the agents. A dependency agent is the goal agent of an agent [FD92]. Finally, the jailers are agents that prevent other agents from acting. In <ref> [FJ91] </ref>, they also present the possible behaviors of an agent; the will to be satisfied, the will to be free, and the obligation to flee (an agent's answer to aggression). <p> A slave can be satisfied only after its master has been satisfied. * situated satisfactions | it means that agents' actions depend on the current situation of the environment. * fleeing rules | following <ref> [FJ91] </ref>, "an eco-agent must flee where the one that told him to flee does not want to go", and "if an eco-agent seeking for its satisfaction is able to reach its satisfaction state at the time of its flee, then the eco-agent will reach its satisfaction state".
Reference: [F.V89] <author> F.V.Kratochwil. </author> <title> Rules, Norms and Decisions. </title> <booktitle> Cambridge studies in international relations:2 1989, </booktitle> <year> 1989. </year>
Reference-contexts: Signals are used to guide ants and bee workers towards food. In human societies, agreements have to be adopted to limit or eliminate conflicts since man has no fixed signals as animals do [Bar67]. It has also been argued <ref> [F.V89] </ref> that norms are necessary to achieve cooperation. 1.2.2 DAI When do agents cooperate? Based on previous DAI work, cooperation seems to occur between artificial agents when they help each other in achieving their goals. They were able to cooperate because they knew about others' goals, plans or intentions.
Reference: [GDW91] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> A decision theoretic approach to coordinaitng multiagent interaction. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The agents will be committed to vote honestly if their welfare function is computed as the weighted sum or average of the individual utilities. 2.1.6 Modeling Other agents beliefs and knowledge Another method to coordinate the activities of autonomous agents is the Recursive Modeling Method presented in <ref> [GDW91] </ref>. Each agent models the other agents in a recursive manner and thus acquires probabilistic knowledge about the expected utility values that the other agents have about their preferences, abilities and the world.
Reference: [GK93] <author> Barbara Grosz and Sarit Kraus. </author> <title> Collaborative plans for group activities. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, page to appear, </booktitle> <year> 1993. </year>
Reference-contexts: Finally, it will choose the action that maximizes its utility based on a probabilistic combination of the information encompassed in these models. Agents have to take their beliefs and intentions into consideration to plan collabora-tively <ref> [GK93] </ref>. The collaborative agents build full plans from partial plans that they can change dynamically over time. Grosz and Kraus introduce four different intention operators. Int.To and Int.Th. represent the intentions that an agent has adopted. <p> This operator is defined recursively on full and partial Shared Plans. Those, are based on full individual plans and partial individual plans that each agent might have to achieve some action. 21 Following <ref> [GK93] </ref>, an individual full plan specifies a complete plan P that agent G k has at time T P to do action ff at time T ff using recipe R ff in the context C ff .
Reference: [GR93] <author> Claudia V. Goldman and Jeffrey S. Rosenschein. </author> <title> Emergent coordination through the use of cooperative state-changing rules. </title> <booktitle> In Proceedings of the Twelth International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 171-185, </pages> <address> Hidden Valley, Pennsylvania, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Two heuristics were tested. One induces the agents to push their closest tiles first. The second causes the agents to push tiles that were pre-assigned by their designers. The agents' behavior has been tested each time in two cases when either the meta-rule was applied or not <ref> [GR93] </ref>. 4.2.3 Scenario1 The first scenario is described in Figure 4.6. h 2 h 3 0 4 9 37 Push Closest Tile First The aim of each agent in this example is to fill as many holes as it can.
Reference: [Kra93] <author> Sarit Kraus. </author> <title> Agents contracting tasks in non-collaborative environments. </title> <booktitle> In Proceedings of the Elevnth National Conference on Artificial Intelligence, page to appear, </booktitle> <year> 1993. </year> <month> 57 </month>
Reference-contexts: The producers are those that have the ability to carry the cargo units among the links. The producers will serve the consumers when the latter supply the necessary resources. Other Economic Models The concept of contracts was also used in <ref> [Kra93] </ref> to coordinate activity among agents. An agent A will have a contract with agent B when A wants some of its tasks to be done by B. It might be the case that A cannot perform the sub-task or that B can do it better. <p> It might be the case that A cannot perform the sub-task or that B can do it better. Kraus uses a reward method based on a monetary system to convince agents to accept the contracts. 10 Models of economics and game theory were also adapted to DAI situations in <ref> [Kra93] </ref>. The mechanism of sub-contracting is evaluated according to the simplicity, Pareto-optimality, and stability criteria. 2.1.2 Air Traffic Control Several strategies of cooperation have been taken into consideration for avoiding collisions in air traffic control.
Reference: [KW91] <author> Sarit Kraus and Jonathan Wilkenfeld. </author> <title> Negotiations over time in a multi agent environment:preliminary report. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 56-61, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Formally, W orth i (f ) states how much of agent's i's goal is achieved in state f [ZR93a]. 17 Kraus' Research Negotiation as a means of achieving activity coordination, has also been considered in <ref> [KW91] </ref>. This work analyzes the negotiation process using game theory too, but their model was built for an N -agent environment N 3 in which time is taken into consideration. Agents might change their preferences over time. <p> The agents have a common goal to achieve. Each of them wants to do as little as possible to help carry out their task. The agents are assumed to have full information, to be rational, and to commit themselves to the agreements they have reached. In <ref> [KW91] </ref>, they analyze an environment with three agents where each one of them has a set of capabilities and tools, and they want to achieve their goal as fast as possible.
Reference: [LP81] <author> Harry R. Lewis and Christos H. Papadimitriou. </author> <title> Elements of the theory of computation. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1981. </year>
Reference-contexts: For that purpose a taxonomy of cooperative state-changing rules is needed. 5.1 A Multi-Agent Deterministic Finite Automaton Definition 5.1 A deterministic finite automaton (DFA) is a quintuple <ref> [LP81] </ref> M = (K; ; ffi; s; F ) where * K is a finite set of states, * is an alphabet, * ffi : K fi ! K is the transition function, * s is the initial state, * F K is the set of final states For the purpose <p> A final state is defined as a state where all the agents have achieved their goals. If the assignments arrive dynamically, there might be an arrow from any final state to the initial state. According to <ref> [LP81] </ref>, a configuration of a DFA is any element K fi fl .
Reference: [LR57] <author> R.Duncan Luce and Howard Raiffa. </author> <title> Games and Decisions. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1957. </year>
Reference-contexts: The definitions needed for this protocol (standard from game theory) are as follows: * A deal ffi is called individual rational if for every agent i 2 fA; Bg; U tility i (ffi) 0 * A deal ffi is named Pareto optimal if :9ffi 0 s:t: ffi 0 ffi <ref> [LR57] </ref> * The negotiation set is defined as the set of all deals that are individual rational and Pareto optimal.
Reference: [MFH88] <author> T. Malone, R. Fikes, and M. Howard. </author> <title> Enterprise: A market-like task sched-uler for distributed computing environments. </title> <editor> In B. A. Huberman, editor, </editor> <booktitle> The Ecology of Computation, </booktitle> <pages> pages 177-205. </pages> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: Enterprise A system that was built using a variation of this protocol is Enterprise <ref> [MFH88] </ref>. The protocol is used to schedule tasks among different processors connected by a local area network. The personal workstations are dedicated to their owners but when they are idle, they serve as general purpose machines. The processors communicate through the Inter-Process Communication (IPC) facility.
Reference: [MLM + 92] <author> T. A. Montgomery, J. Lee, D. J. Musliner, E. H. Durfee, D. Damouth, and Y. </author> <title> So. MICE Users Guide. </title> <institution> Artificial Intelligence Laboratory, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, Michigan, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: They have full information about external aspects of the domain, seeing where there are holes, tiles, barriers, and agents, but not necessarily any information about other agents' internal goals. 4.2 Simulations Experiments were carried out using the MICE distributed agent testbed <ref> [MLM + 92] </ref>. In the following sections, we showed that it might be worth to let the agents act according to a meta-rule, i.e., a cooperative state-changing rule in order to improve the performance of the system.
Reference: [MT90] <author> Yoram Moses and Moshe Tennenholtz. </author> <title> Artificial social systems part 1:basic principles. </title> <type> Technical Report CS90-12, </type> <institution> Weizmann Institute, </institution> <year> 1990. </year>
Reference-contexts: In this case the request for help is not specified in the same way it would be if a communication protocol were used, but the agent that helps acts according to its beliefs about a specific agent and action. More recent work <ref> [MT90, ST92b, ST92a, MT92, ST93] </ref> deals with social laws that are imposed on the agents' behavior. These laws might prevent agents from getting into conflict situations. Avoiding conflicts is a way of achieving coordination, but it is a different case from the explicit one. <p> The information gathered by the agent will be used to constrain its plan or to reduce ambiguity and uncertainty in its model. 23 2.3 Artificial Social Systems Moses and Tennenholtz have suggested applying the society metaphor to artificial systems to improve the performance of the agents operating in this society <ref> [MT90] </ref>. The issues that are to be dealt with when analyzing a multi-agent environment concern synchronization, coordination of the agents' activities, cooperative ways to achieve tasks, and how safety and fairness constraints on the system can be guaranteed. <p> This organization will ensure that the agents could carry out plans to achieve their goals in an efficient manner. The goals of the social system as presented in <ref> [MT90] </ref> are: * Create a "global social order" on the world that will determine the time, place, means and style of the activities to be performed by the agents. * Reduce the complexity of agents' local reasoning. <p> In our model, agents might work less when they behave according to the rule than when acting alone in the world. As pointed out in <ref> [MT90, pg.6] </ref>, "The amount of necessary interaction between the agents for the purpose of traveling from one point to another then becomes considerably smaller, and the traffic flow may still improve considerably".
Reference: [MT92] <author> Yoram Moses and Moshe Tennenholtz. </author> <title> On computational aspects of artificial social systems. </title> <booktitle> In Proceedings of the Eleventh International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 267-283, </pages> <address> The Homestead, Glen Arbor, MI, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: In this case the request for help is not specified in the same way it would be if a communication protocol were used, but the agent that helps acts according to its beliefs about a specific agent and action. More recent work <ref> [MT90, ST92b, ST92a, MT92, ST93] </ref> deals with social laws that are imposed on the agents' behavior. These laws might prevent agents from getting into conflict situations. Avoiding conflicts is a way of achieving coordination, but it is a different case from the explicit one. <p> and s j= ' holds then DO (s) 6= a * An execution of a plan from state s 0 is a sequence of states s 0 ; s 1 ; s 2 ; ::: such that s i+1 2 T (s i ; DO (s i ); sl) In <ref> [MT92] </ref>, the social law has been presented as a restriction on the actions that an agent could perform. In general, they expect to have laws expressed in a higher-level language.
Reference: [PR90] <author> Martha E. Pollack and Marc Ringuette. </author> <title> Introducing the tileworld: Experimentally evaluating agent architectures. </title> <booktitle> In Proceedings of The National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-189, </pages> <address> Boston, Massachusetts, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: both kinds of rules can be merged so as to avoid conflicts and at the same time improve the global performance of the system. 30 Chapter 4 The TileWorld Domain, Simulations and Analysis 4.1 The domain The TileWorld domain that is considered here is a multi-agent version of the TileWorld <ref> [PR90] </ref> (see Figure 4.1). A 1 h 2 h 4 We use a simple variation of the multi-agent TileWorld introduced in [ZR93a]. A hole in the grid is represented by a dashed line surrounding the letter h i .
Reference: [SCHRW80] <author> R. Steeb, S. Cammarata, F. Hayes-Roth, and R. Wesson. </author> <title> Distributed intelligence for air fleet control. </title> <type> Technical Report WD-839-ARPA, </type> <institution> The Rand Corporation, </institution> <month> December </month> <year> 1980. </year>
Reference-contexts: The performance of the systems that were designed according to this policy is higher than the centralized task systems. The main drawback is due to the high need for message transferring. In a former paper <ref> [SCHRW80] </ref>, they presented six distinct architectures for distributed planning in the air traffic control domain. Whenever cooperation could take place, it would be achieved through explicit communication among the processors in the environment.
Reference: [Smi88] <author> Reid G. Smith. </author> <title> The contract net protocol: High level communication and control in a distributed problem solver. </title> <editor> In Alan H. Bond and Les Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 357-366. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: They might also wait for better bids until the expiration time is reached. Whenever a manager assigns a task to a contractor, it sends him an award message that consists of specifications about the task to be executed and additional information required by the contractor. In <ref> [Smi88] </ref> are presented several ways to improve the efficiency of the protocol by not overloading the communication channels.
Reference: [ST92a] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: initial experimental results and observations (preliminary report). </title> <booktitle> In Principles of knowledge representation and reasoning: Proceedings of the Third International Conference(KR92), </booktitle> <address> Cambridge, Massachusetts, </address> <month> Oct </month> <year> 1992. </year> <month> 58 </month>
Reference-contexts: In this case the request for help is not specified in the same way it would be if a communication protocol were used, but the agent that helps acts according to its beliefs about a specific agent and action. More recent work <ref> [MT90, ST92b, ST92a, MT92, ST93] </ref> deals with social laws that are imposed on the agents' behavior. These laws might prevent agents from getting into conflict situations. Avoiding conflicts is a way of achieving coordination, but it is a different case from the explicit one. <p> According to those laws, an agent that has no goal has to move constantly. The authors suggest that those agents should stay in predefined rest areas. Whenever the characteristics of the society are unknown, not all the rules could be designed a priori. In <ref> [ST92a] </ref>, they investigate how global conventions could be developed in such environments. The agents can occasionally meet with others and by observing the others (and based on its local information) will update their strategies. The objective of [ST92a] is to analyze the convergence of the agents' update functions. <p> In <ref> [ST92a] </ref>, they investigate how global conventions could be developed in such environments. The agents can occasionally meet with others and by observing the others (and based on its local information) will update their strategies. The objective of [ST92a] is to analyze the convergence of the agents' update functions. In their experiments agents start with a randomly generated single bit. The agents will observe other agents and might also exchange more information, so that they will converge on a single bit. <p> Several update functions were tested <ref> [ST92a] </ref>. They appear in increasing order of efficiency of the convention evolution process.
Reference: [ST92b] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies (preliminary report). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In this case the request for help is not specified in the same way it would be if a communication protocol were used, but the agent that helps acts according to its beliefs about a specific agent and action. More recent work <ref> [MT90, ST92b, ST92a, MT92, ST93] </ref> deals with social laws that are imposed on the agents' behavior. These laws might prevent agents from getting into conflict situations. Avoiding conflicts is a way of achieving coordination, but it is a different case from the explicit one. <p> Formal definitions were given in <ref> [ST92b] </ref> * A social law is a set of constraints (a i ; ' i ) where a 2 A (a set of actions) and ' 2 L (a first order language with an entailement relation j=). ' i is the most general statement about the states in which the agent
Reference: [ST93] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On social laws for artificial agent societies: Off-line design. </title> <journal> Artificial Intelligence, </journal> <note> page to appear, </note> <year> 1993. </year>
Reference-contexts: In this case the request for help is not specified in the same way it would be if a communication protocol were used, but the agent that helps acts according to its beliefs about a specific agent and action. More recent work <ref> [MT90, ST92b, ST92a, MT92, ST93] </ref> deals with social laws that are imposed on the agents' behavior. These laws might prevent agents from getting into conflict situations. Avoiding conflicts is a way of achieving coordination, but it is a different case from the explicit one. <p> Those presented before regard the agents as a whole. All the agents must behave according to them. Basically, the agents operating in an artificial social system will coordinate by restricting their activities. The multiple robots navigation problem has been investigated in the mobile robots domain in <ref> [ST93] </ref>. m robots are spread out in an n fi n grid. Several robots might collide when they occupy the same grid location. At each time unit a robot can move to a neighbor location or stay in its original place. <p> The social system designers have to levy traffic laws which will guarantee that no collisions will occur while the agents are carrying out their goals. Distinct traffic laws were suggested <ref> [ST93] </ref> based on the vision region of the agents. 25 * All the agents move constantly in a zig-zag pattern * The grid is divided into coarser regions with specific laws to be abided by agents inside them. <p> A Comparison between Social Laws and Cooperative State-Changing Rules 29 * Constraining | The social laws restrict the agents' plans. Our extra work actually add actions. The agents perform extra actions while pursuing their own goal. * Efficiency | Traffic laws <ref> [ST93] </ref> improve efficiency, but never to the level of standalone agents. In our model, agents might work less when they behave according to the rule than when acting alone in the world. <p> In our model, the agents have degrees of cooperation, they can be more or less cooperative. * Bound on extra work | In the traffic world described in <ref> [ST93] </ref>, the amount of extra work is bounded by the scenario, and may be arbitrarily large. The cooperative state-changing rule, in contrast, induces extra work that is bounded by an agent-specific variable, and can be controlled (for each agent). * Fragility | In the traffic world [ST93], the rule might break <p> traffic world described in <ref> [ST93] </ref>, the amount of extra work is bounded by the scenario, and may be arbitrarily large. The cooperative state-changing rule, in contrast, induces extra work that is bounded by an agent-specific variable, and can be controlled (for each agent). * Fragility | In the traffic world [ST93], the rule might break down globally if single agents don't participate. That is not the case in our model.
Reference: [Syc88] <author> K. Sycara. </author> <title> Resolving goal conflicts via negotiation. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 245-250, </pages> <address> St. Paul, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The coordination relationships that the agents could be involved in, in the generalized PGP algorithm, are subtask, facilitates, overlaps, and temporal. 2.1.4 Negotiation Sycara's Research Agents might operate in non-cooperative environments where goals conflicts could be resolved by a negotiation mechanism. A model for such resolution was provided in <ref> [Syc88] </ref>. Negotiation is used to find a compromise that is acceptable to the conflicting agents. Their goals might not be totally satisfied. The negotiation process can be seen as a search in a dynamic space consisting of the agents' beliefs about other agents' beliefs. <p> The negotiation process can be seen as a search in a dynamic space consisting of the agents' beliefs about other agents' beliefs. This space changes dynamically as the agents' proposals are revealed. An implementation of such a model to solve labor disputes is the PERSUADER <ref> [Syc88] </ref> system. Its input consists of a set of conflicting goals and the dispute context. Its output is a plan that can be achieved by the conflicting parties in a form of a compromise, or it is a failure message if the parties could not agree upon any compromise.
Reference: [TM89] <author> Moshe Tennenholtz and Yoram Moses. </author> <title> On cooperation in a multi-entity model. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 918-923, </pages> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The objects are the passive entities in the domain. This model is general. For the construction of a multi-agent automaton, it might be enough to use less knowledge than all that is encompassed in the aforementioned model. Agents have been represented as sets of nondeterministic finite state machines in <ref> [TM89] </ref>. Active agents could perform actions out of a finite set of actions in each physical state of the machines.
Reference: [Wel92] <author> Michael P. Wellman. </author> <title> A general-equilibrium approach to distributed transportation planning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The agents will coordinate their actions by transferring information among themselves, evaluating the tasks' specifications, and by mutually selecting the partners for the assignment. WALRAS Another system that takes an economic approach to solve a problem distributed among several agents, based on a price mechanism, has been proposed by <ref> [Wel92] </ref>. Agents can act as consumers or as producers. They can sell, buy and consume goods as consumers. Their utility function determines their preferences over the goods. The producers are able to transform some kind of goods into others according to their production function. <p> A price is clearing when the quantity of the good that has been demanded is balanced with the quantity that has been supplied. The system presented in <ref> [Wel92] </ref>, WALRAS, computes for each market the equilibrium price. It updates the prices according to the agents' bids. The agents submit supply and demand curves of goods and prices and not just single quantities for a particular price. The auction adjusts the agents' prices to clear. <p> The auction adjusts the agents' prices to clear. Agents do not negotiate directly with one another, but they coordinate their activities according to the current price that is updated by the auction. The example shown in <ref> [Wel92] </ref> deals with the transportation problem. There is a network of locations connected by directed links with costs associated with them. The task is to allocate a given set of cargo movements over the network at the minimum cost.
Reference: [ZR89a] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Negotiation and task sharing among autonomous agents in cooperative domains. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 912-917, </pages> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means.
Reference: [ZR89b] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Negotiation and task sharing in a non-cooperative domain. </title> <booktitle> In Proceedings of the Ninth Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 307-327, </pages> <address> Rosario, Washington, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means.
Reference: [ZR90] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Negotiation and conflict resolution in non-cooperative domains. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 100-105, </pages> <address> Boston, Massachusetts, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means. <p> Domains in which the negotiation set might be empty are called noncooperative domains and domains in which the negotiation set is never empty are called cooperative domains <ref> [ZR90] </ref> * A deal is a conflict deal if the agents agree only on their original tasks sharing The agents acting according to the negotiation protocol behave in the following way: * In each step t 0, both agents simultaneously offer the deals ffi (A; t) and ffi (B; t) such <p> on the conflict deal 8i 2 fA; Bg; U tility i (ffi (i; t)) = U tility i (ffi (i; t 1)) a compromise when at step t, if 9j 6= i 2 fA; Bg s:t: U tility j (ffi (i; t)) U tility j (ffi (j; t)) 16 In <ref> [ZR90] </ref>, the utility function is defined for an agent i agreeing on a deal ffi as the difference between the worth i attaches to its goal and the cost of its part in the deal.
Reference: [ZR91] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Incomplete information and deception in multi-agentnegotiation. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 225-231, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means.
Reference: [ZR93a] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Compromise in negotiation: Exploiting worth functions over states. </title> <type> Technical Report 93-3, </type> <institution> Leibniz Center for Computer Science, Hebrew University, </institution> <year> 1993. </year>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means. <p> Some states are better than others. The agents' goals are not fixed but they are described by a worth function over the final states. Formally, W orth i (f ) states how much of agent's i's goal is achieved in state f <ref> [ZR93a] </ref>. 17 Kraus' Research Negotiation as a means of achieving activity coordination, has also been considered in [KW91]. This work analyzes the negotiation process using game theory too, but their model was built for an N -agent environment N 3 in which time is taken into consideration. <p> A 1 h 2 h 4 We use a simple variation of the multi-agent TileWorld introduced in <ref> [ZR93a] </ref>. A hole in the grid is represented by a dashed line surrounding the letter h i . Tiles are represented by black squares ( ) inside the grid squares. Obstacles are represented by thick black lines ( ). <p> Consider, for example, the simple interaction shown in Figure 4.2 (a variation on an example from <ref> [ZR93a] </ref>). h 2 h 3 0 4 9 32 For the moment, let's assume that agent A 1 wants to fill holes 1 and 2, and that agent A 2 wants to fill holes 2 and 3 (this assumes that the agents were assigned these goals a priori ).
Reference: [ZR93b] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> A domain theory for task oriented negotiation. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, page to appear, </booktitle> <year> 1993. </year> <month> 59 </month>
Reference-contexts: The payoff can be updated by changing the importance that the agent attaches to the issue or by changing the utility value of it. 15 Zlotkin and Rosenschein's Research A negotiation mechanism was also used by Zlotkin and Rosenschein <ref> [ZR89a, ZR91, ZR90, ZR89b, ZR93b, ZR93a] </ref> to make agent interactions more efficient. Agents will communicate explicitly their desires, and by negotiating they can sometimes compromise to get mutually acceptable agreements. Their analysis is done by game theoretic means. <p> Two new types of deals are defined. One is the semi-cooperative deal type that consists of the agents bringing the world to some state in which they will flip a coin and in this way the agents decide whose goals will be achieved <ref> [ZR93b] </ref>. The multi-plan deal type are those that have the agents cooperating even after the coin toss; i.e., whenever the agents flip a coin, both of them will go on executing actions to achieve the coin-toss winner's goal. The development of a domain theory for negotiation has also been started [ZR93b]. <p> <ref> [ZR93b] </ref>. The multi-plan deal type are those that have the agents cooperating even after the coin toss; i.e., whenever the agents flip a coin, both of them will go on executing actions to achieve the coin-toss winner's goal. The development of a domain theory for negotiation has also been started [ZR93b]. Its aim is to classify domains where agents can operate in a way that an appropriate negotiation mechanism could be chosen as an interaction method for the agents. The domains can be classified as follows [ZR93b]: * Task Oriented Domains | these are cooperative domains, in which the agents might <p> The development of a domain theory for negotiation has also been started <ref> [ZR93b] </ref>. Its aim is to classify domains where agents can operate in a way that an appropriate negotiation mechanism could be chosen as an interaction method for the agents. The domains can be classified as follows [ZR93b]: * Task Oriented Domains | these are cooperative domains, in which the agents might benefit from the existence of other agents in the same environment.
References-found: 43

