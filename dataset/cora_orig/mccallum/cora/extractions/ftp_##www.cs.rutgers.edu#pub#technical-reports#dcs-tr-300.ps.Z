URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-300.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: PARALLELIZING UNSTRUCTURED SPARSE MATRIX COMPUTATIONS ON LARGE-SCALE MULTIPROCESSORS  Written under the direction of  
Author: BY SESHADRI VENUGOPAL 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Professor Gerard Richter and approved by  
Date: October, 1993  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair. </author> <title> A high performance algorithm using pre-processing for the sparse matrix-vector multiplication. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 32-41, </pages> <year> 1992. </year>
Reference-contexts: The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; <ref> [1; 3] </ref>; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; <ref> [1; 2] </ref>g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. In that Figure, the root node's 1 2 3 4 5 6 7 6 81,2 3,3 2.5 6.5 1 8 1 2 3 7 1 3 4 5 6 8 split value is 4:5. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval. <p> For sparse matrix-vector multiplication, we sketch how the methodology can be ap plied to parallelize an algorithm recently proposed by Agarwal, Gustavson and Zubair <ref> [1] </ref>. 214 Their algorithm is based on preprocessing the sparse matrix to extract a block-based structure, and using the extracted structure to perform several iterations of the multiplication. 8.1 Block sparse triangular systems The partitioning of the matrix L implicitly defines a partitioning of the matrix L T , and the <p> One of the most recent algorithms for sparse matrix-vector multiplication is the one proposed by Agarwal, Gustavson and Zubair <ref> [1] </ref>. The main idea of their algorithm is to exploit any regular block structures or features in the sparse matrix A, which could lead to better exploitation of cache and vector processing or floating 225 point units, and therefore result in high performance. <p> Given the BCR, and a mapping of blocks to processors, we can apply the scheduling and communication optimization algorithms of Chapter 6 to complete the parallelization. In the following, we sketch the different types of partitions of the matrix that are constructed by FEBA as described in <ref> [1] </ref>, and relate them to the AVR for blocks. There are four types of blocks in the partitioning of matrix A. A generic block of each type is defined below, with a brief description of the data structures used. <p> The rows of the matrix are then partitioned in such a way that each partition consists of ml rows, where ml is a tunable parameter. Each such ml fi n submatrix is a block. The storage for the block is in the ITPACK format, which is summarized in <ref> [1] </ref>. Essentially, the format consists of two matrices: an ml fi n 0 matrix AC which holds the data values and a parallel ml fi n 0 matrix KA which holds column index numbers, where n 0 n.
Reference: [2] <author> F. L. Alvarado. </author> <title> The Sparse Matrix Manipulation System users manual. </title> <type> Technical report, </type> <institution> University of Wisconsin, Madison, Wisc., </institution> <year> 1990. </year>
Reference-contexts: The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; <ref> [2; 5] </ref>; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; <ref> [2; 3] </ref>; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; <ref> [1; 2] </ref>g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. In that Figure, the root node's 1 2 3 4 5 6 7 6 81,2 3,3 2.5 6.5 1 8 1 2 3 7 1 3 4 5 6 8 split value is 4:5. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; <p> These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. <p> In the following, unless otherwise specified, the B* matrices were reordered using MMD and the L* matrices were reordered using AND. The selection of the reordering schemes was arbitrary. We used SPARSKIT [61] and the Wisconsin Sparse Matrix Manipulation System <ref> [2] </ref> for generating and converting the test matrices into various formats, and for ordering and symbolically factoring most of 169 the test matrices.
Reference: [3] <author> C. Ashcraft. </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices. </title> <type> Technical Report ECA-TR-148, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, Washington, </address> <year> 1990. </year>
Reference-contexts: Lindex gives pointers to the beginning of the list of non-zero elements for each column. Thus, the nonzero elements in column j are stored in the Lval array from location Lindex [j] through Lindex [j + 1] 1. For instance, Lindex <ref> [3] </ref> = 6 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex [4] = 9 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. <p> For the grid problem, the communication volume is O (n 2 log n p P ) when parallelized on P processors. Their block-to-processor assignment scheme assumes that the the processors are laid out in the form of a p p grid. Using domains <ref> [3, 4] </ref> reduces the volume to O (n 2 log P p Our partitioning strategy uses the BLOCC scheme as the building block. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; <ref> [3; 7] </ref>; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; <ref> [1; 3] </ref>; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; <ref> [2; 3] </ref>; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; <p> These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [4] <author> C. Ashcraft, S. Eisenstat, J. W. H. Liu, and A. Sherman. </author> <title> A comparison of three column-based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year>
Reference-contexts: Thus, the nonzero elements in column j are stored in the Lval array from location Lindex [j] through Lindex [j + 1] 1. For instance, Lindex [3] = 6 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex <ref> [4] </ref> = 9 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. Thus, Lval [6] through Lval [8] store the non-zero values in column 3 and Lstruct [6] through Lstruct [8] hold the corresponding row numbers. <p> We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation. <p> For the grid problem, the communication volume is O (n 2 log n p P ) when parallelized on P processors. Their block-to-processor assignment scheme assumes that the the processors are laid out in the form of a p p grid. Using domains <ref> [3, 4] </ref> reduces the volume to O (n 2 log P p Our partitioning strategy uses the BLOCC scheme as the building block. <p> The inefficiencies arise in 68 the assembly and the merge steps. As a consequence, these methods yield the best computational efficiency when the frontal matrices are large. In the distributed implementations of multifrontal methods, columns are distributed among processors, as in the fan-out and fan-in methods <ref> [48, 4, 57, 69] </ref>. To achieve reasonable balance of load, the work involved in the partial factorization and in the assembly steps of a frontal matrix may itself get distributed among processors. Columns of partial updates need to be communicated both in the assembly and merge steps. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; <ref> [4; 5] </ref>; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. <p> 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [5] <author> C. Ashcraft, S. C. Eisenstat, and J. W. H. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> This index matching adds to the bookkeeping cost and is a major source of computational inefficiency in the fan-out algorithm. In the distributed fan-in method (see, for example <ref> [5] </ref>), for any column j, the cmod (j; k) tasks for different k's are distributed among several processors. Typically, the product formation part of a cmod (j,k) task is performed by processor P (k) . <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; <ref> [2; 5] </ref>; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; <ref> [4; 5] </ref>; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. <p> are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. <p> 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [6] <author> D. H. Bailey. </author> <booktitle> Misleading performance in the supercomputing field. In Proceedings of Supercomputing '92, </booktitle> <pages> pages 155-158, </pages> <year> 1992. </year>
Reference-contexts: For instance, Lindex [3] = 6 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex [4] = 9 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. Thus, Lval <ref> [6] </ref> through Lval [8] store the non-zero values in column 3 and Lstruct [6] through Lstruct [8] hold the corresponding row numbers. Here, 3, 5, and 6 are the respective row numbers of the non-zeros in column 3. <p> Thus, Lval <ref> [6] </ref> through Lval [8] store the non-zero values in column 3 and Lstruct [6] through Lstruct [8] hold the corresponding row numbers. Here, 3, 5, and 6 are the respective row numbers of the non-zeros in column 3. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; <ref> [6; 8] </ref>; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results. <p> Each of these four components is tightly coupled with the other three. Minimizing any one of the components may adversely affect one or more of the remaining three components. In the rest of the chapter, we follow the guidelines outlined by Bailey <ref> [6] </ref> and Barr and Hickman [8, 9] to report the results from our experiments, starting with the following description of the testing environment. 7.3 Testing environment We used SHAPE and a block sparse Cholesky numerical factorization code to conduct experiments on a suite of test matrices.
Reference: [7] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the 3rd ACM SIG-PLAN Symposium on Principles and Practice of Programming Languages, </booktitle> <pages> pages 213-223, </pages> <year> 1991. </year>
Reference-contexts: The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; <ref> [3; 7] </ref>; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; <p> These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval. <p> One may have to use a training set of task computational kernels on each target machine to estimate the performance and then use these estimates to run the simulator. The training set method was introduced by Balasundaram et. al. <ref> [7] </ref> to statically estimate the computation and communication performance and thereby evaluate the relative efficiency of different data partitioning schemes for distributed programs. We can also use the training set method to estimate the communication costs on the target architecture for the various communication primitives used by the program.
Reference: [8] <author> R. S. Barr and B. L. Hickman. </author> <title> Reporting computational experiments with parallel algorithms: Issues, measures and experts' opinions. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 2-18, </pages> <year> 1993. </year>
Reference-contexts: Thus, Lval [6] through Lval <ref> [8] </ref> store the non-zero values in column 3 and Lstruct [6] through Lstruct [8] hold the corresponding row numbers. Here, 3, 5, and 6 are the respective row numbers of the non-zeros in column 3. <p> Thus, Lval [6] through Lval <ref> [8] </ref> store the non-zero values in column 3 and Lstruct [6] through Lstruct [8] hold the corresponding row numbers. Here, 3, 5, and 6 are the respective row numbers of the non-zeros in column 3. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints 106 and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; <ref> [6; 8] </ref>; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. Let jU j = N and let I = [x o ; y o ] be a query interval. <p> Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results. <p> The most well-known metric to evaluate parallel performance of an MIMD code 161 is speedup. There are several definitions of speedup, summarized by Barr and Hick-man <ref> [8] </ref>. However, for sparse Cholesky factorization, reporting performance using a single speedup number is hardly enlightening, and in fact, may be quite misleading. There are two reasons for this. <p> Each of these four components is tightly coupled with the other three. Minimizing any one of the components may adversely affect one or more of the remaining three components. In the rest of the chapter, we follow the guidelines outlined by Bailey [6] and Barr and Hickman <ref> [8, 9] </ref> to report the results from our experiments, starting with the following description of the testing environment. 7.3 Testing environment We used SHAPE and a block sparse Cholesky numerical factorization code to conduct experiments on a suite of test matrices.
Reference: [9] <author> R. S. Barr and B. L. Hickman. </author> <title> Using parallel empirical testing to advance algorithm research. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 29-32, </pages> <year> 1993. </year>
Reference-contexts: Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results. <p> Each of these four components is tightly coupled with the other three. Minimizing any one of the components may adversely affect one or more of the remaining three components. In the rest of the chapter, we follow the guidelines outlined by Bailey [6] and Barr and Hickman <ref> [8, 9] </ref> to report the results from our experiments, starting with the following description of the testing environment. 7.3 Testing environment We used SHAPE and a block sparse Cholesky numerical factorization code to conduct experiments on a suite of test matrices.
Reference: [10] <author> S. H. Bokhari. </author> <title> Complete exchange on the iPSC-860. </title> <type> Technical Report ICASE Report No. 91-4, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, Hampton, VA, </institution> <year> 1991. </year>
Reference-contexts: It assumes that the receiving processor has allocated space and sends the message off in a single trip. If the destination processor has not issued a receive, the message is lost. It has been shown in <ref> [10] </ref> that the time in microseconds to communicate a message of length m bytes over distance d on the iPSC/860 is t = 95 + 0:394m + 10:3d using force type messages.
Reference: [11] <author> C. C. Chen and Y.H. Hu. </author> <title> A practical scheduling algorithm for parallel LU factorization in circuit simulation. </title> <booktitle> IEEE International Symposium on Circuits and Systems, </booktitle> <volume> 3 </volume> <pages> 1788-1791, </pages> <year> 1989. </year> <month> 232 </month>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation.
Reference: [12] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <journal> Computing Systems in Engineering, </journal> <volume> 3(1) </volume> <pages> 43-52, </pages> <year> 1992. </year>
Reference-contexts: Fortran M is built on the paradigm of task parallelism, and can be used to coordinate multiple data-parallel computations. In recent work <ref> [12] </ref>, the authors Choudhury et. al. assert that parallel computing demands support from software that precisely and effectively captures the structure of the application for better performance. They define about ten broad classes of computations, each of which is large enough to warrant individually tailored category-specific software support. <p> Increasing emphasis is being placed on having the compiler itself generate parts of the run-time software. In <ref> [12] </ref>, the authors conclude that for the implicit multiphase loosely synchronous problems, there is still a clear need for development of appropriate run-time support targeted toward SIMD and MIMD distributed memory architectures. Our goal is to start building such a system for large-scale multiprocessors that support the MIMD programming model.
Reference: [13] <author> R. Das, J. Saltz, and H. Berryman. </author> <title> A manual for Parti runtime primitives. </title> <type> Technical Report Interim Report 17, </type> <institution> ICASE, NASA Langley Research Center, </institution> <year> 1991. </year>
Reference-contexts: The preprocessing step or inspector constructs communication schedules which are then realized as communication messages by the executor which uses these schedules during loop execution. Both the inspector and executor are supported by a library of run-time message-passing procedures called Parti <ref> [13] </ref>. The language ARF (ARguably Fortran) [78] has been designed by taking Fortran 77 and adding extensions to serve as an interface between application programs and Parti primitives. 17 Unstructured sparse matrix computations are characterized by indirect array referencing.
Reference: [14] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <pages> pages 372-379, </pages> <year> 1992. </year>
Reference-contexts: The partitioner is discussed in Chapter 5. Two-dimensional block-based partitioning for dense matrices is known to have superior asymptotic communication behavior compared to column-based partitioning, and possesses better cache utilization properties <ref> [68, 52, 14] </ref>. However, exploiting these features in the Cholesky factorization of general sparse matrices is a complex task and requires sophisticated automatic tools.
Reference: [15] <author> J. J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <type> Technical Report 88, </type> <institution> Mathematical and Computer Science Division, Argonne National Lab., </institution> <year> 1986. </year>
Reference-contexts: These aimed at matrix-vector opera tions. Finally, for computers with a hierarchy of memory and true parallel processing capabilities, it is desirable to partition matrices into blocks and to perform the computations by using matrix-matrix operations on the blocks <ref> [15] </ref>. This approach avoids excessive movement of data to and from memory and gives a surface-to-volume effect for the ratio of operations to data movement.
Reference: [16] <author> J. J. Dongarra, F. G. Gustavson, and A. Karp. </author> <title> Implementing linear algebra algorithms for dense matrices on a vector pipeline machine. </title> <journal> SIAM Review, </journal> <volume> 26-1:91-112, </volume> <year> 1984. </year>
Reference-contexts: Apart from the data access pattern, the size and shape of data partitions also de pends on the architecture. Dongarra et.al. <ref> [16] </ref> cite several methods of row-oriented and column-oriented partitionings for matrix multiplication and LU decomposition, choosing the partitioning depending on the vector machine used. The different sets of BLAS (Basic Linear Algebra Subprograms) routines also give an indication as to how partitions change depending on hardware.
Reference: [17] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15-1:1-14, </volume> <year> 1989. </year>
Reference-contexts: 4410 352261 37667507 37575497 17075 linear statics L7PT16 7-point Laplacian 4096 257288 36260111 36150554 63907 discretization L7PT20 7-point Laplacian 8000 646839 136190284 135896617 165265 discretization L7PT25 7-point Laplacian 15625 1719292 556839224 556052830 402495 discretization L9PT127 9-point Laplacian 16129 563345 42934697 42675065 145164 discretization Table 7.1: Test matrices. sparse matrix collection <ref> [17] </ref>. These matrices are highly unstructured. For brevity, these matrices will also be referred to as B13, B15, B16, B17, B24, and B28, respectively, and collectively, as B* matrices. The 7-point Laplacian discretizations are for structured 3-D grid problems, and the last test case is a structured 2-D grid problem.
Reference: [18] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: The main source of computational inefficiency in the two methods described above is in the use of indirect addressing. This difficulty is overcome in the multifrontal methods where dense matrix partial factorizations are performed <ref> [18, 45] </ref>. These algorithms make use of the structure associated with the columns of a supernode.
Reference: [19] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 138-153, </pages> <year> 1990. </year>
Reference-contexts: We use the macro-dataflow model of task execution. In this model, a task starts executing only when all its input data are available, and executes to completion without interruption. This model has previously been used by Sarkar [64], Wu and Gajski [79], Lewis <ref> [19] </ref>, and Yang and Gerasoulis [30]. The order of execution of tasks is determined only by the precedence relationships among them and the availability of data on which they operate, without any kind of synchronization constraints.
Reference: [20] <author> K. Eswar, P. Sadayappan, C.-H. Huang and V. Visvanathan. </author> <title> Supernodal sparse Cholesky factorization on distributed-memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, 3 </booktitle> <address> 18-3:22, </address> <year> 1993. </year>
Reference-contexts: A more thorough investigation needs to be conducted to ensure that we see this improvement consistently, and that second pass is more efficient than the first pass. 7.8 Comparison with a distributed multifrontal code Recently, Eswar et. al. <ref> [20] </ref> have shown very good timings for the sparse Cholesky factorization of a 127 fi 127 grid matrix on a 32-processor iPSC/860, using a new supernodal algorithm.
Reference: [21] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <type> Technical Report MCS-P237-0992, </type> <institution> Argonne National Laboratory, </institution> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: The implementation then automatically extracts and exploits the task-level concurrency. The authors demonstrate how Jade exploits concurrency in sparse Cholesky factorization. Fortran M <ref> [21] </ref>, a language derived from Fortran 77 by means of message-passing extensions, facilitates a modular or object-oriented approach to parallel program design.
Reference: [22] <author> F. Gao and B. N. Parlett. </author> <title> A note on communication analysis of parallel sparse cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 59-60, </pages> <year> 1990. </year>
Reference-contexts: Assuming load balancing, and a distributed fan-out algorithm, the communication volume for the subtree-to-subcube assignment is O (P n 2 ) in [26]. This result was improved by Gao and Parlett <ref> [22] </ref>, who showed that the communication volume is O (n 2 ) per processor . The traffic is thus balanced among the processors. 37 3.
Reference: [23] <author> G. A. Geist and E. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18-4:291-314, </volume> <year> 1989. </year>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation.
Reference: [24] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a hypercube. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 129-156, </pages> <year> 1989. </year>
Reference-contexts: Then, the solution to the system is obtained by solving the following the two triangular systems Ly = b and L T x = y. The left part of Figure 2.1 shows a sequential, column-oriented Cholesky factorization algorithm for an N fi N matrix A <ref> [24] </ref>. In this and other codes which follow, it is assumed that the entries of L are initialized to the corresponding entries of A. The outermost for loop can be treated as steps or stages of factorization. <p> But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation. <p> This process is recursively repeated until the number of subgrids at some level is equal to or more than P . In the bottom-up scheme described in <ref> [24] </ref>, columns are assigned to processors in wrap-around fashion beginning at the leaf level of the elimination tree and going towards the root. 4.3 Results for column-based partitioning We first list the known theoretical results for communication during the Cholesky fac torization of the sparse matrix for the regular grid problem. <p> The primary aim is to illustrate the application of the methodology, so the algorithms we propose are not necessarily competitive with other existing algorithms for the solution of sparse triangular systems <ref> [81, 24, 69, 41] </ref>. The notations used in this chapter for the block partitions of L are defined in Chapter 5.
Reference: [25] <author> A. George, M. Heath, and J. W. H. Liu. </author> <title> Sparse Cholesky factorization on a local memory multiprocessor. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: We say that a processor owns column i, if that processor performs the cdiv (i) task. We denote a processor by P (i) , if that processor owns column i. We denote by pQ, the processor which is numbered Q. In the distributed fan-out method (see, for example <ref> [25] </ref>), for any column j, all cmod (j; k) tasks and cdiv (j) task take place in the processor P (j) .
Reference: [26] <author> A. George, J. W. H. Liu, and E. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10 </volume> <pages> 287-298, </pages> <year> 1989. </year> <month> 233 </month>
Reference-contexts: Details of the data dependencies among the separators are described in [52]. 4.2.1 Column assignment schemes The subtree-to-subcube scheme of assigning columns to processors was proposed by George, Liu and Ng <ref> [26] </ref> for hypercube network based distributed memory machines. The scheme is based on recursively mapping a subtree of the elimination tree to a subcube of the hypercube. <p> Assuming load balancing, and a distributed fan-out algorithm, George, Liu and Ng <ref> [26] </ref> showed that the communication volume for the bottom-up assignment is O (P n 2 log n). 2. Assuming load balancing, and a distributed fan-out algorithm, the communication volume for the subtree-to-subcube assignment is O (P n 2 ) in [26]. <p> balancing, and a distributed fan-out algorithm, George, Liu and Ng <ref> [26] </ref> showed that the communication volume for the bottom-up assignment is O (P n 2 log n). 2. Assuming load balancing, and a distributed fan-out algorithm, the communication volume for the subtree-to-subcube assignment is O (P n 2 ) in [26]. This result was improved by Gao and Parlett [22], who showed that the communication volume is O (n 2 ) per processor . The traffic is thus balanced among the processors. 37 3. George et.al [26] also showed that assuming load balance, and a distributed fanout algorithm, the communication volume <p> the communication volume for the subtree-to-subcube assignment is O (P n 2 ) in <ref> [26] </ref>. This result was improved by Gao and Parlett [22], who showed that the communication volume is O (n 2 ) per processor . The traffic is thus balanced among the processors. 37 3. George et.al [26] also showed that assuming load balance, and a distributed fanout algorithm, the communication volume is at least O (P n 2 ), irrespective of the ordering scheme and the scheme for assigning columns to processors. <p> Hulbert and Zmijewski [37] showed that for their column-based algorithm with the subtree-to-subcube assignment, the communication volume is O (P n 2 ), and the total number of messages is O (P n log P ). They also show that the fan-out algorithm in <ref> [26] </ref> communicates O (P n log n) messages. 5. Pothen and Sun [57] showed that their distributed multifrontal algorithm using clique trees, and a subtree-to-subcube mapping of the clique tree to processors, requires a communication volume of O (P n 2 ).
Reference: [27] <author> J. A. George, M. T. Heath, and J. W. H. Liu. </author> <title> Parallel Cholesky factorization on a shared-memory multiprocessor. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 77 </volume> <pages> 165-187, </pages> <year> 1986. </year>
Reference-contexts: In the literature, the scaling operations on column k are grouped into a single cdiv (k) task and the update of a column j using column k is defined to be a cmod (j; k) task <ref> [27] </ref>. The code on the right hand part of Figure 2.1 uses these tasks.
Reference: [28] <author> J. A. George and J. W. H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliff, NJ, </address> <year> 1981. </year>
Reference-contexts: The code on the right hand part of Figure 2.1 uses these tasks. This version in which a column is factored and immediately used to update all columns to its right is called the sub-matrix form of Cholesky factorization <ref> [28] </ref>. 2.2 Sequential sparse Cholesky factorization When the system of equations is sparse, several other considerations come into play in designing the factorization algorithm. As the factorization proceeds from left to right, entries which were originally zeros may be replaced by non-zero values this is called fill-in. <p> Let S be a separator in the grid which is ordered ahead of V or H , with the property that every grid point in S is connected to g in the elimination graph, just prior to the elimination of S. Please refer to <ref> [28] </ref> 46 m g B B' D D' 2m A B C D g for a discussion of the elimination graph model. Let g c be the column in the matrix corresponding to the grid point g. <p> The time taken to factor an n fi n sub-grid at level i which is bordered on all four sides is 371n 3 =12 + O (n 2 log 2 n), see <ref> [28] </ref>. Ignoring the lower order term and replacing n by n=2 l , a processor takes time (371n 3 )=(12 fl 2 3l ) to compute the lowest level sub-grid assigned to it. <p> The computational work in factoring the grid matrix is O (n 3 ) and the number of nonzero elements in the factored matrix is O (n 2 log n) <ref> [28] </ref>. Thus, with both column 59 and block partitioning methods, the average computational work associated with each processor is O (n 3 =P ) and the average number of nonzero elements assigned to each processor are O (n 2 log n=P ). <p> In performing the experiments, two reordering schemes were used. These were: the recursive automatic nested dissection (AND) scheme <ref> [28] </ref> and Liu's multiple minimum degree ordering (MMD) scheme [42]. In the following, unless otherwise specified, the B* matrices were reordered using MMD and the L* matrices were reordered using AND. The selection of the reordering schemes was arbitrary. <p> The idea is that the ordering should result in an elimination tree, (or a dependency graph, in general), that is short and wide rather than tall and skinny, thus providing more parallelism. Two well-known ordering schemes are the multiple minimum degree (MMD) [42] and automatic nested dissection (AND) <ref> [28] </ref>, which were aimed at reducing the fill-in during the factorization, so as to ultimately reduce the computational work. In recent years, the aggressive effort in parallelizing sparse Cholesky factorization has resulted in renewed investigation for designing new ordering schemes which would generate more parallelism for factorization.
Reference: [29] <author> J. A. George and J. W. H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <type> Technical Report ORNL/TM-10452, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tenn., </institution> <year> 1987. </year>
Reference-contexts: If this matrix is reordered using the nested dissection reordering method <ref> [29] </ref>, the ordering is optimal with respect to both number of arithmetic operations and fill-in for sequential sparse Cholesky factorization of the corresponding sparse matrix. The number of multiplicative operations is 9:87n 3 and the number of non-zeros in the Cholesky factor is 7:75n 2 log n [29]. <p> dissection reordering method <ref> [29] </ref>, the ordering is optimal with respect to both number of arithmetic operations and fill-in for sequential sparse Cholesky factorization of the corresponding sparse matrix. The number of multiplicative operations is 9:87n 3 and the number of non-zeros in the Cholesky factor is 7:75n 2 log n [29].
Reference: [30] <author> A. Gerasoulis and T. Yang. </author> <title> On the granularity and clustering of directed acyclic task graphs. </title> <type> Technical Report DCS-TR-153, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: The problem of partitioning to minimize the cost function is shown to be NP-complete. We do not know of any task partitioning heuristics with guaranteed performance bounds. Yang and Gerasoulis <ref> [30] </ref> have performed a formal analysis of granularity for task graphs and propose a new definition of the granularity for a DAG which captures the trade-off point between parallelization and sequentialization. It is not known as to how this granularity analysis may be used to partition unstructured computations. <p> We use the macro-dataflow model of task execution. In this model, a task starts executing only when all its input data are available, and executes to completion without interruption. This model has previously been used by Sarkar [64], Wu and Gajski [79], Lewis [19], and Yang and Gerasoulis <ref> [30] </ref>. The order of execution of tasks is determined only by the precedence relationships among them and the availability of data on which they operate, without any kind of synchronization constraints. During the parallelization process, the data blocks are distributed among or mapped to the processors.
Reference: [31] <author> J. L. Gustafson. </author> <title> The "tar baby" of computing: Performance analysis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 19-21, </pages> <year> 1993. </year>
Reference-contexts: Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results. <p> Therefore, a poor speedup figure for a parallel algorithm does not necessarily imply that the algorithm is inefficient it may be due to an inherent limitation in the exploitable parallelism in the problem. Second, as pointed out by Gustafson <ref> [31] </ref>, the conventional speedup figure that measures reduction in execution time with increase in the number of processors penalizes faster absolute speed. For example, consider the iPSC/2 and the iPSC/860 machines.
Reference: [32] <author> M. T. Heath, G. A. Geist, and J. B. Drake. </author> <title> Early experience with the Intel iPSC/860 at Oak Ridge National Laboratory. </title> <type> Technical Report ORNL/TM-11655, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: However, Sun uses double precision arithmetic, whereas we use single precision arithmetic for the numerical operations performed towards factorization. Both use the Kuck math library routines themultifrontal code uses daxpy, ddot and dscal, while the block code uses sdot. Heath, Geist and Drake <ref> [32] </ref> give execution rates for sdot and ddot on the i860. Their assembler version ddot is about 1 Mflops slower than their assembler version of sdot for vector lengths up to 1000. They also give asymptotic execution rates for row-Cholesky, which uses dot product.
Reference: [33] <author> M. T. Heath, E. Ng, and B. W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: As in the fan-out method, the cdiv (j) task is executed in the processor P (j) . Compared to the fan-out method, the fan-in method has lower communication costs, both in terms of total number of messages and the amount of data transmitted during the factorization <ref> [33] </ref>. These gains are significant when the sparse matrix has some regularity such as those representing uniform grid graphs. The main drawback of the fan-in method as compared to the fan-out method, is the difficulty in balancing the computational work load among processors.
Reference: [34] <author> M. T. Heath and P. Raghavan. </author> <title> A cartesian parallel nested dissection algorithm. </title> <type> Technical Report UIUCDCS-R-92-1772, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: A good ordering scheme for parallel factorization would therefore be one that would create a significantly larger amount of added parallelism for factorization, without giving away too much additional computational work. Recent ordering algorithms include spectral nested dissection (SND) [56] and a cartesian nested dissection (CND) <ref> [34] </ref>. The SND algorithm has been shown to incur fill-in that is quite close to MMD and much smaller than AND. It outperforms both MMD and AND with respect to parallelism offered by the elimination tree for factoring.
Reference: [35] <author> J. Helin and R. Berrendorf. </author> <title> Analyzing the performance of message passing mimd hypercubes: A study with the Intel iPSC/860. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 376-385, </pages> <year> 1991. </year>
Reference-contexts: This has to increasing communication to computation ratios. For instance, the iPSC/860 has a higher communication to communication ratio than the iPSC/2, an earlier generation machine <ref> [35] </ref>. It is evident that if we want to see high performance for any algorithm on current large-scale multiprocessors, we need to exploit parallelism at a coarse grain level. <p> Second, as pointed out by Gustafson [31], the conventional speedup figure that measures reduction in execution time with increase in the number of processors penalizes faster absolute speed. For example, consider the iPSC/2 and the iPSC/860 machines. Helin and Berrendorf <ref> [35] </ref> have studied the computation and communication properties of the iPSC/860 and compared these with the iPSC/2. Their study shows that a single iPSC/860 processor is nearly 50 times as fast as a single processor of the iPSC/2, on the average. <p> The Opcount columns list the total number of floating point arithmetic operations 168 in the numerical factorization. This is obtained by assuming the following: an add and multiply are one flop each, a reciprocal is three flops, a square root and a divide are four flops each <ref> [35] </ref>. Each entry under "Block" is the number of floating point operations in the block-partitioned method and each entry under "Column" is the number of floating point operations in the column-partitioned method. In the block method, we perform some additional computation due to the T R tasks.
Reference: [36] <author> S. Hiranandani, K. Kennedy, and C. W. Tseng. </author> <title> Compiler support for machine independent parallel programming in Fortran D. </title> <type> Technical Report TR91-149, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: These drawbacks make effective manual parallelization impractical for most realistic problems. 2.4 Compiler techniques for parallelization Fortran D, a version of Fortran enhanced with data decomposition specifications <ref> [36] </ref>, provides constructs for specifying regular and irregular distribution of arrays to pro cessors. An irregular distribution is described at run-time by a mapping function.
Reference: [37] <author> L. Hulbert and E. Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In other words, this is asymptotically the minimum volume of communication for the fan out algorithm, given the assumption of load balance. 4. Hulbert and Zmijewski <ref> [37] </ref> showed that for their column-based algorithm with the subtree-to-subcube assignment, the communication volume is O (P n 2 ), and the total number of messages is O (P n log P ). They also show that the fan-out algorithm in [26] communicates O (P n log n) messages. 5.
Reference: [38] <author> J. A. G. Jess and H. G. M. Kees. </author> <title> A data structure for parallel L/U decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C31-3:231-239, </volume> <year> 1982. </year>
Reference-contexts: The notion of elimination tree structure has been used extensively to describe and implement many aspects of sparse matrix computations (see e.g., <ref> [38, 44, 67] </ref>). Each node in the elimination tree corresponds to a column of A.
Reference: [39] <author> N. Karmarkar. </author> <title> A new parallel architecture for sparse matrix computation based on finite projective geometries. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 358-369, </pages> <year> 1991. </year>
Reference-contexts: There are several applications in which the same symbolic structure is repeatedly used for computations with varying data values. These include circuit 5 simulation, numerical solution of partial differential equations, linear programming, control systems, Newton methods for nonlinear optimization problems. Some of these are described in <ref> [39, 65, 66] </ref> The methodology follows a two-phase process of partitioning and scheduling. The partitioning phase and a part of the scheduling phase constitute the preprocessing portion of the methodology. The rest of the scheduling phase, including the optimized communication support for the computations, is combined with the numerical computations.
Reference: [40] <author> C. Koelbel, P. Mehrotra, J. Saltz, and S. Berryman. </author> <title> Parallel loops on distributed machines. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, 1990. </booktitle> <address> Charleston, SC. </address> <month> 234 </month>
Reference-contexts: Computations which can be accurately characterized at compile-time are called regular computations; those which cannot are described as irregular computations. The communication messages required for irregular computations cannot be decided at compile-time. A technique for run-time preprocessing of loops is proposed in <ref> [63, 40] </ref>, given that the dependencies are known before entering the loop at run time, and that the dependencies do not change in the course of executing the loop iterations.
Reference: [41] <author> P. S. Kumar, M. K. Kumar, and A. Basu. </author> <title> Parallel algorithms for sparse triangular system solution. </title> <journal> Parallel Computing, </journal> <volume> 19(2) </volume> <pages> 187-196, </pages> <year> 1993. </year>
Reference-contexts: The primary aim is to illustrate the application of the methodology, so the algorithms we propose are not necessarily competitive with other existing algorithms for the solution of sparse triangular systems <ref> [81, 24, 69, 41] </ref>. The notations used in this chapter for the block partitions of L are defined in Chapter 5.
Reference: [42] <author> J. W. H. Liu. </author> <title> Modification of the minimum-degree algorithm by multiple elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 </volume> <pages> 141-153, </pages> <year> 1985. </year>
Reference-contexts: In performing the experiments, two reordering schemes were used. These were: the recursive automatic nested dissection (AND) scheme [28] and Liu's multiple minimum degree ordering (MMD) scheme <ref> [42] </ref>. In the following, unless otherwise specified, the B* matrices were reordered using MMD and the L* matrices were reordered using AND. The selection of the reordering schemes was arbitrary. <p> The idea is that the ordering should result in an elimination tree, (or a dependency graph, in general), that is short and wide rather than tall and skinny, thus providing more parallelism. Two well-known ordering schemes are the multiple minimum degree (MMD) <ref> [42] </ref> and automatic nested dissection (AND) [28], which were aimed at reducing the fill-in during the factorization, so as to ultimately reduce the computational work.
Reference: [43] <author> J. W. H. Liu. </author> <title> Computational models and task scheduling for parallel sparse Cholesky factorization. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> Depending on the granularity at which parallelism is extracted, parallel factorization methods can be grouped into four categories, each with its own model of parallel computation. In <ref> [43] </ref>, the author has described three computational models based on task parallelism. We prefer to classify the models based on data parallelism. This classification is better suited for the methods proposed in this paper. <p> We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation.
Reference: [44] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <type> Technical Report CS-87-12, </type> <institution> Department of Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <year> 1987. </year>
Reference-contexts: The notion of elimination tree structure has been used extensively to describe and implement many aspects of sparse matrix computations (see e.g., <ref> [38, 44, 67] </ref>). Each node in the elimination tree corresponds to a column of A.
Reference: [45] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> Department of Computer Science, York University, </institution> <address> North York, Ontario, Canada, </address> <year> 1990. </year>
Reference-contexts: The main source of computational inefficiency in the two methods described above is in the use of indirect addressing. This difficulty is overcome in the multifrontal methods where dense matrix partial factorizations are performed <ref> [18, 45] </ref>. These algorithms make use of the structure associated with the columns of a supernode.
Reference: [46] <author> H. D. Loshin and A. Vasilevsky. </author> <title> Parallel reporting guidelines are on target. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 22-23, </pages> <year> 1993. </year>
Reference-contexts: Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results.
Reference: [47] <author> L. Lu and M. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, Aug. 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The language ARF (ARguably Fortran) [78] has been designed by taking Fortran 77 and adding extensions to serve as an interface between application programs and Parti primitives. 17 Unstructured sparse matrix computations are characterized by indirect array referencing. Lu and Chen <ref> [47] </ref> propose a hybrid compiler and run-time approach for parallelizing loops with indirections and pointers. In this approach, the scheduler itself is generated by the compiler with run-time support, while Saltz's approach used in Parti is purely run-time, with the scheduling code being composed of hand-written run-time library routines.
Reference: [48] <author> R. F. Lucas, T. Blank, and J. T. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer-Aided Design, CAD-6, </journal> <volume> No. 6 </volume> <pages> 981-991, </pages> <year> 1987. </year>
Reference-contexts: The inefficiencies arise in 68 the assembly and the merge steps. As a consequence, these methods yield the best computational efficiency when the frontal matrices are large. In the distributed implementations of multifrontal methods, columns are distributed among processors, as in the fan-out and fan-in methods <ref> [48, 4, 57, 69] </ref>. To achieve reasonable balance of load, the work involved in the partial factorization and in the assembly steps of a frontal matrix may itself get distributed among processors. Columns of partial updates need to be communicated both in the assembly and merge steps.
Reference: [49] <author> E. L. Lusk. </author> <title> Speedups and insights. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 24-25, </pages> <year> 1993. </year>
Reference-contexts: Recently, there has been a spate of articles regarding the reporting of experimental results to demonstrate the performance of parallel algorithms <ref> [6, 8, 31, 49, 9, 46] </ref>. The two primary issues addressed in these articles are: the metrics to be chosen to evaluate parallel performance, and the guidelines to be followed to report experimental results.
Reference: [50] <author> K. Mehlhorn. </author> <title> Data Structures and Algorithms, Vol. 3 Multidimensional Searching and Computational Geometry. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: An interval tree for S of depth O (log N ) can be constructed in time O (N + n log N + n log n). iii. A can be computed in time O (log N + jAj). See <ref> [50] </ref> for a detailed description of the interval tree and its uses. Grouping and storage of intervals Clusters are partitioned and the dependencies involving blocks in the cluster are computed by proceeding left to right along the matrix, processing clusters one by one. <p> Steps 2 and 3 build and store the intervals in the interval tree. For details on how to store the intervals in an interval tree, refer <ref> [50] </ref>.
Reference: [51] <author> Mo Mu and J. R. Rice. </author> <title> The structure of parallel sparse matrix algorithms for solving elliptic partial differential equations on hypercubes. </title> <type> Technical Report CSD-TR-976, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1990. </year>
Reference-contexts: Apart from this, the partitioning or decomposition of tasks and data has to be sensitive to the machine granularity, so that the same partitioner may be ported to different architectures with different granularities. A study of various algorithmic components and performance aspects of PDE sparse solvers is presented in <ref> [51] </ref>. An outcome of the study was the observation that sparse matrix techniques should be used in different ways at different stages of the factorization because the problem's nature varies from very sparse to dense during the solution process.
Reference: [52] <author> V. Naik and M. Patrick. </author> <title> Data traffic reduction schemes for Cholesky factorization on asynchronous multiprocessor systems. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1989. </year> <note> Also available as IBM Research Report RC 14500. </note>
Reference-contexts: The partitioner is discussed in Chapter 5. Two-dimensional block-based partitioning for dense matrices is known to have superior asymptotic communication behavior compared to column-based partitioning, and possesses better cache utilization properties <ref> [68, 52, 14] </ref>. However, exploiting these features in the Cholesky factorization of general sparse matrices is a complex task and requires sophisticated automatic tools. <p> However, exploiting these features in the Cholesky factorization of general sparse matrices is a complex task and requires sophisticated automatic tools. Naik and Patrick <ref> [52] </ref> 7 have proposed a two-dimensional block partitioning scheme for the Cholesky factorization of sparse matrices arising from a class of structured grid problems, and has shown that the data traffic complexity for this scheme is optimal. <p> In general, corresponding to each separator in the grid, there is a clique of columns in the grid matrix, and corresponding to each grid point in this separator, 36 there is a column in this clique. Details of the data dependencies among the separators are described in <ref> [52] </ref>. 4.2.1 Column assignment schemes The subtree-to-subcube scheme of assigning columns to processors was proposed by George, Liu and Ng [26] for hypercube network based distributed memory machines. The scheme is based on recursively mapping a subtree of the elimination tree to a subcube of the hypercube.
Reference: [53] <author> V. K. Naik. </author> <title> Multiprocessing: Trade-offs in Computation and Communication. </title> <publisher> Kluwer Academic, </publisher> <year> 1993. </year>
Reference-contexts: We have another recurrence relation M (n; P ) = m (n; P ) + 4M (n=2; P=4), solving for which gives the us required result, M (n; P ) 20P n log n = O (P n log n) 4.4 Results for block-based partitioning Naik <ref> [54, 53] </ref> proposed a block partitioning scheme and proved that it incured a data traffic of O (n 2 p P ) in factoring the sparse grid matrix described earlier.
Reference: [54] <author> V. K. Naik. </author> <title> On the Computation and Communication Tradeoffs and their Impact on the Performance of Asynchronous Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Duke University, Durham, NC., </institution> <year> 1988. </year>
Reference-contexts: We have another recurrence relation M (n; P ) = m (n; P ) + 4M (n=2; P=4), solving for which gives the us required result, M (n; P ) 20P n log n = O (P n log n) 4.4 Results for block-based partitioning Naik <ref> [54, 53] </ref> proposed a block partitioning scheme and proved that it incured a data traffic of O (n 2 p P ) in factoring the sparse grid matrix described earlier. <p> We then present our analysis of computation time, communication time and idle time for this partitioning scheme. We also derive a bound on the total number of messages for this partitioning scheme. 40 4.4.1 Sparse BLOCC scheme Naik, in <ref> [54] </ref>, proposes a scheme for dense Cholesky factorization called the block oriented column Cholesky factorization scheme, or BLOCC scheme for short. The scheme is as follows. Let P processors be assigned to factorize a dense m fi m symmetric positive definite matrix. <p> Each partition is assigned a separate processor. For details of the factorization of the entire matrix, including the allocation of processors to separators, see <ref> [54] </ref>. 4.4.2 Sparse BLOCC scheme for message-passing multiprocessors We extend the sparse BLOCC scheme to message-passing multiprocessors. In order to understand the performance benefits and the limitations of the block scheme, we have developed a model to represent the parallel computations in factoring the sparse matrix. <p> We now derive the total number of messages sent during the factorization using the sparse BLOCC scheme, without using the performance model. We show that the total number of messages is O (P p The intuition for this result is as follows. Naik <ref> [54] </ref> showed that the data traffic for the BLOCC scheme is O (n 2 p P ). In this partitioning scheme, the size of a block is constant, namely O (n 2 =P ).
Reference: [55] <author> L. S. Ostrouchov, M. T. Heath, and C. H. Romine. </author> <title> Modeling speedup in parallel sparse matrix factorization. </title> <type> Technical Report TM-11786, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year> <month> 235 </month>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning [5, 11, 23, 24, 43, 81]. In <ref> [55] </ref>, a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. The conclusion was that this poor performance is mainly due to the poor communication performance of the current distributed memory message-passing architectures relative to their floating point speed.
Reference: [56] <author> A. Pothen, H. D. Simon, L. Wang, and S. T. Barnard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: A good ordering scheme for parallel factorization would therefore be one that would create a significantly larger amount of added parallelism for factorization, without giving away too much additional computational work. Recent ordering algorithms include spectral nested dissection (SND) <ref> [56] </ref> and a cartesian nested dissection (CND) [34]. The SND algorithm has been shown to incur fill-in that is quite close to MMD and much smaller than AND. It outperforms both MMD and AND with respect to parallelism offered by the elimination tree for factoring.
Reference: [57] <author> A. Pothen and C. Sun. </author> <title> A distributed multifrontal algorithm using clique trees. </title> <type> Technical Report CTC91TR72, </type> <institution> Advanced Computing Research Institute, Cornell Theory Center, Cornell University, </institution> <year> 1991. </year>
Reference-contexts: They also show that the fan-out algorithm in [26] communicates O (P n log n) messages. 5. Pothen and Sun <ref> [57] </ref> showed that their distributed multifrontal algorithm using clique trees, and a subtree-to-subcube mapping of the clique tree to processors, requires a communication volume of O (P n 2 ). <p> The inefficiencies arise in 68 the assembly and the merge steps. As a consequence, these methods yield the best computational efficiency when the frontal matrices are large. In the distributed implementations of multifrontal methods, columns are distributed among processors, as in the fan-out and fan-in methods <ref> [48, 4, 57, 69] </ref>. To achieve reasonable balance of load, the work involved in the partial factorization and in the assembly steps of a frontal matrix may itself get distributed among processors. Columns of partial updates need to be communicated both in the assembly and merge steps. <p> This 195 algorithm is an improvement on an earlier version designed by Pothen and Sun <ref> [57] </ref>. We will refer to the latter version as the old multifrontal code and the former as the new multifrontal code.
Reference: [58] <author> M. C. Rinard, D. J. Scales, and M. S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <year> 1993. </year>
Reference-contexts: In this approach, the scheduler itself is generated by the compiler with run-time support, while Saltz's approach used in Parti is purely run-time, with the scheduling code being composed of hand-written run-time library routines. Jade <ref> [58] </ref> is a high-level, implicitly parallel language designed for coarse-grain, task-level concurrency. To parallelize an application, the programmer has to specify a decomposition of data into atomic units, a decomposition of the sequential program into tasks and a description of how each task will access data.
Reference: [59] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Department of Electrical Engineering and Computer Science, Stanford University, </institution> <year> 1992. </year> <note> Technical report CSL-TR-92-555. </note>
Reference-contexts: Thirdly, for a fixed machine granularity, single-processor performance depends largely on the shape of blocks which is determined by data access patterns in the algorithm, cache behavior, and vector lengths. A detailed analysis of blocking for hierarchical memory is done in <ref> [59] </ref>. We use the BLOCC scheme as the basis for our partitioning, but enhance it considerably to satisfy the requirements pointed out by the preceding discussion.
Reference: [60] <author> E. Rothberg and A. Gupta. </author> <title> An efficient block oriented approach to parallel sparse Cholesky factorization. </title> <type> Technical Report STAN-CS-92-1438, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1992. </year>
Reference-contexts: The block-based methods, on the other hand, scale well under the assumptions made here. 60 In other work, Rothberg and Gupta, in <ref> [60] </ref>, propose a block fan-out algorithm for distributed sparse Cholesky factorization. For the grid problem, the communication volume is O (n 2 log n p P ) when parallelized on P processors. <p> In [75], we examined, in detail, the effect of two partitioning parameters on the computation speeds, communication costs, extent of processor idling because of load imbalances, and bookkeeping overheads during the parallel numerical factorization using a sparse block code on an iPSC/860 machine. In <ref> [60] </ref>, a block method has been suggested for partitioning sparse matrices in Cholesky factorization. Based on simulation results, the authors have claimed their method to be efficient. In this chapter, we describe the design of the parallel partitioner.
Reference: [61] <author> Y. Saad. Sparskit: </author> <title> a basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> RIACS, NASA Ames Research Center, </institution> <year> 1990. </year>
Reference-contexts: In the following, unless otherwise specified, the B* matrices were reordered using MMD and the L* matrices were reordered using AND. The selection of the reordering schemes was arbitrary. We used SPARSKIT <ref> [61] </ref> and the Wisconsin Sparse Matrix Manipulation System [2] for generating and converting the test matrices into various formats, and for ordering and symbolically factoring most of 169 the test matrices.
Reference: [62] <author> P. Sadayappan and V. Visvanathan. </author> <title> Modeling and optimal scheduling of parallel sparse Gaussian elimination. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, pages Vol III: Algorithms and Applications, </booktitle> <pages> 54-61, </pages> <year> 1988. </year>
Reference-contexts: Representation for block-based unstructured computations: We have introduced a novel explicit representation called the Block Computation Representation, or BCR for short. We examined two other representations, the Directed Acyclic Graph (DAG) [80] and the Minimally Constrained Task Graph (MCTG) for fine-grain Gaus sian elimination <ref> [62] </ref>, before devising the BCR and tailoring it for the class of block-based unstructured computations using the owner computes rule. <p> We study the issue of representing an unstructured computation in which data has been partitioned into blocks, and the owner computes rule is assumed. In this study, we examine two other representations, the Directed Acyclic Graph (DAG) [80] and the Minimally Constrained Task Graph (MCTG) for fine-grain Gaussian elimination <ref> [62] </ref>, neither of which assume the owner computes rule. <p> The actual order in which the non-simultaneous tasks may be executed is determined at execution time, depending on the order of arrival of the data blocks on which these tasks operate. Sadayappan and Visvanathan <ref> [62] </ref> call this the "non-simultaneity" constraint. They incorporate this in a new representation to model parallel sparse Gaussian Elimination, called the Minimally Constrained Task Graph (MCTG). Each node of an MCTG represent a computation which updates one element of the matrix. Hence, it is a fine grain graph.
Reference: [63] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <booktitle> In Proceedings of the 1st Symposium on Parallel Algorithms and Architectures, 1989. </booktitle> <address> Santa Fe, NM. </address>
Reference-contexts: Computations which can be accurately characterized at compile-time are called regular computations; those which cannot are described as irregular computations. The communication messages required for irregular computations cannot be decided at compile-time. A technique for run-time preprocessing of loops is proposed in <ref> [63, 40] </ref>, given that the dependencies are known before entering the loop at run time, and that the dependencies do not change in the course of executing the loop iterations.
Reference: [64] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: We propose a new representation, called the Block Computation Representation (BCR), for block-based, unstructured computations, that is different from both the DAG and the MCTG. 20 3.1 Partitioning of unstructured computations Sarkar <ref> [64] </ref> has dealt with task partitioning at the computation graph level. One can start with the entire algorithm as one task and recursively split the computations, thereby unrolling the graph. Or, starting with the statement level dependency graph, one could coalesce nodes together into bigger tasks. <p> We use the macro-dataflow model of task execution. In this model, a task starts executing only when all its input data are available, and executes to completion without interruption. This model has previously been used by Sarkar <ref> [64] </ref>, Wu and Gajski [79], Lewis [19], and Yang and Gerasoulis [30]. The order of execution of tasks is determined only by the precedence relationships among them and the availability of data on which they operate, without any kind of synchronization constraints.
Reference: [65] <author> T. Schlick. </author> <title> Modified Cholesky factorizations for sparse preconditioners. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14(2) </volume> <pages> 424-445, </pages> <year> 1993. </year>
Reference-contexts: There are several applications in which the same symbolic structure is repeatedly used for computations with varying data values. These include circuit 5 simulation, numerical solution of partial differential equations, linear programming, control systems, Newton methods for nonlinear optimization problems. Some of these are described in <ref> [39, 65, 66] </ref> The methodology follows a two-phase process of partitioning and scheduling. The partitioning phase and a part of the scheduling phase constitute the preprocessing portion of the methodology. The rest of the scheduling phase, including the optimized communication support for the computations, is combined with the numerical computations.
Reference: [66] <author> T. Schlick and M. Overton. </author> <title> A powerful truncated Newton method for potential energy minimization. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 8(7) </volume> <pages> 1025-1039, </pages> <year> 1987. </year>
Reference-contexts: There are several applications in which the same symbolic structure is repeatedly used for computations with varying data values. These include circuit 5 simulation, numerical solution of partial differential equations, linear programming, control systems, Newton methods for nonlinear optimization problems. Some of these are described in <ref> [39, 65, 66] </ref> The methodology follows a two-phase process of partitioning and scheduling. The partitioning phase and a part of the scheduling phase constitute the preprocessing portion of the methodology. The rest of the scheduling phase, including the optimized communication support for the computations, is combined with the numerical computations.
Reference: [67] <author> R. Schreiber. </author> <title> A new implementation of sparse Gaussian elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 </volume> <pages> 256-276, </pages> <year> 1982. </year>
Reference-contexts: The notion of elimination tree structure has been used extensively to describe and implement many aspects of sparse matrix computations (see e.g., <ref> [38, 44, 67] </ref>). Each node in the elimination tree corresponds to a column of A.
Reference: [68] <author> R. Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report 92.13, </type> <institution> Research Institute for Advanced Computer Science (RIACS), NASA Ames Research Center, </institution> <year> 1992. </year>
Reference-contexts: The partitioner is discussed in Chapter 5. Two-dimensional block-based partitioning for dense matrices is known to have superior asymptotic communication behavior compared to column-based partitioning, and possesses better cache utilization properties <ref> [68, 52, 14] </ref>. However, exploiting these features in the Cholesky factorization of general sparse matrices is a complex task and requires sophisticated automatic tools. <p> This sum is c 1 P (1 log p= P ) for some constant c 1 . Summing this over all processors, asymptotically, we have that the total number of messages sent during the factorization is O (P P ), which proves the theorem. 4.5 Conclusions Schreiber, in <ref> [68] </ref>, investigates the scalability of distributed sparse Cholesky factorization using column-mapped methods and two dimensional block mapping methods.
Reference: [69] <author> C. Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical Report CTC92TR102, </type> <institution> Advanced Computing Research Institute, Cornell Theory Center, Cornell University, </institution> <year> 1992. </year> <month> 236 </month>
Reference-contexts: The inefficiencies arise in 68 the assembly and the merge steps. As a consequence, these methods yield the best computational efficiency when the frontal matrices are large. In the distributed implementations of multifrontal methods, columns are distributed among processors, as in the fan-out and fan-in methods <ref> [48, 4, 57, 69] </ref>. To achieve reasonable balance of load, the work involved in the partial factorization and in the assembly steps of a frontal matrix may itself get distributed among processors. Columns of partial updates need to be communicated both in the assembly and merge steps. <p> To the best of our knowledge, the best comprehensive performance results for sparse Cholesky factorization for both unstructured and structured matrices on the iPSC/860 are obtained from a distributed multifrontal algorithm developed by Sun <ref> [69] </ref>. This 195 algorithm is an improvement on an earlier version designed by Pothen and Sun [57]. We will refer to the latter version as the old multifrontal code and the former as the new multifrontal code. <p> It is only a rough idea, for several reasons. Only the factorization times are presented by Sun in <ref> [69] </ref>. The communication, idle and overhead times are not given, so it is hard to get much insight into the general behavior of that algorithm. Although the B13 and B15 matrices in [69] are ordered using minimum degree ordering, the number of non-zeros in the factor is 271671 for B13 and <p> Only the factorization times are presented by Sun in <ref> [69] </ref>. The communication, idle and overhead times are not given, so it is hard to get much insight into the general behavior of that algorithm. Although the B13 and B15 matrices in [69] are ordered using minimum degree ordering, the number of non-zeros in the factor is 271671 for B13 and 651222 for B15. In both cases, the number of nonzeros is 2% more than those reported in Table 7.1. <p> The primary aim is to illustrate the application of the methodology, so the algorithms we propose are not necessarily competitive with other existing algorithms for the solution of sparse triangular systems <ref> [81, 24, 69, 41] </ref>. The notations used in this chapter for the block partitions of L are defined in Chapter 5.
Reference: [70] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <type> CM5 Technical Summary, </type> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: This leads to overheads of system message buffer management and recopying. We refer to the first protocol as the direct protocol, and the second as the indirect protocol. The Intel iPSC/860 supports the direct protocol. The Thinking Machines CM5 provides both blocking and non-blocking message passing functions <ref> [70, 71] </ref>. A non-blocking function returns as soon as the processor has announced its readiness to send or receive. The processor can then perform other work, while waiting for the other processor to announce its readiness.
Reference: [71] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> CMMD Reference Manual, </note> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: This leads to overheads of system message buffer management and recopying. We refer to the first protocol as the direct protocol, and the second as the indirect protocol. The Intel iPSC/860 supports the direct protocol. The Thinking Machines CM5 provides both blocking and non-blocking message passing functions <ref> [70, 71] </ref>. A non-blocking function returns as soon as the processor has announced its readiness to send or receive. The processor can then perform other work, while waiting for the other processor to announce its readiness.
Reference: [72] <author> S. Venugopal and V. K. Naik. </author> <title> A comparison of block and column methods in distributed sparse Cholesky factorization. </title> <note> In preparation. </note>
Reference-contexts: With such a hybrid partitioning method, higher performance and scalability is achievable, provided a right set of parameters is used in partitioning the 63 sparse matrix <ref> [74, 72] </ref>. In [75], we examined, in detail, the effect of two partitioning parameters on the computation speeds, communication costs, extent of processor idling because of load imbalances, and bookkeeping overheads during the parallel numerical factorization using a sparse block code on an iPSC/860 machine.
Reference: [73] <author> S. Venugopal and V. K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference-contexts: It is described in Chapter 3, and its construction for block-based sparse Cholesky factorization is described in Chapter 5. Parallel partitioner for block-based sparse Cholesky factorization: We have developed a parallel partitioner for the sparse Cholesky factorization of any general sparse matrix <ref> [73, 74, 75] </ref>, which partitions the symbolic factor into a hybrid mixture of sparse columns, dense triangles and dense rectangles. Tuning knobs in the form of parameters are provided in order that the partitioner may be ported across problems with widely ranging sparsity structures and across architectures with different granu-larities. <p> Large structured and unstructured matrices encountered in real-life applications may contain several million non-zero entries and, in such cases, any type of systematic block partitioning by hand is simply impractical. To address these issues, we started our investigation of general 2-D block partitioning and scheduling in <ref> [73] </ref>, where we introduced an automatic, general purpose block-based partitioning scheme that takes into account the sparsity structure of the matrix in extracting the partitions, and allows for control of the partition granularity.
Reference: [74] <author> S. Venugopal and V. K. Naik. </author> <title> Shape: A parallelization tool for sparse matrix computations. </title> <type> Technical Report DCS-TR-290, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1992. </year> <note> Also available as IBM Research Report RC 17899. </note>
Reference-contexts: It is described in Chapter 3, and its construction for block-based sparse Cholesky factorization is described in Chapter 5. Parallel partitioner for block-based sparse Cholesky factorization: We have developed a parallel partitioner for the sparse Cholesky factorization of any general sparse matrix <ref> [73, 74, 75] </ref>, which partitions the symbolic factor into a hybrid mixture of sparse columns, dense triangles and dense rectangles. Tuning knobs in the form of parameters are provided in order that the partitioner may be ported across problems with widely ranging sparsity structures and across architectures with different granu-larities. <p> Using some of the principles proposed in that paper, we have developed a parallel partitioner for sparse Cholesky factorization on message passing multiprocessor systems, first described in <ref> [74] </ref>. The partitioning is a mix of dense blocks and sparse columns, and allows for parametric control to make the partitioning sensitive to both the matrix structure and the machine granularity. <p> With such a hybrid partitioning method, higher performance and scalability is achievable, provided a right set of parameters is used in partitioning the 63 sparse matrix <ref> [74, 72] </ref>. In [75], we examined, in detail, the effect of two partitioning parameters on the computation speeds, communication costs, extent of processor idling because of load imbalances, and bookkeeping overheads during the parallel numerical factorization using a sparse block code on an iPSC/860 machine.
Reference: [75] <author> S. Venugopal and V. K. Naik. </author> <title> Towards understanding block partitioning for sparse Cholesky factorization. </title> <type> Technical Report RC 18666, </type> <institution> IBM, </institution> <year> 1993. </year> <note> A condensed version appeared in the Proceedings of the 7th Int'l. Parallel Processing Symposium, </note> <year> 1993, </year> <pages> pp. 792-796. </pages>
Reference-contexts: It is described in Chapter 3, and its construction for block-based sparse Cholesky factorization is described in Chapter 5. Parallel partitioner for block-based sparse Cholesky factorization: We have developed a parallel partitioner for the sparse Cholesky factorization of any general sparse matrix <ref> [73, 74, 75] </ref>, which partitions the symbolic factor into a hybrid mixture of sparse columns, dense triangles and dense rectangles. Tuning knobs in the form of parameters are provided in order that the partitioner may be ported across problems with widely ranging sparsity structures and across architectures with different granu-larities. <p> A detailed analysis of blocking for hierarchical memory is done in [59]. We use the BLOCC scheme as the basis for our partitioning, but enhance it considerably to satisfy the requirements pointed out by the preceding discussion. In <ref> [75] </ref>, experimental results from iPSC/860 are presented comparing the performance of a column-based method with that of our 2-D block-based method for distributed sparse Cholesky factorization. The results indicate that 2-D block partitioning reduces the total number of messages and the total volume of data transferred among processors. <p> With such a hybrid partitioning method, higher performance and scalability is achievable, provided a right set of parameters is used in partitioning the 63 sparse matrix [74, 72]. In <ref> [75] </ref>, we examined, in detail, the effect of two partitioning parameters on the computation speeds, communication costs, extent of processor idling because of load imbalances, and bookkeeping overheads during the parallel numerical factorization using a sparse block code on an iPSC/860 machine. <p> In this section we show how these parameters can be selected based on knowing how they affect the different cost components of the parallel time. The results presented here are extracted from experiments we conducted on an earlier version of our numerical factorization code <ref> [75] </ref>. The differences of note from the current version are (a) the task computational cost in the earlier version was higher for the RRR type tasks, and (b) the communication was not optimized in the earlier version.
Reference: [76] <author> S. Venugopal, V. K. Naik, and J. Saltz. </author> <title> Performance of distributed sparse Cholesky factorization with pre-scheduling. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 52-61, </pages> <year> 1992. </year>
Reference-contexts: These primitives also alleviate the programming effort on the part of the user, enabling the user to write sequential-looking SPMD code. The methodology is structured in the spirit of the run-time compilation approach pioneered by Saltz [78]. In <ref> [76] </ref>, we showed the effectiveness of this two-phase scheduling approach of pre-pass with execution-time support, including the use of primitives, on column-based sparse Cholesky factorization. The general scheduler and its specific application to block-based sparse Cholesky factorization is discussed in Chapter 6. <p> The primitives GET BLOCK and READY BLOCK provide a communication interface to the user. They use the communication schedule from GET BLOCK to conduct optimized message-passing at execution time. 135 All these primitives have been implemented for the column sparse Cholesky factorization on the iPSC/860 <ref> [76] </ref>. To explain the functionality of the primitives in detail, we first describe the structures used in interface between the program and the primitives. We then describe the functional details of each primitive. The program-primitives interface uses the AVR representation for blocks, described in Section 3.2.1.
Reference: [77] <author> O. Wing and J. W. Huang. </author> <title> A computation model of parallel solution of linear equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:632-638, </volume> <year> 1980. </year>
Reference-contexts: Also, the associated bookkeeping costs tend to be high on general purpose systems. The parallel factorization method presented in <ref> [77] </ref> is based on this model. The granularity of parallel operations is increased if parallelism is extracted at the level of a non-zero element in L. Refer to this as the element-level model of parallelism.
Reference: [78] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 26-30, </pages> <year> 1991. </year>
Reference-contexts: These primitives also alleviate the programming effort on the part of the user, enabling the user to write sequential-looking SPMD code. The methodology is structured in the spirit of the run-time compilation approach pioneered by Saltz <ref> [78] </ref>. In [76], we showed the effectiveness of this two-phase scheduling approach of pre-pass with execution-time support, including the use of primitives, on column-based sparse Cholesky factorization. The general scheduler and its specific application to block-based sparse Cholesky factorization is discussed in Chapter 6. <p> The preprocessing step or inspector constructs communication schedules which are then realized as communication messages by the executor which uses these schedules during loop execution. Both the inspector and executor are supported by a library of run-time message-passing procedures called Parti [13]. The language ARF (ARguably Fortran) <ref> [78] </ref> has been designed by taking Fortran 77 and adding extensions to serve as an interface between application programs and Parti primitives. 17 Unstructured sparse matrix computations are characterized by indirect array referencing.
Reference: [79] <author> M.Y. Wu and D. Gajski. </author> <title> A programming aid for hypercube architectures. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 349-372, </pages> <year> 1988. </year>
Reference-contexts: We use the macro-dataflow model of task execution. In this model, a task starts executing only when all its input data are available, and executes to completion without interruption. This model has previously been used by Sarkar [64], Wu and Gajski <ref> [79] </ref>, Lewis [19], and Yang and Gerasoulis [30]. The order of execution of tasks is determined only by the precedence relationships among them and the availability of data on which they operate, without any kind of synchronization constraints.
Reference: [80] <author> T. Yang. </author> <title> Scheduling and Code Generation for Parallel Architectures. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1992. </year> <note> Technical report DCS-TR-. </note>
Reference-contexts: The allocation and scheduling pre-pass are the additional preprocessing steps after partitioning. Representation for block-based unstructured computations: We have introduced a novel explicit representation called the Block Computation Representation, or BCR for short. We examined two other representations, the Directed Acyclic Graph (DAG) <ref> [80] </ref> and the Minimally Constrained Task Graph (MCTG) for fine-grain Gaus sian elimination [62], before devising the BCR and tailoring it for the class of block-based unstructured computations using the owner computes rule. <p> We study the issue of representing an unstructured computation in which data has been partitioned into blocks, and the owner computes rule is assumed. In this study, we examine two other representations, the Directed Acyclic Graph (DAG) <ref> [80] </ref> and the Minimally Constrained Task Graph (MCTG) for fine-grain Gaussian elimination [62], neither of which assume the owner computes rule. <p> The weight of an edge is a function of the amount of data flowing from the output of the source task to the input of the destination task. For a formal definition of a DAG, see 23 Yang <ref> [80] </ref>. There is one primary limitation in the DAG representation for algorithms in which each task is an update to some data block, and where the updates to a data block may take place in any relative order.
Reference: [81] <author> E. Zmijewski. </author> <title> Sparse Cholesky Factorization on a Multiprocessor. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithacs, NY, </address> <year> 1987. </year> <type> Technical report 87-856. </type>
Reference-contexts: But for large problems, manual techniques have to be confined to simple partitioning and scheduling schemes. For Cholesky factorization, much work has been done in parallelization based on simple column partitioning <ref> [5, 11, 23, 24, 43, 81] </ref>. In [55], a study was conducted to explain the poor performance of the then existing column-based sparse Cholesky factorization algorithms on message-passing systems. <p> The primary aim is to illustrate the application of the methodology, so the algorithms we propose are not necessarily competitive with other existing algorithms for the solution of sparse triangular systems <ref> [81, 24, 69, 41] </ref>. The notations used in this chapter for the block partitions of L are defined in Chapter 5.
Reference: [82] <author> E. Zmijewski and J. R. Gilbert. </author> <title> A parallel algorithm for large sparse symbolic and numeric Cholesky factorization on a multiprocessor. </title> <type> Technical Report 86-733, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1986. </year> <month> 237 </month>
Reference-contexts: We shall refer to such data distribution schemes as column-based partitioning methods. Most commonly used and studied parallel sparse factorization methods are based on this model <ref> [11, 23, 43, 82, 24, 4] </ref>. This is so because of its relative simplicity in analysis and the close resemblance of the data structures with the algebraic representation.
References-found: 82

