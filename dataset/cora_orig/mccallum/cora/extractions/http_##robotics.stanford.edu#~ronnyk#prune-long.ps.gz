URL: http://robotics.stanford.edu/~ronnyk/prune-long.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fjbradfor,brodleyg@ecn.purdue.edu  fclayk,ronnyk,brunkg@engr.sgi.com  
Phone: 2  
Author: Jeffrey P. Bradford Clayton Kunz Ron Kohavi Cliff Brunk Carla E. Brodley 
Keyword: Pruning Decision Trees with Misclassification Costs  
Address: West Lafayette, IN 47907  2011 N. Shoreline Blvd. Mountain View, CA 94043  
Affiliation: Purdue University  Data Mining and Visualization Silicon Graphics, Inc.  
Note: 08-FEB-1998  
Abstract: A short version of this paper appeared in ECML-98 Abstract. We describe an experimental study of pruning methods for decision tree classifiers in two learning situations: minimizing loss and probability estimation. In addition to the two most common methods for error minimization, CART's cost-complexity pruning and C4.5's error-based pruning, we study the extension of cost-complexity pruning to loss and two pruning variants based on Laplace corrections. We perform an empirical comparison of these methods and evaluate them with respect to the following three criteria: loss, mean-squared-error (MSE), and log-loss. We provide a bias-variance decomposition of the MSE to show how pruning affects the bias and variance. We found that applying the Laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods, both for loss minimization and for estimating probabilities. Unlike in error minimization, and somewhat surprisingly, performing no pruning led to results that were on par with other methods in terms of the evaluation criteria. The main advantage of pruning was in the reduction of the decision tree size, sometimes by a factor of 10. While no method dominated others on all datasets, even for the same domain different pruning mechanisms are better for different loss matrices. We show this last result using Receiver Operating Characteristics (ROC) curves. as a research note
Abstract-found: 1
Intro-found: 1
Reference: <author> Bernardo, J. M. & Smith, A. F. </author> <year> (1993), </year> <title> Bayesian Theory, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: A different measure of probability estimates is log-loss, which is sometimes claimed to be a natural measure of the goodness of probability estimates <ref> (Bernardo & Smith 1993, Mitchell 1997) </ref>.
Reference: <author> Berry, M. J. A. & Linoff, G. </author> <year> (1997), </year> <title> Data Mining Techniques: For Marketing, Sales, and Customer Support, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: A probability distribution may be used to adjust the prediction to minimize the expected loss or to supply a confidence level associated with the prediction; in addition, a probability distribution may also be used to generate a lift curve <ref> (Berry & Linoff 1997) </ref>. Pruning for loss minimization or for probability estimation can lead to different pruning behavior than does pruning for error minimization. <p> tree has several ad 4 vantages: it can give a confidence level for its predictions; it can be used with different loss matrices, computing the best label for each instance using the probability distribution and the loss matrix at hand; and it can be used to generate a lift curve <ref> (Berry & Linoff 1997) </ref>. The KL-pruning that we introduce prunes only if the distribution of a node and its children are similar. Specifically, the method is based on the Kullback-Leibler (KL) distance (Cover & Thomas 1991) between the parent distribution and its children.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: To combat this overfitting problem, the tree is pruned back with the goal of identifying the tree with the lowest error rate on previously unobserved instances, breaking ties in favor of smaller trees <ref> (Breiman, Friedman, Olshen & Stone 1984, Quinlan 1993) </ref>. <p> Several pruning methods have been introduced in the literature, including cost-complexity pruning <ref> (Breiman et al. 1984) </ref>, reduced error pruning and pessimistic pruning (Quinlan 1987), error-based pruning (Quinlan 1993), penalty pruning (Mansour 1997), and MDL pruning (Quinlan & Rivest 1989, Mehta, Rissanen & Agrawal 1995, Wallace & Patrick 1993). <p> In this paper, we investigate the behavior of several pruning algorithms. In addition to the two most common methods for error minimization, cost-complexity pruning <ref> (Breiman et al. 1984) </ref> and error-based pruning (Quinlan 1993), we study the extension of cost-complexity pruning to loss and two pruning variants based on Laplace corrections (Cestnik 1990, Good 1965). <p> Such trees are sometimes called class probability trees <ref> (Breiman et al. 1984) </ref>. Several methods have been proposed to predict class distributions, including frequency counts, Laplace corrections, and smoothing (Breiman et al. 1984, Buntine 1992, Oliver & Hand 1995). In our experiments, we use the former two methods. <p> Such trees are sometimes called class probability trees (Breiman et al. 1984). Several methods have been proposed to predict class distributions, including frequency counts, Laplace corrections, and smoothing <ref> (Breiman et al. 1984, Buntine 1992, Oliver & Hand 1995) </ref>. In our experiments, we use the former two methods. The frequency-counts method simply predicts a distribution based on the counts at the leaf the test instance falls into. <p> The crux of the problem is to find an honest estimate of error <ref> (Breiman et al. 1984) </ref>, which is defined as one that is not overly optimistic for a tree that was built to minimize errors in the first place. <p> The two most commonly used pruning algorithms for error minimization are error-based pruning (Quinlan 1993) and cost-complexity pruning <ref> (Breiman et al. 1984) </ref>. The error-based pruning algorithm used in C4.5 estimates the error of a leaf by computing a statistical confidence interval of the resubstitution error (error on the training set for the leaf) assuming an independent binomial model and selecting the upper bound of the confidence interval. <p> In many practical applications, it is important not only to classify each instance correctly or to minimize loss, but to also give a probability distribution on the classes. To measure the error between the true probability distribution and the predicted distribution, the mean-squared error (MSE) can be used <ref> (Breiman et al. 1984, Definition 4.18) </ref>.
Reference: <author> Buntine, W. </author> <year> (1992), </year> <title> `Learning classification trees', </title> <journal> Statistics and Computing 2(2), </journal> <pages> 63-73. </pages>
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <editor> in L. C. Aiello, ed., </editor> <booktitle> `Proceedings of the ninth European Conference on Artificial Intelligence', </booktitle> <pages> pp. 147-149. </pages>
Reference-contexts: In addition to the two most common methods for error minimization, cost-complexity pruning (Breiman et al. 1984) and error-based pruning (Quinlan 1993), we study the extension of cost-complexity pruning to loss and two pruning variants based on Laplace corrections <ref> (Cestnik 1990, Good 1965) </ref>. We perform an empirical comparison of these methods and evaluate them with respect to the following criteria: loss under two matrices, average mean-squared-error (MSE), and average log-loss.
Reference: <author> Cover, T. M. & Thomas, J. A. </author> <year> (1991), </year> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The KL-pruning that we introduce prunes only if the distribution of a node and its children are similar. Specifically, the method is based on the Kullback-Leibler (KL) distance <ref> (Cover & Thomas 1991) </ref> between the parent distribution and its children. For each node, we estimate the class distribution using the Laplace correction detailed in Section 2.1.
Reference: <author> Danyluk, A. & Provost, F. </author> <year> (1993), </year> <title> Small disjuncts in action: Learning to diagnose errors in the telephone network local loop, </title> <editor> in P. Utgoff, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Tenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 81-88. </pages>
Reference: <author> Draper, B. A., Brodley, C. E. & Utgoff, P. E. </author> <year> (1994), </year> <title> `Goal-directed classification using linear machine decision trees', </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 16(9), </journal> <pages> 888-893. </pages>
Reference-contexts: This result differs from error minimization, where pruning was consistently shown to help. Pruning based on loss matrices performed better than pruning based on error for frequency counts for all methods. This result (for frequency counts) has been observed previously for reduced error/cost pruning <ref> (Draper, Brodley & Utgoff 1994) </ref>. When the Laplace correction was used, pruning with loss matrices performed better than error-based pruning (eb-lc) for the 100:1 (ccp-lc, lp) but there was no significant difference for the 10:1 loss matrix.
Reference: <author> Esposito, F., Malerba, D. & Semeraro, G. </author> <year> (1995a), </year> <title> A further study of pruning methods in decision tree induction, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> `Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics', </booktitle> <pages> pp. 211-218. </pages>
Reference: <author> Esposito, F., Malerba, D. & Semeraro, G. </author> <year> (1995b), </year> <title> Simplifying decision trees by pruning and grafting: New results, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> `Machine Learning: ECML-95 (Proc. European Conf. on Machine Learning, 1995)', Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 287-290. </pages>
Reference: <author> Fawcett, T. & Provost, F. </author> <year> (1996), </year> <title> Combining data mining and machine learning for effective user profiling, </title> <booktitle> in `Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 8-13. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference-contexts: Only in a few cases did Laplace correction lead to a higher MSE than frequency counts. To provide a deeper understanding of the MSE results, we ran a set of experiments using the bias-variance decomposition of the MSE <ref> (Geman, Bienen-stock & Doursat 1992) </ref>. The bias-variance decomposition is a tool for analyzing learning scenarios that have a quadratic loss function.
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: The Laplace correction method biases the probability towards a uniform distribution. Specifically, if a node has m instances, c of which are from a given class, in a k-class problem, the probability assigned to the class is (c + 1)=(m + k) <ref> (Good 1965, Cestnik 1990) </ref>. Given a probability distribution and a loss matrix, it is simple to compute the class with the expected minimal loss by multiplying the probability distribution vector by the loss matrix.
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1996), </year> <title> Data mining using MLC ++ : A machine learning library in C ++ , in `Tools with Artificial Intelligence', </title> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 234-245. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: The MSE is therefore bounded between zero and two <ref> (Kohavi & Wolpert 1996) </ref>, so in this paper we use half the MSE as a "normalized MSE" in so that it is a number between 0% and 100%. <p> The basic decision tree growing algorithm is implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1996) </ref> and called MC4 (MLC ++ C4.5). It is a Top-Down Decision Tree (TDDT) induction algorithm very similar to C4.5.
Reference: <author> Kohavi, R. & Wolpert, D. H. </author> <year> (1996), </year> <title> Bias plus variance decomposition for zero-one loss functions, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 275-283. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: The MSE is therefore bounded between zero and two <ref> (Kohavi & Wolpert 1996) </ref>, so in this paper we use half the MSE as a "normalized MSE" in so that it is a number between 0% and 100%. <p> The basic decision tree growing algorithm is implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1996) </ref> and called MC4 (MLC ++ C4.5). It is a Top-Down Decision Tree (TDDT) induction algorithm very similar to C4.5.
Reference: <author> Kubat, M., Holte, R. & Matwin, S. </author> <year> (1997), </year> <title> Learning when negative examples abound, </title> <booktitle> in `The 9th European Conference on Machine Learning, Poster Papers', </booktitle> <pages> pp. 146-153. </pages>
Reference: <author> Mansour, Y. </author> <year> (1997), </year> <title> Pessimistic decision tree pruning based on tree size, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Several pruning methods have been introduced in the literature, including cost-complexity pruning (Breiman et al. 1984), reduced error pruning and pessimistic pruning (Quinlan 1987), error-based pruning (Quinlan 1993), penalty pruning <ref> (Mansour 1997) </ref>, and MDL pruning (Quinlan & Rivest 1989, Mehta, Rissanen & Agrawal 1995, Wallace & Patrick 1993). Esposito, Malerba & Se-meraro (1995a, 1995b) have compared several of these pruning algorithms for error minimization.
Reference: <author> Mehta, M., Rissanen, J. & Agrawal, R. </author> <year> (1995), </year> <title> MDL-based decision tree pruning, </title> <editor> in U. M. Fayyad & R. Uthurusamy, eds, </editor> <booktitle> `Proceedings of the first international conference on knowledge discovery and data mining', </booktitle> <publisher> AAAI Press, </publisher> <pages> pp. 216-221. </pages>
Reference: <author> Merz, C. J. & Murphy, P. M. </author> <year> (1997), </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: We therefore decided to require at least 500 instances and train on only 25% of the data, leaving the remaining instances for testing. Ten datasets, shown in Table 1 with their characteristics, were chosen from the UCI repository <ref> (Merz & Murphy 1997) </ref>. For all files we trained on 25% of the data and tested on 75% of the data, repeating the process 10 times.
Reference: <author> Mitchell, T. M. </author> <year> (1997), </year> <title> Machine Learning, </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Oates, T. & Jensen, D. </author> <year> (1997), </year> <title> The effects of training set size on decision tree complexity, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 254-262. </pages> <note> 13 Oliver, </note> <author> J. & Hand, D. </author> <year> (1995), </year> <title> On pruning and averaging decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 430-437. </pages>
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T. & Brunk, C. </author> <year> (1994), </year> <title> Reducing misclassification costs, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Provost, F. & Fawcett, T. </author> <year> (1997), </year> <title> Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions, </title> <editor> in D. Hecker-man, H. Mannila, D. Pregibon & R. Uthurusamy, eds, </editor> <booktitle> `Proceedings of the third international conference on Knowledge Discovery and Data Mining', </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: While no method dominated others on all datasets, even for the same domain different pruning mechanisms are better for different loss matrices. We show this last result using Receiver Operating Characteristics (ROC) curves <ref> (Provost & Fawcett 1997) </ref>. 2 The Pruning Algorithms and Evaluation Criteria 2.1 Probability Estimation and Loss Minimization at the Leaves A decision tree can be used to estimate a probability distribution on the label values rather than to make a single prediction. <p> The algorithms had the following average log-losses: ccp (0.400), eb (0.411), kl (0.417), lp (0.419), np (0.429). 3.4 ROC Curves The Receiver Operating Characteristic curves provide a way of showing how false positive predictions increase as true positive predictions increase <ref> (Provost & Fawcett 1997) </ref>. The curves are generated by varying the loss matrix (in our case from a ratio of 20 to 1 to a ratio of 1 to 20) and plotting the number of false and true positive identifications of the goal class for the test-set.
Reference: <author> Quinlan, J. R. </author> <year> (1987), </year> <title> `Simplifying decision trees', </title> <journal> International Journal of Man-Machine Studies 27, </journal> <pages> 221-234. </pages>
Reference-contexts: Several pruning methods have been introduced in the literature, including cost-complexity pruning (Breiman et al. 1984), reduced error pruning and pessimistic pruning <ref> (Quinlan 1987) </ref>, error-based pruning (Quinlan 1993), penalty pruning (Mansour 1997), and MDL pruning (Quinlan & Rivest 1989, Mehta, Rissanen & Agrawal 1995, Wallace & Patrick 1993). Esposito, Malerba & Se-meraro (1995a, 1995b) have compared several of these pruning algorithms for error minimization.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Several pruning methods have been introduced in the literature, including cost-complexity pruning (Breiman et al. 1984), reduced error pruning and pessimistic pruning (Quinlan 1987), error-based pruning <ref> (Quinlan 1993) </ref>, penalty pruning (Mansour 1997), and MDL pruning (Quinlan & Rivest 1989, Mehta, Rissanen & Agrawal 1995, Wallace & Patrick 1993). Esposito, Malerba & Se-meraro (1995a, 1995b) have compared several of these pruning algorithms for error minimization. <p> In this paper, we investigate the behavior of several pruning algorithms. In addition to the two most common methods for error minimization, cost-complexity pruning (Breiman et al. 1984) and error-based pruning <ref> (Quinlan 1993) </ref>, we study the extension of cost-complexity pruning to loss and two pruning variants based on Laplace corrections (Cestnik 1990, Good 1965). We perform an empirical comparison of these methods and evaluate them with respect to the following criteria: loss under two matrices, average mean-squared-error (MSE), and average log-loss. <p> The resubstitution error (error rate on the training set) does not provide a suitable estimate because a leaf-node replacing a subtree will never have fewer errors on the training set than the subtree. The two most commonly used pruning algorithms for error minimization are error-based pruning <ref> (Quinlan 1993) </ref> and cost-complexity pruning (Breiman et al. 1984).
Reference: <author> Quinlan, J. R. & Rivest, R. L. </author> <year> (1989), </year> <title> `Inferring decision trees using the minimum description length principle', </title> <booktitle> Information and Computation 80, </booktitle> <pages> 227-248. </pages>
Reference: <author> Turney, P. </author> <year> (1997), </year> <note> Cost-sensitive learning. http://ai.iit.nrc.ca/bibliographies/cost-sensitive.html. </note>
Reference: <author> Wallace, C. & Patrick, J. </author> <year> (1993), </year> <title> `Coding decision trees', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 7-22. 14 </pages>
References-found: 28

