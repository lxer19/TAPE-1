URL: http://www.cs.indiana.edu/hyplan/shockema/cdg.pac.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/shockema/resume.html
Root-URL: http://www.cs.indiana.edu
Title: PAC Learning Constraint Dependency Grammar Constraints  
Author: Mary P. Harper, Christopher M. White, Stephen A. Hockema, and Randall A. Helzerman 
Address: Building  West Lafayette, IN 47907-1285  
Affiliation: 1285 Electrical Engineering  School of Electrical and Computer Engineering Purdue University  
Abstract: Constraint Dependency Grammar (CDG) [11, 13] is a constraint-based grammatical formalism that has proven effective for processing English [5] and improving the accuracy of spoken language understanding systems [4]. However, prospective users of CDG face a steep learning curve when trying to master this powerful formalism. Therefore, a recent trend in CDG research has been to try to ease the burden of grammar writers by developing methods for automatically learning CDG grammars from annotated sentences [22, 23]. In this paper, we prove that CDG grammar constraints are PAC learnable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Aizenstein and L. Pitt. </author> <title> On the learnability of disjunctive normal form formulas. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 183-208, </pages> <year> 1995. </year>
Reference-contexts: Since there are no negated terms in a monotone DNF formula that is to be learned, it is a simple matter to collect together all of the prime implicants that are required to represent the reduced DNF. Below is the algorithm for learning monotone DNF from Aizenstein and Pitt <ref> [1] </ref> (adapted from Valiant [21]): LEARN-MONOTONE-DNF () 1. h ; 2. a a randomly generated labeled example 3. if a is a positive example then 4. for i 1 to n 5. if membership (a with v i set to 0)=true then 6. a a with v i set to 0 <p> Hence it is possible to greedily collect the prime implicants for the target formula when it is monotone. However, if f is a reduced non-monotone DNF formula, iterated consensus can derive additional prime implicants not in f . In fact Aizenstein and Pitt <ref> [1] </ref> constructed a formula f that creates an exponential number of prime implicants by the process of iterated consensus. On the other hand, if the target DNF formula has distance 2 or greater, then its terms cannot undergo consensus. Because of this, Aizenstein and Pitt [1] prove that distance-2 DNF formulas <p> In fact Aizenstein and Pitt <ref> [1] </ref> constructed a formula f that creates an exponential number of prime implicants by the process of iterated consensus. On the other hand, if the target DNF formula has distance 2 or greater, then its terms cannot undergo consensus. Because of this, Aizenstein and Pitt [1] prove that distance-2 DNF formulas over a finite set of variables can be learned using the exact learning approach in O (jf j 2 n) time, where jf j is the size of the reduced target formula and n are the number of variables that can appear in a term. <p> Hence we can use the algorithm to learn DNF developed by Valiant [21] (or equiv alently by Aizenstein and Pitt <ref> [1] </ref>) to learn ARVs and ARVPs in polynomial time. 4.4 Mapping ARVs to DNF In this section we will map the the set of positive ARVs to a boolean formula that will evaluate to true when a role value is allowed by a positive ARV and evaluate to false otherwise.
Reference: [2] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75(2) </volume> <pages> 87-106, </pages> <year> 1987. </year> <month> 23 </month>
Reference-contexts: This model requires that the concept c be learned exactly, using equivalence queries to ensure that the concept is successfully learned. If the current concept is equivalent to the target concept, then the learning algorithm is complete; otherwise, an oracle must respond with a counterexample. Angluin <ref> [2] </ref> has shown that any concept learnable in the exact model is learnable in the PAC model. Because the question of the equivalence of non-regular grammars is not in general decideable, we have chosen to use the PAC-learning model, as recommended by Angluin [2]. <p> Angluin <ref> [2] </ref> has shown that any concept learnable in the exact model is learnable in the PAC model. Because the question of the equivalence of non-regular grammars is not in general decideable, we have chosen to use the PAC-learning model, as recommended by Angluin [2]. We will demonstrate that ARVs and ARVPs can be mapped to a specific type of disjunctive normal form that is PAC learnable.
Reference: [3] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: In the PAC model, so long as the distribution D is the same during learning and testing, the algorithm should give a good result, regardless of the distribution type. Another alternative is the exact learning model developed by Angluin <ref> [3] </ref>. This model requires that the concept c be learned exactly, using equivalence queries to ensure that the concept is successfully learned. If the current concept is equivalent to the target concept, then the learning algorithm is complete; otherwise, an oracle must respond with a counterexample.
Reference: [4] <author> M. P. Harper and R. A. Helzerman. </author> <title> Extensions to constraint dependency parsing for spoken language processing. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9(3) </volume> <pages> 187-234, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Constraint Dependency Grammar (CDG) [11, 13] is a constraint-based grammatical formalism that has proven effective for processing English [5] and improving the accuracy of spoken language understanding systems <ref> [4] </ref>. However, prospective users of CDG face a steep learning curve when trying to fl This material is based upon work supported by a grant from the Intel Research Council and the National Science Foundation under Grant No. IRI-9704358. <p> This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees. <p> Positive examples from the ARV-a space can be handled similarly. 4.6 Adding Features to CDG and its Impact on Learn ability Harper and Helzerman modified CDG to accommodate parsing sentences containing words that are lexically ambiguous <ref> [5, 4] </ref> by, in effect, creating a role value with each lexical category. A word can also have ambiguity in its associated feature information, like number, person, or case. For example, the noun fish can take a number/person value of third person singular or third person plural. <p> For example, the noun fish can take a number/person value of third person singular or third person plural. Often feature information is useful for disambiguating parses for sentences, so features were added to CDG in <ref> [4] </ref>. In order to correctly enforce feature value constraints, each role value must also be assigned a single feature value for each feature type.
Reference: [5] <author> M. P. Harper and R. A. Helzerman. </author> <title> Managing multiple knowledge sources in constraint-based parsing of spoken language. </title> <journal> Fundamenta Informati-cae, </journal> <volume> 23(2,3,4):303-353, </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction Constraint Dependency Grammar (CDG) [11, 13] is a constraint-based grammatical formalism that has proven effective for processing English <ref> [5] </ref> and improving the accuracy of spoken language understanding systems [4]. However, prospective users of CDG face a steep learning curve when trying to fl This material is based upon work supported by a grant from the Intel Research Council and the National Science Foundation under Grant No. IRI-9704358. <p> This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees. <p> Positive examples from the ARV-a space can be handled similarly. 4.6 Adding Features to CDG and its Impact on Learn ability Harper and Helzerman modified CDG to accommodate parsing sentences containing words that are lexically ambiguous <ref> [5, 4] </ref> by, in effect, creating a role value with each lexical category. A word can also have ambiguity in its associated feature information, like number, person, or case. For example, the noun fish can take a number/person value of third person singular or third person plural.
Reference: [6] <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to uniform distribution. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations for Computer Science, </booktitle> <pages> pages 42-53, </pages> <year> 1994. </year>
Reference-contexts: We chose the PAC-learning model for demonstrating that ARVPs are poly-nomially learnable because the PAC model is a distribution-free approach (in contrast to more restricted approaches <ref> [6, 7, 8, 9] </ref> that make specific assumptions about the distributions of input examples). In the PAC model, so long as the distribution D is the same during learning and testing, the algorithm should give a good result, regardless of the distribution type.
Reference: [7] <author> J. Jackson. </author> <title> The Harmonic Sieve: A Novel Application of Fourier Analysis to Machine Learning Theory and Practice. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: We chose the PAC-learning model for demonstrating that ARVPs are poly-nomially learnable because the PAC model is a distribution-free approach (in contrast to more restricted approaches <ref> [6, 7, 8, 9] </ref> that make specific assumptions about the distributions of input examples). In the PAC model, so long as the distribution D is the same during learning and testing, the algorithm should give a good result, regardless of the distribution type.
Reference: [8] <author> R. Khardon. </author> <title> On using the Fourier transform to learn disjoint DNF. </title> <journal> Information Processing Letters, </journal> <volume> 49(5) </volume> <pages> 219-222, </pages> <year> 1994. </year>
Reference-contexts: We chose the PAC-learning model for demonstrating that ARVPs are poly-nomially learnable because the PAC model is a distribution-free approach (in contrast to more restricted approaches <ref> [6, 7, 8, 9] </ref> that make specific assumptions about the distributions of input examples). In the PAC model, so long as the distribution D is the same during learning and testing, the algorithm should give a good result, regardless of the distribution type.
Reference: [9] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <journal> SIAM Journal of Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year>
Reference-contexts: We chose the PAC-learning model for demonstrating that ARVPs are poly-nomially learnable because the PAC model is a distribution-free approach (in contrast to more restricted approaches <ref> [6, 7, 8, 9] </ref> that make specific assumptions about the distributions of input examples). In the PAC model, so long as the distribution D is the same during learning and testing, the algorithm should give a good result, regardless of the distribution type.
Reference: [10] <author> A.K. Mackworth. </author> <title> Consistency in networks of relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 99-118, </pages> <year> 1977. </year>
Reference-contexts: Step 5: Enforcing Arc Consistency. Finally, arc consistency (or filtering) is enforced in order to eliminate role values that are inconsistent with all of the role values that can be assigned to another role in the parse of a sentence <ref> [10, 16, 17] </ref>. In this case, the role value is incompatible with all sentence parses and can therefore be deleted. The fourth constraint network in Figure 1 demonstrates the effect of enforcing arc consistency. Preparation for binary constraints, binary constraint propagation, and arc consistency can be merged together during parsing.
Reference: [11] <author> H. Maruyama. </author> <title> Constraint dependency grammar. </title> <type> Technical Report #RT0044, </type> <institution> IBM, </institution> <address> Tokyo, Japan, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Constraint Dependency Grammar (CDG) <ref> [11, 13] </ref> is a constraint-based grammatical formalism that has proven effective for processing English [5] and improving the accuracy of spoken language understanding systems [4]. <p> This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> finally abstract role value tuples are shown to be PAC-learnable. 2 CDG and Parsing In this section, we will first define CDG and then give an example of a simple CDG that will be used to illustrate the parsing algorithm. 2.1 CDG Definitions Constraint Dependency Grammar (CDG), introduced by Maruyama <ref> [11, 12, 13] </ref>, uses constraints rather than production rules for parsing. The parsing algorithm is framed as a constraint satisfaction problem; the parsing rules are the constraints and the solutions are the parses. <p> The constants allowed in C include elements and subsets of [ R [ L. Maruyama <ref> [11] </ref> originally allowed C to also contain as constants the numbers corresponding to the position of a word; however, exact position information is not required, nor is it very useful for describing legal grammatical relations in a grammar 1 . <p> A CDG also has a degree parameter, which is the number of roles in the grammar. The set of languages accepted by a CDG grammar is a superset of the set of languages that can be accepted by context-free grammars. In fact, Maruyama <ref> [11, 12] </ref> proves that any arbitrary CFG converted to Griebach Normal form can be converted into a CDG grammar with a degree of two and an arity of two that accepts the same language as the CFG. <p> addition, CDG can accept languages that CFGs cannot, for example, a n b n c n (where a, b, and c are terminal symbols) and ww (where w is some string of terminal symbols). 1 The only reference to a position in a sentence found in any published CDG grammar <ref> [11] </ref> is to the word in the first position of a sentence. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees.
Reference: [12] <author> H. Maruyama. </author> <title> Constraint dependency grammar and its weak generative capacity. </title> <booktitle> Computer Software, </booktitle> <year> 1990. </year>
Reference-contexts: This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> finally abstract role value tuples are shown to be PAC-learnable. 2 CDG and Parsing In this section, we will first define CDG and then give an example of a simple CDG that will be used to illustrate the parsing algorithm. 2.1 CDG Definitions Constraint Dependency Grammar (CDG), introduced by Maruyama <ref> [11, 12, 13] </ref>, uses constraints rather than production rules for parsing. The parsing algorithm is framed as a constraint satisfaction problem; the parsing rules are the constraints and the solutions are the parses. <p> A CDG also has a degree parameter, which is the number of roles in the grammar. The set of languages accepted by a CDG grammar is a superset of the set of languages that can be accepted by context-free grammars. In fact, Maruyama <ref> [11, 12] </ref> proves that any arbitrary CFG converted to Griebach Normal form can be converted into a CDG grammar with a degree of two and an arity of two that accepts the same language as the CFG.
Reference: [13] <author> H. Maruyama. </author> <title> Structural disambiguation with constraint propagation. </title> <booktitle> In The Proceedings of the Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 31-38, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Constraint Dependency Grammar (CDG) <ref> [11, 13] </ref> is a constraint-based grammatical formalism that has proven effective for processing English [5] and improving the accuracy of spoken language understanding systems [4]. <p> This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> finally abstract role value tuples are shown to be PAC-learnable. 2 CDG and Parsing In this section, we will first define CDG and then give an example of a simple CDG that will be used to illustrate the parsing algorithm. 2.1 CDG Definitions Constraint Dependency Grammar (CDG), introduced by Maruyama <ref> [11, 12, 13] </ref>, uses constraints rather than production rules for parsing. The parsing algorithm is framed as a constraint satisfaction problem; the parsing rules are the constraints and the solutions are the parses. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees.
Reference: [14] <author> W. Menzel. </author> <title> Parsing of spoken language under time constraints. </title> <booktitle> In 11th European Conference on Artificial Intelligence, </booktitle> <pages> pages 560-564, </pages> <year> 1994. </year> <month> 24 </month>
Reference-contexts: This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees.
Reference: [15] <author> W. Menzel. </author> <title> Robust processing of natural language. </title> <booktitle> In Proceedings of the 19th Annual German Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: This paper first introduces Constraint Dependency Grammar in section 2. This description of CDG is necessarily brief; for additional information on this topic, please refer to papers written by several CDG researchers <ref> [4, 5, 11, 12, 13, 14, 15] </ref>. Section 3 describes the concept of abstract role value tuples as an alternative way of representing CDG constraints. <p> Fortunately, it is possible to construct another view of role values given that constraints in a CDG do not need to use the exact position of a word or a 8 modifiee in the sentence to parse sentences <ref> [4, 5, 11, 13, 14, 15] </ref>; they only need to test the relative positions between role values and their modifiees.
Reference: [16] <author> R. Mohr and T. C. Henderson. </author> <title> Arc and path consistency revisited. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 225-233, </pages> <year> 1986. </year>
Reference-contexts: Step 5: Enforcing Arc Consistency. Finally, arc consistency (or filtering) is enforced in order to eliminate role values that are inconsistent with all of the role values that can be assigned to another role in the parse of a sentence <ref> [10, 16, 17] </ref>. In this case, the role value is incompatible with all sentence parses and can therefore be deleted. The fourth constraint network in Figure 1 demonstrates the effect of enforcing arc consistency. Preparation for binary constraints, binary constraint propagation, and arc consistency can be merged together during parsing.
Reference: [17] <author> U. Montanari. </author> <title> Networks of constraints: Fundamental properties and applications to picture processing. </title> <journal> Information Science, </journal> <volume> 7 </volume> <pages> 95-132, </pages> <year> 1974. </year>
Reference-contexts: Step 5: Enforcing Arc Consistency. Finally, arc consistency (or filtering) is enforced in order to eliminate role values that are inconsistent with all of the role values that can be assigned to another role in the parse of a sentence <ref> [10, 16, 17] </ref>. In this case, the role value is incompatible with all sentence parses and can therefore be deleted. The fourth constraint network in Figure 1 demonstrates the effect of enforcing arc consistency. Preparation for binary constraints, binary constraint propagation, and arc consistency can be merged together during parsing.
Reference: [18] <author> S. Muroga. </author> <title> Logic Design and Switching Theory. </title> <editor> Robert E. </editor> <publisher> Krieger Publishing Company, </publisher> <address> Malabar, Florida, </address> <year> 1990. </year>
Reference-contexts: In iterated consensus, we begin with all of the initial terms of f and add any implicant obtained by consensus of the terms of f to f . Note that every prime implicant of a DNF formula f can be obtained by iterated consensus over the terms of f <ref> [18] </ref>. For two terms, t 1 and t 2 , distance (t 1 , t 2 ) is the number of distinct variables in the terms such that each variable appears negated in one term and 15 unnegated in the other.
Reference: [19] <author> Y. Schabes, A. Abeille, and A. K. Joshi. </author> <title> Parsing strategies with `lexical-ized' grammars: Application to Tree Adjoining Grammars. </title> <booktitle> In Proceedings of the International Conference on Computational Linguistics, </booktitle> <pages> pages 578-583, </pages> <year> 1988. </year>
Reference-contexts: Another benefit of the ARV representation is that it provides a convenient way to more completely lexicalize CDG. A grammar is lexicalized <ref> [20, 19] </ref> if it consists of: (1) a finite set of elementary structures, each of which is associated with a lexical item; (2) a finite set of lexical items, each associated with one or more of the structures; and (3) a finite number of operations for combining the structures.
Reference: [20] <author> B. Srinivas. </author> <title> `Almost parsing' technique for language modeling. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1173-1176, </pages> <year> 1996. </year>
Reference-contexts: Another benefit of the ARV representation is that it provides a convenient way to more completely lexicalize CDG. A grammar is lexicalized <ref> [20, 19] </ref> if it consists of: (1) a finite set of elementary structures, each of which is associated with a lexical item; (2) a finite set of lexical items, each associated with one or more of the structures; and (3) a finite number of operations for combining the structures.
Reference: [21] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Hence, constraints with constant arity a can be represented using abstract role values from the ARV-a space, A a . 4 PAC-learning of ARVs and ARVPs 4.1 PAC-learning model The PAC model of learning introduced by Valiant <ref> [21] </ref> captures some intuition about learning and provides a useful abstraction for developing provably correct, efficient algorithms for learning a concept. <p> Any DNF formula with distance 2 or greater cannot undergo consensus. 4.3 PAC-learnability of Monotone DNF and Distance 2 DNF Although learnability of arbitrary DNF formulas is an open problem, there are polynomial time algorithms for learning certain restricted subclasses of DNF. For example, Valiant <ref> [21] </ref> introduced a polynomial algorithm for learning monotone DNF, that is DNF with no negated literals in any term. This algorithm learns by collecting all of the prime implicants of the unknown formula from positive examples provided at random. <p> Below is the algorithm for learning monotone DNF from Aizenstein and Pitt [1] (adapted from Valiant <ref> [21] </ref>): LEARN-MONOTONE-DNF () 1. h ; 2. a a randomly generated labeled example 3. if a is a positive example then 4. for i 1 to n 5. if membership (a with v i set to 0)=true then 6. a a with v i set to 0 7. t variables (a) <p> If d is the number of terms in the reduced target DNF concept and t is the number of variables, then the algorithm requires L ( 1 ffi ; d) 2 h (d+log e ffi ) positive examples, and dt membership queries <ref> [21] </ref>. If a formula f is a monotone DNF formula, then consensus is not possible between any two terms of f . Hence it is possible to greedily collect the prime implicants for the target formula when it is monotone. <p> We will demonstrate ARVs can be PAC learned because they can be mapped to a DNF such that all terms associated with positive examples of ARVs are essential prime implicants that cannot undergo consensus. Hence we can use the algorithm to learn DNF developed by Valiant <ref> [21] </ref> (or equiv alently by Aizenstein and Pitt [1]) to learn ARVs and ARVPs in polynomial time. 4.4 Mapping ARVs to DNF In this section we will map the the set of positive ARVs to a boolean formula that will evaluate to true when a role value is allowed by a
Reference: [22] <author> C. M. White. </author> <title> Rapid grammar development and parsing of constraint dependency grammars using abstract role values. </title> <type> Ph.D. Dissertation Proposal, </type> <year> 1998. </year>
Reference-contexts: Therefore, a recent trend in CDG re-search has been to try to ease the burden of grammar writers by developing methods for automatically learning CDG constraints from annotated sentences <ref> [22, 23] </ref>. In order to demonstrate that CDG grammar constraints can be effectively learned in polynomial time, we will prove that CDG constraints are PAC learnable from positive training examples.
Reference: [23] <author> C. M. White, M. P. Harper, T. Lane, and R. A. Helzerman. </author> <title> Inductive learning of abstract role values derived from a constraint dependency grammar. In Proceedings of the Workshop on Automata Induction, Grammatical Inference, and Language Acquisition, </title> <month> July </month> <year> 1997. </year> <note> Available via WWW at http://www.cs.cmu.edu/~pdupont/mlworkshop. html. 25 </note>
Reference-contexts: Therefore, a recent trend in CDG re-search has been to try to ease the burden of grammar writers by developing methods for automatically learning CDG constraints from annotated sentences <ref> [22, 23] </ref>. In order to demonstrate that CDG grammar constraints can be effectively learned in polynomial time, we will prove that CDG constraints are PAC learnable from positive training examples.
References-found: 23

