URL: http://www.cs.berkeley.edu/~chema/papers/NOW-mpeg/now-mpeg.ps
Refering-URL: http://www.cs.berkeley.edu/~chema/papers/NOW-mpeg/
Root-URL: http://www.cs.berkeley.edu
Email: e-mail: fchema,gewekeg@cs.berkeley.edu  
Title: The Design of a Parallel MPEG-2 Encoder  
Author: Jose Mara Gonzalez and Andrew Geweke 
Address: Berkeley, California 94720  
Affiliation: Electrical Engineering and Computer Sciences, Computer Science Division University of California, Berkeley  
Abstract: In this paper we present the motivation for and design of NAME, a software architecture that allows flexible, fast MPEG-2 digital-video encoding using the parallelism of a network of workstations (NOW). MPEG-2 encoding is an extremely processor-intensive task. Solutions to date have been slow, inflexible, or both. We are using the power of a NOW to implement a software-only, parallel MPEG-2 encoder using a flexible, extensible software framework. In particular, we are providing a system capable of a) adapting the workload, via dynamic distribution of jobs, to a system where processing units have variable performance, and b) supporting an environment where complex video-coding research can be carried out in an easy way. We plan to use the resulting experience to bring our ideas and software to further architectures, including a new Pentium SMP-based architecture. We expect that the use of a software architecture to accomplish this encoding will allow for incremental speed increases, and, most importantly, that additional software components will be able to be merged into the architecture to allow for further processing, such as that of digital video effects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. </author> <title> Le Gall "MPEG: A video compression standard for multimedia applications". </title> <journal> Communications of the ACM, April 1991, </journal> <volume> vol. 34, no. 4, </volume> <pages> pp. 46-58. </pages>
Reference: [2] <institution> Advanced Television Systems Committee "A compilation of Advanced Television System Committee Standards". </institution> <month> March </month> <year> 1997. </year>
Reference: [3] <author> K. Gong and L. A. </author> <title> Rowe "Parallel MPEG-1 Video Encoding". Picture Coding Symposium, </title> <address> Septem-ber 1994, Sacramento, CA, </address> <pages> pp. 67-70, 1994-9. </pages>
Reference-contexts: This approach was suggested by <ref> [3] </ref>. This method's influence in the PQR system is mainly in the performance. <p> The main reason for not including it into the first implementation's objectives was the poor results obtained with it on <ref> [3] </ref> (which is justified by the authors by the I/O overheads), and in the porbable CPU-bound nature of the job. 7.2 Allowing macroslice-height selection Even though its influence is very architecture-dependent, the system could allow the user to tune this parameter for best encoding performance, or even integrate it in the
Reference: [4] <author> K. Shen, L. A. Rowe and E. J. </author> <title> Delp "A Parallel Implementation of an MPEG1 Encoder: Faster Than Real-Time!". </title> <booktitle> Proceedings of the SPIE Conference on Digital Video Compression: Algorithms and Technologies, February 1995, </booktitle> <address> San Jose, CA. </address>
Reference: [5] <author> K. Shen and E. J. </author> <title> Delp "A spatial-temporal parallel approach for real-time MPEG video compression". </title> <booktitle> Proceedings of the 25th International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1996, </year> <note> Bloomingdale, Illi-nois, pp. II-100-II-107. </note>
Reference-contexts: Ideally, we would send each byte exactly once. However, as we will see below, achieving this goal would produce long latencies and, perhaps, very "bursty" network traffic. Thus, a compromise must be made. In the MPEG encoding literature ([3], <ref> [5] </ref>), two types of parallelism can be identified: temporal and spatial. By temporal parallelism, we mean that we can send different frames of a sequence to different encoding nodes; by spatial parallelism, we mean that we can send different parts of a single frame to different encoding nodes. <p> The approach is quite similar to the one used in <ref> [5] </ref>; however, we have improved its flexibility to deal with the huge network workload. In the referenced approach, the master distributes GoPs among "task modules", which are groups of processing elements (PE). Afterwards, each task module splits its GoPs among its constituent PEs.
Reference: [6] <author> S. Smoot, </author> <type> personal communication, </type> <month> September </month> <year> 1997. </year>
Reference-contexts: A very promising approach to this problem is the Generalized Rate Control System (GRCS <ref> [6] </ref>). GRCS employs many of these feature detectors in a combined predictor that sets the Q-scale for each block in a picture. Our model goes further in the dynamic control of the encoding parameters.
Reference: [7] <author> G. W. Cook and E. J. </author> <title> Delp "The use of high performance computing in JPEG image compression". </title> <booktitle> Proceedings of the Twenty-Seventh Asilomar Conference on Signals, Systems, and Computers, </booktitle> <month> November </month> <year> 1993, </year> <title> Pacific Grove, </title> <booktitle> California, </booktitle> <pages> pp. 846-851. </pages>
Reference-contexts: We close with a discussion of some future directions and some conclusions. 2 Goals MPEG video encoding is a task that lends itself easily to parallelism. The main problem in actually creating a parallel MPEG-2 encoder is the movement of raw data. In fact, Cook and Delp <ref> [7] </ref>, write that "the greatest difficulty lies not with the compression algorithm per se, but with the parallel architecture".
Reference: [8] <author> K. Shen, G. W. Cook, L. H. Jamieson, and E. J. </author> <title> Delp "An overview of parallel processing approaches to image compression". </title> <booktitle> Proceedings of the SPIE Conference on Image and Video Compression, February 1994, </booktitle> <address> San Jose, </address> <booktitle> California, </booktitle> <volume> vol. 2186, </volume> <pages> pp. 197-208. </pages>
Reference: [9] <author> G. K. </author> <title> Wallace "The JPEG Still Picture Compression Standard" Communications of the ACM, </title> <journal> April 1991, </journal> <volume> vol. 34, No. 4, </volume> <pages> pp. 30-44. </pages>
Reference-contexts: While the bit-rate is not affected at all, the quality of the final stream is much more influenced by the MPEG encoding itself: For color images with moderately complex scenes, JPEG encoding produces the following levels of picture quality for the indicated ranges of compression <ref> [9] </ref>: 0.25 - 0.5 bits/pixel: moderate to good quality, sufficient for some applications. 0.5 - 0.75 bits/pixel: good to very good quality, sufficient for many applications. 0.75 - 1.5 bits/pixel: excellent quality, sufficient for most applications. 1.5 - 2.0 bits/pixel: usually indistinguishable from the original, sufficient for the most demanding applications.
Reference: [10] <author> K. Shen and E. J. </author> <title> Delp "A fast algorithm for video parsing using MPEG compressed sequences". </title> <booktitle> Proceedings of the International Conference on Image Processing, </booktitle> <address> October 1995, Washington, D.C., </address> <pages> pp. 252-255. </pages>
Reference: [11] <author> D. Banks and L. A. </author> <title> Rowe "Analysis Tools for MPEG-1 Video Streams". </title>
Reference: [12] <author> B. G. Haskell, A. Puri, and A. N. </author> <title> Netravali "Digital Video: An introduction to MPEG-2". </title> <editor> Ed. </editor> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference: [13] <author> V. Jacobson and M. J. </author> <title> Karels "Congestion Avoidance and Control" </title>
Reference-contexts: be possible to include in the nodes a smart allocation policy so that they could free memory with data they have used and know they are not going to use anymore. 5 This approach was similar to the slow-start mechanism on the TCP with congestion avoidance network protocol described in <ref> [13] </ref>. to avoid swamping in the nodes: they would not have been able to know they were swamped because the virtual memory system would have hidden it by starting to swap data to the disk.
Reference: [14] <author> K. Harty and D. R. </author> <title> Cheriton "Application-Controlled Physical Memory using External Page-Cache Management" </title>
Reference: [15] <author> I. Leslie et al. </author> <title> "The Design and Implementation of an Operating System to Support Distributed Multimedia Applications" </title>
Reference-contexts: possibility to choose to which node send the data, and this affects the global performance, what policy should be used to select the nodes? Of course, due to the lack of features in the NOW's operating system (a thin GLUnix layer atop Solaris), we cannot rely upon predictability nor consistency <ref> [15] </ref> in the network nor in the CPUs. But we have opted to approximate the existence of consistency in the behavior of the resource allocation most of the time. <p> Unfortunately, this would require the change of the operating system to a QoS-guaranteeing one like Nemesis <ref> [15] </ref>. 7.4 NAME's management of its own paging policy As mentioned previously, one of the assumings of our application is that the system where the encoder lies, Berkeley's NOW, is a relatively idle resource.
References-found: 15

