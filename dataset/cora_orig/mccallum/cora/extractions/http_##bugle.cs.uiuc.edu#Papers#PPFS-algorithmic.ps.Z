URL: http://bugle.cs.uiuc.edu/Papers/PPFS-algorithmic.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Projects/PPFS/ppfs.html
Root-URL: http://www.cs.uiuc.edu
Email: fesmirni,elford,alavery,reed,achieng@cs.uiuc.edu  
Title: Algorithmic Influences on I/O Access Patterns and Parallel File System Performance  
Author: Evgenia Smirni Christopher L. Elford Andrew J. Lavery Daniel A. Reed Andrew A. Chien 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: For many scalable parallel applications, the input/output (I/O) barrier rivals or exceeds that of computation and interprocessor communication. Consequently, scalable parallel secondary and tertiary storage systems are necessary to satisfy the resource demands of many national challenge problems. At present, one major challenge facing the designers of such storage systems is the wide range of I/O access patterns and the lack of general purpose file system policies that achieve high performance for variable I/O requirements. We analyze the I/O behavior of two scientific applications on the Intel Paragon XP/S. Although the two applications solve the same scientific problem and their I/O access patterns are qualitatively similar, their interactions with the file system are decidedly different. Using the results of a detailed I/O characterization, we tune the file system policies of the Portable Parallel File System (PPFS), an experimental parallel file system that provides a flexible infrastructure for selecting file policies that satisfy the I/O demands of each application. Our results show that appropriate tuning of file system policy parameters to I/O demands can significantly increase I/O throughput. Based on these results, we describe a set of principles for flexible, adaptive file systems that dynamically adjust policy parameters using runtime identification of behavior and performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Acharya, A., Uysal, M., Bennett, R., Mendelson, A., Beynon, M., Hollingsworth, J. K., Saltz, J., and Sussman, A. </author> <title> Tuning the performance of I/O intensive parallel applications. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems (Philadelphia, </booktitle> <month> May </month> <year> 1996), </year> <pages> pp. 15-27. </pages>
Reference-contexts: presented in the following sections. 4 Portable Parallel File System Various studies have suggested that to improve on I/O performance and reduce application execution times, the application developer should carefully tune application access patterns to match the idiosyncrasies of the parallel file system and the available, default file system policies <ref> [20, 1, 21] </ref>. This approach reduces the application portability across various platforms but is necessary to improve application I/O transfer rates. An alternative approach is to adjust the file system policies to the application I/O patterns.
Reference: [2] <author> Bennett, R., Bryant, K., Sussman, A., Das, R., and Saltz, J. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference (October 1994), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 10-20. </pages>
Reference-contexts: Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies. For example, Galley [13], Panda [19], PASSION [4] and Jovian <ref> [2] </ref> support external multi-dimensional arrays. These array based systems can reorder disk data distributions and aggregate collective I/O requests to more effectively utilize disk bandwidth. 2 Targeting users of application domains, domain specific I/O libraries create intuitive user in-terface for I/O.
Reference: [3] <author> Bernholdt, D. </author> <type> Personal communication, </type> <month> August </month> <year> 1996. </year>
Reference-contexts: MESSKIT consists of three programs, psetup, pargos, and pscf, that correspond to the three logical phases. Although the qualitative behavior of the two applications is the same, their quantitative spatial and temporal I/O behavior differs due to the different algorithmic and implementation approaches chosen by their developers <ref> [3] </ref>: * In NWChem, the integrals are written to disk in parallel with the first Fock matrix construc tion, procedures that are separated in MESSKIT. 3 * The two codes use different integral evaluation libraries, with different timings of the integral evaluation phase. * NWChem uses a preconditioned conjugate gradient approach
Reference: [4] <author> Bordawekar, R., Thakur, R., and Choudhary, A. </author> <title> Efficient compilation of out-of-core data parallel programs. </title> <type> Tech. Rep. SCCS-622, </type> <institution> NPAC, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies. For example, Galley [13], Panda [19], PASSION <ref> [4] </ref> and Jovian [2] support external multi-dimensional arrays. These array based systems can reorder disk data distributions and aggregate collective I/O requests to more effectively utilize disk bandwidth. 2 Targeting users of application domains, domain specific I/O libraries create intuitive user in-terface for I/O.
Reference: [5] <author> Crandall, P., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> Input/Output characterization of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing 1995 (1996), </booktitle> <pages> pp. </pages> <publisher> CD-ROM. </publisher>
Reference-contexts: Finally, x6 summarizes our findings and outlines future work. 2 Related Work Early I/O characterization efforts for scientific applications focused on the application I/O access patterns on vector supercomputers [12, 14]. These studies concluded that I/O behavior was highly regular, cyclical, and bursty. In contrast, recent studies <ref> [5, 20, 21] </ref> of parallel I/O have demonstrated that there is much variation in temporal and spatial patterns across applications, with both very small and very large request sizes, sequential and complex strided accesses, shared and non-shared files, and access time scales ranging from microseconds to minutes.
Reference: [6] <author> Foster, I., and Nieplocha, J. </author> <title> ChemIO: High-performance I/O for computational chemistry applications. </title> <note> WWW http://www.mcs.anl.gov/chemio/, February 1996. </note>
Reference-contexts: These array based systems can reorder disk data distributions and aggregate collective I/O requests to more effectively utilize disk bandwidth. 2 Targeting users of application domains, domain specific I/O libraries create intuitive user in-terface for I/O. ChemIO <ref> [6] </ref> allows the development of portable, high-performance computational chemistry codes without requiring the user to concentrate on system dependent parallel I/O optimizations. SOLAR [22] is a portable high-performance library for out-of-core dense matrix computations.
Reference: [7] <institution> High Performance Computational Chemistry Group, Pacific Northwest National Laboratory. </institution> <month> NWChem, </month> <title> A Computational Chemistry Package for Parallel Computers, Version 1.1. </title> <address> Richland, Washington, 99352, USA, </address> <year> 1995. </year>
Reference-contexts: understanding of the I/O requirements of a class of parallel applications, allow staging of external arrays as well as special optimizations for shared and private files. 3 Application I/O Characterization We examine the I/O behavior of two chemistry codes that perform ab initio electronic structure calculations using the Hartree-Fock algorithm <ref> [7] </ref>. Hartree-Fock computes the electron density around a molecule by considering each electron in the molecule in the collective field of the others. The calculation iterates until the field felt by each electron is consistent with that of the other electrons.
Reference: [8] <author> Huber, J. V., Elford, C. L., Reed, D. A., Chien, A. A., and Blumenthal, D. S. </author> <title> PPFS: A high-performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing (July 1995), </booktitle> <pages> pp. 385-394. </pages>
Reference-contexts: Alternatively, file system policies that adjust data distribution across storage devices and adapt their data management policies to accommodate the application I/O access patterns offer a viable alternative to code restructuring [18]. The Portable Parallel File System (PPFS) <ref> [8] </ref> is an experimental software system that provides a flexible application programming interface (API) for controlling data distribution and data management policies. It is implemented as a user level library and provides sufficient flexibility for selecting cache sizes and replacement policies, prefetch and write behind thresholds and limits. <p> The Portable Parallel File System (PPFS) is a user level parallel I/O library that allows rapid exploration of new file system policies across several hardware platforms. Its major advantage is its extensible infrastructure for portable, parallel I/O <ref> [8] </ref>. Because portability is of great importance, PPFS is built on top of the underlying file systems, requiring only standard, user-level read/write 9 access rather than relying on modifications of the operating system. <p> Again, prefetch parameters must be carefully tuned to limit resource requirements and to interleave prefetches with demand accesses. With selection of appropriate controls, it is possible to achieve performance improvements over native file systems <ref> [8] </ref>. However, caution is required while using these controls: they must be well tuned to the application access pattern and hardware characteristics. An inappropriate prefetching technique, (e.g., sequential prefetching for a random access pattern), can degrade performance.
Reference: [9] <author> Intel Corporation. </author> <title> Paragon System User's Guide. </title> <publisher> Intel SSD, </publisher> <address> Beaverton, OR, </address> <year> 1995. </year>
Reference-contexts: Flexible data distribution and data management policies can accommodate the variability of parallel I/O access patterns [11]. A growing number of parallel platform vendors provide limited user control over file data distribution and request aggregation <ref> [10, 9] </ref>. Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies.
Reference: [10] <author> King, S. M. Installing, </author> <title> Managing, and Using the IBM AIX Parallel I/O File System. </title> <institution> Information Development Department, IBM Kingston, </institution> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Flexible data distribution and data management policies can accommodate the variability of parallel I/O access patterns [11]. A growing number of parallel platform vendors provide limited user control over file data distribution and request aggregation <ref> [10, 9] </ref>. Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies.
Reference: [11] <author> Kotz, D., and Ellis, C. S. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> 218-230. </pages>
Reference-contexts: Flexible data distribution and data management policies can accommodate the variability of parallel I/O access patterns <ref> [11] </ref>. A growing number of parallel platform vendors provide limited user control over file data distribution and request aggregation [10, 9]. Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations.
Reference: [12] <author> Miller, E. L., and Katz, R. H. </author> <title> Input/Output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91 (November 1991), </booktitle> <pages> pp. 567-576. </pages>
Reference-contexts: Finally, x6 summarizes our findings and outlines future work. 2 Related Work Early I/O characterization efforts for scientific applications focused on the application I/O access patterns on vector supercomputers <ref> [12, 14] </ref>. These studies concluded that I/O behavior was highly regular, cyclical, and bursty.
Reference: [13] <author> Nieuwejaar, N., and Kotz, D. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing (May 1996). To appear. </booktitle>
Reference-contexts: Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies. For example, Galley <ref> [13] </ref>, Panda [19], PASSION [4] and Jovian [2] support external multi-dimensional arrays. These array based systems can reorder disk data distributions and aggregate collective I/O requests to more effectively utilize disk bandwidth. 2 Targeting users of application domains, domain specific I/O libraries create intuitive user in-terface for I/O.
Reference: [14] <author> Pasquale, B. K., and Polyzos, G. C. </author> <title> Dynamic I/O characterization of I/O intensive scientific applications. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994), </booktitle> <pages> pp. 660-669. </pages>
Reference-contexts: Finally, x6 summarizes our findings and outlines future work. 2 Related Work Early I/O characterization efforts for scientific applications focused on the application I/O access patterns on vector supercomputers <ref> [12, 14] </ref>. These studies concluded that I/O behavior was highly regular, cyclical, and bursty.
Reference: [15] <author> Poole, J. T. </author> <title> Scalable I/O Initiative. </title> <institution> California Institute of Technology, </institution> <note> Available at http://www.ccsf.caltech.edu/SIO/, 1996. 16 </note>
Reference-contexts: Clearly, detailed understanding of the interaction of I/O request patterns with the hardware and software of parallel I/O systems is prerequisite to file system design and optimization. As part of the Scalable I/O Initiative (SIO) <ref> [15] </ref> the goals of this work are twofold: produce a detailed characterization of I/O behavior of large scientific applications and utilize this characterization to explore effective parallel file system policies.
Reference: [16] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable performance analysis: The Pablo performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: To characterize and compare the I/O access patterns of MESSKIT and NWChem, we used the Pablo performance environment's I/O instrumentation software <ref> [16] </ref> to capture each code's I/O behavior including the individual request sizes, their timings, and their spatial and temporal patterns. This characterization will drive in x5 the tuning of the Portable Parallel File System (PPFS) policies. <p> To characterize application I/O and the Intel parallel file system responses, we used an extended version of the Pablo performance analysis environment <ref> [16, 17] </ref>. Pablo is a portable performance environment that supports performance data capture and analysis. To capture and analyze application I/O data, we exploited Pablo's I/O extension interfaces that perform both real-time calculation of statistical summaries and capture of detailed event traces for off-line analysis.
Reference: [17] <author> Reed, D. A., Elford, C. L., Madhyastha, T., Scullin, W. H., Aydt, R. A., and Smirni, E. </author> <title> I/O, performance analysis, and performance data immersion. </title> <booktitle> In Proceedings of MASCOTS '96 (Feb. </booktitle> <year> 1996), </year> <pages> pp. 1-12. </pages>
Reference-contexts: To characterize application I/O and the Intel parallel file system responses, we used an extended version of the Pablo performance analysis environment <ref> [16, 17] </ref>. Pablo is a portable performance environment that supports performance data capture and analysis. To capture and analyze application I/O data, we exploited Pablo's I/O extension interfaces that perform both real-time calculation of statistical summaries and capture of detailed event traces for off-line analysis.
Reference: [18] <author> Reed, D. A., Elford, C. L., Madhyastha, T., Smirni, E., and Lamm, S. L. </author> <title> The next frontier: Interactive and closed loop performance steering. </title> <booktitle> In Proceedings of the 1996 International Conference on Parallel Processing Workshop (August 1996), </booktitle> <pages> pp. 20-31. </pages>
Reference-contexts: Alternatively, file system policies that adjust data distribution across storage devices and adapt their data management policies to accommodate the application I/O access patterns offer a viable alternative to code restructuring <ref> [18] </ref>. The Portable Parallel File System (PPFS) [8] is an experimental software system that provides a flexible application programming interface (API) for controlling data distribution and data management policies.
Reference: [19] <author> Seamons, K. E., Chen, Y., Jones, P., Jozwiak, J., and Winslett, M. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95 (December 1995). </booktitle>
Reference-contexts: Experimental parallel I/O libraries offer an alternative approach to explore I/O performance optimizations. These I/O systems assume a fixed model of parallel I/O and provide an experimental infrastructure with data distributions and file system policies. For example, Galley [13], Panda <ref> [19] </ref>, PASSION [4] and Jovian [2] support external multi-dimensional arrays. These array based systems can reorder disk data distributions and aggregate collective I/O requests to more effectively utilize disk bandwidth. 2 Targeting users of application domains, domain specific I/O libraries create intuitive user in-terface for I/O.
Reference: [20] <author> Smirni, E., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> I/O requirements of scientific applications: An evolutionary view. </title> <booktitle> In High Performance Distributed Computing (1996), </booktitle> <pages> pp. 49-59. </pages>
Reference-contexts: As part of the Scalable I/O Initiative (SIO) [15] the goals of this work are twofold: produce a detailed characterization of I/O behavior of large scientific applications and utilize this characterization to explore effective parallel file system policies. Our growing experience with parallel I/O characterization <ref> [20] </ref> underscores that for reducing application execution time, it is necessary to tune the application I/O requirements via code restructuring to the idiosyncrasies of the underlying parallel file system. <p> Finally, x6 summarizes our findings and outlines future work. 2 Related Work Early I/O characterization efforts for scientific applications focused on the application I/O access patterns on vector supercomputers [12, 14]. These studies concluded that I/O behavior was highly regular, cyclical, and bursty. In contrast, recent studies <ref> [5, 20, 21] </ref> of parallel I/O have demonstrated that there is much variation in temporal and spatial patterns across applications, with both very small and very large request sizes, sequential and complex strided accesses, shared and non-shared files, and access time scales ranging from microseconds to minutes. <p> presented in the following sections. 4 Portable Parallel File System Various studies have suggested that to improve on I/O performance and reduce application execution times, the application developer should carefully tune application access patterns to match the idiosyncrasies of the parallel file system and the available, default file system policies <ref> [20, 1, 21] </ref>. This approach reduces the application portability across various platforms but is necessary to improve application I/O transfer rates. An alternative approach is to adjust the file system policies to the application I/O patterns. <p> Figure 6 illustrates a representative sample of the achieved average read times by changing the number of prefetched units U and the total number of records kept in cache A. On the Intel Paragon, requests greater than 64 KB (i.e., the default stripe size) perform well <ref> [20] </ref>. Because both NWChem and MESSKIT use records larger than 64 KB, a prefetch unit of 1 record is a good performance choice. However, excessively large prefetch units incur additional overhead.
Reference: [21] <author> Smirni, E., Elford, C. L., Reed, D. A., and Chien, A. </author> <title> Performance modeling of a parallel i/o system: An application based approach. </title> <note> In submitted for publication (1996). </note>
Reference-contexts: Finally, x6 summarizes our findings and outlines future work. 2 Related Work Early I/O characterization efforts for scientific applications focused on the application I/O access patterns on vector supercomputers [12, 14]. These studies concluded that I/O behavior was highly regular, cyclical, and bursty. In contrast, recent studies <ref> [5, 20, 21] </ref> of parallel I/O have demonstrated that there is much variation in temporal and spatial patterns across applications, with both very small and very large request sizes, sequential and complex strided accesses, shared and non-shared files, and access time scales ranging from microseconds to minutes. <p> presented in the following sections. 4 Portable Parallel File System Various studies have suggested that to improve on I/O performance and reduce application execution times, the application developer should carefully tune application access patterns to match the idiosyncrasies of the parallel file system and the available, default file system policies <ref> [20, 1, 21] </ref>. This approach reduces the application portability across various platforms but is necessary to improve application I/O transfer rates. An alternative approach is to adjust the file system policies to the application I/O patterns.
Reference: [22] <author> Toledo, S., and Gustavson, F. G. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems (Philadelphia, </booktitle> <month> May </month> <year> 1996), </year> <pages> pp. 28-40. 17 </pages>
Reference-contexts: ChemIO [6] allows the development of portable, high-performance computational chemistry codes without requiring the user to concentrate on system dependent parallel I/O optimizations. SOLAR <ref> [22] </ref> is a portable high-performance library for out-of-core dense matrix computations.
References-found: 22

