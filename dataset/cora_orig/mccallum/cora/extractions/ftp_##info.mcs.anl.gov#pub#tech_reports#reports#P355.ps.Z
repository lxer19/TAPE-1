URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P355.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: Complexity in Numerical Optimization  Some Bounds on the Complexity of Gradients, Jacobians, and Hessians  
Author: P.M. Pardalos, Editor Andreas Griewank 
Address: Argonne, IL 60439 USA  
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory,  
Note: c fl1993 World Scientific Publishing Co.  This work was supported by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Abstract: The evaluation or approximation of derivatives is an important part of many nonlinear computations. The cost of evaluating first- and second-derivative matrices is often assumed to grow linearly and quadratically with the number of independent variables, respectively. It is shown here that much tighter bounds can be achieved through the exploitation of partial function- and argument-separability in combination with the forward and reverse mode of computational, or automatic, differentiation. The new separability concepts facilitate the reduction of chromatic numbers and maximal row lengths, which determine the complexity of the Curtis-Powell-Reid and Newsam-Ramsdell schemes for estimating sparse derivative matrices. Because of the duality between the forward and reverse modes these techniques can be applied to Jacobians as well as their transposes and the associated row-intersection graphs. In contrast to differencing, computational differentiation yields derivative values free of truncation errors and without any parameter dependence. A key result presented in this paper is that gradients and Hessians of partially separable functions can also be obtained surprisingly cheaply in the easily implemented forward mode as well as in the more sophisticated reverse mode of computational differentiation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, J. J. Mor e, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <type> Preprint MCS-P348-0193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1993. </year>
Reference-contexts: Here we have assumed that f has already been evaluated at the base point x. Fortunately, the factor 3 in (1.2) is quite pessimistic. On the other hand, the leading constant 2 does not take account of some of the overhead in the derivative code. Empirically it was found <ref> [1] </ref> that, when the forward mode is implemented as compilable code and p is in the double digits, then the resulting run-time ratio is typically between half and twice the divided difference ratio (1 + p).
Reference: [2] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 1-29. </pages>
Reference-contexts: A related issue is the relative efficiency of dynamically sparse vector operations with indirect addressing versus dense vector operations on contiguous arrays. While it is relatively easy to generate compilable code for the forward mode of automatic differentiation (see, e.g., <ref> [2] </ref>), all current implementations of the reverse mode incur a large number of procedure calls or other interpretive overheads, unless the original evaluation source is very restricted.
Reference: [3] <author> C. Bischof, G. Corliss, and A. Griewank, </author> <title> Computing second- and higher-order derivatives through univariate Taylor series, </title> <type> Preprint MCS-P296-0392, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Ar-gonne, Illinois, </institution> <year> 1992. </year>
Reference-contexts: For Jacobians the temporal complexity is strictly additive, but for Hessians the operations count may grow by a factor of two as a result of slicing <ref> [3] </ref>. In the constrained optimization case, one only needs projections of the objective and constraint Hessians to the range space of S anyway.
Reference: [4] <author> B. Christianson, </author> <title> Automatic Hessians by reverse accumulation, </title> <note> IMA J. of Numerical Analysis, 12 (1992) pp. 135-150 </note> . 
Reference-contexts: Similar bounds have been derived repeatedly in the automatic differentiation literature (see, e.g., [16], [17], and <ref> [4] </ref> as recent references). 27 4.1 First and Second Derivatives in the Forward Mode Suppose the independent variables x are considered as linear functions x (d) j x + Sd of the differentiation parameter vector d 2 IR p .
Reference: [5] <author> B. Christianson, </author> <title> Reverse accumulation and accurate rounding error estimates for Taylor series coefficients, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 81-94. </pages>
Reference-contexts: Even lower complexities can be achieved if the forward and reverse modes are combined (see, for example, <ref> [5] </ref>). By combining Propositions 1 and 2 we obtain our final result.
Reference: [6] <author> T. F. Coleman and Jin-Yi Cai, </author> <title> The cyclic coloring problem and estimation of sparse Hessain matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7 (1986), </volume> <pages> pp. 221-235. </pages>
Reference-contexts: Using the terminology of Coleman and Cai <ref> [6] </ref>, we find for the path and cyclic chromatic numbers O (G) and O 0 (G) ae ( ^ J) = O (G) O 0 (G) O (G) O (G 2 ) : (2:3) Here G 2 is the column-intersection graph of the Hessian, which does not reflect its symmetry. <p> It is interesting to note that, if one were to use CPR in the forward mode to evaluate the gradient of a scalar function f and then to use directional derivatives of this vector function rf in an indirect substitution method as described and analyzed in <ref> [6] </ref>, then by (2.3) the complexity would be proportional to OPSff g times O (G)O 0 (G) O 2 (G) = O 2 ( ^ J); where G is the incidence graph of r 2 f , which coincides by Lemma 1 with the column-intersection graph of the expanded Jacobian ^
Reference: [7] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Fortran subroutines for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 346-347. </month> <title> [8] , Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 329-345. </pages>
Reference: [9] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: Fortunately, much of the excellent research that has been conducted regarding the estimation of sparse Jacobians and Hessians by differencing (see, e.g., [10], <ref> [9] </ref>, [18], [20]) carries over to computational differentiation. The main difference is that, instead of approximating Jacobian vector products by divided differences, one obtains them without any truncation errors by the forward mode of automatic differentiation.
Reference: [10] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. 34 </pages>
Reference-contexts: Fortunately, much of the excellent research that has been conducted regarding the estimation of sparse Jacobians and Hessians by differencing (see, e.g., <ref> [10] </ref>, [9], [18], [20]) carries over to computational differentiation. The main difference is that, instead of approximating Jacobian vector products by divided differences, one obtains them without any truncation errors by the forward mode of automatic differentiation. <p> As a first approximation one may view the forward mode as truncation- and parameter-free equivalent of divided differences. The relationship is close enough that the CPR <ref> [10] </ref> and the NR [18] approaches for estimating sparse Jacobians J from some product J S with p &lt; n can be applied virtually unchanged.
Reference: [11] <author> G. H. Golub, and C. F. Van Loan, </author> <title> Matrix Computations, second edition, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore (1988) </address>
Reference-contexts: In comparing the NR and CPR methods, we have so far ignored the fact that the former scheme requires the solution of ^m linear Vandermonde systems. According to <ref> [11] </ref> this adds ^m X 2:5 ^m jX (^y k )j 2 2:5 ^mae ( ^ J) 2 floating-point operations to complexity.
Reference: [12] <author> A. Griewank, </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 35-54. </pages>
Reference-contexts: Depending on the order in which the chain rule is applied, one obtains the forward, reverse, or 2 mixed mode of computational differentiation. As was shown in <ref> [12] </ref>, even a sophisti-cated implementation of the reverse mode of computational differentiation involves a logarithmic increase in the storage requirement, but the extra data can be stored and accessed sequentially in successive forward and reverse sweeps. <p> Rather than generating and storing the full execution trace in one piece, one can break it into slices that are (re)generated several times from snapshots taken at judiciously selected checkpoints. A detailed analysis of this recursive reverse mode in <ref> [12] </ref> shows that there exists a constant c such that for all integers r 1, the reversal costs can be limited according to OPSf f g r OPSff g (1.8) SAMf f g = c RAMff g r q 7 As an alternative to accepting a constant increase in the operations <p> As we have mentioned in Subsection 1.3, it was shown in <ref> [12] </ref> that this program reversal can be performed at the computational costs (1.8) and (1.9) for some integer r, which determines a trade-off between temporal and spatial complexity. <p> Together with the cost for evaluating r' j and that for reversing the program as described in <ref> [12] </ref>, this yields the operations count as well as the SAM requirement.
Reference: [13] <author> A. Griewank and G. F. Corliss, eds., </author> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference: [14] <author> A. Griewank and S. Reese, </author> <title> On the calculation of Jacobian matrices by the Markowitz rule, in Automatic Differentiation of Algorithms: Theory, Implementation, </title> <editor> and Application (A. Griewank and G. Corliss, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 126-135. </pages>
Reference-contexts: In contrast, this nearly perfect data locality is lost if one tries to minimize the operations count for Jacobian evaluations by the general vertex elimination scheme described in <ref> [14] </ref>. Moreover, the combinatorial task of finding an elimination ordering that absolutely minimizes the operations count is conjectured to be NP hard. <p> Since this can easily happen even when J is dense, one would certainly wish to do better than applying the standard forward or reverse mode. In fact, this case would be ideal for the more general elimination procedure described in <ref> [14] </ref>, whose optimal application is conjectured to be an NP-hard combinatorial problem. On the downside, even if greedy heuristics are used, the resulting Markowitz-like procedure requires RAM of order OPSff g, which appears to be a serious drawback on larger problems.
Reference: [15] <author> A. Griewank and Ph.L. </author> <title> Toint On the unconstrained optimization of partially separable objective functions, in Nonlinear Optimization 1981 (M. </title> <editor> J. D. Powell, ed.), </editor> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981, </year> <pages> pp. 301-312. </pages>
Reference-contexts: Especially for large-scale problems, one can expect that not only the Jacobian J but especially the derivative tensor f 00 2 IR mfinfin is quite sparse. As observed in <ref> [15] </ref>, any scalar function h 2 C (IR n ) whose Hessian is sparse can be decomposed into a sum h (x) = k where the projection P k picks out the subset of components in x on which h k depends in a nontrivial fashion.
Reference: [16] <author> Masao Iri, </author> <title> History of automatic differentiation and rounding estimation, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 1-16 </pages> . 
Reference-contexts: Similar bounds have been derived repeatedly in the automatic differentiation literature (see, e.g., <ref> [16] </ref>, [17], and [4] as recent references). 27 4.1 First and Second Derivatives in the Forward Mode Suppose the independent variables x are considered as linear functions x (d) j x + Sd of the differentiation parameter vector d 2 IR p .
Reference: [17] <author> R. D. Neidinger, </author> <title> An efficient method for the numerical evaluation of partial derivatives of arbitrary order, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18(1992), </volume> <pages> pp. 159-173 </pages> . 
Reference-contexts: Similar bounds have been derived repeatedly in the automatic differentiation literature (see, e.g., [16], <ref> [17] </ref>, and [4] as recent references). 27 4.1 First and Second Derivatives in the Forward Mode Suppose the independent variables x are considered as linear functions x (d) j x + Sd of the differentiation parameter vector d 2 IR p .
Reference: [18] <author> G. N. Newsam and J. D. Ramsdell, </author> <title> Estimation of sparse Jacobian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 4 (1983), </volume> <pages> pp. 404-417 </pages> . 
Reference-contexts: Fortunately, much of the excellent research that has been conducted regarding the estimation of sparse Jacobians and Hessians by differencing (see, e.g., [10], [9], <ref> [18] </ref>, [20]) carries over to computational differentiation. The main difference is that, instead of approximating Jacobian vector products by divided differences, one obtains them without any truncation errors by the forward mode of automatic differentiation. <p> As a first approximation one may view the forward mode as truncation- and parameter-free equivalent of divided differences. The relationship is close enough that the CPR [10] and the NR <ref> [18] </ref> approaches for estimating sparse Jacobians J from some product J S with p &lt; n can be applied virtually unchanged. In the CPR approach the rows of the seed matrix S are Cartesian p vectors, whereas in the NR approach S is usually chosen as a Vandermonde matrix. <p> In this way the whole Jacobian can be computed by using only 2 log 2 n Jacobian-vector evaluations. Using the method of Newsam and Ramsdell <ref> [18] </ref>, one can do even better. Let s be any n-vector whose components oe i are all different from each other. <p> According to [11] this adds ^m X 2:5 ^m jX (^y k )j 2 2:5 ^mae ( ^ J) 2 floating-point operations to complexity. As pointed out in <ref> [18] </ref>, the conditioning of these linear systems can be improved by defining the Vandermonde matrix S using only O ( ^ J) distinct real abscissas or defining them as complex roots of unity if the chromatic number is still too large.
Reference: [19] <author> L. B. Rall, </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 120, </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference: [20] <author> Trond Steihaug and A. K. M. Shahadat Hossain, </author> <title> Graph coloring and the estimation of sparse Jacobian matrices using row and column partitioning, </title> <type> Report 72, </type> <institution> Department of Informatics, University of Bergen, </institution> <year> 1992. </year>
Reference-contexts: Fortunately, much of the excellent research that has been conducted regarding the estimation of sparse Jacobians and Hessians by differencing (see, e.g., [10], [9], [18], <ref> [20] </ref>) carries over to computational differentiation. The main difference is that, instead of approximating Jacobian vector products by divided differences, one obtains them without any truncation errors by the forward mode of automatic differentiation. <p> Analogously ^ f and ^ F may be called vertical expansions of f . Conversely we may refer to f as horizontal and vertical contraction of f and F or ^ f and ^ F , respectively. 2.4 Relation to Multicoloring It has often been observed <ref> [20] </ref> that for a partitioned vector function f (x) = f (1) (x) # the partial Jacobians J (i) = i j 0 for i = 1; 2 may satisfy O (J ) &gt; O (J (1) ) + O (J (2) ) : Then the CPR approach should be applied <p> Hence we have the representation f (x) = F (Ax) with A T = [I; I] T 2 IR nfi2n : The extended Jacobian takes the form F 0 = J (1) 0 # an expansion that has also been considered in <ref> [20] </ref>. <p> The energy of such an arrangement is often modeled as the sum of n (n + 1)=2 pairwise interactions f ij . This situation has alos been examined in some detail as the exponential example in <ref> [20] </ref>. Now suppose we wish to compute the gradient of f , given a computer program for its evaluation.
Reference: [21] <author> Stephen A. Vavasis, </author> <title> Nonlinear Optimization, Complexity Issues, </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1991. </year> <month> 35 </month>
Reference-contexts: As in numerical ordinary differential equations and other nonlinear computational fields, designers of optimization methods have usually assumed that derivatives are hard to come by and that their provision belongs to the realm of the user. This is sometimes called the black box model <ref> [21] </ref>, where the optimization algorithm relies exclusively on a subroutine for evaluating objectives and gradients at given arguments, usually with unspecified precision.
References-found: 20

