URL: http://www.cogs.susx.ac.uk/users/christ/papers/parity-here-to-stay.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Parity: The Problem that Won't Go Away  
Author: Chris Thornton 
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: It is well-known that certain learning methods (e.g., the perceptron learning algorithm) cannot acquire complete, parity mappings. But it is often overlooked that state-of-the-art learning methods such as C4.5 and backpropagation cannot generalise from incomplete parity mappings. The failure of such methods to generalise on parity mappings may be sometimes dismissed on the grounds that it is `impossible' to generalise over such mappings, or that parity problems are mathematical constructs having little to do with real-world learning. However, this paper argues that such a dismissal is unwarranted. It shows that parity mappings are hard to learn because they are statistically neutral and that statistical neutrality is a property which we should expect to encounter frequently in real-world contexts. It also shows that the generalization failure on parity mappings occurs even when large, minimally incomplete mappings are used for training purposes, i.e., when claims about the impossibility of generalization are particularly suspect.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hinton, G. and Sejnowski, T. </author> <year> (1986). </year> <title> Learning and relearning in boltzmann machines. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II (pp. </booktitle> <pages> 282-317). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It is well known that parity problems are statistically neutral <ref> [1] </ref>. This means that all conditional output probabilities exhibited by a parity mapping have `chance' values, i.e., that no input/output associations exist.
Reference: [2] <author> Murphy, P. and Pazzani, M. </author> <year> (1991). </year> <title> ID2-of-3: constructive induction of m-of-n concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (ML91). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [3] <author> Breiman, L., Friedman, J., Olshen, R. and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: Learning methods that rely solely on the exploitation of statistical effects produce worst case performance on such problems. Algorithms such as M-of-N concept learners (2) are not within this class since they use knowledge of a relevant relationship (`M-of-Nness'). However algorithms in the CART family <ref> [3] </ref> form ideal examples. ID3 [4,5] for example, constructs a decision tree by recursively partitioning the training set until every pair in a given partition maps onto the same output value.
Reference: [4] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: In fact, the recoding turns out to be of benefit even to methods which are restricted to using axis-parallel discriminations. As an illustration of this, I noted that the generalization performance of the standard ID3 algorithm <ref> [4] </ref> turns out to be improve its generalization performance on MONKS2 by nearly 10% as a result of using a sparse coding. The generalization performance increases from from 67.9% [12] to 77%.
Reference: [5] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [6] <author> Wnek, J. and Michalski, R. </author> <year> (1994). </year> <title> Discovering representation space transformations for learning concept descriptions combining DNF and m-of-n rules. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference-contexts: This means that no variable value is of any use at all in the initial, splitting-up of cases and each, successive, split produces the same, non-uniform situation at a finer-grained level. <ref> [6] </ref> The result is that ID3 necessarily builds a `lookup table' for the consumer problem, i.e., a decision tree that captures no generalisations whatsoever and which has one leaf node per case in the training set. The decision tree actually produced in shown in Figure 1. Fig. 1.
Reference: [7] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: Fig. 1. Decision tree produced by ID3 on consumer problem. 2.3 Acquiring complete, neutral mappings with backpropagation Of course not all learning algorithms rely solely on the exploitation of statistical effects. Thus, not all algorithms produce worst-case performance on neutral mappings. The backpropagation algorithm <ref> [7] </ref> for example, had been widely touted as having the ability to exploit structural and/or relational properties of the target mapping [8]. However, even with backpropagation, the damaging effects of neutrality can be readily discerned. These are demonstrated in the algorithm's generalization performance on neutral mappings.
Reference: [8] <author> Beale, R. and Jackson, T. </author> <year> (1990). </year> <title> Neural Computing: An Introduction. </title> <publisher> Adam Hilger. </publisher>
Reference-contexts: Thus, not all algorithms produce worst-case performance on neutral mappings. The backpropagation algorithm [7] for example, had been widely touted as having the ability to exploit structural and/or relational properties of the target mapping <ref> [8] </ref>. However, even with backpropagation, the damaging effects of neutrality can be readily discerned. These are demonstrated in the algorithm's generalization performance on neutral mappings.
Reference: [9] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II (pp. </booktitle> <pages> 318-362). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Explaining its generalization failure on neutral mappings is thus not straightforward. The simplest hypothesis may be that, despite its manifest success in the acquisition of small, complete parity mappings <ref> [9] </ref> backpropagation relies primarily on the exploitation of statistical effects and is thus unable to deal properly with neutral mappings. There are several arguments in favour of this hypothesis.
Reference: [10] <author> Thornton, C. </author> <year> (1992). </year> <title> Techniques in Computational Learning: An Introduction. </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: There are several arguments in favour of this hypothesis. First, the backpropagation learning algorithm is a generalization of the least-mean-squares algorithm <ref> [10] </ref> (and perceptron learning algorithm [11]) which is effectively an iterative method for deriving statistical input/output correlations. Thus the backpropagation learning method is based on a method for exploiting statistical effects. Second, the generalization performance observed in the 4-bit parity tests tended to be much worse than chance.
Reference: [11] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There are several arguments in favour of this hypothesis. First, the backpropagation learning algorithm is a generalization of the least-mean-squares algorithm [10] (and perceptron learning algorithm <ref> [11] </ref>) which is effectively an iterative method for deriving statistical input/output correlations. Thus the backpropagation learning method is based on a method for exploiting statistical effects. Second, the generalization performance observed in the 4-bit parity tests tended to be much worse than chance.
Reference: [12] <author> Thrun, S., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fisher, D., Fahlman, S., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J. and Zhang, J. </author> <year> (1991). </year> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <institution> CMU-CS-91-197, School of Computer Science, Carnegie-Mellon University. </institution>
Reference-contexts: algorithms (such as backpropagation) which rely on the creation of such discriminations is thus enhanced. 3.2 Backpropagation and the MONKS2 problem The analysis of backpropagation's performance on the sparse-coded consumer problem may help to explain certain aspects of the performance results reported by Thrun et al. for the MONKS2 problem <ref> [12] </ref>. In this problem, the input/output rule is that `exactly two of the six [input] attributes have their first value'. This problem is `similar to parity problems. It combines different attributes in a way which makes it complicated to describe in DNF or CNF' [12]. <p> et al. for the MONKS2 problem <ref> [12] </ref>. In this problem, the input/output rule is that `exactly two of the six [input] attributes have their first value'. This problem is `similar to parity problems. It combines different attributes in a way which makes it complicated to describe in DNF or CNF' [12]. The task analysis prompts us to restate this in terms of the statistical/relational distinction. <p> In fact, if we apply the standard ID3 algorithm to MONKS2 represented in a sparse binary coding, the generalization performance of the algorithm on the fixed unseen cases jumps by nearly 10% from the level of 67.9% reported in <ref> [12] </ref>. <p> To emphasise the point about sparse coding I presented some data relating to the MONKS2 problem which featured in the recent international learning competition <ref> [12] </ref>. I noted that this problem is inherently relational and thus statistically neutral (modulo incidental effects), and that the reported level of generalization achieved by backpropagation (100% correct) therefore comes as a surprise. <p> As an illustration of this, I noted that the generalization performance of the standard ID3 algorithm [4] turns out to be improve its generalization performance on MONKS2 by nearly 10% as a result of using a sparse coding. The generalization performance increases from from 67.9% <ref> [12] </ref> to 77%.
Reference: [13] <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine learning, </booktitle> <pages> 3 (pp. </pages> <month> 63-91). </month> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: As I have said, this recoding is of benefit to any 5 Holte's recent demonstration that many of the learning problems in the UCI repository can be learned using one-level decision trees <ref> [13] </ref> suggests that practical learning problems are typically not neutral.
References-found: 13

