URL: http://www.cs.yale.edu/users/csuros-miklos/papers/NoisyReport.ps
Refering-URL: http://www.cs.yale.edu/users/csuros-miklos/papers.html
Root-URL: http://www.cs.yale.edu
Title: Learning variable memory length Markov chains from noisy output CS690/691 progress report  
Author: Miklos Cs-uros advisor: Dana Angluin 
Date: November 13, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Complex data sequences such as DNA or speech are often modeled by Hidden Markov Models [8] or Probabilistic Finite Automata. Both models have severe theoretical drawbacks. Abe and Warmuth <ref> [1] </ref> proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. [5] proved that nor are Probabilistic Finite Automata learnable in the PAC-sense [2] unless noisy parity functions are learnable. <p> Definition 2.2 The model class of Probabilistic Finite Automata is the set of five-tuples hQ; ; t; fl; i: * Q: a finite set of states. * = f 1 ; : : : ; m g: finite alphabet. * fl : Q fi 7! <ref> [0; 1] </ref>: next symbol probability function, 8q 2 Q: P * t : Q fi 7! Q: transition function. * : Q 7! [0; 1]: initial probability distribution, P 3 Definition 2.3 The PFA hQ; ; t; fl; i generates the string s = s 1 s 2 s ` (where <p> i: * Q: a finite set of states. * = f 1 ; : : : ; m g: finite alphabet. * fl : Q fi 7! <ref> [0; 1] </ref>: next symbol probability function, 8q 2 Q: P * t : Q fi 7! Q: transition function. * : Q 7! [0; 1]: initial probability distribution, P 3 Definition 2.3 The PFA hQ; ; t; fl; i generates the string s = s 1 s 2 s ` (where s i 2 ) with the following probability: p s = M (s) = q 0 2Q ` Y fl (q i1 ; <p> Each character of the output is altered by the same 3 Throughout this part, qualitative notions as "often", "close", "much larger": are precisely defined by factors and inequalities using the input parameters of the algorithm. For sake of simplicity, we omit them here. 5 probability - 2 <ref> [0; 1] </ref>. If a symbol gets corrupted, then it is replaced by a different symbol of the alphabet chosen with uniform probability. Figure 2 shows the binary symmetric channel. Each bit of the output of the machine is complemented with probability -.
Reference: [2] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Both models have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. [5] proved that nor are Probabilistic Finite Automata learnable in the PAC-sense <ref> [2] </ref> unless noisy parity functions are learnable. Ron, Singer and Tishby [9] proposed the use of Probabilistic Finite Suffix Automata (PFSA), or variable memory length Markov chains, which form a subclass of Probabilistic Finite Automata. They proved that PFSA are efficiently learnable and described a polynomial-time learning algorithm.
Reference: [3] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The initial probabilities are in brackets. The relative entropy gives a bound on the L 1 distance, as D [ p k q ] 2 1 : (See Cover and Thomas <ref> [3] </ref>.) Since the L 1 norm bounds the L 2 ; L 3 ; : : : ; and L 1 norms from above, a good hypothesis remains a good one for other distance functions, as well. 2 2.2 Model classes of Probabilistic Finite Automata An example of a Probabilistic Finite
Reference: [4] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference-contexts: (d)) p fl (B s (d))j * fl g ` [ fj^p fl (B s (d)) p fl (B s (d))j &gt; * fl g ` X IPf j^p fl (B s (d)) p fl (B s (d))j &gt; * fl g ffi = 1 ffi; by Hoeffding's inequality 4 <ref> [4] </ref>. 9 Target machine T Sample of strings Algorithm I. Estimate probabilities II. Core of the algorithm Hypothesis machine H 3.3 Algorithm when noise is known We will use the algorithm of Ron, Singer and Tishby [9]. The main structure of the learning process is sketched in Figure 3.
Reference: [5] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire and L. Sellie. </author> <title> On the Learnability of discrete distributions. </title> <booktitle> STOC 94, </booktitle> <pages> 273-282, </pages> <year> 1994. </year> <note> Available electronically as http://www.cs.huji.ac.il/labs/learning/Papers/dists-stoc94.ps.gz </note>
Reference-contexts: Both models have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. <ref> [5] </ref> proved that nor are Probabilistic Finite Automata learnable in the PAC-sense [2] unless noisy parity functions are learnable. Ron, Singer and Tishby [9] proposed the use of Probabilistic Finite Suffix Automata (PFSA), or variable memory length Markov chains, which form a subclass of Probabilistic Finite Automata. <p> Recall that by the result of Kearns et al. <ref> [5] </ref>, it is unlikely to have an efficient learning algorithm for the model class of PFA. We will therefore impose further restrictions to make efficient learning possible.
Reference: [6] <author> P. Lancaster and M. Tismenetsky. </author> <title> The Theory of Matrices. </title> <publisher> Academic Press, </publisher> <address> Orlando, </address> <year> 1985. </year>
Reference-contexts: N 1 k ) is the k-th "Kronecker-power" of N 1 (vs. N 1 1 ). Proof. The recursion for the noise transformation matrices is clear from the noise model. For the inverses, use the fact that (A B) = A 1 B 1 (see, eg., <ref> [6] </ref>). <p> A compatible matrix norm is the column norm defined as kAk fl = max n X ja ij j for matrix A = [a ij ] n i;j=1 . (See <ref> [6] </ref>.) It is easy to prove that for any matrices A and B, kA Bk fl kAk fl kBk fl : Therefore, fl flN 1 fl fl fl 1 fl fl . So we have the following lemma (cf.
Reference: [7] <author> G. Lugosi. </author> <title> Learning with an unreliable teacher. </title> <journal> Pattern Recognition, </journal> <pages> 79-87, </pages> <year> 1992. </year>
Reference-contexts: They proved that PFSA are efficiently learnable and described a polynomial-time learning algorithm. In this report we consider the case when the output of the target machine is seen only through a noisy channel. Among others, Lugosi <ref> [7] </ref> considered learning from noisy data and showed that consistent learning is possible in case of the binary symmetric channel even if the noise rate is not known.
Reference: [8] <author> L. R. Rabiner. </author> <title> A tutorial on Hidden Markov Models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 </volume> <pages> 257-285, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Complex data sequences such as DNA or speech are often modeled by Hidden Markov Models <ref> [8] </ref> or Probabilistic Finite Automata. Both models have severe theoretical drawbacks. Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP.
Reference: [9] <author> D. Ron, Y. Singer and N. Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> COLT 94, </booktitle> <pages> 35-46, </pages> <year> 1994. </year> <note> Available electronically as http://www.cs.huji.ac.il/labs/learning/Papers/psa_colt94.ps.gz 22 </note>
Reference-contexts: Abe and Warmuth [1] proved that Hidden Markov Models are not learnable in time polynomial in the alphabet size unless RP=NP. Kearns et al. [5] proved that nor are Probabilistic Finite Automata learnable in the PAC-sense [2] unless noisy parity functions are learnable. Ron, Singer and Tishby <ref> [9] </ref> proposed the use of Probabilistic Finite Suffix Automata (PFSA), or variable memory length Markov chains, which form a subclass of Probabilistic Finite Automata. They proved that PFSA are efficiently learnable and described a polynomial-time learning algorithm. <p> In Section 4 we will show that the same algorithm also works if we have a very good estimate of the noise. 2 Probabilistic Finite Suffix Automata This section is mainly a summary of the results of Ron et al. <ref> [9] </ref>. 1 2.1 How good is a hypothesis? A data sequence is defined as a string over a finite alphabet . For DNA sequencing, the alphabet consists of the four bases, a set of sounds may serve as alphabet in a pronunciation model and so on. <p> If the strings s have the distribution defined by p but we use the optimal code for the distribution defined by q, then D [ p k q ] is asymptotically the expected number of extra bits used compared to the optimal encoding. <ref> [9] </ref> uses natural logarithm instead of base 2, so we follow their definition. 2 The states of this binary PFA are Q = f1; 10; 00g, arrows represent the transition function with the transition probabilities. The initial probabilities are in brackets. <p> In many cases, the generation of the next symbol depends on fewer than ` previously generated symbols. This justifies the term "variable memory length Markov chain". The PFSA learning algorithm of Ron, Singer and Tishby <ref> [9] </ref> uses a sample of prefixes of length ` + 1 that are independently generated by an `-PFSA M = hQ; ; t; fl; i. <p> Estimate probabilities II. Core of the algorithm Hypothesis machine H 3.3 Algorithm when noise is known We will use the algorithm of Ron, Singer and Tishby <ref> [9] </ref>. The main structure of the learning process is sketched in Figure 3. From a sample of strings generated by the target automaton T , the algorithm estimates the string probabilities and outputs a hypothesis machine H. <p> The modified definition of a typical sample suited for our needs and notation follows. Definition 3.2 (cf. Definition 6.1 in <ref> [9] </ref>) A sample generated according to PFSA M is (* 0 ; * 1 ; * 2 ; fl min )-typical if for the probabilities ^p s estimated using the sample, the following two properties hold: 1.
References-found: 9

