URL: ftp://synapse.cs.byu.edu/pub/papers/rudolph_95b.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: e-mail: george@axon.cs.byu.edu  
Title: Word Perfect Corp. A TRANSFORMATION FOR IMPLEMENTING LOCALIST NEURAL NETWORKS  
Author: George L. Rudolph and Tony R. Martinez 
Keyword: Neural Networks, Competitive Learning, Counterpropagation, Localist Networks, Dynamic Topologies, Reconfigurable Architectures, Implementation Design.  
Address: Provo, Utah 84602  
Affiliation: Brigham Young University, Computer Science Department  
Note: In Neural Parallel and Scientific Computations, vol. 3, no. 2, pp. 173-188, 1995. This research is funded by grants from Novell Inc. and  
Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and typically suffer from a number of shortcomings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces Location-Independent Transformations (LITs) as a general strategy for parallel implementation of feedforward networks that use dynamic topologies. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. This paper deals specifically with LITs for localist ANNslocalist in the sense that ultimately one node is responsible for each output. In particular, this paper presents LITs for two ANNs: a) the single-layer competitive learning network, and b) the counterpropagation network, which combines elements of supervised learning with competitive learning. The complexity of both learning and execution algorithms for both ANNs is linear in the number of inputs and logarithmic in the number of nodes in the original network. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almasi, G. S., A. Gottlieb. </author> <year> (1989). </year> <title> Highly Parallel Computing. </title> <address> Redwood City, CA: </address> <publisher> The Benjamin/Cummings Publishing Company, Inc. </publisher>
Reference-contexts: Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not. Of course, general parallel machines, like the Connection Machine [9] and the CRAY <ref> [1] </ref>, can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation. LIT redesigns the original network into a hierarchical, parallel network of Location Independent nodes.
Reference: [2] <author> DARPA. </author> <year> (1988). </year> <title> Neural Network Study. </title> <publisher> AFCEA International Press. </publisher>
Reference-contexts: These features allow ANNs to solve various applications not handled well by current conventional computational mechanisms. Application areas include, but are not limited to, problems requiring learning, such as pattern recognition, control and decision systems, speech, and signal analysis <ref> [2] </ref>. Hardware support for ANNs is important for handling large, complex problems in real time. Learning times can exceed tolerable limits for complex applications with conventional computing schemes. Furthermore, hardware is becoming cheaper and easier to design.
Reference: [3] <author> Fahlmann, Scott, C. Lebiere. </author> <booktitle> The Cascade-Correlation Learning Architechture. in Advances in Neural Information Processing 2. </booktitle> <pages> pp. 524-532. </pages> <publisher> Morgan Kaufmann Publishers: </publisher> <address> Los Altos, CA. </address>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems <ref> [3] </ref>, [10], [12], [14] for a variety of applications. [7], for example, shows that Radial-Basis Function Networks (RBF) with a dynamic topology performed significantly better on NETtalk than the same RBF network with fixed topology.
Reference: [4] <author> Farhat, N., D. Psaltis, A. Prata, and E. Paek. </author> <year> (1985). </year> <title> Optical Implementation of the Hopfield Model. </title> <journal> Applied Optics, v.24, </journal> <volume> #10. </volume> <month> pp.1469-1475. </month>
Reference: [5] <author> Graf, H., L. Jackel, W. Hubbard. </author> <title> (1990).VLSI Implementation of a Neural Network Model. In Artificial Neural Networks: Electronic Implementations. </title> <publisher> Nelson Morgan, Ed. </publisher> <pages> pp. 34-42. </pages>
Reference: [6] <author> Hammerstrom, D., W. Henry, M. Kuhn. </author> <year> (1991). </year> <title> Neurocomputer System for Neural Network Applications. In Parallel Digital Implementations of Neural Networks. </title> <editor> K. Przytula, V. Prasanna, Eds. </editor> <publisher> Prentice-Hall, Inc. </publisher>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs <ref> [6] </ref>, [13], [18]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [7] <author> Haykin, Simon. </author> <year> (1994). </year> <title> Neural Networks: A Comprehensive Foundation . New York: </title> <publisher> Macmillan College Publishing Company, Inc. </publisher> <pages> pp. 266-268. </pages>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], [10], [12], [14] for a variety of applications. <ref> [7] </ref>, for example, shows that Radial-Basis Function Networks (RBF) with a dynamic topology performed significantly better on NETtalk than the same RBF network with fixed topology. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. <p> LIT is not meant to include devising dynamic extensions to static models per se. The existence of dynamic extensions to static models, and the existence of inherently dynamic models such as RCE [14], and dynamic RBF networks <ref> [7] </ref> is assumed. Furthermore, the literature demonstrates the stability and applicability of various models and dynamic extensions. The CL and CPN networks, and the dynamic extension proposed here should be sufficient to indicate how LIT supports localist ANNs, whether these have static, extended dynamic, or inherently dynamic topologies.
Reference: [8] <author> Hecht-Nielsen, R. </author> <title> Counterpropagation Networks. (1987). </title> <journal> Applied Optics, v.26, </journal> <volume> #23. </volume> <pages> pp. 4979-4984. </pages>
Reference-contexts: This helps to illustrate one important point of this papera different, more robust learning equation (algorithm) could be substituted, and the overall transformation would still be the same. 4. TRANSFORMATION OF A COUNTERPROPAGATION NETWORK The goal of CPN is to learn approximations to input-output pairs presented to the network <ref> [8] </ref>. The original network is presented as a four-layer counterpropagation flow between five sets of nodes (figure 5). However, CPN can be viewed as single-layer competitive learner with adjustable output weights (without loss of generality). Throughout section 4, it is necessary to refer to node layers and weight layers both. <p> The learning algorithm guarantees that only one node can win a competition. The learning equations show that a winning node will change its input weights to respond more strongly to the current input, and that the output weights will be adjusted to be more like the desired output <ref> [8] </ref>. 5. COMPLEXITY OF LIT CL AND LIT CPN ALGORITHMS The complexity of the algorithms in sections 3 and 4 can be characterized by serial, pipelined broadcast and gather in a tree of nodes. The width of a broadcast block is assumed to be the size of a single variable.
Reference: [9] <author> Hillis, W. Daniel. </author> <title> (1985).The Connection Machine. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [13], [18]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not. Of course, general parallel machines, like the Connection Machine <ref> [9] </ref> and the CRAY [1], can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation. LIT redesigns the original network into a hierarchical, parallel network of Location Independent nodes.
Reference: [10] <author> Martinez, T.R., D.M. Campbell. </author> <year> (1991). </year> <title> A Self-Adjusting Dynamic Logic Module. </title> <journal> Journal of Parallel and Distributed Computing, v.11, </journal> <volume> #4. </volume> <pages> pp. 303-313. </pages>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], <ref> [10] </ref>, [12], [14] for a variety of applications. [7], for example, shows that Radial-Basis Function Networks (RBF) with a dynamic topology performed significantly better on NETtalk than the same RBF network with fixed topology. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. <p> LITs have been developed for backpropagation [16], and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 <ref> [10] </ref>, [15]. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support dynamic variations of many ANNs.
Reference: [11] <author> Mead, Carver. </author> <year> (1989). </year> <title> Analog VLSI and Neural Systems. </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], <ref> [11] </ref>. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [13], [18]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [12] <editor> Odri, </editor> <publisher> S.V., D.P. </publisher> <address> Petrovacki, G.A. Krstonosic. </address> <year> (1993). </year> <title> Evolutional Development of a Multilevel Neural Network. Neural Networks, </title> <journal> v.6, </journal> <volume> #4. </volume> <pages> pp. 583-595. </pages> <publisher> Pergamon Press Ltd.: </publisher> <address> New York. </address>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], [10], <ref> [12] </ref>, [14] for a variety of applications. [7], for example, shows that Radial-Basis Function Networks (RBF) with a dynamic topology performed significantly better on NETtalk than the same RBF network with fixed topology. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11].
Reference: [13] <author> Ramacher, U., W. Raab, J. Anlauf, U. Hachmann, J. Beichter, N. Brls, M. Weiling, E. Schneider, R. Mnner, J. Gl. </author> <year> (1993). </year> <title> Multiprocessor and Memory Architecture of the Neurocomputer SYNAPSE-1. </title> <booktitle> Proceedings, World Congress on Neural Networks 1993, v. </booktitle> <volume> 4. </volume> <pages> pp. 775-778. </pages> <publisher> INNS Press. </publisher>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], <ref> [13] </ref>, [18]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [14] <author> Reilly, D.L., L.N. Cooper, C. Elbaum. </author> <year> (1988). </year> <title> Learning Systems Based on Multiple Neural Networks. (Internal paper). Nestor, </title> <publisher> Inc. </publisher>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], [10], [12], <ref> [14] </ref> for a variety of applications. [7], for example, shows that Radial-Basis Function Networks (RBF) with a dynamic topology performed significantly better on NETtalk than the same RBF network with fixed topology. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. <p> LIT is not meant to include devising dynamic extensions to static models per se. The existence of dynamic extensions to static models, and the existence of inherently dynamic models such as RCE <ref> [14] </ref>, and dynamic RBF networks [7] is assumed. Furthermore, the literature demonstrates the stability and applicability of various models and dynamic extensions.
Reference: [15] <author> Rudolph G., and T.R. Martinez. </author> <year> (1991). </year> <title> An Efficient Static Topology for Modeling ASOCS. </title> <booktitle> International Conference on Artificial Neural Networks, </booktitle> <address> Helsinki, Finland. </address> <booktitle> In Artificial Neural Networks, Kohonen et al, </booktitle> <pages> pp. 279-734. </pages> <publisher> North Holland: Elsevier Publishers. </publisher>
Reference-contexts: LITs have been developed for backpropagation [16], and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 [10], <ref> [15] </ref>. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support dynamic variations of many ANNs.
Reference: [16] <author> Rudolph G., Martinez, T. R. </author> <year> (1995). </year> <title> A Transformation for Implementing Efficient Dynamic Backprpagation Networks. </title> <booktitle> To appear in the Proceedings of the International Cconference on Artificial Neural Networks and Genetic Algorithms. </booktitle>
Reference-contexts: LITs have been developed for backpropagation <ref> [16] </ref>, and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 [10], [15]. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support dynamic variations of many ANNs.
Reference: [17] <editor> Rumelhart, D., J. McClelland, et. al. </editor> <year> (1986). </year> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <publisher> v.1. MIT Press. </publisher>
Reference-contexts: TRANSFORMATION OF A COMPETITIVE LEARNING NETWORK The goal of the CL model is to spontaneously classify sets of similar inputs to the same class, and sets of different inputs into different classes, according to critical features discovered by the network <ref> [17] </ref>. The original CL has one output node for each output class, and weights from each input node to every output node. <p> Add a new node in the networkinitialize the weights to be equal to X. 10. If at the end of the current epoch, then any node that has not won at least once self-deletes from the network. One note on the CL model: The learning equation (3) (see <ref> [17] </ref>) gives a very non robust type of learning. Both the zero vector and the vector with all ones will always be put in the class with the lowest number, because in each case, the activations of all nodes is the same.
Reference: [18] <author> Shams, S. </author> <title> Dream MachineA Platform for Efficient Implementation of Neural Networks with Arbitrarily Complex Interconnect Structures. </title> <type> (1992).Technical Report CENG 92-23. PhD Dissertation, </type> <institution> USC. </institution>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [11]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [13], <ref> [18] </ref>. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [19] <author> Stout, M., G. Rudolph, T.R. Martinez, L. Salmon. </author> <year> (1994). </year> <title> A VLSI Implementation of a Parallel, Self-Organizing Learning Model. </title> <booktitle> To appear in Proceedings of the 12th IEEE International Conference on Pattern Recognition. </booktitle>
Reference-contexts: The deletion of a hidden node in the original network, such as node 5, is accomplished in the transformed network by marking the corresponding node as free. No other nodes are affected. A prototype VLSI chip set has been fabricated as proof-of-concept of the overall LIT strategy <ref> [19] </ref>. 3. TRANSFORMATION OF A COMPETITIVE LEARNING NETWORK The goal of the CL model is to spontaneously classify sets of similar inputs to the same class, and sets of different inputs into different classes, according to critical features discovered by the network [17].
References-found: 19

