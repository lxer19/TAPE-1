URL: http://www.dcs.ex.ac.uk/~pclane/ecai98.ps
Refering-URL: http://www.dcs.ex.ac.uk/~pclane/
Root-URL: http://www.dcs.ex.ac.uk
Title: Simple Synchrony Networks: Learning Generalisations across Syntactic Constituents  
Author: Peter Lane 
Abstract: This paper describes a training algorithm for Simple Synchrony Networks (SSNs), and reports on experiments in language learning using a recursive grammar. The SSN is a new connectionist architecture combining a technique for learning about patterns across time, Simple Recurrent Networks (SRNs), with Temporal Synchrony Variable Binding (TSVB). The use of TSVB means the SSN can learn about entities in the training set, and generalise this information to entities in the test set. In the experiments, the network is trained on sentences with up to one embedded clause, and with some words restricted to certain classes of constituent. During testing, the network generalises information learned to sentences with up to three embedded clauses, and with words appearing in any constituent. These results demonstrate that SSNs learn generalisations across syntactic constituents. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J L Elman, </author> <title> `Finding structure in time', </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211, </pages> <year> (1990). </year>
Reference-contexts: at different levels of embedding. 1 Department of Computer Science, University of Exeter, Prince of Wales Road, EXETER EX4 4PT, pclane@dcs.exeter.ac.uk. 2 SIMPLE SYNCHRONY NETWORKS The Simple Synchrony Network (SSN) described in this paper is a combination of two elements, of which the first is the Simple Recurrent Network (SRN) <ref> [1] </ref>. The SRN is based on a standard multi-layer connectionist network, with layers of input, hidden and output units. In addition, the SRN has a layer of context units, one per hidden unit. <p> Again, weight changes must be made to every copy of a link. 3 EXPERIMENTAL RESULTS The ability of SRNs to learn grammatical information from samples of language has been demonstrated in, for example, <ref> [1] </ref> and [4]. The experiments with the SSN described here differ from such work in using a structured output representation, i.e. a parse tree. The experiments use a recursive grammar, as described in [2], to test the ability of SSNs to learn generalisations across syntactic constituents.
Reference: [2] <author> R F Hadley and M B Hayward, </author> <title> `Strong semantic systematic-ity from unsupervised connectionist learning', </title> <booktitle> Proceedings of the Seventeenth Conference of the Cognitive Science Society, </booktitle> <address> Pittsburgh, PA., </address> <year> (1995). </year>
Reference-contexts: The argument in [3] is empirically validated by the results reported in this paper. The next section describes a training algorithm for one variety of connectionist networks which use TSVB, called the Simple Synchrony Network (SSN). This is followed by results on experiments with a recursive grammar (taken from <ref> [2] </ref>), testing the SSN on its ability to generalise information learned about words in one constituent to words appearing in different constituents, and also at different levels of embedding. 1 Department of Computer Science, University of Exeter, Prince of Wales Road, EXETER EX4 4PT, pclane@dcs.exeter.ac.uk. 2 SIMPLE SYNCHRONY NETWORKS The Simple <p> The experiments with the SSN described here differ from such work in using a structured output representation, i.e. a parse tree. The experiments use a recursive grammar, as described in <ref> [2] </ref>, to test the ability of SSNs to learn generalisations across syntactic constituents. The grammar generates simple sentences, based on the pattern `noun verb noun', from twelve nouns and eight verbs. Relative clauses use the relative pronoun `who' and the pattern `who verb noun'. <p> This ability of the SSN is a marked improvement on the output representation of the standard SRN. The experimental procedure follows that in <ref> [2] </ref>. The training set of 1370 sentences is formed from 75% simple sentences, with no embeddings, and 25% with one embedding. In addition, two-thirds of nouns are restricted to certain syntactic positions.
Reference: [3] <author> J Henderson, </author> <title> `A connectionist architecture with inherent sys-tematicity', </title> <booktitle> Proceedings of the Eighteenth Conference of the Cognitive Science Society, </booktitle> <address> La Jolla, CA, </address> <year> (1996). </year>
Reference-contexts: Units pulsing in synchrony represent features about one entity; units pulsing asynchronously represent features about different entities. Because link weights are constant across the pulses in a computation step, every entity will have the same computation applied to it. This property is used by Henderson <ref> [3] </ref> to demonstrate that connectionist networks using TSVB inherently generalise information learned about one syntactic constituent to other syntactic constituents, when applied to natural language tasks. <p> For example, a network trained to describe words as nouns when seen only in the subject position of sentences, will generalise that information to the same words appearing in the object position of sentences in the test set. The argument in <ref> [3] </ref> is empirically validated by the results reported in this paper. The next section describes a training algorithm for one variety of connectionist networks which use TSVB, called the Simple Synchrony Network (SSN).
Reference: [4] <author> S Lawrence, C L Giles, and S Fong, </author> <title> `Natural language grammatical inference with recurrent neural networks', </title> <journal> IEEE Trans. on Knowledge and Data Engineering (accepted), </journal> <year> (1998). </year>
Reference-contexts: Again, weight changes must be made to every copy of a link. 3 EXPERIMENTAL RESULTS The ability of SRNs to learn grammatical information from samples of language has been demonstrated in, for example, [1] and <ref> [4] </ref>. The experiments with the SSN described here differ from such work in using a structured output representation, i.e. a parse tree. The experiments use a recursive grammar, as described in [2], to test the ability of SSNs to learn generalisations across syntactic constituents.
Reference: [5] <author> D E Rumelhart, G E Hinton, and R J Williams, </author> <title> `Learning internal representations by error propagation', </title> <editor> In D.E. Rumel-hart and J.L. McClelland, (eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> (1986). </year>
Reference-contexts: Activation then flows into the output units, which compute output activations for every entity. The experiments illustrate how pulsing output units can represent syntactic structure. The algorithm for training SSNs is based upon an extension of Backpropagation Through Time (BPTT) <ref> [5] </ref>. When training a recurrent network, BPTT begins by unfolding the network over time, making a copy of the network for every time step in the sequence. The result is a feed-forward network, with the recurrent links carrying activation between copies of the network at different times.
Reference: [6] <author> L Shastri and V Ajjanagadde, </author> <title> `From simple associations to systematic reasoning: A connectionist representation of rules, variables, and dynamic bindings using temporal synchrony', </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 16, </volume> <pages> 417-494, </pages> <year> (1993). </year> <title> Machine Learning and Data Mining 470 P. </title> <type> Lane </type>
Reference-contexts: 1 INTRODUCTION Temporal Synchrony Variable Binding (TSVB) <ref> [6] </ref> is an extension to the representational power of standard connectionist networks. Units in a standard connectionist network indicate the presence of a feature in a computation step by the strength of their activation. The core idea of TSVB is that units can also pulse within each computation step. <p> This step gives the feed-forward component of the network information about the previous inputs, enabling it to learn about patterns across time. The second element used in the SSN is Temporal Synchrony Variable Binding (TSVB) <ref> [6] </ref>, the core idea of which is that units within each computation step can pulse. These pulsing units can be used to cycle through a set of entities.
References-found: 6

