URL: http://www.cse.unsw.edu.au/~cs9417/QuinlanAAAI96.ps
Refering-URL: http://www.cse.unsw.edu.au/~cs9417/
Root-URL: http://www.cse.unsw.edu.au
Email: quinlan@cs.su.oz.au  
Title: Bagging, Boosting, and C4.5  
Author: J. R. Quinlan 
Address: Sydney Sydney, Australia 2006  
Affiliation: University of  
Abstract: Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> 1996. </year> <title> Bagging predictors. Machine Learning, </title> <publisher> forthcoming. </publisher>
Reference-contexts: The data for classifier learning systems consists of attribute-value vectors or instances. Both bootstrap aggregating or bagging <ref> (Breiman 1996) </ref> and boosting (Freund and Schapire 1996a) manipulate the training data in order to generate different classifiers. Bagging produces replicate training sets by sampling with replacement from the training instances.
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. </author> <year> 1984. </year> <title> Classification and regression trees. </title> <address> Bel-mont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: To classify an instance x, a vote for class k is recorded by every classifier for which C t (x) = k and C fl (x) is then the class with the most votes (ties being resolved arbitrarily). Using CART <ref> (Breiman, Friedman, Olshen, and Stone 1984) </ref> as the learning system, Breiman (1996) reports results of bagging on seven moderate-sized datasets.
Reference: <author> Brodley, C. E. </author> <year> 1993. </year> <title> Addressing the selective superiority problem: automatic algorithm/model class selection. </title> <booktitle> In Proceedings 10th International Conference on Machine Learning, </booktitle> <pages> 17-24. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W. L. </author> <year> 1991. </year> <title> Learning classification trees. </title> <editor> In Hand, D. J. (ed), </editor> <booktitle> Artificial Intelligence Frontiers in Statistics, </booktitle> <pages> 182-201. </pages> <address> London: </address> <publisher> Chapman & Hall. </publisher>
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> Megainduction: a test flight. </title> <booktitle> In Proceedings 8th International Workshop on Machine Learning, </booktitle> <pages> 596-599. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chan, P. K. and Stolfo, S. J. </author> <year> 1995. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proceedings 12th International Conference on Machine Learning, </booktitle> <pages> 90-98. </pages> <address> San Francisco: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Dietterich, T. G., and Bakiri, G. </author> <year> 1995. </year> <title> Solving mul-ticlass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research 2: </journal> <pages> 263-286. </pages>
Reference-contexts: Heath, Kasif, and Salzberg 1993), and counting op erations (Murphy and Pazzani 1991; Zheng 1995). * Use of error-correcting codes when there are more than two classes <ref> (Dietterich and Bakiri 1995) </ref>. * Decision trees that incorporate classifiers of other kinds (Brodley 1993; Ting 1994). * Automatic methods for setting learning system pa rameters (Kohavi and John 1995).
Reference: <author> Freund, Y., and Schapire, R. E. </author> <year> 1996a. </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting.Unpublished manuscript, available from the authors' home pages ("http://www.research. att.com/orgs/ssr/people/fyoav,schapireg"). An extended abstract appears in Computational Learning Theory: </title> <booktitle> Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> 23-27, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The data for classifier learning systems consists of attribute-value vectors or instances. Both bootstrap aggregating or bagging (Breiman 1996) and boosting <ref> (Freund and Schapire 1996a) </ref> manipulate the training data in order to generate different classifiers. Bagging produces replicate training sets by sampling with replacement from the training instances. <p> Breiman notes: "The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy." Boosting The version of boosting investigated in this paper is AdaBoost.M1 <ref> (Freund and Schapire 1996a) </ref>. Instead of drawing a succession of independent bootstrap samples from the original instances, boosting maintains a weight for each instance the higher the weight, the more the instance influences the classifier learned.
Reference: <author> Freund, Y., and Schapire, R. E. </author> <year> 1996b. </year> <title> Experiments with a new boosting algorithm. </title> <type> Unpublished manuscript. </type>
Reference: <author> Heath, D., Kasif, S., and Salzberg, S. </author> <year> 1993. </year> <title> Learning oblique decision trees. </title> <booktitle> In Proceedings 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1002-1007. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., and John, G. H. </author> <year> 1995. </year> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Proceedings 12th International Conference on Machine Learning, </booktitle> <pages> 304-311. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <editor> Murphy, P. M., and Pazzani, M. J. </editor> <year> 1991. </year> <title> ID2-of-3: constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proceedings 8th International Workshop on Machine Learning, </booktitle> <pages> 183-187. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Salzberg 1993), and counting op erations (Murphy and Pazzani 1991; Zheng 1995). * Use of error-correcting codes when there are more than two classes (Dietterich and Bakiri 1995). * Decision trees that incorporate classifiers of other kinds (Brodley 1993; Ting 1994). * Automatic methods for setting learning system pa rameters <ref> (Kohavi and John 1995) </ref>. On typical datasets, all have been shown to lead to more accurate classifiers at the cost of additional computation that ranges from modest to substantial. There has recently been renewed interest in increasing accuracy by generating and aggregating multiple classifiers.
Reference: <author> Quinlan, J. R. </author> <year> 1987. </year> <title> Inductive knowledge acquisition: a case study. </title> <editor> In Quinlan, J. R. (ed), </editor> <booktitle> Applications of Expert Systems. </booktitle> <address> Wokingham, UK: </address> <publisher> Addison Wesley. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In bagging, each component classifier has the same vote, while boosting assigns different voting strengths to component classifiers on the basis of their accuracy. This paper examines the application of bagging and boosting to C4.5 <ref> (Quinlan 1993) </ref>, a system that learns decision tree classifiers. After a brief summary of both methods, comparative results on a substantial number of datasets are reported. Although boosting generally increases accuracy, it leads to a deterioration on some datasets; further experiments probe the reason for this.
Reference: <author> Ragavan, H., and Rendell, L. </author> <year> 1993. </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proceedings 10th International Conference on Machine Learning, </booktitle> <pages> 252-259. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: easily measured (as opposed to intelligibility, which is more subjective), while the rapid increase in computers' performance/cost ratio has de-emphasized computational issues in most applications. 1 In the active subarea of learning decision tree classifiers, examples of methods that improve accuracy are: * Construction of multi-attribute tests using logical combinations <ref> (Ragavan and Rendell 1993) </ref>, arithmetic combinations (Utgoff and Brodley 1990; 1 For extremely large datasets, however, learning time can remain the dominant issue (Catlett 1991; Chan and Stolfo 1995).
Reference: <author> Ting, K. M. </author> <year> 1994. </year> <title> The problem of small disjuncts: its remedy in decision trees. </title> <booktitle> In Proceedings 10th Canadian Conference on Artificial Intelligence, </booktitle> <pages> 91-97. </pages>
Reference: <author> Utgoff, P. E., and Brodley, C. E. </author> <year> 1990. </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings 7th International Conference on Machine Learning, </booktitle> <pages> 58-65. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vapnik, V. </author> <year> 1983. </year> <title> Estimation of Dependences Based on Empirical Data. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Zheng, Z. </author> <year> 1995. </year> <title> Constructing nominal X-of-N attributes. </title> <booktitle> In Proceedings 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1064-1070. </pages> <address> San Fran-cisco: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 18

