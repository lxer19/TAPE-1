URL: http://www.cis.hut.fi/~mikkok/intro.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00385.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Using Self-Organizing Maps and Learning Vector Quantization for Mixture Density Hidden Markov Models  
Author: MIKKO KURIMO 
Degree: Thesis for the degree of Doctor of Technology to be presented with due permission for  
Date: 1997  
Address: P.O.Box 2200 FIN-02015 HUT, Finland  ESPOO  
Affiliation: Helsinki University of Technology Neural Networks Research Centre  public  Helsinki University of Technology Department of Computer Science and Engineering Laboratory of Computer and Information Science  
Note: Ma 87 UDC 681.327.12:534.75 ACTA POLYTECHNICA SCANDINAVICA MATHEMATICS, COMPUTING AND MANAGEMENT IN ENGINEERING SERIES No. 87  examination and criticism in Auditorium F1 of the Helsinki University of Technology on the 3rd of October, at 12 o'clock noon.  
Abstract-found: 0
Intro-found: 1
Reference: <author> Amari, S. </author> <year> (1967). </year> <title> A theory of adaptive pattern classiters. </title> <journal> IEEE Transactions on Electronic Computers, 16(3):299307. </journal>
Reference-contexts: For decisions between multiple classes (M) there exist several possibilities. In (Juang and Katagiri, 1992) the misclassitcation of x 2 C k is measured by d k (x) = g k (x) + [ M 1 i;i6=k which is a continuous extension of the one used in <ref> (Amari, 1967) </ref>, where only the discriminant dierences between the correct class and the confusing classes (g i &gt; g k ) are averaged. In (12) is a positive number to control the relative eect between larger and smaller discriminants.
Reference: <author> Bahl, L., Brown, P., de Souza, P., and Mercer, R. </author> <year> (1986). </year> <title> Maximum mutual information estimation of hidden Markov model parameters for speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 4952. </pages>
Reference-contexts: The ML criterion is actually not particularly eective to minimize the number of misclassitcations, but it was used in the experiments to compare initializations, anyhow, because the advanced training algorithms like the segmental GPD (Juang and Katagiri, 1992) or the maximum mutual information (MMI) <ref> (Bahl et al., 1986) </ref> are even more vulnerable to bad initializations. Actually, the normal procedure for HMM training is to apply GPD or MMI only for models trst trained by ML methods (in Figure 3 GPD or MMI would substitute the corrective training).
Reference: <author> Bahl, L., Brown, P., de Souza, P., and Mercer, R. </author> <year> (1988). </year> <title> A new algorithm for the estimation of hidden Markov model parameters. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 493496. </pages>
Reference-contexts: The actual training loop (Figure 3) consists of alternating segmentation and likelihood maximization 28 phases (Rabiner, 1989). When the HMMs are ready their performance can be tuned by corrective training <ref> (Bahl et al., 1988) </ref>. The investigations to apply SOM for the training of MDHMMs were motivated mainly by the following three ideas and needs: 1.
Reference: <author> Baker, J. M. </author> <year> (1975). </year> <title> The DRAGON system an overview. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, 23(1):2429. </journal>
Reference: <author> Baldi, P. and Chauvin, Y. </author> <year> (1996). </year> <title> Hybrid modeling, HMM/NN architectures, and protein applications. Neural Computation, </title> <publisher> 8:15411561. </publisher>
Reference-contexts: The majority of applications have been in speech recognition (Rabiner, 1989; Juang and Rabiner, 1991), but successive results are also reported in other telds, for example, in the handwritten script recognition (Cho and Kim, 1995) and in the modeling of the protein chains <ref> (Baldi and Chauvin, 1996) </ref>. Assumptions and detnitions. The basic assumption in the Markov models is that the system has a tnite number of possible states, but it can occupy only one of them at a time.
Reference: <author> Bauer, H.-U., Der, R., and Herrmann, M. </author> <year> (1996). </year> <title> Controlling the magnitcation factor of self-organizing feature maps. Neural Computation, </title> <publisher> 8(4):757771. </publisher>
Reference: <author> Baum, L. </author> <year> (1972). </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. Inequalities, </title> <publisher> 3:18. </publisher>
Reference-contexts: q 0 t=1 Since the generating state sequence is unknown the actual probability of the observation sequence for the model is Pr (Oj) = q T Y a q t1 q t b q t (O t ) : (18) The probability (18) can be computed using the forward-backward procedure <ref> (Baum, 1972) </ref>. The dynamic programming by Viterbi algorithm (Forney, 1973) is commonly used to decode the most likely state sequence behind the observations by maximizing recursively the probability Pr (O; qj) (17).
Reference: <author> Baum, L. and Petrie, T. </author> <year> (1966). </year> <title> Statistical inference for probabilistic functions of tnite state Markov chains. </title> <journal> Annals of Mathematical Statistics, 37:15541563. </journal>
Reference-contexts: The estimation of the HMM parameters using the maximum likelihood (ML) criterion, i.e. maximization of Pr (Oj) over , is done using the Baum-Welch algorithm <ref> (Baum and Petrie, 1966) </ref>. Anyhow, a simpler ML training can be obtained by replacing the maximization of Pr (Oj) by the maximization of the likelihood of the most probable state sequence obtained by the Viterbi search.
Reference: <author> Bellegarda, J. and Nahamoo, D. </author> <year> (1990). </year> <title> Tied mixture continuous parameter modeling for speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, 38(12):20332045. </journal>
Reference: <author> Bocchieri, E. </author> <year> (1993). </year> <title> Vector quantization for the ecient computation of continuous density likelihoods. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 2, </volume> <pages> pages 692 695. </pages>
Reference-contexts: This can be done by clustering the Gaussians after the HMM training <ref> (Bocchieri, 1993) </ref>, but by the SOM this is conveniently incorporated in the codebook training. Suggestions here for search that exploit the SOM structure include the topological and the tree K-best search. These methods also utilize the correlation between the successive feature vectors.
Reference: <author> Bourlard, H. </author> <year> (1995). </year> <title> Towards increasing speech recognition error rates. </title> <booktitle> In Proceedings of 4th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 883894, </pages> <address> Madrid, Spain. </address>
Reference-contexts: Publication 6 compares then the results of some options for the total training process. Some of the main alternative HMM training methods have been presented, for example in (Kurimo, 1994) and <ref> (Morgan and Bourlard, 1995) </ref>. 2.3.2 Limitations and gains In theoretical considerations it seem to exist several limitations of the applicability of HMMs for the phoneme modeling. First, the assumption of phonemes as the invariable speech units is insucient. <p> any intelligible insights into the system. 2.4 Overview of speech recognition applications Considerable progress has been made in ASR technology in the recent years (Young, 1996) and many new products have been launched, but still completely new approaches need to be developed before robust, general-purpose speech rec-ognizers will be available <ref> (Bourlard, 1995) </ref>. Current applications work only in relatively controlled environments and well-specited domains. For example, almost all applications are still restricted to the recognition of distinct words from a predetned vocabulary. Anyhow, such applications can be rather helpful in certain tasks.
Reference: <author> Bourlard, H. and Wellekens, C. J. </author> <year> (1990). </year> <title> Links between Markov models and multilayer perceptrons. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(12):11671178. </journal> <volume> 46 Bradburn, </volume> <editor> D. </editor> <year> (1989). </year> <title> Reducing transmission error eects using a self-organizing network. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), </booktitle> <volume> volume 2, </volume> <pages> pages 531537, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE Service Center. </publisher>
Reference: <author> Chang, P.-C. and Juang, B.-H. </author> <year> (1992). </year> <title> Discriminative template training for dynamic programming speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 493496, </pages> <address> San Francisco,USA. </address>
Reference: <author> Cho, S.-B. and Kim, J. H. </author> <year> (1995). </year> <title> An HMM/MLP architecture for sequence recognition. Neural Computation, </title> <publisher> 7:358369. </publisher>
Reference-contexts: The majority of applications have been in speech recognition (Rabiner, 1989; Juang and Rabiner, 1991), but successive results are also reported in other telds, for example, in the handwritten script recognition <ref> (Cho and Kim, 1995) </ref> and in the modeling of the protein chains (Baldi and Chauvin, 1996). Assumptions and detnitions. The basic assumption in the Markov models is that the system has a tnite number of possible states, but it can occupy only one of them at a time.
Reference: <author> Chou, W., Juang, B., and Lee, C. </author> <year> (1992). </year> <title> Segmental GPD training of HMM based speech recognizer. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 473476, </pages> <address> San Francisco,USA. </address>
Reference-contexts: 3.3.5 Relations to other corrective training algorithms The relations and mutual dierences of the segmental LVQ3 and other corrective training algorithms for HMMs are discussed in Publication 6 with emphasis on the direct error corrective training algorithms (Bahl et al., 1988; Mizuta and Nakajima, 1990), and the application of GPD <ref> (Chou et al., 1992) </ref>. Because of some observed weaknesses in the traditional HMM training methods, which, however, have to a large extent been eliminated from the newer models, there has been suggested a wide range of other training algorithms, more or less related to the discriminative training.
Reference: <author> Cottrell, M. and Fort, J.-C. </author> <year> (1987). </year> <title> tude d'un processus d'auto-organisation. </title>
Reference: <author> Annales de l'Institut Henri Poincar, 23(1):120. </author> <note> (in French). </note>
Reference: <editor> Cottrell, M., Fort, J.-C., and Pages, G. </editor> <year> (1997). </year> <title> Theoretical aspects of the SOM algorithm. </title> <booktitle> In Workshop on Self-Organizing Maps, </booktitle> <pages> pages 246267, </pages> <address> Espoo, Finland. </address>
Reference: <author> Digalakis, V., Monaco, P., and Murveit, H. </author> <year> (1996). </year> <title> Genones: Generalized mixture tying in continuous hidden Markov model-based speech recognizers. </title> <journal> IEEE Transactions on Speech and Audio Processing, 4(4):281289. </journal>
Reference: <author> Dugast, C., Devillers, L., and Aubert, X. </author> <year> (1994). </year> <title> Combining TDNN and HMM in a hybrid system for improved continuous-speech recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, 2(1):217223. </journal>
Reference-contexts: The densities can be approximated by, for example, multilayer perceptrons (MLPs) (Bourlard and Wellekens, 1990; Franzini et al., 1990; Renals et al., 1994), time delay neural networks (TDNNs) <ref> (Dugast et al., 1994) </ref> or radial basis function networks (RBFs) (Huang and Lippmann, 1991; Singer and Lippmann, 1992; Renals et al., 1994). 22 The Gaussian RBFs are actually very close to Gaussian kernel density estimators; the dierence is only in the way they are trained (Renals et al., 1991; Renals et
Reference: <author> Erwin, E., Obermayer, K., and Schulten, K. </author> <year> (1992a). </year> <title> Self-organizing maps: Ordering, convergence properties and energy functions. </title> <journal> Biological Cybernetics, 67(1):4755. </journal>
Reference: <author> Erwin, E., Obermayer, K., and Schulten, K. </author> <year> (1992b). </year> <title> Self-organizing maps: Stationary states, metastability and convergence rate. </title> <journal> Biological Cybernetics, 67(1):3545. </journal>
Reference-contexts: (x) D D+2 , where D is the dimension of the input vectors (Zador, 1982; Cottrell et al., 1997). 14 Depending on the shape of the neighborhood function, there may exist stationary states other than the ordered state (4) as can be shown for M = D = 1 SOMs <ref> (Erwin et al., 1992b) </ref>. Due to the meta-stable states the ordering time may increase by orders of magnitude, if too narrow pre-specited neighborhood function is used throughout the process.
Reference: <author> Feller, W. </author> <year> (1966). </year> <title> An Introduction to Probability Theory and its Applications, volume II. </title> <publisher> John Wiley & Sons Inc., </publisher> <address> New York. </address>
Reference-contexts: a codebook organized into a 14x10 SOM grid for one randomly selected input vector. 3.2.4 Characteristics of the mixture density approximation by SOM Any bounded continuous PDF can be approximated with an arbitrary accuracy using a weighted sum of Gaussian densities, if the number of the Gaussians is large enough <ref> (Feller, 1966) </ref>. If no prior knowledge of the functional form of the PDF is assumed, the mixture density estimation can be reduced to weighted kernel density estimation with, for example, Gaussian kernel functions.
Reference: <author> Flanagan, J. A. </author> <year> (1996). </year> <title> Self-organisation in Kohonen's SOM. Neural Networks, 9(7):11851197. 47 Forney, </title> <editor> G. D. </editor> <year> (1973). </year> <title> The Viterbi algorithm. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 61(3):268 278. </address>
Reference-contexts: For example, for M = D &gt; 1 SOMs organized into a rectangular grid and input x 2 [0; 1] D , it is shown in <ref> (Flanagan, 1996) </ref> that the SOM will attain a dimension-wise ordered state (5) in a tnite time with probability one. However, since no absorbing states for the general M = D &gt; 1 SOMs are known, the process will also abandon that state in a tnite time (Fort and Pages, 1996). <p> However, since no absorbing states for the general M = D &gt; 1 SOMs are known, the process will also abandon that state in a tnite time (Fort and Pages, 1996). According to <ref> (Flanagan, 1996) </ref>, the M = D &gt; 1 SOM is in a dimension-wise ordered state, if for all input dimensions r = 1; : : : ; M holds either m ir &lt; m jr &lt; m kr or m ir &gt; m jr &gt; m kr (5) for any triple
Reference: <editor> Fort, J.-C. and Pages, G. </editor> <year> (1996). </year> <title> About the Kohonen algorithm: Strong or weak self-organization? Neural Networks, </title> <publisher> 9(5):773785. </publisher>
Reference-contexts: However, since no absorbing states for the general M = D &gt; 1 SOMs are known, the process will also abandon that state in a tnite time <ref> (Fort and Pages, 1996) </ref>.
Reference: <author> Franzini, M., Lee, K.-F., and Waibel, A. </author> <year> (1990). </year> <title> Connectionist Viterbi training: a new hybrid method for continuous speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 425428, </pages> <address> Albuquerque, NM. </address>
Reference: <author> Friedman, J., Baskett, F., and Shustek, L. </author> <year> (1975). </year> <title> An algorithm for tnding nearest neighbors. </title> <journal> IEEE Transactions on Computers, 24:10001006. </journal>
Reference-contexts: A complete search through the codebook is performed periodically to react for abrupt feature changes (Lopez-Gonzalo and Hernandez-Gomez, 1993). In (Lampinen and Oja, 1989) the local search process starts from an initial guess based on the best match approximation obtained by the projections to the trst few vector components <ref> (Friedman et al., 1975) </ref> and in (Zhao and Rowden, 1991) the search begins always from the center of the codebook. In (Kohonen, 1996; Kohonen et al., 1996c) a pointer to the previous best match is assigned for each training vector, which are used several times in the training.
Reference: <author> Galindo, P. L. </author> <year> (1995). </year> <title> A competitive algorithm for training HMM for speech recognition. </title> <booktitle> In Proceedings of 4th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 21872190, </pages> <address> Madrid, Spain. </address>
Reference: <author> Gauvain, J.-L. and Lee, C.-H. </author> <year> (1994). </year> <title> Maximum a posteriori estimation for multi-variate Gaussian mixture observations of Markov chains. </title> <journal> IEEE Transactions on Speech and Audio Processing, 2(2):291298. </journal>
Reference: <author> Gillick, L. and Cox, S. </author> <year> (1989). </year> <title> Some statistical issues in the comparison of speech recognition algorithms. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 532535, </pages> <address> Glasgow, Scotland. </address>
Reference-contexts: This is done by selecting the model which gives the lowest average error rate after a long training session. The tested model is assumed to be near enough to the benchmark when the average recognition result is not signitcantly dierent (tested by the matched-pairs and the McNemar's statistical tests <ref> (Gillick and Cox, 1989) </ref>) from the benchmark. The results in Publication 6 show that there are remarkable dierences depending on the applied initialization methods. By the criteria described above, the SOM and LVQ initialization methods suit best to the MDHMMs.
Reference: <author> Gorin, A. and Mammone, R. J. </author> <year> (1994). </year> <title> Introduction to the special issue on neural networks for speech processing. </title> <journal> IEEE Transactions on Speech and Audio Processing, 2(1):113114. </journal>
Reference-contexts: As known for neural computation in general, very complex systems can be modeled in appropriate accuracy by connecting a sucient amount of simple units <ref> (Gorin and Mammone, 1994) </ref>. Special care must, however, be taken to control that there is enough training data and the data represents the task well. Otherwise the huge models will be over-ttted and will generalize poorly to independent test data.
Reference: <author> Hmlinen, A. </author> <year> (1995). </year> <title> Self-organizing Map and Reduced Kernel Density Estimation. </title> <type> PhD thesis, </type> <institution> University of Jyvskyl, Jyvskyl, Finland. </institution>
Reference-contexts: In addition to aiding in some VQ applications, this oers a way to approximate the probability density function (PDF) of the input. The PDF approximation can be used directly in maximum likelihood classitcation of static vectors <ref> (Hmlinen, 1995) </ref> or, combined with state transition probabilities, to classify pattern sequences in the HMM framework as explained in this work (Section 3.2.4).
Reference: <author> Holmstrm, L. and Hmlinen, A. </author> <year> (1993). </year> <title> The self-organizing reduced kernel density estimator. </title> <booktitle> In Proceedings of the International Conference on Neural Networks (ICNN), </booktitle> <pages> pages 417421, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE Service Center. </publisher>
Reference: <author> Huang, W. Y. and Lippmann, R. P. </author> <year> (1991). </year> <title> HMM speech recognition with neural net discrimination. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 194 202. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Huang, X. and Jack, M. </author> <year> (1989). </year> <title> Semi-continuous hidden Markov models for speech signals. Computer Speech and Language, </title> <journal> 3(3):239252. </journal> <note> 48 Huo, </note> <author> Q., Chan, C., and Lee, C.-H. </author> <year> (1995). </year> <title> Bayesian adaptive learning of the parameters of hidden Markov model for speech recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, 3(5):334345. </journal>
Reference: <author> Iwamida, H., Katagiri, S., McDermott, E., and Tohkura, Y. </author> <year> (1990). </year> <title> A hybrid speech recognition system using HMMs with an LVQ-trained codebook. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 489492. </pages>
Reference: <author> Jalanko, M. </author> <year> (1980). </year> <title> Studies of Learning Projective Methods in Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution>
Reference-contexts: The system was able to segment and label speech into phoneme classes using the Learning Subspace Method producing phonemic strings which were correct to 81 % for one speaker <ref> (Jalanko, 1980) </ref>. The corresponding word recognition accuracy of 94 % was obtained by this system in a closed thousand-word vocabulary applying a suitable post-processing.
Reference: <author> Jelinek, F. </author> <year> (1976). </year> <title> Continuous speech recognition by statistical methods. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 64(4):532536. </address>
Reference: <author> Jelinek, F. and Mercer, R. </author> <year> (1980). </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of an International Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands. </address> <publisher> North-Holland. </publisher>
Reference: <author> Juang, B.-H. </author> <year> (1985). </year> <title> Maximum likelihood estimation for mixture multivari-ate stochastic observation of Markov chains. </title> <journal> AT&T Technical Journal, 64(6):12351249. </journal>
Reference-contexts: It can be shown <ref> (Juang, 1985) </ref> that the maximum likelihood HMM training will eventually lead to at least locally optimal parameter values, but it is the duty of the initialization to make sure that the obtained result will be as close as possible to the global optimum.
Reference: <author> Juang, B.-H. and Katagiri, S. </author> <year> (1992). </year> <title> Discriminative learning for minimum error classitcation. </title> <journal> IEEE Transactions on Signal Processing, 40(12):30433054. </journal>
Reference-contexts: It is also possible to use an appropriately weighted sum of contributions of several closest references or reference sequences as in (Chang and Juang, 1992; McDermott and Katagiri, 1994). For decisions between multiple classes (M) there exist several possibilities. In <ref> (Juang and Katagiri, 1992) </ref> the misclassitcation of x 2 C k is measured by d k (x) = g k (x) + [ M 1 i;i6=k which is a continuous extension of the one used in (Amari, 1967), where only the discriminant dierences between the correct class and the confusing classes <p> In (12) is a positive number to control the relative eect between larger and smaller discriminants. In <ref> (Juang and Katagiri, 1992) </ref> detnition (12) is used to detne a framework of discriminative training methods called GPD (Generalized Probabilistic Descent) to minimize the expected cost of the misclassitcations. <p> The ML criterion is actually not particularly eective to minimize the number of misclassitcations, but it was used in the experiments to compare initializations, anyhow, because the advanced training algorithms like the segmental GPD <ref> (Juang and Katagiri, 1992) </ref> or the maximum mutual information (MMI) (Bahl et al., 1986) are even more vulnerable to bad initializations.
Reference: <author> Juang, B.-H. and Rabiner, L. R. </author> <year> (1990). </year> <title> The segmental K-means algorithm for estimating parameters of hidden Markov models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, 38(9):16391641. </journal>
Reference-contexts: For the analysis of the convergence of the suggested MDHMM training method mostly the same guidelines are valid as for the segmental K-means <ref> (Juang and Rabiner, 1990) </ref>. The dierence between the segmental K-means and segmental SOM is the same as between the normal K-means (MacQueen, 1967) and the normal batch SOM (Kohonen, 1995) as analyzed in (Luttrell, 1990), for example.
Reference: <author> Juang, B.-H. and Rabiner, L. R. </author> <year> (1991). </year> <title> Hidden Markov models for speech recognition. </title> <journal> Technometrics, 33(3):251272. </journal>
Reference: <author> Kangas, J. </author> <year> (1995). </year> <title> Increasing the error tolerance in transmission of vector quantized images by self-organizing maps. </title> <editor> In Fogelman-Souli, F. and Gallinari, P., editors, </editor> <booktitle> Proceedings of ICANN'95, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 287291. </pages> <note> EC2 et Cie. </note>
Reference: <author> Kapadia, S., Valtchev, V., and Young, S. </author> <year> (1993). </year> <title> MMI training for continuous phoneme recognition on the TIMIT database. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 2, </volume> <pages> pages 491494. </pages>
Reference: <author> Kaski, S. </author> <year> (1996). </year> <title> Data Exploration Using Self-Organizing Maps. </title> <type> PhD thesis, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution> <note> 49 Kasslin, </note> <editor> M., Kangas, J., and Simula, O. </editor> <year> (1992). </year> <title> Process state monitoring using self-organizing maps. </title> <editor> In Aleksander, I. and Taylor, J., editors, </editor> <booktitle> Artitcial Neural Networks, </booktitle> <volume> 2, volume I, </volume> <pages> pages 15321534, </pages> <address> Amsterdam, Netherlands. </address> <publisher> North-Holland. </publisher>
Reference-contexts: It is also possible to use SOM to tnd out the most signitcant data components that aect the ordering by viewing the component planes of the SOM weight vectors. The ability to visualize multidimensional data is exploited in many application areas, for example, data mining <ref> (Kaski, 1996) </ref> and process analysis (Kasslin et al., 1992). The probability density of the input space is projected to the SOM by the point density of units such that the SOM weight vectors are at the densest in the areas where the most accurate vector quantization is needed.
Reference: <author> Katagiri, S. and Lee, C.-H. </author> <year> (1993). </year> <title> A new hybrid algorithm for speech recognition based on HMM segmentation and learning vector quantization. </title> <journal> IEEE Transactions on Speech and Audio Processing, 1(4):421430. </journal>
Reference-contexts: For example, if the reference vectors are carefully initialized, most of the few samples that still satisfy the other update conditions of the LVQ2, exist already around the boundary area <ref> (Katagiri and Lee, 1993) </ref>. <p> The selection of the nearest reference and the parameter adjustment formulas are normally expressed using the Eu-clidean distance metric, but corresponding expressions can be derived for other metrics that may be more suitable for certain applications <ref> (Katagiri and Lee, 1993) </ref>. Measuring the misclassitcation. In LVQ and stochastic gradient descent algorithms the number of expected classitcation errors is minimized using mis-classitcations or near-misses observed in stochastic training samples.
Reference: <author> Katagiri, S., Lee, C.-H., and Juang, B.-H. </author> <year> (1991). </year> <title> New discriminative training algorithms based on the generalized probabilistic descent method. </title> <booktitle> In Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 298308, </pages> <address> Princeton, New Jersey, USA. </address>
Reference-contexts: In the extreme case of (12), where ! 1, only the largest discriminant (class C i ) aects the misclassi-tcation measure d k (x) = g k (x) + max g i (x) : (13) This approach leads to the LVQ2 algorithm (Komori and Katagiri, 1992). In <ref> (Katagiri et al., 1991) </ref> it is shown that the practically ecient simple LVQ2 actually does approximate well the more complicated gradient search GPD imple mentations. Why LVQ? The minimization of classitcation errors is the main objective in most pattern recognition applications.
Reference: <author> Kim, D.-S., Lee, S.-Y., Han, M.-S., Lee, C.-H., Park, J.-G., and Suh, S.-W. </author> <year> (1994). </year> <title> Multi-dimensional HMM parameter estimation using self-organizing feature map for speech recognition. </title> <booktitle> In Proceedings of the 3rd International Conference on Fuzzy Logic, Neural Nets and Soft Computing, </booktitle> <pages> pages 541542, </pages> <address> Iizuka, Japan. </address> <booktitle> Fuzzy Logic Systems Institute. </booktitle>
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Clustering, taxonomy, and topological maps of patterns. </title> <booktitle> In Proc. 6ICPR, Int. Conf. on Pattern Recognition, </booktitle> <pages> pages 114128, </pages> <address> Washington, DC. </address> <publisher> IEEE Computer Soc. Press. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1986a). </year> <title> Dynamically expanding context, with application to the correction of symbol strings in recognition of continuous speech. </title> <booktitle> In Proceedings of the 8th International Conference on Pattern Recognition, </booktitle> <pages> pages 11481151, </pages> <address> Paris, France. </address>
Reference-contexts: The post-processing can exploit language dependent syntax and point out uncommon phoneme combinations from the raw phoneme sequences. An optional post-processing module in the applied ASR system is based on the Dynamically Expanding Context algorithm (DEC) 26 <ref> (Kohonen, 1986a) </ref>.The recognition of long phoneme versions like /AA/ from their short counterparts is a source of some frequent errors, as well. Here, the distinction is made using phoneme dependent duration limits learned iteratively during the model training. This simple separation does not take any context information into account.
Reference: <author> Kohonen, T. </author> <year> (1986b). </year> <title> Learning vector quantization for pattern recognition. </title> <type> Report TKK-F-A601, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution>
Reference: <author> Kohonen, T. </author> <year> (1990a). </year> <title> Improved versions of learning vector quantization. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), </booktitle> <volume> volume 1, </volume> <pages> pages 545550, </pages> <address> San Diego, California. </address>
Reference-contexts: LVQ2. The version described here as LVQ2 corresponds actually the LVQ2.1 <ref> (Kohonen, 1990a) </ref>. For each stochastic input sample x 2 R D , the adjustments are performed for the two best-matching codebook vectors m c and m c 0 , which are found using the minimum Euclidean distance criterion (1).
Reference: <author> Kohonen, T. </author> <year> (1990b). </year> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 78(9):14641480. </address>
Reference-contexts: The most familiar version of the algorithm <ref> (Kohonen, 1990b) </ref> uses the minimum Euclidean distance criterion for D-dimensional stochastic sample x 2 R D to tnd the winner unit c (the best-matching unit, BMU) of the total of N SOM units. <p> The value of * should reect the width of the adjustment window around the border between classes c and c 0 so that with a narrow window the stabilizing learning steps (10) are small (i.e. * is small) <ref> (Kohonen, 1990b) </ref>. In some cases the window constraint is unnecessary and it can be removed and thus * = 1 applied as suggested in (Kohonen, 1995). <p> This is to avoid the risk of improper step sizes, which might follow from the occurrence of both severe and slight errors in the same word. Another dierence is that the method applies the LVQ2-type learning law <ref> (Kohonen, 1990b) </ref> only to the detected actual misrecognition cases. Thus no tuning occurs, if the decoded phonemes match to the correct phoneme segmentation obtained by constraining the Viterbi search to the known correct transcription from the word list.
Reference: <author> Kohonen, T. </author> <year> (1991). </year> <title> Workstation-based phonetic typewriter. </title> <booktitle> In Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 279 288, </pages> <address> Princeton, New Jersey, USA. </address>
Reference: <author> Kohonen, T. </author> <year> (1992). </year> <title> New developments of learning vector quantization and the self-organizing map. </title> <booktitle> In Symposium on Neural Networks; Alliances and Perspectives in Senri (SYNAPSE'92), </booktitle> <address> Osaka, Japan. </address> <institution> Senri Int. Information Institute. 50 Kohonen, T. </institution> <year> (1993). </year> <title> Things you haven't heard about the self-organizing map. </title> <booktitle> In Proceedings of the International Conference on Neural Networks (ICNN), </booktitle> <pages> pages 11471156, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE Service Center. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1995). </year> <title> Self-Organizing Maps. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: The value of the neighborhood function h ci (t) can be, for example, the learning rate ff (t) 2]0; 1 [ , if the array distance between m i and m c is smaller than the neighborhood radius r (t) and zero otherwise <ref> (Kohonen, 1995) </ref>. The desired form of convergence and characteristics of the result is ensured by the gradual decrease of ff (t) and r (t) while t increases. <p> The SOM is, however, not intended for optimal classitcation, but the classitcation accuracy of the SOM codebook can be improved by LVQ methods <ref> (Kohonen, 1995) </ref> which are discussed for this work in Sections 2.2 and 3.3. The smoothness of the obtained mapping allows ecient exploitation of the available modeling capacity, when the amount of codebook vectors is much larger than the number of the distinct input clusters. <p> However, in SOM training their eect is not necessarily so bad since the use of training neighborhood provides smoothing also for units that are seldom chosen to be the BMU <ref> (Kohonen, 1995) </ref>. 15 If a set of best matching SOM units, say the top tve ranks, are wanted for the application, the ordering of the units can be used to apply some very fast approximative search methods for large codebooks. <p> An additional constraint for the update is that the sample x should lie in a narrow window between m c and m c 0 <ref> (Kohonen, 1995) </ref>. 16 LVQ3. <p> In some cases the window constraint is unnecessary and it can be removed and thus * = 1 applied as suggested in <ref> (Kohonen, 1995) </ref>. For example, if the reference vectors are carefully initialized, most of the few samples that still satisfy the other update conditions of the LVQ2, exist already around the boundary area (Katagiri and Lee, 1993). <p> Furthermore, the performance is often measured only in a particular independent test data set so that the theoretical convergence properties do not necessarily coincide well with practical results. In the LVQ learning laws <ref> (Kohonen, 1995) </ref> much emphasis has been given to the average performance in dierent dicult practical classitcation experiments in order to ensure that the methods will actually work in various contexts. The classitcation has been determined according to the class of the nearest reference vector as explained in the previous section. <p> The neighborhood function h o;m &gt; 0, if the unit m belongs to the neighborhood of the best-matching unit o of the current codebook (thus h o;m depends on t via the index o). From the dierent kinds of neighborhood functions h o;m <ref> (Kohonen, 1995) </ref>, the simple bubble type is used here for simplicity. <p> The dierence between the segmental K-means and segmental SOM is the same as between the normal K-means (MacQueen, 1967) and the normal batch SOM <ref> (Kohonen, 1995) </ref> as analyzed in (Luttrell, 1990), for example. If the SOM neighborhood size is small enough to ensure the increase of the likelihood of the model in the parameter adaptation steps the direction of the convergence can be expected to be close to that in the segmental K-means. <p> The solution experimented in Publication 1 borrowed the idea of the data based majority voting used commonly in labeling of SOM units, e.g. see <ref> (Kohonen, 1995) </ref>. Despite some possible problems caused by the variation of the number of labels per phoneme classes, the LVQ initialization experiments showed lower error rates compared to using only SOMs or K-means. <p> To overcome such possible convergence problems, the so-called Wegstein moditcation of the parameter adjustments could be applied in the same way as suggested for batch version of SOM <ref> (Kohonen, 1995) </ref>. Since the segmental LVQ3 training method has provided good experimental results (see Publications 4 and 6), it seems to suit well for the segmental training as such, anyhow. The exact adjustment laws of the segmental LVQ3 are presented in Publication 4.
Reference: <author> Kohonen, T. </author> <year> (1996). </year> <title> The speedy SOM. </title> <type> Technical Report A33, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution>
Reference-contexts: This is typical for using SOM for density function approximation purposes as in this work. For example, the search could then begin from the neighborhood of good BMU candidates and gradually proceed towards the direction of improved match values <ref> (Kohonen, 1996) </ref> (see the Section 3.2.5 for a more exact description).
Reference: <author> Kohonen, T., Barna, G., and Chrisley, R. </author> <year> (1988). </year> <title> Statistical pattern recognition with neural networks: Benchmarking studies. </title> <booktitle> In Proceedings of the International Conference on Neural Networks (ICNN), </booktitle> <volume> volume I, </volume> <pages> pages 6168, </pages> <address> Los Alamitos, CA. </address> <publisher> IEEE Computer Soc. Press. </publisher>
Reference: <author> Kohonen, T., Hynninen, J., Kangas, J., and Laaksonen, J. </author> <year> (1996a). </year> <title> SOM_PAK: the self-organizing map programming package. </title> <type> Report A31, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution> <note> Program package available via WWW at URL http://nucleus.hut.t/nnrc/som_pak. </note>
Reference: <author> Kohonen, T., Hynninen, J., Kangas, J., Laaksonen, J., and Torkkola, K. </author> <year> (1996b). </year> <title> LVQ_PAK: the learning vector quantization programming package. </title> <type> Report A30, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution> <note> Program package available via WWW at URL http://nucleus.hut.t/nnrc/lvq_pak. </note>
Reference-contexts: There exist several versions of LVQ described in (Kohonen, 1995; McDermott, 1990; Makino et al., 1992) which have slightly dierent properties. The ones used here are the LVQ2 <ref> (Kohonen et al., 1996b) </ref>, which is ecient for tne tuning the decision borders between the competing classes and the LVQ3 (Kohonen et al., 1996b) which adds a stabilizing term to LVQ2 for improved long-run behavior. LVQ2. The version described here as LVQ2 corresponds actually the LVQ2.1 (Kohonen, 1990a). <p> There exist several versions of LVQ described in (Kohonen, 1995; McDermott, 1990; Makino et al., 1992) which have slightly dierent properties. The ones used here are the LVQ2 <ref> (Kohonen et al., 1996b) </ref>, which is ecient for tne tuning the decision borders between the competing classes and the LVQ3 (Kohonen et al., 1996b) which adds a stabilizing term to LVQ2 for improved long-run behavior. LVQ2. The version described here as LVQ2 corresponds actually the LVQ2.1 (Kohonen, 1990a).
Reference: <author> Kohonen, T., Kaski, S., Lagus, K., and Honkela, T. </author> <year> (1996c). </year> <title> Very large two-level SOM for the browsing of newsgroups. </title> <editor> In von der Malsburg, C., von Seelen, W., Vorbrggen, J. C., and Sendho, B., editors, </editor> <booktitle> Proceedings of ICANN96, International Conference on Artitcial Neural Networks, </booktitle> <address> Bochum, Germany, </address> <month> July 16-19, </month> <year> 1996, </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 1112, </volume> <pages> pages 269 274. </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Kohonen, T., Oja, E., Simula, O., Visa, A., and Kangas, J. </author> <year> (1996d). </year> <title> Engineering application of the self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 84(10):1358 1384. </address>
Reference: <author> Koikkalainen, P. </author> <year> (1995). </year> <title> Fast deterministic self-organizing maps. </title> <editor> In Fogelman-Souli, F. and Gallinari, P., editors, </editor> <booktitle> Proceedings of ICANN'95, International Conference on Artitcial Neural Networks Paris, </booktitle> <address> France, </address> <month> 9-13 October </month> <year> 1995, </year> <booktitle> volume 2, </booktitle> <pages> pages 6368. </pages> <note> EC2 et Cie. </note>
Reference-contexts: In this example the search will tnally cover only 13 units and tnd the trst and second best matches but miss the third. <ref> (Koikkalainen, 1995) </ref>. In Publication 5, only the set of K best matches for the previous observation vector is stored and it is assumed that because the successive observations in the sequence resemble each other, the new set of best matches can be found starting from the previous set.
Reference: <author> Koikkalainen, P. and Oja, E. </author> <year> (1990). </year> <title> Self-organizing hierarchical feature maps. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), </booktitle> <volume> volume II, </volume> <pages> pages 279284, </pages> <address> Piscataway, NJ. </address> <institution> IEEE Service Center. </institution> <note> 51 Komori, </note> <author> T. and Katagiri, S. </author> <year> (1992). </year> <title> GPD training of dynamic programming-based speech recognizers. </title> <journal> J. Acoustical Society of Japan, 13(6):341349. </journal>
Reference-contexts: The largest codebooks tested here have, however, contained only 140 units per phoneme for 80-dimensional inputs, so gains might be more signitcant, if really large SOMs would be required. The tree-search SOM <ref> (Koikkalainen and Oja, 1990) </ref> suits well to the fast approximative search for large codebooks, because the tree structure oers O (log N ) search complexity instead of the normal O (N ).
Reference: <author> Komori, Y., Yamada, M., Yamamoto, H., and Ohora, Y. </author> <year> (1995). </year> <title> An ecient output probability computation for continuous HMM using rough and detailed models. </title> <booktitle> In Proceedings of 4th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 10871090, </pages> <address> Madrid, Spain. </address>
Reference: <author> Kurimo, M. </author> <year> (1992). </year> <title> Combinations of adaptive vector quantization methods and hidden Markov models in speech recognition. </title> <type> Master's thesis, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution> <note> (in Finnish). </note>
Reference: <author> Kurimo, M. </author> <year> (1994). </year> <title> Application of learning vector quantization and self-organizing maps for training continuous density and semi-continuous Markov models. </title> <type> Licentiate's Thesis, </type> <institution> Helsinki University of Technology, Espoo, Fin-land. </institution>
Reference-contexts: The prominent system was used as a basis for several reports (Kurimo, 1992; Mntysalo, 1992; Utela, 1992). The continuous density HMMs were trst applied as a reference method, but after the introduction of the semi-continuous density models and LVQ based training methods the work leading to <ref> (Kurimo, 1994) </ref> and this thesis began to take shape. The motivation for this work has been to enhance the modeling and training methods to decrease the recognition errors produced by the HMM decoding system and study the eects of the extension of the system to use higher dimensional feature vectors. <p> Publication 6 compares then the results of some options for the total training process. Some of the main alternative HMM training methods have been presented, for example in <ref> (Kurimo, 1994) </ref> and (Morgan and Bourlard, 1995). 2.3.2 Limitations and gains In theoretical considerations it seem to exist several limitations of the applicability of HMMs for the phoneme modeling. First, the assumption of phonemes as the invariable speech units is insucient.
Reference: <author> Kurimo, M. </author> <year> (1997). </year> <title> SOM based density function approximation for mixture density HMMs. </title> <booktitle> In Workshop on Self-Organizing Maps, </booktitle> <pages> pages 813, </pages> <address> Espoo, Finland. </address>
Reference-contexts: Some experimental comparisons. According to the ASR experiments reported in Publication 5 and recent checks with 80-dimensional context vectors <ref> (Kurimo, 1997) </ref>, the obtained speed-up compared to the unordered complete search (optimized by PDC as well) is about 2642 % depending on the code-book size and the feature vectors. The corresponding increase of the average number of errors is about 410 %.
Reference: <author> Kurimo, M. and Torkkola, K. </author> <year> (1992a). </year> <title> Application of SOMs and LVQ in training continuous density hidden Markov models. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 543546, </pages> <address> Ban, Canada. </address>
Reference: <author> Kurimo, M. and Torkkola, K. </author> <year> (1992b). </year> <title> Combining LVQ with continuous density hidden Markov models in speech recognition. </title> <booktitle> In Proceedings of the SPIE's Conference on Neural and Stochastic Methods in Image and Signal Processing, </booktitle> <volume> volume 1766, </volume> <pages> pages 726734, </pages> <address> San Diego, USA. </address>
Reference: <author> Lampinen, J. and Oja, E. </author> <year> (1989). </year> <title> Fast self-organization by the probing algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN), </booktitle> <volume> volume II, </volume> <pages> pages 503507, </pages> <address> Piscataway, NJ. </address> <publisher> IEEE Service Center. </publisher>
Reference-contexts: A complete search through the codebook is performed periodically to react for abrupt feature changes (Lopez-Gonzalo and Hernandez-Gomez, 1993). In <ref> (Lampinen and Oja, 1989) </ref> the local search process starts from an initial guess based on the best match approximation obtained by the projections to the trst few vector components (Friedman et al., 1975) and in (Zhao and Rowden, 1991) the search begins always from the center of the codebook.
Reference: <author> Lee, C.-H., Lin, C.-H., and Juang, B.-H. </author> <year> (1990). </year> <title> A study on speaker adaptation of continuous density HMM parameters. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 145148. </pages>
Reference: <author> Liporace, L. A. </author> <year> (1982). </year> <title> Maximum likelihood estimation for multivariate observations of Markov sources. </title> <journal> IEEE Transactions on Information Theory, 28(5):729734. </journal>
Reference: <author> Lippmann, R. </author> <year> (1989). </year> <title> Review of neural networks for speech recognition. Neural Computation, </title> <journal> 1(1):138. </journal> <note> 52 Ljolje, </note> <author> A., Ephraim, Y., and Rabiner, L. </author> <year> (1990). </year> <title> Estimation of hidden Markov model parameters by minimizing empirical error rate. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 709712. </pages>
Reference: <author> Lopez-Gonzalo, E. and Hernandez-Gomez, L. A. </author> <year> (1993). </year> <title> Fast vector quantization using neural maps for CELP at 2400 bps. </title> <booktitle> In Proceedings of 3rd European Conference on Speech Communication and Technology, </booktitle> <volume> volume 1, </volume> <pages> pages 55 58, </pages> <address> Berlin, Germany. </address>
Reference: <author> Luttrell, S. P. </author> <year> (1990). </year> <title> Derivation of a class of training algorithms. </title> <journal> IEEE Trans. on Neural Networks, 1(2):229232. </journal>
Reference-contexts: The dierence between the segmental K-means and segmental SOM is the same as between the normal K-means (MacQueen, 1967) and the normal batch SOM (Kohonen, 1995) as analyzed in <ref> (Luttrell, 1990) </ref>, for example. If the SOM neighborhood size is small enough to ensure the increase of the likelihood of the model in the parameter adaptation steps the direction of the convergence can be expected to be close to that in the segmental K-means.
Reference: <author> MacQueen, J. </author> <year> (1967). </year> <title> Some methods for classitcation and analysis of multivariate observations. </title> <booktitle> In Proceedings of Fifth Berkeley Symposium on Math. Statist. and Prob., </booktitle> <pages> pages 281297. </pages>
Reference-contexts: For the analysis of the convergence of the suggested MDHMM training method mostly the same guidelines are valid as for the segmental K-means (Juang and Rabiner, 1990). The dierence between the segmental K-means and segmental SOM is the same as between the normal K-means <ref> (MacQueen, 1967) </ref> and the normal batch SOM (Kohonen, 1995) as analyzed in (Luttrell, 1990), for example.
Reference: <author> Makhoul, J., Roucos, S., and Gish, H. </author> <year> (1985). </year> <title> Vector quantization in speech coding. </title> <booktitle> Proceedings of IEEE, </booktitle> <address> 73(11):15511588. </address>
Reference-contexts: HMMs which have parametric output density functions are generally called continuous density HMMs (CDHMMs) and those having mixture density functions mixture density HMMs (MDHMMs). Another traditional way is to use vector quantization (VQ) <ref> (Makhoul et al., 1985) </ref> to transform the features into a set of output symbols and then estimate their discrete probability distribution in each state (Rabiner et al., 1983).
Reference: <author> Makino, S., Endo, M., Sone, T., and Kido, K. </author> <year> (1992). </year> <title> Recognition of phonemes in continuous speech using a modited LVQ2 method. </title> <journal> J. Acoustical Society of Japan, 13(6):351360. </journal>
Reference: <author> Mntysalo, J. </author> <year> (1992). </year> <title> Some experiments on LVQ-based speech recognition with high-dimensional context vectors. </title> <type> Master's thesis, </type> <institution> Helsinki University of Technology. </institution> <note> (in Finnish). </note>
Reference: <author> McDermott, E. </author> <year> (1990). </year> <title> LVQ3 for phoneme recognition. </title> <booktitle> In Proc. Spring Meet. Acoust. Soc. Jpn., </booktitle> <pages> pages 151152. </pages>
Reference: <author> McDermott, E. and Katagiri, S. </author> <year> (1994). </year> <title> Prototype-based minimum classitca-tion error/ generalized probabilistic descent training for various speech units. Computer Speech and Language, </title> <publisher> 8(4):351368. </publisher>
Reference: <author> Mizuta, S. and Nakajima, K. </author> <year> (1990). </year> <title> An optimal discriminative training method for continuous mixture density HMMs. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 245248, </pages> <month> Kobe,Japan. </month>
Reference: <author> Monte, E. </author> <year> (1992). </year> <title> Smoothing HMMs by means of a SOM. </title> <booktitle> In Proceedings of 1992 International Conference on Spoken Language Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 535537, </pages> <address> Ban, Canada. </address>
Reference: <author> Morgan, N. and Bourlard, H. </author> <year> (1995). </year> <title> Neural networks for statistical recognition of continuous speech. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 83(5):742770. </address> <note> 53 Mulier, </note> <author> F. and Cherkassky, V. </author> <year> (1995). </year> <title> Self-organization as an iterative kernel smoothing process. Neural Computation, </title> <publisher> 7(6):11651177. </publisher>
Reference-contexts: Publication 6 compares then the results of some options for the total training process. Some of the main alternative HMM training methods have been presented, for example in (Kurimo, 1994) and <ref> (Morgan and Bourlard, 1995) </ref>. 2.3.2 Limitations and gains In theoretical considerations it seem to exist several limitations of the applicability of HMMs for the phoneme modeling. First, the assumption of phonemes as the invariable speech units is insucient.
Reference: <author> Niles, L. T. and Silverman, H. F. </author> <year> (1990). </year> <title> Combining hidden Markov model and neural network classiters. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 417420. </pages>
Reference-contexts: The reason is perhaps that since their structure is so simple and mathematically feasible, the models can be extended easily. It can be shown <ref> (Niles and Silverman, 1990) </ref> that the HMM training is actually very close to some ANN methods. In practice, the complexity of the data and the theoretical limitations of the HMMs can be then partly overcome by brute force, i.e by increasing codebooks, input dimensions, and the number of models.
Reference: <author> Nilsson, N. </author> <year> (1965). </year> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Parzen, E. </author> <year> (1962). </year> <title> On estimation of a probability density function and mode. </title> <journal> Annals of Mathematical Statistics, 33:10651076. </journal>
Reference-contexts: The weights are inversely proportional to the distance of the units. This weighting leads actually to density model family known as the kernel density estimators or Parzen estimators <ref> (Parzen, 1962) </ref> with suitably chosen kernel functions. In the broad sense, as applied in this work, all the SCHMMs and CDHMMs, where the density model consists of a tnite set of weighted multivari-ate unimodal kernel functions, belong to the mixture density HMMs (MDHMMs).
Reference: <author> Paul, D. B. </author> <year> (1989). </year> <title> The Lincoln robust continuous speech recognizer. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 449452, </pages> <address> Glasgow, Scotland. </address>
Reference: <author> Peinado, A. M., Segura, J. C., Rubio, A. J., and Benitez, M. C. </author> <year> (1994). </year> <title> Using multiple vector quantization and semicontinuous hidden Markov models for speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 6164. </pages>
Reference: <author> Rabiner, L., Levinson, S., and Sondhi, M. </author> <year> (1983). </year> <title> On the application of vector quantization and hidden Markov models to speaker-independent isolated word recognition. </title> <journal> Bell System Technical Journal, 62:10751105. </journal>
Reference-contexts: Another traditional way is to use vector quantization (VQ) (Makhoul et al., 1985) to transform the features into a set of output symbols and then estimate their discrete probability distribution in each state <ref> (Rabiner et al., 1983) </ref>.
Reference: <author> Rabiner, L., Wilpon, J., and Juang, B. </author> <year> (1986). </year> <title> A segmental K-means training procedure for connected word recognition. </title> <journal> AT&T Technical Journal, </journal> <volume> 64:21 40. </volume>
Reference-contexts: The optimal model is then = arg max max Pr (O; qj) : (19) This latter method is called the segmental K-means or the Viterbi training and it can be shown <ref> (Rabiner et al., 1986) </ref> to have the same asymptotic behavior as the Baum-Welch training, but with less numerical diculties. Output density models. The output probability density models aim at providing the conditional probabilities for the output features of the system asso ciated with each state. <p> For the training depending on the gradually improving Viterbi segmentation of the data, the batch training is a natural choice to ensure equal weighting of the training words. When the neighborhood is reduced to zero the segmental SOM equals the segmental K-means <ref> (Rabiner et al., 1986) </ref>, which is the conventional ML training algorithm for HMMs. For the analysis of the convergence of the suggested MDHMM training method mostly the same guidelines are valid as for the segmental K-means (Juang and Rabiner, 1990).
Reference: <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 77(2):257286. </address>
Reference-contexts: To prevent too low sequence probabilities special tricks need to be 23 used. For example, the probabilities can be normalized at each time step or instead of multiplication in (17) the summation of logarithmized probabilities can be used <ref> (Rabiner, 1989) </ref>. The joining of the discrete weights and transition probabilities to the density function values having much wider dynamical range may sometimes cause problems, as well. <p> The parameters of each state are then obtained by tnding a model that maximizes the likelihood of the associated data. The actual training loop (Figure 3) consists of alternating segmentation and likelihood maximization 28 phases <ref> (Rabiner, 1989) </ref>. When the HMMs are ready their performance can be tuned by corrective training (Bahl et al., 1988). The investigations to apply SOM for the training of MDHMMs were motivated mainly by the following three ideas and needs: 1. <p> The results revealed that the LVQ codebooks tend to lead to models of lower error rates than the conventional K-means codebooks both in the Baum-Welch and Viterbi training <ref> (Rabiner, 1989) </ref>. For the Viterbi training the LVQ initialization seemed also to be slightly better than the SOM initialization. In Publication 1 the LVQ codebooks were generated for the SCHMMs where all states for every phoneme share the same large codebook of Gaussians.
Reference: <author> Rabiner, L. R. </author> <year> (1994). </year> <title> Applications of voice processing to telecommunications. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 82(2):199228. </address>
Reference-contexts: At the other end the training of models which should perform adequately for anyone needs serious eorts. State-of-art in ASR error rates. The following brief list exhibits the word error rates of representatives of some important categories <ref> (Rabiner, 1994) </ref> for vocabulary dependent recognizers. It is noteworthy that the evaluations (Rabiner, 1994) were applied in laboratory conditions and the performance in real-world conditions is usually much worse. <p> At the other end the training of models which should perform adequately for anyone needs serious eorts. State-of-art in ASR error rates. The following brief list exhibits the word error rates of representatives of some important categories <ref> (Rabiner, 1994) </ref> for vocabulary dependent recognizers. It is noteworthy that the evaluations (Rabiner, 1994) were applied in laboratory conditions and the performance in real-world conditions is usually much worse. The sentence error rate for the continuous speech tasks is much higher as well which makes these systems unsuitable for real-world applications. <p> Previously the emphasis has been on the manufacturing sector, but in the last years the markets in telecommunications have shown exponential growth in sales tgures, e.g. see <ref> (Rabiner, 1994) </ref>. Telecommunications applications, as the following examples, usually reduce costs by replacing human attendants or provide new services previously impossible to implement. 1. Automation of operator assisted calls which need only to distinguish between the charging options 2. Automation of directory assistance based on spelled or spoken names 3.
Reference: <author> Rainton, D. and Sagayama, S. </author> <year> (1992). </year> <title> Appropriate error criterion selection for continuous speech HMM minimum error training. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 233 236, </pages> <month> Ban,Canada. </month>
Reference-contexts: These whole word path probability dierences follow from the straight derivation of the loss function, e.g. see <ref> (Rainton and Sagayama, 1992) </ref>. Anyhow, 42 here only the given learning rate and the local dierences of the likelihood values provided by the rival mixtures determine the extent of the parameter adjustments.
Reference: <author> Renals, S., Morgan, N., and Bourlard, H. </author> <year> (1991). </year> <title> Probability estimation by feed-forward networks in continuous speech recognition. </title> <booktitle> In Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 309318, </pages> <address> Princeton, New Jersey, USA. </address> <note> 54 Renals, </note> <author> S., Morgan, N., Bourlard, H., Cohen, M., and Franco, H. </author> <year> (1994). </year> <title> Connectionist probability estimators in HMM speech recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, 2(1):161174. </journal>
Reference: <author> Renals, S., Morgan, N., Bourlard, H., Franco, H., and Cohen, M. </author> <year> (1992). </year> <title> Connectionist optimization of tied mixture hidden Markov models. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 167174. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Ritter, H. </author> <year> (1989). </year> <title> Asymptotic level density for a class of vector quantization processes. </title> <type> Report A9, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution>
Reference-contexts: and Schulten, 1988) that the following conditions are necessary and sucient to reach the existing stable state from all suciently close initial states with a pre-specited neighborhood function using any small positive function ff (t) : lim Z T ff (t)dt = 1 ; (7) T !1 As shown in <ref> (Ritter, 1989) </ref> for M = D = 1 SOMs, the width of the neighborhood function h ci aects strongly the asymptotic point density of the SOM units.
Reference: <author> Ritter, H. </author> <year> (1991). </year> <title> Asymptotic level density for a class of vector quantization processes. </title> <journal> IEEE Transactions on Neural Networks, 2(1):173175. </journal>
Reference: <author> Ritter, H. and Schulten, K. </author> <year> (1986). </year> <title> On the stationary state of Kohonen's self-organizing sensory mapping. </title> <journal> Biological Cybernetics, 54(1):99106. </journal>
Reference-contexts: If no neighbors are adjusted, the magnitcation factor is proportional to p (x) 1=3 and as the number of adjusted neighbors is increased it will increase towards p (x) 2=3 <ref> (Ritter and Schulten, 1986) </ref>. If the number of adjusted neighbors is constant through the learning process the asymptotic value of the magnitcation factor can be determined exactly (Ritter, 1989; Ritter, 1991). For multidimensional input the analysis of the magnitcation factor of the SOM is very dicult.
Reference: <author> Ritter, H. and Schulten, K. </author> <year> (1988). </year> <title> Convergence properties of Kohonen's topology preserving maps: uctuations, stability, and dimension selection. </title> <journal> Biological Cybernetics, 60(1):5971. </journal>
Reference-contexts: Stochastic approximation (Robbins and Monro, 1951) can be used to reach a suitable stable state for the SOM and for a M = D = 1 SOM it can be shown <ref> (Ritter and Schulten, 1988) </ref> that the following conditions are necessary and sucient to reach the existing stable state from all suciently close initial states with a pre-specited neighborhood function using any small positive function ff (t) : lim Z T ff (t)dt = 1 ; (7) T !1 As shown in
Reference: <author> Robbins, H. and Monro, S. </author> <year> (1951). </year> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, 22:400407. </journal>
Reference-contexts: Asymptotic density of the SOM units. The convergence of the process can be forced by decreasing the learning rate gradually towards zero following a schedule that allows enough learning and controls the balance between the eect of successive adaptations. Stochastic approximation <ref> (Robbins and Monro, 1951) </ref> can be used to reach a suitable stable state for the SOM and for a M = D = 1 SOM it can be shown (Ritter and Schulten, 1988) that the following conditions are necessary and sucient to reach the existing stable state from all suciently close
Reference: <author> Segura, J., Rubio, A., Peinado, A., Garcia, P., and Roman, R. </author> <year> (1994). </year> <title> Multiple VQ hidden Markov modelling for speech recognition. Speech Communication, </title> <publisher> 14:163170. </publisher>
Reference: <author> Seide, F. </author> <year> (1995). </year> <title> Fast likelihood computation for continuous mixture densities using a tree-based nearest neighbor search. </title> <booktitle> In Proceedings of 4th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 10791082, </pages> <address> Madrid, Spain. </address>
Reference-contexts: The idea of tree-search for mixture densities has been used without the SOM structure, e.g. in <ref> (Seide, 1995) </ref>. The loss of accuracy in tree-search follows from the sequential branching decisions by which most of the units are eliminated from the individual inspection. The neighboring search areas for tree-search SOM can be made slightly overlapping for better cover and search accuracy.
Reference: <author> Singer, E. and Lippmann, R. P. </author> <year> (1992). </year> <title> A speech recognizer using radial basis function neural networks in an HMM framework. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <volume> volume 1, </volume> <pages> pages 629632, </pages> <address> San Francisco,USA. </address>
Reference: <author> Torkkola, K., Kangas, J., Utela, P., Kaski, S., Kokkonen, M., Kurimo, M., and Kohonen, T. </author> <year> (1991). </year> <title> Status report of the Finnish phonetic typewriter project. </title> <editor> In Kohonen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 771776, </pages> <address> Amsterdam, Netherlands. </address> <publisher> North-Holland. </publisher> <address> 55 Utela, P. </address> <year> (1992). </year> <title> Phoneme recognition with discrete density Markov models. </title> <type> Master's thesis, </type> <institution> Helsinki University of Technology. </institution> <note> (in Finnish). </note>
Reference: <author> Young, S. </author> <year> (1996). </year> <title> A review of large-vocabulary continuous-speech recognition. </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pages 4557. </pages>
Reference-contexts: The heavily extended models are of the black-box type and do not necessarily oer any intelligible insights into the system. 2.4 Overview of speech recognition applications Considerable progress has been made in ASR technology in the recent years <ref> (Young, 1996) </ref> and many new products have been launched, but still completely new approaches need to be developed before robust, general-purpose speech rec-ognizers will be available (Bourlard, 1995). Current applications work only in relatively controlled environments and well-specited domains.
Reference: <author> Young, S. and Woodland, P. </author> <year> (1994). </year> <title> State clustering in HMM based speech recognition. Computer Speech and Language, </title> <publisher> 8(4):369384. </publisher>
Reference: <author> Zador, P. L. </author> <year> (1982). </year> <title> Asymptotic quantization error of continuous signals and the quantization dimension. </title> <journal> IEEE Transactions on Information Theory, IT-28(2):139148. </journal>

References-found: 110

