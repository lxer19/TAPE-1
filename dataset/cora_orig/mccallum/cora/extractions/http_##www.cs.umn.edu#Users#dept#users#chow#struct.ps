URL: http://www.cs.umn.edu/Users/dept/users/chow/struct.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/chow/
Root-URL: http://www.cs.umn.edu
Title: A PRIORI SPARSITY PATTERNS FOR PARALLEL SPARSE APPROXIMATE INVERSE PRECONDITIONERS  
Author: EDMOND CHOW 
Keyword: Key words. preconditioned iterative methods, sparse approximate inverses, graph theory, parallel computing  
Note: AMS subject classifications. 65F10, 65F35, 65F50, 65Y05  
Abstract: Parallel algorithms for computing sparse approximations to the inverse of a sparse matrix either use a prescribed sparsity pattern for the sparse approximate inverse, or attempt to generate a good pattern as part of the algorithm. Finding a good pattern a priori seems difficult for general matrices, and thus research has focused instead on much more expensive schemes that test all candidate matrix locations during the construction of the approximate inverse. This paper breaks with the trend and proposes some simple but effective parallel algorithms to find good sparsity patterns for sparse approximate inverses beforehand. New algorithms were designed by interpreting existing algorithms with matrix graph theory and by using heuristics about the Green's function of the PDE problem. A parallel implementation demonstrates that these methods are significantly faster than adaptive methods of constructing the approximate inverse, while giving preconditioners of comparable quality. The additional effort of adaptive sparsity pattern calculations is not always required. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. T. Barnard, L. M. Bernardo, and H. D. Simon, </author> <title> An MPI implementation of the SPAI preconditioner on the T3E, </title> <type> Tech. Report LBNL-40794, </type> <institution> Lawrence Berkeley National Laboratory, Berkeley, </institution> <address> CA, </address> <year> 1997. </year>
Reference-contexts: This is a good approximation in many cases. This implementation also assumed A is structurally symmetric, so that one-sided interprocessor communication is not necessary. A more recent parallel implementation of SPAI <ref> [1, 2] </ref> implements the algorithm exactly. This code implements one-sided communication with the Message Passing Interface (MPI), and uses dynamic load balancing in case some processors finish computing their rows earlier than others. <p> This latter figure was selected from <ref> [1] </ref> which shows it as an example of an effective sparse approximate inverse pattern for this problem. (There are, however, some bothersome features of this example: the approximate inverse is four independent diagonal blocks.) Note that we can approximate the adaptively generated pattern (d) very well by the pattern (c) generated
Reference: [2] <author> S. T. Barnard and R. Clay, </author> <title> A portable MPI implementation of the SPAI preconditioner in ISIS++, </title> <booktitle> in Proc. Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Minneapolis, MN, </address> <month> March, </month> <year> 1997. </year>
Reference-contexts: This is a good approximation in many cases. This implementation also assumed A is structurally symmetric, so that one-sided interprocessor communication is not necessary. A more recent parallel implementation of SPAI <ref> [1, 2] </ref> implements the algorithm exactly. This code implements one-sided communication with the Message Passing Interface (MPI), and uses dynamic load balancing in case some processors finish computing their rows earlier than others. <p> In this section we illustrate the main advantage of these preconditioners: their very low construction costs compared to the adaptive schemes. We test our parallel implementation of Algorithm 3.1 for constructing sparse approximate inverses, and report timings in comparison to a parallel version of SPAI <ref> [2] </ref>. Our code, called ParaSAILS (parallel sparse approximate inverse, least squares), is implemented as a precondi-tioner object in the ISIS++ solver library [11], and will be distributed with ISIS++. The parallel SPAI preconditioner is also implemented in ISIS++.
Reference: [3] <author> M. W. Benson and P. O. Frederickson, </author> <title> Iterative solution of large sparse linear systems arising in certain multidimensional approximation problems, </title> <journal> Utilitas Math., </journal> <volume> 22 (1982), </volume> <pages> pp. 127-140. </pages>
Reference-contexts: This idea is supported by the decay in the elements that can be seen in the discrete Green's function for many problems [26]. These sparsity patterns were used by Benson and Frederickson <ref> [3] </ref> in the symmetric case, who defined them to be q-local operators. Given a graph G (A) of a symmetric matrix A, column j of the matrix of a q-local operator consists of nonzeros corresponding to vertex j in G (A) and its qth level nearest-neighbors.
Reference: [4] <author> M. Benzi and M. T uma, </author> <title> Orderings for factorized sparse approximate inverse preconditioners, </title> <type> Tech. Report LA-UR-98-xxx, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1998. </year> <title> [5] , A sparse approximate inverse preconditioner for nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 19 (1998), </volume> <pages> pp. 968-994. </pages>
Reference-contexts: This result, however, is interesting for triangular matrices. In particular, it can be used to compute the number of nonzeros in the exact inverse of a Cholesky factor, based on the height of all the nodes in the elimination tree <ref> [4, 6] </ref>. Experimentally, fewer nonzeros in the exact inverse factor translates into lower construction cost and better performance for factorized approximate inverses computed by an incomplete biconjugation process [4, 6]. <p> of nonzeros in the exact inverse of a Cholesky factor, based on the height of all the nodes in the elimination tree <ref> [4, 6] </ref>. Experimentally, fewer nonzeros in the exact inverse factor translates into lower construction cost and better performance for factorized approximate inverses computed by an incomplete biconjugation process [4, 6]. Reordering strategies that minimize the height of the elimination tree and thus the number of nonzeros in the inverse factors have been designed and tested. Thus the above result allows some prediction of how well these approximate inverses might perform on a given problem. 3.
Reference: [6] <author> R. Bridson and W.-P. Tang, </author> <title> An ordering method for a factorized approximate inverse pre-conditioner, </title> <note> SIAM J. Sci. Comput., submitted (1998). </note>
Reference-contexts: This result, however, is interesting for triangular matrices. In particular, it can be used to compute the number of nonzeros in the exact inverse of a Cholesky factor, based on the height of all the nodes in the elimination tree <ref> [4, 6] </ref>. Experimentally, fewer nonzeros in the exact inverse factor translates into lower construction cost and better performance for factorized approximate inverses computed by an incomplete biconjugation process [4, 6]. <p> of nonzeros in the exact inverse of a Cholesky factor, based on the height of all the nodes in the elimination tree <ref> [4, 6] </ref>. Experimentally, fewer nonzeros in the exact inverse factor translates into lower construction cost and better performance for factorized approximate inverses computed by an incomplete biconjugation process [4, 6]. Reordering strategies that minimize the height of the elimination tree and thus the number of nonzeros in the inverse factors have been designed and tested. Thus the above result allows some prediction of how well these approximate inverses might perform on a given problem. 3.
Reference: [7] <author> E. Chow, </author> <title> Robust Preconditioning for Sparse Linear Systems, </title> <type> PhD thesis, </type> <institution> University of Min-nesota, </institution> <year> 1997. </year>
Reference-contexts: For the unfactored form, we also display the result of the SPAI method reported in [19], using their choice of parameters. Adaptive methods for factored forms are also available <ref> [5, 7, 25] </ref> but were not tested here. Global thresholds (shown in each table) on a scaled matrix were used to perform these sparsifications. In the tables, we also show the number of nonzeros nnz in the unfactored preconditioners (the entry for LS/FSAI is the number of nonzeros in A).
Reference: [8] <author> E. Chow and Y. Saad, </author> <title> Approximate inverse techniques for block-partitioned matrices, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18 (1997), </volume> <pages> pp. </pages> <month> 1657-1675. </month> <title> [9] , Approximate inverse preconditioners via sparse-sparse iterations, </title> <journal> SIAM J. Sci. Com-put., </journal> <volume> 19 (1998), </volume> <pages> pp. </pages> <month> 995-1023. </month> <title> [10] , Parallel approximate inverse preconditioners, </title> <booktitle> in Proc. Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Minneapolis, MN, March 14-17, </address> <year> 1997. </year>
Reference-contexts: Sparse approximate inverses are also compulsory components in block incomplete factorizations when the blocks are large and sparse. This latter method can be a middle ground between approximate inverse preconditioners and incomplete factorizations <ref> [8] </ref>. For a survey of parallel sparse approximate inverse preconditioners, see [10]. 2. Graph interpretations of approximate inverse sparsity patterns. 2.1. Use of the pattern of A.
Reference: [11] <author> R. L. Clay, K. D. Mish, and A. B. Williams, </author> <title> ISIS++: Iterative scalable implicit solver (in C++), </title> <type> Tech. Report SAND97-8535, </type> <institution> Sandia National Laboratories, Livermore, </institution> <address> CA, </address> <year> 1997. </year> <note> 14 EDMOND CHOW </note>
Reference-contexts: We test our parallel implementation of Algorithm 3.1 for constructing sparse approximate inverses, and report timings in comparison to a parallel version of SPAI [2]. Our code, called ParaSAILS (parallel sparse approximate inverse, least squares), is implemented as a precondi-tioner object in the ISIS++ solver library <ref> [11] </ref>, and will be distributed with ISIS++. The parallel SPAI preconditioner is also implemented in ISIS++. Both these codes generate a sparse approximate inverse partitioned across processors by rows; thus left preconditioning is used.
Reference: [12] <author> J. D. F. Cosgrove and J. C. D iaz, </author> <title> Structural properties of the graph of augmented sparse approximate inverses, </title> <booktitle> in Proc. 1990 Symposium on Applied Computing, </booktitle> <editor> H. Berghel, J. Tal-burt, and D. Roach, eds., Fayetteville, AR, </editor> <booktitle> 1990, </booktitle> <publisher> IEEE Computer Press, Los Alamitos, CA, </publisher> <pages> pp. 131-136. </pages>
Reference-contexts: If higher levels of q are invoked, the storage required grows very quickly [17]. There have been proposals to use an augmented pattern of A as a prescribed sparsity pattern (without using the full 2-local operator). Cosgrove and Diaz <ref> [12] </ref> suggested adding nonzeros to column j of S in a way that minimizes the number of new rows introduced into the jth least-squares matrix (in expression (1.2)).
Reference: [13] <author> J. D. F. Cosgrove, J. C. D iaz, and A. Griewank, </author> <title> Approximate inverse preconditioning for sparse linear systems, </title> <journal> Intl. J. Comp. Math., </journal> <volume> 44 (1992), </volume> <pages> pp. 91-110. </pages>
Reference-contexts: This process is repeated until a threshold on the residual norm has been satisfied, or a maximum number of nonzeros has been reached <ref> [9, 13, 19] </ref>. We refer to these as adaptive procedures. <p> For this method to be efficient, sparse-sparse operations must be used: the product of a sparse matrix by a sparse vector with p nonzeros only involves p columns of the sparse matrix. Another adaptive procedure, called SPAI <ref> [13, 19] </ref>, uses a numerical test to determine which nonzero locations should be added to S.
Reference: [14] <author> S. Demko, W. F. Moss, and P. W. Smith, </author> <title> Decay rates for inverses of band matrices, </title> <journal> Math. Comp., </journal> <volume> 43 (1984), </volume> <pages> pp. 491-499. </pages>
Reference-contexts: For diagonally dominant A, the entries in A 1 decay rapidly away from the diagonal <ref> [14] </ref>, and a banded pattern for S will produce a good approximate inverse. For more general problems, it is not clear how best to choose S.
Reference: [15] <author> V. Deshpande, M. J. Grote, P. Messmer, and W. Sawyer, </author> <title> Parallel implementation of a sparse approximate inverse preconditioner, in Parallel Algorithms for Irregularly Structured Problems (Proc. IRREGULAR '96), </title> <editor> A. Ferreira, J. Rolim, Y. Saad, and T. Yang, eds., </editor> <address> Santa Barbara, CA, </address> <year> 1996, </year> <pages> pp. 63-74. </pages>
Reference-contexts: Algebraically, this means that the nonzero locations of r and Ae k do not intersect, and the value of the test is zero. An efficient implementation of SPAI uses these graph ideas to narrow down the indices k that need to be checked. An early parallel implementation of SPAI <ref> [15] </ref> tested only the first level neighbors of a vertex, rather than both the first and second levels. This is a good approximation in many cases. This implementation also assumed A is structurally symmetric, so that one-sided interprocessor communication is not necessary.
Reference: [16] <author> M. R. </author> <title> Field, An efficient parallel preconditioner for the conjugate gradient algorithm, </title> <type> Tech. Report HDL-TR-97-175, </type> <institution> Hitachi Dublin Laboratory, Trinity College, </institution> <address> Dublin, </address> <year> 1997. </year> <title> [17] , Improving the performance of parallel factorised sparse approximate inverse precondi-tioners, </title> <type> Tech. Report HDL-TR-98-199, </type> <institution> Hitachi Dublin Laboratory, Trinity College, </institution> <address> Dublin, </address> <year> 1998. </year>
Reference-contexts: A parallel version of FSAI has been implemented by Field <ref> [16] </ref>. Huckle [21] has proposed some envelope sparsity patterns from which the adaptive SPAI algorithm can select its pattern. This gives an upper bound on the interprocessor communication required by a parallel implementation [20].
Reference: [18] <author> J. R. Gilbert, </author> <title> Predicting structure in sparse matrix computations, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 (1994), </volume> <pages> pp. 62-79. </pages>
Reference-contexts: This is stated in one form, for example, in <ref> [18] </ref>. A heuristic that is often employed is that vertices closer to vertex j along these directed paths are more important, and should be retained in an approximate inverse sparsity pattern.
Reference: [19] <author> M. Grote and T. Huckle, </author> <title> Parallel preconditioning with sparse approximate inverses, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18 (1997), </volume> <pages> pp. 838-853. </pages>
Reference-contexts: This process is repeated until a threshold on the residual norm has been satisfied, or a maximum number of nonzeros has been reached <ref> [9, 13, 19] </ref>. We refer to these as adaptive procedures. <p> For this method to be efficient, sparse-sparse operations must be used: the product of a sparse matrix by a sparse vector with p nonzeros only involves p columns of the sparse matrix. Another adaptive procedure, called SPAI <ref> [13, 19] </ref>, uses a numerical test to determine which nonzero locations should be added to S. <p> Numerical tests. 4.1. Preconditioning quality. First we test the quality of sparsity patterns generated by these new methods on small problems from the Harwell-Boeing collection. In particular, we chose problems that were tested with SPAI <ref> [19] </ref> in order to make comparisons. We performed tests in exactly the same conditions: we solve the same linear systems using GMRES (20) to a relative residual tolerance of 10 8 with a zero initial guess. <p> For the unfactored form, we also display the result of the SPAI method reported in <ref> [19] </ref>, using their choice of parameters. Adaptive methods for factored forms are also available [5, 7, 25] but were not tested here. Global thresholds (shown in each table) on a scaled matrix were used to perform these sparsifications.
Reference: [20] <author> T. Huckle, </author> <title> PVM-implementation of sparse approximate inverse preconditioners for solving large sparse linear equations, </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <volume> vol. 1156, </volume> <booktitle> Parallel Virtual Machine-EuroPVM'96, </booktitle> <publisher> Springer, </publisher> <year> 1996, </year> <pages> pp. </pages> <month> 166-173. </month> <title> [21] , Approximate sparsity patterns for the inverse of a matrix and preconditioning, </title> <booktitle> in Prelim. Proc. IMACS World Congress 1997, </booktitle> <editor> R. Weiss and W. Schonauer, eds., </editor> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: A parallel version of FSAI has been implemented by Field [16]. Huckle [21] has proposed some envelope sparsity patterns from which the adaptive SPAI algorithm can select its pattern. This gives an upper bound on the interprocessor communication required by a parallel implementation <ref> [20] </ref>. These patterns are also augmented patterns of A, for example, G ((A T A) k A T ) and G ((I + jAj + jA T j) k A T ) for small integers k.
Reference: [22] <author> L. Yu. Kolotilina and A. Yu. Yeremin, </author> <title> On a family of two-level preconditionings of the incomplete block factorization type, </title> <journal> Soviet J. Numer. Anal. Math. Model., </journal> <volume> 1 (1986), </volume> <pages> pp. </pages> <month> 293-320. </month> <title> [23] , Factorized sparse approximate inverse preconditionings I. </title> <journal> Theory, SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. </pages> <month> 45-58. </month> <title> [24] , Factorized sparse approximate inverse preconditionings II. Solution of 3D FE systems on massively parallel computers, </title> <journal> Intl. J. High Speed Computing, </journal> <volume> 7 (1995), </volume> <pages> pp. 191-215. </pages>
Reference-contexts: Finally, it is interesting to note that when using weighted Frobenius norms, the choice of weight matrix might compensate for using a possibly poorly prescribed sparsity pattern. This was proposed in <ref> [22] </ref>, but nothing concrete was presented. 2.2. Insights from adaptive schemes. Adaptive schemes usually generate patterns very different, and often much sparser, than the pattern of A. Nevertheless, the patterns produced by adaptive schemes can be interpreted using the graph of A.
Reference: [25] <author> Y. Saad, </author> <title> Iterative Methods for Sparse Linear Systems, </title> <publisher> PWS Publishing Co., </publisher> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: For the unfactored form, we also display the result of the SPAI method reported in [19], using their choice of parameters. Adaptive methods for factored forms are also available <ref> [5, 7, 25] </ref> but were not tested here. Global thresholds (shown in each table) on a scaled matrix were used to perform these sparsifications. In the tables, we also show the number of nonzeros nnz in the unfactored preconditioners (the entry for LS/FSAI is the number of nonzeros in A).
Reference: [26] <author> W.-P. Tang, </author> <title> Towards an effective sparse approximate inverse preconditioner, </title> <note> SIAM J. Matrix Anal. Appl., to appear (1998). </note>
Reference-contexts: This idea is supported by the decay in the elements that can be seen in the discrete Green's function for many problems <ref> [26] </ref>. These sparsity patterns were used by Benson and Frederickson [3] in the symmetric case, who defined them to be q-local operators. <p> Fig. 3.1. Green's function for a point on a PDE with convection. Without additional physical information such as the direction of flow, however, it is possible to use an algebraic technique called sparsification, i.e., dropping small elements in a matrix. Tang <ref> [26] </ref> showed that sparsifying a matrix prior to applying the adaptive SPAI algorithm is effective for anisotropic problems. The observation is that the storage and therefore operation count required for preconditioners produced this way is much smaller. In this paper, we apply a similar technique for a priori sparsity patterns.
References-found: 19

