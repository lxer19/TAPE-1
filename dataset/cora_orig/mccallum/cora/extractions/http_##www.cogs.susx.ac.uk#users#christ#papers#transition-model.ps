URL: http://www.cogs.susx.ac.uk/users/christ/papers/transition-model.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)273 678856  
Title: A General Model of the Implicit/Explicit Transition in Representational Redescription  
Author: Chris Thornton 
Date: November 30, 1994  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Representational redescription involves transforming implicit representations into (more) explicit respresntations. Existing characterisations of the way in which this operation can be performed lack generality. The paper presents a more general model and shows how it relates to computational procedures for constructive induction.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Karmiloff-Smith, A. </author> <year> (1990). </year> <title> Constraints on representational change: evidence from children's drawing. </title> <journal> Cognition, </journal> <pages> 34 (pp. 57-83). </pages>
Reference: [2] <author> Karmiloff-Smith, A. </author> <year> (1992). </year> <title> Beyond modularity: a developmental perspective on Cognitive Science. </title> <publisher> Cambridge,Ma.: MIT Press/Bradford books. </publisher>
Reference: [3] <author> Karmiloff-Smith, A. </author> <title> (Forthcoming). In D.M. Peterson (Ed.), Internal Representations and External Notations: A Developmental Perspective. </title> <publisher> Intellect. </publisher>
Reference: [4] <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: It argues that there is a fundamental link between the redescriptive, developmental processes envisaged by Karmiloff-Smith and certain 1 complex forms of computational learning, known as constructive induction (or CI) methods within the Machine Learning community <ref> [4] </ref>. The paper begins (section two) by describing the basic model of induction which is used in Machine Learning and considers the difference between hard and easy learning problems.
Reference: [5] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: Some problems which look hard can turn out to be easy, while problems which seem easy can turn out to be effectively intractable. To illustrate this, consider the two problems shown below. The first one is from <ref> [5] </ref>. The input/output pairs shown here form a portion of the target mapping.
Reference: [6] <author> Thornton, C. </author> <title> (Forthcoming). Unsupervised Constructive Learning. </title>
Reference-contexts: The first problem is relatively easy for most learning methods but the second is in fact pathologically hard for such methods <ref> [6] </ref> and thus, presumably, in need of representation change. Our intuitions then, regarding learning-problem difficulty can sometimes be very wide of the mark. <p> Given this formalization of learning, a task analysis can be derived by enumerating the ways in which the learner's output guesses may be justified by the feedback source <ref> [6] </ref>. The feedback is, of course, ultimately derived from the target mapping. Thus guesses that are justified by the feedback must, at a deeper level, be justified by the contents of the target mapping.
Reference: [7] <author> Dietterich, T., London, B., Clarkson, K. and Dromey, G. </author> <year> (1982). </year> <title> Learning and inductive inference. </title> <editor> In P. Cohen and E. Feigenbaum (Eds.), </editor> <booktitle> The Handbook of Artificial Intelligence: Vol III. </booktitle> <address> Los Altos: </address> <publisher> Kaufmann. </publisher> <pages> 7 </pages>
Reference-contexts: If it was not obvious before, the introduction of this new terminology should drive home the point that this analysis gives a foundation to the well-known Machine Learning heuristic which states that `learning relationships is hard' <ref> [7] </ref>. 3.2 The implicit/explicit translation The task analysis shows that there are essentially two forms of learning: an easy form which involves exploiting statistical effects and a hard form which involves exploiting relational effects.
References-found: 7

