URL: http://polaris.cs.uiuc.edu/reports/1267.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: granston@cs.LeidenUniv.nl  sasha@csrd.uiuc.edu  
Title: Compile Time Techniques for Using the Priority Data Cache to Reduce Memory Access Delays  
Author: Elana D. Granston Alexander V. Veidenbaum 
Keyword: Key words: data access optimization, data prioritization, compiler-directed cache management, memory hierarchies, data reuse, data locality, dependence analysis, program optimization.  
Note: Much of this work was completed while Elana D. Granston was at the Center for Supercomputing Research and Development. Support was provided by the Department of Energy under Grant No. DE-FG02-85ER25001, Cray Research Inc., and the Esprit Agency under Grant No. APPARC 6634 BRA III. Support was provided by the NASA Ames Research Center under Grant No. NASA NCC 2-559, and the National Science Foundation under Grant No. NSF 89-20891.  
Date: October 18, 1992  
Address: Niels Bohrweg 1, 2333 CA Leiden, The Netherlands  1308 W. Main St., Urbana, Illinois 61801  
Affiliation: High Performance Computing Division Department of Computer Science Leiden University  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: The potential of high-performance systems, especially parallel machines, is generally limited by the bandwidth between processors and memory. To achieve the performance of which these machines should be capable, global memory access delays must be alleviated. One approach is to better utilize local storage. To this end, we previously proposed a new local storage facility, the priority data cache (PDC), that supports integrated hardware-software control. This paper focuses on developing a compile-time methodology for using the PDC. We detail the basic algorithm and propose enhancements based on initial performance results. 
Abstract-found: 1
Intro-found: 1
Reference: [AK86] <author> Randy Allen and Ken Kennedy. </author> <title> Vector Register Allocation. </title> <type> Technical Report COMP TR86-45, </type> <institution> Rice University, </institution> <year> 1986. </year> <month> Revised March </month> <year> 1987 </year> <month> and August </month> <year> 1988. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89, WL91] </ref> or local memory [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Condition (i): Execution of the statement at the dependence's source guarantees execution of the statement at its sink. In general, Condition (i) can be met by ensuring that the sink statement post-dominates the source statement in the program's control flow graph <ref> [ASU86] </ref>. Symbolic execution of the program may yield other cases where this can be guaranteed. 14 Condition (ii): Data accessed at the source of the dependence by a given processor will be reac--cessed at the sink by the same processor.
Reference: [Bel66] <author> L. A. Belady. </author> <title> A Study of Replacement Algorithms for a Virtual-Storage Computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: When the compiler does not resolve conflicts for cache space, the hardware does. In determining which data to cache, the cache hit ratio is maximized by giving priority to data that will be reused the soonest <ref> [Bel66] </ref>. Although such data are not immune to replacement, they can only be displaced as a result of more recent accesses to other data with equal or higher priority. To simplify the ensuing discussion, each cache line is assumed to be one word long.
Reference: [BJEW91] <author> Francois Bodin, William Jalby, Christine Eisenbeis, and Daniel Windheiser. </author> <title> Window-Based Register Allocation. </title> <type> Technical report, </type> <institution> INRIA, </institution> <year> 1991. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89, WL91] </ref> or local memory [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> A simple heuristic algorithm for selecting dependences is presented in Figure 9. It is interesting to note that the idea of selecting dependences in this order was independently adopted by Bodin et al. <ref> [BJEW91] </ref>, who study the problem of allocating array elements to scalar registers. 5.3 Step 3: Mark Memory Access Instructions After the final selection of high priority dependences has been made, memory access instructions must be augmented to affect these decisions. First, consider the case of dependences involving scalar references.
Reference: [CCK87] <author> David Callahan, John Cocke, and Ken Kennedy. </author> <title> Estimating Interlock and Improving Balance for Pipelined Processors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 295-304, </pages> <month> August </month> <year> 1987. </year>
Reference: [CCK90] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Improving Register Allocation for Subscripted Variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 25(6), </volume> <pages> pages 53-65, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89, WL91] </ref> or local memory [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [Chi89] <author> Chi-Hung Chi. </author> <title> Compiler-Driven Cache Management Using A State Level Transition Model. </title> <type> PhD thesis, </type> <institution> School of Electrical Engineering, Purdue University, </institution> <month> December </month> <year> 1989. </year> <month> 31 </month>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89, WL91] </ref> or local memory [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Consequently, for this class of applications, traditional hardware resolution schemes are less effective. 3 Several integrated hardware-software strategies have previously been applied to problems of optimizing data accesses in memory hierarchies. <ref> [Chi89, HS90] </ref> propose hardware-assisted solutions to the problem of register allocation in the presence of aliased variables. The addresses associated with register values are stored in associative caches so that data accesses can be checked against them and redirected to the appropriate registers, as necessary. <p> Although this approach has potential for improving the effectiveness of registers, the cost per storage unit is relatively high. Therefore, this technique cannot be cost-effectively applied to local memory. Chi <ref> [Chi89] </ref> discusses the design of an on-chip hardware cache that can be selectively bypassed if the compiler can detect that accessed data will be stored in registers or that data will not be reused before being displaced. <p> The success of this technique in practice depends on the compiler's ability to detect with certainty (or near certainty) that data will not be reaccessed before being displaced. For the range of cache sizes explored in <ref> [Chi89] </ref> (32-1024 words), a single access, especially an access to a block of data, can cause a significant portion of cached data to be displaced. <p> Although the SP /P R field can take on four values, only three of these combinations are used to effect the compile time mangement decisions described in Sections 4 and 5. The remaining unused combination could be used to signify other actions, such as selective cache bypassing <ref> [Chi89] </ref>. Note that the number of priority levels is restricted to two. Supporting many software-assignable priority levels can be expensive both in space and in hardware complexity. It is not immediately obvious that, in general, a programmer or compiler writer could effectively use a larger number of priority levels.
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Within loops, the compiler can accomplish this by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed <ref> [Gor89, Por89, CKP91] </ref>. Another approach is to float prefetches out of loops altogether [GGV90].
Reference: [CP90] <author> David Callahan and Allan Porterfield. </author> <title> Data Cache Performance of Supercomputer Applications. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 564-572, </pages> <year> 1990. </year>
Reference: [CV91] <author> Yung-Chin Chen and Alexander V. Veidenbaum. </author> <title> Comparison and Analysis of Software and Directory Coherence Schemes. </title> <type> Technical Report 1055, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs [LRW91, FST91, WL91, McK92, Win92] or tune caches <ref> [CV91] </ref> so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing.
Reference: [EJWB90] <author> Christine Eisenbeis, William Jalby, Daniel Windheiser, and Francois Bodin. </author> <title> A Strategy for Array Management in Local Memory. </title> <booktitle> In Workshop on Parallel on Programming Languages and Compilers for Parallel Programming, </booktitle> <year> 1990. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [ETW92] <author> C. Eisenbeis, O. Temam, and H. Wijshoff. </author> <title> On Efficiently Characterizing the Solutions of Linear Diophantine Equations and Its Application to Data Dependence Analysis. </title> <type> Technical report, </type> <institution> Department of Computer Science, </institution> <type> Technical Report RUU-CS-92-01, </type> <institution> Utrecht University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated. <p> To avoid overcounting in such cases, the size of the joint reference window for ffi j and ffi k should be computed as the maximum of their individual reference window sizes, rather than the sum. Additional techniques for obtaining accurate estimates of joint reference windows can be found in <ref> [FST91, KK91, ETW92] </ref>. To summarize Step 2, dependences are selected on the basis of increasing reuse time, subject to the constraint that that the total amount of high priority data remains below the threshold level.
Reference: [FST91] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness (Extended Abstract). </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs <ref> [LRW91, FST91, WL91, McK92, Win92] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated. <p> To avoid overcounting in such cases, the size of the joint reference window for ffi j and ffi k should be computed as the maximum of their individual reference window sizes, rather than the sum. Additional techniques for obtaining accurate estimates of joint reference windows can be found in <ref> [FST91, KK91, ETW92] </ref>. To summarize Step 2, dependences are selected on the basis of increasing reuse time, subject to the constraint that that the total amount of high priority data remains below the threshold level. <p> During the second step, the window size computation should be adjusted to reflect that a datum might now occupy a fraction of a cache line rather than multiple cache lines. Techniques for handling this case can be found in <ref> [FST91] </ref>. The third step proceeds 26 as before. Because we consider dependences that arise from both spatial and temporal reuse opportunities and select them in order of increasing reuse time, we achieve our goal of considering both temporal and spatial locality simultaneously, favoring each where appropriate. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [GGV90] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Veidenbaum. </author> <title> Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Within loops, the compiler can accomplish this by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed [Gor89, Por89, CKP91]. Another approach is to float prefetches out of loops altogether <ref> [GGV90] </ref>. Our basic priority assignment algorithm can be extended to support both these types of software prefetching by treating the point of prefetching as the source of a candidate dependence, and the point of use as its sink.
Reference: [GJG88a] <author> Kyle Gallivan, William Jalby, and Dennis Gannon. </author> <title> On the Problem of Optimizing Data Transfers for Complex Memory Systems. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 238-253, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [GJG88b] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for Cache and Local Memory Management. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> The general class of dependences with one or more distinct constant distant vectors is roughly analogous to the set of uniformly-generated dependences, originally introduced by Gannon et al. <ref> [GJG88b] </ref>. <p> The cache lines that contain the data to be designated high priority at time t are collectively known as the reference window <ref> [GJG88b] </ref> ffi at time t, denoted W t (ffi). Let ! be the size of an individual datum measured in words. <p> There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated. <p> Since distance vector information is available, for some cases, it may be possible to detect conflicts between data to be designated high priority and avoid assigning these data high priority. For these cases, spatial locality can be relied upon instead. Alternatively, data copying techniques <ref> [GJG88b, LRW91] </ref>, which provide a more general mechanism for eliminating cache conflicts, can be applied as a preprocessing step. We also note that no single threshold level worked best for all cases.
Reference: [Goo83] <author> J. R. Goodman. </author> <title> Using Cache Memory to Reduce Processor-Memory Traffic. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Note that the number of opportunities to reuse local data copies depends on the length of the time slice. This should not be a problem in practice, since there are other reasons as well for adopting long time slices in high-performance systems <ref> [Goo83] </ref>. 11 Using Additional Priority Levels Thus far, we have concentrated on utilizing a two-level priority scheme.
Reference: [Gor89] <author> Edward H. Gornish. </author> <title> Compile Time Analysis for Data Prefetching. </title> <type> Master's thesis, </type> <institution> Center for Supercomputing Research and Development, </institution> <type> Technical Report 949, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Within loops, the compiler can accomplish this by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed <ref> [Gor89, Por89, CKP91] </ref>. Another approach is to float prefetches out of loops altogether [GGV90].
Reference: [Gra92] <author> Elana D. Granston. </author> <title> Reducing Memory Access Delays in Large-Scale, Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: In other cases, precious local memory space may be wasted saving the same data multiple times. Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs <ref> [LMY88, Gra92] </ref>, making it difficult to block programs [LRW91, FST91, WL91, McK92, Win92] or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> We also restrict ourselves to dependences involving scalar references, where each reference instance involves exactly one scalar variable or array element. However, the generalization to multiple loops and vector references is straightforward <ref> [Gra92] </ref>. 5.1 Step 1: Identify High Priority Candidates The success of compile time management depends on both the selection of high priority data and the minimization of run time overhead. <p> Each processor was assumed to have a private, 8K, direct-mapped PDC. Given this cache configuration, multiplication of 293x293 matrices was shown to perform worse than that of 300x300 matrices, due to cache conflicts <ref> [LRW91, Gra92] </ref>. Hence, both of these were chosen to study the effects of cache conflicts. For the sake of brevity, only a brief synopsis of these experiments will be presented here. A detailed account can be found in [Gra92]. <p> Hence, both of these were chosen to study the effects of cache conflicts. For the sake of brevity, only a brief synopsis of these experiments will be presented here. A detailed account can be found in <ref> [Gra92] </ref>. For all the cases described above, employing a threshold level greater than 0% yielded hit ratios at most 2% higher than the corresponding base cases. In some cases, base case hit ratios were already very high, so there was little to be gained.
Reference: [GV91] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> An Integrated Hardware/Software Solution for Effective Management of Local Storage in High-Performance Systems. </title> <booktitle> In 32 Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 83-90, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Local storage facilities can take on various forms, for example, registers, software-controlled local memory, and cache. While these facilities generally allow us to take advantage of either compile time knowledge or run time knowledge, we would like to take advantage of both. Therefore, in a previous paper <ref> [GV91] </ref>, we proposed a new local storage facility, the priority data cache, that supports integrated hardware-software control. This paper focuses on compilation issues involved in utilizing this local storage facility effectively. We also present some performance enhancements, based on initial performance results.
Reference: [HS78] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, Inc., </publisher> <year> 1978. </year>
Reference-contexts: The amount of high priority data is equivalent to the joint reference window of ffi and the set of dependences selected thus far. Assuming a bound on both the PDC size and the components of the distance vectors, the optimal solution to this problem is N P -complete <ref> [HS78] </ref>. A simple heuristic algorithm for selecting dependences is presented in Figure 9.
Reference: [HS90] <author> Ben Heggy and Mary Lou Soffa. </author> <title> Architectural Support for Register Allocation in the Presence of Aliasing. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 730-739, </pages> <year> 1990. </year>
Reference-contexts: Consequently, for this class of applications, traditional hardware resolution schemes are less effective. 3 Several integrated hardware-software strategies have previously been applied to problems of optimizing data accesses in memory hierarchies. <ref> [Chi89, HS90] </ref> propose hardware-assisted solutions to the problem of register allocation in the presence of aliased variables. The addresses associated with register values are stored in associative caches so that data accesses can be checked against them and redirected to the appropriate registers, as necessary.
Reference: [KK91] <author> Apostolos D. Kallis and David Klappholz. </author> <title> Reaching Definitions Analysis on Code Containing Array References. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated. <p> To avoid overcounting in such cases, the size of the joint reference window for ffi j and ffi k should be computed as the maximum of their individual reference window sizes, rather than the sum. Additional techniques for obtaining accurate estimates of joint reference windows can be found in <ref> [FST91, KK91, ETW92] </ref>. To summarize Step 2, dependences are selected on the basis of increasing reuse time, subject to the constraint that that the total amount of high priority data remains below the threshold level.
Reference: [LMY88] <author> David J. Lilja, David Marcovitz, and Pen-Chung Yew. </author> <title> Memory Referencing Behavior and a Cache Performance Metric in a Shared Memory Multiprocessor. </title> <type> Technical Report 836, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: In other cases, precious local memory space may be wasted saving the same data multiple times. Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs <ref> [LMY88, Gra92] </ref>, making it difficult to block programs [LRW91, FST91, WL91, McK92, Win92] or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. Such systems are also plagued by coherence problems that can further limit cache effectiveness, especially when large cache line sizes are employed <ref> [LMY88, SD88] </ref>. Enlisting conservative cache coherence algorithms and utilizing short cache line sizes can limit opportunities to capitalize on existing spatial and temporal locality.
Reference: [LRW91] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance of Blocked Algorithms. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs <ref> [LRW91, FST91, WL91, McK92, Win92] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> Each processor was assumed to have a private, 8K, direct-mapped PDC. Given this cache configuration, multiplication of 293x293 matrices was shown to perform worse than that of 300x300 matrices, due to cache conflicts <ref> [LRW91, Gra92] </ref>. Hence, both of these were chosen to study the effects of cache conflicts. For the sake of brevity, only a brief synopsis of these experiments will be presented here. A detailed account can be found in [Gra92]. <p> Since distance vector information is available, for some cases, it may be possible to detect conflicts between data to be designated high priority and avoid assigning these data high priority. For these cases, spatial locality can be relied upon instead. Alternatively, data copying techniques <ref> [GJG88b, LRW91] </ref>, which provide a more general mechanism for eliminating cache conflicts, can be applied as a preprocessing step. We also note that no single threshold level worked best for all cases.
Reference: [McK92] <author> Kathryn S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <type> Technical Report CRPC-TR92214, </type> <month> April </month> <year> 1992. </year>
Reference-contexts: Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs <ref> [LRW91, FST91, WL91, McK92, Win92] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing.
Reference: [MJ91] <author> J. D. Martens and D. N. Jayasimha. </author> <title> Compiling Loops for Hierarchical Memory Multiprocessors. </title> <type> Technical Report OSU-CISRC-7/91-TR16, </type> <institution> Computer and Information Science Research Center, Ohio State University, </institution> <year> 1991. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [Por89] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, Technical Report Rice COMP TR89-93, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Within loops, the compiler can accomplish this by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed <ref> [Gor89, Por89, CKP91] </ref>. Another approach is to float prefetches out of loops altogether [GGV90]. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [SD88] <author> C. Scheurich and M. Dubois. </author> <title> The Design of Lockup-Free Cache for High-Performance Multiprocessors. </title> <booktitle> In Supercomputing '88, </booktitle> <pages> pages 352-359, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. Such systems are also plagued by coherence problems that can further limit cache effectiveness, especially when large cache line sizes are employed <ref> [LMY88, SD88] </ref>. Enlisting conservative cache coherence algorithms and utilizing short cache line sizes can limit opportunities to capitalize on existing spatial and temporal locality.
Reference: [Win92] <author> Daniel Windheiser. </author> <title> Data Locality Optimization. </title> <type> PhD thesis, </type> <institution> IRISA INRIA-Rennes, </institution> <year> 1992. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs <ref> [LRW91, FST91, WL91, McK92, Win92] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> There are several existing approaches for computing an upper bound for k W (ffi) k. A very simple method is presented below. More precise techniques can be found in <ref> [EJWB90, FST91, GJG88b, KK91, ETW92, Win92] </ref>. Note that to prevent thrashing of the PDC, it is very important that the amount of high priority data remain beneath the threshold level. It is significantly less critical if the amount of high priority data is overestimated.
Reference: [WL91] <author> Michael Wolf and Monica Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26(6), </volume> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89, WL91] </ref> or local memory [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Consequently, parallel machines typically rely on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Memory reference patterns are more variable in such programs [LMY88, Gra92], making it difficult to block programs <ref> [LRW91, FST91, WL91, McK92, Win92] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, typical applications consist of large data sets that can exceed cache size and cause thrashing. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [Wol87] <author> Michael Joseph Wolfe. </author> <title> Iteration Space Tiling for Memory Hierarchies. </title> <booktitle> In Proceedings of the 3rd SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 357-361, </pages> <year> 1987. </year>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
Reference: [Wol89] <author> Michael Joseph Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <year> 1989. </year> <month> 33 </month>
Reference-contexts: As mentioned earlier, in numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89, WL91] or local memory <ref> [EJWB90, GJG88a, GJG88b, MJ91, Wol87, Wol89, Win92] </ref> is significantly more complex than allocating scalar data, especially in the presence of nonlinear and subscripted subscripts, symbolic terms, and aliasing problems. <p> It is important to note that our compile time methodology can only exploit the locality that is inherent in a program. Hence, it can and should be combined with the growing body of locality 30 enhancing program transformations <ref> [AK86, Wol87, Por89, Wol89, EJWB90, FST91, WL91] </ref>, which can be applied as a preprocessing step. Optimizations to minimize cache conflicts, such as data copying, should also prove beneficial.
References-found: 33

