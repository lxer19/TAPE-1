URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/diffuse-nips7.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Email: bengioy@IRO.UMontreal.CA  paolo@mcculloch.ing.unifi.it  
Title: Diffusion of Credit in Markovian Models  
Author: Yoshua Bengio Paolo Frasconi 
Address: Montreal, Qc, Canada H3C-3J7  Italy  
Affiliation: Dept. I.R.O., Universite de Montreal,  Dipartimento di Sistemi e Informatica Universita di Firenze,  
Abstract: This paper studies the problem of diffusion in Markovian models, such as hidden Markov models (HMMs) and how it makes very difficult the task of learning of long-term dependencies in sequences. Using results from Markov chain theory, we show that the problem of diffusion is reduced if the transition probabilities approach 0 or 1. Under this condition, standard HMMs have very limited modeling capabilities, but input/output HMMs can still perform interesting computations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Bengio and P. Frasconi. </author> <title> Credit assignment through time: Alternatives to backpropagation. </title> <editor> In J. D. Cowan, et al., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: A simple consequence of the theorem for stochastic matrices is the following: Corollary 1 Suppose A is a primitive stochastic matrix. Then its largest eigenvalue is 1 and there is only one corresponding right eigenvector, which is 1 = <ref> [1; 1 1] </ref> 0 . Furthermore, all other eigenvalues &lt; 1. Proof. A1 = 1 by definition of stochastic matrices. This eigenvector is unique and all other eigenvalues &lt; 1 by the Perron-Frobenius Theorem. <p> These arguments were also supported by experiments on artificial data, studying the phenomenon of diffusion of credit and the corresponding difficulty in training HMMs to learn long-term dependencies. IOHMMs <ref> [1] </ref> introduce a reparameterization of the problem: instead of directly learning the transition probabilities, we learn parameters of a function of an input sequence. Even with a fully connected topology, transition probabilities computed at each time step might be very close to 0 and 1. <p> Even with a fully connected topology, transition probabilities computed at each time step might be very close to 0 and 1. Because of the non-stationarity, more interesting computations can emerge than the simple cycles studied above. For example in [3] we found IOHMMs effective in grammar inference tasks. In <ref> [1] </ref> comparative experiments were performed with a preliminary version of IOHMMs and other algorithms such as recurrent networks, on artificial data on which the span of long-term dependencies was controlled. IOHMMs were found much better than the other algorithms at learning these tasks.
Reference: [2] <author> Y. Bengio and P. Frasconi. </author> <title> An Input Output HMM Architecture. In this volume: </title> <editor> J. D. Cowan, et al., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: We thus called Input/Output HMM (IOHMM) this kind of non-homogeneous HMM. In <ref> [3, 2] </ref> we proposed a connectionist implementation of IOHMMs. In both cases, training requires propagating forward probabilities and backward probabilities, taking products with the transition probability matrix or its transpose.
Reference: [3] <author> Y. Bengio and P. Frasconi. </author> <title> An EM approach to learning sequential behavior. </title> <type> Technical Report RT-DSI-11/94, </type> <institution> University of Florence, </institution> <year> 1994. </year>
Reference-contexts: We thus called Input/Output HMM (IOHMM) this kind of non-homogeneous HMM. In <ref> [3, 2] </ref> we proposed a connectionist implementation of IOHMMs. In both cases, training requires propagating forward probabilities and backward probabilities, taking products with the transition probability matrix or its transpose. <p> Even with a fully connected topology, transition probabilities computed at each time step might be very close to 0 and 1. Because of the non-stationarity, more interesting computations can emerge than the simple cycles studied above. For example in <ref> [3] </ref> we found IOHMMs effective in grammar inference tasks. In [1] comparative experiments were performed with a preliminary version of IOHMMs and other algorithms such as recurrent networks, on artificial data on which the span of long-term dependencies was controlled.
Reference: [4] <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper presents an important new element in our research on the problem of learning long-term dependencies in sequences. In our previous work <ref> [4] </ref> we found theoretical reasons for the difficulty in training recurrent networks (or more generally parametric non-linear dynamical systems) to learn long-term dependencies. <p> The extent of the long-term context is controlled by the self transition probabilities of states 2 and 5, = P (q t = 2jq t1 2) = P (q t = 5jq t1 = 5). Span or "half-life" is log (:5)= log (), i.e. span = :5). Following <ref> [4] </ref>, data was generated for various span of long-term dependencies (0:1 to 1000). For each series of experiments, varying the span, 20 different training trials were run per span value, with 100 training sequences 2 .
Reference: [5] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 7(1), </volume> <year> 1995. </year>
Reference-contexts: Each of these sub-state variables can operate at a different time scale, thus allowing credit to propagate over long temporal spans for some of these variables. Another interesting issue to be investigated is whether techniques of symbolic prior knowledge injection (such as in <ref> [5] </ref>) can be exploited to choose good topologies. One advantage, compared to traditional neural network approaches, is that the model has an underlying finite state structure and is thus well suited to inject discrete transition rules.
Reference: [6] <author> E. Seneta. </author> <title> Nonnegative Matrices and Markov Chains. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: A k &gt; 0. An irreducible matrix is either periodic or primitive (i.e. of period 1). A primitive stochastic matrix is necessarily allowable. 2.2 The Perron-Frobenius Theorem Theorem 1 (See <ref> [6] </ref>, Theorem 1.1.) Suppose A is an n fi n non-negative primitive matrix. <p> We discuss how ergodicity coefficients can be used to measure the difficulty in learning long-term dependencies. Finally, we find that in order to avoid all diffusion, the transitions should be deterministic (0 or 1 probability). 3.1 Training Standard HMMs Theorem 2 (See <ref> [6] </ref>, Theorem 4.2.) If A is a primitive stochastic matrix, then as t ! 1, A t ! 1v 0 where v 0 is the unique stationary distribution of the Markov chain. The rate of approach is geometric. <p> Furthermore, o (A 1 A 2 ) o (A 1 )o (A 2 )(see <ref> [6] </ref>). 3.3 Products of Stochastic Matrices Let A (1;t) = A 1 A 2 A t1 A t denote a forward product of stochastic matrices A 1 ; A 2 ; A t . <p> Theorem 3 (See <ref> [6] </ref>, Lemma 3.3 and 3.4.) Let A (1;t) a forward product of non-negative and allowable matrices, then the products A (1;t) are weakly ergodic if and only if the following conditions both hold: 1. 9t 0 s.t. <p> matrices, row-proportionality is equivalent to row-equality since rows sum to 1. lim t!1 A (t 0 ;t) does not need to exist in order to have weak ergodicity. 3.4 Canonical Decomposition and Periodic Graphs Any non-negative matrix A can be rewritten by relabeling its indices in the following canonical decomposition <ref> [6] </ref>, with diagonal blocks B i , C i and Q: A = B @ 0 B 2 0 0 0 C s+1 0 0 0 0 C r 0 1 C (1) where B i and C i are irreducible, B i are primitive and C i are periodic. <p> A more difficult case is the one of (A (t 0 ;t) ) jk with initial state j 2 S C i . Let d i be the period of the i th periodic block C i . It can be shown <ref> [6] </ref> that taking d products of periodic matrices with the same incidence matrix and period d yields a block diagonal matrix whose d blocks are primitive. Thus C (t 0 ;t) retains information about the initial block in which q t was.
References-found: 6

