URL: http://www.cs.purdue.edu/research/PaCS/ps/eclipse_ft_cpe.ps
Refering-URL: http://www.cs.purdue.edu/research/PaCS/eclipse.html
Root-URL: http://www.cs.purdue.edu
Title: Fail-Safe Concurrency in the EcliPSe System  
Author: Felipe Knop and Vernon Rego and Vaidy Sunderam 
Note: Research supported in part by NATO-CRG900108, ONR-9310233, ONR-9310278, and ARO-93G0045 Research supported by CNPq-Brazil, process number 260059/91.9.  
Date: May 3, 1995  
Address: West Lafayette, Indiana 47907  Atlanta, Georgia 30322  
Affiliation: Department of Computer Sciences Purdue University  Department of Math and Computer Science Emory University  
Abstract: Local or wide-area heterogeneous workstation clusters are relatively cheap and highly effective, though inherently unstable operating environments for long-running distributed computations. We found this to be the case in early experiments with a prototype of the EcliPSe system, a software toolkit for replicative applications on heterogeneous workstation clusters. Hardware or network failures in computations that executed for over a day were not uncommon. In this work, a variety of features for the incorporation of failure resilience in the EcliPSe system are described. Key characteristics of this fault-tolerant system are ease of use, low state-saving cost, system scalability, and good performance. We present results of some experiments demonstrating low state-saving overheads and small system recovery times, as a function of the amount of state saved. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.P. Birman and T.A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <journal> Operating Systems Review, </journal> <volume> 21(5) </volume> <pages> 123-138, </pages> <year> 1987. </year>
Reference-contexts: The drawback of the last approach is that modifications are required of the operating system so that necessary paging information can be accessed. Future operating systems may make this paging information available to application programs. Few cluster-computing systems have reported fault-tolerant support. ISIS <ref> [1] </ref> specializes in group communication based on atomic broadcasts to a process group. Faulty processes are not replaced; the user is expected to have had initiated backup processes, which are activated upon failure.
Reference: [2] <author> K.P. Birman and T.A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference: [3] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> A message system supporting fault tolerance. </title> <journal> Operating Systems Review, </journal> <volume> 17(5) </volume> <pages> 90-99, </pages> <year> 1983. </year>
Reference: [4] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: An overview of the problems of fault-tolerance can be found in [6]. A particularly difficult task is the saving of distributed system state. This problem was described in [21], initially solved in <ref> [4] </ref>, and later addressed by a number of authors ([2, 3, 8, 13, 15, 20, 24, 29], to mention only a few). This multiplicity of solutions stems partly from assumptions of different failure semantics [6] and architectures.
Reference: [5] <author> H. Clark and B. McMillin. </author> <title> DAWGS a distributed compute server utilizing idle workstations. </title> <journal> Journal of parallel and distributed computing, </journal> <volume> 14 </volume> <pages> 175-186, </pages> <year> 1992. </year>
Reference-contexts: Faulty processes are not replaced; the user is expected to have had initiated backup processes, which are activated upon failure. REM [23] relies on replicated processes to achieve fault tolerance, an approach which consumes CPU resources even in the absence of failures. DAWGS <ref> [5] </ref> checkpoints processes periodically by copying process images (user and kernel space) and open files to stable storage. Apparently, neither of the latter two systems attempts to save the state of the entire distributed application.
Reference: [6] <author> F. Cristian. </author> <title> Understanding fault-tolerant distributed systems. </title> <journal> Communications of the ACM, </journal> <volume> 34(2) </volume> <pages> 56-78, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Without fault tolerance, these computations may never run to completion (for example, see the analysis in [30]). Despite significant attention given to the problem of fault-tolerance in recent years, reliable distributed systems are notoriously difficult to design. An overview of the problems of fault-tolerance can be found in <ref> [6] </ref>. A particularly difficult task is the saving of distributed system state. This problem was described in [21], initially solved in [4], and later addressed by a number of authors ([2, 3, 8, 13, 15, 20, 24, 29], to mention only a few). <p> This problem was described in [21], initially solved in [4], and later addressed by a number of authors ([2, 3, 8, 13, 15, 20, 24, 29], to mention only a few). This multiplicity of solutions stems partly from assumptions of different failure semantics <ref> [6] </ref> and architectures. For example, in [7] it is assumed that processor clocks are synchronized within * units of each other and that interprocess communication delays are smaller than a quantity ffi with high probability.
Reference: [7] <author> F. Cristian and F. Jahanian. </author> <title> A timestamp-based checkpointing protocol for long-lived distributed computations. </title> <booktitle> In 10th Symposium on Reliable Distributed Systems, IEEE, </booktitle> <pages> pages 12-20, </pages> <year> 1991. </year>
Reference-contexts: This problem was described in [21], initially solved in [4], and later addressed by a number of authors ([2, 3, 8, 13, 15, 20, 24, 29], to mention only a few). This multiplicity of solutions stems partly from assumptions of different failure semantics [6] and architectures. For example, in <ref> [7] </ref> it is assumed that processor clocks are synchronized within * units of each other and that interprocess communication delays are smaller than a quantity ffi with high probability.
Reference: [8] <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> The performance of consistent checkpointing. </title> <booktitle> In 11th Symposium on Reliable Distributed Systems,IEEE, </booktitle> <pages> pages 39-47, </pages> <year> 1992. </year>
Reference-contexts: With the same objective, the work described in [20] relies on a broadcast medium to allow for the non-intrusive recording of messages which may then be selectively replayed in the event of recovery. Also application-transparent, the work in <ref> [8] </ref> focuses on reducing state-saving overhead through the use of incremental copy-on-write checkpointing and log-structured files (as stable storage). The drawback of the last approach is that modifications are required of the operating system so that necessary paging information can be accessed.
Reference: [9] <author> G. Fox. </author> <title> Parallel computing comes of age: supercomputer level parallel computations at Caltech. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 1 </volume> <pages> 63-103, </pages> <year> 1989. </year>
Reference-contexts: The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication. Typical examples include general simulations (e.g., discrete-event, stochastic or Monte Carlo) [22], or even coarse-grained data-parallel computations (e.g., domain decomposition) <ref> [9] </ref>. In the general case, partial results from each computing process are accumulated over all processes and placed at a central process in real time.
Reference: [10] <author> P. Heidelberger. </author> <title> Discrete event simulations and parallel processing: statistical properties. </title> <journal> SIAM journal on scientific computing, </journal> <volume> 9(6) </volume> <pages> 1114-1132, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: For example, suppose that each sampler generates a result array of 10 double precision numbers for the monitor. This data item is declared in a main program as follows: eclipse_decls - double type_result <ref> [10] </ref>; - The eclipse decls block defines the region that the preprocessor must act on. <p> For an n-sampler system, D [M C (j)] generally corresponds to a state in which the monitor has received a cumulative total of j i results from sampler i, 1 i n. In the special case of tree-combining for replicative simulation applications, bias-free estimation <ref> [10] </ref> requires that j i = j for 1 i n. In other words, tree-combining forces the monitor to obtain the same number of samples from each sampler at the end of each phase. <p> In a deterministic computation, such missing data will guarantee results that are different from the results that would have been generated by a failure-free computation. Even in a stochastic computation, such missing data is guaranteed to bias estimates or cause result inconsistency <ref> [10] </ref>. Similar problems may occur also in the absence of tree-combining. For a few applications this aberrant behavior may be acceptable. For example, results generated by different samplers may be statistically indistinguishable from one another and therefore replaceable. For most applications, however, and in the interests of bias prevention [10], such <p> inconsistency <ref> [10] </ref>. Similar problems may occur also in the absence of tree-combining. For a few applications this aberrant behavior may be acceptable. For example, results generated by different samplers may be statistically indistinguishable from one another and therefore replaceable. For most applications, however, and in the interests of bias prevention [10], such behavior is unacceptable. To avoid result inconsistency, we provide a data versioning scheme in EcliPSe. Several versions of checkpoint data are saved and, in the event of a process failure, system state is restored to the most recent stable state.
Reference: [11] <author> F. Knop, E. Mascarenhas, V. Rego, and V. Sunderam. </author> <title> An introduction to fault tolerant parallel simulation with EcliPSe. </title> <booktitle> In Winter Simulation Conference, </booktitle> <pages> pages 700-707, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: In essence, we provide application checkpointing and rollback using a little help from the programmer to indicate what must be saved - 3 along with some application structuring so that not much state is saved at each checkpoint. The features we describe for fault-tolerance are embedded in EcliPSe <ref> [11, 18, 22, 25] </ref>, a toolkit for heterogeneous cluster computing. The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication. <p> As is evident from Figure 8, the monitor's occupation-level for 32 processors is large enough to make execution more sensitive to an increase in state size. A performance monitoring tool described in <ref> [11] </ref> led us determine that incoming packet rate at the monitor was greater than 160 packets/second, for a run consisting of 32 processors with 12800 bytes of extra state saved at checkpoints.
Reference: [12] <author> F. Knop, V. Rego, and V. Sunderam. </author> <title> Fault-tolerant Models in EcliPSe: User Perspectives. </title> <type> Technical report, </type> <institution> Purdue University (in preparation), </institution> <year> 1995. </year>
Reference: [13] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference: [14] <author> J. Le on, A.L. Fisher, and P. Steenkiste. </author> <title> Fail-safe PVM: a portable package for distributed programming with transparent recovery. </title> <type> Technical Report CMU-CS-93-124, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: An alternative to such application-dependent fault tolerance is application-independent fault tolerance, implemented at the message passing level. Fail-safe PVM <ref> [14] </ref> a system which adds fault-tolerance support to the PVM library [26] provides application-independent fault tolerance. A key advantage of this approach is that it does not require application-code modification.
Reference: [15] <author> P-J Leu and B. Bhargava. </author> <title> Concurrent robust checkpointing and recovery in distributed systems. </title> <booktitle> In Fourth Conference on Data Engineering, IEEE, </booktitle> <pages> pages 154-163, </pages> <year> 1988. </year>
Reference: [16] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> An efficient checkpoint method for multicomputers with wormhole routing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(3), </volume> <year> 1991. </year>
Reference-contexts: For example, in [7] it is assumed that processor clocks are synchronized within * units of each other and that interprocess communication delays are smaller than a quantity ffi with high probability. The work in <ref> [16] </ref> is applicable to multicomputers, and takes advantage of their interconnection topology to reduce the number of messages required at a checkpoint. The idea of making support for fault-tolerance transparent at the application level is clearly an attractive one.
Reference: [17] <author> E. Mascarenhas and V. Rego. GenA: </author> <title> A GUI for Generation of ACES Applications. </title> <type> Technical report, </type> <institution> Purdue University (in preparation), </institution> <year> 1995. </year>
Reference: [18] <author> H. Nakanishi, V. Rego, and V. Sunderam. </author> <title> Superconcurrent simulation of polymer chains on heterogeneous networks. 1992 Gordon Bell Prize Paper, </title> <booktitle> Proceedings of the Fifth High-Performance Computing and Communications Conference: Supercomputing '92, </booktitle> <month> November </month> <year> 1992. </year> <month> 30 </month>
Reference-contexts: Often, cluster computing is performed with a set of homogeneous workstations sharing a network file server on a local network. More recently, the term cluster computing has also come to mean the use of heterogeneous processors, including hardware multiprocessors, which communicate over wide-area networks <ref> [18, 19] </ref>. Future availability of large numbers of underutilized high-performance machines at different institutions interconnected via fiber-optic links will only encourage heterogeneous cluster-based computing. Indeed, several software systems for workstation-cluster computing are already available. <p> In essence, we provide application checkpointing and rollback using a little help from the programmer to indicate what must be saved - 3 along with some application structuring so that not much state is saved at each checkpoint. The features we describe for fault-tolerance are embedded in EcliPSe <ref> [11, 18, 22, 25] </ref>, a toolkit for heterogeneous cluster computing. The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication.
Reference: [19] <author> I. Peterson. </author> <title> Mix-and-match computing: Scientific computing without supercomputers. </title> <journal> Science News, </journal> <volume> 143(18) </volume> <pages> 280-284, </pages> <year> 1993. </year>
Reference-contexts: Often, cluster computing is performed with a set of homogeneous workstations sharing a network file server on a local network. More recently, the term cluster computing has also come to mean the use of heterogeneous processors, including hardware multiprocessors, which communicate over wide-area networks <ref> [18, 19] </ref>. Future availability of large numbers of underutilized high-performance machines at different institutions interconnected via fiber-optic links will only encourage heterogeneous cluster-based computing. Indeed, several software systems for workstation-cluster computing are already available.
Reference: [20] <author> M.L. Powell and D.L. Presotto. </author> <title> PUBLISHING: A reliable broadcast communication mechanism. </title> <journal> Operating Systems Review, </journal> <volume> 17(5) </volume> <pages> 100-109, </pages> <year> 1983. </year>
Reference-contexts: Borg et al provide such transparency at the cost of having to record every message issued 2 by any process; the operating system is made to provide combined support for message passing and fault--tolerance. With the same objective, the work described in <ref> [20] </ref> relies on a broadcast medium to allow for the non-intrusive recording of messages which may then be selectively replayed in the event of recovery. Also application-transparent, the work in [8] focuses on reducing state-saving overhead through the use of incremental copy-on-write checkpointing and log-structured files (as stable storage).
Reference: [21] <author> B. Randell. </author> <title> System structure for software fault-tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2), </volume> <year> 1975. </year>
Reference-contexts: Despite significant attention given to the problem of fault-tolerance in recent years, reliable distributed systems are notoriously difficult to design. An overview of the problems of fault-tolerance can be found in [6]. A particularly difficult task is the saving of distributed system state. This problem was described in <ref> [21] </ref>, initially solved in [4], and later addressed by a number of authors ([2, 3, 8, 13, 15, 20, 24, 29], to mention only a few). This multiplicity of solutions stems partly from assumptions of different failure semantics [6] and architectures.
Reference: [22] <author> V. J. Rego and V. S. Sunderam. </author> <title> Experiments in Concurrent Stochastic Simulation: The EcliPSe Paradigm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(1) </volume> <pages> 66-84, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: In essence, we provide application checkpointing and rollback using a little help from the programmer to indicate what must be saved - 3 along with some application structuring so that not much state is saved at each checkpoint. The features we describe for fault-tolerance are embedded in EcliPSe <ref> [11, 18, 22, 25] </ref>, a toolkit for heterogeneous cluster computing. The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication. <p> The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication. Typical examples include general simulations (e.g., discrete-event, stochastic or Monte Carlo) <ref> [22] </ref>, or even coarse-grained data-parallel computations (e.g., domain decomposition) [9]. In the general case, partial results from each computing process are accumulated over all processes and placed at a central process in real time. <p> Declarations make explicit the flow of information between these processes and allow automatic data conversion for execution on heterogeneous environments. 3 The term sampler denotes a process that does computational work. It was originally used to describe statistical-sampling computations which formed the basis for the EcliPSe prototype <ref> [22] </ref>. 4 The term monitor denotes a process which oversees work performed by samplers. 5 EcliPSe declarations are handled by a special preprocessor, which allows the user to make declarations using a C-like syntax.
Reference: [23] <author> G.C. Shoja. </author> <title> A distributed facility for load sharing and parallel processing among workstations. </title> <journal> Journal of systems and software, </journal> <volume> 14 </volume> <pages> 163-172, </pages> <year> 1991. </year>
Reference-contexts: Few cluster-computing systems have reported fault-tolerant support. ISIS [1] specializes in group communication based on atomic broadcasts to a process group. Faulty processes are not replaced; the user is expected to have had initiated backup processes, which are activated upon failure. REM <ref> [23] </ref> relies on replicated processes to achieve fault tolerance, an approach which consumes CPU resources even in the absence of failures. DAWGS [5] checkpoints processes periodically by copying process images (user and kernel space) and open files to stable storage.
Reference: [24] <author> L.M. Silva and J.G. Silva. </author> <title> Global checkpointing for distributed programs. </title> <booktitle> In 11th Symposium on Reliable Distributed Systems, IEEE, </booktitle> <pages> pages 155-162, </pages> <year> 1992. </year>
Reference: [25] <author> V. S. Sunderam and V. J. Rego. </author> <title> EcliPSe: A system for High Performance Concurrent Simulation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(11) </volume> <pages> 1189-1219, </pages> <year> 1991. </year>
Reference-contexts: In essence, we provide application checkpointing and rollback using a little help from the programmer to indicate what must be saved - 3 along with some application structuring so that not much state is saved at each checkpoint. The features we describe for fault-tolerance are embedded in EcliPSe <ref> [11, 18, 22, 25] </ref>, a toolkit for heterogeneous cluster computing. The toolkit is primarily geared towards replicative applications, where a computation is replicated on distinct processors with minimal interprocess communication.
Reference: [26] <author> V.S. Sunderam. </author> <title> PVM: a framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: An alternative to such application-dependent fault tolerance is application-independent fault tolerance, implemented at the message passing level. Fail-safe PVM [14] a system which adds fault-tolerance support to the PVM library <ref> [26] </ref> provides application-independent fault tolerance. A key advantage of this approach is that it does not require application-code modification.
Reference: [27] <author> B. Topol. Conch: </author> <title> Second generation heterogeneous computing. </title> <type> Technical report, Master thesis, </type> <institution> Department of Math and Computer Science, Emory University, </institution> <year> 1992. </year>
Reference-contexts: Action 1 is performed by Conch <ref> [27] </ref>, a virtual machine and interprocess communication library that provides low level support to EcliPSe. Once Conch creates the replacement process and restores the virtual machine topology, it hands over to EcliPSe the task of recovery.
Reference: [28] <author> L.H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical report, </type> <institution> Engineering Research Center for Computational Field Simulation, Mississippi State University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Cluster computing, a low-cost alternative to supercomputers, involves the use of workstation clusters to solve compute-intensive problems with algorithms that are amenable to distribution <ref> [28] </ref>. In recent years this mode of computation has grown to envelop an increasing number of applications, primarily for scientific problems. Often, cluster computing is performed with a set of homogeneous workstations sharing a network file server on a local network. <p> Future availability of large numbers of underutilized high-performance machines at different institutions interconnected via fiber-optic links will only encourage heterogeneous cluster-based computing. Indeed, several software systems for workstation-cluster computing are already available. Turcotte <ref> [28] </ref> presents an overview of the state of practice on this subject, including a description of no less than 60 software packages that enable users to unleash the combined computing power of networked workstations.
Reference: [29] <author> Y.-M. Wang and W.K. Fuchs. </author> <title> Lazy checkpoint coordination for bounding rollback propagation. </title> <booktitle> In 12th Symposium on Reliable Distributed Systems, IEEE, </booktitle> <pages> pages 78-85, </pages> <year> 1993. </year>
Reference: [30] <author> C. Zhang and C.-Q. Yang. </author> <title> Analytical analysis of reliability for executing remote programs on idling workstations. </title> <booktitle> In Ninth Annual International Phoenix Conference on Computers and Communications, IEEE, </booktitle> <pages> pages 10-16, </pages> <year> 1990. </year> <pages> APPENDIX </pages>
Reference-contexts: Typical examples include replicative applications with data parallelism or distinct simulation runs. Reliable cluster computing, however, is still a problem for long-running computations (e.g., day-long or week-long computations). Without fault tolerance, these computations may never run to completion (for example, see the analysis in <ref> [30] </ref>). Despite significant attention given to the problem of fault-tolerance in recent years, reliable distributed systems are notoriously difficult to design. An overview of the problems of fault-tolerance can be found in [6]. A particularly difficult task is the saving of distributed system state.
References-found: 30

