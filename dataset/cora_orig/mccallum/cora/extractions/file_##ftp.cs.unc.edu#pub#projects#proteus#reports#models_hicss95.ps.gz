URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/models_hicss95.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/proteus/proteus-publications.html
Root-URL: http://www.cs.unc.edu
Title: Models and Resource Metrics for Parallel and Distributed Computation  
Author: Zhiyong Li, Peter H. Mills, John H. Reif 
Note: Proc. 28th Annual Hawaii International Conference on System Sciences (HICSS-28 Parallel Al gorithms Software Technology Track), (Wailea, Maui, Hawaii, January 3-6), IEEE Press, 1995. Abstract  
Address: Durham, N.C. 27708-0129  
Affiliation: Department of Computer Science, Duke University,  
Abstract: This paper presents a framework of using resource met-rics to characterize the various models of parallel computation. Our framework reflects the approach of recent models to abstract architectural details into several generic parameters, which we call resource metrics. We examine the different resource metrics chosen by different parallel models, categorizing the models into four classes: the basic synchronous models, and extensions of the basic models which more accurately reflect practical machines by incorporating notions of asynchrony, communication cost and memory hierarchy. We then present a new parallel computation model, the LogP-HMM model, as an illustration of design principles based on the framework of resource metrics. The LogP-HMM model extends an existing parameterized network model (LogP) with a sequential hierarchical memory model (HMM) characterizing each processor. The result accurately captures both network communication costs and the effects of multileveled memory such as local cache and I/O. We examine the potential utility of our model in the design of near optimal sorting and FFT algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [AACS87] <author> A. Aggarwal, B. Alpern, A. Chandra, and M. Snir, </author> <title> "A model for hierarchical memory," </title> <booktitle> in Proc. 19th ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 305-314, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: For example, a sequential computer is suitably char-acterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF90] </ref>, reflect these resources quite well and therefore provide a common base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines. <p> Then each processor simultaneously sorts N P elements in its local memory and no communication between processors are required. In this case, the sorting lower bound for N P elements in each processor is also the lower bound for the whole sorting procedure. By the result in <ref> [AACS87] </ref>, the sorting lower bound for N P elements in HMM model with memory access cost f (x)=log x is exactly ( N P log N P ): A similar argument can be used for FFT computation. Therefore we prove the theorem. 2 FFT Algorithm 1. <p> After each processor accepts the N P 2 messages, it begins Step II which comprises the computation of the non-input nodes of l disjoint P -input butterflies. By the result of <ref> [AACS87] </ref>, Step I computation needs O (m log m log log m) time. <p> Sorting. A near optimal sorting algorithm can be obtained by using columnsort [Lei85] and the modified median-sort algorithm proposed in <ref> [AACS87] </ref>. We will denote this modified median-sort algorithm as HMM-sorting in the rest of the paper and we will use the result that the HMM-sorting can sort N elements in O (N log N log log N ) time.
Reference: [AC94] <author> B. Alpern and L. Carter, </author> <title> "Towards a model for portable parallel performance: exposing the memory hierarchy," </title> <booktitle> in Portability and Performance for Parallel Processing, </booktitle> <pages> pp. 21-41, </pages> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH <ref> [AC94] </ref>, and P-UMH [NV91]). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource metrics. <p> This leads an optimal algorithm for large problem instances and reasonable g value. 4.3. Hierarchical models The Parallel Memory Hierarchy model (PMH) <ref> [AC94] </ref> and Parallel Hierarchical Memory model (P-HMM) [VS94] discussed in this section address the concerns of memory hierarchy in a parallel setting. The P-HMM primarily originates from considering in a parallel network the existence of secondary or disk memory. <p> PMH on the other hand uses "memory hierarchy" as a more general technique to model not only the hierarchy within a processor but also the communication characteristics of a parallel machine. Parallel Memory Hierarchy model. The PMH is a so-called generic model which defines a class of specific models <ref> [AC94] </ref>. In the PMH model, a parallel computer is modeled as a tree and each node of the tree is called a module. All of the leaf modules are used to denote the processors and the internal modules hold the data. <p> Even though this model is termed a parallel memory hierarchy, the internal modules need not necessarily correspond to actual memory modules of the real machine; when modeling the CM-5 for example, many of the modules are used to capture the interprocessor communication capabilities <ref> [AC94] </ref>. The PMH model derives from the developers' experience in tuning code for memory hierarchy. Many sequential algorithms have been developed for the original sequential UMH model. However, perhaps because of the complexity and generality of the model, not too many algorithms have been developed for the PMH parallel model.
Reference: [ACF90] <author> B. Alpern, L. Carter, and E. Feig, </author> <title> "Uniform memory hierarchies," </title> <booktitle> in Proc. 31st IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: For example, a sequential computer is suitably char-acterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF90] </ref>, reflect these resources quite well and therefore provide a common base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines.
Reference: [ACS87] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "Hierarchical memory with block transfer," </title> <booktitle> in Proc. 28th Symp. on Foundations of Computer Science, </booktitle> <pages> pp. 204-216, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: For example, a sequential computer is suitably char-acterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF90] </ref>, reflect these resources quite well and therefore provide a common base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines.
Reference: [ACS89] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "On communication latency in PRAM computation," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-21, </pages> <year> 1989. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM <ref> [ACS89] </ref>, Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> LPRAM. The Local-Memory PRAM (LPRAM) model <ref> [ACS89] </ref> consists of a shared global memory and a set of processors with unbounded local memory executing in lock-step. The access protocol to global memory is CREW.
Reference: [ACS90] <author> A. Aggarwal, A. K. Chandra, and M. Snir, </author> <title> "Communication complexity of PRAMs," </title> <journal> J. Theoretical Computer Science, </journal> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: BPRAM. An extension of the LPRAM, the Block PRAM (BPRAM), is described in <ref> [ACS90] </ref>. The BPRAM takes into account the reduced cost for transferring a contiguous block of data. The BPRAM model is defined with two parameters L (latency or startup time) and b (block size). The cost of accessing local memory is unit time.
Reference: [AV88] <author> A. Aggarwal and J. Vitter, </author> <title> "The input/output complexity of sorting and related problems," </title> <journal> Comm. ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 1116-1127, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks. Therefore it is necessary to design parallel algorithms which consider the possible data movement between main and secondary memory <ref> [AV88] </ref>, and more generally which consider multiple levels of memory including register and cache.
Reference: [Ble90] <author> G. E. Blelloch, </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The most recent variation is QRQW [GMR94], which assumes that simultaneous access to the same memory block will be inserted into a request queue and served in a FIFO manner. VRAM. Another extension of the serial RAM model is the Vector Random Access Machine (VRAM) <ref> [Ble90] </ref>. The VRAM is a serial random access machine with the addition of a vector memory, a vector processor, and vector input and output ports. Typical vector instructions include elementwise operations, data movement operations, scans, and packs.
Reference: [BNK92] <author> A. Bar-Noy and S. Kipnis, </author> <title> "Designing broadcasting algorithms in the Postal model for message-passing systems," </title> <booktitle> in Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-22, </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model <ref> [BNK92] </ref>, BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> The cost of accessing local memory is unit time. However the cost of transmitting a block of size b of contiguous locations from global memory is L + b . Postal model. The Postal model <ref> [BNK92] </ref> is a distributed memory model with the constraint that the point-to-point communication has latency . Several elegant optimal broadcast and summation algorithms have been designed based on this model, which were then extended for the LogP model [KSSS93].
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> "LogP: Towards a realistic model of parallel computation," </title> <booktitle> in Proc. 4th ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 1-12, </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP <ref> [CKP + 93] </ref>), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> The communication overhead is the time that the processor engages in sending and receiving a message. In most cases, the value is dependent on the communication protocol implemented in a practical machine. For example, in the CM-5 it could be a linear function of the message size <ref> [CKP + 93] </ref>. Block transfer capability. In most architectures, a significant cost (latency) is incurred to access the first of a contiguous block of words, but after that, successive words can be accessed in unit time. Memory hierarchy. <p> The LogP model uses the parameters L (an upper bound of latency for transmitting a single message), o (the computation overhead of handling a message), g (a lower bound of time interval between consecutive message transmissions at a processor) and P (the number of processors) <ref> [CKP + 93] </ref>. In contrast to the BSP model, it removes the barrier synchronization requirement (h-relation in BSP) and allows the processors to run asynchronously. <p> Examples using this strategy can be found in the FFT algorithm discussed below and also in [KSSS93]. FFT. The data layout and communication scheduling are two key aspects to achieving an effective algorithm for the FFT problem under the LogP model. Three methods of data layout are discussed in <ref> [CKP + 93] </ref>. The cyclic layout assigns the ith row of the butterfly to the ith processor. The block layout places the first N P rows on the first processor, the next N P rows on the second processor, and so on.
Reference: [CZ89] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating asynchrony into the PRAM model," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 169-178, </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: This problem has spurred the development of several extensions of the PRAM which attempt to make the model more practical while still preserving much of its simplicity. The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM <ref> [CZ89] </ref>), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH <p> It is worth noting that a variant of the Phase PRAM, the Phase LPRAM model, accounts as well for the cost of communication latency. APRAM. The Asynchronous PRAM (APRAM) is a "fully" asynchronous model <ref> [CZ89] </ref>. The APRAM model consists of a global shared memory and a set of processes with their own local memories. The basic operations executed by the APRAM process are called events. An APRAM computation is denoted as the set of possible serializations of events executed by the processes. <p> For an algorithm the round complexity is defined as the maximum round complexity over all of the possible computations. An example of APRAM algorithm design using these measures for the problem of summation may be found in <ref> [CZ89] </ref>. 4.2.
Reference: [FW78] <author> S. Fortune and J. Wyllie, </author> <title> "Parallelism in random access machines," </title> <booktitle> in Proc.10th ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 114-118, </pages> <year> 1978. </year>
Reference-contexts: Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model <ref> [FW78] </ref>. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free. The PRAM model could be thought as one extreme which makes a large number of assumptions in order to simplify algorithm design.
Reference: [Gib89] <author> P. B. Gibbons, </author> <title> "A more practical PRAM model," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: This problem has spurred the development of several extensions of the PRAM which attempt to make the model more practical while still preserving much of its simplicity. The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM <ref> [Gib89] </ref> and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the <p> Asynchronous models Among the first extensions to the PRAM were the Phase PRAM and APRAM models, which incorporate some notion of asynchronous execution. Phase PRAM. The Phase PRAM <ref> [Gib89] </ref> extends the PRAM model with semi-asynchrony. A Phase PRAM machine consists of a shared global memory, a set of P sequential processors, and a private local memory for each processor. <p> The cost of global read, global write and local operations are the same constant. The cost of a synchronization step is dependent on the number of processors. Space limitations preclude presentation of a Phase PRAM algorithm design for FFT computation; an example may be found in <ref> [Gib89] </ref>. It is worth noting that a variant of the Phase PRAM, the Phase LPRAM model, accounts as well for the cost of communication latency. APRAM. The Asynchronous PRAM (APRAM) is a "fully" asynchronous model [CZ89].
Reference: [GMR94] <author> P. B. Gibbons, Y. Matias, and V. Ramachan-dran, </author> <title> "The QRQW PRAM : Accounting for contention in parallel algorithms," </title> <booktitle> in Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pp. 638-647, </pages> <year> 1994. </year>
Reference-contexts: Protocols include EREW (exclusive read exclusive write), CREW (concurrent read exclusive write), and CRCW (concurrent read concurrent write). The latter protocol can be further divided into several classes by the semantics of the concurrent write. The most recent variation is QRQW <ref> [GMR94] </ref>, which assumes that simultaneous access to the same memory block will be inserted into a request queue and served in a FIFO manner. VRAM. Another extension of the serial RAM model is the Vector Random Access Machine (VRAM) [Ble90].
Reference: [Goo93] <author> M. Goodrich, </author> <title> "Parallel algorithms column I: Models of computation," </title> <journal> SIGACT News, </journal> <volume> vol. 24, </volume> <pages> pp. 16-21, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The challenge then becomes to design a general model of parallel computation in such a way that it balances being sufficiently detailed to reflect realistic aspects impacting performance while still remaining abstract enough to be machine-independent and amenable to analysis <ref> [Ski91, Goo93] </ref>. Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model [FW78]. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free.
Reference: [KSSS93] <author> R. Karp, A. Sahay, E. Santos, and K. Schauser, </author> <title> "Optimal broadcast and summation in the LogP model," </title> <booktitle> in Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 142-153, </pages> <year> 1993. </year>
Reference-contexts: Postal model. The Postal model [BNK92] is a distributed memory model with the constraint that the point-to-point communication has latency . Several elegant optimal broadcast and summation algorithms have been designed based on this model, which were then extended for the LogP model <ref> [KSSS93] </ref>. Algorithms other than broadcast and summation have largely not been presented for this model. Bulk-Synchronous Parallel model. The BSP is a distributed memory model [Val90]. <p> Moreover, it is often reasonable to ignore the parameter of o in a practical machine, such as in a machine with low bandwidth (high g value). Examples using this strategy can be found in the FFT algorithm discussed below and also in <ref> [KSSS93] </ref>. FFT. The data layout and communication scheduling are two key aspects to achieving an effective algorithm for the FFT problem under the LogP model. Three methods of data layout are discussed in [CKP + 93]. The cyclic layout assigns the ith row of the butterfly to the ith processor.
Reference: [Lei85] <author> T. Leighton, </author> <title> "Tight bounds on the complexity of parallel sorting," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 34, no. 3, </volume> <pages> pp. 344-354, </pages> <year> 1985. </year>
Reference-contexts: Sorting. A near optimal sorting algorithm can be obtained by using columnsort <ref> [Lei85] </ref> and the modified median-sort algorithm proposed in [AACS87]. We will denote this modified median-sort algorithm as HMM-sorting in the rest of the paper and we will use the result that the HMM-sorting can sort N elements in O (N log N log log N ) time.
Reference: [NV91] <author> M. Nodine and J. Vitter, </author> <title> "Large-scale sorting in parallel memories," </title> <booktitle> in Proc. 3rd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 29-39, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH <ref> [NV91] </ref>). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource metrics. <p> The model can be extended to allow block transfer; the resulting model is called the P-BT model [VS94]. Several other extensions which use the UMH memory model and PRAM interconnection respectively are discussed in <ref> [NV91] </ref>. Two factors are critical for developing effective P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT. We present the following P-HMM algorithm from [VS94], performing the N -input FFT when N P 2 : 1.
Reference: [Sah92] <author> A. Sahay, </author> <title> "Hiding communication costs in bandwidth-limited parallel FFT computation," </title> <type> Technical Report UCB/CSD 92/722, </type> <institution> UC Berke-ley, </institution> <year> 1992. </year>
Reference-contexts: The naive communication schedule stalls on the first send. The technique of overlapping computation and communication can be used to eliminate this stall for the large problem instances. The method introduced in <ref> [Sah92] </ref> staggers the different starting rows for different processors: processor i starts with its iN P 2 -th row, proceeds to the last row, and wraps around. This leads an optimal algorithm for large problem instances and reasonable g value. 4.3. <p> FFT Algorithm 2. Algorithm 1 basically uses block layout for the input data. This algorithm will use the hybrid data layout and a tighter upper bound can be derived. The idea of the hybrid method has been discussed in section 4.2. A more detailed discussion can be found in <ref> [Sah92] </ref>. Using the similar notation given in [Sah92], we de- note m = N P and l = m P . Two steps are used to compute the N -input FFT. <p> This algorithm will use the hybrid data layout and a tighter upper bound can be derived. The idea of the hybrid method has been discussed in section 4.2. A more detailed discussion can be found in <ref> [Sah92] </ref>. Using the similar notation given in [Sah92], we de- note m = N P and l = m P . Two steps are used to compute the N -input FFT.
Reference: [Ski91] <author> D. Skillicorn, </author> <title> "Models for practical parallel computation," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 133-158, </pages> <year> 1991. </year>
Reference-contexts: The challenge then becomes to design a general model of parallel computation in such a way that it balances being sufficiently detailed to reflect realistic aspects impacting performance while still remaining abstract enough to be machine-independent and amenable to analysis <ref> [Ski91, Goo93] </ref>. Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model [FW78]. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free.
Reference: [Val90] <author> L. Valiant, </author> <title> "A bridging model for parallel computation," </title> <journal> Comm. ACM, </journal> <volume> vol. 33, </volume> <pages> pp. 103-111, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP <ref> [Val90] </ref>, and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> Several elegant optimal broadcast and summation algorithms have been designed based on this model, which were then extended for the LogP model [KSSS93]. Algorithms other than broadcast and summation have largely not been presented for this model. Bulk-Synchronous Parallel model. The BSP is a distributed memory model <ref> [Val90] </ref>. Like the Phase PRAM, the BSP is also a semi-asynchronous model because it requires synchronization after each "su-perstep", within which the processes can run asynchronously. <p> If the length of a superstep is L, then L local operations and a b L g c relation message pattern can be realized. The parameters of the machine are therefore L, g and P (the number of processors). A FFT algorithm for the BSP is described in <ref> [Val90] </ref>. LogP model. The LogP model is motivated by current technological trends in high performance com puting towards networks of large-grained sophisticated processors.
Reference: [VS90] <author> J. Vitter and E. Shriver, </author> <title> "Optimal disk I/O with parallel block transfer," </title> <booktitle> in Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 159-169, </pages> <year> 1990. </year>
Reference-contexts: Many sequential algorithms have been developed for the original sequential UMH model. However, perhaps because of the complexity and generality of the model, not too many algorithms have been developed for the PMH parallel model. Parallel Hierarchical Memory model. The P-HMM model is also called the parallel I/O model <ref> [VS90, VS94] </ref>. It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks.
Reference: [VS94] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Algorithms for parallel memory II: Hierarchical multilevel memories," </title> <journal> Algorithmica, </journal> <year> 1994. </year>
Reference-contexts: APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS89], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multi-leveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM <ref> [VS94] </ref>, PMH [AC94], and P-UMH [NV91]). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource metrics. <p> This leads an optimal algorithm for large problem instances and reasonable g value. 4.3. Hierarchical models The Parallel Memory Hierarchy model (PMH) [AC94] and Parallel Hierarchical Memory model (P-HMM) <ref> [VS94] </ref> discussed in this section address the concerns of memory hierarchy in a parallel setting. The P-HMM primarily originates from considering in a parallel network the existence of secondary or disk memory. <p> Many sequential algorithms have been developed for the original sequential UMH model. However, perhaps because of the complexity and generality of the model, not too many algorithms have been developed for the PMH parallel model. Parallel Hierarchical Memory model. The P-HMM model is also called the parallel I/O model <ref> [VS90, VS94] </ref>. It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks. <p> The model can be extended to allow block transfer; the resulting model is called the P-BT model <ref> [VS94] </ref>. Several other extensions which use the UMH memory model and PRAM interconnection respectively are discussed in [NV91]. Two factors are critical for developing effective P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT. <p> Two factors are critical for developing effective P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT. We present the following P-HMM algorithm from <ref> [VS94] </ref>, performing the N -input FFT when N P 2 : 1. Compute p p N -inputs FFTs. Assume that ith FFT is on the ith contiguous group of p P tracks (or memory levels). 2. <p> ( N P log N Therefore, we have following recurrence T (N; P ) = 2 N T ( N ; P ) + O ( N which gives the result O ( N P log N log log N log P ): This matches the FFT lower bound in <ref> [VS94] </ref> and so is optimal. 5. LogP-HMM model The resource metrics chosen by different parallel models discussed so far are summarized in Table 1. <p> In this paper we will focus on the extension of the LogP model with HMM, which we call LogP-HMM. 5.1. Definition of the model The LogP-HMM model, pictured in Figure 1, is defined much like the parallel hierarchy memory model <ref> [VS94] </ref>. A LogP-HMM machine consists of a set of asynchronously executing processors, each with an unlimited local memory. The local memory is organized as a sequence of layers with increasing size, where the size of layer i is 2 i . <p> Therefore we prove the theorem. 2 FFT Algorithm 1. Using the technique in <ref> [VS94] </ref> and the algorithm given in section 4.3.2, we can com pute the FFT on a LogP-HMM machine. The time needed by this algorithm is O ( L+o P log N P ): Steps 1 and 4 take p p P log N P time. <p> Again the second term is for transferring the data in the mem ory hierarchy. Summing all of them together, we get the following result: O ( N P log log N which matches the lower bound O ( N P log N log log N proven in <ref> [VS94] </ref>. And therefore it is optimal. 6. Conclusions This paper presents a framework of using resource metrics to characterize the various models of parallel computation.
References-found: 23

