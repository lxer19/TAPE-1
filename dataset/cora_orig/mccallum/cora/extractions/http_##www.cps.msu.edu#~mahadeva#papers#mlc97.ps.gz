URL: http://www.cps.msu.edu/~mahadeva/papers/mlc97.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/winter98-mlrg.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (mahadeva,marchall@csee.usf.edu)  (das,gosavi@eng.usf.edu)  
Title: Self-Improving Factory Simulation using Continuous-time Average-Reward Reinforcement Learning  
Author: Sridhar Mahadevan and Nicholas Marchalleck Tapas K. Das and Abhijit Gosavi 
Address: Tampa, Florida 33620  Tampa, FL 33620  
Affiliation: Department of Computer Science and Engineering University of South Florida  Department of Industrial Engineering University of South Florida  
Abstract: Many factory optimization problems, from inventory control to scheduling and reliability, can be formulated as continuous-time Markov decision processes. A primary goal in such problems is to find a gain-optimal policy that minimizes the long-run average cost. This paper describes a new average-reward algorithm called SMART for finding gain-optimal policies in continuous time semi-Markov decision processes. The paper presents a detailed experimental study of SMART on a large unreliable production inventory problem. SMART outperforms two well-known reliability heuristics from industrial engineering. A key feature of this study is the integration of the reinforcement learning algorithm directly into two commercial discrete-event simulation packages, ARENA and CSIM, paving the way for this approach to be applied to many other factory optimization problems for which there already exist simulation models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Mas-sachusetts, </address> <year> 1995. </year>
Reference-contexts: The reward structure is also different: taking an action in a state results in both an immediate lump-sum reward, as well as an accumulated reward generated at some fixed rate until the next decision epoch. We briefly introduce the Semi-Markov Decision Process (SMDP) model here <ref> [1, 11] </ref>.
Reference: [2] <author> S.J. Bradtke and M. Duff. </author> <title> Reinforcement learning methods for continuous-time markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: Finally, it illustrates how RL can be integrated into widely available discrete-event simulation packages (in particular, CSIM and ARENA 1 ). There has been previous work on continuous-time SMDP's in the RL literature. In particular, Bradtke and Duff <ref> [2] </ref> describe how the well-known discounted RL algorithms, including TD () [15] and Q-learning [17], can be extended to SMDP's. Crites and Barto [3] describe an impressive application of Q-learning to a large scale SMDP problem of controlling a group of elevators.
Reference: [3] <author> R. Crites and A. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Neural Information Processing Systems (NIPS). </booktitle> <year> 1996. </year>
Reference-contexts: There has been previous work on continuous-time SMDP's in the RL literature. In particular, Bradtke and Duff [2] describe how the well-known discounted RL algorithms, including TD () [15] and Q-learning [17], can be extended to SMDP's. Crites and Barto <ref> [3] </ref> describe an impressive application of Q-learning to a large scale SMDP problem of controlling a group of elevators. Both these papers primarily focus on an optimality framework, where the goal of the decision maker is to maximize the discounted sum of rewards. <p> Many real-world SMDP's, including the elevator task <ref> [3] </ref> and the production inventory task de scribed below, are unichain. 3 SMART: An Average-Reward Algorithm We now describe a new model-free average-reward algorithm (SMART) for finding gain-optimal policies for SMDP's. The derivation of this algorithm from the Bellman equation for SMDP's (Equation 5) is fairly straightforward. <p> Details of the algorithm are given in Figure 2. The learning rate ff n and the exploration rate p n are both decayed slowly to 0 (we used a Moody-Darken search-then-converge procedure). 3.1 Value Function Approximation in SMDP's In most interesting SMDP's, such as the elevator problem <ref> [3] </ref>, or the production inventory problem described below, the number of states is usually so large as to 2 We use the notation u ff v as an abbreviation for the stochastic approximation update rule u (1 ff)u + ffv. 1. <p> In particular, the state space in our problem is a 10-dimensional integer space. Following Crites and Barto <ref> [3] </ref>, we also used a feed-forward net to represent the action value function. Equation 7 used in SMART is replaced by a step which involves updating the the weights of the net. <p> In these cases, instead of having a single agent governing the whole system, it may be more appropriate to design a hierarchical control system where each subsystem is controlled using separate agents. The elevator problem <ref> [3] </ref> is a simplified example of such a multi-agent system, where the agents are homogeneous and control identical subsystems. Global optimization in a system consisting of heterogeneous agents poses a significantly challenging problem. Sethi and Zhang [13] present an in-depth theoretical study of hierarchical optimization for complex factories.
Reference: [4] <author> T. Das and S. Sarkar. </author> <title> Optimal preventive maintenance in a production inventory system. </title> <note> Submitted. </note>
Reference-contexts: Das and Sarkar <ref> [4] </ref> present a mathematical model of such a system for the case where only a single product is produced. Here we consider a similar system except that we add the extra complexity of multiple products, and multiple buffers.
Reference: [5] <author> F. Van der Duyn Schouten and S. Vanneste. </author> <title> Maintenance optimization of a production system with buffer capacity. </title> <journal> European Journal of Operational Research, </journal> <volume> 82 </volume> <pages> 323-338, </pages> <year> 1995. </year>
Reference-contexts: COR fl can be determined by using a simple gradient descent algorithm to search for the optimum maintenance interval t fl m . Age Replacement (AR): This heuristic has been studied in the context of machine maintenance by Schouten <ref> [5] </ref>. Here, the machine is maintained when it reaches age T . If the machine fails before time T , it is repaired. Both repair and maintenance renew the machine (i.e., reset its age to zero).
Reference: [6] <author> S. Gershwin. </author> <title> Manufacturing Systems Engineering. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: This work is part of a major cross-disciplinary study of industrial process optimization using reinforcement learning. This paper only discussed a production system with a single machine, but real factories are much more complex systems consisting of numerous interrelated subsystems of machines <ref> [6] </ref>. In these cases, instead of having a single agent governing the whole system, it may be more appropriate to design a hierarchical control system where each subsystem is controlled using separate agents.
Reference: [7] <author> I.B. Gertsbakh. </author> <title> Models of Preventive Maintenance. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, Netherlands, </address> <year> 1976. </year>
Reference-contexts: Since we know of no optimal solution to this problem, we compare the results produced by SMART to two well-known heuristic procedures, COR and AR. A brief description of the heuristics follows. Coefficient of Operational Readiness (COR): The coefficient of operational readiness (COR) heuristic <ref> [7] </ref> attempts to maximize the uptime of the machine. The coefficient of operational readiness is given by: uptime uptime + downtime (11) where uptime is defined as the total time spent producing, and downtime is the time spent performing repairs and maintenances. <p> C r 4 It can be shown that the average number of failures does not exceed k = F (t m ) 1F (t m ) , where F (t m ) is the value of the cumulative distribution function at t m of the random variable, time between failures <ref> [7] </ref>. denote the cost of maintenance and repair respectively. The renewal cycle time CT is given as CT = 0 (x+t repair )f (x)dx+ Z 1 (T +t maint )f (x)dx; where f (x) is the probability density function of the time to machine failure.
Reference: [8] <author> A. Law and W. </author> <title> Kelton. Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, USA, </address> <year> 1991. </year>
Reference-contexts: Since dynamic programming methods, such as policy or value iteration, cannot be applied to large SMDP's, a standard approach to studying these problems is through the use of discrete-event simulation <ref> [8] </ref>. Although simulation is valuable in evaluating the expected long-run performance of a fixed policy, it is of limited use in finding good or optimal policies. Reinforcement learning (RL) is an ideal approach to solving large SMDP's, since it can be easily combined with discrete-event simulation models.
Reference: [9] <author> S. Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 159-196, </pages> <year> 1996. </year>
Reference-contexts: Reinforcement learning (RL) is an ideal approach to solving large SMDP's, since it can be easily combined with discrete-event simulation models. However, work in average-reward reinforcement learning has so far been limited to small discrete-time Markov decision processes (MDP's) <ref> [9, 12, 14, 16] </ref>. This paper introduces a new model-free average-reward algorithm for continuous-time SMDP's called SMART. Second, it presents the first large scale experimental test of an average-reward RL algorithm on a realistic multi-product unreliable production inventory system (with a state space in excess of 10 15 states).
Reference: [10] <author> D. Pegden, R. Sadowski, and R. Shannon. </author> <title> Introduction to Simulation using SIMAN. </title> <publisher> McGraw Hill, </publisher> <address> New York, USA, </address> <year> 1995. </year>
Reference-contexts: ARENA is a graphical simulation language based on the well-known SIMAN modeling language <ref> [10] </ref>. The attraction of ARENA is that it allows a nice animation of the simulation, which is useful for understanding and demonstration purposes. However, the results described below were obtained with a simulation model implemented using CSIM, a C++ library of routines for creating process-oriented discrete-event models (see Figure 4).
Reference: [11] <author> M. L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> Wi-ley Interscience, </publisher> <address> New York, USA, </address> <year> 1994. </year>
Reference-contexts: These problems can be formally characterized as continuous-time semi-Markov decision processes (SMDP's), where the goal is to find an optimal policy that minimizes the long-term average cost <ref> [11] </ref>. Since dynamic programming methods, such as policy or value iteration, cannot be applied to large SMDP's, a standard approach to studying these problems is through the use of discrete-event simulation [8]. <p> The reward structure is also different: taking an action in a state results in both an immediate lump-sum reward, as well as an accumulated reward generated at some fixed rate until the next decision epoch. We briefly introduce the Semi-Markov Decision Process (SMDP) model here <ref> [1, 11] </ref>.
Reference: [12] <author> A. Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 298-305. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1993. </year>
Reference-contexts: Reinforcement learning (RL) is an ideal approach to solving large SMDP's, since it can be easily combined with discrete-event simulation models. However, work in average-reward reinforcement learning has so far been limited to small discrete-time Markov decision processes (MDP's) <ref> [9, 12, 14, 16] </ref>. This paper introduces a new model-free average-reward algorithm for continuous-time SMDP's called SMART. Second, it presents the first large scale experimental test of an average-reward RL algorithm on a realistic multi-product unreliable production inventory system (with a state space in excess of 10 15 states).
Reference: [13] <author> S. Sethi and Q. Zhang. </author> <title> Hierarchical Decision Making in Stochastic Manufacturing Systems. </title> <publisher> Birkhauser, </publisher> <year> 1994. </year>
Reference-contexts: The elevator problem [3] is a simplified example of such a multi-agent system, where the agents are homogeneous and control identical subsystems. Global optimization in a system consisting of heterogeneous agents poses a significantly challenging problem. Sethi and Zhang <ref> [13] </ref> present an in-depth theoretical study of hierarchical optimization for complex factories. We are currently exploring such hierarchical extensions of SMART for more complex factory processes, such as jobshops and flowshops. Acknowledgements This research is supported in part by an NSF CAREER Award Grant No. IRI-9501852 (to Sridhar Ma-hadevan).
Reference: [14] <author> S. Singh. </author> <title> Reinforcement learning algorithms for average-payoff Markovian decision processes. </title> <booktitle> In Proceedings of the 12th AAAI. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Reinforcement learning (RL) is an ideal approach to solving large SMDP's, since it can be easily combined with discrete-event simulation models. However, work in average-reward reinforcement learning has so far been limited to small discrete-time Markov decision processes (MDP's) <ref> [9, 12, 14, 16] </ref>. This paper introduces a new model-free average-reward algorithm for continuous-time SMDP's called SMART. Second, it presents the first large scale experimental test of an average-reward RL algorithm on a realistic multi-product unreliable production inventory system (with a state space in excess of 10 15 states).
Reference: [15] <author> R.S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Finally, it illustrates how RL can be integrated into widely available discrete-event simulation packages (in particular, CSIM and ARENA 1 ). There has been previous work on continuous-time SMDP's in the RL literature. In particular, Bradtke and Duff [2] describe how the well-known discounted RL algorithms, including TD () <ref> [15] </ref> and Q-learning [17], can be extended to SMDP's. Crites and Barto [3] describe an impressive application of Q-learning to a large scale SMDP problem of controlling a group of elevators.
Reference: [16] <author> P. Tadepalli and D. </author> <title> Ok. Auto-exploratory average-reward reinforcement learning. </title> <booktitle> In Proceedings of the Thirteenth AAAI, </booktitle> <pages> pages 881-887. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Reinforcement learning (RL) is an ideal approach to solving large SMDP's, since it can be easily combined with discrete-event simulation models. However, work in average-reward reinforcement learning has so far been limited to small discrete-time Markov decision processes (MDP's) <ref> [9, 12, 14, 16] </ref>. This paper introduces a new model-free average-reward algorithm for continuous-time SMDP's called SMART. Second, it presents the first large scale experimental test of an average-reward RL algorithm on a realistic multi-product unreliable production inventory system (with a state space in excess of 10 15 states).
Reference: [17] <author> C.J. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Kings College, </institution> <address> Cambridge, England, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: There has been previous work on continuous-time SMDP's in the RL literature. In particular, Bradtke and Duff [2] describe how the well-known discounted RL algorithms, including TD () [15] and Q-learning <ref> [17] </ref>, can be extended to SMDP's. Crites and Barto [3] describe an impressive application of Q-learning to a large scale SMDP problem of controlling a group of elevators.
References-found: 17

