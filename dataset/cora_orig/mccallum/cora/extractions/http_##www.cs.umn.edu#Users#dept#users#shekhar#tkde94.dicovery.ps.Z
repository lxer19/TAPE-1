URL: http://www.cs.umn.edu/Users/dept/users/shekhar/tkde94.dicovery.ps.Z
Refering-URL: http://www.cs.umn.edu/Users/dept/users/shekhar/
Root-URL: http://www.cs.umn.edu
Title: Learning Transformation Rules for Semantic Query Optimization: A Data-Driven Approach  
Author: Shashi Shekhar, Babak Hamidzadeh, Ashim Kohli, and Mark Coyle 
Keyword: Rule discovery, semantic query optimization, discovery in databases.  
Address: Minneapolis, MN 55455  
Affiliation: Computer Science Dept., University of Minnesota,  
Abstract: Learning query transformation rules is vital for the success of semantic query optimization in domains where the user cannot provide a comprehensive set of integrity constraints. Finding these rules is a discovery task because of the lack of targets. Previous approaches to learning query transformation rules have been based on analyzing past queries. We propose a new approach to learning query transformation rules based on analyzing the existing data in the database. This paper describes a framework and a closure algorithm for learning rules from a given data-distribution. We characterize the correctness, completeness and complexity of the proposed algorithm and provide a detailed example to illustrate the framework. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. J. King, </author> <title> QUIST : A system for semantic query optimization in relational databases, </title> <booktitle> Proc. 7th VLDB Conf., </booktitle> <year> (1981). </year>
Reference-contexts: The resulting rule-set is then used by a semantic query optimizer to transform a user-specified query into a semantically equivalent query that has a smaller execution cost. 1.2. Example Database We will use a well-known database schema <ref> [1] </ref> to illustrate various definitions and algorithms. The database schema, the relation sizes, and the various indexes available are shown in Table 1. The implicit joins that underlie the logical access paths among the relations are listed in Fig. 2. <p> The antecedent restriction is generated from the restriction clauses of the queries arriving at the system. The set of free variables for the consequent is generated by heuristics such as index introduction described in <ref> [1] </ref>. The antecedent restriction is evaluated against the database state to retrieve and summarize all - 5 - possible values of the free variables to form the consequent. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 13, 14] </ref> for queries, integrity constraints and query-transformation rules. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P. <p> We will omit the names of the predicates without loss of precision, since attribute names are unique. We will also omit (for all) quantifiers by implying universal quantification of free variables. The formal rules for dropping the quantifiers are discussed in <ref> [1] </ref>. Furthermore, we will assume the queries to be defined on a natural join of all relations in the shipping database. Example: Query : Find the destinations of all ships with DeadWt &gt; 400 and DollarValue &gt; 4000. <p> This heuristic speculates that there may be several interesting patterns besides the user-specified ones, in the data distributions on the attributes used in each user-defined integrity constraint. Other Heuristics: A different strategy is to allow the user to provide a heuristic <ref> [1] </ref> as to what attributes to choose. The user can use his domain knowledge, and his knowledge of attribute spaces to select a set of attributes to be represented in the grid. Another strategy in attribute selection is the use of the information in the database system catalog. <p> An unindexed attribute can be favored as a candidate in the grid, since rules that can replace the restrictions on this attribute with restrictions on indexed attributes, may often lead to large cost savings in semantic query optimization <ref> [1] </ref>. Similarly, if there are grid-file [39] indices defined on some attribute pairs, then these attributes can be selected for rule discovery, since the grid already exists. Historical data on query distribution is another source of information which can facilitate attribute selection. <p> In particular, we plan to integrate our work with query-driven approaches, in order to monitor the performance of the discovered query transformation rules during actual database operation. We would like to utilize the heuristics for semantic query optimization <ref> [1] </ref> for selecting attribute sets with which to create grids. Furthermore, we would like to characterize the set of oblique grids, which may yield useful query transformation rules. - 28 - 8.
Reference: 2. <author> Hammer and Zdonik, </author> <title> Knowledge Based query processing, </title> <booktitle> Proc. 6th Conf. on VLDB, </booktitle> <year> (1980). </year>
Reference: 3. <author> S .T. Shenoy and Z. M. Ozsoyoglu, </author> <title> Design and Implementation of a Semantic Query Optimizer, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pp. </pages> <month> 362-375 </month> <year> (1989). </year>
Reference: 4. <author> H. H. Pang, H. J. Lu, and B. C. Ooi, </author> <title> An Efficient Semantic Query Optimization Algorithm, </title> <booktitle> Proc. Data Engineering Conference, IEEE, </booktitle> <year> (1991). </year>
Reference: 5. <author> G. M. Lohman, </author> <title> Panel Discussion on Semantic Query Optimization, </title> <booktitle> Proc. Data Engineering Conf., </booktitle> <year> (1985). </year>
Reference: 6. <author> S. Shekhar, J. Srivastava, and S. Dutta, </author> <title> A Formal Model of Trade-offs between Execution and Optimization Costs in Semantic Query Optimization, </title> <booktitle> Intl. Conf. on Very Large Databases (VLDB) (reprinted in N.H. </booktitle> <address> J.D.K.E. </address> <year> 1992), (1988). </year>
Reference-contexts: The optimizer generates a number of query execution strategies (called query plans) and then attaches an estimated cost to each by using a cost model based on the the expected number of CPU instructions and page fetches <ref> [6, 42] </ref>. The execution cost estimates provided by the optimizer are dependent on the system configuration and thus we do not give any units. However, for our present purpose only their relative values are important. Total execution cost for each query is shown in Table 8.
Reference: 7. <author> C. T. Yu and W. Sun, </author> <title> Automatic Knowledge Acquisition and Maintenance for Semantic Query Optimization, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pp. </pages> <month> 362-375 </month> <year> (1989). </year>
Reference-contexts: Further, the set of query transformation rules can be - 2 - expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database <ref> [7, 10, 11] </ref>. In this paper we propose a data-driven discovery approach to learning query transformation rules. A data distribution-based approach is useful for two reasons. First, discovery of the specific patterns in the data distribution can identify useful query transformation rules. <p> We summarize the alternative approaches and bring out our main contributions in this section. 2.1. Learning in Databases for Semantic Query Optimization The learning of query-transformation rules can be query-driven or data-driven. In query-driven frameworks <ref> [7, 10-12] </ref>, the search for new query-transformation rules is guided by the set of queries which arrive at the database using query comparisons [7] and hypothesis generation and testing [10-12]. <p> Learning in Databases for Semantic Query Optimization The learning of query-transformation rules can be query-driven or data-driven. In query-driven frameworks [7, 10-12], the search for new query-transformation rules is guided by the set of queries which arrive at the database using query comparisons <ref> [7] </ref> and hypothesis generation and testing [10-12]. In query comparison, the set of queries arriving after the last update are analyzed by comparing the set of tuples retrieved to answer various queries. <p> In query comparison, the set of queries arriving after the last update are analyzed by comparing the set of tuples retrieved to answer various queries. If the set of tuples retrieved by two queries are identical, then query-transformation rules relating the restrictions in the two queries can be added <ref> [7] </ref>. In hypothesis generation and testing approaches [10, 12], the queries are used to generate a candidate query-transformation rule. A candidate rule consists of an antecedent restriction and the set of variables in the consequent. <p> These techniques are not suitable for learning the FOPL formulas typical of query-transformation rules. 2.3. Contributions We propose a qualitative law discovery system for the problem of query-transformation rule learning. Our approach is data-driven, in contrast to the previous approaches <ref> [7, 12] </ref> to learning query-transformation rules. Being query driven, previous approaches have the disadvantage that cost savings occur only if queries are repeated. This approach might incur larger costs when many new queries arrive. Also, due to a lack of relevant queries, many useful query-transformation rules may never be learned. <p> We also attach an alternative relational interpretation to query Q as R x (Q ), representing a relation containing the set of tuples retrieved by query in a current database state x <ref> [7] </ref>. Definition: Two queries Q 1 and Q 2 are semantically equivalent in the current database state x if R x (Q 1 ) = R x (Q 2 ). <p> Another classification of integrity constraint types is based on the current and historic states of a data base. State constraints refer to all constraints on the data that must hold on each state of the database <ref> [7] </ref>. Transition constraints specify the way in which one state of the database - 9 - can be transformed to another state. A rule ensuring monotonically increasing salary values is a transition constraint [33, 35-37]. <p> For example, IC 0 is a query-transformation rule but IC 1 is not a query transformation rule. Query-transformation rules subsume the "simple rules" learned in rule discovery for query optimization [10, 12], since "simple rules" are universally quantified. These rules also subsume the rules learned via automatic knowledge acquisition <ref> [7] </ref>, since the latter are based on set comparisons. 3.3. User-Defined and Discovered Query-transformation Rules User-defined transformation rules are a subset of user-defined explicit integrity constraints. <p> Ranges can be viewed as logical expressions which are true for all tuples that are contained within them. The domain of attributes in a database can be divided into two classes: the ordered- and the unordered-valued attributes <ref> [7] </ref>. Ordered attributes are those whose values do belong to an ordered domain. The domain of ordered attributes can be divided into discrete- and continuous-valued attributes. Discrete-valued attributes are those whose values are integer-valued. Continuous-valued attributes are those whose values are real-valued.
Reference: 8. <author> S. T. Shenoy and Z.M.Ozsoyoglu, </author> <title> A System for Semantic Query Optimization, </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pp. </pages> <month> 181-195 </month> <year> (1987). </year>
Reference: 9. <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <institution> Bell Telephone Laboratories (1974). </institution>
Reference: 10. <author> M. Siegel, </author> <title> Automatic Rule Derivation for Semantic Query Optimization. </title> <type> Ph.D. </type> <institution> diss., Boston Univ. </institution> <year> (1988). </year>
Reference-contexts: Further, the set of query transformation rules can be - 2 - expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database <ref> [7, 10, 11] </ref>. In this paper we propose a data-driven discovery approach to learning query transformation rules. A data distribution-based approach is useful for two reasons. First, discovery of the specific patterns in the data distribution can identify useful query transformation rules. <p> If the set of tuples retrieved by two queries are identical, then query-transformation rules relating the restrictions in the two queries can be added [7]. In hypothesis generation and testing approaches <ref> [10, 12] </ref>, the queries are used to generate a candidate query-transformation rule. A candidate rule consists of an antecedent restriction and the set of variables in the consequent. The antecedent restriction is generated from the restriction clauses of the queries arriving at the system. <p> These rules are characterized by universal quantification in the well-formed formulas representing those queries. For example, IC 0 is a query-transformation rule but IC 1 is not a query transformation rule. Query-transformation rules subsume the "simple rules" learned in rule discovery for query optimization <ref> [10, 12] </ref>, since "simple rules" are universally quantified. These rules also subsume the rules learned via automatic knowledge acquisition [7], since the latter are based on set comparisons. 3.3. User-Defined and Discovered Query-transformation Rules User-defined transformation rules are a subset of user-defined explicit integrity constraints.
Reference: 11. <author> M. Siegel, </author> <title> Automatic Rule Derivation for Semantic Query Optimization, </title> <booktitle> Proc. of the Second International Conference on Expert Database Systems, </booktitle> <pages> pp. </pages> <institution> 371-385 George Mason Foundation, </institution> <year> (1988). </year>
Reference-contexts: Further, the set of query transformation rules can be - 2 - expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database <ref> [7, 10, 11] </ref>. In this paper we propose a data-driven discovery approach to learning query transformation rules. A data distribution-based approach is useful for two reasons. First, discovery of the specific patterns in the data distribution can identify useful query transformation rules.
Reference: 12. <author> M. Siegel, E. Sciore, and S. Salveter, </author> <title> Rule Discovery for Query Optimization, Knowledge Discovery in Databases, </title> <publisher> The AAAI Press, </publisher> <year> (1991). </year> <month> - 30 </month> - 
Reference-contexts: If the set of tuples retrieved by two queries are identical, then query-transformation rules relating the restrictions in the two queries can be added [7]. In hypothesis generation and testing approaches <ref> [10, 12] </ref>, the queries are used to generate a candidate query-transformation rule. A candidate rule consists of an antecedent restriction and the set of variables in the consequent. The antecedent restriction is generated from the restriction clauses of the queries arriving at the system. <p> These techniques are not suitable for learning the FOPL formulas typical of query-transformation rules. 2.3. Contributions We propose a qualitative law discovery system for the problem of query-transformation rule learning. Our approach is data-driven, in contrast to the previous approaches <ref> [7, 12] </ref> to learning query-transformation rules. Being query driven, previous approaches have the disadvantage that cost savings occur only if queries are repeated. This approach might incur larger costs when many new queries arrive. Also, due to a lack of relevant queries, many useful query-transformation rules may never be learned. <p> These rules are characterized by universal quantification in the well-formed formulas representing those queries. For example, IC 0 is a query-transformation rule but IC 1 is not a query transformation rule. Query-transformation rules subsume the "simple rules" learned in rule discovery for query optimization <ref> [10, 12] </ref>, since "simple rules" are universally quantified. These rules also subsume the rules learned via automatic knowledge acquisition [7], since the latter are based on set comparisons. 3.3. User-Defined and Discovered Query-transformation Rules User-defined transformation rules are a subset of user-defined explicit integrity constraints.
Reference: 13. <author> C. L. Chang, </author> <title> DEDUCE 2: Further Investigations of Deduction in Relational Data bases, pp. 201-236 in Logic and Data Bases, </title> <editor> ed. J. Minker, </editor> <publisher> Plenum Press, </publisher> <address> New York (1978). </address>
Reference-contexts: Data-driven approaches can be based on the learning algorithms developed in Artificial Intelligence (AI). Many of these learning algorithms discover rules represented in languages similar to First Order Predicate Logic (FOPL), and these rules can be used to represent general integrity constraints <ref> [13, 14] </ref> and query-transformation rules. For example, the representation languages used in AQ15 [15] and in the conceptual clustering algorithm Cluster 2 [16] are fairly close to FOPL. 2.2. Learning and Discovery techniques in AI The AI learning algorithms are based on supervised concept learning and unsupervised discovery. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 13, 14] </ref> for queries, integrity constraints and query-transformation rules. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P.
Reference: 14. <author> J. M. Nicolas, </author> <title> Logic for Improving Integrity Checking in Relational Databases, </title> <publisher> Acta Informatica 18 pp. 227-253 Springer Verlag, </publisher> <year> (1982). </year>
Reference-contexts: Data-driven approaches can be based on the learning algorithms developed in Artificial Intelligence (AI). Many of these learning algorithms discover rules represented in languages similar to First Order Predicate Logic (FOPL), and these rules can be used to represent general integrity constraints <ref> [13, 14] </ref> and query-transformation rules. For example, the representation languages used in AQ15 [15] and in the conceptual clustering algorithm Cluster 2 [16] are fairly close to FOPL. 2.2. Learning and Discovery techniques in AI The AI learning algorithms are based on supervised concept learning and unsupervised discovery. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 13, 14] </ref> for queries, integrity constraints and query-transformation rules. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P.
Reference: 15. <author> R. S. Michalski, </author> <title> A Theory and Methodology of Inductive Learning, in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California (1986). </address>
Reference-contexts: Many of these learning algorithms discover rules represented in languages similar to First Order Predicate Logic (FOPL), and these rules can be used to represent general integrity constraints [13, 14] and query-transformation rules. For example, the representation languages used in AQ15 <ref> [15] </ref> and in the conceptual clustering algorithm Cluster 2 [16] are fairly close to FOPL. 2.2. Learning and Discovery techniques in AI The AI learning algorithms are based on supervised concept learning and unsupervised discovery. <p> After training, these algorithms will be able to correctly classify sample data into one of the learned concepts. Examples of supervised concept learning algorithms include AQ <ref> [15] </ref> and ID3 [17]. Supervised concept learning algorithms cannot directly be applied to the problem of learning query-transformation rules. In this domain, there may be no a priori concepts to provide the set of training examples. Unsupervised discovery involves learning without the help of external tutors or examples.
Reference: 16. <author> R. S. Michalski and R. E. Stepp, </author> <title> Learning from Observation: Conceptual Clustering, pp. 331-363 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, California (1983). </address>
Reference-contexts: For example, the representation languages used in AQ15 [15] and in the conceptual clustering algorithm Cluster 2 <ref> [16] </ref> are fairly close to FOPL. 2.2. Learning and Discovery techniques in AI The AI learning algorithms are based on supervised concept learning and unsupervised discovery. <p> Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [24]. Other conceptual clustering algorithms include Discon [25], RUMMAGE [26] and Cluster 2 <ref> [16] </ref>. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [16, 27]. The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws. <p> The basic theory and an algorithm for conceptual clustering have been developed in [24]. Other conceptual clustering algorithms include Discon [25], RUMMAGE [26] and Cluster 2 [16]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters <ref> [16, 27] </ref>. The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws.
Reference: 17. <author> J. R. Quinlan, </author> <title> Probabilistic Decision Trees, pp. 140-152 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. Yves Kodratoff, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California (1990). </address>
Reference-contexts: After training, these algorithms will be able to correctly classify sample data into one of the learned concepts. Examples of supervised concept learning algorithms include AQ [15] and ID3 <ref> [17] </ref>. Supervised concept learning algorithms cannot directly be applied to the problem of learning query-transformation rules. In this domain, there may be no a priori concepts to provide the set of training examples. Unsupervised discovery involves learning without the help of external tutors or examples.
Reference: 18. <author> S. C. </author> <title> Shapiro, </title> <journal> Encyclopedia of Artificial Intelligence, </journal> <note> A Wiley-Interscience Publication (1990). </note>
Reference-contexts: In this domain, there may be no a priori concepts to provide the set of training examples. Unsupervised discovery involves learning without the help of external tutors or examples. Algorithms for unsupervised discovery have addressed the problems of taxonomy formation and discovery of empirical laws <ref> [18] </ref>. Taxonomy formation is concerned with the discovery of classification rules using numerical clustering and conceptual clustering. Comprehensive surveys of clustering methods can be found in [19-23]. Numerical clustering methods partition data into clusters based on their distance in an n-dimensional space.
Reference: 19. <author> R. R. Sokal and R. H. Sneath, </author> <title> Principles of Numerical Taxonomy, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco (1963). </address>
Reference: 20. <author> R. M. Cormark, </author> <title> A review of classification, </title> <journal> pp. 134-321 in J. Roy. Stat. Soc., Series A, </journal> <year> (1971). </year>
Reference: 21. <author> M. R. Anderberg, </author> <title> Clustering analysis, </title> <publisher> Academic Press, </publisher> <address> New York (1973). </address>
Reference: 22. <author> J. C. Gower, </author> <title> A comparison of some methods of cluster analysis, </title> <type> Biometrics 23, </type> <pages> pp. </pages> <month> 623-637 </month> <year> (1967). </year>
Reference: 23. <author> E. Diday and J. C. Simon, </author> <title> Clustering analysis, Communication and Cybernetics, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> (1976). </year>
Reference: 24. <author> R. S. Michalski, </author> <title> Knowledge acquisition through conceptual clustering: A theoretical framework and an algorithm for partitioning data into conjunctive concepts, </title> <journal> J. Pol. Anal. Inform. </journal> <volume> Sys 4, </volume> <pages> pp. </pages> <month> 219-244 </month> <year> (1980). </year>
Reference-contexts: Numeric clustering methods are not interesting, since they do not generate the FOPL formulas needed to represent query-transformation rules. Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in <ref> [24] </ref>. Other conceptual clustering algorithms include Discon [25], RUMMAGE [26] and Cluster 2 [16]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [16, 27].
Reference: 25. <author> P. Langley and S. Sage, </author> <title> Conceptual Clustering as Discrimination Learning, </title> <booktitle> Proceedings of the Fifth Biennial Conference of the Canadian Society for Computational Studies of Intelligence, </booktitle> <pages> pp. </pages> <month> 95-98 </month> <year> (1984). </year>
Reference-contexts: Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [24]. Other conceptual clustering algorithms include Discon <ref> [25] </ref>, RUMMAGE [26] and Cluster 2 [16]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [16, 27]. The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws.
Reference: 26. <author> D. Fisher, </author> <title> A Hierarchical Conceptual Clustering Algorithm, </title> <type> Technical Report, </type> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine (1984). </address>
Reference-contexts: Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [24]. Other conceptual clustering algorithms include Discon [25], RUMMAGE <ref> [26] </ref> and Cluster 2 [16]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [16, 27]. The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws.
Reference: 27. <author> S. J. Hanson, </author> <title> Conceptual Clustering and Categorization: Bridging the Gap between Induction and Causal Models, pp. 235-268 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. Yves Kodratoff, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California (1990). </address>
Reference-contexts: The basic theory and an algorithm for conceptual clustering have been developed in [24]. Other conceptual clustering algorithms include Discon [25], RUMMAGE [26] and Cluster 2 [16]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters <ref> [16, 27] </ref>. The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws.
Reference: 28. <author> S. P. Ghosh, </author> <title> Statictics Metadata: Linear Regression Analysis, pp. 3-17 in Foundations of Data Organization, </title> <editor> ed. Katsumi Tanaka, </editor> <publisher> Plenum Press, </publisher> <address> New York (1987). </address>
Reference-contexts: The discovery techniques for learning empirical laws can discover quantitative as well as qualitative laws. Quantitative laws use numeric-valued variables and mathematical functions that summarize the data, and can be discovered via techniques such as regression analysis <ref> [28] </ref>. - 6 - Previous work on automatic discovery of quantitative rules includes the Bacon system [29]. The discovery of quantitative rules in databases has also been pursued in Forty-Niner [30]. This system examines several possible subsets of data for possible empirical laws.
Reference: 29. <author> P. Langley, J. Zytkow, H. Simon, and G. Bradshaw, </author> <title> Rediscovering Chemistry with the BACON System, pp. 307-330 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, </address> <month> Cal-ifornia </month> <year> (1983). </year>
Reference-contexts: Quantitative laws use numeric-valued variables and mathematical functions that summarize the data, and can be discovered via techniques such as regression analysis [28]. - 6 - Previous work on automatic discovery of quantitative rules includes the Bacon system <ref> [29] </ref>. The discovery of quantitative rules in databases has also been pursued in Forty-Niner [30]. This system examines several possible subsets of data for possible empirical laws. The subsets are created by using projection, slicing and aggregation operations in a multi-dimensional attribute space.
Reference: 30. <author> J. Zytkow and J. Baker, </author> <title> Interactive Mining of Regularities in Databases, Knowledge Discovery in Databases, </title> <publisher> The AAAI Press, </publisher> <year> (1991). </year>
Reference-contexts: The discovery of quantitative rules in databases has also been pursued in Forty-Niner <ref> [30] </ref>. This system examines several possible subsets of data for possible empirical laws. The subsets are created by using projection, slicing and aggregation operations in a multi-dimensional attribute space.
Reference: 31. <author> D. B. Lenat, </author> <title> The Role of Heuristics in Learning by Discovery: Three Case Studies, in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, California (1983). </address>
Reference-contexts: Qualitative laws represent logical relationships among the data. Such relationships either consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM <ref> [31] </ref> and Glauber [32]. Both methods, use domain heuristics to direct the search for interesting concepts. Qualitative empirical laws are close to query-transformation rules. For example, integrity constraints represent regularities in the data distribution, and these constraints can be represented in FOPL like languages. <p> Both methods, use domain heuristics to direct the search for interesting concepts. Qualitative empirical laws are close to query-transformation rules. For example, integrity constraints represent regularities in the data distribution, and these constraints can be represented in FOPL like languages. Unfortunately, previously designed learning algorithms like AM <ref> [31] </ref> cannot be used directly for learning query-transformation rules, due to their dependence on the domains for which they were developed. Most of qualitative empirical law discovery systems focus on learning number-theoretic, algebraic, statistical or qualitative physics oriented formulas.
Reference: 32. <author> P. Langley, J. Zytkow, H. Simon, and G. Bradshaw, </author> <title> The Search for Regularity: Four Aspects of Scientific Discovery, pp. 425-469 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Mor-gan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California (1986). </address>
Reference-contexts: Qualitative laws represent logical relationships among the data. Such relationships either consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM [31] and Glauber <ref> [32] </ref>. Both methods, use domain heuristics to direct the search for interesting concepts. Qualitative empirical laws are close to query-transformation rules. For example, integrity constraints represent regularities in the data distribution, and these constraints can be represented in FOPL like languages.
Reference: 33. <author> R. Elmasri and G. Wiederhold, </author> <title> Data Model Integration Using the Structural Model, </title> <booktitle> Proc. International Conference on Management of Data, ACM SIGMOD, </booktitle> <year> (1979). </year>
Reference-contexts: Transition constraints specify the way in which one state of the database - 9 - can be transformed to another state. A rule ensuring monotonically increasing salary values is a transition constraint <ref> [33, 35-37] </ref>. We distinguish between integrity constraints and query-transformation rules to provide a more accurate description of their nature. Query-transformation rules are explicit state integrity constraints, which are useful for semantic query optimization for universally quantified queries.
Reference: 34. <author> M. Hammer and D. McLeod, </author> <title> Semantic Integrity in a Relational Data Base System, </title> <booktitle> VLDB, </booktitle> <year> (1975). </year>
Reference: 35. <author> K. Eswaran and D. D. Chamberlin, </author> <title> Functional Specifications of a Subsystem for Database Integrity, </title> <booktitle> VLDB, </booktitle> <year> (1975). </year>
Reference: 36. <author> E. Codd, </author> <title> Extending the Database Relational Model to Capture More Meaning, </title> <journal> TODS 4:4(December 1979). </journal>
Reference: 37. <author> P. Chen, </author> <title> The Entity Relationship Mode-Toward a Unified View of Data, </title> <journal> TODS 1:1(March 1976). </journal>
Reference: 38. <author> J. Schmidt and J. Swenson, </author> <title> On the Semantics of the Relational Model, </title> <booktitle> SIGMOD, </booktitle> <year> (1975). </year>
Reference: 39. <author> K. Y. Whang and R. Krishnamurthy, </author> <title> The Multilevel Grid File ADynamic Hierarchical Multidimentional File Structure, </title> <booktitle> International Symposium on Database Systems for Advanced Applications, </booktitle> <address> (Tokyo, Japan, </address> <month> April, </month> <year> 1991). </year> <month> - 31 </month> - 
Reference-contexts: An unindexed attribute can be favored as a candidate in the grid, since rules that can replace the restrictions on this attribute with restrictions on indexed attributes, may often lead to large cost savings in semantic query optimization [1]. Similarly, if there are grid-file <ref> [39] </ref> indices defined on some attribute pairs, then these attributes can be selected for rule discovery, since the grid already exists. Historical data on query distribution is another source of information which can facilitate attribute selection.
Reference: 40. <author> M. V. Mannino, P. Chu, and T. Sager, </author> <title> Statistical Profile Estimation in Database Systems, </title> <journal> Computing Surveys 20, </journal> <volume> No. 3ACM, </volume> <month> (September, </month> <year> 1988). </year>
Reference-contexts: There exist several statistical methods that help in decomposing the domain into a finite set of ranges for a grid coordinate. A detailed discussion of these strategies in context of estimating the population distribution is given in <ref> [40] </ref>. We examine the relevance of some of these well-known strategies in the context of rule discovery now. Range selection is often addressed in the context of bar charts and histograms. <p> Another criterion to guide the choice of range selection mechanism, is the ease of implementation. For a univariate distribution, the equi-width and equi-height criteria are quite simple to implement. However, the variable-width selection criterion can be computationally costly <ref> [40] </ref>. In case of multivariate distributions the implementation of the equi-height criterion can also become difficult [40]. Needless to say, implementing the variable-width criterion becomes extremely difficult in a - 20 - multivariate distribution [40]. The equi-width criterion can be used to select ranges for multi-variate distributions. <p> For a univariate distribution, the equi-width and equi-height criteria are quite simple to implement. However, the variable-width selection criterion can be computationally costly <ref> [40] </ref>. In case of multivariate distributions the implementation of the equi-height criterion can also become difficult [40]. Needless to say, implementing the variable-width criterion becomes extremely difficult in a - 20 - multivariate distribution [40]. The equi-width criterion can be used to select ranges for multi-variate distributions. However, the choice of interval width remains the critical decision. <p> However, the variable-width selection criterion can be computationally costly <ref> [40] </ref>. In case of multivariate distributions the implementation of the equi-height criterion can also become difficult [40]. Needless to say, implementing the variable-width criterion becomes extremely difficult in a - 20 - multivariate distribution [40]. The equi-width criterion can be used to select ranges for multi-variate distributions. However, the choice of interval width remains the critical decision. Heuristic approaches based on Grid Files [41] can be used to determine interval widths.
Reference: 41. <author> K.Whang, S. Kim, and G. Wiederhold, </author> <title> Dynamic Maintenance of Data Distribution for Selectivity Estimation, </title> <institution> Dept. of Computer Science, Stanford University (September, </institution> <year> 1991). </year>
Reference-contexts: Needless to say, implementing the variable-width criterion becomes extremely difficult in a - 20 - multivariate distribution [40]. The equi-width criterion can be used to select ranges for multi-variate distributions. However, the choice of interval width remains the critical decision. Heuristic approaches based on Grid Files <ref> [41] </ref> can be used to determine interval widths. Analysis of the effectiveness of these heuristics should be a focus of future work in this area. 5. Generation of Rule Closure In this section, we first describe the domain of attributes and clauses that represent restrictions on attributes.
Reference: 42. <author> L. F. Mackert and G. M. Lohman, </author> <title> R* Optimizer Validation and Performance Evaluation for Local Queries, </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pp. 84-95 ACM, </pages> <year> (1986). </year>
Reference-contexts: The optimizer generates a number of query execution strategies (called query plans) and then attaches an estimated cost to each by using a cost model based on the the expected number of CPU instructions and page fetches <ref> [6, 42] </ref>. The execution cost estimates provided by the optimizer are dependent on the system configuration and thus we do not give any units. However, for our present purpose only their relative values are important. Total execution cost for each query is shown in Table 8.
References-found: 42

