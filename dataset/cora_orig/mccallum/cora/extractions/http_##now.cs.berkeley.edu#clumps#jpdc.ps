URL: http://now.cs.berkeley.edu/clumps/jpdc.ps
Refering-URL: http://now.cs.berkeley.edu/clumps/index.html
Root-URL: 
Title: The Nexus Approach to Integrating Multithreading and Communication  
Author: Ian Foster Carl Kesselman Steven Tuecke 
Abstract: Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, E. Lazowska, and H. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <year> 1989. </year>
Reference-contexts: A lower-level interface designed specifically as a compiler target would most likely result in better performance <ref> [1, 8, 20] </ref> and will be investigated in future research. 3.3 Discussion It is instructive to compare and contrast Nexus mechanisms with other parallel programming libraries that seek to integrate multithreading and communication. Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations.
Reference: [2] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> User-level interpro-cess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Converse supports a message-driven execution model using a construct similar to an RSR, but does not support the concept of a GP. The RSR and GP mechanisms are also related to the remote procedure call (RPC) <ref> [5, 2] </ref>. However, while research has identified RPC as an important structuring principle for client-server systems, we argue that it is not an appropriate primitive mechanism for parallel computing.
Reference: [3] <author> R. Bhoedjang, T. Rumlhl, R. Hofman, K. Langendoen, and H. Bal. Panda: </author> <title> A portable platform to support parallel programming languages. </title> <booktitle> In Symposium on Experiences with Distributed and Multiprocessor Systems IV, </booktitle> <pages> pages 213-226, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Remote-memory put/get operations and active messages are examples of relatively high-performance, low-functionality mechanisms. Many systems support only nonblocking handlers [6, 15]. At the lower-performance, higher-functionality end of the scale we see systems such as Panda <ref> [3] </ref> and Isis [4], which create a new thread for every incoming RPC. Compromise approaches are also possible, such as optimistic active messages [45], which creates a thread only when runtime tests determine that a handler would suspend. <p> Additional threads can be created by using this GP to make RSRs to the new context. Hence, threads are created as a byproduct of communication, and no initial threads are required to bootstrap a computation. Other systems with similar goals to Nexus are Panda <ref> [3] </ref>, Cilk [6], Athapascan [15], and 7 Converse [38]. Panda provides threads, RPCs, and ordered group communication as primitive operations. We discuss the suitability of RPC as a primitive below. Ordered group communication can be an important primitive in distributed-computing applications [4], but is of less relevance in parallel computing.
Reference: [4] <author> K. Birman. </author> <title> The process group approach to reliable distributed computing. </title> <journal> CACM, </journal> <volume> 36(12) </volume> <pages> 37-53, </pages> <year> 1993. </year>
Reference-contexts: Remote-memory put/get operations and active messages are examples of relatively high-performance, low-functionality mechanisms. Many systems support only nonblocking handlers [6, 15]. At the lower-performance, higher-functionality end of the scale we see systems such as Panda [3] and Isis <ref> [4] </ref>, which create a new thread for every incoming RPC. Compromise approaches are also possible, such as optimistic active messages [45], which creates a thread only when runtime tests determine that a handler would suspend. <p> Panda provides threads, RPCs, and ordered group communication as primitive operations. We discuss the suitability of RPC as a primitive below. Ordered group communication can be an important primitive in distributed-computing applications <ref> [4] </ref>, but is of less relevance in parallel computing. Cilk and Athapascan provide run-to-completion threads to provide a substrate for automatic load-balancing algorithms; the more full-functioned threads required for many parallel applications are not supported.
Reference: [5] <author> A. Birrell and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Trans. on Comput. Syst., </journal> <volume> 2 </volume> <pages> 39-59, </pages> <year> 1984. </year>
Reference-contexts: Converse supports a message-driven execution model using a construct similar to an RSR, but does not support the concept of a GP. The RSR and GP mechanisms are also related to the remote procedure call (RPC) <ref> [5, 2] </ref>. However, while research has identified RPC as an important structuring principle for client-server systems, we argue that it is not an appropriate primitive mechanism for parallel computing.
Reference: [6] <author> R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proc. Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-216. </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: Given these uncertainties, it is not surprising that researchers have investigated a range of approaches. Remote-memory put/get operations and active messages are examples of relatively high-performance, low-functionality mechanisms. Many systems support only nonblocking handlers <ref> [6, 15] </ref>. At the lower-performance, higher-functionality end of the scale we see systems such as Panda [3] and Isis [4], which create a new thread for every incoming RPC. <p> Additional threads can be created by using this GP to make RSRs to the new context. Hence, threads are created as a byproduct of communication, and no initial threads are required to bootstrap a computation. Other systems with similar goals to Nexus are Panda [3], Cilk <ref> [6] </ref>, Athapascan [15], and 7 Converse [38]. Panda provides threads, RPCs, and ordered group communication as primitive operations. We discuss the suitability of RPC as a primitive below. Ordered group communication can be an important primitive in distributed-computing applications [4], but is of less relevance in parallel computing.
Reference: [7] <author> R. Boothe and A. Ranade. </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(2), </volume> <year> 1992. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O <ref> [7, 21] </ref>, for load balancing [14], and for implementing process abstractions in parallel languages [10, 12, 25, 28, 33, 44]. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [8] <author> P. Buhr and R. Stroobosscher. </author> <title> The system: Providing light-weight concurrency on shared-memory multiprocessor systems running Unix. </title> <journal> Software Practice and Experience, </journal> <pages> pages 929-964, </pages> <month> September </month> <year> 1990. </year> <month> 19 </month>
Reference-contexts: A lower-level interface designed specifically as a compiler target would most likely result in better performance <ref> [1, 8, 20] </ref> and will be investigated in future research. 3.3 Discussion It is instructive to compare and contrast Nexus mechanisms with other parallel programming libraries that seek to integrate multithreading and communication. Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations. <p> A thread-safe communication library can reduce the need 18 for locks, as can a tighter integration of messaging, threading, and other system functions. The cost of thread creation and scheduling can be reduced by using lighter-weight threads <ref> [8, 20] </ref>. It also appears straightforward to map certain classes of RSR to hardware get and put operations when these are available. Hence, we anticipate future developments in both software and hardware tending to reduce the overhead associated with Nexus mechanisms.
Reference: [9] <author> R. Butler and E. Lusk. </author> <title> Monitors, message, and clusters: The p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Because communication patterns are predictable, the programmer or compiler can place a matching receive for each send in a program. Libraries supporting this model, such as p4 <ref> [9] </ref>, 2 PVM [18], and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D [36], Vienna Fortran [11], and pC ++ [29].
Reference: [10] <author> K. M. Chandy and C. Kesselman. </author> <title> CC ++ : A declarative concurrent object oriented programming notation. In Research Directions in Object Oriented Programming. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication. <p> Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries [30, 27, 40] and to implement several parallel languages <ref> [10, 39, 25] </ref>. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. In brief, the principal contributions of this paper are threefold: 1. <p> writing systems administration and management programs; an implementation of the MPI message-passing standard based on the MPICH [30] framework; the CAVEcomm communication library [40], used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system; and a variety of high level parallel programming languages including CC++ <ref> [10] </ref>, CYES-C++ [39], and Fortran M [25]. The experiments that we describe use simple benchmark programs that perform point-to-point communication between two threads using a variety of mechanisms. These apparently trivial benchmark programs are valuable because they allow us to measure accurately the overheads associated with various forms of RSR.
Reference: [11] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <year> 1992. </year>
Reference-contexts: Libraries supporting this model, such as p4 [9], 2 PVM [18], and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D [36], Vienna Fortran <ref> [11] </ref>, and pC ++ [29]. In less regular, loosely synchronous problems, a programmer or compiler may possess global knowledge of the successive phases executed by a program, but not of the the precise sequence of operations and communications performed in each phase on each processor.
Reference: [12] <author> B. M. Chapman, P. Mehrotra, J. Van Rosendale, and H. Zima. </author> <title> A software architecture of multidisciplinary applications: Integrating task and data parallelism. </title> <booktitle> In Proc. CONPAR94-VAPP VI 3rd Intl Conf. on Vector and Parallel Processing, </booktitle> <volume> LNCS 854, </volume> <pages> pages 664-676. </pages> <publisher> Springer Verlag, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [13] <author> A. Chowdappa, A. Skjellum, and N. Doss. </author> <title> Thread-safe message passing with p4 and MPI. </title> <type> Technical Report TR-CS-941025, </type> <institution> Computer Science Department and NSF Engineering Research Center, Mississippi State University, </institution> <year> 1994. </year>
Reference-contexts: Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations. Early libraries of this sort often incorporated implicit state such as default message buffers, and hence were not easily integrated with thread libraries <ref> [13, 22] </ref>. The more modern Message Passing Interface (MPI) [31] is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system [34] augments MPI calls with a thread identifier.
Reference: [14] <author> N.P. Chrisochoides. </author> <title> A unified appoach for static and dynamic load balancing of computations for parallel numerical grid generation. </title> <booktitle> In Proc. 4th Intl Conf. on Numerical Grid Generation in Computational Fluid Dynamics and Related Fields, </booktitle> <year> 1994. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing <ref> [14] </ref>, and for implementing process abstractions in parallel languages [10, 12, 25, 28, 33, 44]. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [15] <author> M. Cristaller, J. Briat, and M. Riviere. ATHAPASCAN-0: </author> <title> Concepts structurants simples pour une programmation parallele efficace. </title> <journal> Calculateurs Paralleles, </journal> <volume> 7(2) </volume> <pages> 173-196, </pages> <year> 1995. </year>
Reference-contexts: Given these uncertainties, it is not surprising that researchers have investigated a range of approaches. Remote-memory put/get operations and active messages are examples of relatively high-performance, low-functionality mechanisms. Many systems support only nonblocking handlers <ref> [6, 15] </ref>. At the lower-performance, higher-functionality end of the scale we see systems such as Panda [3] and Isis [4], which create a new thread for every incoming RPC. <p> Additional threads can be created by using this GP to make RSRs to the new context. Hence, threads are created as a byproduct of communication, and no initial threads are required to bootstrap a computation. Other systems with similar goals to Nexus are Panda [3], Cilk [6], Athapascan <ref> [15] </ref>, and 7 Converse [38]. Panda provides threads, RPCs, and ordered group communication as primitive operations. We discuss the suitability of RPC as a primitive below. Ordered group communication can be an important primitive in distributed-computing applications [4], but is of less relevance in parallel computing.
Reference: [16] <editor> D. Culler et al. </editor> <booktitle> Parallel programming in Split-C. In Proc. Supercomputing '93. ACM, </booktitle> <year> 1993. </year>
Reference-contexts: As an example, a handler that implements the get and put operations found in Split-C <ref> [16] </ref> can take advantage of this optimization. In addition to the RSR mechanism just described, Nexus also provides routines for creating and managing threads within a context.
Reference: [17] <author> W. J. Dally et al. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Information Processing 89, </booktitle> <year> 1989. </year>
Reference-contexts: If priorities are supported, we can reduce the delay between RSR arrival and processing by running the communication at high priority so that an incoming RSR preempts the execution of computation threads. However, while certain specialized systems support low-overhead preemption (e.g., the MIT J Machine <ref> [17] </ref>), in most existing systems preemption is an expensive operation involving a processor interrupt. 4.4 Interrupt-driven Scheduling A fourth approach to RSR implementation uses a processor interrupt to notify a thread sched-uler of a pending RSR.
Reference: [18] <author> J. Dongarra, G. Geist, R. Manchek, and V. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <booktitle> In Computers in Physics, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Because communication patterns are predictable, the programmer or compiler can place a matching receive for each send in a program. Libraries supporting this model, such as p4 [9], 2 PVM <ref> [18] </ref>, and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D [36], Vienna Fortran [11], and pC ++ [29]. <p> The data transferred between contexts by an RSR is specified by a buffer and packing and unpacking functions similar to those found in MPI [24] and PVM <ref> [18] </ref>. As we will demonstrate in Section 5, the buffer manipulation routines for RSRs can exploit the same buffer management optimizations that are used by traditional message passing systems; hence, no specialized data-transfer mechanisms are required. An RSR invocation proceeds in three steps: 1.
Reference: [19] <author> N. Suzuki (ed.). </author> <title> Shared Memory Multiprocessing. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Lightweight threads are often used to simplify the design and implementation of applications that must simulate or respond to events that can occur in an unpredictable sequence [32]. They are also, increasingly, used to exploit fine-grained concurrency in programs designed to run on shared-memory multiprocessors <ref> [19] </ref>. However, until recently lightweight threads were not used extensively in high-performance distributed-memory parallel computing, in large part because the focus of application development in this environment was on regular problems and algorithms.
Reference: [20] <author> D. Engler, G. Andrews, and D. Lowenthal. Filaments: </author> <title> Efficient support for fine-grained parallelism. </title> <type> Technical Report 93-13, </type> <institution> Dept. of Computer Science, U. Arizona, Tuscon, Ariz., </institution> <year> 1993. </year>
Reference-contexts: A lower-level interface designed specifically as a compiler target would most likely result in better performance <ref> [1, 8, 20] </ref> and will be investigated in future research. 3.3 Discussion It is instructive to compare and contrast Nexus mechanisms with other parallel programming libraries that seek to integrate multithreading and communication. Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations. <p> A thread-safe communication library can reduce the need 18 for locks, as can a tighter integration of messaging, threading, and other system functions. The cost of thread creation and scheduling can be reduced by using lighter-weight threads <ref> [8, 20] </ref>. It also appears straightforward to map certain classes of RSR to hardware get and put operations when these are available. Hence, we anticipate future developments in both software and hardware tending to reduce the overhead associated with Nexus mechanisms.
Reference: [21] <author> E. Felton and D. McNamee. </author> <title> Improving the performance of message-passing applications by multithreading. </title> <booktitle> In Proc. 1992 Scalable High Performance Computing Conf., </booktitle> <pages> pages 84-89. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O <ref> [7, 21] </ref>, for load balancing [14], and for implementing process abstractions in parallel languages [10, 12, 25, 28, 33, 44]. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication. <p> The more modern Message Passing Interface (MPI) [31] is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system [34] augments MPI calls with a thread identifier. In Mach [46] and NewThreads <ref> [21] </ref>, threads communicate by sending messages to specific ports; different threads can receive from a port at different times. Tags, communicators, thread identifiers, and ports all allow messages to be sent to or received from a specific thread.
Reference: [22] <author> A. Ferrari and V. S. Sunderam. TPVM: </author> <title> Distributed concurrent computing with lightweight processes. </title> <type> Technical Report CSTR-940802, </type> <institution> University of Virginia, </institution> <year> 1994. </year> <month> 20 </month>
Reference-contexts: Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations. Early libraries of this sort often incorporated implicit state such as default message buffers, and hence were not easily integrated with thread libraries <ref> [13, 22] </ref>. The more modern Message Passing Interface (MPI) [31] is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system [34] augments MPI calls with a thread identifier.
Reference: [23] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification, ver-sion 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Libraries supporting this model, such as p4 [9], 2 PVM [18], and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) <ref> [23] </ref>, Fortran-D [36], Vienna Fortran [11], and pC ++ [29]. In less regular, loosely synchronous problems, a programmer or compiler may possess global knowledge of the successive phases executed by a program, but not of the the precise sequence of operations and communications performed in each phase on each processor.
Reference: [24] <author> Message Passing Interface Forum. </author> <title> Document for a standard message-passing interface, </title> <month> March </month> <year> 1994. </year> <note> (available from netlib). </note>
Reference-contexts: Because communication patterns are predictable, the programmer or compiler can place a matching receive for each send in a program. Libraries supporting this model, such as p4 [9], 2 PVM [18], and MPI <ref> [24] </ref>, are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D [36], Vienna Fortran [11], and pC ++ [29]. <p> A minimal approach would be simply to combine a message-passing communication library, such as MPI <ref> [24] </ref>, and a thread library, such as POSIX threads [37]. However, we argue that the 3 communication primitives provided by current message-passing libraries are not an appropriate basis for multithreaded applications. <p> Yet because point-to-point communication mechanisms direct messages to processes, they provide only a limited namespace for communication. Message tags and communication contexts <ref> [24] </ref> help to some extent, but depend on either global knowledge of how the tag space will be used, or a synchronous, collective operation to establish a new context. <p> The data transferred between contexts by an RSR is specified by a buffer and packing and unpacking functions similar to those found in MPI <ref> [24] </ref> and PVM [18]. As we will demonstrate in Section 5, the buffer manipulation routines for RSRs can exploit the same buffer management optimizations that are used by traditional message passing systems; hence, no specialized data-transfer mechanisms are required. An RSR invocation proceeds in three steps: 1.
Reference: [25] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 25(1), </volume> <year> 1994. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication. <p> Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries [30, 27, 40] and to implement several parallel languages <ref> [10, 39, 25] </ref>. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. In brief, the principal contributions of this paper are threefold: 1. <p> an implementation of the MPI message-passing standard based on the MPICH [30] framework; the CAVEcomm communication library [40], used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system; and a variety of high level parallel programming languages including CC++ [10], CYES-C++ [39], and Fortran M <ref> [25] </ref>. The experiments that we describe use simple benchmark programs that perform point-to-point communication between two threads using a variety of mechanisms. These apparently trivial benchmark programs are valuable because they allow us to measure accurately the overheads associated with various forms of RSR.
Reference: [26] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: We explain how these two mechanisms interact with each other and with threads, and how they can be used to implement a wide variety of parallel program structures. Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus <ref> [26] </ref>, which we and others have used to build a variety of higher-level communication libraries [30, 27, 40] and to implement several parallel languages [10, 39, 25]. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms.
Reference: [27] <author> I. Foster and R. Olson. </author> <title> A guide to parallel and distributed programming in nPerl. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year> <note> http://www.mcs.anl.gov/nexus/nperl/. </note>
Reference-contexts: Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries <ref> [30, 27, 40] </ref> and to implement several parallel languages [10, 39, 25]. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. <p> We emphasize that these experiments are performed not with an artificial test harness, but with a full-featured multithreaded communication library that is in regular use in a variety of large-scale application projects and compiler research projects. Parallel libraries and languages that use Nexus include nPerl <ref> [27] </ref>, an RPC extension to the Perl 5 scripting language used for writing systems administration and management programs; an implementation of the MPI message-passing standard based on the MPICH [30] framework; the CAVEcomm communication library [40], used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system;
Reference: [28] <author> I. Foster and S. Taylor. </author> <title> A compiler approach to scalable concurrent program design. </title> <journal> ACM TOPLAS, </journal> <note> 1994. to appear. </note>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [29] <author> D. Gannon et al. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Libraries supporting this model, such as p4 [9], 2 PVM [18], and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D [36], Vienna Fortran [11], and pC ++ <ref> [29] </ref>. In less regular, loosely synchronous problems, a programmer or compiler may possess global knowledge of the successive phases executed by a program, but not of the the precise sequence of operations and communications performed in each phase on each processor.
Reference: [30] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message-passing interface standard. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries <ref> [30, 27, 40] </ref> and to implement several parallel languages [10, 39, 25]. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. <p> Parallel libraries and languages that use Nexus include nPerl [27], an RPC extension to the Perl 5 scripting language used for writing systems administration and management programs; an implementation of the MPI message-passing standard based on the MPICH <ref> [30] </ref> framework; the CAVEcomm communication library [40], used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system; and a variety of high level parallel programming languages including CC++ [10], CYES-C++ [39], and Fortran M [25].
Reference: [31] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Distributed-memory computers are most commonly programmed by using communication libraries that support point-to-point and collective communication operations. Early libraries of this sort often incorporated implicit state such as default message buffers, and hence were not easily integrated with thread libraries [13, 22]. The more modern Message Passing Interface (MPI) <ref> [31] </ref> is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system [34] augments MPI calls with a thread identifier.
Reference: [32] <author> D. Grunwald. </author> <title> A user's guide to AWESIME: An object-oriented parallel programming and sumulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Lightweight threads are often used to simplify the design and implementation of applications that must simulate or respond to events that can occur in an unpredictable sequence <ref> [32] </ref>. They are also, increasingly, used to exploit fine-grained concurrency in programs designed to run on shared-memory multiprocessors [19].
Reference: [33] <author> M. Haines and W. Bohm. </author> <title> An evaluation of software multithreading in a conventional distributed-memory multiprocessor. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Processing, </booktitle> <pages> pages 106-113. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [34] <author> M. Haines, D. Cronk, and P. Mehrotra. </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> In Proc. Supercomputing '94, </booktitle> <pages> pages 350-359, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The more modern Message Passing Interface (MPI) [31] is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system <ref> [34] </ref> augments MPI calls with a thread identifier. In Mach [46] and NewThreads [21], threads communicate by sending messages to specific ports; different threads can receive from a port at different times.
Reference: [35] <author> M. Haines, P. Mehrotra, and D. Cronk. Ropes: </author> <title> Support for collective operations among distributed threads. </title> <type> Technical Report 95-36, </type> <institution> ICASE, </institution> <year> 1995. </year>
Reference-contexts: This issue become particularly important in applications that use RSRs to achieve remote memory access, and when we attempt to provide fast collective operations among threads, as is required (for example) when a multithreaded computation incorporates SPMD components <ref> [35] </ref>. A general solution to this problem will require new OS structures that integrate communication and multithreading more tightly than current systems.
Reference: [36] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Libraries supporting this model, such as p4 [9], 2 PVM [18], and MPI [24], are designed for programmer use but are also used as compiler tar-gets by data-parallel languages such as High Performance Fortran (HPF) [23], Fortran-D <ref> [36] </ref>, Vienna Fortran [11], and pC ++ [29]. In less regular, loosely synchronous problems, a programmer or compiler may possess global knowledge of the successive phases executed by a program, but not of the the precise sequence of operations and communications performed in each phase on each processor.
Reference: [37] <editor> IEEE. IEEE P1003.1c/D10: </editor> <title> Draft standard for information technology portable operating systems interface (POSIX), </title> <month> September </month> <year> 1994. </year> <month> 21 </month>
Reference-contexts: A minimal approach would be simply to combine a message-passing communication library, such as MPI [24], and a thread library, such as POSIX threads <ref> [37] </ref>. However, we argue that the 3 communication primitives provided by current message-passing libraries are not an appropriate basis for multithreaded applications. Reasons include the following: * Point-to-point communication routines assume that threads exist on both the sending and receiving sides of a communication. <p> In addition to the RSR mechanism just described, Nexus also provides routines for creating and managing threads within a context. These functions are modeled on a subset of the POSIX threads standard <ref> [37] </ref> and provide for local thread creation, termination, condition variables, mutual exclusion, yield of the current thread, etc.
Reference: [38] <author> L. V. Kale, M. Bhandarkar, N. K. Jagathesan, and S. Krishnan. </author> <title> Converse: An interoper--able framework for parallel programming. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, UIUC, </institution> <year> 1994. </year>
Reference-contexts: Hence, threads are created as a byproduct of communication, and no initial threads are required to bootstrap a computation. Other systems with similar goals to Nexus are Panda [3], Cilk [6], Athapascan [15], and 7 Converse <ref> [38] </ref>. Panda provides threads, RPCs, and ordered group communication as primitive operations. We discuss the suitability of RPC as a primitive below. Ordered group communication can be an important primitive in distributed-computing applications [4], but is of less relevance in parallel computing.
Reference: [39] <author> R. Pandey. </author> <title> A Compositional Approach to Concurrent Programming. </title> <type> PhD thesis, </type> <institution> The University of Texas at Austin, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries [30, 27, 40] and to implement several parallel languages <ref> [10, 39, 25] </ref>. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. In brief, the principal contributions of this paper are threefold: 1. <p> administration and management programs; an implementation of the MPI message-passing standard based on the MPICH [30] framework; the CAVEcomm communication library [40], used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system; and a variety of high level parallel programming languages including CC++ [10], CYES-C++ <ref> [39] </ref>, and Fortran M [25]. The experiments that we describe use simple benchmark programs that perform point-to-point communication between two threads using a variety of mechanisms. These apparently trivial benchmark programs are valuable because they allow us to measure accurately the overheads associated with various forms of RSR.
Reference: [40] <author> M. Papka. </author> <title> The CAVEcomm library. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year> <note> http://www.mcs.anl.gov/FUTURES LAB/VR/APPS/C2C/. </note>
Reference-contexts: Our proposed global pointer and remote service request mechanisms have been incorporated into a multithreaded communication library called Nexus [26], which we and others have used to build a variety of higher-level communication libraries <ref> [30, 27, 40] </ref> and to implement several parallel languages [10, 39, 25]. We use a Nexus implementation to perform detailed performance studies of our proposed communication mechanisms on several parallel platforms. The results of these studies provide insights into the costs associated with their implementation in different environments. <p> Parallel libraries and languages that use Nexus include nPerl [27], an RPC extension to the Perl 5 scripting language used for writing systems administration and management programs; an implementation of the MPI message-passing standard based on the MPICH [30] framework; the CAVEcomm communication library <ref> [40] </ref>, used to achieve client-server interactions between high-performance computing applications and the CAVE virtual reality system; and a variety of high level parallel programming languages including CC++ [10], CYES-C++ [39], and Fortran M [25].
Reference: [41] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <institution> Computer Science Technical Report CS-TR-3055, University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: If the unknown sequence is repeated many times, a technique called runtime compilation can be used to obtain and cache knowledge about the communication patterns used in the program; this knowledge can then be applied by an SPMD message-passing library. This approach is used in the CHAOS library <ref> [41] </ref>, for example. In other situations, the sequence of operations performed cannot feasibly be determined or reused. In some such situations, an adequate solution is to extend the message-passing model to allow single-sided operations.
Reference: [42] <author> A. Silberschatz, J. Peterson, and P. Galvin. </author> <title> Operating Systems Concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: GPs can be created dynamically; once created, a GP can be communicated between nodes by including it in an RSR. A GP can be thought of as a capability <ref> [42] </ref> granting rights to operate on the associated endpoint. As its name implies, a GP is associated with a memory location in the address space (context) to which it refers. This association of an address with an endpoint is not essential to the realization of the endpoint concept.
Reference: [43] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The data is then transferred and the remote operation performed without user code on the remote processor executing an explicit receive operation. The remote operation may be performed by specialized hardware (e.g., remote memory put/get operations on the Cray T3D) or by an interrupt handler. Active messages <ref> [43] </ref> are a well-known instantiation of this concept. The set of operations that can be performed by a single-sided communication mechanism such as active messages is typically restricted because of the need to run in an interrupt service routine. <p> Probe operations can be performed whenever a computation thread calls a thread or communication operation. (This approach is used successfully to dispatch active message handlers in some implementations of active messages <ref> [43] </ref>.) They can also be introduced into user computation code automatically, by a compiler or object-code instrumentation tool. In both cases, it may be feasible for the computation thread that detects RSRs to execute the handlers (or create threads) directly; alternatively, the computation thread can enable a high-priority communication thread.
Reference: [44] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> TAM | a compiler controlled threaded abstract machine. </title> <journal> J. Parallel and Distributed Computing, </journal> <year> 1992. </year>
Reference-contexts: Multithreading has proven useful for overlapping computation and communication or I/O [7, 21], for load balancing [14], and for implementing process abstractions in parallel languages <ref> [10, 12, 25, 28, 33, 44] </ref>. A difficult issue that must be addressed if multithreading is to be used successfully in distributed-memory environments is the integration of threads and communication.
Reference: [45] <author> D. A. Wallach, W. C. Hsieh, K. Johnson, M. F. Kaashoek, and W. E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <type> Technical report, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Many systems support only nonblocking handlers [6, 15]. At the lower-performance, higher-functionality end of the scale we see systems such as Panda [3] and Isis [4], which create a new thread for every incoming RPC. Compromise approaches are also possible, such as optimistic active messages <ref> [45] </ref>, which creates a thread only when runtime tests determine that a handler would suspend. In designing the Nexus system, we took into account the fact that we are targeting non-SPMD applications, in which relatively complex handler functions are commonplace.
Reference: [46] <author> M. Young et al. </author> <title> The duality of memory and communication in Mach. </title> <booktitle> In Proc. 11th Symp. on Operating System Principles, </booktitle> <pages> pages 63-76. </pages> <publisher> ACM, </publisher> <year> 1987. </year> <month> 22 </month>
Reference-contexts: The more modern Message Passing Interface (MPI) [31] is designed to be thread safe, but provides no explicit support for communication between threads. The Chant runtime system [34] augments MPI calls with a thread identifier. In Mach <ref> [46] </ref> and NewThreads [21], threads communicate by sending messages to specific ports; different threads can receive from a port at different times. Tags, communicators, thread identifiers, and ports all allow messages to be sent to or received from a specific thread.
References-found: 46

