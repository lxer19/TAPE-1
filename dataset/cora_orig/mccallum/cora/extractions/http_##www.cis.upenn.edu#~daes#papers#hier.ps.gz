URL: http://www.cis.upenn.edu/~daes/papers/hier.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/
Root-URL: 
Email: greiner@scr.siemens.com  grove@research.nj.nec.com  daes@linc.cis.upenn.edu  
Title: On learning hierarchical classifications  
Author: Russell Greiner Adam Grove Dale Schuurmans 
Keyword: multicategory classification, hierarchical  
Note: classifications, document classification Also:  
Address: 755 College Road East Princeton, NJ 08540-6632  4 Independence Way Princeton, NJ 08540  Philadelphia, PA 19104-6228  Princeton, NJ  
Affiliation: Siemens Corporate Research  NEC Research Institute  Institute for Research in Cognitive Science University of Pennsylvania  NEC Research Institute,  
Abstract: Many significant real-world classification tasks involve a large number of categories which are arranged in a hierarchical structure; for example, classifying documents into subject categories under the library of congress scheme, or classifying world-wide-web documents into topic hierarchies. We investigate the potential benefits of using a given hierarchy over base classes to learn accurate multi-category classifiers for these domains. First, we consider the possibility of exploiting a class hierarchy as prior knowledge that can help one learn a more accurate classifier. We explore the benefits of learning category-discriminants in a hard top-down fashion and compare this to a soft approach which shares training data among sibling categories. In doing so, we verify that hierarchies have the potential to improve prediction accuracy. But we argue that the reasons for this can be subtle. Sometimes, the improvement is only because using a hierarchy happens to constrain the expressiveness of a hypothesis class in an appropriate manner. However, various controlled experiments show that in other cases the performance advantage associated with using a hierarchy really does seem to be due to the prior knowledge it encodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: However, none of these techniques exploit the existence of a prior hierarchical structure over the class labels. Finally, we note that decision tree classifiers <ref> [1, 24] </ref>, which classify examples in a hierarchical fashion and naturally encode multicategory classification schemes, are not designed to exploit a hierarchy on class labels to obtain better performance.
Reference: [2] <author> T. G. Dietterich and G Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> JAIR, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <year> 1995. </year>
Reference-contexts: The most important recent advance in this area perhaps is Dietterich and Bakiri's approach to arranging binary classifiers in error correcting output codes to achieve high classification accuracy in multicategory domains <ref> [2] </ref>. However, none of these techniques exploit the existence of a prior hierarchical structure over the class labels.
Reference: [3] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for optimality of the simple bayesian classifier. </title> <booktitle> In Proceedings 13th International Conference on Machine Learning (ML '96), </booktitle> <pages> pages 105-122, </pages> <year> 1996. </year>
Reference: [4] <author> R. O. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 11], actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance [7], which considers learning a hierarchy on classes in an unsupervised fashion (see also hierarchical clustering <ref> [4] </ref>). However, this does not address the issue of how best to exploit the presumptive knowledge expressed by a given hierarchy to learn a more accurate classifier.
Reference: [5] <author> B. Efron and C. Morris. </author> <title> Stein's estimation rule and its competitorsan empirical Bayes approach. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 68 </volume> <pages> 117-130, </pages> <year> 1977. </year>
Reference-contexts: Our initial motivation for this was as another way of addressing the expressive power issue. But we also find that these soft training techniques, which connect to a large and important literature on Bayesian and empirical Bayesian statistics <ref> [6, 5, 22, 11] </ref>, actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance [7], which considers learning a hierarchy on classes in an unsupervised fashion (see also
Reference: [6] <author> B. Efron and C. Morris. </author> <title> Stein's paradox in statistics. </title> <publisher> Scientific American, </publisher> <pages> pages 119-127, </pages> <year> 1977. </year>
Reference-contexts: Our initial motivation for this was as another way of addressing the expressive power issue. But we also find that these soft training techniques, which connect to a large and important literature on Bayesian and empirical Bayesian statistics <ref> [6, 5, 22, 11] </ref>, actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance [7], which considers learning a hierarchy on classes in an unsupervised fashion (see also
Reference: [7] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental concept clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: connect to a large and important literature on Bayesian and empirical Bayesian statistics [6, 5, 22, 11], actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance <ref> [7] </ref>, which considers learning a hierarchy on classes in an unsupervised fashion (see also hierarchical clustering [4]). However, this does not address the issue of how best to exploit the presumptive knowledge expressed by a given hierarchy to learn a more accurate classifier.
Reference: [8] <author> J. Friedman. </author> <title> Another approach to polychotomous classification. </title> <type> Technical report, </type> <institution> Stan ford University, Statistics Department, </institution> <year> 1996. </year>
Reference: [9] <author> J. Friedman. </author> <title> On bias, variance, 0/1-loss, and the curse-of-dimensionality. </title> <type> Technical report, </type> <institution> Stanford University, Statistics Department, </institution> <year> 1996. </year>
Reference: [10] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Comp., </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference: [11] <author> E. George, U. Makov, and A. Smith. </author> <title> Fully Bayesian hierarchical analysis for exponential families via Monte Carlo computation. </title> <editor> In P. Freeman and A. Smith, editors, </editor> <booktitle> Aspects of Uncertainty. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Our initial motivation for this was as another way of addressing the expressive power issue. But we also find that these soft training techniques, which connect to a large and important literature on Bayesian and empirical Bayesian statistics <ref> [6, 5, 22, 11] </ref>, actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance [7], which considers learning a hierarchy on classes in an unsupervised fashion (see also
Reference: [12] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in databases: an attribute-oriented approach. </title> <booktitle> In VLDB-92, </booktitle> <pages> pages 547-559, </pages> <year> 1992. </year>
Reference-contexts: Hierarchies also arise in many other ways in the KDD and ML research. For instance, it is common for research in KDD to consider hierarchies on attribute values <ref> [13, 12] </ref>. But work of this type generally considers learning a relation over database tuples (a generalized relation [12]), classifies tuples in a binary fashion as being in or out of the learned relation. <p> Hierarchies also arise in many other ways in the KDD and ML research. For instance, it is common for research in KDD to consider hierarchies on attribute values [13, 12]. But work of this type generally considers learning a relation over database tuples (a generalized relation <ref> [12] </ref>), classifies tuples in a binary fashion as being in or out of the learned relation. Here, generalization or specialization on attribute values is simply a technique for expanding or restricting the extent of the learned relation in an appropriate way.
Reference: [13] <author> J. Han, Y. Fu, W. Wei, J. Chiang, W. Gong, K. Koperski, D. Li, and Y. Lu. </author> <title> Dbminer: a system for mining knowledge in large relational databases. </title> <booktitle> In KDD-96, </booktitle> <pages> pages 250-255, </pages> <year> 1996. </year>
Reference-contexts: Hierarchies also arise in many other ways in the KDD and ML research. For instance, it is common for research in KDD to consider hierarchies on attribute values <ref> [13, 12] </ref>. But work of this type generally considers learning a relation over database tuples (a generalized relation [12]), classifies tuples in a binary fashion as being in or out of the learned relation.
Reference: [14] <author> D. J. </author> <title> Hand. Discrimination and Classification. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [15] <author> M. I. Jordan and Jacobs R.A. </author> <title> Hierarchical mixtures of experts and the em algorithm. </title> <booktitle> Neural Computation, </booktitle> <pages> pages 181-214, </pages> <year> 1994. </year> <month> 10 </month>
Reference: [16] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI-95, </booktitle> <year> 1995. </year>
Reference: [17] <author> R. Kohavi and D. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In ML-96, </booktitle> <pages> pages 275-283, </pages> <year> 1996. </year>
Reference: [18] <author> D. Koller and M. Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <type> Technical report, </type> <institution> Stanford University, Computer Science Department, </institution> <year> 1997. </year>
Reference-contexts: Our results do show that if one asks why hierarchies have the effect they do, the answer can be surprisingly subtle. 4 Related Work The work closest to ours is the earlier paper of Koller and Sahami <ref> [18] </ref>. <p> Their work differs from ours in important respects, and we view our work as complementary. The techniques in <ref> [18] </ref> seem to be principally motivated by the desire to improve accuracy (i.e., to obtain good performance in an absolute sense). <p> They did indeed discover an advantage in using a hierarchies, just as we do, in spite of the difference in algorithms being considered. On the other hand, our goal is to uncover and delineate the possible sources of improvement from exploiting a hierarchy, and so unlike <ref> [18] </ref> we have tried very hard to control for expressive power in a rigorous fashion; one price we pay being able to make clean comparisons is that we have used simpler algorithms.
Reference: [19] <author> K. Lang, G. Hinton, and A. Waibel. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43, </pages> <year> 1990. </year>
Reference-contexts: Although an important technique in database mining applications, this is somewhat orthogonal to the issue we address here. There is a significant literature on learning flat multicategory classifications. For example, this is predominant in the connectionist literature on perceptual learning tasks such as speech and handwriting recognition <ref> [19, 20] </ref>. The most important recent advance in this area perhaps is Dietterich and Bakiri's approach to arranging binary classifiers in error correcting output codes to achieve high classification accuracy in multicategory domains [2].
Reference: [20] <author> Y. le Cun, B. Boser, J. S. Denker, D. Henderson, D. E. Howard, W. Hubbard, and L. D. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Comp., </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: Although an important technique in database mining applications, this is somewhat orthogonal to the issue we address here. There is a significant literature on learning flat multicategory classifications. For example, this is predominant in the connectionist literature on perceptual learning tasks such as speech and handwriting recognition <ref> [19, 20] </ref>. The most important recent advance in this area perhaps is Dietterich and Bakiri's approach to arranging binary classifiers in error correcting output codes to achieve high classification accuracy in multicategory domains [2].
Reference: [21] <author> D. Lewis, R. Schapire, J. Callan, and R. Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In SIGIR-96, </booktitle> <pages> pages 298-306, </pages> <year> 1996. </year>
Reference: [22] <author> J. Maritz and T. Lwin. </author> <title> Emprical Bayes Methods. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Our initial motivation for this was as another way of addressing the expressive power issue. But we also find that these soft training techniques, which connect to a large and important literature on Bayesian and empirical Bayesian statistics <ref> [6, 5, 22, 11] </ref>, actually seem to hold promise for being an effective technique for exploiting hierarchies in a robust and general fashion. 8 To distinguish this general approach, note that there is other work, for instance [7], which considers learning a hierarchy on classes in an unsupervised fashion (see also
Reference: [23] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference: [24] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <month> [25] </month> <year> 1997. </year> <title> Text Categorization Test Collection, </title> <address> http://www.research.att.com/ lewis/reuters21578.html. </address>
Reference-contexts: However, none of these techniques exploit the existence of a prior hierarchical structure over the class labels. Finally, we note that decision tree classifiers <ref> [1, 24] </ref>, which classify examples in a hierarchical fashion and naturally encode multicategory classification schemes, are not designed to exploit a hierarchy on class labels to obtain better performance.
Reference: [26] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference: [27] <author> G. Salton, A. Wong, and C. Yang. </author> <title> A vector space model for automatic indexing. </title> <journal> CACM, </journal> <volume> 18(11) </volume> <pages> 613-620, </pages> <year> 1975. </year>
Reference: [28] <author> H. Schuetze, D. Hull, and J. Pederson. </author> <title> A comparison of classifiers and document repre sentations for the routing problem. </title> <booktitle> In SIGIR-95, </booktitle> <pages> pages 229-237, </pages> <year> 1995. </year>
Reference: [29] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [30] <author> Erik Wiener, Jan O. Pedersen, and Andreas S. Weigend. </author> <title> A neural network approach to topic spotting. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 317-332, </pages> <address> Las Vegas, NV, </address> <year> 1995. </year> <institution> ISRI; Univ. of Nevada, </institution> <address> Las Vegas. </address> <month> 11 </month>
References-found: 29

