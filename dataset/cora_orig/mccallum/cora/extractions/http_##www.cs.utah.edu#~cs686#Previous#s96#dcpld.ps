URL: http://www.cs.utah.edu/~cs686/Previous/s96/dcpld.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s96/
Root-URL: 
Title: Abstract  
Abstract: This paper investigates hardware support for fine-grain distributed shared memory (DSM) in networks of workstations. To reduce design time and implementation cost relative to dedicated DSM systems, we decouple the functional hardware components of DSM support, allowing greater use of off-the-shelf devices. We present two decoupled systems, Typhoon-0 and Typhoon-1. Typhoon-0 uses an off-the-shelf protocol processor and network interface; a custom access control device is the only DSM-specific hardware. To demonstrate the feasibility and simplicity of this access control device, we designed and built an FPGA-based version in under one year. Typhoon-1 also uses an off-the-shelf protocol processor, but integrates the network interface and access control devices for higher performance. We compare the performance of the two decoupled systems with two integrated systems via simulation. For six benchmarks on 32 nodes, Typhoon-0 ranges from 30% to 309% slower than the best integrated system, while Typhoon-1 ranges from 13% to 132% slower. Four of the six benchmarks achieve speedups of 12 to 18 on Typhoon-0 and 15 to 26 on Typhoon-1, compared with 19 to 35 on the best integrated system. Two benchmarks are hampered by high communication overheads, but selectively replacing shared-memory operations with message passing provides speedups of at least 16 on both decoupled systems. These speedups indicate that decoupled designs can potentially provide a cost-effective alternative to complex high-end DSM systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware. <p> The second, modeled after Typhoon [37], replaces the state machine with a user-level protocol processor to allow execution of optimized application-specific protocols. All of these systems can be built from off-the-shelf workstations, unlike those that replace standard cache or memory controllers, such as Alewife <ref> [1] </ref>, FLASH [13], and S3.mp [33]. In our evaluation, all of the systems use a single processor per node for computation, although small-scale (e.g., four-way) bus- based multiprocessor nodes may be more cost-effective. All of the designs we describe are compatible with multiprocessor nodes. <p> These sends must cross the bus on all three systems; the tight coupling of the network interface and protocol processor on Typhoon only improves performance on the receiving node. 4 Related work Alewife <ref> [1] </ref> was the first hybrid hardware/software DSM system. As with later variants from other researchers [22, 45, 16], custom hardware generates requests and handles responses on the caching side and implements some basic directory functions.
Reference: [2] <author> Thomas E. Anderson, David E. Culler, David A. Patterson, </author> <title> and the NOW team. A Case For NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1):5464, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Technological and economic trends make it increasingly cost- effective to assemble large-scale parallel systems from off-the- shelf workstations and networks <ref> [2] </ref>. These networks of workstations, or NOWs, primarily target message-passing applications. Although shared-memory applications can be run on these machines, the lack of explicit support for this model introduces significant overheads. This paper explores hardware support for cache-coherent distributed shared memory (DSM) on workstation- based systems.
Reference: [3] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Breakdown of remote miss latency. Values are 200 MHz processor cycle counts except where noted. <ref> [3] </ref>, parallelized for shared memory [9]. Barnes is from the SPLASH-2 suite [44]. Em3d is a shared-memory version of an original Split-C program from Berkeley [12]. Dsmc, moldyn, and unstructured are irregular applications originally from Maryland [32].
Reference: [4] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In COMPCON 1993, </booktitle> <year> 1993. </year>
Reference-contexts: At the other extreme, many systems use no DSM-specific hardware. These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software <ref> [4, 39, 23] </ref> or using standard virtual memory hardware [29, 10, 24]. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W.
Reference: [5] <author> Matthias A. Blumrich, Cesary Dubnicki, Edward W. Felten, and Kai Li. </author> <title> Protected, User-level DMA for the SHRIMP Network Interface. </title> <booktitle> In Proceedings of the 2nd International Symposium on HighPerformance Computer Architecture (HPCA), </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Since Typhoon-0 can become a bus master and issue burst transactions, these additional features do not significantly increase complexity. The shadow space mapping solves the address translation and protection issues usually associated with user-level DMA <ref> [5] </ref>. Because the DMA size is limited to a single cache block, there is only a small window during which the virtual-to-physical translation must remain valid, so we can use a simple technique to prevent the DMA mechanism from using a stale translation [19].
Reference: [6] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathon Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: General- purpose microprocessors rapidly and exibly sequence operations and manage data, making them suitable for protocol processing. Commercial network technologies such as switched Ethernet, ATM, and Myrinet [7], and research efforts such as SHRIMP <ref> [6] </ref>, promise rapid advances in messaging performance. Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. <p> This allows the use of previously written application-specific protocols [14, 32], which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices <ref> [6, 19, 43] </ref>. Using simulation, we quantify the performance impact of decouplingthat is, given subsystems with similar capabilities, what is the performance penalty for implementing those subsystems as separate components rather than integrating them into a single device? We compare the performance of Typhoon-0 and Typhoon-1 with two integrated systems. <p> How-- ever, passing virtual addresses as data requires a translation and a protection check in the receiving component. We avoid both of these using a shadow space <ref> [6, 19, 43] </ref>. The access control device supports a physical address rangethe shadow spaceas large as, and at a fixed offset from, the machines physical memory. Accesses to the shadow space are interpreted as operations on the real memory space. <p> In contrast, StarT-NG runs a software handler for every remote reference that misses in the hardware cache, even if the data is cached in local DRAM; FLASH must execute software on every local hardware cache miss. Kontothanassis and Scott [26] propose using network interfaces such as SHRIMP <ref> [6] </ref> to implement weakly consistent page- based DSM.
Reference: [7] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Industry trends suggest that now, or in the near future, we can apply commodity components in two of these three areas. General- purpose microprocessors rapidly and exibly sequence operations and manage data, making them suitable for protocol processing. Commercial network technologies such as switched Ethernet, ATM, and Myrinet <ref> [7] </ref>, and research efforts such as SHRIMP [6], promise rapid advances in messaging performance. Access control is the only function for which there is no prospective off-the-shelf solution. <p> Each workstation has two ROSS HyperSPARC CPUs; by software convention, one acts as the compute processor and the other acts as the protocol processor. The nodes in our Typhoon-0 implementation are connected by a Myricom Myrinet network <ref> [7] </ref>. tags are implemented as a two-bit directory with one tag for every block in physical memory. The bus logic observes every coherent bus transaction, and indexes the tag SRAM with the physical address. <p> Assuming cache hits, this sequence takes ten cycles on the HyperSPARC, which has a one cycle load-use delay. other than the network interface itself, which could be a separate single-chip device such as the Myrinet LANai <ref> [7] </ref>, Dolphin/LSI Logic SCI NodeChip [30], or Cray SCX adapter [40]. Typhoon-1 incorporates three specific advances over Typhoon-0: user-level cache block DMA with combined access control, a tagged block buffer, and enhanced message dispatching.
Reference: [8] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Proceedings of the Seventh ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <year> 1995. </year>
Reference-contexts: The queues are directly accessed by the Simple COMA systems hardware state machine. Though it is impossible to predict the commodity network interface of the future, we believe that it will provide a queue abstraction <ref> [8] </ref> and have roughly similar performance characteristics. 2.2 Integrated systems To provide a reference point for the decoupled designs performance, we study two systems that tightly integrate DSM support functions in a single device. The first system is an idealized implementation of Simple COMA [18].
Reference: [9] <author> Doug Burger and Sanjay Mehta. </author> <title> Parallelizing Appbt for a SharedMemory Multiprocessor. </title> <type> Technical Report 1286, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> September </month> <year> 1985. </year>
Reference-contexts: Breakdown of remote miss latency. Values are 200 MHz processor cycle counts except where noted. [3], parallelized for shared memory <ref> [9] </ref>. Barnes is from the SPLASH-2 suite [44]. Em3d is a shared-memory version of an original Split-C program from Berkeley [12]. Dsmc, moldyn, and unstructured are irregular applications originally from Maryland [32]. All of the benchmarks are written in C and compiled with gcc version 2.6.3 at optimization level -O2.
Reference: [10] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software [4, 39, 23] or using standard virtual memory hardware <ref> [29, 10, 24] </ref>. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W. Pfile, and David A. <p> Typhoon-0 and Typhoon-1 use virtual address translation to map remote pages into local DRAM, a feature shared with Typhoon, Simple COMA [18], and page-based software DSM systems <ref> [29, 10, 24] </ref>. Because cached remote data is transparently accessed, protocol handlers are only executed when coherence action is required.
Reference: [11] <author> Derek Chiou, Boon S. Ang, Arvind, Michael J. Beckerle, Andy Boughton, Robert Greiner, James E. Hicks, and James C. Hoe. StarTNG: </author> <title> Delivering Seamless Parallel Computing. </title> <type> Technical Report CSG Memo 371, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface. To avoid deadlock, the CPU must handle interrupts while memory accesses are outstanding, precluding some existing off- the-shelf microprocessors. FLASH [13], StarT-NG <ref> [11] </ref>, and Typhoon [37] perform all protocol processingon both the directory and the caching nodesin software. FLASH and Typhoon execute protocol soft ware on a custom processor integrated with the network interface; FLASH also incorporates the memory controller on this device.
Reference: [12] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Breakdown of remote miss latency. Values are 200 MHz processor cycle counts except where noted. [3], parallelized for shared memory [9]. Barnes is from the SPLASH-2 suite [44]. Em3d is a shared-memory version of an original Split-C program from Berkeley <ref> [12] </ref>. Dsmc, moldyn, and unstructured are irregular applications originally from Maryland [32]. All of the benchmarks are written in C and compiled with gcc version 2.6.3 at optimization level -O2.
Reference: [13] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware. <p> The second, modeled after Typhoon [37], replaces the state machine with a user-level protocol processor to allow execution of optimized application-specific protocols. All of these systems can be built from off-the-shelf workstations, unlike those that replace standard cache or memory controllers, such as Alewife [1], FLASH <ref> [13] </ref>, and S3.mp [33]. In our evaluation, all of the systems use a single processor per node for computation, although small-scale (e.g., four-way) bus- based multiprocessor nodes may be more cost-effective. All of the designs we describe are compatible with multiprocessor nodes. <p> In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface. To avoid deadlock, the CPU must handle interrupts while memory accesses are outstanding, precluding some existing off- the-shelf microprocessors. FLASH <ref> [13] </ref>, StarT-NG [11], and Typhoon [37] perform all protocol processingon both the directory and the caching nodesin software. FLASH and Typhoon execute protocol soft ware on a custom processor integrated with the network interface; FLASH also incorporates the memory controller on this device.
Reference: [14] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, Ioannis Schoinas, Mark D. Hill, James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 380389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: To exploit the exibility of software protocol processing, both decoupled designs run user-level protocol handlers and support the Tempest interface [35]. This allows the use of previously written application-specific protocols <ref> [14, 32] </ref>, which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices [6, 19, 43]. <p> For each of the Tempest systems, we also ran existing custom protocols that were hand-optimized for each application <ref> [14, 32] </ref>. In general, these protocols use the programmers knowledge of sharing and synchronization patterns to send explicit update messages for critical data in the computation loop. Speedups for these versions are presented as the taller, hatched bars in Figure 5.
Reference: [15] <author> Babak Falsafi and David A. Wood. </author> <title> When does Dedicated Protocol Processing Make Sense? Technical Report 1302, </title> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: This configuration provides a direct comparison to the integrated systems, which also have dedicated protocol processing resources. However, in the long term, we expect that decoupled systems will dynamically schedule protocol processing along with computation across all of the processors in a multiprocessor node <ref> [15] </ref>. We find, not surprisingly, that the decoupled designs have significantly higher communication overheads. A simple remote miss takes roughly four times longer on Typhoon-0 than on either of the integrated systems.
Reference: [16] <author> Hakan Grahn and Per Stenstrom. </author> <title> Efficient Strategies for SoftwareOnly Directory Protocols in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 3847, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: These sends must cross the bus on all three systems; the tight coupling of the network interface and protocol processor on Typhoon only improves performance on the receiving node. 4 Related work Alewife [1] was the first hybrid hardware/software DSM system. As with later variants from other researchers <ref> [22, 45, 16] </ref>, custom hardware generates requests and handles responses on the caching side and implements some basic directory functions. In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface.
Reference: [17] <author> Linley Gwennap. </author> <title> Intels P6 Bus Designed for Multiprocessing. </title> <type> Microprocessor Report, 9(7), </type> <month> May 30, </month> <year> 1995. </year>
Reference-contexts: The access control device masks the arbiter to keep the processor off the bus until the access can be completed [28]. While this may be difficult to implement on existing systems, its performance is representative of emerging systems which support deferred responses, either explicitly (like the Intel P6 <ref> [17] </ref>) or using a split-transaction bus. (Unfortunately, our SPARCstation 20 implementation must generate a bus error; the kernel trap vector is modified to spin on a ag which is set when the data arrives.) Results were obtained using a detailed execution-driven discrete-event simulator.
Reference: [18] <author> Erik Hagersten, Ashley Saulsbury, and Anders Landin. </author> <title> Simple COMA Node Implementations. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The first integrated system is an idealized Simple COMA <ref> [18] </ref> implementation that combines a network interface and access control with an infinitely fast hardwired protocol state machine. The second, modeled after Typhoon [37], replaces the state machine with a user-level protocol processor to allow execution of optimized application-specific protocols. <p> The second subsection describes the integrated systems. The final subsection describes features common to both decoupled systems and then the systems themselves. 2.1 Common framework support hardware (represented by the cloud in Figure 1) encompasses three components: access control, messaging, and protocol processing. As in Simple COMA <ref> [18] </ref> and Typhoon [37], each nodes local DRAM acts as a cache for remote data using a combination of virtual address translation and fine-grain access control. Virtual address translation directs remote data accesses to local physical memory. Accesses that require local memory allocation are detected and handled via page faults. <p> The first system is an idealized implementation of Simple COMA <ref> [18] </ref>. The network interface queues and access control snooping logic are tightly coupled with an infinitely fast hardwired state machine implementing a full-map invalidation-based coherence protocol. We assume this device has zero-cycle access to all protocol state information. Only network queue and memory bus interface delays are charged. <p> Like Typhoon-0 and Typhoon-1, StarT-NG uses a commodity CPU as a protocol processor, although StarT-NG places the network interface on the CPUs level 2 cache bus. Typhoon-0 and Typhoon-1 use virtual address translation to map remote pages into local DRAM, a feature shared with Typhoon, Simple COMA <ref> [18] </ref>, and page-based software DSM systems [29, 10, 24]. Because cached remote data is transparently accessed, protocol handlers are only executed when coherence action is required.
Reference: [19] <author> John Heinlein, Kourosh Gharachorloo, Scott A. Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 3850, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This allows the use of previously written application-specific protocols [14, 32], which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices <ref> [6, 19, 43] </ref>. Using simulation, we quantify the performance impact of decouplingthat is, given subsystems with similar capabilities, what is the performance penalty for implementing those subsystems as separate components rather than integrating them into a single device? We compare the performance of Typhoon-0 and Typhoon-1 with two integrated systems. <p> How-- ever, passing virtual addresses as data requires a translation and a protection check in the receiving component. We avoid both of these using a shadow space <ref> [6, 19, 43] </ref>. The access control device supports a physical address rangethe shadow spaceas large as, and at a fixed offset from, the machines physical memory. Accesses to the shadow space are interpreted as operations on the real memory space. <p> Because the DMA size is limited to a single cache block, there is only a small window during which the virtual-to-physical translation must remain valid, so we can use a simple technique to prevent the DMA mechanism from using a stale translation <ref> [19] </ref>. Typhoon-1 provides a single status bit that indicates that a DMA is outstanding; the operating system temporarily disables DMA initiation and waits for this bit to clear before invalidating a translation that was potentially used for DMA.
Reference: [20] <author> Mark Heinrich, Jeffrey Kuskin, David Ofelt, John Heinlein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 274285, </pages> <year> 1994. </year>
Reference-contexts: The idealized Simple COMA system requires one additional cycle per message, for a total of 301 processor cycles, or about 1.5 ms. For comparison, the FLASH designers report remote read miss latencies of 1.11 and 1.45 ms, depending on whether the data is dirty in the remote processors cache <ref> [20] </ref>. 1 Because these fundamental latencies dominate, Typhoon takes only 33% longer to satisfy the miss despite the cost of running software handlers. The decoupled designs do not fare as well in this comparison.
Reference: [21] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 111122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Once memory is updated, the tag can be upgraded. Typhoon-0 supports dispatch of access fault and message handlers using a cacheable control register called the dispatch register. The dispatch register combines a user-specified base address with status information to form a program counter <ref> [21] </ref>. The status portion of the PC forms an index into a code table, much like a processor trap vector table. The protocol processor polls for events by performing an indirect jump to the PC location.
Reference: [22] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):300318, </volume> <month> November </month> <year> 1993. </year> <note> An earlier version appeared in ASPLOS V. </note>
Reference-contexts: These sends must cross the bus on all three systems; the tight coupling of the network interface and protocol processor on Typhoon only improves performance on the receiving node. 4 Related work Alewife [1] was the first hybrid hardware/software DSM system. As with later variants from other researchers <ref> [22, 45, 16] </ref>, custom hardware generates requests and handles responses on the caching side and implements some basic directory functions. In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface.
Reference: [23] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <type> Technical Report LCS-TM-517, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: At the other extreme, many systems use no DSM-specific hardware. These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software <ref> [4, 39, 23] </ref> or using standard virtual memory hardware [29, 10, 24]. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W.
Reference: [24] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <type> Technical Report 93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software [4, 39, 23] or using standard virtual memory hardware <ref> [29, 10, 24] </ref>. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W. Pfile, and David A. <p> Typhoon-0 and Typhoon-1 use virtual address translation to map remote pages into local DRAM, a feature shared with Typhoon, Simple COMA [18], and page-based software DSM systems <ref> [29, 10, 24] </ref>. Because cached remote data is transparently accessed, protocol handlers are only executed when coherence action is required.
Reference: [25] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware.
Reference: [26] <author> Leonidas I. Kontothanassis and Michael L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture (HPCA), </booktitle> <pages> pages 286295, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In contrast, StarT-NG runs a software handler for every remote reference that misses in the hardware cache, even if the data is cached in local DRAM; FLASH must execute software on every local hardware cache miss. Kontothanassis and Scott <ref> [26] </ref> propose using network interfaces such as SHRIMP [6] to implement weakly consistent page- based DSM.
Reference: [27] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-Independent Executable Editing. </title> <booktitle> In Proceedings of the SIGPLAN 95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 291300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The cache, MBus, and device simulation are detailed enough that they were used for initial design verification of the Typhoon-0 implementation. Actual SPARC binaries are rewritten (using a tool based on EEL <ref> [27] </ref>) to replace memory accesses with calls to the simulator and to add instrumentation to count instruction execution cycles. All software protocols were written in C and compiled and linked with the simulated benchmarks.
Reference: [28] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):63 79, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware. <p> The access control device masks the arbiter to keep the processor off the bus until the access can be completed <ref> [28] </ref>.
Reference: [29] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software [4, 39, 23] or using standard virtual memory hardware <ref> [29, 10, 24] </ref>. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W. Pfile, and David A. <p> Typhoon-0 and Typhoon-1 use virtual address translation to map remote pages into local DRAM, a feature shared with Typhoon, Simple COMA [18], and page-based software DSM systems <ref> [29, 10, 24] </ref>. Because cached remote data is transparently accessed, protocol handlers are only executed when coherence action is required.
Reference: [30] <institution> LSI Logic Inc. </institution> <note> L64601 SCI NodeChip Technical Manual. </note>
Reference-contexts: Assuming cache hits, this sequence takes ten cycles on the HyperSPARC, which has a one cycle load-use delay. other than the network interface itself, which could be a separate single-chip device such as the Myrinet LANai [7], Dolphin/LSI Logic SCI NodeChip <ref> [30] </ref>, or Cray SCX adapter [40]. Typhoon-1 incorporates three specific advances over Typhoon-0: user-level cache block DMA with combined access control, a tagged block buffer, and enhanced message dispatching.
Reference: [31] <author> Michael Marchetti, Leonidas Kontothanassis, Ricardo Bianchini, and Michael L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Ninth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Dsmc, moldyn, and unstructured are irregular applications originally from Maryland [32]. All of the benchmarks are written in C and compiled with gcc version 2.6.3 at optimization level -O2. All of the benchmarks except em3d use a first-touch migrate- once scheme <ref> [31] </ref> to improve the assignment of pages to home nodes. The first node to touch a page after the parallel phase of the program begins becomes the pages home for the remainder of the execution.
Reference: [32] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: To exploit the exibility of software protocol processing, both decoupled designs run user-level protocol handlers and support the Tempest interface [35]. This allows the use of previously written application-specific protocols <ref> [14, 32] </ref>, which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices [6, 19, 43]. <p> Values are 200 MHz processor cycle counts except where noted. [3], parallelized for shared memory [9]. Barnes is from the SPLASH-2 suite [44]. Em3d is a shared-memory version of an original Split-C program from Berkeley [12]. Dsmc, moldyn, and unstructured are irregular applications originally from Maryland <ref> [32] </ref>. All of the benchmarks are written in C and compiled with gcc version 2.6.3 at optimization level -O2. All of the benchmarks except em3d use a first-touch migrate- once scheme [31] to improve the assignment of pages to home nodes. <p> For each of the Tempest systems, we also ran existing custom protocols that were hand-optimized for each application <ref> [14, 32] </ref>. In general, these protocols use the programmers knowledge of sharing and synchronization patterns to send explicit update messages for critical data in the computation loop. Speedups for these versions are presented as the taller, hatched bars in Figure 5.
Reference: [33] <author> A. Nowatzyk, M. Monger, M. Parkin, E. Kelly, M. Browne, G. Aybay, and D. Lee. S3.mp: </author> <title> A Multiprocessor in a Matchbox. </title> <booktitle> In Proc. </booktitle> <address> PASA, </address> <year> 1993. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware. <p> The second, modeled after Typhoon [37], replaces the state machine with a user-level protocol processor to allow execution of optimized application-specific protocols. All of these systems can be built from off-the-shelf workstations, unlike those that replace standard cache or memory controllers, such as Alewife [1], FLASH [13], and S3.mp <ref> [33] </ref>. In our evaluation, all of the systems use a single processor per node for computation, although small-scale (e.g., four-way) bus- based multiprocessor nodes may be more cost-effective. All of the designs we describe are compatible with multiprocessor nodes.
Reference: [34] <author> Robert W. Pfile. </author> <title> Typhoon-Zero Implementation: The Vortex Module. </title> <type> Technical Report 1290, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Typhoon-0 also uses a generic network; a custom access control module per node is its only DSM-specific hardware. To demonstrate the feasibility and relative simplicity of this access control device, two people implemented it in less than one year using two FPGAs and two SRAMs <ref> [34] </ref>. Typhoon-1 uses a similar access control device that also integrates the network interface. This integration improves performance by eliminating data movement through the protocol processor. <p> The first two components are off-the-shelf devices, while the third is custom hardware. To demonstrate the feasibility of the access control device, we implemented it for a cluster of SPARCstation 20 workstations <ref> [34] </ref>. The device is a standard-sized MBus module (approximately 3.3" by 5.8") containing two Altera 81188-2A FPGAs (each containing 12,000 usable gates), two 4M by 1 static RAMs, and a few miscellaneous parts.
Reference: [35] <author> Steven K. Reinhardt. </author> <title> Tempest Interface Specification (Revision 1.2.1). </title> <type> Technical Report 1267, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Both devices use cacheable control registers, a novel technique that increases the efficiency of communication from bus devices to processors by leveraging the local bus coherence protocol. To exploit the exibility of software protocol processing, both decoupled designs run user-level protocol handlers and support the Tempest interface <ref> [35] </ref>. This allows the use of previously written application-specific protocols [14, 32], which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices [6, 19, 43].
Reference: [36] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: All software protocols were written in C and compiled and linked with the simulated benchmarks. Full application results were obtained by simulating the nodes of a system in parallel on a Thinking Machines CM-5 using a conservative, synchronous parallel simulation algorithm based on the Wisconsin Wind Tunnel <ref> [36] </ref>. 3.1 Micro-evaluation To gain insight into the overheads of these systems, we trace a single remote read miss and break down the latency into its components. We assume a cache page has been previously allocated on the caching node and the block is unshared at the home node.
Reference: [37] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Access control is the only function for which there is no prospective off-the-shelf solution. We can leverage available components by decoupling DSM supportthat is, by building simple access control hardware that co-exists with off-the-shelf processors and networks. In contrast, dedicated DSM systems <ref> [1, 28, 13, 25, 33, 37] </ref> integrate all three functions in custom hardware. This hardware provides high performance, but requires complex custom chips that are costly both to design and to manufacture. At the other extreme, many systems use no DSM-specific hardware. <p> The first integrated system is an idealized Simple COMA [18] implementation that combines a network interface and access control with an infinitely fast hardwired protocol state machine. The second, modeled after Typhoon <ref> [37] </ref>, replaces the state machine with a user-level protocol processor to allow execution of optimized application-specific protocols. All of these systems can be built from off-the-shelf workstations, unlike those that replace standard cache or memory controllers, such as Alewife [1], FLASH [13], and S3.mp [33]. <p> The final subsection describes features common to both decoupled systems and then the systems themselves. 2.1 Common framework support hardware (represented by the cloud in Figure 1) encompasses three components: access control, messaging, and protocol processing. As in Simple COMA [18] and Typhoon <ref> [37] </ref>, each nodes local DRAM acts as a cache for remote data using a combination of virtual address translation and fine-grain access control. Virtual address translation directs remote data accesses to local physical memory. Accesses that require local memory allocation are detected and handled via page faults. <p> We assume this device has zero-cycle access to all protocol state information. Only network queue and memory bus interface delays are charged. The second system, Typhoon <ref> [37] </ref>, combines a network interface, access control logic, and a user-level protocol processor on a single device (see Figure 2). Dispatch hardware rapidly invokes user handlers in response to message arrivals and block access faults. <p> In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface. To avoid deadlock, the CPU must handle interrupts while memory accesses are outstanding, precluding some existing off- the-shelf microprocessors. FLASH [13], StarT-NG [11], and Typhoon <ref> [37] </ref> perform all protocol processingon both the directory and the caching nodesin software. FLASH and Typhoon execute protocol soft ware on a custom processor integrated with the network interface; FLASH also incorporates the memory controller on this device.
Reference: [38] <author> ROSS Technology Inc. </author> <title> SPARC RISC Users Guide, </title> <month> September </month> <year> 1993. </year>
Reference-contexts: We simulate 32-node systems. Each node has a 200 MHz dual- issue SPARC processor. We assume a perfect instruction cache and a 1 MB direct-mapped data cache with 64-byte address blocks and 32-byte subblocks. The instruction latencies, issue rules, and memory hierarchy are modeled after the Ross HyperSPARC <ref> [38, 41] </ref>. The processor (s), memory, access control and/or network interface devices within each node are connected by a 50 MHz MBus. The MBus is a 64-bit, multiplexed address/data bus that maintains coherence on 32-byte blocks using a MOESI protocol [42].
Reference: [39] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: At the other extreme, many systems use no DSM-specific hardware. These systems send messages over existing networks and do protocol processing in software on general-purpose CPUsas do the decoupled systems we propose. However, they perform access control either entirely in software <ref> [4, 39, 23] </ref> or using standard virtual memory hardware [29, 10, 24]. The cost of avoiding custom hardware for access control is that the user must compromise on performance, program <br>- ming model, or both. Decoupled Hardware Support for Distributed Shared Memory Steven K. Reinhardt, Robert W. <p> Speedups for these versions are presented as the taller, hatched bars in Figure 5. These protocols were written and optimized for a very different system Blizzard-E <ref> [39] </ref> on the CM-5with much slower processors and even higher relative overheads. Although their impact is reduced by the lower overheads of these hardware-assisted systems, all of the protocols still provide some improvement over the default shared memory.
Reference: [40] <author> Steve Scott. </author> <title> The SCX Channel: A New, Supercomputer-Class System Interconnect. Hot Interconnects III, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: Assuming cache hits, this sequence takes ten cycles on the HyperSPARC, which has a one cycle load-use delay. other than the network interface itself, which could be a separate single-chip device such as the Myrinet LANai [7], Dolphin/LSI Logic SCI NodeChip [30], or Cray SCX adapter <ref> [40] </ref>. Typhoon-1 incorporates three specific advances over Typhoon-0: user-level cache block DMA with combined access control, a tagged block buffer, and enhanced message dispatching. First, Typhoon-1 uses Typhoon-0s shadow space to support user-level DMA at cache-block granularity so that data transfers need not pass through the protocol processor.
Reference: [41] <author> Doug Shore. </author> <type> Personal communication, </type> <month> November </month> <year> 1994. </year>
Reference-contexts: We simulate 32-node systems. Each node has a 200 MHz dual- issue SPARC processor. We assume a perfect instruction cache and a 1 MB direct-mapped data cache with 64-byte address blocks and 32-byte subblocks. The instruction latencies, issue rules, and memory hierarchy are modeled after the Ross HyperSPARC <ref> [38, 41] </ref>. The processor (s), memory, access control and/or network interface devices within each node are connected by a 50 MHz MBus. The MBus is a 64-bit, multiplexed address/data bus that maintains coherence on 32-byte blocks using a MOESI protocol [42].
Reference: [42] <author> Sun Microsystems Inc. </author> <title> SPARC MBus Interface Specification, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: The processor (s), memory, access control and/or network interface devices within each node are connected by a 50 MHz MBus. The MBus is a 64-bit, multiplexed address/data bus that maintains coherence on 32-byte blocks using a MOESI protocol <ref> [42] </ref>. On a cache miss, the critical word is returned from main memory 140 ns (seven bus cycles or 28 processor cycles) after the request is issued on the MBus. Miss detection, processor/bus clock synchronization, and bus arbitration add 11-14 processor cycles to the total miss latency.
Reference: [43] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: This allows the use of previously written application-specific protocols [14, 32], which selectively replace shared-memory operations with message passing operations to improve performance. Shadow spaces allow user-level handlers to directly convey protected addresses to the bus devices <ref> [6, 19, 43] </ref>. Using simulation, we quantify the performance impact of decouplingthat is, given subsystems with similar capabilities, what is the performance penalty for implementing those subsystems as separate components rather than integrating them into a single device? We compare the performance of Typhoon-0 and Typhoon-1 with two integrated systems. <p> How-- ever, passing virtual addresses as data requires a translation and a protection check in the receiving component. We avoid both of these using a shadow space <ref> [6, 19, 43] </ref>. The access control device supports a physical address rangethe shadow spaceas large as, and at a fixed offset from, the machines physical memory. Accesses to the shadow space are interpreted as operations on the real memory space.
Reference: [44] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24 36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Breakdown of remote miss latency. Values are 200 MHz processor cycle counts except where noted. [3], parallelized for shared memory [9]. Barnes is from the SPLASH-2 suite <ref> [44] </ref>. Em3d is a shared-memory version of an original Split-C program from Berkeley [12]. Dsmc, moldyn, and unstructured are irregular applications originally from Maryland [32]. All of the benchmarks are written in C and compiled with gcc version 2.6.3 at optimization level -O2.
Reference: [45] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156168, </pages> <month> May </month> <year> 1993. </year> <note> Also appeared in CMG Transactions, Spring 1994. </note>
Reference-contexts: These sends must cross the bus on all three systems; the tight coupling of the network interface and protocol processor on Typhoon only improves performance on the receiving node. 4 Related work Alewife [1] was the first hybrid hardware/software DSM system. As with later variants from other researchers <ref> [22, 45, 16] </ref>, custom hardware generates requests and handles responses on the caching side and implements some basic directory functions. In all of these designs, a single device integrates this hardware control with the cache and/or memory controllers and the network interface.
Reference: [46] <author> David A. Wood and Mark D. Hill. </author> <title> Cost-Effective Parallel Computing. </title> <journal> IEEE Computer, </journal> <volume> 28(2):6972, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: However, application-specific protocols significantly improve their performance, providing speedups of 16 or more on both decoupled systems. These speedups, combined with the simplicity of the required hardware, indi <p>- cate that decoupled designs have the potential to provide cost- effective parallelism <ref> [46] </ref> for shared-memory programs. In the next section, we describe the systems in more detail. Section 3 presents simulated performance results.
References-found: 46

