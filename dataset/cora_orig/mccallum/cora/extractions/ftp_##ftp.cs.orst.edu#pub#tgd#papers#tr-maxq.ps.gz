URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-maxq.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00347.html
Root-URL: 
Email: tgd@cs.orst.edu  
Title: Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition  
Author: Thomas G. Dietterich 
Date: August 24, 1997  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: This paper describes the MAXQ method for hierarchical reinforcement learning based on a hierarchical decomposition of the value function and derives conditions under which the MAXQ decomposition can represent the optimal value function. We show that for certain execution models, the MAXQ decomposition will produce better policies than Feudal Q learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P., & Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Adv. in Neural Info. Proc. Sys., </booktitle> <volume> 5. </volume> <pages> (pp. </pages> <publisher> 271-278) Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical reinforcement learning: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. </pages> <address> 167-173) San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 323-339. 7 </pages>
References-found: 3

