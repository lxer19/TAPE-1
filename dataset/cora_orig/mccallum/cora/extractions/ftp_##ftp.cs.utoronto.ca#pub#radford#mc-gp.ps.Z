URL: ftp://ftp.cs.utoronto.ca/pub/radford/mc-gp.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/res-mcmc.html
Root-URL: 
Email: radford@stat.utoronto.ca  
Title: Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification  
Author: Radford M. Neal 
Date: 20 January 1997  
Address: Toronto, Toronto, Ontario, Canada  
Affiliation: Department of Statistics and Department of Computer Science University of  
Abstract: Technical Report No. 9702, Department of Statistics, University of Toronto Abstract. Gaussian processes are a natural way of defining prior distributions over functions of one or more input variables. In a simple nonparametric regression problem, where such a function gives the mean of a Gaussian distribution for an observed response, a Gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases. Hyperparameters that define the covariance function of the Gaussian process can be sampled using Markov chain methods. Regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations. Software is now available that implements these methods using covariance functions with hierarchical parameterizations. Models defined in this way can discover high-level properties of the data, such as which inputs are relevant to predicting the response. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barber, D. and Williams, C. K. I. </author> <title> (1997) "Gaussian processes for Bayesian classification via hybrid Monte Carlo", </title> <note> to appear in Advances in Neural Information Processing Systems 9. </note>
Reference: <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <title> (1987) "Hybrid Monte Carlo", </title> <journal> Physics Letters B, </journal> <volume> vol. 195, </volume> <pages> pp. 216-222. </pages>
Reference: <author> Gibbs, M. N. and MacKay, D. J. C. </author> <title> (1997a) "Efficient implementation of Gaussian processes", </title> <type> draft manuscript. </type>
Reference-contexts: It may also be possible to reduce the time requirements using more sophisticated algorithms <ref> (Gibbs and MacKay 1997a) </ref>. The characteristics of a Gaussian process model can easily be controlled by writing the covariance function in terms of "hyperparameters".
Reference: <author> Gibbs, M. N. and MacKay, D. J. C. </author> <title> (1997b) "Variational Gaussian process classifiers", </title> <type> draft manuscript. </type>
Reference: <author> Gilks, W. R. and Wild, P. </author> <title> (1992) "Adaptive rejection sampling for Gibbs sampling", </title> <journal> Applied Statistics, </journal> <volume> vol. 41, </volume> <pages> pp. 337-348. </pages>
Reference: <author> Horn, R. A. and Johnson, C. R. </author> <title> (1985) Matrix Analysis, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: One way to construct a variety of covari-ance functions is by adding and multiplying together other covariance functions, since the element-by-element sum or product of any two symmetric, positive semidefinite matrices is also symmetric and positive semidefinite <ref> (Horn and Johnson 1985, 7.1.3 and 7.5.3) </ref>. Sums of covariance functions are useful in defining models with an additive structure, since the covariance function for a sum of independent Gaussian processes is simply the sum of their separate covariance functions.
Reference: <author> Horowitz, A. M. </author> <title> (1991) "A generalized guided Monte Carlo algorithm", </title> <journal> Physics Letters B, </journal> <volume> vol. 268, </volume> <pages> pp. 247-252. </pages>
Reference: <author> Mardia, K. V. and Marshall, R. J. </author> <title> (1984) "Maximum likelihood estimation of models for residual covariance in spatial regression", </title> <journal> Biometrika, </journal> <volume> vol. 71, </volume> <pages> pp. 135-146. </pages>
Reference-contexts: One approach to adapting these hyper-parameters to the observed data is to estimate them by maximum likelihood (or maximum penalized likelihood), as has long been done in the context of spatial statistics <ref> (eg, Mardia and Marshall 1984) </ref>. In a fully Bayesian approach, the hyperparameters are given prior distributions. Predictions are then made by averaging over the posterior distribution for the hyperparameters, which can be done using Markov chain Monte Carlo methods. <p> For some of the Markov chain sampling methods, the derivatives of L with respect to the various hy-perparameters are also required. The derivative of the log likelihood with respect to a hyperparameter can be written as follows <ref> (Mardia and Marshall 1984) </ref>: @L = 2 @ + 2 @ The trace of the product in the first term can be computed in time proportional to n 2 , assuming that C 1 has already been computed.
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic Inference Using Markov Chain Monte Carlo Methods, </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto. </institution> <note> Available in Postscript via URL http://www.cs.utoronto.ca/~radford/. </note>
Reference-contexts: The number of hyperparameters will vary from around three or four for a very simple regression model up to several dozen or more for a model with many inputs, whose relevances are individually controlled using hyperparameters such as the u of equation (15). Markov chain Monte Carlo methods (see <ref> (Neal 1993) </ref> for a review) seem to be the only feasible approach to performing these integrations, at least for the more complex models. <p> The software supports this option, along with a variety of other Markov chain sampling methods. However, simple methods such as this explore the region of high probability by an inefficient random walk. It is probably better for most models to use a method that can suppress these random walks <ref> (Neal 1993, 1996) </ref>. The most appropriate way to suppress random walks for this problem seems to be to use the hybrid Monte Carlo method of Duane, Kennedy, Pendleton, and Roweth (1987), or the variant of this method due to Horowitz (1991).
Reference: <author> Neal, R. M. </author> <title> (1994) "An improved acceptance procedure for the hybrid Monte Carlo algorithm", </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 111, </volume> <pages> pp. 194-203. </pages>
Reference-contexts: In the original hybrid Monte Carlo method of Duane, et al. (1987), several leapfrog updates are done, after which a decision whether to accept the result is made. The momentum is also randomized at this time. A variation using "windows" of states <ref> (Neal 1994) </ref> can be used to increase the acceptance probability. In the variation due to Horowitz (1991), an acceptance decision is made after each leapfrog update, after which the momentum is only partially randomized. I refer to this as hybrid Monte Carlo with "persistence" of the momentum.
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Recently, I have shown that many Bayesian regression models based on neural networks converge to Gaussian processes in the limit of an infinite network <ref> (Neal 1996) </ref>. This has motivated examination of Gaussian process models for the high-dimensional applications to which neural networks are typically applied (Williams and Rasmussen 1996). <p> I have employed the hybrid Monte Carlo method to do Bayesian inference for neural network models <ref> (Neal 1996) </ref>, and Rasmussen (1996) has used it for Gaussian process regression. Several variants of the hybrid Monte Carlo method are supported by the Markov chain modules that I use for both the neural network and the Gaussian process software. <p> Such a model can automatically determine an appropriate additive decomposition, if an additive model is in fact appropriate. This mirrors a similar idea for neural network models <ref> (Neal 1996, Section 5.2) </ref>. The implementation described here is rather straightforward. Most operations are performed in the simplest way that gives acceptable results. A number of modifications can be contemplated. Faster convergence could probably be obtained by updating the latent variables using hybrid Monte Carlo rather than Gibbs sampling.
Reference: <author> O'Hagan, A. </author> <title> (1978) "Curve fitting and optimal design for prediction" (with discussion), </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 40, </volume> <pages> pp. 1-42. </pages>
Reference: <author> O'Hagan, A. </author> <booktitle> (1994) Bayesian Inference (Volume 2B in Kendall's Advanced Theory of Statistics). </booktitle>
Reference: <author> Rasmussen, C. </author> <title> (1996) Evaluation of Gaussian Processes and other Methods for Non-Linear Regression, </title> <type> Ph.D. Thesis, </type> <institution> University of Toronto, Department of Computer Science. </institution> <note> Available in Postscript via URL http://www.cs.utoronto.ca/~carl/. </note>
Reference-contexts: Recently, I have shown that many Bayesian regression models based on neural networks converge to Gaussian processes in the limit of an infinite network (Neal 1996). This has motivated examination of Gaussian process models for the high-dimensional applications to which neural networks are typically applied <ref> (Williams and Rasmussen 1996) </ref>. The empirical work of Rasmussen (1996) has demonstrated that Gaussian process models have better predictive performance than several other nonparametric 1 regression methods over a range of tasks with varying characteristics. <p> In a fully Bayesian approach, the hyperparameters are given prior distributions. Predictions are then made by averaging over the posterior distribution for the hyperparameters, which can be done using Markov chain Monte Carlo methods. These two approaches often give similar results <ref> (Williams and Rasmussen 1996, Rasmussen 1996) </ref>, but the fully Bayesian approach may be more robust when the models are elaborate. Applying Gaussian process models to classification problems presents new computational problems, since the joint distribution of all quantities is no longer Gaussian.
Reference: <author> Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, P. </author> <title> (1989) "Design and analysis of computer experiments" (with discussion), </title> <journal> Statistical Science, </journal> <volume> vol. 4, </volume> <pages> pp. 409-435. </pages>
Reference: <author> Thisted, R. A. </author> <title> (1988) Elements of Statistical Computing, </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher> <editor> von Mises, R. </editor> <booktitle> (1964) Mathematical Theory of Probability and Statistics, </booktitle> <address> New York: </address> <publisher> Academic Press. 23 Wahba, </publisher> <editor> G. </editor> <title> (1978) "Improper priors, spline smoothing and the problem of guarding against model errors in regression", </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 40, </volume> <pages> pp. 364-372. </pages>
Reference-contexts: The present implementation is based on finding the Cholesky decomposition of C | that is, the lower-triangular matrix, L, for which C = LL T . The Cholesky decomposition can be found by a simple algorithm <ref> (see, for example, Thisted 1988, Section 3.3) </ref>, which runs in time proportional to n 3 .
Reference: <author> Williams, C. K. I. and Rasmussen, C. E. </author> <title> (1996) "Gaussian processes for regression", </title> <editor> in D. S. Touretzky, M. C. Mozer, and M. E. </editor> <booktitle> Hasselmo (editors) Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Recently, I have shown that many Bayesian regression models based on neural networks converge to Gaussian processes in the limit of an infinite network (Neal 1996). This has motivated examination of Gaussian process models for the high-dimensional applications to which neural networks are typically applied <ref> (Williams and Rasmussen 1996) </ref>. The empirical work of Rasmussen (1996) has demonstrated that Gaussian process models have better predictive performance than several other nonparametric 1 regression methods over a range of tasks with varying characteristics. <p> In a fully Bayesian approach, the hyperparameters are given prior distributions. Predictions are then made by averaging over the posterior distribution for the hyperparameters, which can be done using Markov chain Monte Carlo methods. These two approaches often give similar results <ref> (Williams and Rasmussen 1996, Rasmussen 1996) </ref>, but the fully Bayesian approach may be more robust when the models are elaborate. Applying Gaussian process models to classification problems presents new computational problems, since the joint distribution of all quantities is no longer Gaussian.
Reference: <author> Yaglom, A. M. </author> <title> (1987) Correlation Theory of Stationary and Related Random Functions, Volume I: Basic Results, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher> <pages> 24 </pages>
References-found: 18

