URL: http://www.eecs.umich.edu/techreports/cse/1998/CSE-TR-373-98.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse98.html
Root-URL: http://www.cs.umich.edu
Email: -postiffm, tyson, tnm-@eecs.umich.edu  
Title: Performance Limits of Trace Caches  
Author: Matt Postiff, Gary Tyson, and Trevor Mudge 
Date: September 8, 1998  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory, EECS Department University of Michigan  
Abstract: A growing number of studies have explored the use of trace caches as a mechanism to increase instruction fetch bandwidth. The trace cache is a memory structure that stores statically noncontiguous but dynamically adjacent instructions in contiguous memory locations. When coupled with an aggressive trace or multiple branch predictor, it can fetch multiple basic blocks per cycle using a single-ported cache structure. This paper compares trace cache performance to the theoretical limit of a three-block fetch mechanism equivalent to an idealized 3-ported instruction cache with a perfect alignment network. Several new metrics are defined to formalize analysis of the trace cache. These include fragmentation, duplication, indexability, and efficiency metrics. We show that performance is more limited by branch mispredictions than ability to fetch multiple blocks per cycle. As branch prediction improves, high duplication and the resulting low efficiency are shown to be among the reasons that the trace cache does not reach its upper bound. Based on the shortcomings of the trace cache discovered in this paper, we identify some potential future research areas. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chih-Po Wen. </author> <title> Improving Instruction Supply Efficiency in Superscalar Architectures Using Instruction Trace Buffers. </title> <booktitle> Proc. Symp. Applied Computing. </booktitle> <volume> Volume 1, </volume> <pages> pp. 28-36, </pages> <month> March, </month> <year> 1992. </year>
Reference-contexts: The branch address cache [3] requires a highly interleaved instruction cache to support multiple accesses per cycle. The trace cache is an extension of the fill unit and loop/trace buffer <ref> [1] </ref> that attempts to collect noncontiguous basic blocks from the dynamic instruction stream into a single contiguous cache memory location [6][8][9][10][11][12]. The trace cache is compared to several of the previous proposals in [8]. A diagram of the trace cache fetch mechanism is shown in Figure 1.
Reference: [2] <author> S. Melvin, M. Shebanow, and Y. Patt. </author> <title> Hardware support for large atomic units in dynamically scheduled machines. </title> <booktitle> 21st Intl. Symp. On Microarchitecture, </booktitle> <pages> pp. 60-63, </pages> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: Section 7 concludes. 2. Related Work and the Trace Cache Fetch Mechanism Many caching techniques have been proposed to enhance instruction fetch in superscalar processors. The fill unit assembles multiple instructions from a single basic block for single-cycle issue to a wide-issue processor <ref> [2] </ref>[4][5]. The fill unit in [2] is a post-decode cache for CISC instructions which contains partially renamed groups of micro-operations. It was primarily intended as a mechanism to allow a large number of micro-operations to be executed concurrently on an out-of-order processor.
Reference: [3] <author> Tse-Yu Yeh, Deborah T. Marr and Yale N. Patt. </author> <title> Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache. </title> <booktitle> 1993 Intl. Conf. Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <month> July, </month> <year> 1993. </year>
Reference-contexts: The collapsing buffer [7] relies on multiple accesses to a branch target buffer to produce the addresses needed for fetching multiple basic blocks in a single 3 cycle. The branch address cache <ref> [3] </ref> requires a highly interleaved instruction cache to support multiple accesses per cycle. The trace cache is an extension of the fill unit and loop/trace buffer [1] that attempts to collect noncontiguous basic blocks from the dynamic instruction stream into a single contiguous cache memory location [6][8][9][10][11][12].
Reference: [4] <author> Manoj Franklin and Mark Smotherman. </author> <title> A Fill-Unit Approach to Multiple Instruction Issue. </title> <booktitle> Proceedings of the 27th Intl. Symp. on Microarchitecture, </booktitle> <pages> pp. 162-171, </pages> <month> Nov, </month> <year> 1994. </year>
Reference-contexts: It was primarily intended as a mechanism to allow a large number of micro-operations to be executed concurrently on an out-of-order processor. In conjunction with the decoded instruction cache, this model reduces both the decoding and dependency checking necessary in the critical execution path. The fill unit of <ref> [4] </ref> is designed to eliminate complex dependency checking logic in the processors critical path by assembling instructions into VLIW format and caching the result in a separate shadow cache. The work in [5] is an extension for superscalar processors with complex decoding requirements.
Reference: [5] <author> Mark Smotherman and Manoj Franklin. </author> <title> Improving CISC Instruction Decoding Performance Using a Fill Unit. </title> <booktitle> Proc. 28th Intl. Symp. Microarchitecture, </booktitle> <pages> pp. 219-229, </pages> <month> Nov, </month> <year> 1995. </year>
Reference-contexts: The fill unit of [4] is designed to eliminate complex dependency checking logic in the processors critical path by assembling instructions into VLIW format and caching the result in a separate shadow cache. The work in <ref> [5] </ref> is an extension for superscalar processors with complex decoding requirements. More recently, several fetch mechanisms have been proposed to reduce the impact of branches in the instruction stream.
Reference: [6] <author> Alexander Peleg and Uri Weiser. </author> <title> Dynamic Flow Instruction Cache Memory Organized Around Trace Segments Independent of Virtual Address Line. </title> <institution> United States Patent 5,381,533, </institution> <note> January 10, 1995. http://www.patents.ibm.com/. </note>
Reference: [7] <author> T. Conte, K. Menezes, P. Mills, and B. Patel. </author> <title> Optimization of instruction fetch mechanisms for high issue rates. </title> <booktitle> 22nd Intl. Symp. On Computer Architecture, </booktitle> <pages> pp. 333-344, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The work in [5] is an extension for superscalar processors with complex decoding requirements. More recently, several fetch mechanisms have been proposed to reduce the impact of branches in the instruction stream. The collapsing buffer <ref> [7] </ref> relies on multiple accesses to a branch target buffer to produce the addresses needed for fetching multiple basic blocks in a single 3 cycle. The branch address cache [3] requires a highly interleaved instruction cache to support multiple accesses per cycle.
Reference: [8] <author> Eric Rotenberg, Steve Bennett and Jim Smith. </author> <title> Trace Cache: A Low Latency Approach to High Bandwidth Instruction Fetching. </title> <institution> University of Wisconsin, Madison Tech. </institution> <type> Report. </type> <month> April, </month> <year> 1996. </year>
Reference-contexts: The trace cache is an extension of the fill unit and loop/trace buffer [1] that attempts to collect noncontiguous basic blocks from the dynamic instruction stream into a single contiguous cache memory location [6]<ref> [8] </ref>[9][10][11][12]. The trace cache is compared to several of the previous proposals in [8]. A diagram of the trace cache fetch mechanism is shown in Figure 1. The branch predictor is either a multiple branch predictor [10] or a trace predictor [12]. The fill unit collects basic blocks and builds traces for storage in the trace cache. <p> Previous studies have compared trace cache performance to the performance of sequential fetching mechanisms, i.e. a fetch engine that can fetch up to one branch (SEQ.1) or up to 3 branches where the first two are predicted not taken (SEQ.3) <ref> [8] </ref>. While this highlights the performance improvement of fetching non-contiguous blocks over fetching only sequential blocks, it is not an upper bound to performance. <p> As the 3-block case is the limit of performance for the trace cache modeled, the trace cache can only provide a performance benefit of around 20% for these programs. These results are more optimistic than previously-published data would suggest <ref> [8] </ref>, though no previous study has shown the true 3-block fetch limit. As can be seen from the data on the right side of Table 3, there is potential for significant improvement in trace cache performance when branch prediction is perfectoften 60% or more improvements in IPC.
Reference: [9] <author> Eric Rotenberg, Steve Bennett and James E. Smith. </author> <title> Trace Cache: A Low Latency Approach to High Bandwidth Instruction Fetching. </title> <booktitle> Proceedings of the 29th Intl. Symp. on Microarchitecture, </booktitle> <pages> pp. 24-34, </pages> <month> Dec, </month> <year> 1996. </year> <month> 15 </month>
Reference: [10] <author> Sanjay Jeram Patel, Daniel Holmes Friendly and Yale N. Patt. </author> <title> Critical Issues Regarding the Trace Cache Fetch Mechanism. </title> <institution> Computer Science and Engineering, University of Michigan Tech. </institution> <type> Report. </type> <month> May, </month> <year> 1997. </year>
Reference-contexts: basic blocks from the dynamic instruction stream into a single contiguous cache memory location [6][8][9]<ref> [10] </ref>[11][12]. The trace cache is compared to several of the previous proposals in [8]. A diagram of the trace cache fetch mechanism is shown in Figure 1. The branch predictor is either a multiple branch predictor [10] or a trace predictor [12]. The fill unit collects basic blocks and builds traces for storage in the trace cache. It merges several basic blocks into a single trace whereas earlier fill units stopped at the first branch instruction.
Reference: [11] <author> Daniel Holmes Friendly, Sanjay Jeram Patel and Yale N. Patt. </author> <title> Alternative Fetch and Issue Policies for the Trace Cache Fetch Mechanism. </title> <booktitle> Proc. 30th Intl. Symp. Microarchitecture, </booktitle> <pages> pp. 24-33, </pages> <month> Dec, </month> <year> 1997. </year>
Reference: [12] <author> Eric Rotenberg, Quinn Jacobson, Yiannakis Sazeides and Jim Smith. </author> <title> Trace Processors. </title> <booktitle> Proc. 30th Intl. Symp. Microarchitecture, </booktitle> <pages> pp. 138-148, </pages> <month> Dec, </month> <year> 1997. </year>
Reference-contexts: instruction stream into a single contiguous cache memory location [6][8][9][10][11]<ref> [12] </ref>. The trace cache is compared to several of the previous proposals in [8]. A diagram of the trace cache fetch mechanism is shown in Figure 1. The branch predictor is either a multiple branch predictor [10] or a trace predictor [12]. The fill unit collects basic blocks and builds traces for storage in the trace cache. It merges several basic blocks into a single trace whereas earlier fill units stopped at the first branch instruction. <p> It is essentially wasted storage. Fragmentation is related directly to the trace selection policy. More conservative trace selection results in shorter traces, and thus higher fragmentation. Rotenberg <ref> [12] </ref> showed that average trace length was reduced by about 20 percent when backward branch and call instructions were added to the trace termination conditions. During a particular clock cycle, fragmentation is the ratio of empty instruction slots to total instruction slots. <p> Trading off Fragmentation, Duplication, and IPC There is a fundamental trade-off to be made between the performance metrics introduced above. The constrained trace selection policy mentioned in Section 3.2 will serve as a good example. It was noted in <ref> [12] </ref> that the average trace length is reduced by conservative trace selection, that is, adding trace termination conditions. While shorter traces mean that fragmentation will increase, our simulation results show that duplication decreases correspondingly. <p> The low efficiency is primarily caused by code duplication. When associativity is increased, the efficiency gain possible because of decreased fragmentation is outweighed by increased duplication. The overall performance of the trace cache is determined by the hit rate and the length of the trace lines referenced. Experiments in <ref> [12] </ref> indicate that trace cache hit rates range from 60%- 90%, and our experiments confirm this trend. However, previous experiments required that the address in the fetch request must be located in the first entry in some trace line.
Reference: [13] <institution> Personal communication from Eric Rotenberg, University of Wisconsin, Madison, </institution> <month> February 26, </month> <year> 1998. </year>
Reference-contexts: In such cases, duplication is due to the multiple inclusion of fork and join points in the control flow graph. This is illustrated in Figure 2 (A). Duplication of this type is called fanout redundancy <ref> [13] </ref>. Duplication may also occur because of a loop whose length is not an integer multiple of the maximum trace cache line size. This case is illustrated in Figure 2 (B). Duplication of this kind is called shift redundancy [13]. <p> Duplication of this type is called fanout redundancy <ref> [13] </ref>. Duplication may also occur because of a loop whose length is not an integer multiple of the maximum trace cache line size. This case is illustrated in Figure 2 (B). Duplication of this kind is called shift redundancy [13]. If N is the number of instructions that can fit in the trace cache line, a loop of L instructions will result in N/GCD (L, N) trace lines. In the case where a loop has one more instruction than the trace cache line can hold (i.e.
Reference: [14] <author> Douglas C. Burger and Todd M. Austin. </author> <title> The SimpleScalar Tool Set, </title> <type> Version 2.0. </type> <institution> University of Wisconsin, Madison Tech. </institution> <type> Report. </type> <month> June, </month> <year> 1997. </year>
Reference-contexts: Retirement rate is a function of branch prediction accuracy, pipeline depth (or branch resolution time) and issue width. Retirement rate will be considered in Section 5.2. 4. Simulation Environment Simulation results were obtained with a modified version of the sim-outorder simulator from the SimpleScalar tools <ref> [14] </ref>. For all experiments, the SPEC95 integer benchmarks were run on the input sets listed in Table 1. The benchmark binaries provided in the SimpleScalar distribution are used in these experiments. The parameters common across all configurations simulated are shown in Table 2.
Reference: [15] <author> Sanjay J. Patel, Marius Evers and Yale N. Patt. </author> <title> Improving Trace Cache Effectiveness with Branch Promotion and Trace Packing. </title> <booktitle> Proc. 25th Intl. Symp. Computer Architecture, </booktitle> <pages> pp. 262-271, </pages> <month> June, </month> <year> 1998. </year>
Reference-contexts: The trace cache does eliminate the need for a multi-ported cache structure but may instead require a multi-ported branch prediction structure or a single-ported structure with a complex selection mechanism (see <ref> [15] </ref>, for example). Instead of trying to fetch past multiple branches, we think an interesting avenue of related research would be to de-emphasize branch prediction and find other means to increase performance. Compiler optimizations should help.
Reference: [16] <author> Eric Hao, Po-Yung Chang and Yale N. Patt. </author> <title> The Effect of Speculatively Updating Branch History on Branch Prediction Accuracy, Revisited. </title> <booktitle> Proc. 27th Intl. Symp. Microarchitecture, </booktitle> <pages> pp. 228-232, </pages> <month> Nov, </month> <year> 1994. </year>
Reference-contexts: The branch predictor uses speculative history information. All wrong-path history bits are squashed once a mis-prediction has been identified. This is done because neither speculative update nor non-speculative update alone provide reasonable performance <ref> [16] </ref>. The reason that speculative and non-speculative branch history update policies fail is that trace cache processing enables considerable speculation, resulting in non-speculative history that is too old, or speculative history that contains too many history bits from the wrong path.
References-found: 16

