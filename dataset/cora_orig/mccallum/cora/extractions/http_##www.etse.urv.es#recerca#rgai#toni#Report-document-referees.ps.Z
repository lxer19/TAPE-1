URL: http://www.etse.urv.es/recerca/rgai/toni/Report-document-referees.ps.Z
Refering-URL: http://www.etse.urv.es/recerca/rgai/toni/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: amoreno@etse.urv.es  
Title: Modelling rational inquiry in non-ideal agents  
Author: Antonio Moreno Escola Tecnica Superior d'Enginyeria 
Keyword: rational inquiry, doxastic logics, logical omniscience, perfect reasoning, possible worlds, Kripke semantics, dynamic multi-dimensional belief analysis  
Address: Salou, s/n. 43006-Tarragona  
Affiliation: Departament d'Enginyeria Informatica Universitat Rovira i Virgili (URV) Carretera de  
Abstract: The construction of rational agents is one of the goals that has been pursued in Artificial Intelligence (AI). In most of the architectures that have been proposed for this kind of agents, its behaviour is guided by its set of beliefs. In our work, rational agents are those systems that are permanently engaged in the process of rational inquiry; thus, their beliefs keep evolving in time, as a consequence of their internal inference procedures and their interaction with the environment. Both AI researchers and philosophers are interested in having a formal model of this process, and this is the main topic in our work. Beliefs have been formally modelled in the last decades using doxastic logics. The possible worlds model and its associated Kripke semantics provide an intuitive semantics for these logics, but they seem to commit us to model agents that are logically omniscient and perfect reasoners. We avoid these problems by replacing possible worlds by conceivable situations, which are all the situations that the modelled agent is capable of considering. In this document we show how this notion of conceivable situations may be used to model the process of rational inquiry in which a non-ideal rational agent is engaged. We define a wide class of agents, called rational inquirers, which are a general abstraction of any kind of non-ideal agent. We show how the beliefs of these kind of agents evolve in time as a consequence of a multi-dimensional belief analysis, and we use the framework of conceivable situations in order to model this evolution. 
Abstract-found: 1
Intro-found: 1
Reference: [AGM85] <author> Alchourron, C., Gardenfors, P., Makinson, D., </author> <title> "On the logic of theory change: partial meet functions for contraction and revision", </title> <journal> Journal of Symbolic Logic 50, </journal> <pages> pp. 510-530, </pages> <year> 1985. </year>
Reference-contexts: This means that this work will have connections with the AI field of belief revision, which has received much attention in recent years (especially from the logical approach taken by Alchourron, Gardenfors and Makinson; see e.g. <ref> [AGM85] </ref>, [Gard88]). 3.7 Limited belief analysis There are several aspects of the belief analysis that limit the reasoning capabilities of rational inquirers, and prevent them from being ideal agents: * Tableaux are not closed automatically in the logical analysis.
Reference: [AlNe84] <author> Almukdad, A., Nelson, D., </author> <title> "Constructive falsity and inexact predicates", </title> <journal> Journal of Symbolic Logic 49, </journal> <pages> pp. 231-233, </pages> <year> 1984. </year>
Reference: [AnBe75] <author> Anderson, A., Belnap, N., </author> <title> "Entailment: the logic of relevance and necessity", </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1975. </year>
Reference: [Appe85] <author> Appelt, D., </author> <title> "Planning English sentences", </title> <booktitle> Studies in Natural Language Processing, </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: Appelt describes a similar approach in <ref> [Appe85] </ref>, where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, [Moor83], [Moor85]).
Reference: [BaPe83] <author> Barwise, J., Perry, J., </author> <title> "Situations and attitudes", </title> <publisher> Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: Thus, it may consider the class of cosis represented by the tableau as 41 Although the idea of "successful belief" in <ref> [BaPe83] </ref> is of a probabilistic nature. 42 This information, as opposed to the one received in the experimental dimension, has not been requested by the agent; it has just been received, e.g. from sensors or other agents.
Reference: [Barw88] <editor> Barwise, J., </editor> <booktitle> "The situation in logic", CSLI Lecture Notes, </booktitle> <volume> No. 17, </volume> <year> 1988. </year>
Reference-contexts: The agent, in the course of an inferential process performed on its beliefs, could reach the conclusion that q is clearly unacceptable (e.g. it contradicts a large set of other actual beliefs). 14 In some respects they will be similar to Barwise and Perry's situations ([BaPe83], <ref> [Barw88] </ref>), because they will not be required to describe every aspect of the world, but maybe just a small portion of it.
Reference: [BeMa77] <author> Bell, J., Machover, M., </author> <title> "A course in Mathematical Logic", </title> <publisher> North Holland, </publisher> <year> 1977. </year>
Reference-contexts: In fact, the possibility of adding this kind of tautologies in the analytic tableaux is a well-known idea in the tradition of classical proof theory, as described e.g. in <ref> [BeMa77] </ref>.
Reference: [Beln77] <author> Belnap, N., </author> <title> "A useful four-valued logic", in Modern uses of multiple-valued logic, </title> <editor> Epstein,G., Dunn, J. </editor> <booktitle> (Eds.), </booktitle> <pages> pp. 5-37, </pages> <publisher> Reidel, </publisher> <year> 1977. </year>
Reference-contexts: worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in <ref> [Beln77] </ref> and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Bene97] <author> Benerecetti, M., Cimatti, A., Giunchiglia, E., Giunchiglia, F., Serafini, L., </author> <title> "Formal specification of beliefs in multi-agent systems", </title> <booktitle> in [MWJ97], </booktitle> <pages> pp. 117-130, </pages> <year> 1997. </year>
Reference-contexts: This view was clearly considered by Konolige in his Ph.D. thesis ([Kono86a]), where each agent was modelled with a base set of beliefs and a (possibly incomplete) set of inference rules (see [More97]). A similar idea is followed by Benerecetti et al. in <ref> [Bene97] </ref>, where the notion of a context (an axiomatic formal system) is used to model the reasoning capabilities of ideal and real reasoners. 11 Hintikka ([Hint75]) defines them as those worlds so subtly inconsistent that the inconsistency could not be expected to be perceived by an everyday logician, however competent.
Reference: [Bent84] <author> Van Benthem, J., </author> <title> "Correspondence theory", </title> <booktitle> in Handbook of Philosophical Logic, </booktitle> <volume> Vol. </volume> <editor> III, Eds. D. Gabbay, </editor> <booktitle> F.Guenthner, </booktitle> <pages> pp. 167-284, </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1984. </year>
Reference-contexts: The union of these problems is usually referred to in the literature as the problem of logical omniscience 9 (even 6 This relationship is studied in a branch of modal logic known as correspondence theory, see e.g.. <ref> [Bent84] </ref>. 7 A binary relation R has those properties when the following conditions hold: * Reflexive: 8x; Rxx * Transitive: 8x; y; z; (Rxy ^ Ryz) ) Rxz * Euclidean: 8x; y; z; (Rxy ^ Rxz) ) Ryz * Serial: 8x9y such that Rxy 8 However, some instances of the axiom
Reference: [Beth55] <author> Beth, E., </author> <title> "Semantic entailment and formal derivability", from Mededelingen van de Koninklijke Nederlandse Akademie van Wetenschappen, </title> <journal> Afdeling Letterkunde, </journal> <volume> Vol. 18, no. 13, </volume> <pages> pp. 309-342, </pages> <year> 1955. </year>
Reference-contexts: as it were) but indirectly, through their involvement in a reinforcement/disabling process which eliminates conceivable situations (that are then seen as "impossible") and so reinforces -as belief, now turned into knowledge- what active experience has indirectly but forcefully shown. 39 Hintikka uses the classical Beth-style analytic tableaux, as described in <ref> [Beth55] </ref>. 40 Makes the following question in the experimental dimension of analysis, to be described later: is there any individual a such that Bird (a) and not F lies (a)? Modelling rational inquiry in non-ideal agents A.Moreno 39 This analysis not only matches some dynamic models of concept formation in Psychology
Reference: [BIP88] <author> Bratman, M., Israel, D., Pollack, M., </author> <title> "Plans and resource-bounded practical reasoning", </title> <booktitle> Computational Intelligence 4, </booktitle> <pages> pp. 349-355, </pages> <year> 1988. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], <ref> [BIP88] </ref>, [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [Brat87] <author> Bratman, M., </author> <title> "Intentions, plans and practical reason", </title> <publisher> Harvard Univ. Press, </publisher> <year> 1987. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in <ref> [Brat87] </ref>, [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [Broo90] <author> Brooks, R., </author> <title> "Elephants don't play chess", </title> <booktitle> Robotics and Autonomous Systems 6, </booktitle> <pages> pp. 3-15, </pages> <year> 1990. </year>
Reference: [Broo91] <author> Brooks, R., </author> <title> "Intelligence without representation", </title> <booktitle> Artificial Intelligence 47, </booktitle> <pages> pp. 139-159, </pages> <year> 1991. </year>
Reference-contexts: These modal operators can be interpreted in a 1 See [WoJe95] or [Mull97] for extensive reviews of different approaches, including reactive architectures ([Broo90], <ref> [Broo91] </ref>), that do not adhere to the more classical deliberative agent conception. 2 These components will find their counterparts in the multi-dimensional belief analysis performed by our rational inquirers, as will be shown later. 3 The existential modal operator is defined in the following way: 3A :2:A.
Reference: [Cher86] <author> Cherniak, C., </author> <title> "Minimal rationality", </title> <publisher> Bradford Books, MIT Press, </publisher> <year> 1986. </year>
Reference: [CRW97] <author> Cavedon, L., Rao, A., Wobcke, W. (Eds.), </author> <title> "Intelligent Agent Systems: </title> <booktitle> Theoretical and Practical Issues", Lecture Notes in Artificial Intelligence 1209, Springer Verlag 1997. Modelling rational inquiry in non-ideal agents A.Moreno 47 </booktitle>
Reference: [CoLe90] <author> Cohen, P., Levesque, H., </author> <title> "Intention is choice with commitment", </title> <booktitle> Artificial Intelligence 42, </booktitle> <pages> pp. 213-261, </pages> <year> 1990. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], <ref> [CoLe90] </ref>, [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. <ref> [CoLe90] </ref>, [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [Cres72] <author> Cresswell, M., </author> <title> "Intensional logics and logical truth", </title> <journal> Journal of Philosophical Logic 1, </journal> <pages> pp. 2-15, </pages> <year> 1972. </year>
Reference-contexts: literature to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in <ref> [Cres72] </ref> and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Cres73] <author> Cresswell, M., </author> <title> "Logics and languages", </title> <publisher> Methuen, </publisher> <year> 1973. </year>
Reference-contexts: consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and <ref> [Cres73] </ref>, setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [CuPo90] <editor> Cummings, Pollock, J. (Eds.), </editor> <booktitle> "Philosophy and AI: essays at the interface", </booktitle> <year> 1990. </year>
Reference: [DBB95] <author> Da Costa, N., Beziau, J., Bueno, O., </author> <title> "Aspects of paraconsistent logic", </title> <journal> Bulletin of the IGPL, </journal> <volume> Vol. 3, No. 4, </volume> <pages> pp. 597-614, </pages> <year> 1995. </year> <note> (http://www.mpi-sb.mpg.de/igpl/Journal/V3-4/#Dacosta) </note>
Reference-contexts: Rescher and Brandom in [ReBr79]; many inconsistency-tolerant logics have been proposed in the literature ([Wagn97]), such as Belnap's four-valued logic ([Beln77]), Priest's nonmonotonic logic of minimal inconsistency ([Prie89]) or several paraconsistent logics ([AlNe84], <ref> [DBB95] </ref>)).
Reference: [Delg95] <author> Delgrande, J., </author> <title> "A framework for logics of explicit belief", </title> <booktitle> Computational Intelligence 11, </booktitle> <pages> pp. 47-86, </pages> <year> 1995. </year>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], <ref> [Delg95] </ref>). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]). Other approaches ([FHV90], [FaHa85], [FHMV95]) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation. <p> subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in [Shoh91]; Fagin and Halpern modelled this situation in their logic of local reasoning, [FaHa85]; Delgrande also considered this idea in <ref> [Delg95] </ref>). * It is indeed possible to define arguably interesting procedures of inquiry over inconsistent belief sets (as shown e.g. in [ReBr79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [Hofs80]) proved so many times). * Some theories have been considered acceptable
Reference: [Denn78] <author> Dennett, D., "Brainstorms", Bradford Books, </author> <year> 1978. </year>
Reference-contexts: agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in <ref> [Denn78] </ref> and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [Denn84] <author> Dennett, D., </author> <title> "Elbow room", </title> <publisher> Oxford University Press, </publisher> <year> 1984. </year>
Reference-contexts: to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and <ref> [Denn84] </ref>, the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [Devl91] <author> Devlin, K., </author> <title> "Logic and information", </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: The name can be a little bit misleading, though, due to the inherent consistency of situations in that framework (incoherent abstract situations being the exception, as defined by Devlin in <ref> [Devl91] </ref>).
Reference: [Elgo88] <author> Elgot-Drapkin, J., "Step-logic: </author> <title> reasoning situated in time", </title> <type> Ph.D. thesis, </type> <institution> University of Maryland, </institution> <year> 1988. </year>
Reference-contexts: Nirkhe et al. (see [NKP94]) show how step-logics ([EMP90], <ref> [Elgo88] </ref>, [ElPe90]) may be used as a way to model the agent's ongoing process of reasoning; they even take into account the actual time that the agent consumes in its reasoning processes.
Reference: [ElPe90] <author> Elgot-Drapkin, J., Perlis, D., </author> <title> "Reasoning situated in time I: Basic concepts", </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2, </journal> <pages> pp. 75-98, </pages> <year> 1990. </year>
Reference-contexts: Nirkhe et al. (see [NKP94]) show how step-logics ([EMP90], [Elgo88], <ref> [ElPe90] </ref>) may be used as a way to model the agent's ongoing process of reasoning; they even take into account the actual time that the agent consumes in its reasoning processes.
Reference: [EMP90] <author> Elgot-Drapkin, J., Miller, M., Perlis, D., </author> <title> "Memory, reason, and time: the step-logic approach", </title> <booktitle> in [CuPo90], </booktitle> <pages> pp. 79-103, </pages> <year> 1990. </year>
Reference: [FaHa85] <author> Fagin, R., Halpern, J., </author> <title> "Belief, awareness and limited reasoning", Procs. </title> <booktitle> of the Ninth International Joint Conference on Artificial Intelligence, IJCAI-85, </booktitle> <pages> pp. 491-501, </pages> <year> 1985. </year>
Reference-contexts: Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]). Other approaches ([FHV90], <ref> [FaHa85] </ref>, [FHMV95]) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation. <p> The partiality or incompleteness of possible worlds has been traditionally accepted in the AI literature, the most common justifications being the following: * The agent can be unaware of certain facts (Fagin and Halpern tried to take this fact into account in their logic of general awareness, <ref> [FaHa85] </ref>, see [More97]). * The agent can have limited resources (e.g. the time required or the space needed to perform a given inference). <p> every inference; if it focuses in a subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in [Shoh91]; Fagin and Halpern modelled this situation in their logic of local reasoning, <ref> [FaHa85] </ref>; Delgrande also considered this idea in [Delg95]). * It is indeed possible to define arguably interesting procedures of inquiry over inconsistent belief sets (as shown e.g. in [ReBr79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [Hofs80]) proved so many times).
Reference: [FHV90] <author> Fagin, R., Halpern, J., Vardi, M., </author> <title> "A non-standard approach to the logical omniscience problem", </title> <institution> Research report RJ7234, IBM Research Division, Almaden Research Center, </institution> <month> December </month> <year> 1990. </year>
Reference: [FHV95] <author> Fagin, R., Halpern, J., Vardi, M., </author> <title> "A non-standard approach to the logical omniscience problem", </title> <booktitle> Artificial Intelligence 79, </booktitle> <pages> pp. 203-240, </pages> <year> 1995. </year>
Reference: [FHMV95] <author> Fagin, R., Halpern, J., Moses, Y., Vardi, M., </author> <title> "Reasoning about knowledge", </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]). Other approaches ([FHV90], [FaHa85], <ref> [FHMV95] </ref>) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation. <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and <ref> [FHMV95] </ref>. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c]. <p> Appelt describes a similar approach in [Appe85], where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, [Moor83], [Moor85]). Fagin et al. show in <ref> [FHMV95] </ref> how the Modelling rational inquiry in non-ideal agents A.Moreno 14 reasoning processes followed by the muddy children in order to answer the question posed to them may be modelled with a progressive restriction in the accessibility relation between the states that they consider possible.
Reference: [Fitt83] <author> Fitting, M., </author> <title> "Proof methods for modal and intuitionistic logics", </title> <address> D. </address> <publisher> Reidel Publishing Co., </publisher> <year> 1983. </year>
Reference-contexts: ) = 0 * J ( _ ) = !; otherwise * J ( ^ ) = 0; if I () = 0 or I ( ) = 0 * J ( ^ ) = !; otherwise 33 This notation was developed in [Smul68], and has been used e.g. in <ref> [Fitt83] </ref>, [Fitt96]. It allows us to write in a compact way many similar rules.
Reference: [Fitt96] <author> Fitting, M., </author> <title> "First-order logic and automated theorem proving", </title> <publisher> Springer Ver-lag, </publisher> <year> 1996. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 48 </title>
Reference-contexts: = 0 * J ( _ ) = !; otherwise * J ( ^ ) = 0; if I () = 0 or I ( ) = 0 * J ( ^ ) = !; otherwise 33 This notation was developed in [Smul68], and has been used e.g. in [Fitt83], <ref> [Fitt96] </ref>. It allows us to write in a compact way many similar rules.
Reference: [Fuhr91] <author> Furhmann, A., </author> <title> "Theory contraction through base contraction", </title> <journal> Journal of Philosophical Logic 20, </journal> <pages> pp. 175-203, </pages> <year> 1991. </year>
Reference: [GaHu91] <author> Gabbay, D., Hunter, A., </author> <title> "Making inconsistency respectable: a logical framework for inconsistency in reasoning", </title> <booktitle> Proccedings of the International Workshop on Fundamentals of Artificial Intelligence Research, </booktitle> <address> FAIR-91, </address> <year> 1991. </year>
Reference-contexts: that it represents (note that an open tableau can contain an inconsistent set of formulas, e.g. if the logical development of the tableau has not (yet) shown the contradiction that may be hidden in the set, waiting for further analysis to appear). * As Gabbay and Hunter point out in <ref> [GaHu91] </ref>, inconsistency in information is the norm (e.g. most systems have databases or knowledge bases where information may be obtained from different sources, or from not fully reliable sources, as commented in [Roos92]), and we should find ways to formalize it appropriately.
Reference: [Gard88] <author> Gardenfors, P., </author> <title> "Knowledge in flux", </title> <publisher> Cambridge University Press, </publisher> <year> 1988 </year>
Reference-contexts: This means that this work will have connections with the AI field of belief revision, which has received much attention in recent years (especially from the logical approach taken by Alchourron, Gardenfors and Makinson; see e.g. [AGM85], <ref> [Gard88] </ref>). 3.7 Limited belief analysis There are several aspects of the belief analysis that limit the reasoning capabilities of rational inquirers, and prevent them from being ideal agents: * Tableaux are not closed automatically in the logical analysis.
Reference: [GeIn89] <author> Georgeff, M., Ingrand, F., </author> <title> "Decision-making in an embedded reasoning system", </title> <booktitle> Proceedings of the Eleventh Joint Conference on Artificial Intelligence, IJCAI-89, </booktitle> <year> 1989. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in <ref> [GeIn89] </ref>, the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [HaLe96] <author> Halpern, J., Lakemeyer, G., </author> <title> "Multi-agent only knowing", </title> <booktitle> in Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, TARK-96, </booktitle> <pages> pp. 251-265, </pages> <year> 1996. </year>
Reference: [Halp86] <author> Halpern, J., </author> <title> "Reasoning about knowledge: an overview", </title> <booktitle> Proceedings of the First Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <editor> TARK-86, Ed. J. </editor> <booktitle> Halpern, </booktitle> <pages> pp. 1-17, </pages> <year> 1986. </year>
Reference: [Halp93] <author> Halpern, J., </author> <title> "Reasoning about only knowing with many agents", </title> <booktitle> in Proceedings of the Conference of the American Association for Artificial Intelligence, AAAI-93, </booktitle> <pages> pp. 655-661, </pages> <year> 1993. </year>
Reference: [HaMo92] <author> Halpern, J., Moses Y., </author> <title> "A guide to completeness and complexity for modal logics of knowledge and belief", </title> <booktitle> Artificial Intelligence 54, </booktitle> <pages> pp. 319-379, </pages> <year> 1992. </year>
Reference-contexts: both in AI and in Philosophy is how to build a formal model of rational inquiry; this is the aim of our work. 1.2 Formal models of belief 1.2.1 Doxastic logics The natural start point in our work was to consider logics of knowledge and belief (epistemic and doxastic logics, <ref> [HaMo92] </ref>). These modal logics are used to analyse in a formal way the reasoning about knowledge or belief performed by an agent.
Reference: [Hint62] <author> Hintikka, J., </author> <title> "Knowledge and belief", </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, N.Y., </address> <year> 1962. </year>
Reference: [Hint73] <author> Hintikka, J., </author> <title> "Logic, language-games and information", </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1973. </year>
Reference: [Hint75] <author> Hintikka, J., </author> <title> "Impossible possible worlds vindicated", </title> <journal> Journal of Philosophical Logic 4, </journal> <pages> pp. 475-484, </pages> <year> 1975. </year>
Reference-contexts: A particularly interesting suggestion was made by Hintikka in <ref> [Hint75] </ref>, where he proposed the idea of considering [logically] impossible [epistemically] possible worlds 11 ; he seems not to have pursued this idea, though. This is the path followed in our work, as will be apparent in the rest of this document. <p> repeteadly proposed in the literature to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in <ref> [Hint75] </ref>, non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Hint81] <author> Hintikka, J., </author> <title> "On the logic of an interrogative model of scientific inquiry", </title> <type> Synthese 47, </type> <pages> pp. 69-83, </pages> <year> 1981. </year>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 38 also been suggested by Hintikka in his interrogative model of inquiry (see e.g. <ref> [Hint81] </ref>, [Hint86], [Hint87], [Hint88], [Hint92]), in which he models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature.
Reference: [Hint86] <author> Hintikka, J., </author> <title> "Reasoning about knowledge in philosophy: the paradigm of epis-temic logic", </title> <booktitle> Proceedings of the First Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <editor> TARK-86, Ed. </editor> <booktitle> J.Y.Halpern, </booktitle> <pages> pp. 63-80, </pages> <year> 1986. </year>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 38 also been suggested by Hintikka in his interrogative model of inquiry (see e.g. [Hint81], <ref> [Hint86] </ref>, [Hint87], [Hint88], [Hint92]), in which he models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature. <p> This is an example of how the agent may combine different dimensions of analysis in order to refine its set of beliefs. Thus, as Hintikka suggested in <ref> [Hint86] </ref>, doubt can be understood as the beginning of a dynamic process that, by reducing the number of conceivable situations that the agent considers, reinforces one side of the doubt over the other, sets the conditions to verify it (and falsify the other) and tendentially gives credence to it.
Reference: [Hint87] <author> Hintikka, J., </author> <title> "Knowledge representation and the interrogative model of inquiry", </title> <booktitle> International Philosophy Congress, </booktitle> <pages> pp. 1077-1084, </pages> <year> 1987. </year>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 38 also been suggested by Hintikka in his interrogative model of inquiry (see e.g. [Hint81], [Hint86], <ref> [Hint87] </ref>, [Hint88], [Hint92]), in which he models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature.
Reference: [Hint88] <author> Hintikka, J., </author> <title> "What is the logic of experimental inquiry?", </title> <type> Synthese 74, </type> <pages> pp. 173-190, </pages> <year> 1988. </year>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 38 also been suggested by Hintikka in his interrogative model of inquiry (see e.g. [Hint81], [Hint86], [Hint87], <ref> [Hint88] </ref>, [Hint92]), in which he models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature. <p> Thus, this dimension models the agent's acquisition of data in the actual world (obtained via experiences, tests, etc.). The root of this dimension can be traced (as Hintikka points out in <ref> [Hint88] </ref>) as far as Kant, who argued in his Critique of pure reason that Reason has to take into account observations of the environment; he thinks that Reason must not approach Nature as a student, that takes everything that its teacher chooses to say, but as a judge who formulates questions <p> as linear, intuitionistic or relevance logic. * Experimental analysis Which type of experiences will be allowed in the experimental dimension of analysis? How does the restriction on the allowed answers from the environment change the potential results of the process of inquiry? Those facts are important, as Hintikka notes in <ref> [Hint88] </ref> and [Hint92]. There is a whole hierarchy of possibilities: we could allow only atomic questions with boolean answers, or we could permit the agent to make any question and receive any answer.
Reference: [Hint92] <author> Hintikka, J., </author> <title> "The interrogative model of inquiry as a general theory of argumentation", </title> <journal> Communication and Cognition, </journal> <volume> Vol. 25, </volume> <pages> Nos. 2-3, pp. 221-242, </pages> <year> 1992. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 49 </title>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 38 also been suggested by Hintikka in his interrogative model of inquiry (see e.g. [Hint81], [Hint86], [Hint87], [Hint88], <ref> [Hint92] </ref>), in which he models the process of scientific inquiry with plays of the interrogative game, which is a game played by the Inquirer and Nature. <p> intuitionistic or relevance logic. * Experimental analysis Which type of experiences will be allowed in the experimental dimension of analysis? How does the restriction on the allowed answers from the environment change the potential results of the process of inquiry? Those facts are important, as Hintikka notes in [Hint88] and <ref> [Hint92] </ref>. There is a whole hierarchy of possibilities: we could allow only atomic questions with boolean answers, or we could permit the agent to make any question and receive any answer.
Reference: [HoMe88] <author> Van der Hoek, W., Meyer, J.-J., </author> <title> "Possible logics for belief", </title> <institution> Rapport IR-170, Vrije Universiteit Amsterdam, </institution> <year> 1988. </year>
Reference: [Hofs80] <author> Hofstadter, D., </author> <title> "Godel, Escher, Bach: an Eternal Golden Braid", </title> <publisher> Basic Books Inc. Publishers, </publisher> <year> 1980. </year>
Reference-contexts: logic of local reasoning, [FaHa85]; Delgrande also considered this idea in [Delg95]). * It is indeed possible to define arguably interesting procedures of inquiry over inconsistent belief sets (as shown e.g. in [ReBr79]). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. <ref> [Hofs80] </ref>) proved so many times). * Some theories have been considered acceptable for a certain period of time (e.g. Frege's set theory) for many people, but they have been proved to be inconsistent later (as Russell did with Frege's theory).
Reference: [HMP96] <editor> Huang, Z., Masuch, M., Polos, L., "ALX, </editor> <title> an action logic for agents with bounded rationality", </title> <booktitle> Artificial Intelligence 82, </booktitle> <pages> pp. 75-127, </pages> <year> 1996. </year>
Reference-contexts: ) Rxz * Euclidean: 8x; y; z; (Rxy ^ Rxz) ) Ryz * Serial: 8x9y such that Rxy 8 However, some instances of the axiom schema A1 are not standard propositional tautologies, e.g. (B i ' _ :B i '). 9 It is worth pointing out a comment made in <ref> [HMP96] </ref>: omniscience is relative to the chosen language. Full rationality in an absolute sense would require a language isomorphic to the "real world", but such a language is not available. Thus, language fashions and sets limits to the agents' way of seeing things.
Reference: [HuCr68] <author> Hughes, G.E., Cresswell, M.J., </author> <title> "An introduction to modal logic", </title> <publisher> Methuen and Co. Ltd. </publisher> <editor> Eds., </editor> <year> 1968. </year>
Reference-contexts: Modelling rational inquiry in non-ideal agents A.Moreno 7 System KT is defined as system K plus the axiom of knowledge. If the axiom of positive introspection is added to system T , system KT 4 (also known as S4, see <ref> [HuCr68] </ref>) is obtained; in turn S4 can be transformed into S5 (KT 45) by adding the axiom of negative introspection. The system weak S5 (K45) contains the axioms of introspection but does not contain the axiom of knowledge. Adding the axiom of consistency to K45, the system KD45 is obtained.
Reference: [Jasp94] <author> Jaspars, J., </author> <title> "Calculi for Constructive Communication", Institute for Logic, Language and Computation, </title> <type> ILLC Dissertation Series 1994-4, </type> <year> 1994. </year>
Reference: [KiGe97] <author> Kinny, D., Georgeff, M., </author> <booktitle> "Modelling and design of multi-agent systems", in [MWJ97], </booktitle> <pages> pp. 1-20, </pages> <year> 1997. </year>
Reference-contexts: last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], <ref> [KiGe97] </ref> or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [Klee52] <author> Kleene, S., </author> <title> "Introduction to metamathematics", </title> <publisher> North Holland, </publisher> <year> 1952. </year>
Reference: [Kono85] <author> Konolige, K., </author> <title> "Belief and incompleteness", </title> <booktitle> Formal theories of the commonsense world, </booktitle> <pages> pp. 359-403, </pages> <editor> Ed. Hobbs, Moore, </editor> <year> 1985. </year>
Reference: [Kono86a] <author> Konolige, K., </author> <title> "A Deduction Model of Belief", </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: The main idea would be to represent the beliefs of the agent with an axiomatic system (a set of core axioms plus a set of inference rules), in a Konolige's deduction model of belief fashion ([Kono85], <ref> [Kono86a] </ref>). The agent's beliefs would then be all those propositions that can be obtained from the set of axioms with the application of the inference rules. 43 This fact has connections with some of Rescher's observations on coherence ([Resc73]).
Reference: [Kono86b] <author> Konolige, K., </author> <title> "What awareness isn't: a sentential view of implicit and explicit belief", </title> <booktitle> Proceedings of the First Conference on Theoretical Aspects of Reasoning about Knowledge, TARK-86, </booktitle> <pages> pp. 241-250, </pages> <year> 1986. </year>
Reference: [KrSu95] <author> Kraus, S., Subrahmanian, V., </author> <title> "Multiagent reasoning with probability, time, and beliefs", </title> <journal> International Journal of Intelligent Systems 10, </journal> <pages> pp. 459-499, </pages> <year> 1995. </year>
Reference-contexts: Nirkhe et al. (see [NKP94]) show how step-logics ([EMP90], [Elgo88], [ElPe90]) may be used as a way to model the agent's ongoing process of reasoning; they even take into account the actual time that the agent consumes in its reasoning processes. Kraus and Subrahmanian develop in <ref> [KrSu95] </ref> a family of temporal logics in which belief update is captured by how the agent's beliefs about the present are changing over time.
Reference: [Krip63] <author> Kripke, S., </author> <title> "A semantical analysis of modal logic I: normal modal propositional calculi", </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen Mathematik 9, </journal> <pages> pp. 67-96, </pages> <year> 1963. </year>
Reference: [Lake87] <author> Lakemeyer, G., </author> <title> "Tractable meta-reasoning in propositional logics of belief", </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artifical Intelligence, IJCAI-87, </booktitle> <pages> pp. 402-408, </pages> <year> 1987. </year>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. <ref> [Lake87] </ref>, [LaLe88], [Lake91a], [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]).
Reference: [LaLe88] <author> Lakemeyer, G., Levesque, H., </author> <title> "A tractable knowledge representation service with full introspection", </title> <booktitle> Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge, TARK-88, </booktitle> <pages> pp. 145-159, </pages> <year> 1988. </year>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], <ref> [LaLe88] </ref>, [Lake91a], [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]).
Reference: [Lake91a] <author> Lakemeyer, G., </author> <title> "On the relation between explicit and implicit belief", </title> <booktitle> Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, KRR-91, </booktitle> <pages> pp. 368-375, </pages> <year> 1991. </year>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], <ref> [Lake91a] </ref>, [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]).
Reference: [Lake91b] <author> Lakemeyer, G., </author> <title> "A computationally attractive first-order logic of belief", </title> <booktitle> in JELIA-90, Proceedings of the European Workshop on Logics in AI, Lecture Notes in Artificial Intelligence 478, </booktitle> <pages> pp. 333-347, </pages> <publisher> Springer Verlag, </publisher> <year> 1991. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 50 </title>
Reference: [Lake93] <author> Lakemeyer, G., </author> <title> "All they know: a study in multi-agent epistemic reasoning", </title> <booktitle> in Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, IJCAI-93, </booktitle> <pages> pp. 376-381, </pages> <year> 1993. </year>
Reference: [Lake94] <author> Lakemeyer, G., </author> <title> "Limited reasoning in first-order knowledge bases", </title> <booktitle> Artificial Intelligence 71, </booktitle> <pages> pp. 213-255, </pages> <year> 1994. </year> <note> (http://www.informatik.uni-bonn.de/~gerhard/papers.html) </note>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], <ref> [Lake94] </ref>, [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]). Other approaches ([FHV90], [FaHa85], [FHMV95]) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation.
Reference: [Leve84] <author> Levesque, H.J., </author> <title> "A logic of implicit and explicit belief", </title> <booktitle> Proceedings of the Conference of the American Association for Artificial Intelligence, AAAI-84, </booktitle> <pages> pp. 198-202, </pages> <year> 1984. </year>
Reference-contexts: connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in <ref> [Leve84] </ref>). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [McAr88] <author> McArthur, G., </author> <title> "Reasoning about knowledge and belief: a survey", </title> <booktitle> Computational Intelligence 4, </booktitle> <pages> pp. 223-243, </pages> <year> 1988. </year>
Reference-contexts: Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], <ref> [McAr88] </ref>, [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c]. <p> This is the path followed in our work, as will be apparent in the rest of this document. The main roots of these problems are the assumptions of completeness and consistency that underlie the possible worlds model. As McArthur notes in <ref> [McAr88] </ref>, since worlds are complete 12 the agent is forced to have beliefs about the way that everything would be in all of the accessible worlds; moreover, since worlds as consistent as well, everything that follows from the agent's beliefs must also be believed 13 .
Reference: [McPa87] <author> McPartlin, M., </author> <title> "Theories of formal epistemology", M.Sc. </title> <type> Thesis, </type> <institution> Centre for Cognitive Science, University of Edinburgh, </institution> <year> 1987. </year>
Reference-contexts: Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in <ref> [McPa87] </ref>, [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [Moli91] <author> Molin, T., "Tractatus de ratione circumscripta: </author> <title> rationality for finite agents", </title> <institution> Lund University Cognitive Studies 1, </institution> <year> 1991. </year>
Reference: [Mont70] <author> Montague, R., </author> <title> "Universal grammar", </title> <type> Theoria 36, </type> <pages> pp. 373-398, </pages> <year> 1970. </year>
Reference: [Moor83] <author> Moore, R., </author> <title> "Semantical considerations on nonmonotonic logic", </title> <booktitle> Artificial Intelligence Center Technical Note 284, SRI International, </booktitle> <address> Menlo Park, California, </address> <year> 1983. </year>
Reference-contexts: Appelt describes a similar approach in [Appe85], where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, <ref> [Moor83] </ref>, [Moor85]).
Reference: [Moor85] <author> Moore, R., </author> <title> "A formal theory of knowledge and action", </title> <booktitle> in Formal theories of the commonsense world, </booktitle> <pages> pp. 319-358, </pages> <editor> Ed. Hobbs, Moore, </editor> <year> 1985. </year>
Reference-contexts: Appelt describes a similar approach in [Appe85], where he argues that actions can generate knowledge by restricting the possible worlds that are consistent with the agents' knowledge after performance of the action (following ideas from Moore, [Moor83], <ref> [Moor85] </ref>).
Reference: [More93] <author> Moreno, A., </author> <note> "Que hacer para no saberlo todo", Working Report LSI-93-15-T, </note> <institution> Department of Software, Technical University of Catalonia (UPC), </institution> <year> 1993. </year>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: <ref> [More93] </ref>, [More95b]. * In Catalan: [More95c]. Modelling rational inquiry in non-ideal agents A.Moreno 10 2 Modelling tools 2.1 Conceivable situations We have just mentioned several proposals that have tried to solve (or, at least, partially alleviate) the problems of logical omniscience and perfect reasoning (both in AI and in Philosophy).
Reference: [More95a] <author> Moreno, A., </author> <title> "Dynamic belief analysis", </title> <type> Research Report 73, HCRC, </type> <institution> Human Communication Research Centre, University of Edinburgh, </institution> <address> Scotland, United Kingdom, </address> <year> 1995. </year> <note> (http://www.cogsci.ed.ac.uk/hcrc/publications/) </note>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: <ref> [More95a] </ref>, [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [More95b] <editor> Moreno, A., "Analisis dinamico de las creencias", </editor> <booktitle> Proceedings of the VI Conference of the Spanish Association for AI, CAEPIA-95, </booktitle> <pages> pp. 103-113, </pages> <address> Alicante, </address> <year> 1995 </year>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], <ref> [More95b] </ref>. * In Catalan: [More95c]. Modelling rational inquiry in non-ideal agents A.Moreno 10 2 Modelling tools 2.1 Conceivable situations We have just mentioned several proposals that have tried to solve (or, at least, partially alleviate) the problems of logical omniscience and perfect reasoning (both in AI and in Philosophy).
Reference: [More95c] <author> Moreno, A., "Modelitzacio de creences dinamiques", </author> <booktitle> in Butllet de l'Associacio Catalana d'Intel.ligencia Artificial 3. Presented at the Young Researchers Meeting, Artificial Intelligence Research Institute (IIIA), </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: <ref> [More95c] </ref>. Modelling rational inquiry in non-ideal agents A.Moreno 10 2 Modelling tools 2.1 Conceivable situations We have just mentioned several proposals that have tried to solve (or, at least, partially alleviate) the problems of logical omniscience and perfect reasoning (both in AI and in Philosophy).
Reference: [More96a] <author> Moreno, A., </author> <title> "Limited logical belief analysis", </title> <institution> Research Report 96-13-R, Department of Software, Technical University of Catalonia (UPC), </institution> <year> 1995. </year> <note> (http://www-lsi.upc.es/www/dept/techreps) </note>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], <ref> [More96a] </ref>, [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [More96b] <author> Moreno, A., </author> <title> "Limited logical belief analysis", </title> <booktitle> Proceedings of the Iberoamerican Conference on Artificial Intelligence, IBERAMIA-96, Cholula, Mexico, </booktitle> <pages> pp. 250-259, </pages> <year> 1996. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 51 </title>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], <ref> [More96b] </ref>, [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [More97] <author> Moreno, A., </author> <title> "How to avoid knowing it all", </title> <institution> Research Report DEI-RR-97-013, Computer Science Department, University Rovira i Virgili (URV), </institution> <year> 1997. </year>
Reference-contexts: A brief review of some of the most well known approaches to the problems of logical omniscience and perfect reasoning is contained in <ref> [More97] </ref>. It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], [Delg95]). <p> Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], [Delg95]). Wansing shows in [Wans90] how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see <ref> [More97] </ref>). Other approaches ([FHV90], [FaHa85], [FHMV95]) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation. <p> Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], <ref> [More97] </ref>. * In Spanish: [More93], [More95b]. * In Catalan: [More95c]. <p> The partiality or incompleteness of possible worlds has been traditionally accepted in the AI literature, the most common justifications being the following: * The agent can be unaware of certain facts (Fagin and Halpern tried to take this fact into account in their logic of general awareness, [FaHa85], see <ref> [More97] </ref>). * The agent can have limited resources (e.g. the time required or the space needed to perform a given inference). <p> This view was clearly considered by Konolige in his Ph.D. thesis ([Kono86a]), where each agent was modelled with a base set of beliefs and a (possibly incomplete) set of inference rules (see <ref> [More97] </ref>).
Reference: [Morl97] <author> Morley, D., </author> <title> "Semantics of BDI agents and their environment", </title> <booktitle> in [CRW97], </booktitle> <pages> pp. 119-134, </pages> <year> 1997. </year>
Reference-contexts: it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or <ref> [Morl97] </ref>, the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]).
Reference: [MoSa95] <author> Moreno, A., Sales, T., </author> <title> "Dynamic belief modelling", </title> <booktitle> Fourth International Colloquium on Cognitive Science, </booktitle> <address> Donostia, </address> <month> May </month> <year> 1995. </year> <note> An extended version is available as Research Report 95-28-R, </note> <institution> Department of Software, Technical University of Catalonia (UPC), </institution> <year> 1995. </year> <note> (http://www-lsi.upc.es/www/dept/techreps) </note>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], <ref> [MoSa95] </ref>, [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [MoSa96a] <author> Moreno, A., Sales, T., </author> <title> "Dynamic belief analysis", </title> <booktitle> Workshop on Agent Theories, Architectures and Languages, ATAL-96, at the European Conference on Artificial Intelligence, </booktitle> <address> ECAI-96, Budapest, </address> <note> August 1996 (see [MoSa97a]). </note>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], <ref> [MoSa96a] </ref>, [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [MoSa96b] <author> Moreno, A., Sales, T., </author> <title> "Limited logical belief analysis", </title> <booktitle> Workshop on Theoretical and Practical Foundations of Intelligent Agents, at the Pacific Rim International Conference on Artificial Intelligence, </booktitle> <address> PRICAI-96, Cairns, Australia, </address> <note> 1996 (see [MoSa97b]). </note>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], <ref> [MoSa96b] </ref>, [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [MoSa97a] <author> Moreno, A., Sales, T., </author> <title> "Dynamic belief analysis", </title> <booktitle> in [MWJ97], </booktitle> <pages> pp. 87-102, </pages> <year> 1997. </year>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], <ref> [MoSa97a] </ref>, [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [MoSa97b] <author> Moreno, A., Sales, T., </author> <title> "Limited logical belief analysis", </title> <booktitle> in [CRW97], </booktitle> <pages> pp. 104-118, </pages> <year> 1997. </year>
Reference-contexts: Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], <ref> [MoSa97b] </ref>, [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [Mull97] <author> Muller, J., </author> <title> "Control architectures for autonomous and interacting agents: a survey", </title> <booktitle> in [CRW97], </booktitle> <pages> pp. 1-26, </pages> <year> 1997. </year>
Reference-contexts: In propositional modal logic two unary operators (2 and 3) are added to propositional logic; they are called the universal modal operator and the existential modal operator 3 , respectively. These modal operators can be interpreted in a 1 See [WoJe95] or <ref> [Mull97] </ref> for extensive reviews of different approaches, including reactive architectures ([Broo90], [Broo91]), that do not adhere to the more classical deliberative agent conception. 2 These components will find their counterparts in the multi-dimensional belief analysis performed by our rational inquirers, as will be shown later. 3 The existential modal operator is
Reference: [MWJ97] <editor> Muller, J., Wooldridge, M., Jennings, N. (Eds.), </editor> <booktitle> "Intelligent Agents III: Agent theories, architectures and languages", Lecture Notes in Artificial Intelligence 1193, </booktitle> <publisher> Springer Verlag 1997. </publisher>
Reference: [NKP94] <author> Nirkhe, M., Kraus, S., Perlis, D., </author> <title> "Thinking takes time: a modal active-logic for reasoning in time", </title> <institution> CS-TR-3249, Computer Science Dept., University of Maryland, </institution> <year> 1994. </year> <note> (http://www.cs.umd.edu/TRs/TRumiacs-no-abs.html) </note>
Reference-contexts: Nirkhe et al. (see <ref> [NKP94] </ref>) show how step-logics ([EMP90], [Elgo88], [ElPe90]) may be used as a way to model the agent's ongoing process of reasoning; they even take into account the actual time that the agent consumes in its reasoning processes.
Reference: [Perl94] <author> Perlis, D., </author> <title> "Logic for a lifetime", </title> <institution> CS-TR-3278, Computer Science Dept., University of Maryland, </institution> <year> 1994. </year> <note> (http://www.cs.umd.edu/TRs/TRumiacs-no-abs.html) </note>
Reference-contexts: Logical omniscience and perfect reasoning are also accepted in the classical puzzles of the literature of reasoning about knowledge, such as the muddy children, the wise men or the cheating wives ([FHMV95]). As Perlis points out in <ref> [Perl94] </ref>, omniscient formalisms have the major advantage of being simpler and easier to study, and can be taken as modelling ideal reasoners against which real (human or artificial) reasoners can be measured as approximations.
Reference: [Perl97] <author> Perlis, D., </author> <title> "Sources of, and exploiting, inconsistency: </title> <note> preliminary report", Journal of Applied Non-Classical Logics 7, special double issue on Handling inconsistency in knowledge systems, 1997. (http://www.informatik.uni-leipzig.de/~gwagner/special-issue.html) </note>
Reference-contexts: Thus, inconsistency may be seen as a useful tool to direct the processes of reasoning and learning, rather than something to be avoided by any means. Perlis ([Perl94], <ref> [Perl97] </ref>) also shares this point of view; he argues that dealing with conflicting data is part and parcel of what commonsense reasoning is all about, and he states that contradictions are "our friends", that guide us to look more closely at what we are thinking, that signal us that a change
Reference: [Poll90] <author> Pollock, J., "OSCAR: </author> <title> A general theory of rationality", </title> <booktitle> in [CuPo90], </booktitle> <pages> pp. 189-213, </pages> <year> 1990. </year>
Reference-contexts: (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in <ref> [Poll90] </ref> or [Poll95]). In this document we take the expression "rational agents" to refer to those agents that, apart from complying with the previous definition, are permanently engaged in a process of rational inquiry (defined by Brandom in [ReBr79] as the [rationally controlled] transformation of belief over time).
Reference: [Poll95] <author> Pollock, J., </author> <title> "Cognitive carpentry", </title> <publisher> Bradford Books, MIT Press, </publisher> <year> 1995. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 52 </title>
Reference-contexts: the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or <ref> [Poll95] </ref>). In this document we take the expression "rational agents" to refer to those agents that, apart from complying with the previous definition, are permanently engaged in a process of rational inquiry (defined by Brandom in [ReBr79] as the [rationally controlled] transformation of belief over time).
Reference: [Popp34] <author> Popper, K., </author> <title> "The logic of scientific discovery", </title> <editor> Ed. Tecnos, </editor> <publisher> 1977 printing. </publisher>
Reference: [Prie89] <author> Priest, G., </author> <title> "Reasoning about truth", </title> <booktitle> Artificial Intelligence 39, </booktitle> <pages> pp. 231-244, </pages> <year> 1989. </year>
Reference: [RaGe91] <author> Rao, A., Georgeff, M., </author> <title> "Modelling rational agents within a BDI-architecture", </title> <booktitle> Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> KRR-91, </address> <year> 1991. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], <ref> [RaGe91] </ref>, [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], <ref> [RaGe91] </ref>, [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [RaGe93] <author> Rao, A., Georgeff, M., </author> <title> "Verification of agent-oriented situated systems: a model-theoretic approach", </title> <booktitle> AAAI Workshop on Formal Models of Rational Action, </booktitle> <pages> pp. 115-124, </pages> <year> 1993. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], <ref> [RaGe93] </ref>, [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], <ref> [RaGe93] </ref>, [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [RaGe95a] <author> Rao, A., Georgeff, M., </author> <title> "The semantics of intention maintenance for rational agents", </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI-95, </booktitle> <pages> pp. 704-710, </pages> <year> 1995. </year>
Reference-contexts: In the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], <ref> [RaGe95a] </ref>, [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], <ref> [RaGe95a] </ref>, [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [RaGe95b] <author> Rao, A., Georgeff, M., </author> <title> "BDI agents: from theory to practice", </title> <booktitle> Proceedings of the International Conference on Multi-Agent Systems, ICMAS-95, </booktitle> <year> 1995. </year>
Reference-contexts: the last decade it has been argued that, if the agent is to display a rational behaviour, it needs to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], <ref> [RaGe95b] </ref>, [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], <ref> [RaGe95b] </ref>). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [Rant75] <author> Rantala, V., </author> <title> "Urn models: a new kind of non-standard model for first-order logic", </title> <journal> Journal of Philosophical Logic, </journal> <volume> Vol. 4, </volume> <pages> pp. 455-474, </pages> <year> 1975. </year>
Reference: [Reic89] <author> Reichgelt, H., </author> <title> "Logics for reasoning about knowledge and belief", </title> <journal> Knowledge Engineering Review, </journal> <volume> Vol. 4, </volume> <pages> No.2, pp. 119-139, </pages> <year> 1989. </year>
Reference-contexts: Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], <ref> [Reic89] </ref> and [FHMV95]. The development of our ideas from the beginning of this research may be found in these reports and articles: * In English: [More95a], [MoSa95], [More96a], [More96b], [MoSa96a], [MoSa96b], [MoSa97a], [MoSa97b], [More97]. * In Spanish: [More93], [More95b]. * In Catalan: [More95c].
Reference: [ReBr79] <author> Rescher, N., Brandom, R., </author> <title> "The Logic of Inconsistency", </title> <editor> Rowman and Littlefield Eds., </editor> <year> 1979. </year>
Reference-contexts: In this document we take the expression "rational agents" to refer to those agents that, apart from complying with the previous definition, are permanently engaged in a process of rational inquiry (defined by Brandom in <ref> [ReBr79] </ref> as the [rationally controlled] transformation of belief over time). Intuitively, these agents are constantly trying to make their beliefs as similar as possible to the facts that hold in the real world. <p> Nevertheless, this possibility can be seriously entertained (some philosophers have indeed argued for the feasibility of this kind of worlds, e.g. Rescher and Brandom in <ref> [ReBr79] </ref>; many inconsistency-tolerant logics have been proposed in the literature ([Wagn97]), such as Belnap's four-valued logic ([Beln77]), Priest's nonmonotonic logic of minimal inconsistency ([Prie89]) or several paraconsistent logics ([AlNe84], [DBB95])). <p> the beliefs are considered (as remarked by Shoham in [Shoh91]; Fagin and Halpern modelled this situation in their logic of local reasoning, [FaHa85]; Delgrande also considered this idea in [Delg95]). * It is indeed possible to define arguably interesting procedures of inquiry over inconsistent belief sets (as shown e.g. in <ref> [ReBr79] </ref>). * These kinds of world are perfectly conceivable (and even depictable in pictures, as Escher (see e.g. [Hofs80]) proved so many times). * Some theories have been considered acceptable for a certain period of time (e.g. <p> possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in <ref> [ReBr79] </ref> and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Resc73] <author> Rescher, N., </author> <title> "The coherence theory of truth", </title> <publisher> Clarendon, </publisher> <year> 1973. </year>
Reference-contexts: where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and <ref> [Resc73] </ref> or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Roos92] <author> Roos, N., </author> <title> "A logic for reasoning with inconsistent knowledge", </title> <booktitle> Artificial Intelligence 57, </booktitle> <pages> pp. 69-103, </pages> <year> 1992. </year>
Reference-contexts: hidden in the set, waiting for further analysis to appear). * As Gabbay and Hunter point out in [GaHu91], inconsistency in information is the norm (e.g. most systems have databases or knowledge bases where information may be obtained from different sources, or from not fully reliable sources, as commented in <ref> [Roos92] </ref>), and we should find ways to formalize it appropriately. Moreover, when an agent detects an inconsistency in its beliefs, it may interpret that fact as a signal to take external actions, such as asking the user, invoking a truth maintenance system, activating/deactivating certain inference rules, etc.
Reference: [RoRo72] <author> Routley, P., Routley, V., </author> <title> "Semantics of first degree entailment", </title> <booktitle> No^us 6, </booktitle> <pages> pp. 335-359, </pages> <year> 1972. </year>
Reference-contexts: the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in [Scot70], impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and <ref> [RoRo72] </ref>, non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [RuNo95] <author> Russell, S., Norvig, P., </author> <title> "Artificial Intelligence. A Modern Approach.", </title> <booktitle> Prentice Hall Series in Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: . . . . . . . . . . . . . . . . . . 34 Modelling rational inquiry in non-ideal agents A.Moreno 4 1 Aim of the work 1.1 Introduction Artificial Intelligence (AI) has pursued the goal of constructing rational agents for a long time ([Wool92], <ref> [RuNo95] </ref>, [RuSu95], [Russ95]). This kind of agents may be defined ([RuNo95]) as systems that have some kind of perception and try to act upon the environment so as to achieve their goals, given their beliefs; thus, beliefs somehow guide their behaviour (e.g. they may be used to choose between alternative courses
Reference: [Russ95] <author> Russell, S., </author> <title> "Rationality and intelligence", </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, IJCAI-95, </booktitle> <pages> pp. 950-957, </pages> <year> 1995. </year> <note> (http://www.cs.berkeley.edu/%7Erussell/) </note>
Reference-contexts: . . . . . . . . . . . . . . . . 34 Modelling rational inquiry in non-ideal agents A.Moreno 4 1 Aim of the work 1.1 Introduction Artificial Intelligence (AI) has pursued the goal of constructing rational agents for a long time ([Wool92], [RuNo95], [RuSu95], <ref> [Russ95] </ref>).
Reference: [RuSu95] <author> Russell, S., Subramanian, D., </author> <title> "Provably bounded-optimal agents", </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> pp. 575-609, </pages> <year> 1995. </year> <note> (http://www.cs.washington.edu/research/jair/home.html) </note>
Reference-contexts: . . . . . . . . . . . . . . . . . 34 Modelling rational inquiry in non-ideal agents A.Moreno 4 1 Aim of the work 1.1 Introduction Artificial Intelligence (AI) has pursued the goal of constructing rational agents for a long time ([Wool92], [RuNo95], <ref> [RuSu95] </ref>, [Russ95]).
Reference: [Scot70] <author> Scott. D., </author> <title> Advice in modal logic", in Philosophical problems in logic, </title> <editor> Ed. K. Lambert, </editor> <publisher> Reidel, </publisher> <year> 1970. </year>
Reference-contexts: In fact, it has been repeteadly proposed in the literature to consider impossible possible worlds, in the sense of having possible worlds where the usual logical connectives do not behave in the usual way, or tautologies may not be true, or inconsistent formulas may hold (see e.g. non-designated indices in <ref> [Scot70] </ref>, impossible possible worlds in [Hint75], non-classical worlds in [Cres72] and [Cres73], setups in [Beln77] and [RoRo72], non-standard worlds in [ReBr79] and [Resc73] or situations in [Leve84]). The drawback of these approaches is that, although they alleviate the problems of logical omniscience and perfect reasoning, they cause different problems.
Reference: [Shoh90] <author> Shoham, Y., </author> <title> "Agent-oriented programming", </title> <type> Technical report STAN-CS-90-1335, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> October </month> <year> 1990. </year> <title> Modelling rational inquiry in non-ideal agents A.Moreno 53 </title>
Reference-contexts: to have a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in <ref> [Shoh90] </ref> or [Shoh93], or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], <ref> [Shoh90] </ref>, [Shoh93], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [Shoh91] <author> Shoham, Y., </author> <title> "Varieties of context", </title> <booktitle> in Artificial Intelligence and Mathematical Theory of Computation, </booktitle> <pages> pp. 393-408, </pages> <editor> V. Lifschitz Ed., </editor> <publisher> Academic Press, </publisher> <address> N.Y., </address> <year> 1991. </year>
Reference-contexts: The agent can be unable to take all its beliefs into account in every inference; if it focuses in a subset of them (call that a context), it can draw conclusions which are consistent within the context but inconsistent if all the beliefs are considered (as remarked by Shoham in <ref> [Shoh91] </ref>; Fagin and Halpern modelled this situation in their logic of local reasoning, [FaHa85]; Delgrande also considered this idea in [Delg95]). * It is indeed possible to define arguably interesting procedures of inquiry over inconsistent belief sets (as shown e.g. in [ReBr79]). * These kinds of world are perfectly conceivable (and
Reference: [Shoh93] <author> Shoham, Y., </author> <title> "Agent-oriented programming", </title> <booktitle> Artificial Intelligence 60, </booktitle> <pages> pp. 51-92, </pages> <year> 1993. </year>
Reference-contexts: a module capable of handling its beliefs (see e.g. the Procedural Reasoning System (PRS) proposed in [GeIn89], the BDI (Belief-Desire-Intention) architecture considered in [Brat87], [BIP88], [CoLe90], [RaGe91], [RaGe93], [RaGe95a], [RaGe95b], [KiGe97] or [Morl97], the implicit agent architecture developed in [Denn78] and [Denn84], the Agent-oriented programming paradigm proposed in [Shoh90] or <ref> [Shoh93] </ref>, or the defeasible reasoner suggested in [Poll90] or [Poll95]). <p> In that context there is an external view of knowledge, because it is the designer of the system who ascribes knowledge to the processes. Processes do not compute their knowledge, and they are not required to answer questions about it. As Shoham notes in <ref> [Shoh93] </ref>, in these applications the knowledge possessed by agents is so simple that complexity of internal reasoning can be neglected. <p> Some logics of rational action have also been proposed recently, but all of them seem to share the basic limitations of the possible worlds model in some way (see e.g. [CoLe90], [Shoh90], <ref> [Shoh93] </ref>, [RaGe91], [RaGe93], [RaGe95a], [RaGe95b]). Extensive reviews can also be found in [McPa87], [McAr88], [Reic89] and [FHMV95].
Reference: [Smul68] <author> Smullyan, </author> <title> R.M., "First-order logic", </title> <publisher> Springer Verlag, </publisher> <year> 1968. </year>
Reference-contexts: The logical analysis of the agent's beliefs is performed using a modified version of the analytic tableaux method (see e.g. <ref> [Smul68] </ref> for a presentation of the classical method). The formulas contained in a tableau are divided into two sets, called left column and right column. <p> I () = 0 and I ( ) = 0 * J ( _ ) = !; otherwise * J ( ^ ) = 0; if I () = 0 or I ( ) = 0 * J ( ^ ) = !; otherwise 33 This notation was developed in <ref> [Smul68] </ref>, and has been used e.g. in [Fitt83], [Fitt96]. It allows us to write in a compact way many similar rules.
Reference: [Smul86] <author> Smullyan, </author> <title> R.M., "Logicians who reason about themselves", </title> <booktitle> Proceedings of the First Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <editor> TARK-86, Ed. </editor> <booktitle> J.Halpern, </booktitle> <pages> pp. 341-352, </pages> <year> 1986. </year>
Reference-contexts: agents will have more expressive power and they will be able to have beliefs about other agents' beliefs. 18 This syntactic restriction will also be eliminated in the future. 19 If both were to match, the agent would be a perfect knower, but this is exceptional and, as suggested in <ref> [Smul86] </ref>, even unstable: the norm is that what is true and what is believed are quite unrelated.
Reference: [Vard86] <author> Vardi, M., </author> <title> "On epistemic logic and logical omniscience", </title> <booktitle> Proceedings of the First Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <editor> TARK-86, Ed. </editor> <booktitle> J.Y.Halpern, </booktitle> <pages> pp. 293-305, </pages> <year> 1986. </year>
Reference: [Wagn97] <author> Wagner, G., </author> <title> introduction to the special issue in Handling inconsistency in knowledge systems, </title> <note> Journal of Applied Non-Classical Logics 7, 1997. (http://www.informatik.uni-leipzig.de/~gwagner/special-issue.html) </note>
Reference: [Wans90] <author> Wansing, H., </author> <title> "A general possible worlds framework for reasoning about knowledge and belief", </title> <type> Studia Logica 49 (4), </type> <pages> pp. 523-539, </pages> <year> 1990. </year>
Reference-contexts: It describes with some detail proposals from Konolige ([Kono86a]), Hintikka ([Hint86]), Rantala ([Rant75]), Levesque ([Leve84]), Fagin and Halpern ([FaHa85]), Montague ([Mont70]) and Vardi ([Vard86]). Some of these classical approaches have been extended and improved in more recent proposals (e.g. [Lake87], [LaLe88], [Lake91a], [Lake94], [Delg95]). Wansing shows in <ref> [Wans90] </ref> how some of these logics can be seen as particular instances of a more general non-normal worlds semantics (see [More97]). Other approaches ([FHV90], [FaHa85], [FHMV95]) are based in similar ideas, as decoupling the evaluation of a primitive proposition from the evaluation of its negation.
Reference: [WoJe95] <author> Wooldridge, M., Jennings, N., </author> <title> "Intelligent agents: </title> <journal> theory and practice", Knowledge Engineering Review 10 (2), </journal> <note> 1995. (http://www.dlib.com/people/mjw/papers.html) </note>
Reference-contexts: In propositional modal logic two unary operators (2 and 3) are added to propositional logic; they are called the universal modal operator and the existential modal operator 3 , respectively. These modal operators can be interpreted in a 1 See <ref> [WoJe95] </ref> or [Mull97] for extensive reviews of different approaches, including reactive architectures ([Broo90], [Broo91]), that do not adhere to the more classical deliberative agent conception. 2 These components will find their counterparts in the multi-dimensional belief analysis performed by our rational inquirers, as will be shown later. 3 The existential modal
Reference: [Wool92] <author> Wooldridge, M., </author> <title> "The logical modelling of computational multi-agent systems", </title> <type> PhD Thesis, </type> <institution> University of Manchester, </institution> <year> 1992. </year> <note> (http://www.dlib.com/people/mjw/papers.html) </note>
Reference: [Wool95] <author> Wooldridge, M., </author> <title> "An abstract general model and logic of resource-bounded believers", in Representing Mental States and Mechanims, </title> <booktitle> Proceedings of the 1995 AAAI Spring Symposium, </booktitle> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1995. </year> <title> (http://www.dlib.com/people/mjw/papers.html) Modelling rational inquiry in non-ideal agents A.Moreno 54 </title>
References-found: 123

