URL: http://www.cs.umn.edu/Users/dept/users/kumar/simplex.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Performance and Scalability of the Parallel Simplex Method for Dense Linear Programming Problems An Extended Abstract  
Author: George Karypis and Vipin Kumar 
Date: May 21, 1994  
Affiliation: Computer Science Department University of Minnesota  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. F. Benders. </author> <title> Partitioning procedures for solving mixed variable programming problems. </title> <journal> Numerische Mathematik, </journal> (4):238-252, 1962. 
Reference-contexts: We have recently developed a highly parallel formulation for sparse linear programming problems using interior point methods [11]. Nevertheless, dense linear programming problems have some genuine applications. For instance the Dantzig-Wolfe decomposition [4] or Benders decomposition <ref> [1] </ref> generate highly dense master problems. More applications 1 leading to dense linear programming problems are discussed in [5].
Reference: [2] <author> A. A. Bertossi and M. A. Bonuccelli. </author> <title> A VLSI implementation of the simplex algorithm. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(2):241-247, </volume> <month> February </month> <year> 1987. </year>
Reference-contexts: Onaga and Nagayasu [15] proposed a VLSI wavefront array for the simplex method, and Bertossi and Bonuccelli <ref> [2] </ref> proposed a VLSI mesh of trees implementation. Stunkel and Reed [16, 17] proposed an one-dimensional partitioning of the tablue for a hypercube-connected computer. They investigated both column-wise and row-wise partitions, and experimentally evaluated their partitioning schemes on a 16-processor iPSC/2.
Reference: [3] <author> R. Butler and E. Lusk. </author> <title> User's guide to the P4 parallel programming system. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1992. </year>
Reference-contexts: For two-dimensional partitioning, mesh-CT does not have the same performance as a hypercube because the size of the messages used in one-to-all broadcast is too small. 5 Experimental Results We performed experiments on the nCUBE 2 hypercube-connected computer and a network of SUN workstations. We used P4 <ref> [3] </ref> to implement the one-dimensional simplex formulations. Some of our preliminary experimental results are shown in Table 3. From this table we see that on nCUBE 2, the cyclic one-dimensional partitioning yields good speedups, and the speedup increases linearly as the number of processors increase particularly for large problem instances.
Reference: [4] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton Iniversity Press, </publisher> <address> Princeton, NJ, </address> <year> 1963. </year>
Reference-contexts: 1 Introduction Linear programming is of fundamental importance in many optimization problems. The simplex method <ref> [4] </ref> is a commonly used way of solving linear programming problems. Solving large instances of linear programming problems takes significant time; thus, solving linear programming problems in parallel offers a viable path for speeding up this type of computation. <p> In this paper, we are mainly concerned with linear programs whose constraint matrix is dense. Such problems are commonly solved using the tablue method <ref> [4] </ref>. Many parallel simplex formulations for dense linear programming problems have been developed [16, 17, 10, 5]. In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. <p> We have recently developed a highly parallel formulation for sparse linear programming problems using interior point methods [11]. Nevertheless, dense linear programming problems have some genuine applications. For instance the Dantzig-Wolfe decomposition <ref> [4] </ref> or Benders decomposition [1] generate highly dense master problems. More applications 1 leading to dense linear programming problems are discussed in [5].
Reference: [5] <author> J. Eckstein, L. C. Polymenakos, R. Qi, V. I. Ragulin, and S. A. Zenios. </author> <title> Data-parallel implementations of dense linear programming algorithms. </title> <type> Technical Report TMC-230, </type> <institution> Thinking Machines Corporations, </institution> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: In this paper, we are mainly concerned with linear programs whose constraint matrix is dense. Such problems are commonly solved using the tablue method [4]. Many parallel simplex formulations for dense linear programming problems have been developed <ref> [16, 17, 10, 5] </ref>. In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. <p> In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. Some of the parallel formulations of the simplex method we analyze were previously proposed <ref> [16, 10, 5] </ref> while the remaining are new schemes. We present analysis for a variety of architectures including hypercube, mesh, and network of workstations. Our analysis shows that a network of workstations has better scalability than the other architectures for some parallel simplex formulations. <p> Nevertheless, dense linear programming problems have some genuine applications. For instance the Dantzig-Wolfe decomposition [4] or Benders decomposition [1] generate highly dense master problems. More applications 1 leading to dense linear programming problems are discussed in <ref> [5] </ref>. Furthermore, in problems in which the nonzero elements are a fixed percentage d of the total number of elements, the formulations presented here lead to scalable algorithms. 2 Parallel Simplex Formulations Parallel formulations of the simplex method vary in the way the tablue T is distributed among processors. <p> In that case each processor gets m=p rows of T . However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D <ref> [16, 17, 5] </ref> New scheme New scheme Cyclic 1D [16, 17] New scheme New scheme Blocked 2D [10, 5] New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the <p> However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D [16, 17, 5] New scheme New scheme Cyclic 1D [16, 17] New scheme New scheme Blocked 2D <ref> [10, 5] </ref> New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the simplex method for dense problems have been proposed. <p> They investigated both column-wise and row-wise partitions, and experimentally evaluated their partitioning schemes on a 16-processor iPSC/2. Ho, Chen, Lin, and Sheu [10] proposed a two-dimensional partitioning of the tablue for a hypercube-connected computer and analyze its performance. Recently, Eckstein, Polymenakos, Qi, Ragulin, and Zenios <ref> [5] </ref> presented data-parallel implementations of dense linear programming algorithms. Table 1 provides a summary of the various parallel simplex formulations proposed so far.
Reference: [6] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. </author> <title> Isoefficiency function: A scalability metric for parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, Special Issue on Parallel and Distributed Systems: From Theory to Practice, </journal> <volume> 1 </volume> (3):12-21, 1993. 
Reference-contexts: However, most of the work done so far is based on experimental evaluation and the scalability of the various formulations have not been fully investigated. 4 Scalability Analysis We analyzed the scalability of all parallel formulations of the simplex method presented in Section 2 using the isoefficiency function <ref> [13, 6] </ref>. The isoefficiency function of a parallel algorithm determines the rate at which the problem size should increase with respect to the number of processors to maintain certain efficiency.
Reference: [7] <author> Ananth Grama and Vipin Kumar. </author> <title> Scalability analysis of partitioning strategies for finite element graphs. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 83-92, </pages> <year> 1992. </year>
Reference-contexts: For example, if W is the sequential complexity of an algorithm, then an isoefficiency function of f (p) indicates that in order to maintain fixed efficiency E, as the number of processors increase, W should increase at the rate of f (p). Isoefficiency analysis <ref> [7, 14, 9, 8, 12] </ref> has been successfully used in the past to analyze the scalability of a wide range of parallel algorithms. Since the simplex method is an iterative algorithm, we concentrate on the scalability of the iterations of the simplex method.
Reference: [8] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: For example, if W is the sequential complexity of an algorithm, then an isoefficiency function of f (p) indicates that in order to maintain fixed efficiency E, as the number of processors increase, W should increase at the rate of f (p). Isoefficiency analysis <ref> [7, 14, 9, 8, 12] </ref> has been successfully used in the past to analyze the scalability of a wide range of parallel algorithms. Since the simplex method is an iterative algorithm, we concentrate on the scalability of the iterations of the simplex method.
Reference: [9] <author> Anshul Gupta, Vipin Kumar, and Ahmed Sameh. </author> <title> Performance and scalability of preconditioned conjugate gradient methods on parallel computers. </title> <type> Technical Report TR 92-64, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1992. </year> <booktitle> A short version appears in the Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 664-674, </pages> <year> 1993. </year>
Reference-contexts: For example, if W is the sequential complexity of an algorithm, then an isoefficiency function of f (p) indicates that in order to maintain fixed efficiency E, as the number of processors increase, W should increase at the rate of f (p). Isoefficiency analysis <ref> [7, 14, 9, 8, 12] </ref> has been successfully used in the past to analyze the scalability of a wide range of parallel algorithms. Since the simplex method is an iterative algorithm, we concentrate on the scalability of the iterations of the simplex method.
Reference: [10] <author> H. F. Ho, G. H. Chen, S. H. Lin, and J. P. Sheu. </author> <title> Solving linear programming on fixed-size hypercubes. </title> <booktitle> In International Conference on Parallel Processesing, </booktitle> <pages> pages 112-116, </pages> <year> 1988. </year>
Reference-contexts: In this paper, we are mainly concerned with linear programs whose constraint matrix is dense. Such problems are commonly solved using the tablue method [4]. Many parallel simplex formulations for dense linear programming problems have been developed <ref> [16, 17, 10, 5] </ref>. In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. <p> In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. Some of the parallel formulations of the simplex method we analyze were previously proposed <ref> [16, 10, 5] </ref> while the remaining are new schemes. We present analysis for a variety of architectures including hypercube, mesh, and network of workstations. Our analysis shows that a network of workstations has better scalability than the other architectures for some parallel simplex formulations. <p> However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D [16, 17, 5] New scheme New scheme Cyclic 1D [16, 17] New scheme New scheme Blocked 2D <ref> [10, 5] </ref> New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the simplex method for dense problems have been proposed. <p> Stunkel and Reed [16, 17] proposed an one-dimensional partitioning of the tablue for a hypercube-connected computer. They investigated both column-wise and row-wise partitions, and experimentally evaluated their partitioning schemes on a 16-processor iPSC/2. Ho, Chen, Lin, and Sheu <ref> [10] </ref> proposed a two-dimensional partitioning of the tablue for a hypercube-connected computer and analyze its performance. Recently, Eckstein, Polymenakos, Qi, Ragulin, and Zenios [5] presented data-parallel implementations of dense linear programming algorithms. Table 1 provides a summary of the various parallel simplex formulations proposed so far.
Reference: [11] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> A parallel formulation of interior point algorithms. </title> <type> Technical Report (TR 94-20), </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: Most of the linear programming problems are sparse, and sequential codes for them exploit sparsity to speedup computation. We have recently developed a highly parallel formulation for sparse linear programming problems using interior point methods <ref> [11] </ref>. Nevertheless, dense linear programming problems have some genuine applications. For instance the Dantzig-Wolfe decomposition [4] or Benders decomposition [1] generate highly dense master problems. More applications 1 leading to dense linear programming problems are discussed in [5].
Reference: [12] <author> George Karypis and Vipin Kumar. </author> <title> Unstructured Tree Search on SIMD Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994 (to appear). available as Technical Report TR 92-21, </note> <institution> Computer Science Department, Un iversity of Minnesota. </institution> <month> 5 </month>
Reference-contexts: For example, if W is the sequential complexity of an algorithm, then an isoefficiency function of f (p) indicates that in order to maintain fixed efficiency E, as the number of processors increase, W should increase at the rate of f (p). Isoefficiency analysis <ref> [7, 14, 9, 8, 12] </ref> has been successfully used in the past to analyze the scalability of a wide range of parallel algorithms. Since the simplex method is an iterative algorithm, we concentrate on the scalability of the iterations of the simplex method.
Reference: [13] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: However, most of the work done so far is based on experimental evaluation and the scalability of the various formulations have not been fully investigated. 4 Scalability Analysis We analyzed the scalability of all parallel formulations of the simplex method presented in Section 2 using the isoefficiency function <ref> [13, 6] </ref>. The isoefficiency function of a parallel algorithm determines the rate at which the problem size should increase with respect to the number of processors to maintain certain efficiency. <p> The isoefficiency functions of the one-dimensional partitioning along the x dimension is the same as that along the y dimension. CT stands for cut-through routing while SF stands for store-and-forward routing. time. In contrast, the same operation takes fi (M log p) on a hypercube <ref> [13] </ref>. The scalability is the same for mesh with cut-through routing (mesh-CT) and hypercube although it is worse for mesh with store-and-forward routing (mesh-SF). The reason is that one-to-all broadcast of M elements takes fi (M log p + p p) time on a mesh-CT [13] and fi (M p p) <p> log p) on a hypercube <ref> [13] </ref>. The scalability is the same for mesh with cut-through routing (mesh-CT) and hypercube although it is worse for mesh with store-and-forward routing (mesh-SF). The reason is that one-to-all broadcast of M elements takes fi (M log p + p p) time on a mesh-CT [13] and fi (M p p) on a mesh-SF.
Reference: [14] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: For example, if W is the sequential complexity of an algorithm, then an isoefficiency function of f (p) indicates that in order to maintain fixed efficiency E, as the number of processors increase, W should increase at the rate of f (p). Isoefficiency analysis <ref> [7, 14, 9, 8, 12] </ref> has been successfully used in the past to analyze the scalability of a wide range of parallel algorithms. Since the simplex method is an iterative algorithm, we concentrate on the scalability of the iterations of the simplex method.
Reference: [15] <author> K. Onaga and H. Nagayasu. </author> <title> A wavefront-driven algorithm for linear programming on dataflow processor-arrays. </title> <booktitle> In Proceedings of International Computer Symposium, </booktitle> <pages> pages 739-746, </pages> <year> 1984. </year>
Reference-contexts: Onaga and Nagayasu <ref> [15] </ref> proposed a VLSI wavefront array for the simplex method, and Bertossi and Bonuccelli [2] proposed a VLSI mesh of trees implementation. Stunkel and Reed [16, 17] proposed an one-dimensional partitioning of the tablue for a hypercube-connected computer.
Reference: [16] <author> C. B. Stunkel and D. A. Reed. </author> <title> Hypercube implementation of the simplex aglorithm. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> volume ||, pages 1473-1482, </pages> <year> 1988. </year>
Reference-contexts: In this paper, we are mainly concerned with linear programs whose constraint matrix is dense. Such problems are commonly solved using the tablue method [4]. Many parallel simplex formulations for dense linear programming problems have been developed <ref> [16, 17, 10, 5] </ref>. In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. <p> In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. Some of the parallel formulations of the simplex method we analyze were previously proposed <ref> [16, 10, 5] </ref> while the remaining are new schemes. We present analysis for a variety of architectures including hypercube, mesh, and network of workstations. Our analysis shows that a network of workstations has better scalability than the other architectures for some parallel simplex formulations. <p> In that case each processor gets m=p rows of T . However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D <ref> [16, 17, 5] </ref> New scheme New scheme Cyclic 1D [16, 17] New scheme New scheme Blocked 2D [10, 5] New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the <p> However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D [16, 17, 5] New scheme New scheme Cyclic 1D <ref> [16, 17] </ref> New scheme New scheme Blocked 2D [10, 5] New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the simplex method for dense problems have been proposed. <p> Onaga and Nagayasu [15] proposed a VLSI wavefront array for the simplex method, and Bertossi and Bonuccelli [2] proposed a VLSI mesh of trees implementation. Stunkel and Reed <ref> [16, 17] </ref> proposed an one-dimensional partitioning of the tablue for a hypercube-connected computer. They investigated both column-wise and row-wise partitions, and experimentally evaluated their partitioning schemes on a 16-processor iPSC/2.
Reference: [17] <author> C. B. Stunkel, D. C. Rudolph, W. K. Fuchs, and D. A. Reed. </author> <title> Linear optimizations: A case study in performance analysis. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <volume> volume I, </volume> <pages> pages 265-268, </pages> <year> 1989. </year> <month> 6 </month>
Reference-contexts: In this paper, we are mainly concerned with linear programs whose constraint matrix is dense. Such problems are commonly solved using the tablue method [4]. Many parallel simplex formulations for dense linear programming problems have been developed <ref> [16, 17, 10, 5] </ref>. In this paper we provide a comprehensive performance and scalability analysis of many parallel formulations of the simplex method for a variety of architectures. <p> In that case each processor gets m=p rows of T . However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D <ref> [16, 17, 5] </ref> New scheme New scheme Cyclic 1D [16, 17] New scheme New scheme Blocked 2D [10, 5] New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the <p> However, since in most problems m &lt; n, this partitioning scheme exploits less parallelism than the partitioning along the x dimension. 2 Architecture Parallel Formulation Hypercube Mesh Network of Workstations Blocked 1D [16, 17, 5] New scheme New scheme Cyclic 1D <ref> [16, 17] </ref> New scheme New scheme Blocked 2D [10, 5] New scheme New scheme Cyclic 2D New Scheme New scheme New scheme Table 1: Summary of parallel simplex formulations. 3 Previous Work on Parallel Simplex A variety of parallel formulations of the simplex method for dense problems have been proposed. <p> Onaga and Nagayasu [15] proposed a VLSI wavefront array for the simplex method, and Bertossi and Bonuccelli [2] proposed a VLSI mesh of trees implementation. Stunkel and Reed <ref> [16, 17] </ref> proposed an one-dimensional partitioning of the tablue for a hypercube-connected computer. They investigated both column-wise and row-wise partitions, and experimentally evaluated their partitioning schemes on a 16-processor iPSC/2.
References-found: 17

