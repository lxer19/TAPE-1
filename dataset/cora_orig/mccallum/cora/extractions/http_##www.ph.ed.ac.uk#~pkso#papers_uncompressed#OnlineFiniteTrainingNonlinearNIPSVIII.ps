URL: http://www.ph.ed.ac.uk/~pkso/papers_uncompressed/OnlineFiniteTrainingNonlinearNIPSVIII.ps
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Email: P.Sollich@ed.ac.uk  D.Barber@aston.ac.uk  
Title: Online learning from finite training sets in nonlinear networks  
Author: Peter Sollich David Barber 
Address: Edinburgh EH9 3JZ, U.K.  Birmingham B4 7ET, U.K.  
Affiliation: Department of Physics University of Edinburgh  Department of Applied Mathematics Aston University  
Note: To appear in: Advances in Neural Information Processing Systems 10 M I Jordan, M J Kearns, and S A Solla (eds) Cambridge, MA. MIT Press. 1998.  
Abstract: Online learning is one of the most common forms of neural network training. We present an analysis of online learning from finite training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios. Dynamical equations are derived for an appropriate set of order parameters; these are exact in the limiting case of either linear networks or infinite training sets. Preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets, but may not yet account correctly for the presence of local minima.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Biehl and H. </author> <title> Schwarze. </title> <journal> Journal of Physics A, </journal> <volume> 28 </volume> <pages> 643-656, </pages> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest <ref> [1, 5, 7, 3] </ref>. In online learning, the weights of a network (`student') are updated immediately after presentation of each training example (input-output pair) in order to reduce the error that the network makes on that example. <p> These restrictions are, either that the size of the training set is infinite, or that the learning rate is small <ref> [1, 5, 4] </ref>. Finite training sets present a significant analytical difficulty as successive weight updates are correlated, giving rise to highly non-trivial generalization dynamics. <p> In the present work, we extend our analysis to nonlinear networks. The particular model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings. Its online learning dynamics has been studied comprehensively for infinite training sets <ref> [1, 5] </ref>. <p> It is straightforward to show that the generalization error depends, not on a detailed description of all the network weights, but only on the overlap parameters Q ll 0 = 1 N w T N w T m <ref> [1, 5, 7] </ref>. In the case of infinite ff, it is possible to obtain a closed set of equations governing the overlap parameters Q; R [5]. For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates [7].
Reference: [2] <author> A. C. C. Coolen, S. N. Laughton, and D. </author> <title> Sherrington. </title> <booktitle> In NIPS 8, </booktitle> <pages> pp. 253-259, </pages> <publisher> MIT Press, 1996; S.N. </publisher> <editor> Laughton, A.C.C. Coolen, and D. </editor> <title> Sherrington. </title> <journal> Journal of Physics A, </journal> <volume> 29 </volume> <pages> 763-786, </pages> <year> 1996. </year>
Reference-contexts: Its online learning dynamics has been studied comprehensively for infinite training sets [1, 5]. In order to carry out our analysis, we adapt tools originally developed in the statistical mechanics literature which have found application, for example, in the study of Hopfield network dynamics <ref> [2] </ref>. 2 MODEL AND OUTLINE OF CALCULATION For an N -dimensional input vector x, the output of the soft committee machine is given by y = l=1 r N l x (1) where the nonlinear activation function g (h l ) = erf (h l = p 2) acts on the <p> For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates [7]. In order to overcome this difficulty, we use a technique developed originally to study statistical physics systems <ref> [2] </ref>. Initially, consider the dynamics of a general vector of order parameters, denoted by , which are functions of the network weights w. <p> Future work will have to show whether the theory can be extended to cope with non-Gaussian student activations without incurring the technical difficulties of dynamical replica theory <ref> [2] </ref>, and whether this will help to capture the effects of local minima and, more generally, `rough' training error surfaces. Acknowledgments : We would like to thank Ansgar West for helpful discussions.
Reference: [3] <editor> See for example: </editor> <booktitle> The dynamics of online learning. Workshop at NIPS'95. </booktitle>
Reference-contexts: 1 INTRODUCTION The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest <ref> [1, 5, 7, 3] </ref>. In online learning, the weights of a network (`student') are updated immediately after presentation of each training example (input-output pair) in order to reduce the error that the network makes on that example.
Reference: [4] <author> T. Heskes and B. </author> <title> Kappen. </title> <journal> Physical Review A, </journal> <volume> 44 </volume> <pages> 2718-2762, </pages> <year> 1994. </year>
Reference-contexts: These restrictions are, either that the size of the training set is infinite, or that the learning rate is small <ref> [1, 5, 4] </ref>. Finite training sets present a significant analytical difficulty as successive weight updates are correlated, giving rise to highly non-trivial generalization dynamics.
Reference: [5] <author> D. Saad and S. A. </author> <title> Solla Physical Review E, </title> <address> 52:4225, </address> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest <ref> [1, 5, 7, 3] </ref>. In online learning, the weights of a network (`student') are updated immediately after presentation of each training example (input-output pair) in order to reduce the error that the network makes on that example. <p> These restrictions are, either that the size of the training set is infinite, or that the learning rate is small <ref> [1, 5, 4] </ref>. Finite training sets present a significant analytical difficulty as successive weight updates are correlated, giving rise to highly non-trivial generalization dynamics. <p> In the present work, we extend our analysis to nonlinear networks. The particular model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings. Its online learning dynamics has been studied comprehensively for infinite training sets <ref> [1, 5] </ref>. <p> It is straightforward to show that the generalization error depends, not on a detailed description of all the network weights, but only on the overlap parameters Q ll 0 = 1 N w T N w T m <ref> [1, 5, 7] </ref>. In the case of infinite ff, it is possible to obtain a closed set of equations governing the overlap parameters Q; R [5]. For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates [7]. <p> In the case of infinite ff, it is possible to obtain a closed set of equations governing the overlap parameters Q; R <ref> [5] </ref>. For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates [7]. In order to overcome this difficulty, we use a technique developed originally to study statistical physics systems [2]. <p> It is clear that if the integral in (4) depends on w only through (w), then the average is unnecessary and the resulting dynamical equations are exact. This is in fact the case for ff ! 1 and = fQ; Rg, the standard order parameters mentioned above <ref> [5] </ref>. If this cannot be achieved, one should choose a set of order parameters to obtain approximate equations which are as close as possible to the exact solution. <p> The Gaussian averages in (11) can be straightforwardly evaluated in a manner similar to the infinite training set case <ref> [5] </ref>, and we omit the rather cumbersome explicit form of the resulting equations. We note that, in contrast to the infinite training set case, the student activations h l and the noise variables c s and ~ s are now correlated through equation (10).
Reference: [6] <author> P. </author> <title> Sollich. </title> <journal> Journal of Physics A, </journal> <volume> 27 </volume> <pages> 7771-7784, </pages> <year> 1994. </year>
Reference-contexts: i = p N m (w l ) T P fl Aw fl a fl R lm ; (8) where we have used P fl A = a fl P fl with a fl `the' eigenvalue of A in the fl-th eigenspace; this is well defined for ! 1 (see <ref> [6] </ref> for details of the eigenvalue spectrum).
Reference: [7] <author> P. Sollich and D. Barber. </author> <title> In NIPS 9, pp.274-280, MIT Press, </title> <journal> 1997; Europhysics Letters, </journal> <volume> 38 </volume> <pages> 477-482, </pages> <year> 1997. </year>
Reference-contexts: 1 INTRODUCTION The analysis of online gradient descent learning, as one of the most common forms of supervised learning, has recently stimulated a great deal of interest <ref> [1, 5, 7, 3] </ref>. In online learning, the weights of a network (`student') are updated immediately after presentation of each training example (input-output pair) in order to reduce the error that the network makes on that example. <p> For linear networks, the difficulties encountered with finite training sets and non-infinitesimal learning rates can be overcome by extending the standard set of descriptive (`order') parameters to include the effects of weight update correlations <ref> [7] </ref>. In the present work, we extend our analysis to nonlinear networks. The particular model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings. Its online learning dynamics has been studied comprehensively for infinite training sets [1, 5]. <p> It is straightforward to show that the generalization error depends, not on a detailed description of all the network weights, but only on the overlap parameters Q ll 0 = 1 N w T N w T m <ref> [1, 5, 7] </ref>. In the case of infinite ff, it is possible to obtain a closed set of equations governing the overlap parameters Q; R [5]. For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates [7]. <p> In the case of infinite ff, it is possible to obtain a closed set of equations governing the overlap parameters Q; R [5]. For finite training sets, however, this is no longer possible, due to the correlations between successive weight updates <ref> [7] </ref>. In order to overcome this difficulty, we use a technique developed originally to study statistical physics systems [2]. Initially, consider the dynamics of a general vector of order parameters, denoted by , which are functions of the network weights w. <p> training inputs, b s = p p X ~ 1 Here we assume that the system size N is large enough that the mean values of the parameters alone describe the dynamics sufficiently well (i.e., self-averaging holds). 2 The order parameters actually used in our calculation for the linear perceptron <ref> [7] </ref> are Laplace transforms of these projected order parameters. As ! 1, these order parameters become functionals of a continuous variable 3 .
References-found: 7

