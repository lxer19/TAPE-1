URL: http://www.cs.washington.edu/homes/echris/papers/lcpc96.ps
Refering-URL: http://www.cs.washington.edu/homes/echris/
Root-URL: 
Title: Factor-Join: A Unique Approach to Compiling Array Languages for Parallel Machines  
Author: Bradford L. Chamberlain Sung-Eun Choi E Christopher Lewis Calvin Lin Lawrence Snyder W. Derrick Weathersby 
Address: Seattle, WA 98195-2350 USA  Austin, TX 78712 USA  
Affiliation: University of Washington,  University of Texas,  
Note: Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, 1996.  
Abstract: This paper describes a new approach to compiling and optimizing array languages for parallel machines. This approach first decomposes array language operations into factors, where each factor corresponds to a different communication or computation structure. Optimizations are then achieved by combining, or joining, these factors. Because factors preserve high level information about array operations, the analysis necessary to perform these join operations is simpler than that required for scalar programs. In particular, we show how data parallel programs written in the ZPL programming language are compiled and optimized using the factor-join approach, and we show that a small number of factors are sufficient to represent ZPL programs.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Saman P. Amarasinghe, Jennifer M. Anderson, Monica S. Lam, and Amy W. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: HPF and ZPL are similar in the types of parallel operations that they support, though ZPL makes clear to the programmer the execution cost of each operation [22]. Considerable research has been devoted to automatically parallelizing Fortran 77 programs <ref> [1, 23, 11] </ref>. In contrast to the ZPL approach in which the language was designed to facilitate the recognition and exploitation of parallelism, the primary effort for automatically parallelizing compilers is in recognizing, exposing and efficiently exploiting the parallelism hidden in a sequential program.
Reference: 2. <author> Ray Barriuso and Allan Knies. </author> <title> SHMEM user's guide for C. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The combined communication compiles to procedure calls in the ZPL runtime library. These procedures are optimized for the particular platform's strengths [7]. For example, on the SP2, the procedure calls are mapped to MPI library routines, while our T3D implementation uses the native SHMEM library routines <ref> [2] </ref>. In this way, a single copy of optimized code exploits the strengths of all target platform. Though we have introduced collective communication operations as each producing a single T-factor, we often use multi-part T-factors for these operations.
Reference: 3. <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: We optimize computation and communication separately. NESL is a data-parallel programming language that emphasizes nested parallelism [4]. NESL source code is compiled to an intermediate vector-based code, called Vcode, which is either interpreted or compiled <ref> [3] </ref>. The Vcode intermediate form is well suited for vector and low-latency shared memory machines but not for distributed memory machines. The primary MIMD compilation effort is in increasing the granularity of parallelism and reducing synchronization overhead.
Reference: 4. <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Their fusion concept differs from our join transformation in that it only strives to eliminate intermediate storage. In addition, they assume a shared memory model and thus do not consider explicit communication. We optimize computation and communication separately. NESL is a data-parallel programming language that emphasizes nested parallelism <ref> [4] </ref>. NESL source code is compiled to an intermediate vector-based code, called Vcode, which is either interpreted or compiled [3]. The Vcode intermediate form is well suited for vector and low-latency shared memory machines but not for distributed memory machines.
Reference: 5. <author> Timothy A. Budd. </author> <title> An APL compiler for a vector processor. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 297-313, </pages> <month> July </month> <year> 1984. </year> <booktitle> Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Greenlaw and Snyder demonstrate that the second most common data movement operation in APL (an array subscripted array) is very expensive on parallel machines [10]. Budd describes an APL compiler that decomposes array operations into vector operations for execution on a vector processor <ref> [5] </ref>. This fine grained approach does not extend to distributed memory or MIMD machines. Ju et al. describe a classification and fusion scheme for array language primitives (in APL, Fortran 90, etc.) [16]. Their fusion concept differs from our join transformation in that it only strives to eliminate intermediate storage.
Reference: 6. <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improved data locality. </title> <booktitle> In Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> October 1994. San Jose, CA. </address>
Reference-contexts: This join transformation differs from traditional loop fusion in that the structure of the candidate loop nests is not fixed when the joining decision is made. There are a number of benefits to joining m-loops. Some are traditional, e.g., improved cache locality <ref> [6] </ref> and reduced loop overhead, and others are unique to the array language context, e.g., joining enables contraction of an array to a scalar when the array's definition only reaches uses in the same iteration. Array Contraction.
Reference: 7. <author> Bradford L. Chamberlain, Sung-Eun Choi, and Lawrence Snyder. </author> <title> Ironman: An architecture indepedent communication interface for parallel computers. </title> <note> submitted for publication, </note> <year> 1996. </year>
Reference-contexts: This T-factor is represented in the AST using Send and Receive nodes 3 that 3 The ZPL compiler actually uses the Ironman communication interface which is more hardware independent than a send/receive interface <ref> [7] </ref>. By using machine-dependent libraries and an unassuming interface, Ironman allows the same ZPL object code to exploit each machine's customized interprocessor communication fea tures. This document uses send/receive for simplicity. Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, 1996. (a) (c) Fig. 7. <p> Despite this fixed collective communication interface, ZPL does not lose the advantages of library support. The combined communication compiles to procedure calls in the ZPL runtime library. These procedures are optimized for the particular platform's strengths <ref> [7] </ref>. For example, on the SP2, the procedure calls are mapped to MPI library routines, while our T3D implementation uses the native SHMEM library routines [2]. In this way, a single copy of optimized code exploits the strengths of all target platform.
Reference: 8. <author> Marios D. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In Ninth International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing [21] and is generally superior to the HPF compilers with which it has been compared [19, 22]. ZPL has also been successfully used for scientific and engineering applications <ref> [8, 18, 24] </ref>, and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere [27, 20]. Section 3 describes runtime assumptions that are used in the compilation process.
Reference: 9. <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Con-currency-Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Budd describes an APL compiler that decomposes array operations into vector operations for execution on a vector processor [5]. This fine grained approach does not extend to distributed memory or MIMD machines. Ju et al. describe a classification and fusion scheme for array language primitives <ref> (in APL, Fortran 90, etc.) </ref> [16]. Their fusion concept differs from our join transformation in that it only strives to eliminate intermediate storage. In addition, they assume a shared memory model and thus do not consider explicit communication. We optimize computation and communication separately.
Reference: 10. <author> R. Greenlaw and L. Snyder. </author> <title> Achieving speedups for APL on an SIMD distributed memory machine. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 19(2) </volume> <pages> 111-127, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: APL was not designed with parallelism in mind, thus it encourages the use of locality insensitive operations. Greenlaw and Snyder demonstrate that the second most common data movement operation in APL (an array subscripted array) is very expensive on parallel machines <ref> [10] </ref>. Budd describes an APL compiler that decomposes array operations into vector operations for execution on a vector processor [5]. This fine grained approach does not extend to distributed memory or MIMD machines.
Reference: 11. <author> Manish Gupta and Prithviraj Banerjee. </author> <title> PARADIGM: A compiler for automatic data distribution on multicomputers. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: HPF and ZPL are similar in the types of parallel operations that they support, though ZPL makes clear to the programmer the execution cost of each operation [22]. Considerable research has been devoted to automatically parallelizing Fortran 77 programs <ref> [1, 23, 11] </ref>. In contrast to the ZPL approach in which the language was designed to facilitate the recognition and exploitation of parallelism, the primary effort for automatically parallelizing compilers is in recognizing, exposing and efficiently exploiting the parallelism hidden in a sequential program.
Reference: 12. <author> Manish Gupta, Sam Midkiff, Edith Schonberg, Ven Seshadri, David Shields, Ko-Yang Wang, Wai-Mee Ching, and Ton Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, 1996. factors preserve the source code's high-level semantics. In contrast to this high level approach the IBM HPF compiler may lose semantic information because it scalarizes Fortran 90 array structures early in the compilation process <ref> [12] </ref>. The ZPL compiler thus employs standard compilation concepts and techniques, but extends them to exploit the language's abstractions and to treat arrays atomically. Our presentation of the ZPL compiler will assume an understanding of scalar compilation and concentrate only on areas of difference. These include the following.
Reference: 13. <author> Philip J. Hatcher, Anthony J. Lapadula, Robert R. Jones, Michael J. Quinn, and Ray J. Anderson. </author> <title> A production-quality C* compiler for hypercube multicomput-ers. </title> <booktitle> In Proceedings of Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Despite this similarity, the techniques for compiling these languages greatly differ. In particular, the primary Dataparallel C compilation effort is in overcoming inefficiencies due to the sequential nature of the parent language (C) and the SIMD nature of the language itself <ref> [13] </ref>. As an example of the former, a Dataparallel C program may contain arbitrary C code, which resists static analysis due to pointer arithmetic and weak typing.
Reference: 14. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification Version 1.1. </title> <month> November </month> <year> 1994. </year>
Reference-contexts: High Performance Fortran (HPF) is a language that requires the user specification of parallelism, distribution and alignment via directives in sequential Fortran 77 and Fortran 90 programs <ref> [14] </ref>. The primary compilation effort is in overcoming the sequential nature of the parent language. Arrays are manipulated at the element level, thus optimizations must be performed to vectorize communication and hoist it from inner loops.
Reference: 15. <author> Kenneth E. Iverson. </author> <title> A Programming Language. </title> <publisher> Wiley, </publisher> <year> 1962. </year>
Reference-contexts: There exist a number of compilation efforts that are similar in nature to that of ZPL. Several are summarized below. The APL language supports the atomic manipulation of and computation on whole arrays <ref> [15] </ref>. APL was not designed with parallelism in mind, thus it encourages the use of locality insensitive operations. Greenlaw and Snyder demonstrate that the second most common data movement operation in APL (an array subscripted array) is very expensive on parallel machines [10].
Reference: 16. <author> Dz-Ching R. Ju, Chaun-Lin Wu, and Paul Carini. </author> <title> The classification, fusion, and parallelization of array language primitives. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(10) </volume> <pages> 1113-1120, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This fine grained approach does not extend to distributed memory or MIMD machines. Ju et al. describe a classification and fusion scheme for array language primitives (in APL, Fortran 90, etc.) <ref> [16] </ref>. Their fusion concept differs from our join transformation in that it only strives to eliminate intermediate storage. In addition, they assume a shared memory model and thus do not consider explicit communication. We optimize computation and communication separately. NESL is a data-parallel programming language that emphasizes nested parallelism [4].
Reference: 17. <author> E Christopher Lewis and Calvin Lin. </author> <title> Array contraction in array languages. </title> <type> Technical report, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1996. </year> <month> Forthcoming. </month>
Reference-contexts: The compiler therefore joins m-loops with the goal of enabling maximal array contraction. Using a heuristic ordering of the candidate arrays, all m-loops containing a candidate array are joined if the joining enables contraction of the candidate array. This simple greedy strategy produces very high quality code <ref> [17] </ref>. Our approach not only contracts arrays that a clever scalar language pro grammer would, it often succeeds in non-obvious cases.
Reference: 18. <author> E Christopher Lewis, Calvin Lin, Lawrence Snyder, and George Turkiyyah. </author> <title> A portable parallel n-body solver. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Simon, V. Torczon, and L. Watson, editors, </editor> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 331-336. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing [21] and is generally superior to the HPF compilers with which it has been compared [19, 22]. ZPL has also been successfully used for scientific and engineering applications <ref> [8, 18, 24] </ref>, and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere [27, 20]. Section 3 describes runtime assumptions that are used in the compilation process.
Reference: 19. <author> C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. Lewis, and W. D. Weathersby. </author> <title> ZPL vs. HPF: A comparison of performance and programming style. </title> <type> Technical Report 95-11-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing [21] and is generally superior to the HPF compilers with which it has been compared <ref> [19, 22] </ref>. ZPL has also been successfully used for scientific and engineering applications [8, 18, 24], and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere [27, 20].
Reference: 20. <author> Calvin Lin. </author> <title> ZPL language reference manual. </title> <type> Technical Report 94-10-06, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: ZPL has also been successfully used for scientific and engineering applications [8, 18, 24], and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere <ref> [27, 20] </ref>. Section 3 describes runtime assumptions that are used in the compilation process. The compilation process itself is described in Sect. 4, with an emphasis on its structure and use of the factor-join strategy. <p> A complete listing is provided in Appendix A. In addition to the above operations, ZPL contains a number of expressive abstractions for array manipulation. Due to space limitations, we only give a brief survey below, but complete information is available elsewhere <ref> [27, 20] </ref>. Shattered control flow - ZPL has sequential control flow as long as control statements involve only scalars (e.g., if (scalar=1) then: : :).
Reference: 21. <author> Calvin Lin and Lawrence Snyder. </author> <title> SIMPLE performance results in ZPL. </title> <editor> In Keshav Pingali, Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 361-375. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <booktitle> Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing <ref> [21] </ref> and is generally superior to the HPF compilers with which it has been compared [19, 22]. ZPL has also been successfully used for scientific and engineering applications [8, 18, 24], and its compiler is available on the Web. 2 The remainder of this paper is structured as follows.
Reference: 22. <author> Ton A. Ngo. </author> <title> The Effectiveness of Two Data Parallel Languages, HPF and ZPL. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science, </institution> <year> 1996. </year> <note> In preparation. </note>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing [21] and is generally superior to the HPF compilers with which it has been compared <ref> [19, 22] </ref>. ZPL has also been successfully used for scientific and engineering applications [8, 18, 24], and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere [27, 20]. <p> Arrays are manipulated at the element level, thus optimizations must be performed to vectorize communication and hoist it from inner loops. HPF and ZPL are similar in the types of parallel operations that they support, though ZPL makes clear to the programmer the execution cost of each operation <ref> [22] </ref>. Considerable research has been devoted to automatically parallelizing Fortran 77 programs [1, 23, 11].
Reference: 23. <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> The structure of Parafrase-2: an advanced parallelizing compiler for C and Fortran. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 423-453, </pages> <year> 1990. </year>
Reference-contexts: HPF and ZPL are similar in the types of parallel operations that they support, though ZPL makes clear to the programmer the execution cost of each operation [22]. Considerable research has been devoted to automatically parallelizing Fortran 77 programs <ref> [1, 23, 11] </ref>. In contrast to the ZPL approach in which the language was designed to facilitate the recognition and exploitation of parallelism, the primary effort for automatically parallelizing compilers is in recognizing, exposing and efficiently exploiting the parallelism hidden in a sequential program.
Reference: 24. <author> George Wilkey Richardson. </author> <title> Evaluation of a parallel Chaos router simulator. </title> <type> Master's thesis, </type> <institution> University of Arizona, Department of Electrical and Computer Engineering, </institution> <year> 1995. </year>
Reference-contexts: Previous work has shown that the generated code's performance is comparable with C using explicit message-passing [21] and is generally superior to the HPF compilers with which it has been compared [19, 22]. ZPL has also been successfully used for scientific and engineering applications <ref> [8, 18, 24] </ref>, and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere [27, 20]. Section 3 describes runtime assumptions that are used in the compilation process.
Reference: 25. <author> J.R. Rose and Guy L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <type> Technical Report PL 87-5, </type> <institution> Thinking Machines Corporation, </institution> <year> 1987. </year>
Reference-contexts: The primary MIMD compilation effort is in increasing the granularity of parallelism and reducing synchronization overhead. Cfl and its descendant Dataparallel C are derivatives of C with support for data parallel programming <ref> [25] </ref>. The Dataparallel C domain and the ZPL region both serve as bases for parallelism; they are used to define distributed arrays and to distribute computation. Despite this similarity, the techniques for compiling these languages greatly differ.
Reference: 26. <author> Lawrence Snyder. </author> <title> Foundations of practical aprallel programming languages. </title> <editor> In Tony Hey and Jeanne Ferrante, editors, </editor> <booktitle> Portability and performance for parallel processing, </booktitle> <pages> pages 1-19, </pages> <address> New York, 1994. </address> <publisher> Wiley. </publisher>
Reference-contexts: The language is not ideally suited for certain applications, particularly highly irregular codes. These are handled by ZPL's more general parent language, Advanced ZPL <ref> [26] </ref>. 3 The ZPL Runtime Before describing the ZPL compiler, we state a few assumptions about ZPL's runtime environment. In ZPL, the region is the basis for a program's implied parallelism.
Reference: 27. <author> Lawrence Snyder. </author> <title> The ZPL Programmer's Guide. </title> <month> May </month> <year> 1996. </year>
Reference-contexts: ZPL has also been successfully used for scientific and engineering applications [8, 18, 24], and its compiler is available on the Web. 2 The remainder of this paper is structured as follows. Section 2 briefly introduces basic ZPL language concepts|more complete descriptions are available elsewhere <ref> [27, 20] </ref>. Section 3 describes runtime assumptions that are used in the compilation process. The compilation process itself is described in Sect. 4, with an emphasis on its structure and use of the factor-join strategy. <p> Section 5 discusses the details of joining, and the final two sections present related work and conclusions. 2 ZPL Language Summary ZPL is an implicitly parallel array language designed for scientific computations <ref> [27] </ref>. It is an imperative language, supporting standard data types (integer, float, char, etc.), standard operators (+, -, fl, etc.), C-like assignment operators (+=, fl=, etc.), procedures with by-value and by-reference parameters, recursion, a standard set of control constructs (if, for, while, etc.), and C-like I/O. <p> A complete listing is provided in Appendix A. In addition to the above operations, ZPL contains a number of expressive abstractions for array manipulation. Due to space limitations, we only give a brief survey below, but complete information is available elsewhere <ref> [27, 20] </ref>. Shattered control flow - ZPL has sequential control flow as long as control statements involve only scalars (e.g., if (scalar=1) then: : :).
Reference: 28. <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1989. </year> <booktitle> Appeared in Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Joining M-loops. The joining of m-loop factors has the effect of fusing loops in the object C code. Determining whether two m-loops may be legally joined is similar to the data dependence analysis required to fuse two loop nests <ref> [28] </ref>: (1) both m-loops must iterate over the same region, and (2) for the joined m-loop there must exist a loop nest that preserves the unjoined data dependences and respects semantic restrictions. <p> Array Contraction. Array contraction is a well-known technique for scalar languages <ref> [28] </ref>, but it is more important for array languages because the programmer has no control over the structure of the compiler-generated loops.
References-found: 28

