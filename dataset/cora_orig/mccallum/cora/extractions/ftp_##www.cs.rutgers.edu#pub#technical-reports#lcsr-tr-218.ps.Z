URL: ftp://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-218.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Email: e-mail: fsumit,weinwangg@cs.rutgers.edu  
Phone: Tel: (908) 932-4974  
Title: Optimizing Queries for Coarse Grain Parallelism  
Author: Sumit Ganguly Weining Wang 
Address: New Brunswick, NJ 08904  
Affiliation: Department of Computer Sciences Rutgers University  
Abstract: We consider the problem of optimizing select-project-join relational queries for minimum response time on parallel machines. The design of the optimizer is based on three ideas: (1) the concept and quantification of degree of coarse grain parallelism for an execution tree, (2) the design of a parallelizing scheduler for a tree of coarse grain operations which is provably near optimal, and (3) the analysis of the scheduling algo rithm to obtain a cost formula for parallel execution time. The search algorithm of the optimizer is presented as a multi-dimensional dynamic programming algorithm. We present two three-dimensional search algorithms for the case when placement of relations in the parallel machine do not overlap. We propose the tree placement strategy and demonstrate, by means of examples, how the number of dimensions in the search can be significantly reduced, thereby increasing the efficiency of the search algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.G. Coffman, Jr, M. R. Garey, D.S. Johnson, R. E. Tarjan, </author> <title> Performance Bounds for level-oriented two-dimensional packing algorithms, </title> <journal> SIAM J. Comput., </journal> <volume> 9:4, </volume> <year> 1980,pp. </year> <pages> 809-826. </pages>
Reference-contexts: We present a parallelizing scheduler that schedules operations in a coarse grain execution tree. For the class of executions considered, this scheduling algorithm is provably near optimal. The design of the algorithm is based on the rectangle packing algorithm 2 presented in <ref> [1] </ref>. 3. We analyze the scheduling algorithm to obtain two cost formulae (one more accurate than the other) for the parallel completion time of an execution tree. <p> The algorithm uses the procedure assign operation (oid; rid; np) that assigns resource units rid; rid + 1; : : : ; rid + np 1 to operation specified by oid. The algorithm is an implementation of the following rule: Next Fit Decreasing Height scheduling rule: <ref> [1] </ref> Choose the operation, T , with the largest completion time (i.e., a i ( ^ ffi i )= ^ ffi i ). If there are ^ ffi T resource units available, then assign the first ^ ffi T resource units to T and repeat the rule. <p> We design a scheduler with a near-optimal performance for parallelizing a coarse grain tree. This is based on the schdeuling strategies for rectangle packing presented in <ref> [1] </ref>. 3. We analyze the scheduling algorithm to obtain cost estimates (for example, one estimate is the computing area/number of resource units + length of the critical path of execution) for the parallel time of an execution tree.
Reference: [2] <author> D. DeWitt and J. Gray., </author> <title> The future of high performance database systems, </title> <journal> Communications of the ACM, </journal> <month> June </month> <year> 1992. </year>
Reference: [3] <author> T.H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: We model the cost of communicating a message of M bytes from one sender to one receiver as Communication time = ff + M fi fi where * ff is the startup cost, measured in microseconds (for example, for nCUBE 2, ff = 200 microseconds <ref> [3] </ref>). * fi is the time spent per unit of data sent (for example, for nCUBE 2, fi = 0.6 microsec onds per byte [3]). Thus, the communication costs can be divided into two parts, the startup costs (the ff component) and the message costs (fi component). <p> time = ff + M fi fi where * ff is the startup cost, measured in microseconds (for example, for nCUBE 2, ff = 200 microseconds <ref> [3] </ref>). * fi is the time spent per unit of data sent (for example, for nCUBE 2, fi = 0.6 microsec onds per byte [3]). Thus, the communication costs can be divided into two parts, the startup costs (the ff component) and the message costs (fi component). Below we define the concept of communication area.
Reference: [4] <author> S. Ganguly, </author> <title> Parallel Evaluation of Deductive Database Queries, </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, </institution> <year> 1992. </year> <month> 28 </month>
Reference-contexts: An important experimental result due to Hong [9] and Stonebraker [8] is that the optimal plan for a shared everything parallel machine is the best parallelization of the optimal plan for a sequential machine. As discussed in <ref> [4, 5, 15] </ref>, this result does not generalize to shared nothing parallel architectures. Ganguly, Hasan and Krishnamurthy [5] present a framework for optimizing queries for parallel machines. They observe that optimizing for parallel executions requires extending the System R dynamic programming algorithms to work with more than one dimension. <p> They observe that optimizing for parallel executions requires extending the System R dynamic programming algorithms to work with more than one dimension. However, they do not present any specific practical method of designing the dimensions. Ganguly <ref> [4] </ref> presents a two dimensional search algorithm for optimizing in the presence of data dependencies without considering the problem of scheduling resources. The approach taken by Srivastava and Elsesser [15] is interesting for shared nothing architectures. <p> Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined. <p> Since properties of relational implementation operators are well known, it is easy to construct a table of data dependencies between all pairs of relational operators <ref> [4, 5] </ref>. Single materialization on right subtree: In this paper, we assume that for each right subtree in the operator tree, there is at most one sequential dependency. <p> We therefore believe that the assumption that there is at most a single materialization point in the right subtree of the operator tree includes most interesting executions in the space of left-deep join trees. We assume that there is a centralized cost model (which is physically transparent <ref> [4] </ref>) that can estimate the time of computation of each of the operators in an operator tree and the sizes of the inputs and the outputs for each operator. In this paper, we do not model memory explicitly and leave it to future work. <p> (1 P )P T optimum + (2 + 2f )P T optimum or; P T s (4 + 2f 2 P )P T optimum 2 20 6 Search Algorithm In this section, we design a multi-dimensional dynamic programming search algorithm (multidimensional dynamic programming is called partial order dynamic programming in <ref> [5, 4] </ref>). Given a query, the search algorithm finds the plan with the least value of parallel time among the space of all locally coarse grain parallel executions.
Reference: [5] <author> S. Ganguly, W. Hasan and R. Krishnamurthy, </author> <title> Query Optimization for Parallel Execu--tions, </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data. </booktitle>
Reference-contexts: An important experimental result due to Hong [9] and Stonebraker [8] is that the optimal plan for a shared everything parallel machine is the best parallelization of the optimal plan for a sequential machine. As discussed in <ref> [4, 5, 15] </ref>, this result does not generalize to shared nothing parallel architectures. Ganguly, Hasan and Krishnamurthy [5] present a framework for optimizing queries for parallel machines. They observe that optimizing for parallel executions requires extending the System R dynamic programming algorithms to work with more than one dimension. <p> As discussed in [4, 5, 15], this result does not generalize to shared nothing parallel architectures. Ganguly, Hasan and Krishnamurthy <ref> [5] </ref> present a framework for optimizing queries for parallel machines. They observe that optimizing for parallel executions requires extending the System R dynamic programming algorithms to work with more than one dimension. However, they do not present any specific practical method of designing the dimensions. <p> Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined. <p> Since properties of relational implementation operators are well known, it is easy to construct a table of data dependencies between all pairs of relational operators <ref> [4, 5] </ref>. Single materialization on right subtree: In this paper, we assume that for each right subtree in the operator tree, there is at most one sequential dependency. <p> (1 P )P T optimum + (2 + 2f )P T optimum or; P T s (4 + 2f 2 P )P T optimum 2 20 6 Search Algorithm In this section, we design a multi-dimensional dynamic programming search algorithm (multidimensional dynamic programming is called partial order dynamic programming in <ref> [5, 4] </ref>). Given a query, the search algorithm finds the plan with the least value of parallel time among the space of all locally coarse grain parallel executions.
Reference: [6] <author> M. R. Garey and D.S. Johnson, </author> <title> Computers and Intractability, a Guide to the Theory of NP-completeness, W.H. </title> <publisher> Freeman and Company: </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [7] <author> R.L. Graham, </author> <title> Bounds on Multiprocessing Timing Anomalies, </title> <journal> SIAM J. Appl. Math., </journal> <volume> vol. 17, </volume> <pages> pp. 416-429, </pages> <year> 1969. </year>
Reference: [8] <author> W. Hong and M. Stonebraker, </author> <title> Optimization of Parallel Query Execution Plans in XPRS, </title> <booktitle> Proceedings of the First International Conference on Parallel and Distributed Database Systems, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: An important factor in the success of relational database technology in a sequential computing environment has been the successful design of these query optimizers. However, the design of query optimizers for parallel machines has only recently received attention. An important experimental result due to Hong [9] and Stonebraker <ref> [8] </ref> is that the optimal plan for a shared everything parallel machine is the best parallelization of the optimal plan for a sequential machine. As discussed in [4, 5, 15], this result does not generalize to shared nothing parallel architectures. <p> Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined.
Reference: [9] <author> W. Hong, </author> <title> Exploiting Inter-Operation Parallelism in XPRS, </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data. </booktitle>
Reference-contexts: An important factor in the success of relational database technology in a sequential computing environment has been the successful design of these query optimizers. However, the design of query optimizers for parallel machines has only recently received attention. An important experimental result due to Hong <ref> [9] </ref> and Stonebraker [8] is that the optimal plan for a shared everything parallel machine is the best parallelization of the optimal plan for a sequential machine. As discussed in [4, 5, 15], this result does not generalize to shared nothing parallel architectures. <p> Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined.
Reference: [10] <author> T. Leighton, </author> <title> Introduction to Parallel Algorithms and architectures: arrays, trees and hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: It does not directly relate parallel communication time to parallel computation time. The parameter of coarse grain computation is closely related to the efficiency <ref> [10] </ref> of a parallel computation, which is defined below.
Reference: [11] <author> H. Lu, M.C. Shan and K.L. Tan, </author> <title> Optimization of Multi-Way Join Queries for Parallel Execution, </title> <booktitle> Proceedings of 1991 Very Large Database Conference. </booktitle>
Reference-contexts: They break this circular dependence, by an exhaustive solution, which, however, is computationally expensive. More work is needed to justify the optimality of their heuristic algorithm. Lu, Shan and Tan <ref> [11] </ref> present a greedy heuristic for optimization of multi-way joins. The optimality of the proposed method is not clear. In this paper, we concentrate on the problem of optimizing the response time (also called parallel time) of a given select-project-join query of relational algebra.
Reference: [12] <author> H. Pirahesh, C. Mohan, J.Cheung, T.S. Liu and P. Selinger, </author> <title> Parallelism in Relational Database Systems: </title> <booktitle> Architectural Issues and Design Approaches. Proceedings of the 1991 International Conference on Parallel and Distributed Information Systems. </booktitle>
Reference-contexts: Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined.
Reference: [13] <author> D. Schneider, </author> <title> Complex Query Processing in Multiprocessor Database Machines, </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference: [14] <author> P. Selinger, M.M. Astrahan, D.D. Chamberlain, R.A. Lorie and T.G. Price, </author> <title> Access Path Selection in a Relational Database Management System, </title> <booktitle> Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data. </booktitle>
Reference-contexts: Section 7 discusses the tree placement strategy and its ramifications. Finally, we conclude in Section 8. 2 Execution Space In this section, we briefly discuss the execution space, review the notion of join trees and operator trees and state our assumptions. We consider the space of left-deep join trees <ref> [14] </ref>, where each node is annotated to represent a specific method of joining the operands and of accessing the relations. Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc.
Reference: [15] <author> J. Srivastava and G. Elsesser, </author> <title> Query Optimization for Parallel Relational Databases, </title> <note> Preliminary version appeared in Proceedings of 1993 International Conference on Parallel and Distributed Information Systems. </note>
Reference-contexts: An important experimental result due to Hong [9] and Stonebraker [8] is that the optimal plan for a shared everything parallel machine is the best parallelization of the optimal plan for a sequential machine. As discussed in <ref> [4, 5, 15] </ref>, this result does not generalize to shared nothing parallel architectures. Ganguly, Hasan and Krishnamurthy [5] present a framework for optimizing queries for parallel machines. They observe that optimizing for parallel executions requires extending the System R dynamic programming algorithms to work with more than one dimension. <p> However, they do not present any specific practical method of designing the dimensions. Ganguly [4] presents a two dimensional search algorithm for optimizing in the presence of data dependencies without considering the problem of scheduling resources. The approach taken by Srivastava and Elsesser <ref> [15] </ref> is interesting for shared nothing architectures. They observe that it is not possible to design a good optimizer that does not take the design of a resource scheduler into account creating an apparent circular dependence. They break this circular dependence, by an exhaustive solution, which, however, is computationally expensive. <p> In a scenario where the relations do not overlap on resource units, this results in a 3-dimensional dynamic programming search algorithm. Our design solves the circular dependence problem referred to by Srivastava and Elsesser <ref> [15] </ref> without using an exhaustive search algorithm. 4. Overlap between the "home sites" of database relations increases the number of dimensions. We observe that symmetry in the overlap patterns can be exploited to keep the search algorithm efficient. <p> Each join operation may be a composition of several operations, for example, sorting and merging files, building a hash table and probing it etc. Given a join tree, an operator tree <ref> [4, 5, 8, 9, 12, 15] </ref> is an expansion of each node of the join tree into its constituent operations. Data dependency between adjacent operations in the operator tree is placed on the edges of the tree. We consider two kinds of data dependencies between operations: sequential and pipelined.
Reference: [16] <author> Eugene J. Shekita, Honesty C. Young and Kian-Lee Tan, </author> <title> Multi-Join Optimization for Symmetric Multiprocessors , Proceedings of the 1993 Conference on Very large Databases. </title>
Reference: [17] <author> Weining Wang, </author> <title> Scheduling and Optimizing Database Queries on Parallel Machines, </title> <type> PhD Dissertation, </type> <note> in preparation. </note>
Reference-contexts: All results in this paper can be applied to the case of superlinear speedups with a slight modification <ref> [17] </ref>. 5 1. The pattern of communication, whether one-to-few or all-to-all. 2. The nature of data dependency, whether sequential or pipelined. We discuss each of the above items below. Dependence of Communication Costs on Pattern of Communication. <p> These opera tions are candidates for resource assignment. For simplicity, the scheduling algorithms are presented for the case when an operator tree consists of floating operations only. However, the algorithms and their properties remain unchanged when the tree consists of rooted operations <ref> [17] </ref>. 5.3 Scheduling Independent and Pipelined Operators In this section, we describe an algorithm that assigns resources to a set of independent and pipelined operations. operations T i , 1 i nops. <p> This example also shows how the scheduling algorithm can be improved to give shorter schedules in the average case. The improved algorithms are discussed in <ref> [17] </ref>. 13 Example 2: Consider the problem of scheduling 6 independent tasks into 10 resource units, where the parallel computing area and the degree of parallelism of each task is tabulated below. <p> We propose a novel placement strategy, called the tree placement strategy, and demonstrate, by means of examples, how the number of dimensions in the search algorithm can be significantly reduced, thus increasing its efficiency. Future work involves evaluating the performance of the scheduler and the optimizer (and its variants) <ref> [17] </ref>. Better schedules may be obtained by clustering neighboring operations in an operator tree and is being studied in [17]. A complete algorithm for minimizing the dimensions for a tree placement scheme and other symmetric placement schemes may be found in [18]. <p> Future work involves evaluating the performance of the scheduler and the optimizer (and its variants) <ref> [17] </ref>. Better schedules may be obtained by clustering neighboring operations in an operator tree and is being studied in [17]. A complete algorithm for minimizing the dimensions for a tree placement scheme and other symmetric placement schemes may be found in [18]. Acknowledgements We thank Don Smith for helping us understand the communication model for parallel machines.
Reference: [18] <author> W. Wang, </author> <title> Symmetric Placement Strategies for Parallelizing Query Optimizers, </title> <type> Technical Report, </type> <note> in preparation. 29 </note>
Reference-contexts: Algorithms to find the minimum number of dimensions required to estimate RP T , given a tolerance margin of * is presented in <ref> [18] </ref>. Example 3: Consider TREE1 shown in Figure 10. <p> Better schedules may be obtained by clustering neighboring operations in an operator tree and is being studied in [17]. A complete algorithm for minimizing the dimensions for a tree placement scheme and other symmetric placement schemes may be found in <ref> [18] </ref>. Acknowledgements We thank Don Smith for helping us understand the communication model for parallel machines. We thank Tao Yang for useful comments on the section on coarse granularity and the model of communication.
References-found: 18

