URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P634.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: Efficient Management of Parallelism in Object-Oriented Numerical Software Libraries  
Author: Satish Balay William D. Gropp Lois Curfman McInnes Barry F. Smith 
Abstract: Parallel numerical software based on the message-passing model is enormously complicated. This paper introduces a set of techniques to manage the complexity, while maintaining high efficiency and ease of use. The PETSc 2.0 package uses object-oriented programming to conceal the details of the message passing, without concealing the parallelism, in a high-quality set of numerical software libraries. In fact, the programming model used by PETSc is also the most appropriate for NUMA shared-memory machines, since they require the same careful attention to memory hierarchies as do distributed-memory machines. Thus, the concepts discussed are appropriate for all scalable computing systems. The PETSc libraries provide many of the data structures and numerical kernels required for the scalable solution of PDEs, offering performance portability. 
Abstract-found: 1
Intro-found: 1
Reference: [ACM91] <author> Brett M. Averick, Richard G. Carter, and Jorge J. </author> <title> More. The MINPACK-2 test problem collection. </title> <type> Technical Report ANL/MCS-TM-150, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: One should either use PETSC DECIDE for m and n or ensure that m*n equals the number of processors in the communicator. We employ two-dimensional distributed arrays in the following example. Case 3: Consider the classic Bratu problem <ref> [ACM91] </ref>, 4u e u = 0; 0 &lt; x; y &lt; 1; with boundary conditions u = 0 for x = 0; x = 1; y = 0; y = 1; which is discretized by finite differences on a uniform m by m grid.
Reference: [ASS93] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for unstructured and block structured problems. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 578-587, </pages> <year> 1993. </year>
Reference-contexts: allowing the application programmer to achieve high performance, while the last two focus on ease of use of the libraries. * Performance overlapping communication and computation, determining within the library the details of various repeated communications, and optimizing the resulting message passing code (similar to the inspector/executor model of PARTI/Chaos <ref> [ASS93] </ref>), allowing the user to dictate exactly when certain communication is to occur, and allowing the user to aggregate data for subsequent communication * Ease of use allowing the user to work efficiently with parallel objects without specific regard for what portion of the data is stored on each processor, and <p> Finally, the processor waits for the posted nonblocking sends. Note that the construction of the VecScatter data structure is modeled on the inspector/executor ideas in PARTI/Chaos <ref> [ASS93] </ref>. We have decided to hide the creation of the VecScatter data structure in a single stage. To allow even more overlap of application computation and communication with the nonblocking sends and receives, one could break the context creation into two or three stages.
Reference: [BDV94] <author> Greg Burns, Raja Daoud, and James Vaigl. LAM: </author> <title> An open cluster environment for MPI. </title> <editor> In John W. Ross, editor, </editor> <booktitle> Proceedings of Supercomputing Symposium '94, </booktitle> <pages> pages 379-386. </pages> <institution> University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) [GLDS96b], [MPI94], [GLS94], [SOHL + 95], drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available <ref> [BDV94] </ref>, [GLDS96a]. Another advantage of MPI is that it is fully usable from Fortran 77, C, and C++; this feature allows programmers to use the language that is most appropriate for a particular task or with which they are most comfortable.
Reference: [BGMS95] <author> Satish Balay, William Gropp, Lois Curfman McInnes, and Barry Smith. </author> <title> PETSc 2.0 users manual. </title> <type> Technical Report ANL-95/11, </type> <institution> Argonne National Laboratory, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: This article is not intended as a users guide or introduction to the use of PETSc; for that information we refer readers to the PETSc users manual <ref> [BGMS95] </ref>. Rather, this article discusses in some technical detail several specific aspects that are important in the design of PETSc. <p> Both support the use of any of the linear solvers in a truncated Newton algorithm. Recently, we have added support for using backward Euler timestep-ping schemes. We plan to include additional higher order timestepping schemes in the future. All the solvers are introduced in the PETSc users manual <ref> [BGMS95] </ref>. 5.5 Object-Oriented Features of PETSc This article has touched on several object-oriented features of PETSc without providing a complete overview of the object-oriented design. In this section we focus on how object-oriented techniques are used to organize the overall software package.
Reference: [BGMS96] <author> Satish Balay, William Gropp, Lois Curfman McInnes, and Barry Smith. </author> <note> PETSc home page. http://www.mcs.anl.gov/petsc/petsc.html, December 1996. </note>
Reference-contexts: Object-oriented programming techniques enable us to manage the complexity of efficient numerical message-passing codes. All the PETSc software is freely available and used around the world in a variety of application areas <ref> [BGMS96] </ref>. Our approach does not attempt to completely conceal parallelism from the application programmer. Rather, the user initiates combinations of sequential and parallel phases of computations, but the library handles the detailed (data-structure dependent) message passing required during the coordination of the computations.
Reference: [BL96] <author> A. M. Bruaset and H. P. Langtangen. </author> <title> A Comprehensive set of Tools for Solving Partial Differential Equations: </title> <publisher> Diffpack. Birkhauser, </publisher> <year> 1996. </year>
Reference-contexts: Other researchers are also investigating object-oriented techniques and programming strategies for large-scale numerical software. A few of the projects that are most closely related to PETSc in the problems they address include Diffpack <ref> [BL96] </ref> (a collection of uniprocessor libraries for solving PDEs), Aztec [HST95] (a library for iteratively solving linear systems in parallel), and POOMA [RCH + 96] (a framework for parallel scientific simulation).
Reference: [GLDS96a] <author> William Gropp, Ewing Lusk, Nathan Doss, and Anthony Skjellum. </author> <title> A high-performace, portable implementation of the MPI message passing interface standard. </title> <journal> Parallel Computing, </journal> <volume> 22 </volume> <pages> 789-828, </pages> <year> 1996. </year>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) [GLDS96b], [MPI94], [GLS94], [SOHL + 95], drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available [BDV94], <ref> [GLDS96a] </ref>. Another advantage of MPI is that it is fully usable from Fortran 77, C, and C++; this feature allows programmers to use the language that is most appropriate for a particular task or with which they are most comfortable.
Reference: [GLDS96b] <author> William Gropp, Ewing Lusk, Nathan Doss, and Anthony Skjellum. </author> <note> MPICH home page. http://www.mcs.anl.gov/mpi/mpich/index.html, December 1996. </note>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) <ref> [GLDS96b] </ref>, [MPI94], [GLS94], [SOHL + 95], drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available [BDV94], [GLDS96a].
Reference: [GLS94] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) [GLDS96b], [MPI94], <ref> [GLS94] </ref>, [SOHL + 95], drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available [BDV94], [GLDS96a].
Reference: [HJ88] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers 2. </title> <editor> Adam Hilger, </editor> <year> 1988. </year>
Reference-contexts: The main spectrum includes common memory and bus shared by all processors, common memory connected to all processors through a switch, separate memory "owned" by one processor but directly accessible to all processors, and separate memory accessible only to its local processor <ref> [HJ88] </ref>. Each approach has both advantages and disadvantages. The common memory approach is limited by the ability of the memory banks to serve all processors efficiently, while the distributed-memory approach is limited by the need of all processors to share data.
Reference: [HST95] <author> Scott A. Hutchinson, John N. Shadid, and Ray S. Tuminaro. </author> <title> Aztec user's guide version 1.1. </title> <type> Technical Report SAND95/1559, </type> <institution> Sandia National Laboratories, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Other researchers are also investigating object-oriented techniques and programming strategies for large-scale numerical software. A few of the projects that are most closely related to PETSc in the problems they address include Diffpack [BL96] (a collection of uniprocessor libraries for solving PDEs), Aztec <ref> [HST95] </ref> (a library for iteratively solving linear systems in parallel), and POOMA [RCH + 96] (a framework for parallel scientific simulation). The unique aspect of PETSc compared with other packages is the complete integration of the six guiding principles throughout its design. This article is organized as follows.
Reference: [JP92] <author> Mark T. Jones and Paul E. Plassmann. BlockSolve v1.1: </author> <title> Scalable library software for the parallel solution of sparse linear systems. </title> <type> Technical Report ANL-92/46, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: a variety of Krylov subspace accelerators such as * the conjugate gradient method, * GMRES, * CGS, * Bi-CG-stab, and * transpose-free QMR, and a growing family of preconditioners, such as * block Jacobi, * overlapping additive Schwarz, 24 * ILU (0) and ICC (0) (through an interface to BlockSolve95 <ref> [JP92] </ref>), and * ILU (k) (currently sequential only). There are three main parallel sparse matrix formats: * CSR (compressed sparse row), * block CSR, and * storage by block diagonals. PETSc provides two main approaches to solving nonlinear algebraic systems (in parallel): line search techniques and trust region methods.
Reference: [JP93] <author> Mark T. Jones and Paul E. Plassmann. </author> <title> A parallel graph coloring heuristic. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(3) </volume> <pages> 654-669, </pages> <year> 1993. </year>
Reference-contexts: This may be done in one of two ways: color the underlying grid directly (this again is essentially a local operation), or use a parallel graph coloring algorithm (for example, <ref> [JP93] </ref>), to color the parallel sparse matrix directly. * Determine all the local computation and the communication that will be required for each color in the actual Jacobian approximation. This is an inspector step, and in PETSc it generates a data structure called MatFDColoring.
Reference: [MPI94] <author> MPI: </author> <title> A message-passing interface standard. </title> <journal> International J. Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) [GLDS96b], <ref> [MPI94] </ref>, [GLS94], [SOHL + 95], drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available [BDV94], [GLDS96a].
Reference: [MSGH84] <author> Jorge J. More, Danny C. Sorenson, Burton S. Garbow, and Kenneth E. Hillstrom. </author> <title> The MINPACK project. </title> <editor> In Wayne R. Cowell, editor, </editor> <booktitle> Sources and Development of Mathematical Software, </booktitle> <pages> pages 88-111, </pages> <year> 1984. </year>
Reference-contexts: In most applications the number of colors required can vary between 4 and 50. The use of coloring to compute Jacobians efficiently has been known for many years and is widely used on sequential machines, for example in the library MINPACK <ref> [MSGH84] </ref>. We discuss briefly how efficient Jacobian computation is performed in PETSc. * Compute the nonzero structure of the Jacobian; as indicated above, this is essentially a local computation. * Generate a coloring of the resulting matrix.
Reference: [NAS96] <institution> NAS Parallel Benchmarks home page. </institution> <address> http://www.nas.nasa.gov/NAS/NPB/- index.html, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: For representative parallel computation rates for a similar problem, we consider use of the block tridiagonal (BT) method in a three-dimensional Navier-Stokes simulation within category two of the NAS Parallel Benchmarks (NPB 2) <ref> [NAS96] </ref>. This method solves three sets of uncoupled systems of equations, each of which is block tridiagonal with 5 fi 5 blocks.
Reference: [RCH + 96] <author> J. V. W. Reynders, J. C. Cummings, P. J. Hinker, M. Tholburn, M. Srikant S. Banerjee, S. Karmesin, S. Atlas, K. Keahey, and W. F. Humphrey. POOMA: </author> <title> A FrameWork for Scientific Computing Applications on Parallel Architectures, </title> <type> chapter 14. </type> <year> 1996. </year>
Reference-contexts: A few of the projects that are most closely related to PETSc in the problems they address include Diffpack [BL96] (a collection of uniprocessor libraries for solving PDEs), Aztec [HST95] (a library for iteratively solving linear systems in parallel), and POOMA <ref> [RCH + 96] </ref> (a framework for parallel scientific simulation). The unique aspect of PETSc compared with other packages is the complete integration of the six guiding principles throughout its design. This article is organized as follows.
Reference: [SG96] <author> Barry F. Smith and William D. Gropp. </author> <title> The design of data-structure-neutral libraries for the iterative solution of sparse linear systems. </title> <journal> Scientific Programming, </journal> <volume> 5 </volume> <pages> 329-336, </pages> <year> 1996. </year>
Reference-contexts: These methods are relatively easy to code (since they merely require a matrix-vector product and several vector operations), and the data-structure-neutral implementations within PETSc are independent of underlying matrix and vector data structures <ref> [SG96] </ref>. We emphasize that this data-structure-neutral approach exposes mathematical details in a uniform fashion, without making unnecessary assumptions about the representation of mathematical objects. In particular, the parallelism within the Krylov subspace methods is handled completely within the vector and matrix modules of PETSc.
Reference: [SOHL + 95] <author> Marc Snir, Steve Otto, Steven Huss-Lederman, David Walker, and Jack Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Fortunately, with the development of the Message Passing Interface (MPI) [GLDS96b], [MPI94], [GLS94], <ref> [SOHL + 95] </ref>, drawback (1) is no longer a problem. MPI is an efficient, robust standard to which the major vendors are adhering. In addition, several high-quality implementations are freely available [BDV94], [GLDS96a]. <p> two processes could look like the following: Process 0 code Process 1 code ------------ ------------- MPI_Send (y,......); MPI_Recv (mess,....); x += mess; /* Add the remote data to x */ The Message Passing Interface (MPI) standard contains a wide variety of basic communication routines, including reductions, scatters, and broadcasts [MPI94],[GLS94], <ref> [SOHL + 95] </ref>. But these routines are predicated on both the sending and receiving processors being aware of the data's origin and destination. Consequently, writing complicated message-passing codes is tedious and prone to error.
Reference: [Thi93] <institution> Thinking Machines Corporation. </institution> <note> Users Manual for CM-Fortran. </note> <institution> Thinking Machines Corporation, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction Currently the only general-purpose, efficient, scalable approach to programming distributed-memory parallel systems is the message-passing model. Other approaches, based on parallel languages or compiler directives, have worked well on shared-memory computers, particular hardware platforms (e.g., CM-5) <ref> [Thi93] </ref>, or specific problems but have never been able to demonstrate general applicability.
Reference: [Tid95] <author> M. D. Tidriri. </author> <title> Krylov methods for compressible flows. </title> <type> Technical Report 95-48, </type> <institution> ICASE, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: As part of the conversion to PETSc, we have replaced the explicit boundary conditions with a fully implicit variant <ref> [Tid95] </ref>. The solution technique used for this problem is a Newton-Krylov approach with pseudo-transient continuation and adaptive advancement of the CFL number. Much of the approximate Jacobian matrix has block-band structure corresponding to the three-dimensional, seven-point stencil, with five degrees of freedom per node (three-dimensional momentum, internal energy, and density).
Reference: [WT91] <author> D. Whitfield and L. Taylor. </author> <title> Discretized Newton-relaxation solution of high resolution flux-difference split schemes. </title> <booktitle> In Proceedings of the AIAA Tenth Computational Fluid Dynamics Conference, </booktitle> <pages> pages 134-145, </pages> <year> 1991. </year> <month> 29 </month>
Reference-contexts: The case for which the following data was generated is transonic flow over an ONERA M6 wing, a standard test case originally coded (for a serial computer) by David Whitfield of Mississippi State University in a Newton-iterative formulation with low-CFL, pseudo-transient continuation and explicit enforcement of boundary conditions <ref> [WT91] </ref>. As part of the conversion to PETSc, we have replaced the explicit boundary conditions with a fully implicit variant [Tid95]. The solution technique used for this problem is a Newton-Krylov approach with pseudo-transient continuation and adaptive advancement of the CFL number.
References-found: 22

