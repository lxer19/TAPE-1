URL: http://www.cs.nyu.edu/cs/review95/yap.ps
Refering-URL: http://www.cs.nyu.edu/cs/review95/review95.html
Root-URL: http://www.cs.nyu.edu
Abstract: The theory group of the computer science department at NYU rates as one of the best. However, this group is rather unique in the way it has pioneered the use of deep, beautiful and applicable mathematics to many areas of computer science and related fields. Some notable examples are: parallel algorithms, robotics, CAD/CAM, symbolic computation and computational combinatorial and algebraic geometry. The following essay by Chee Yap, one of our premier theoreticians, shows this pioneering spirit while describing how better exact computation algorithms will help such areas as metrology and tolerancing in manufacturing sciences, robot surgery and physical simulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.S. Choi, J. Sellen, and C. Yap. </author> <title> "Approximate Euclidean shortest path in 3-space." </title> <booktitle> In 10th ACM Symposium on Computational Geometry, </booktitle> <pages> pages 41-48, </pages> <year> 1994. </year>
Reference-contexts: Here * &gt; 0 is an additional input parameter. This problem can be solved in polynomial time <ref> [1] </ref>. (2) One of the reasons why 3ESP may need exponential time is that distinguishing the shortest path from the next shortest path may require high precision. If the gap 0 between the lengths of the two paths is large, less precision may suffice.
Reference: [2] <author> J.S. Choi, J. Sellen, and C. Yap. </author> <title> "Precision-sensitive Euclidean shortest path in 3-Space." </title> <booktitle> In 11th ACM Symp. on Computational Geometry, </booktitle> <pages> pages 350-359, </pages> <year> 1995. </year>
Reference-contexts: If the gap 0 between the lengths of the two paths is large, less precision may suffice. Choi, Sellen and Yap <ref> [2] </ref> introduced the concept of a precision-sensitive algorithm for 3ESP to take advantage of this gap. This algorithm is practical in the sense that it could compute answers for some classes of inputs where previous approaches seem hopeless.
Reference: [3] <author> S. </author> <title> Fortune. </title> <booktitle> In Progress in Computational Geometry, chapter 3, </booktitle> <pages> pages 81-127. </pages> <note> Information Geometers, 1993. </note> <editor> Editor: R. </editor> <publisher> Martin. </publisher>
Reference-contexts: The initial approach taken by these researchers stayed within the fixed-precision computing paradigm: that is, assuming arithmetic is fixed-precision, devise geometric algorithms that are robust . This resulted in a series of research papers (see the surveys <ref> [3, 5] </ref>), which broadly speaking follow three basic approaches: (a) Introduce the geometric analogue of interval arithmetic. That is, replace classical geometric objects by the "thick objects" (points by suitable "balls", lines by "linear zones", etc).
Reference: [4] <author> D. Goldberg. </author> <title> "What every computer scientist should know about floating-point arithmetic." </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1) </volume> <pages> 5-48, </pages> <year> 1991. </year> <month> 6 </month>
Reference-contexts: Today, fixed-precision floating-point computation holds unquestioned dominance in computational science and engineering. Indeed, it is a given that high-performance machines have special hardware support for floating-point arithmetic (see <ref> [4] </ref>.) A major development in floating-point computation took place in the 1980s, when high-performance computer architectures proliferated. There are many possible implementations of floating-point arithmetic. Perhaps, consequently, practically no two architectures have compatible floating-point conventions. The result was a confusing non-portability of numerical code. <p> There are many possible implementations of floating-point arithmetic. Perhaps, consequently, practically no two architectures have compatible floating-point conventions. The result was a confusing non-portability of numerical code. This unacceptable state of affairs eventually led to the now widely accepted IEEE standard <ref> [4] </ref>. Kahan won the 1989 Turing award for his key role in this standard. Nevertheless, the achievements of this IEEE standard should be put in perspective: it simply ensures that numerical code performs consistently across machine architectures. It does little to remove the inevitable failures of such code.
Reference: [5] <author> C. Yap. </author> <title> "Towards exact geometric computation." </title> <booktitle> In Fifth Canadian Conference on Computational Geometry, </booktitle> <pages> pages 405-419, </pages> <address> Waterloo, Canada, </address> <month> August 5-9 </month> <year> 1993. </year> <note> Invited Lecture. To appear CGTA. </note>
Reference-contexts: The initial approach taken by these researchers stayed within the fixed-precision computing paradigm: that is, assuming arithmetic is fixed-precision, devise geometric algorithms that are robust . This resulted in a series of research papers (see the surveys <ref> [3, 5] </ref>), which broadly speaking follow three basic approaches: (a) Introduce the geometric analogue of interval arithmetic. That is, replace classical geometric objects by the "thick objects" (points by suitable "balls", lines by "linear zones", etc). <p> The philosophy here is to give priority to the former, at the expense of the latter. But ultimately, combinatorial consistency requires geometric theorem proving or simultaneous rounding: both tasks are generally infeasible. It appears that these approaches are unlikely to lead to a general solution for robust algorithms (see <ref> [5] </ref>). There is a growing consensus in some quarters of the computational geometry community that exact computation is the best general approach for achieving robustness. For a survey of recent papers using exact computation, see [6]. <p> All the problems treated in contemporary computational geometry are algebraic. Basically, algebraic problems can be solved exactly within single-exponential space (this is related to Tarski's theory of real closed fields). This framework is described in <ref> [5] </ref>. (II) What is the point of doing exact computation when the input data is inherently inexact? In other words, it seems silly to perform calculations with more precision than the input data. To answer this, it is important to make a distinction between "robustness" and "accuracy".
Reference: [6] <author> C. Yap and T. Dub e. </author> <title> "The exact computation paradigm." </title> <editor> In D.-Z. Du and F. K. Hwang, editors, </editor> <title> Computing in Euclidean Geometry. </title> <publisher> World Scientific Press, </publisher> <year> 1994. </year> <note> (To appear, 2nd edition). 7 </note>
Reference-contexts: There is a growing consensus in some quarters of the computational geometry community that exact computation is the best general approach for achieving robustness. For a survey of recent papers using exact computation, see <ref> [6] </ref>. Exact Geometric Computation Exact (Geometric) computation refers to the mode of algorithm design and implementation that assumes that input data is exact and demands that all combinatorial structures be computed exactly. This has two consequences. First, all branching tests in the algorithm are performed without error.
References-found: 6

