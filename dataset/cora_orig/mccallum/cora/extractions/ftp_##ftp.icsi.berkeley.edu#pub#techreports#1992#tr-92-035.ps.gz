URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-035.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Email: email: trompf@rcs.sel.de  
Title: Experiments with Noise Reduction Neural Networks for Robust Speech Recognition  
Author: Michael Trompf 
Note: 1.0 Introduction  
Address: 1947 Center Street, Berkeley, CA 94704 SEL ALCATEL, Dept. ZFZ/SC3, Lorenzstr. 10, 7000 Stuttgart 40, Germany,  
Affiliation: International Computer Science Institute,  
Date: May 1992  
Pubnum: TR-92-035,  
Abstract: S Speech recognition systems with small and medium vocabularies are used as natural human interface in a variety of real world applications. Though they work well in a laboratory environment, a significant loss in recognition performance can be observed in the presence of background noise. In order to make such a system more robust, the development of a neural network based noise reduction module is described in this paper. Based on function approximation techniques using multilayer feedforward networks (Hornik et al. 1990), this approach offers inherent nonlinear capabilities as well as easy training from pairs of corresponding noisy and noise-free signal segments. For the development of a robust nonadaptive system, information about the characteristics of the noise and speech components of the input signal and its past and future context is taken into account. Evaluation of each step is done by a word recognition task and includes experiments with changing signal parameters and Various methods have been developed for the enhancement of a noisy speech signal; for a list of references see e.g. Sorensen (1991). The choice of a particular method highly depends on the application at hand. The approach investigated in this work is to consider noise reduction as a continuous mapping of the noisy input data space to a space of noise-free output data. The optimal mapping function is unknown and can be continuous or discontinuous, linear or nonlinear and variant or invariant in time depending on the input signal characteristics and the complexity of the task. Hornik et al. (1990) and Hecht-Nielsen (1989) have shown that function approximation in high-dimensional spaces can be done by a three-layer feedforward neural network theoretically within any predefined mean squared error accuracy. The results reported from recent applications are encouraging: Tamura et al. (1988, 1989 and 1990) successfully trained a four-layer connectionist model for noise reduction on the speech signal waveform. As a result, they got improvements in listening tests as well as sources to test the robustness of this neural network based approach.
Abstract-found: 1
Intro-found: 0
Reference: <author> Kurkova V. </author> <title> (1991) Kolmogorovs Theorem Is Relevant. </title> <booktitle> Neural Computation 3, </booktitle> <pages> pp. </pages> <month> 617-622, </month> <title> The MIT Press Le Cun Y, Denker J, Solla S (1989) Optimal Brain Damage, </title> <booktitle> NIPS 1989, </booktitle> <volume> Vol 2, </volume> <pages> pp 598-605 Morgan N., </pages> <editor> Bourlard H. </editor> <title> (1989) Generalization and Parameter Estimation in Feedforward Nets: Some Experiments. International Computer Science Institute, Berkeley, TR-89-017 Nowlan S, Hinton G (1991) Simplifying Neural Networks by Soft Weight-Sharing. </title> <institution> Computational Neuroscience Laboratory, The Salk Institute, and Department of Computer Science, Univerity of Toronto. </institution>
Reference-contexts: The approximation capability of the network is closely related to Kolmogorovs superposition theorem, which states that any continuous function with multiple inputs is representable by sums and superpositions of continuous functions of only one variable <ref> (Kurkova 1991) </ref>. Because the mapping between vector pairs can be considered as a superposition of n mapping networks with n inputs and one single output for each coefficient, it is sufficient to look at only one of these networks (figure 1).

References-found: 1

