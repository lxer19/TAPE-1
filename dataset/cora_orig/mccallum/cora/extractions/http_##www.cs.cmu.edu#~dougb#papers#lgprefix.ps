URL: http://www.cs.cmu.edu/~dougb/papers/lgprefix.ps
Refering-URL: http://www.cs.cmu.edu/~dougb/research.html
Root-URL: 
Title: Link Grammar Prefix Measures for Spontaneous Speech Recognition  An interactive demonstration of the full-sentence robust parser  
Author: Doug Beeferman 
Web: URL "http://bobo.link.cs.cmu.edu/cgi-bin/grammar/build-intro-page.cgi".  
Note: Software implementations of the techniques described in this paper can be reviewed at the reader's request by emailing dougb+@cs.cmu.edu.  is online at the  
Date: December 20, 1995  
Abstract: We show how the link grammar formalism can be applied as a left-to-right syntax model to spontaneous speech recognition tasks. We offer algorithms for robustly measuring the grammaticality of partial sentences with respect to a link grammar and examine the measure's behavior over the Wall Street Journal and Switchboard corpora. We discuss why link grammar is useful for tackling the problems that conversational speech poses and suggest various language modeling strategies that exploit the described techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. E. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C. Lai, and R. L. Mercer. </author> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: Still, to approach the estimates of the entropy of English offered by Shannon [8]; Cover and King [2]; and others, it is clear that semantic 16 and pragmatic constraints need to be harnessed in addition to syntax and local lexical relationships <ref> [1] </ref>. Neither a syntax model nor a short-range stochastic model, nor any combination of the two, can tell us that cats don't bark.
Reference: [2] <author> T. M. Cover and R. King. </author> <title> A convergent gambling estimate of the entropy of English. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24 </volume> <pages> 413-421, </pages> <month> Mar </month> <year> 1978. </year>
Reference-contexts: Still, to approach the estimates of the entropy of English offered by Shannon [8]; Cover and King <ref> [2] </ref>; and others, it is clear that semantic 16 and pragmatic constraints need to be harnessed in addition to syntax and local lexical relationships [1]. Neither a syntax model nor a short-range stochastic model, nor any combination of the two, can tell us that cats don't bark.
Reference: [3] <author> J. Godfrey, E. Holliman, and J. McDaniel. </author> <title> Switchboard: Telephone speech corpus for research development. </title> <booktitle> In Proc. ICASSP-92, </booktitle> <pages> pages I-517-520, </pages> <year> 1992. </year>
Reference-contexts: When we survey the current offerings in spontaneous speech recognition we find efficient systems for limited domains such as the RTN-based Phoenix system [13] applied to the ATIS task, or inefficient systems for larger domains such as the GLR*-based JANUS system applied to the Switchboard telephone task <ref> [3] </ref>.
Reference: [4] <author> D. Grinberg, J. Lafferty, and D. Sleator. </author> <title> A robust parsing algorithm for link grammars. </title> <booktitle> In Proceedings of the Fourth International Workshop on Parsing Technologies, </booktitle> <address> Prague/Karlovy Vary, Czech Republic, </address> <year> 1995. </year> <note> Also issued as Technical Report CMU-CS-95-125, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: We explore a measure of prefix grammaticality confidence, called disjunct fanout, based on the geometric mean of disjuncts that survive disjunct pruning, and we compare this to the output of the robust parser for link grammar <ref> [4] </ref>. <p> This is compounded by the looser notion of grammaticality that speakers obey: even disfluency-free sentences do not necessarily parse. A common approach in robust parsing is to search for and extract syntax from the largest grammatical subset of the input, ignoring out-of-vocabulary words and, ideally, disfluencies. Grinberg, et al <ref> [4] </ref> have devised a robust parsing algorithm for link grammar that allows islands of connected components in an unconnected parse candidate to be joined by null links. Null links can connect any adjacent word pair, even out-of-vocabulary words, making extraction of grammatical structure possible for any input string.
Reference: [5] <author> J. Lafferty, D. Sleator, and D. Temperley. </author> <title> Grammatical tri-grams: A probabilistic model of link grammar. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <address> Cambridge, MA, </address> <month> October </month> <year> 1992. </year> <note> Also issued as Technical Report CMU-CS-92-181, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Such a model, interpolated with a purely lexical short-range stochastic model, could capture everything offered by the latter with the added benefit of long-distance syntax dependencies. By contrast, a generative model, suggested by Lafferty, et al <ref> [5] </ref>, views a parse in terms of the links that are used and which words they connect, and models a parse probability as the product of the constituent link probabilities.
Reference: [6] <author> A. Lavie and M. Tomita. </author> <title> GLR fl : An efficient noise-skipping parsing algorithm for context free grammars. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies, </booktitle> <pages> pages 123-134, </pages> <year> 1993. </year>
Reference-contexts: The second approach often requires that the lattice be pared down or split into manageable sub-lattices, as in the case of GLR* <ref> [6] </ref>, and besides losing information, these algorithms become grossly inefficient when either dimension of the lattice becomes large.
Reference: [7] <author> K.F. Lee, H.W. Hon, and R. Reddy. </author> <title> An overview of the SPHINX speech recognition system. </title> <journal> Journal of Acoustics, Speech, and Signal Processing, </journal> <volume> 38(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: Both applications sacrifice some accuracy by loosely coupling the recognition and parsing stages [14]. 2 Given infinite computing resources, it would seem desirable for the syntactic processing of the utterance to parallel the approach to language model integration pioneered by the Sphinx system <ref> [7] </ref>, in which linguistic knowledge is applied as soon as possible during the recognition search. This allows unlikely states to be pruned earlier, saving precious state space for more likely candidates.
Reference: [8] <author> C. E. Shannon. </author> <title> Prediction and entropy of printed English. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: Still, to approach the estimates of the entropy of English offered by Shannon <ref> [8] </ref>; Cover and King [2]; and others, it is clear that semantic 16 and pragmatic constraints need to be harnessed in addition to syntax and local lexical relationships [1]. Neither a syntax model nor a short-range stochastic model, nor any combination of the two, can tell us that cats don't bark.
Reference: [9] <author> D. D. K. Sleator and D. Temperley. </author> <title> Parsing English with a link grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> 5000 Forbes Avenue, Pitts-burgh, PA 15213, </address> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Link grammar, a formalism based on word-pair relationships introduced by Sleator and Temperley <ref> [9] </ref>, is gaining attention in the speech recognition community because it addresses the first two issues: It is straightforward and inexpensive, relative to other models, to create wide-coverage link grammars, and there are efficient and robust parsing algorithms that process them. <p> Each line consists of words that share a common definition. Linkage requirements are specified in terms of the connector names and operators, including conjunction ("&"), disjunction ("or"), and optional ("fg"). Matching rules and details of the dictionary syntax are described in <ref> [9] </ref>. <p> The reader is referred to <ref> [9] </ref> for details on the pseudocode. left [d] and right [d] denote the first connector in a linked list of connectors representing the left and right sides of a disjunct d. <p> The full-sentence parser's pruning algorithm also quickly checks a number of potential violations of the connectivity and planarity requirements before proceeding, the specifics of which are outlined in <ref> [9] </ref>. These too can be incorporated into prefix pruning, though less effectively, with one left-to-right pass. Since prefix pruning is necessarily weaker than the standard full-sentence pruning algorithm, many more disjuncts remain after this step. <p> Since prefix pruning is necessarily weaker than the standard full-sentence pruning algorithm, many more disjuncts remain after this step. At a minimum there are as many prefix parses as the product of 5 Actually there are two stages, called "pruning" and "power pruning", in <ref> [9] </ref>, but we will consolidate them in this paper since the latter stage subsumes the former in effect. 10 the counts, for each word, of disjuncts that have empty left connector lists|the cases in which all right-connectors dangle.
Reference: [10] <author> A. Stolcke. </author> <title> An efficient probabilistic context-free parsing algo-rithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, 1947 Center St.. Berkeley, </institution> <address> CA 94704, </address> <year> 1993. </year> <month> Revised November </month> <year> 1994. </year>
Reference-contexts: Even after pruning, the strict prefix parsing algorithm is too slow for the critical path, no better than incremental versions of its CFG <ref> [10] </ref> and RTN brethren. In many speech applications it is necessary or useful to know the syntactic structure of the final output hypothesis, but for transcription tasks, and in the absence of a parse disambiguation or scoring mechanism, it is wasteful to spend search time resolving structure.
Reference: [11] <author> S. Vempala. </author> <title> Application of the master theorem for solving recurrence relations. </title> <type> Personal correspondence, </type> <month> December </month> <year> 1995. </year>
Reference-contexts: of whether there exists a prefix parse, may be computed more efficiently than inspecting the return value of Prefix-Parse by modifying PrefixCount to return true as soon as the total becomes nonzero, and false only at the end of the routine. 4.3 Efficiency considerations PrefixParse has exponential worst-case running time <ref> [11, 12] </ref> without memoization. Even after applying optimizations analogous to the tricks used in the full-sentence parser it is much slower than Parse| there is so much more to count.
Reference: [12] <author> S. Vempala. </author> <title> On the exponential running time of PrefixCount. </title> <address> Barroom brawl, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: of whether there exists a prefix parse, may be computed more efficiently than inspecting the return value of Prefix-Parse by modifying PrefixCount to return true as soon as the total becomes nonzero, and false only at the end of the routine. 4.3 Efficiency considerations PrefixParse has exponential worst-case running time <ref> [11, 12] </ref> without memoization. Even after applying optimizations analogous to the tricks used in the full-sentence parser it is much slower than Parse| there is so much more to count.
Reference: [13] <author> W. Ward. </author> <title> Understanding spontaneous speech: </title> <booktitle> the Phoenix system. In Proc. ICASSP-91, </booktitle> <pages> pages I-365-367, </pages> <year> 1991. </year>
Reference-contexts: When we survey the current offerings in spontaneous speech recognition we find efficient systems for limited domains such as the RTN-based Phoenix system <ref> [13] </ref> applied to the ATIS task, or inefficient systems for larger domains such as the GLR*-based JANUS system applied to the Switchboard telephone task [3].
Reference: [14] <author> W. Ward. </author> <title> Extracting information in spontaneous speech. </title> <booktitle> Presented at the International Conference for Spoken language processing, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: Both applications sacrifice some accuracy by loosely coupling the recognition and parsing stages <ref> [14] </ref>. 2 Given infinite computing resources, it would seem desirable for the syntactic processing of the utterance to parallel the approach to language model integration pioneered by the Sphinx system [7], in which linguistic knowledge is applied as soon as possible during the recognition search.
Reference: [15] <author> S.R. Young, Alexander G. Hauptmann, Wayne H. Ward, Ed-ward T. Smith, and Philip Werner. </author> <title> High level knowledge sources in usable speech recognition systems. </title> <journal> Communications of the ACM, </journal> <volume> 32(2), </volume> <month> February </month> <year> 1989. </year> <month> 18 </month>
Reference-contexts: 1 Introduction Speech recognition language models based only on local lexical dependencies can be improved by incorporating more grammatical structure, either from longer-range lexical relationships or syntactic constraints <ref> [15] </ref>. <p> The first approach has failed to improve recognition error rates significantly, and has often hurt <ref> [15] </ref>. Furthermore, it is still very slow in the cases of large N-best lists, as per-hypothesis parsing fails to exploit the fact that successive entries in the list often differ by as little as a single function word.
References-found: 15

