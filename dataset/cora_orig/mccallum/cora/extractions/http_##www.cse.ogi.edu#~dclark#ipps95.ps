URL: http://www.cse.ogi.edu/~dclark/ipps95.ps
Refering-URL: http://www.cse.ogi.edu/~dclark/
Root-URL: http://www.cse.ogi.edu
Email: walpole-@cse.ogi.edu  
Title: Scheduling of Parallel Jobs on Dynamic, Heterogenous NetworksJanuary  Scheduling of Parallel Jobs on Dynamic, Heterogenous Networks  
Author: Dan L. Clark, Jeremy Casas, Steve W. Otto, Robert M. Prouty, Jonathan Walpole -dclark, casas, otto, prouty, 
Web: http://www.cse.ogi.edu/DISC/projects/cpe/  
Date: 1995 1  January 31, 1995  
Pubnum: 31,  
Abstract: In using a shared network of workstations for parallel processing, it is not only important to consider heterogeneity and differences in processing power between the workstations but also the dynamics of the system as a whole. In such a computing environment where the use of resources vary as other applications consume and release resources, intelligent scheduling of the parallel jobs onto the available resources is essential to maximize resource utilization. Despite this realization, however, there are few systems available that provide an infrastructure for the easy development and testing of these intelligent schedulers. In this paper, an infrastructure is presented together with a scheduler that is capable of both gang scheduling and dynamic task reallocation of PVM applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, and R. J. M. V. S. Sunderam. </author> <title> Heterogeneous network computing. </title> <booktitle> In Sixth SIAM Conference on Parallel Processing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: These workstation networks are not only cheaper, but also provide a general-purpose computing environment that is typically shared by both parallel and non-parallel application developers and users. Software systems such as the Parallel Virtual Machine (PVM) <ref> [1, 2, 3] </ref> provide an infrastructure where such networks of workstations could be viewed by an applications developer as a large distributed-memory multi-processor machine.
Reference: [2] <author> J. J. Dongarra, A. Geist, R. J. Manchek, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <booktitle> Computers in Physics, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: These workstation networks are not only cheaper, but also provide a general-purpose computing environment that is typically shared by both parallel and non-parallel application developers and users. Software systems such as the Parallel Virtual Machine (PVM) <ref> [1, 2, 3] </ref> provide an infrastructure where such networks of workstations could be viewed by an applications developer as a large distributed-memory multi-processor machine.
Reference: [3] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, R. J. Manchek, S. W. Otto, and J. Walpole. </author> <title> PVM: Experiences, current status and future direction. </title> <booktitle> In Supercomputing 93 Proceedings, </booktitle> <pages> pages 7656, </pages> <year> 1993. </year>
Reference-contexts: These workstation networks are not only cheaper, but also provide a general-purpose computing environment that is typically shared by both parallel and non-parallel application developers and users. Software systems such as the Parallel Virtual Machine (PVM) <ref> [1, 2, 3] </ref> provide an infrastructure where such networks of workstations could be viewed by an applications developer as a large distributed-memory multi-processor machine.
Reference: [4] <author> J. Casas, R. Konuru, S. Otto, R. Prouty, and J. Walpole. </author> <title> Adaptive load migration systems for PVM. </title> <booktitle> In Supercomputing 94 Proceedings, </booktitle> <pages> pages 390399, </pages> <address> Washington D.C, </address> <month> November 14-18 </month> <year> 1994. </year>
Reference-contexts: In this paper, an infrastructure for developing and testing intelligent schedulers is defined. A prototype implementation of the infrastructure based on the Migratable PVM (MPVM) <ref> [4] </ref> framework is presented together with a scheduler that is capable of deciding on task to workstation allocations based on workstation availability, non-uniform processing power, gang scheduling of multiple parallel jobs, and dynamic reallocation of tasks as resource utilization varies.
Reference: [5] <author> J. Casas, D. Clark, R. Konuru, S. Otto, R. Prouty, and J. Walpole. MPVM: </author> <title> A migration transparent version of PVM. </title> <type> Technical report, </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <year> 1995. </year>
Reference-contexts: The new task then continues execution based on the received state. In addition to automating the migration of task state, MPVM also provides task id virtualization and message forwarding/sequencing to ensure that messages are not lost and are received in the correct order <ref> [5] </ref>. Just like standard PVM, MPVM requires a daemon to be running on each node in the system and applications to be linked with the MPVM message passing library.
Reference: [6] <author> K. A. Saqabi, S. W. Otto, and J. Walpole. </author> <title> Gang scheduling in heterogenous distributed systems. </title> <type> Technical report, </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <year> 1994. </year>
Reference-contexts: Time slice 3 has available capacity on nodes mozart and verdi. Scheduling of Parallel Jobs on Dynamic, Heterogenous NetworksJanuary 31, 1995 9 The determination of the mapping between the tasks and the nodes on the network is accomplished by implementing the scheduling algorithm of Al-Saqabi <ref> [6] </ref>. The algorithm takes three basic steps to mathematically determine the allocation of tasks to nodes: distribute, compress and pack. The first step determines hypothetically how fast the job will run given its own time slice distributing the tasks proportionally across all the nodes.
Reference: [7] <author> R. J. Manchek, </author> <note> PVM version 3.3.0 release-notes, June 1994. Anonymous ftp site netlib2.cs.utk.edu directory /pvm3. </note>
Reference-contexts: constrained by the homogenous boundaries imposed by process migration and the need to fix some tasks to nodes but does not change the mathematical foundation of the task placement. 5 Related Work The Parallel Virtual Machine defines an interface for specifying a resource manager, conveniently separating scheduling mechanism from policy <ref> [7] </ref>. The interface clears the way for the development and testing of different scheduling algorithms for use with the PVM system, and is used in our prototype infrastructure. As a resource manager, it receives service requests/information that could affect scheduling decisions like spawn requests or task termination.
Reference: [8] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka. Condor A hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In using this interface for our prototype infrastructure/ scheduler, the interface was extended to included messages regarding load information and support for task migration which was available through MPVM. The work at the University of Wisconsin-Madison by Livny <ref> [8, 9] </ref> integrating the scheduling mechanisms of Condor with PVM provided the impetus for the creation of a well defined interface for resource management which is now part of the standard release of PVM. Condor supports process migration of sequential jobs.
Reference: [9] <author> J. Pruyne and M. Livny. </author> <title> Providing resource management services to parallel applications. </title> <booktitle> In Proceeding of the 2nd workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 152161, </pages> <year> 1995. </year>
Reference-contexts: In using this interface for our prototype infrastructure/ scheduler, the interface was extended to included messages regarding load information and support for task migration which was available through MPVM. The work at the University of Wisconsin-Madison by Livny <ref> [8, 9] </ref> integrating the scheduling mechanisms of Condor with PVM provided the impetus for the creation of a well defined interface for resource management which is now part of the standard release of PVM. Condor supports process migration of sequential jobs.
Reference: [10] <author> B. C. Neuman and S. Rao. </author> <title> The prospero resource manager: A scalable framework for processor allocation in distributed systems. In Concurrency: </title> <journal> Practice and Experience, </journal> <volume> volume 6(4), </volume> <pages> pages 339355. </pages> <publisher> Wiley, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: In our infrastructure, we have extended this capability, with the use of MPVM, to support migration of tasks. Neuman specifies the a framework for scalable resource management on distributed system using a system manager, job manager, and a node manager for the Prospero system <ref> [10] </ref>. The system manager is responsible for allocation of resources among multiple jobs leaving the job manager to assign the specific tasks of a job to a node. The allocation of system wide resources is combined into a single function under in our infrastructure.
Reference: [11] <author> A. Barak, S. Guday, and R. G. Wheeler. </author> <title> The MOSIX Distributed Operating System Load Balancing for Unix. </title> <booktitle> Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The node manager in Prospero is responsible for task management, communication and load monitoring which is divided between the MPVM daemon and a new load manager function in our system. The scheduling mechanism of the MOSIX <ref> [11] </ref> operating system utilizes load values and normalized CPU capacity to determine the location for task placement in a distributed system. The system was implemented by modifying the Berkeley 4.3 kernel which allows the system to provide the normal UNIX interface for task invocation.
Reference: [12] <author> T. Green, R. Pennington, and D. Reynolds. </author> <title> Distributed Queueing System Version 2.1 Release Notes, </title> <month> March </month> <year> 1993. </year> <note> Release Notes. </note>
Reference-contexts: By not building upon mechanisms tied to specific operating systems or even hardware, in general, our infrastructure can be used on a larger set of systems/machines. The Distributed Queueing System (DQS) from Florida State University provides support for batch scheduling of multiple jobs across a heterogenous network <ref> [12] </ref>. DQS also supports parallel jobs under PVM by starting up a new set of PVM daemons at the beginning of a job and shutting down the PVM daemons at job termination adding the overhead of the PVM system initiation to the cost of running the job.
References-found: 12

