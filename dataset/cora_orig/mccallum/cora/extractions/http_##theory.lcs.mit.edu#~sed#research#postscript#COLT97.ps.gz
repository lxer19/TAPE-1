URL: http://theory.lcs.mit.edu/~sed/research/postscript/COLT97.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/
Root-URL: 
Title: Computational Sample Complexity  
Author: Scott E. Decatur Oded Goldreich Dana Ron 
Keyword: Computational Learning Theory, Information vs. Efficient Computation, Pseudo Random Functions, Error Correcting Codes, Wire-Tap Channel.  
Address: Rehovot, Israel.  545 Technology Sq., Cambridge, MA 02139.  
Affiliation: Department of Computer Science and Applied Mathematics, Weizmann Institute of Science,  MIT. Laboratory for Computer Science, MIT,  
Note: An extended abstract of this work has appeared in the proceedings of the 10th COLT, 1997. DIMACS Center, Rutgers University, Piscataway, NJ 08855. Part of this work was done while the author was at the Laboratory for Computer Science, MIT, supported by a grant from the Reed Foundation.  On sabbatical leave at LCS,  Supported by an NSF postdoctoral fellowship.  
Email: E-mail: oded@wisdom.weizmann.ac.il.  E-mail: danar@theory.lcs.mit.edu.  
Date: June 23, 1997  
Abstract: In a variety of PAC learning models, a tradeoff between time and information seems to exist: with unlimited time, a small amount of information suffices, but with time restrictions, more information sometimes seems to be required. In addition, it has long been known that there are concept classes that can be learned in the absence of computational restrictions, but (under standard cryptographic assumptions) cannot be learned in polynomial time regardless of sample size. Yet, these results do not answer the question of whether there are classes for which learning from a small set of examples is infeasible, but becomes feasible when the learner has access to (polynomially) more examples. To address this question, we introduce a new measure of learning complexity called computational sample complexity which represents the number of examples sufficient for polynomial time learning with respect to a fixed distribution. We then show concept classes that (under similar cryptographic assumptions) possess arbitrary sized gaps between their standard (information-theoretic) sample complexity and their computational sample complexity. We also demonstrate such gaps for learning from membership queries and learning from noisy examples. 
Abstract-found: 1
Intro-found: 1
Reference: [AD96] <author> J. Aslam and S. Decatur. </author> <title> On the sample complexity of noise-tolerant learning. </title> <journal> Information Processing Letters, </journal> <volume> 57 </volume> <pages> 189-195, </pages> <year> 1996. </year>
Reference-contexts: In the absence of computational restrictions, any PAC-learnable class can also be learned in the presence of classification noise rate = 1=2fl &lt; 1=2 using a factor of fi (1=fl 2 ) more examples than the noise free case [Lai88, Tal94]. This increase is information theoretically required <ref> [Sim93, AD96] </ref>. Yet for many classes, when computation is restricted in the presence of noisy data, the sample complexity of known algorithms is increased by more than fi (1=fl 2 ).
Reference: [AK91] <author> D. Angluin and M. Kharitonov. </author> <booktitle> When won't membership queries help? In Proceedings of the Twenty-Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 444-453, </pages> <year> 1991. </year>
Reference-contexts: It has long been known that there are concept classes, containing only concepts which are implementable by "small" Boolean circuits, which can be learned in the absence of computational restrictions, yet cannot be learned (using any hypothesis class) in polynomial time (under standard cryptographic assumptions) <ref> [Val84, KV94, AK91, Kha93] </ref>. Yet, these results do not answer the question of whether there are classes for which learning from a small set of examples is infeasible, but becomes feasible when the learner has access to (polynomially) more examples.
Reference: [Ang87] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: One such example is the learning of deterministic finite automata (DFAs). Angluin's algorithm for this class <ref> [Ang87] </ref> can be viewed as drawing the standard Occam-sized sample and outputting a DFA consistent with it. But in order to efficiently find this consistent DFA, the algorithm makes many additional membership queries.
Reference: [BBCM95] <author> C.H. Bennett, G. Brassard, C. Crepeau, and U. Maurer. </author> <title> Generalized privacy amplification. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 41(6) </volume> <pages> 1915-1923, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Crepeau (private comm., April 1997) has informed us that, using the techniques in <ref> [BBCM95, CM97] </ref>, one may obtain an alternative efficient solution to the original Wire-Tap Channel Problem again by using bi-directional communiaction. 13 The crossover probability is the probability that a bit is complemented in the transmission process. 9 3 Proof of Theorem 1.1 We start by describing a construction which satisfies the
Reference: [BEHW87] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Such a phenomenon seems to be present in various learning problems (described below) and we focus on this tradeoff between information and computation. The most common method of learning from examples in the PAC setting is through the use of Occam algorithms <ref> [BEHW87, BEHW89] </ref>. These are algorithms which take as input a set of labeled examples and output a concept from the target class which is consistent with the given set of examples. <p> The slice consists entirely of k (n; 0:1) blocks of equal length, each repeating the corresponding bit of u. The purpose of this slice is to dominate the (information theoretic) sample complexity, and allow us to easily derive tight bounds on it. Information Theoretic Bounds. Applying Occam's Razor <ref> [BEHW87] </ref> to the class C, we obtain itsc (C; n; 0:1) = O (log jC n j) = O (n + O (n) + k (n; 0:1)) = O (k (n; 0:1)) where the last equality is due to k (n; 0:1) &gt; n. <p> Item 2 follows. Lemma 3.1 The concept class described above has (information theoretic) sample complexity itsc (n; *) = fi (k (n; *)). Proof: Clearly, it suffices to learn each of the three slices with error *. Applying Occam's Ra zor <ref> [BEHW87] </ref> to Slices I and II of the class, we obtain itsc (Slices I and II; n; *) = O (n 2 =*) = O (k (n; *)) where the last equality is due to the hypothesis regarding the function k (; ) (i.e., that it is (n 2 =*)). <p> By Occam's Razor <ref> [BEHW87] </ref>, the above concept class has IT SC (n; *) = O (n=*).
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-865, </pages> <year> 1989. </year>
Reference-contexts: Such a phenomenon seems to be present in various learning problems (described below) and we focus on this tradeoff between information and computation. The most common method of learning from examples in the PAC setting is through the use of Occam algorithms <ref> [BEHW87, BEHW89] </ref>. These are algorithms which take as input a set of labeled examples and output a concept from the target class which is consistent with the given set of examples.
Reference: [BM84] <author> M. Blum and S. Micali. </author> <title> How to generate cryptographically strong sequences of pseudorandom bits. </title> <journal> SIAM J. Comput., </journal> <volume> 13 </volume> <pages> 850-864, </pages> <year> 1984. </year> <note> Preliminary version in 23rd FOCS, </note> <year> 1982. </year>
Reference-contexts: Alternatively, the theorem will hold provided k (n; *) = (n 2 log n). 6 Proof of Theorem 1.5 Here we merely use a pseudorandom generator <ref> [BM84, Yao82] </ref>. Specifically, we need a generator, G, which stretches seeds of length n into sequences of length p (n). The concept class, C = fC n g, will correspond to all possible choices of a seed for the generator.
Reference: [BW86] <author> E. Berlekamp and L. Welch. </author> <title> Error correction of algebraic block codes. </title> <type> US Patent 4,633,470, </type> <year> 1986. </year>
Reference-contexts: Using the Berlekamp-Welch Algorithm <ref> [BW86] </ref>, one can efficiently retrieve the information from a codeword provided that at most t of the symbols (i.e., the values at field elements) were corrupted.
Reference: [CK78] <author> I. Csiszar and J. Korner. </author> <title> Broadcast channels with confidential messages. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 24 </volume> <pages> 339-348, </pages> <year> 1978. </year>
Reference-contexts: Efficient coding for the Wire-Tap Channel Problem: Using Theorem 2.1, we obtain an efficient coding scheme for (a strong version of ) the Wire-Tap Channel Problem (cf., [Wyn75]). Actually, we consider a seemingly harder version introduced by Csiszar and Korner <ref> [CK78] </ref>.
Reference: [CM97] <author> C. Cachin and U.M. Maurer. </author> <title> Linking information reconciliation and privacy amplification. </title> <journal> Journal of Cryptology, </journal> <volume> 10(2) </volume> <pages> 97-110, </pages> <year> 1997. </year> <month> 23 </month>
Reference-contexts: Crepeau (private comm., April 1997) has informed us that, using the techniques in <ref> [BBCM95, CM97] </ref>, one may obtain an alternative efficient solution to the original Wire-Tap Channel Problem again by using bi-directional communiaction. 13 The crossover probability is the probability that a bit is complemented in the transmission process. 9 3 Proof of Theorem 1.1 We start by describing a construction which satisfies the
Reference: [CT91] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In particular, we claim that we must have at least (p (n)=fl 2 ) examples in the i th block. We prove the claim by noting that it corresponds to the classical Information Theoretic measure of the mutual information (cf., <ref> [CT91] </ref>) that these samples provide about the string residing in this block. Each example provides information on a single bit residing in the block and the amount of information is merely the capacity of a Binary Symmetric Channel with crossover probability = 1 2 fl.
Reference: [EHKV89] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: This bound depends on the PAC accuracy and confidence parameters and the Vapnik-Chervonenkis dimension (VC-Dimension) [VC71] of the target class. The general lower bound on the number of examples required for learning <ref> [EHKV89] </ref> nearly matches (within a logarithmic factor) the upper bound for Occam algorithms. Thus, the sample complexity for learning is essentially tight when we have an algorithm which finds a consistent concept from the target class.
Reference: [GGM86] <author> O. Goldreich, S. Goldwasser, and S. Micali. </author> <title> How to construct random functions. </title> <journal> Jour. of the ACM, </journal> <volume> 33(4) </volume> <pages> 792-807, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: In the above, and all subsequent theorems, one-way functions are merely used to construct pseudorandom functions <ref> [HILL, GGM86] </ref>. Assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure, one can construct pseudorandom functions in N C (cf., [NR95]), and so all of our "gap theorems" will follow with concept classes having N C circuits. <p> We note that the computational sample complexity under the uniform distribution for this class is fi (p (n) minflog (1=*); log p (n)g). Techniques. The basic idea is to consider concepts which consist of two parts: The first part of the concept is determined by a pseudorandom function (cf., <ref> [GGM86] </ref>), while the second part encodes the seed of such a function. Since it is infeasible to infer a pseudorandom function, the computational-bounded learner is forced to retrieve the seed of the function which is sparsely encoded in the second part. <p> Definition 2.1 (pseudorandom functions <ref> [GGM86] </ref>): Let ` : N 7! N be a polynomially-bounded length function, and F = fF n : n 2 N g where F n = ff ff : ff 2 f0; 1g n g is a (multi)set of 2 n Boolean functions over the domain f0; 1g `(n) . <p> oracle machine M , every positive polynomial p and all sufficiently large n's fi fi 1 where R n denotes the set of all (2 2 `(n) ) Boolean functions over the domain f0; 1g `(n) . 6 Pseudorandom functions exists if and only if there exist one-way functions (cf., <ref> [GGM86] </ref> and [HILL]).
Reference: [HILL] <author> J. H-astad, R. Impagliazzo, L.A. Levin, and M. Luby. </author> <title> Construction of pseudorandom generator from any one-way function. </title> <note> To appear in SIAM J. on Computing . Preliminary versions by Impagliazzo et. </note> <editor> al. </editor> <booktitle> in 21st STOC (1989) and H-astad in 22nd STOC (1990). </booktitle>
Reference-contexts: In the above, and all subsequent theorems, one-way functions are merely used to construct pseudorandom functions <ref> [HILL, GGM86] </ref>. Assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure, one can construct pseudorandom functions in N C (cf., [NR95]), and so all of our "gap theorems" will follow with concept classes having N C circuits. <p> M , every positive polynomial p and all sufficiently large n's fi fi 1 where R n denotes the set of all (2 2 `(n) ) Boolean functions over the domain f0; 1g `(n) . 6 Pseudorandom functions exists if and only if there exist one-way functions (cf., [GGM86] and <ref> [HILL] </ref>).
Reference: [Jus72] <author> J. Justesen. </author> <title> A class of constructive asymptotically good alegbraic codes. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 18 </volume> <pages> 652-656, </pages> <year> 1972. </year>
Reference-contexts: In addition, on input x, algorithm E uses O (jxj) coin tosses. Items 1 and 2 are standard requirements of Coding Theory, first met by Justesen <ref> [Jus72] </ref>. What is non-standard in the above is Item 3. Indeed, Item 3 is impossible if one insists that the encoding algorithm (i.e., E) be deterministic. <p> What is missing in the above is a specific construction satisfying the hypothesis as well as allowing efficient decoding. Such a construction can be obtained by mimicking Justesen's construction <ref> [Jus72] </ref>. <p> Specifically, x i is encoded by the product of the Toeplitz matrix with the vector x i y i . Clearly, we preserve the error-correcting features of Justesen's construction <ref> [Jus72] </ref>. The Secrecy condition is shown analogously to the way in which the Error Correction feature is established in [Jus72]. <p> Specifically, x i is encoded by the product of the Toeplitz matrix with the vector x i y i . Clearly, we preserve the error-correcting features of Justesen's construction <ref> [Jus72] </ref>. The Secrecy condition is shown analogously to the way in which the Error Correction feature is established in [Jus72]. Specifically, we consider the partition of the codeword into consecutive 4`-bit long 8 Our presentation of Justesen's Code is inaccurate but suffices for our purposes. 9 Here we assume that 3t is a prime power. Otherwise, we use the first prime power greater than 3t.
Reference: [Kha93] <author> M. Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <year> 1993. </year>
Reference-contexts: It has long been known that there are concept classes, containing only concepts which are implementable by "small" Boolean circuits, which can be learned in the absence of computational restrictions, yet cannot be learned (using any hypothesis class) in polynomial time (under standard cryptographic assumptions) <ref> [Val84, KV94, AK91, Kha93] </ref>. Yet, these results do not answer the question of whether there are classes for which learning from a small set of examples is infeasible, but becomes feasible when the learner has access to (polynomially) more examples.
Reference: [KV94] <author> M. J. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year> <note> An extended abstract of this paper appeared in STOC89. </note>
Reference-contexts: It has long been known that there are concept classes, containing only concepts which are implementable by "small" Boolean circuits, which can be learned in the absence of computational restrictions, yet cannot be learned (using any hypothesis class) in polynomial time (under standard cryptographic assumptions) <ref> [Val84, KV94, AK91, Kha93] </ref>. Yet, these results do not answer the question of whether there are classes for which learning from a small set of examples is infeasible, but becomes feasible when the learner has access to (polynomially) more examples.
Reference: [Lai88] <author> P. D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: In the absence of computational restrictions, any PAC-learnable class can also be learned in the presence of classification noise rate = 1=2fl &lt; 1=2 using a factor of fi (1=fl 2 ) more examples than the noise free case <ref> [Lai88, Tal94] </ref>. This increase is information theoretically required [Sim93, AD96]. Yet for many classes, when computation is restricted in the presence of noisy data, the sample complexity of known algorithms is increased by more than fi (1=fl 2 ). <p> We refer to the resulting class as to the core class. Fact 5.1 The core class has information theoretic sample complexity itsc (n; *; fl) = O (n 2 =*fl 2 ) = O (k (n; *)=fl 2 ). Proof: Follows from the Noisy Occam Razor <ref> [Lai88] </ref>. We are not interested in an information theoretic lower bound for the core class since the Equalizer will dominate the information theoretic complexities. Thus, we turn to analyze the computational complexities of the core class.
Reference: [Mau91] <author> U. M. Maurer. </author> <title> Perfect cryptographic security from partially independent channels. </title> <booktitle> In 23rd STOC, </booktitle> <pages> pages 561-571, </pages> <year> 1991. </year>
Reference-contexts: obtaining both c sec 2 jE (x)j of the bits of E (x) as well as the entire bsc 1 2 c sec 8 (E (x)). 12 We note that Maurer has shown that this version of the problem can be reduced to the original one by using bidirectional communiaction <ref> [Mau91] </ref>.
Reference: [NR95] <author> M. Naor and O. Reingold. </author> <title> Synthesizers and their application to the parallel construction of pseudo-random functions. </title> <booktitle> In 36th FOCS, </booktitle> <pages> pages 170-181, </pages> <year> 1995. </year> <note> Full version available from the ECCC at http://www.eccc.uni-trier.de/eccc/. </note>
Reference-contexts: In the above, and all subsequent theorems, one-way functions are merely used to construct pseudorandom functions [HILL, GGM86]. Assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure, one can construct pseudorandom functions in N C (cf., <ref> [NR95] </ref>), and so all of our "gap theorems" will follow with concept classes having N C circuits. We next consider classification noise at rate &lt; 1 2 . That is, the label of each example is flipped with probability , independently of all other examples. <p> Pseudorandom functions which can be evaluated by N C circuits (one circuit per each function) exist 7 , assuming either that RSA is a one-way function or that the Diffie-Hellman Key Exchange is secure (cf., <ref> [NR95] </ref>). 2.2 A Probabilistic Coding Scheme We present an efficient probabilistic encoding scheme having constant rate (information/codeword ratio), constant (efficient) error-correction capability for which a (small) constant fraction of the codeword bits yield no information about the plain message.
Reference: [PV88] <author> L. Pitt and L. Valiant. </author> <title> Computational limitations of learning from examples. </title> <journal> Journal of the A.C.M., </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: In these situations, computational restrictions appear to impair learning by requiring more data, but do not completely preclude learning. For example, it is NP-hard to find a k-term-DNF formula 2 consistent with a set of data labeled by a k-term-DNF formula <ref> [PV88] </ref>. The computationally efficient algorithm most commonly used for learning k-term-DNF works by finding a consistent hypothesis from an hypothesis class (kCNF) which strictly contains the target class.
Reference: [Sim93] <author> H. U. Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 402-411. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: In the absence of computational restrictions, any PAC-learnable class can also be learned in the presence of classification noise rate = 1=2fl &lt; 1=2 using a factor of fi (1=fl 2 ) more examples than the noise free case [Lai88, Tal94]. This increase is information theoretically required <ref> [Sim93, AD96] </ref>. Yet for many classes, when computation is restricted in the presence of noisy data, the sample complexity of known algorithms is increased by more than fi (1=fl 2 ).
Reference: [Tal94] <author> M. Talagrand. </author> <title> Sharper bounds for Gaussian and empirical processes. </title> <journal> Ann. Probab., </journal> <volume> 22(1) </volume> <pages> 28-76, </pages> <year> 1994. </year>
Reference-contexts: In the absence of computational restrictions, any PAC-learnable class can also be learned in the presence of classification noise rate = 1=2fl &lt; 1=2 using a factor of fi (1=fl 2 ) more examples than the noise free case <ref> [Lai88, Tal94] </ref>. This increase is information theoretically required [Sim93, AD96]. Yet for many classes, when computation is restricted in the presence of noisy data, the sample complexity of known algorithms is increased by more than fi (1=fl 2 ).
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: It has long been known that there are concept classes, containing only concepts which are implementable by "small" Boolean circuits, which can be learned in the absence of computational restrictions, yet cannot be learned (using any hypothesis class) in polynomial time (under standard cryptographic assumptions) <ref> [Val84, KV94, AK91, Kha93] </ref>. Yet, these results do not answer the question of whether there are classes for which learning from a small set of examples is infeasible, but becomes feasible when the learner has access to (polynomially) more examples.
Reference: [VC71] <author> V.N. Vapnik and A.Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theor. Probability Appl., </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: Blumer et.al. give an upper bound on the number of examples sufficient for an Occam algorithm to provide a good hypothesis. This bound depends on the PAC accuracy and confidence parameters and the Vapnik-Chervonenkis dimension (VC-Dimension) <ref> [VC71] </ref> of the target class. The general lower bound on the number of examples required for learning [EHKV89] nearly matches (within a logarithmic factor) the upper bound for Occam algorithms.
Reference: [Wyn75] <author> A. D. Wyner. </author> <title> The wire-tap channel. </title> <journal> Bell System Technical Journal, </journal> <volume> 54(8) </volume> <pages> 1355-1387, </pages> <month> Oct. </month> <year> 1975. </year> <month> 24 </month>
Reference-contexts: In addition to the standard coding theoretic requirements, this scheme has the property that any constant fraction of the bits in the (randomized) codeword yields no information about the message being encoded. We also use this coding scheme to obtain efficient constructions for the Wire-Tap Channel Problem (cf., <ref> [Wyn75] </ref>) see Proposition 2.2. We believe that this probabilistic coding scheme is of independent interest. Organization. After introducing the cryptographic tools we shall nee, we establish the separation of computational sample complexity from (IT) sample complexity in the basic model. <p> Using the Secrecy feature of the outer code, we conclude that no information is revealed about x. Efficient coding for the Wire-Tap Channel Problem: Using Theorem 2.1, we obtain an efficient coding scheme for (a strong version of ) the Wire-Tap Channel Problem (cf., <ref> [Wyn75] </ref>). Actually, we consider a seemingly harder version introduced by Csiszar and Korner [CK78].
Reference: [Yao82] <author> A. C. Yao. </author> <title> Theory and application of trapdoor functions. </title> <booktitle> In 23rd FOCS, </booktitle> <pages> pages 80-91, </pages> <year> 1982. </year>
Reference-contexts: Alternatively, the theorem will hold provided k (n; *) = (n 2 log n). 6 Proof of Theorem 1.5 Here we merely use a pseudorandom generator <ref> [BM84, Yao82] </ref>. Specifically, we need a generator, G, which stretches seeds of length n into sequences of length p (n). The concept class, C = fC n g, will correspond to all possible choices of a seed for the generator.
References-found: 27

