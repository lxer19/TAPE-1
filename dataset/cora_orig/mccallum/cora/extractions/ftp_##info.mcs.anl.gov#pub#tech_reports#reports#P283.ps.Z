URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P283.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: A Worst-Case Example Using Linesearch Methods for Numerical Optimization with Inexact Gradient Evaluations.  
Author: Richard G. Carter 
Note: Argonne Laboratory Preprint MCS-P283-1291  
Date: December 1991  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Carter, </author> <title> Numerical experience with a class of algorithms for nonlinear optimization using inexact function and gradient information, </title> <type> Tech. Report 89-46, </type> <institution> Institute for Computer Applications in Science and Engineering, </institution> <year> 1989. </year> <note> (SIAM J. </note> <institution> Sci. Statist. Comput., </institution> <month> to appear). </month> <title> [2] , On the global convergence of trust region algorithms using inexact gradient information, </title> <journal> SIAM J.Numer. Anal., </journal> <volume> 28 (1991), </volume> <pages> pp. 251-265. </pages>
Reference-contexts: A full description of the trust region approach is beyond the scope of this paper (the reader is referred to [4] and [7]), but we note that it has been well established (see, for instance, [7], [11], [2], and <ref> [1] </ref> ) that trust region methods are extremely robust with respect to gradient errors. In this paper we present a class of examples demonstrating how linesearch methods can fail if even tiny amounts of error are present in the gradient.
Reference: [3] <author> J. Dennis Jr. and H. Mei, </author> <title> Two new unconstrained optimization algorithms which use function and gradient values, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 28 (1979), </volume> <pages> pp. 453-482. </pages>
Reference: [4] <author> J. Dennis Jr. and R. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year> <month> 6 </month>
Reference-contexts: A broad variety of search procedures and step acceptance criteria have been proposed; a good introduction to these techniques can be found in <ref> [4] </ref>. These procedures assume that d k is a strict descent direction for f (trivially true if g k is exact and B k is positive definite) and will iterate until an ff k is found that provides a sufficient reduction in the function. <p> Trust region methods do not confine their search path to the single direction d k . A full description of the trust region approach is beyond the scope of this paper (the reader is referred to <ref> [4] </ref> and [7]), but we note that it has been well established (see, for instance, [7], [11], [2], and [1] ) that trust region methods are extremely robust with respect to gradient errors.
Reference: [5] <author> E. E. Eskow and R. Schnabel, </author> <title> Software for a new modified Choleski factorization, </title> <type> Tech. Report CU-CS-443-89, </type> <institution> Dept. of Computer Science, University of Colorado at Boulder, </institution> <year> 1989. </year>
Reference: [6] <author> P. Gill, W. Murray, and M. H. Wright, </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference: [7] <author> J. Mor e, </author> <title> Recent developments in algorithms and software for trust region methods, in Mathematical Programming: State of the Art, </title> <editor> A. Bachem, M. Gr-otschel, and B.Korte, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1983, </year> <pages> pp. 258-287. </pages>
Reference-contexts: Trust region methods do not confine their search path to the single direction d k . A full description of the trust region approach is beyond the scope of this paper (the reader is referred to [4] and <ref> [7] </ref>), but we note that it has been well established (see, for instance, [7], [11], [2], and [1] ) that trust region methods are extremely robust with respect to gradient errors. <p> A full description of the trust region approach is beyond the scope of this paper (the reader is referred to [4] and <ref> [7] </ref>), but we note that it has been well established (see, for instance, [7], [11], [2], and [1] ) that trust region methods are extremely robust with respect to gradient errors. In this paper we present a class of examples demonstrating how linesearch methods can fail if even tiny amounts of error are present in the gradient. <p> Other issues are more tangible: linesearch codes are often cited as superior with respect to scale invariance and with respect to linear-algebra-cost-per-iteration, while trust region methods are often regarded as superior for nonconvex problems. However, trust region methods can be made scale invariant with the proper preconditioning <ref> [7] </ref> and can be implemented if desired with very inexpensive linear algebra (e.g. [8],[3], [10]), while modified linesearch techniques exist which at least partially address the issue of negative curvature (e.g. [6],[5]).
Reference: [8] <author> M. Powell, </author> <title> A hybrid method for nonlinear equations, in Numerical Methods for Nonlinear Algebraic Equations, </title> <editor> P. Rabinowitz, ed., Gordon and Breach, </editor> <address> London, </address> <year> 1970, </year> <pages> pp. 87-114. </pages>
Reference: [9] <author> G. Shubin and P. Frank, </author> <title> A comparison of the implicit gradient approach and the variational approach to aerodynamic design optimization, </title> <type> Tech. Report AMS-TR-163, </type> <institution> Applied Mathematics and Statistics Dept., Boeing Computer Services, </institution> <year> 1991. </year>
Reference-contexts: A numerical comparison of actual performance of linesearch versus trust region approaches in the presence of gradient noise for a limited number of test problems can be found in <ref> [9] </ref>.
Reference: [10] <author> T. Steihaug, </author> <title> The conjugate gradient method and trust regions in large scale optimization, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 626-637. </pages>
Reference-contexts: However, trust region methods can be made scale invariant with the proper preconditioning [7] and can be implemented if desired with very inexpensive linear algebra (e.g. [8],[3], <ref> [10] </ref>), while modified linesearch techniques exist which at least partially address the issue of negative curvature (e.g. [6],[5]). In the opinion 5 of the author, none of the arguments raised in the past (with the possible exception of the issue of nonconvexity) are overwhelmingly convincing to objective observers.
Reference: [11] <author> Ph. L. Toint, </author> <title> Global convergence of a class of trust region methods for nonconvex minimization in Hilbert space, </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 8 (1988), </volume> <pages> pp. 231-252. 7 </pages>
Reference-contexts: A full description of the trust region approach is beyond the scope of this paper (the reader is referred to [4] and [7]), but we note that it has been well established (see, for instance, [7], <ref> [11] </ref>, [2], and [1] ) that trust region methods are extremely robust with respect to gradient errors. In this paper we present a class of examples demonstrating how linesearch methods can fail if even tiny amounts of error are present in the gradient.
References-found: 10

