URL: http://www.cs.colorado.edu/homes/brianb/public_html/mypapers/inputsel.ps.Z
Refering-URL: http://www.cs.colorado.edu/homes/brianb/public_html/mypapers.html
Root-URL: http://www.cs.colorado.edu
Email: brianb@cs.colorado.edu  andreas@cs.colorado.edu  
Title: Selecting Input Variables Using Mutual Information and Nonparametric Density Estimation  
Author: Brian V. Bonnlander Andreas S. Weigend 
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Department of Computer Science and Institute of Cognitive Science University of Colorado  
Abstract: In learning problems where a connectionist network is trained with a finite sized training set, better generalization performance is often obtained when unneeded weights in the network are eliminated. One source of unneeded weights comes from the inclusion of input variables that provide little information about the output variables. We propose a method for identifying and eliminating these input variables. The method first determines the relationship between input and output variables using nonparametric density estimation and then measures the relevance of input variables using the information theoretic concept of mutual information. We present results from our method on a simple toy problem and a nonlinear time series.
Abstract-found: 1
Intro-found: 1
Reference: <author> R. Battiti. </author> <title> (1994) Using mutual information for selecting features in supervised neural net learning. </title> <journal> IEEE Transactions on Neural Networks 5(4) </journal> <pages> 537-550. </pages>
Reference: <author> E. B. Baum & D. Haussler. </author> <title> (1989) What size net gives valid generalization? Neural Computation 1(1) </title> <type> 151-160. </type>
Reference: <author> C. M. Bishop. </author> <title> (1994) Mixture Density Networks. </title> <type> Unpublished report, </type> <institution> Aston University. </institution>
Reference: <author> W. Buntine & A. S. Weigend. </author> <title> (1991) Bayesian back-propagation. </title> <booktitle> Complex Systems 5 </booktitle> <pages> 603-643. </pages>
Reference: <author> T. M. Cover & J. A. Thomas. </author> <booktitle> (1991) Elements of Information Theory. </booktitle> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: What I (X i ; Y ) measures is the reduction in uncertainty of Y due to knowledge of the input variables X i <ref> (Cover & Thomas, 1991) </ref>. The uncertainty of a distribution is defined using the formula for entropy H.
Reference: <author> A. M. Fraser & H. L. Swinney. </author> <title> (1986) Independent coordinates for strange attractors from mutual information. </title> <journal> Physical Review A 33(2) </journal> <pages> 1134-1140. </pages>
Reference: <author> G. M. Furnival & R. W. Wilson. </author> <title> (1974) Regressions by leaps and bounds. </title> <journal> Technometrics, </journal> <volume> 16(4) </volume> <pages> 499-511. </pages>
Reference: <author> H. N. Gabow, J. L. Bentley, & R. E. Tarjan. </author> <title> (1984) Scaling and Related Techniques for Geometry Problems. </title> <booktitle> In STOC '84: Proceedings of the Sixteenth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> 135-140. </pages>
Reference: <author> S. Geman, E. Bienenstock & R. Doursat. </author> <title> (1992) Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 4(1) </booktitle> <pages> 1-58. </pages>
Reference: <author> N. A. Gershenfeld & A. S. Weigend. </author> <title> (1994) The future of time series: learning and understanding. </title> <editor> In A. S. Weigend & N. A. Gershenfeld (eds.), </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <pages> 1-70. </pages>
Reference: <author> W. Hardle. </author> <title> (1989) Applied Nonparametric Regression. </title> <address> Cambridge, MA: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Additionally, the parameters in a neural network are more flexible. The advantage of a nonparametric approach is the model's expressive power; it has been shown that many density estimation methods are capable of approximating large classes of distributions, given enough data <ref> (Hardle, 1989) </ref>. <p> In the example, the area under each bin height is 0.5, since each bin contains half of the total number of data points. kernels <ref> (Hardle, 1989) </ref>, and parametric estimates using connectionist networks (Bishop, 1994; Srivastava & Weigend, 1994). We are interested in identifying nonparametric methods that attribute high mutual information to those input variable subsets that result in high prediction accuracy when used in connectionist learning.
Reference: <author> P. M. Lewis. </author> <title> (1962) The characteristic selection problem in recognition systems. </title> <journal> IRE Transactions on information theory, </journal> <pages> 171-178. </pages>
Reference-contexts: Possible alternatives to exhaustive search are branch and bound techniques (for the linear case, see Furnival & Wilson, 1973), heuristic search methods, and additional problem assumptions, such as independence among input variables <ref> (Lewis, 1962) </ref>. 5 RESULTS that input variable X 3 carries information that is redundant with the input variables X 1 and X 2 . The two variables X 1 and X 2 are designed to be nearly useless individually, but to be useful together.
Reference: <author> D. J. C. MacKay. </author> <title> (1992) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4(3) </booktitle> <pages> 448-472. </pages>
Reference: <author> A. J. Miller. </author> <title> (1990) Subset Selection in Regression. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: Search procedures discussed in the linear regression literature, such as forward selection, backward elimination, or stepwise selection, where input variables are added or removed one at a time, work well when the input variables are nearly independent <ref> (Miller, 1990) </ref>. We illustrate with two examples why all input subsets network with five possible input units. For each pattern, every input variable carries the same value corrupted by a small amount of Gaussian noise.
Reference: <author> O. Maron & A. Moore. </author> <title> (1994) Hoeffding races: accelerating model selection search for classification and function approximation. </title> <booktitle> In Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 59-66. </pages>
Reference-contexts: One solution to this problem is to run the splitting algorithm until differences in mutual information estimates become significant; this has been termed racing <ref> (Maron & Moore, 1994) </ref>. associated with certain subsets are labeled with their corresponding time lags; that is, the label f4; 1g indicates the input variable subset formed from the time lags y t4 and y t1 . (b) Equal mass binning. histogram, involving all four input variables, has the largest variance.
Reference: <author> D. W. Scott. </author> <title> (1992) Multivariate Density Estimation: Theory, Practice, and Visualization. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: There is an extensive literature on density estimation, which includes treatment of discrete nonparametric estimates using histogram based techniques <ref> (e.g., Scott, 1992) </ref>, continuous nonparametric estimates using continuous 1 The presence or absence of n input variables in a subset can be represented by an n bit binary vector, where a 1 in the vector represents the presence of a variable, and a 0 represents the absence of a variable.
Reference: <author> A. N. Srivastava & A. S. Weigend. </author> <title> (1994) Computing the probability density in connectionist regression. </title> <booktitle> In Proceedings of the 1994 International Symposium on Artificial Neural Networks (ISANN'94), </booktitle> <address> Tainan, Taiwan. </address>
Reference: <author> A. S. Weigend, B. A. Huberman, & D. E. Rumelhart. </author> <title> (1990) Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209. </pages>
References-found: 18

