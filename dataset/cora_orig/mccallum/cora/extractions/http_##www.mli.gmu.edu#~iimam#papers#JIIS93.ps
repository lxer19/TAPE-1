URL: http://www.mli.gmu.edu/~iimam/papers/JIIS93.ps
Refering-URL: http://www.mli.gmu.edu/~iimam/pap_slct.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: iimam@aic.gmu.edu, michalski@aic.gmu.edu  
Title: Learning Decision Trees from Decision Rules:  
Keyword: Key words: machine learning, inductive learning, decision trees, decision rules, attribute selection.  
Address: Fairfax, VA. 22030  
Affiliation: Center for Artificial Intelligence George Mason University  
Date: 279-304, Sep., 1993  
Note: Published in the Journal of Intelligent Information Systems (JIIS) Vol. 2, No. 3, pp.  Kerschberg, L., Ras, Z., Zemankova, M. (Eds.) Kluwer Academic Publisher, MA  
Abstract: A method and initial results from a comparative study ABSTRACT A standard approach to determining decision trees is to learn them from examples. A disadvantage of this approach is that once a decision tree is learned, it is difficult to modify it to suit different decision making situations. Such problems arise, for example, when an attribute assigned to some node cannot be measured, or there is a significant change in the costs of measuring attributes or in the frequency distribution of events from different decision classes. An attractive approach to resolving this problem is to learn and store knowledge in the form of decision rules, and to generate from them, whenever needed, a decision tree that is most suitable in a given situation. An additional advantage of such an approach is that it facilitates building compact decision trees , which can be much simpler than the logically equivalent conventional decision trees (by compact trees are meant decision trees that may contain branches assigned a set of values , and nodes assigned derived attributes, i.e., attributes that are logical or mathematical functions of the original ones). The paper describes an efficient method, AQDT-1, that takes decision rules generated by an AQ-type learning system (AQ15 or AQ17), and builds from them a decision tree optimizing a given optimality criterion. The method can work in two modes: the standard mode , which produces conventional decision trees, and compact mode, which produces compact decision trees. The preliminary experiments with AQDT-1 have shown that the decision trees generated by it from decision rules (conventional and compact) have outperformed those generated from examples by the well-known C4.5 program both in terms of their simplicity and their predictive accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arciszewski, T., Dybala, T. & Wnek, J., </author> <year> (1992). </year> <title> Method for Evaluation of Learning Systems, Heuristics, </title> <journal> The Journal of Knowledge Engineering, Special Issue on Knowledge Acquisition and Machine Learning, </journal> <volume> Vol. 5. No. 4, </volume> <month> December. </month>
Reference-contexts: These experiments were performed for the U.S. Congressional voting-1984 problem, and the MONK-1 problem (described earlier). There are many ways to test the performance of learning systems, such as holdout, leave-one-out, and cross-validation <ref> (e.g., Arciszewski, Dybala &Wnek, 1992) </ref>. In comparing learning systems, one should apply them to training datasets of different sizes. In this experiment, we applied a variation of the holdout method, in which the training set of examples varies from experiment to experiment. Experiments were performed for two problems.
Reference: <author> Bergadano, F., Giordana, A., Saitta, L., DeMarchi, D., & Brancadori, F., </author> <year> (1990). </year> <title> Integrated Learning in Real Domain, </title> <booktitle> Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> (pp. 322-329), </pages> <address> Austin, TX. </address>
Reference: <author> Bloedorn, E. and Michalski, R. S., </author> <year> (1991a). </year> <title> "Data Driven Constructive Induction in AQl7-PRE: A Method and Experiments," </title> <booktitle> Proceedings of the Third International Conference on Tools for AI, </booktitle> <address> San Jose, California, </address> <month> November 9-14. </month> <note> 20 Bloedorn, </note> <author> E. and Michalski, R.S. </author> , <year> (1991b). </year> <title> "Constructive Induction from Data in AQ17-DCI: Further Experiments," Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 91-12, </type> <institution> Center for Artificial Intelligence, George Mason University, Fairfax, VA, </institution> <month> December. </month>
Reference-contexts: The tree is presented in Figure 8. By running AQDT-1 in compact mode, a simpler decision tree was produced (Figure 9a). In the final experiment, we used AQ17-DCI <ref> (Bloedorn & Michalski, 1991a,b) </ref> to derive decision rules.
Reference: <author> Bratko, I. & Lavrac, N. </author> , <year> (1987). </year> <editor> (Eds.), </editor> <booktitle> Progress in Machine Learning, </booktitle> <address> Sigma Wilmslow, England, </address> <publisher> Press. </publisher>
Reference: <author> Bratko, I. & Kononenko, I., </author> <year> (1986). </year> <title> Learning Diagnostic Rules from Incomplete and Noisy Data in B.Phelps, (edt.), Interactions in AI and statistical method, </title> <publisher> Gower Technical Press. </publisher>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A. & Stone, C.J., </author> <year> (1984). </year> <title> Classification and Regression Trees, </title> <address> Belmont, California: </address> <publisher> Wadsworth Int. Group. </publisher>
Reference-contexts: The essential aspect of any such method is the attribute selection criterion that is used for choosing attributes to be assigned to the nodes of the tree being built. Criteria for that purpose include the entropy reduction measure (e.g., Quinlan, 1979), the gini index of diversity <ref> (Breiman, et al., 1984) </ref>, and many others (Cestnik & Karalic, 1991; Mingers, 1989a). An early algorithm for generating decision trees from examples was proposed by Hunt, Marin and Stone (1966).
Reference: <author> Clark, P. & Niblett, T. </author> , <year> (1987). </year> <title> Induction in Noisy Domains in I. </title> <editor> Bratko and N. Lavrac, (Eds.), </editor> <booktitle> Progress in Machine Learning, </booktitle> <publisher> Sigma Press, Wilmslow. </publisher>
Reference: <author> Cestnik, B. & Bratko, I. </author> , <year> (1991). </year> <title> On Estimating Probabilities in Tree Pruning, </title> <booktitle> Proceeding of EWSL 91, </booktitle> <pages> (pp. 138-150) Porto, </pages> <address> Portugal, </address> <month> March 6-8. </month>
Reference-contexts: Such a method consists of two phases: the creation of an initial decision tree, and tree pruning, done by removing subtrees with small statistical validity, and replacing them by leaf nodes. More recently, pruning has also been used for simplifying decision trees even for noiseless problems <ref> (Cestnik & Bratko, 1991) </ref>. Pruning decision trees improves their simplicity, but reduces their predictive accuracy on the training examples.
Reference: <author> Cestnik, B. & Karalic, A., </author> <year> (1991). </year> <title> The Estimation of Probabilities in Attribute Selection Measures for Decision Tree Induction in Proceeding of the European Summer School on Machine Learning , July 22-31, </title> <address> Priory Corsendonk, Belgium. </address>
Reference-contexts: Such a method consists of two phases: the creation of an initial decision tree, and tree pruning, done by removing subtrees with small statistical validity, and replacing them by leaf nodes. More recently, pruning has also been used for simplifying decision trees even for noiseless problems <ref> (Cestnik & Bratko, 1991) </ref>. Pruning decision trees improves their simplicity, but reduces their predictive accuracy on the training examples.
Reference: <author> Hunt, E., Marin, J. & Stone, P., </author> <year> (1966). </year> <title> Experiments in induction, </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Michalski, R.S. </author> , <year> (1973). </year> <title> AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition, </title> <booktitle> Proceeding of the First International Joint Conference on Pattern Recognition , (pp. </booktitle> <pages> 3-17), </pages> <address> Washington, DC, </address> <month> October 30- November 1. </month>
Reference-contexts: The learned descriptions are represented in the form of a set of decision rules, expressed in an attributional logic calculus, called variable-valued logic 1 or VL1 <ref> (Michalski, 1973) </ref>. A distinctive feature of this representation is that it employs, in addition to standard logic operators, the internal disjunction operator (a disjunction of values of the same attribute in a condition) and the range operator (to express conditions involving a range of discrete or continuous values). <p> Nodes that are assigned such attributes will have a smaller fan out. When attributes are continuous, they are quantized into discrete units representing ranges of values. The above three elementary criteria are combined into one general attribute ranking measure using the lexicographic evaluation functional with tolerances (LEF) <ref> (Michalski, 1973) </ref>. The LEF allows to combine the elementary criteria in different ways. In the default combination, the LEF is defined as: &lt;Disjointness, t1; Dominance, t2; Extent, t3&gt; where t1, t2, t3 are tolerance thresholds. The above LEF ranks attributes this way.
Reference: <author> Michalski, R.S, </author> <year> (1978). </year> <title> Designing Extended Entry Decision Tables and Optimal Decision Trees Using Decision Diagrams, </title> <type> Technical Report No.898 , Urbana: </type> <institution> University of Illinois, </institution> <month> March. </month>
Reference: <author> Michalski, R.S., </author> <year> (1983). </year> <title> A Theory and Methodology of Inductive Learning, </title> <journal> Artificial Intelligence , Vol. </journal> <volume> 20, </volume> <pages> (pp. 111-116). </pages>
Reference-contexts: In order to make the paper self-contained, we briefly describe the AQ15 and AQ17-DCI. AQ15 learns decision rules for a given set of decision classes from examples of decisions, using the STAR methodology <ref> (Michalski, 1983) </ref>. The simplest algorithm based on this methodology, called AQ, starts with a seed example of a given decision class, and generates a set of the most general conjunctive descriptions of the seed 3 (alternative decision rules for the seed example). <p> These operators help to simplify rules involving multivalued discrete attributes; the second operator is also used for creating logical expressions involving continuous attributes. AQ15 can generate decision rules that represent either characteristic or discriminant concept descriptions, depending on the settings of its parameters <ref> (Michalski, 1983) </ref>. A characteristic description states properties that are true for all objects in the concept. The simplest characteristic concept description is in the form of a single conjunctive rule (in general, it can be a set of such rules).
Reference: <author> Michalski, R.S., Mozetic, I., Hong, J. & Lavrac, N., </author> <year> (1986). </year> <title> The MultiPurpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> (pp. 1041-1045), </pages> <address> Philadelphia, PA. </address>
Reference-contexts: A Brief Description of the AQ15 Rule Learning Program The proposed method, called AQDT-1 ( AQ derived Decision T ree - 1), generates decision trees from decision rules. The decision rules used by the program are generated from examples by an AQ-type inductive learning system, specifically, by AQ15 <ref> (Michalski et al., 1986) </ref> or AQ17-DCI (Bloedorn and Michalski, 1991 a,b). In order to make the paper self-contained, we briefly describe the AQ15 and AQ17-DCI. AQ15 learns decision rules for a given set of decision classes from examples of decisions, using the STAR methodology (Michalski, 1983). <p> The u-weight ( unique-weight) of a rule for some class is the number of examples of that class covered only by this rule. The proposed method follows ideas presented in <ref> (Michalski et al., 1986) </ref>, namely, to prune rules of small strength, specifically, the rules with the t-weight or the u-weight below certain threshold. So pruned rules are used for determining a decision tree.
Reference: <author> Michalski, R.S. </author> , <year> (1990). </year> <title> Learning Flexible Concepts: Fundamental Ideas and a Method Based on Two-tiered Representation, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. </booktitle> <address> III , San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June. </month>
Reference: <author> Mingers, J., </author> <year> (1989a). </year> <title> An Empirical Comparison of selection Measures for Decision-Tree Induction, </title> <journal> Machine Learning , Vol. </journal> <volume> 3, No. 3, </volume> <pages> (pp. 319-342), </pages> <publisher> Kluwer Academic Publishers.. </publisher>
Reference: <author> Mingers, J., </author> <year> (1989b). </year> <title> An Empirical Comparison of Pruning Methods for Decision-Tree Induction, </title> <journal> Machine Learning , Vol. </journal> <volume> 3, No. 4, </volume> <pages> (pp. 227-243), </pages> <publisher> Kluwer Academic Publishers.. </publisher>
Reference-contexts: These approaches differ in the criteria for deciding whether or not to prune the tree at some level. A comparison of these pruning approaches is in <ref> (Mingers, 1989b) </ref>. When decision trees are generated from decision rules, it is better to prune the rules before they are used for determining the tree. Pruning decision rules is done on basis of the rule strength. The rule strength is characterized by its t-weight and u-weight.
Reference: <author> Niblett, T. & Bratko, </author> <title> I ., (1986). Learning decision rules in noisy domains Proceeding Expert Systems 86, </title> <address> Brighton, Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Quinlan, J.R., </author> <year> (1979). </year> <title> Discovering Rules By Induction from Large Collections of Examples, </title> <editor> in D. Michie (Edr), </editor> <title> Expert Systems in the Microelectronic Age , Edinburgh University Press. </title> <type> 21 Quinlan, J.R., </type> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Related research Decision trees are normally generated from examples. The essential aspect of any such method is the attribute selection criterion that is used for choosing attributes to be assigned to the nodes of the tree being built. Criteria for that purpose include the entropy reduction measure <ref> (e.g., Quinlan, 1979) </ref>, the gini index of diversity (Breiman, et al., 1984), and many others (Cestnik & Karalic, 1991; Mingers, 1989a). An early algorithm for generating decision trees from examples was proposed by Hunt, Marin and Stone (1966).
Reference: <author> Quinlan, J.R., </author> <year> (1986). </year> <title> Induction of Decision Trees, </title> <journal> Machine Learning Vol. </journal> <volume> 1, No. 1, </volume> <pages> (pp. 81-106), </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Quinlan, J.R., </author> <year> (1987). </year> <title> Simplifying Decision Trees, </title> <journal> International Journal of Man-Machine Studies , 27, </journal> <pages> (pp. 221-234). </pages>
Reference: <author> Quinlan, J.R., </author> <year> (1989). </year> <title> Documentation and users guide for C4.5, </title> <publisher> Unnpublished. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Probabilistic decision trees, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. </booktitle> <address> III , San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June. </month>
Reference-contexts: For example, in some situations it may be difficult to determine the value of the attribute assigned to some node. One would like to avoid measuring this attribute and still be able to classify the example, if this is potentially possible <ref> (Quinlan, 1990) </ref>. If the cost of measuring various attributes changes, it is desirable to restructure the tree so that the inexpensive attributes are evaluated first. A tree restructuring is also desirable, if there is a significant change in the frequency of occurrence of examples from different classes.
Reference: <author> Smyth, P., Goodman, R.M. & Higgins, C., </author> <year> (1990). </year> <title> A Hybrid Rule-based/Bayesian Classifier, </title> <booktitle> Proceedings of ECAI 90 , Stockholm, </booktitle> <month> August. </month>

References-found: 24

