URL: http://drl.cs.uiuc.edu/pubs/seamons-thesis.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/seamons-thesis.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Kent Eldon Seamons 
Date: 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Fern E. Bassow. </author> <title> IBM AIX Parallel I/O File System: Installation, Administration, and Use. </title> <institution> IBM, Kingston, </institution> <address> N.Y., </address> <month> May </month> <year> 1995. </year> <title> Document Number SH34-6065-00. </title>
Reference-contexts: of 2D plane in 4D array Base chunk_memory_base [] = -0, 0-; // Base memory index of the chunk Base chunk_disk_base [] = -0, 0, 0, 0-; // Base disk index of the chunk // chunk object to describe the plane in the density array chunk_disk_base [0] = iteration_index; chunk_disk_base <ref> [1] </ref> = plane_index; Chunk *density_chunk = new Chunk (chunk_rank, chunk_size, chunk_memory_base, chunk_disk_base); // read the array stat = density_plane.readChunk (schema, *density_chunk); return (stat); - // Visualize the density array status visualize_density () - status stat = STATUS_OK; Rank density_plane_rank = 2; // 2D plane of density array Size density_plane_size [] = <p> Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A <ref> [1; 1; 1] </ref>::A [100; 200; 300] and receive a request for A [20; 30; 40]::A [50; 60; 70]). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. <p> The Panda parallel file system architecture is a candidate to be layered above these parallel file systems, providing the user with an easy-to-use interface for optimized array i/o. The Vesta Parallel File System [13] was developed as a research project, and led to IBM's Parallel I/O File System (PIOFS) <ref> [1] </ref>, a commercial product. Vesta provides a user-definable view of parallel files that is applicable to array data. The application is given control over the stripe size. Panda provides a higher-level semantic interface than Vesta. PIOUS [46, 45] is a parallel file system architecture for a network computing environment.
Reference: [2] <author> J. L. Bell and G. S. Patterson, Jr. </author> <title> Data organization in large numerical computations. </title> <journal> The Journal of Supercomputing, </journal> <volume> 1(1), </volume> <year> 1987. </year>
Reference-contexts: Introduction Multidimensional arrays are a fundamental data type in scientific computing and are used extensively across a broad range of applications <ref> [2] </ref>. Often these arrays are persistent, i.e., they outlive the invocation of the program that created them. Portability and performance with respect to input and output (i/o) of arrays pose significant challenges to applications that access large persistent arrays, especially in distributed-memory environments.
Reference: [3] <author> T. Bell, A. Moffat, I. Witten, and J. Zobel. </author> <title> The MG retrieval system: Compressing for space and speed. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 41-42, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This benchmark has queries that include non-array data types and operations not applicable to Panda, so no direct performance comparison is possible with the benchmark results. Compression has occasionally been integrated in data management systems [63, 27]. The MG retrieval system <ref> [3] </ref> uses compression for documents, indexes, and images [75] and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. [41, 42] also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [4] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel i/o. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: It assumes i/o operations are collective, and only a subset of the clients send requests to the i/o devices, to limit network traffic. The i/o devices then send back the appropriate data to the clients. Panda incorporates elements of this strategy for write operations as well. The Jovian library <ref> [4] </ref> is a runtime library for SPMD-style codes. Jovian supports collective i/o operations, assuming that all application processes will participate in the i/o, similar to the highest-level array i/o operations in Panda.
Reference: [5] <author> Rajesh Bordawekar, Alok Choudhary, Ken Kennedy, Charles Koelbel, and Mike Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <type> Technical Report CRPC-TR94507-S, </type> <note> Center for Research on Parallel Computation, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: They propose two new directives to specify which arrays are to be out-of-core and how much memory is available in a processor to hold out-of-core array data. Ideas from [66] and [51] are combined in <ref> [5] </ref> which describes a run-time system employing the two-phase i/o approach for handling array i/o. An advantage of the compiler-based approach to supporting out-of-core computations is that all the intermediate i/o can be completely handled by the compiler, with the user possibly supplying some high-level directive information. <p> Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o <ref> [7, 8, 51, 5, 66] </ref>. In sum, the technology in Panda will be commonly used in future systems that handle persistent arrays.
Reference: [6] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In the case of checkpoint, the data may rarely be utilized, so an approach that optimizes writes at the expense of reads is a reasonable choice, since restart is not a performance critical operation. Natural chunking is a twist on the two-phase i/o strategy <ref> [6] </ref>, where array data in memory is reorganized to a conforming distribution and then written to disk. The use of natural chunking permits Panda to avoid the need to reorganize the array data by assuming the current in-memory array distribution will be the conforming distribution on disk. <p> In Panda, the notions of file and array are decoupled as Panda does not require an array to be stored in a single file. <ref> [6] </ref> describes run-time primitives to support a two-phase access strategy [16, 15] for conducting parallel i/o of arrays. <p> The interface layer in VIP-FS supports two types of parallel file access: a Unix-like access and a mapped access. The mapped access is intended to support array access. VIP-FS supports three access strategies; direct access, two-phase access <ref> [6] </ref>, and assumed-requests access. Assumed-requests access is intended to improve the performance of read operations and has similarities to Panda's server-directed i/o. It assumes i/o operations are collective, and only a subset of the clients send requests to the i/o devices, to limit network traffic.
Reference: [7] <author> Peter Brezany, Michael Gernt, Piyush Mehrotra, and Hans Zima. </author> <title> Concurrent file operations in a High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: In the near future, such compilers may take advantage of the existence of i/o libraries such as Panda. Constructs to specify i/o operations for distributed arrays in Vienna Fortran are proposed in <ref> [7] </ref>. This work was extended in [8] to incorporate the VIPIOS runtime system with the compiler. In their work, the compiler and run-time system cooperate to store array data in alternative orders in a file. The write statement supports writing multiple arrays with a single statement. <p> Second, the programmer can specify that the array be written to the file using a different on-disk distribution than the current in-memory distribution. Reorganization is done using methods similar to the direct method or two-phase i/o <ref> [7] </ref>. They do not use a server-directed i/o strategy. Third, the user can specify that the compiler and run-time system choose the distribution. [7] hints that multiple arrays are output using a non-interleaved schema. <p> Reorganization is done using methods similar to the direct method or two-phase i/o <ref> [7] </ref>. They do not use a server-directed i/o strategy. Third, the user can specify that the compiler and run-time system choose the distribution. [7] hints that multiple arrays are output using a non-interleaved schema. Since the write statement is at a sufficiently high level, it should be possible to support an interleaved schema with the current programming interface. <p> Since the write statement is at a sufficiently high level, it should be possible to support an interleaved schema with the current programming interface. They use self-describing files by storing information about the on-disk array distribution in the file along with the array data. <ref> [7] </ref> provides one example of a 1000 fi 1000 array in a (BLOCK,BLOCK) schema on a 4 fi 4 processor mesh and shows that it is faster to output the array using natural chunking compared to traditional order on an Intel iPSC/860. <p> Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o <ref> [7, 8, 51, 5, 66] </ref>. In sum, the technology in Panda will be commonly used in future systems that handle persistent arrays.
Reference: [8] <author> Peter Brezany, Thomas A. Mueck, and Erich Schikuta. </author> <title> Language, compiler and parallel database support for i/o intensive applications. </title> <booktitle> In Proceedings of the High Performance Computing and Networking 1995 Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1995. </year> <month> 131 </month>
Reference-contexts: In the near future, such compilers may take advantage of the existence of i/o libraries such as Panda. Constructs to specify i/o operations for distributed arrays in Vienna Fortran are proposed in [7]. This work was extended in <ref> [8] </ref> to incorporate the VIPIOS runtime system with the compiler. In their work, the compiler and run-time system cooperate to store array data in alternative orders in a file. The write statement supports writing multiple arrays with a single statement. Three forms of the write statement are available. <p> They argue that alternative on-disk array organizations are useful and are usually lacking in current parallel i/o libraries. Panda is one example of a parallel i/o library that does support such alternatives. No performance data is presented in <ref> [8] </ref>, so no comparison to Panda is possible. A compiler supporting out-of-core arrays is described in [51]. The compiler inserts explicit statements to move array data in and out of memory, guided by i/o distribution statements similar to Fortran D's in-memory data distribution statements. <p> Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o <ref> [7, 8, 51, 5, 66] </ref>. In sum, the technology in Panda will be commonly used in future systems that handle persistent arrays.
Reference: [9] <author> Michael J. Carey, David J. DeWitt, Michael J. Franklin, Nancy E. Hall, Mark L. McAuliffe, Jeffrey F. Naughton, Daniel T. Schuh, Marvin H. Solomon, C. K. Tan, Odysseas G. Tsat-alos, Seth J. White, and Michael J. Zwilling. </author> <title> Shoring up persistent applications. </title> <booktitle> In ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 383-394, </pages> <address> Minneapolis, Minnesota, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: For instance, techniques from Panda would be applicable when building a Datablade [70] for arrays in the Illustra DBMS, the commercial version of POSTGRES. Any object-oriented database <ref> [35, 9] </ref> or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o [7, 8, 51, 5, 66].
Reference: [10] <author> L.T. Chen, R. Drach, M. Keating, S. Louis, D. Rotem, and A. Shoshani. </author> <title> Efficient organization and access of multi-dimensional datasets on tertiary storage systems. </title> <journal> Information Systems Special Issue on Scientific Databases, </journal> <year> 1995. </year>
Reference-contexts: Tertiary storage is a useful medium to store large scientific data sets at lower costs than magnetic disk. How does this research apply to tertiary storage? Tertiary storage was beyond the scope of this research. Other researchers have looked at tertiary storage in the context of chunking <ref> [56, 10] </ref>. (a) What are the proper high-level abstractions for tertiary storage? Can physical and logical data independence be provided? (b) What interface extensions may be required for Panda to effectively support tertiary storage? (c) How do the techniques of chunking and compression in Panda relate to data on tertiary storage?
Reference: [11] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Pon-nusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: With a distributed view, each process requests only the part of the data it requires. Panda's high-level interfaces classify it as providing a global view. The Parallel And Scalable Software for Input-Output (PASSION) project <ref> [11, 67] </ref> at Syra-cuse is a broad effort to provide high performance parallel i/o at the language, compiler, runtime, and file system level.
Reference: [12] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: A parallel file i/o interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: provide high i/o performance and allow the results to be written in a format that is more easily accessible by applications outside the realm of the compiler. 6.5 Parallel i/o The majority of interfaces designed to-date for array i/o are at a lower abstract level than those found in Panda <ref> [12, 24] </ref>. MPI-IO [12] is a proposal for a parallel file i/o interface. The goal of the proposal is to provide a standard for describing parallel i/o operations in MPI applications. <p> MPI-IO <ref> [12] </ref> is a proposal for a parallel file i/o interface. The goal of the proposal is to provide a standard for describing parallel i/o operations in MPI applications. <p> simple, abstract interface can be used on both parallel and sequential machines to insulate programmers from physical storage implementation details, such as chunking, interleaving, compression, and server-directed i/o. (d) The novel feature of Panda's interface is that the height of the abstraction is so great, compared to other proposed interfaces <ref> [12, 24] </ref>. The freedom and flexibility that such a high-level interface gives for an efficient implementation, as witnessed by the three architectures Panda supports, warrants greater attention to this level of interface for scientific application programmers. 2.
Reference: [13] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: The Panda parallel file system architecture is a candidate to be layered above these parallel file systems, providing the user with an easy-to-use interface for optimized array i/o. The Vesta Parallel File System <ref> [13] </ref> was developed as a research project, and led to IBM's Parallel I/O File System (PIOFS) [1], a commercial product. Vesta provides a user-definable view of parallel files that is applicable to array data. The application is given control over the stripe size.
Reference: [14] <author> Thomas W. Crockett. </author> <title> File concepts for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: The current implementation used in the experiments described in this thesis assumes a parallel file system that supports the Unix i/o model where a single file appears as a stream of bytes to all the processes of an application <ref> [14] </ref>. Each processor directly reads and writes the array chunks it contains to the appropriate location in the parallel file. This approach requires no communication between the compute nodes during the actual i/o operation.
Reference: [15] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improving parallel i/o performance using a two-phase access strategy. </title> <type> Technical Report SCCS-406, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: In Panda, the notions of file and array are decoupled as Panda does not require an array to be stored in a single file. [6] describes run-time primitives to support a two-phase access strategy <ref> [16, 15] </ref> for conducting parallel i/o of arrays. In this approach, for read operations, the compute nodes cooperate to bring all the data into memory in a way that minimizes the total number of disk accesses by having the data layout in memory conform to the data layout on disk.
Reference: [16] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance i/o for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: In Panda, the notions of file and array are decoupled as Panda does not require an array to be stored in a single file. [6] describes run-time primitives to support a two-phase access strategy <ref> [16, 15] </ref> for conducting parallel i/o of arrays. In this approach, for read operations, the compute nodes cooperate to bring all the data into memory in a way that minimizes the total number of disk accesses by having the data layout in memory conform to the data layout on disk.
Reference: [17] <author> Juan Miguel del Rosario, Michael Harry, and Alok Choudhary. </author> <title> The design of VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <type> 132 Technical Report SCCS-628, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Striping arrays across multiple i/o nodes allows them to be read and written in parallel [55, 25]. The striping unit in Panda is an array chunk, a distinguishing feature compared to related work where the arrays were written to disk in traditional (row-major or column-major) order <ref> [32, 33, 17] </ref> with the disk block as the striping unit. Under the server-directed i/o architecture, compression can occur on the compute nodes or the i/o nodes in the case of natural chunking. <p> PPFS supports Fortran D distributions as one of the data distributions it understands, which is obviously intended for array data. Similar to Panda, PPFS is implemented to use the Unix file system, MPI for message passing, and C++ to maximize portability of the system. VIP-FS <ref> [17] </ref> is another user-level library designed to be layered above commodity file systems on clusters of workstations. The interface layer in VIP-FS supports two types of parallel file access: a Unix-like access and a mapped access. The mapped access is intended to support array access.
Reference: [18] <author> David DeWitt, Navin Kabra, Jun Luo, Jignesh M. Patel, and Jie-Bing Yu. </author> <title> Client-server Paradise. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: through the use of large chunks, and enhanced support for readers by the use of subchunks within each chunk to give fine-grained locality of access for typical applications that read the data, so that a small working set of data can be assembled in memory without too many disk accesses. <ref> [18] </ref> describes a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future. Paradise supports 2D raster 107 images and divides the images into rectangular tiles to be stored on disk.
Reference: [19] <author> David DeWitt, Jeffrey Naughton, Donovan Schneider, and S. Seshadri. </author> <title> Practical skew handling in parallel joins. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 3-14, </pages> <year> 1992. </year>
Reference-contexts: Panda performs sequential i/o for both reads and writes of array data, while log-structured file systems make no guarantees for sequential reads. 106 6.3 Databases Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins <ref> [19] </ref> and range searching [38]. Unfortunately, the vast amount of work on declustering relational data on parallel machines is not relevant for scientific data access patterns. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [40].
Reference: [20] <author> Patrick C. Fischer and Robert L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Communications of the ACM, </journal> <volume> 22(7) </volume> <pages> 405-415, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Traditional array order is not always optimal for in-memory arrays. Studies have shown that page faulting in a virtual memory environment can sometimes be reduced by storing arrays in alternative orders, such as submatrix storage [43]. <ref> [20] </ref> extends this work by showing efficient strategies to reorganize an array from traditional order to submatrix order. [49] presents a balanced discussion of the issues surrounding in-memory tiling (submatrix storage). <p> Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A <ref> [20; 30; 40] </ref>::A [50; 60; 70]). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens.
Reference: [21] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Panda adopts the BLOCK and * data distribution directives of High Performance Fortran (HPF) <ref> [21] </ref> and applies them to array data stored in a file. The MemoryArrayLayout and DiskArrayLayout classes in Panda provide an abstraction to describe how arrays are organized in memory and on disk. Figure 3.1 presents the public methods for the MemoryArrayLayout and DiskArrayLayout classes. <p> In High Performance Fortran and related languages <ref> [21, 22, 28, 77] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: [22] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR 90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: In High Performance Fortran and related languages <ref> [21, 22, 28, 77] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: [23] <author> Craig S. Freedman, Josef Burger, and David J. Dewitt. </author> <title> SPIFFI | a scalable parallel file system for the Intel Paragon. </title> <journal> Submitted to IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1994. </year>
Reference-contexts: The performance study in the previous 113 chapter showed that logical level disk-directed i/o offers good performance advantages. To the best of our knowledge, Panda is the first implementation of concepts from disk-directed i/o. Traditional parallel file systems provide a Unix-like interface for reading and writing to a file <ref> [52, 23] </ref>. In these systems, a file is simply a stream of bytes, often striped across multiple disks, and the file system has no built-in support for higher-level objects such as arrays. Several efforts are underway to construct scalable parallel file systems.
Reference: [24] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: These high-level operations are common in other domains besides computational fluid dynamics. An example of an astrophysics application with similar i/o needs is described in [69]. An example of an i/o library motivated by the same i/o operations is described in <ref> [24] </ref>. Two application groups at the National Center for Supercomputing Applications at the University of Illinois have i/o needs similar to those described earlier. First, the Astronomy and Astrophysics Group has cosmology applications that perform large computations on the CM5 at NCSA. <p> A common approach taken by application programmers to easily implement checkpoints and restarts on distributed-memory parallel machines is to output multidimensional arrays to disk with each processor storing each subarray assigned to that processor in a separate file and restarting with the same number of processors and processor mesh <ref> [24] </ref>. The advantages 5 of this approach are simple coding and reasonable performance. The disadvantages are the inconvenience of managing large numbers of files when many processors and arrays are involved in a computation and the limitation of restarting with the same number of processors and processor mesh configuration. <p> provide high i/o performance and allow the results to be written in a format that is more easily accessible by applications outside the realm of the compiler. 6.5 Parallel i/o The majority of interfaces designed to-date for array i/o are at a lower abstract level than those found in Panda <ref> [12, 24] </ref>. MPI-IO [12] is a proposal for a parallel file i/o interface. The goal of the proposal is to provide a standard for describing parallel i/o operations in MPI applications. <p> Panda's implementation would gain portability advantages by using MPI-IO internally if it becomes 111 standard on parallel platforms. Panda's high-level interface would permit the implementation to automatically map high-level array i/o semantics to MPI-IO primitives. <ref> [24] </ref> reports on experiences with parallel applications at Argonne National Laboratory. The target applications they describe have i/o requirements that include the operations of checkpoint, restart, and output data, as in Panda. <p> simple, abstract interface can be used on both parallel and sequential machines to insulate programmers from physical storage implementation details, such as chunking, interleaving, compression, and server-directed i/o. (d) The novel feature of Panda's interface is that the height of the abstraction is so great, compared to other proposed interfaces <ref> [12, 24] </ref>. The freedom and flexibility that such a high-level interface gives for an efficient implementation, as witnessed by the three architectures Panda supports, warrants greater attention to this level of interface for scientific application programmers. 2.
Reference: [25] <author> Hector Garcia-Molina and Kenneth Salem. </author> <title> The impact of disk striping on reliability. </title> <journal> IEEE Database Engineering Bulletin, </journal> <volume> 11(1) </volume> <pages> 26-39, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In practice, a distributed file system may prove useful in managing the separate files on the i/o nodes after the application completes and data must be migrated to more permanent storage. Striping arrays across multiple i/o nodes allows them to be read and written in parallel <ref> [55, 25] </ref>. The striping unit in Panda is an array chunk, a distinguishing feature compared to related work where the arrays were written to disk in traditional (row-major or column-major) order [32, 33, 17] with the disk block as the striping unit.
Reference: [26] <author> Al Globus. </author> <title> C++ class library data management for scientific visualization. </title> <type> Technical Report RNR-93-006, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Under the first approach, the entire array from each time step is read into memory. The second approach uses memory mapped files and the virtual memory page faulting features of the operating system to read into memory only those pages that the application actually needs <ref> [26] </ref>. Panda's high-level interfaces are general enough to support implementations of both types of i/o. In the first case, an entire chunked array can be read from disk to a traditionally ordered array in memory using Panda's Array::read () interface.
Reference: [27] <author> G. Graefe and L.D. Shapiro. </author> <title> Data compression and database performance. </title> <booktitle> In Proceedings of the ACM/IEEE Computer Science Symposium on Applied Computing, </booktitle> <year> 1991. </year>
Reference-contexts: This benchmark has queries that include non-array data types and operations not applicable to Panda, so no direct performance comparison is possible with the benchmark results. Compression has occasionally been integrated in data management systems <ref> [63, 27] </ref>. The MG retrieval system [3] uses compression for documents, indexes, and images [75] and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. [41, 42] also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [28] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8), </volume> <month> August </month> <year> 1992. </year> <month> 133 </month>
Reference-contexts: In High Performance Fortran and related languages <ref> [21, 22, 28, 77] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: [29] <author> Interbase Software Corporation. </author> <title> Interbase Data Definition Guide, </title> <year> 1990. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [48, 71], database management systems <ref> [29, 57, 76] </ref>, or hand-coded persistence. It is common for current scientific applications which access persistent arrays to utilize existing file system interfaces directly. Many file systems adopt the Unix file system model, even on parallel machines. <p> Unfortunately, the vast amount of work on declustering relational data on parallel machines is not relevant for scientific data access patterns. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [40]. A few commercial DBMSes support multidimensional arrays, including Interbase <ref> [29] </ref>, Orion [76], and Stratum [57]. Like the file format systems just described, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
Reference: [30] <author> James V. Huber Jr., Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A <ref> [20; 30; 40] </ref>::A [50; 60; 70]). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. <p> Parafile segments are declustered across servers. This allows the user to control how data is striped across servers. A network of workstations version of Panda's parallel file system architecture could be layered above a file system like PIOUS. The Portable Parallel File System (PPFS) <ref> [30] </ref> is a user-level library designed as an experimental testbed to study issues in parallel i/o. It is built above the standard Unix file system for portability to allow for testing on a variety of platforms.
Reference: [31] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file i/o. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 191-204, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: For instance, techniques from Panda would be applicable when building a Datablade [70] for arrays in the Illustra DBMS, the commercial version of POSTGRES. Any object-oriented database [35, 9] or object-oriented file system <ref> [31] </ref> will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o [7, 8, 51, 5, 66].
Reference: [32] <author> David Kotz. </author> <title> Disk-directed i/o for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <month> Revised November 8, </month> <year> 1994. </year>
Reference-contexts: This approach has been labeled the direct method [68] or traditional caching <ref> [32] </ref> and is known 3 to perform poorly in many cases due to excess disk seeks and write-back errors. <p> When Array objects are distributed across multiple processors, all the processors containing any portion of the arrays must collectively participate in the i/o operation to achieve data consistency. The results are undefined if all the processors do not participate. <ref> [32] </ref> points out that most current parallel file systems do not provide a collective i/o interface. The result is that the high-level pattern of the current i/o operation is not entirely visible to the i/o subsystem, thus limiting opportunities for optimization. The collective i/o interface in Panda overcomes this difficulty. <p> Striping arrays across multiple i/o nodes allows them to be read and written in parallel [55, 25]. The striping unit in Panda is an array chunk, a distinguishing feature compared to related work where the arrays were written to disk in traditional (row-major or column-major) order <ref> [32, 33, 17] </ref> with the disk block as the striping unit. Under the server-directed i/o architecture, compression can occur on the compute nodes or the i/o nodes in the case of natural chunking. <p> Note the clients and servers play a different role in the server-directed i/o architecture than in traditional client/server architectures where the clients make requests of the server. While the server-directed i/o architecture was inspired by disk-directed i/o <ref> [32] </ref>, a number of characteristics distinguish between them. Disk-directed i/o assumes detailed information about the physical layout of the file on disk, and sorts disk blocks in order to achieve sequential disk i/o. <p> An i/o system based on the two-phase approach could use Panda to output the data following the reorganization phase. Disk-directed i/o for collective i/o operations involving arrays is proposed in <ref> [32] </ref>. Under this approach, compute nodes tell the i/o nodes about a collective i/o request and, based on this semantic information, the i/o nodes direct the flow of data during the read or write operation. <p> To date, the disk-directed i/o project has focused on rearranging arrays to get them into traditional order on disk, while the Panda work emphasizes the benefits of alternative storage layouts. The simulation in <ref> [32] </ref> shows disk-directed i/o to have advantages over traditional caching. They argue that disk-directed i/o offers all the advantages of two-phase i/o and provides additional advantages as well, such as the ability to overlap the i/o and rearrangement process as well as exploit details of the low-level physical data layout.
Reference: [33] <author> David Kotz. </author> <title> Disk-directed i/o for an out-of-core computation. </title> <type> Technical Report PCS-TR95-251, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> January </month> <year> 1995. </year> <note> Submitted to High Performance Distributed Computing '95. </note>
Reference-contexts: Striping arrays across multiple i/o nodes allows them to be read and written in parallel [55, 25]. The striping unit in Panda is an array chunk, a distinguishing feature compared to related work where the arrays were written to disk in traditional (row-major or column-major) order <ref> [32, 33, 17] </ref> with the disk block as the striping unit. Under the server-directed i/o architecture, compression can occur on the compute nodes or the i/o nodes in the case of natural chunking. <p> Simulations show disk-directed i/o has advantages over traditional caching for an out-of-core LU decomposition problem <ref> [33] </ref>. Simulations also show disk directed i/o is effective for irregularly structured requests, data-dependent distributions, and data-dependent filtering [34]. There are some disadvantages in implementing disk-directed i/o at the file system physical level, as intended by its creators.
Reference: [34] <author> David Kotz. </author> <title> Expanding the potential for disk-directed i/o. </title> <type> Technical Report PCS-TR95-254, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Simulations show disk-directed i/o has advantages over traditional caching for an out-of-core LU decomposition problem [33]. Simulations also show disk directed i/o is effective for irregularly structured requests, data-dependent distributions, and data-dependent filtering <ref> [34] </ref>. There are some disadvantages in implementing disk-directed i/o at the file system physical level, as intended by its creators. <p> How can the same approach of high-level interfaces with an efficient underlying implementation be applied to other data types? (b) Can server directed i/o be applied to additional data types? <ref> [34] </ref> shows through simulations that disk-directed i/o has potential for efficiently processing non-array data. How can these techniques be applied to server-directed i/o? 128 7.3 Technology transfer Technology transfer was a priority throughout this research.
Reference: [35] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb. </author> <title> The ObjectStore database system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In some cases, this means interleaving data of different types on disk, which may be novel in the scientific community; but clustering of data of different types on disk is already in common use in most object-oriented DBMSes <ref> [35] </ref> and in some relational DBMSes, because it gives good performance for applications, and because the use of an abstract interface for data access means that the programmer need not directly face the complexities of interleaving. 7.2 Future research This research establishes a solid foundation for future research activities. <p> For instance, techniques from Panda would be applicable when building a Datablade [70] for arrays in the Illustra DBMS, the commercial version of POSTGRES. Any object-oriented database <ref> [35, 9] </ref> or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o [7, 8, 51, 5, 66].
Reference: [36] <author> David A. Lane. </author> <title> Visualization of time-dependent flow fields. </title> <booktitle> In Proceedings of IEEE Visualization '93, </booktitle> <pages> pages 32-38, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The algorithmic details of one flow solver are given in [54]. The output data consists almost exclusively of multidimensional arrays [73] and is typically analyzed by scientific visualization tools that perform operations such as particle tracing <ref> [37, 36] </ref>. The data management aspects of flow solvers and visualization tools at NAS steered and motivated this research and are examples of the type of real-world applications which will benefit from the results presented in this thesis. <p> Panda's schema files and sequential architecture provide a strategy to ease this process and make it efficient. At NAS, scientific visualization tools are used to analyze the output data from flow solvers. Particle tracing is a common technique used to visualize fluid flow <ref> [37, 36] </ref>. In a prototype particle tracer at NAS that runs on a Silicon Graphics workstation, the user places a `rake' in the grid of output data, and the particle tracer calculates where particles flow over time, based on the output data from the flow solver.
Reference: [37] <author> David A. Lane. </author> <title> Ufat a particle tracer for time-dependent flow fields. </title> <booktitle> In Proceedings of IEEE Visualization '94, </booktitle> <pages> pages 257-264, </pages> <address> Washington, D.C., </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The algorithmic details of one flow solver are given in [54]. The output data consists almost exclusively of multidimensional arrays [73] and is typically analyzed by scientific visualization tools that perform operations such as particle tracing <ref> [37, 36] </ref>. The data management aspects of flow solvers and visualization tools at NAS steered and motivated this research and are examples of the type of real-world applications which will benefit from the results presented in this thesis. <p> Panda's schema files and sequential architecture provide a strategy to ease this process and make it efficient. At NAS, scientific visualization tools are used to analyze the output data from flow solvers. Particle tracing is a common technique used to visualize fluid flow <ref> [37, 36] </ref>. In a prototype particle tracer at NAS that runs on a Silicon Graphics workstation, the user places a `rake' in the grid of output data, and the particle tracer calculates where particles flow over time, based on the output data from the flow solver.
Reference: [38] <author> Jianzhong Li, Jaideep Srivastava, and Doron Rotem. CMD: </author> <title> A multidimensional decluster-ing method for parallel database systems. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Panda performs sequential i/o for both reads and writes of array data, while log-structured file systems make no guarantees for sequential reads. 106 6.3 Databases Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins [19] and range searching <ref> [38] </ref>. Unfortunately, the vast amount of work on declustering relational data on parallel machines is not relevant for scientific data access patterns. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [40].
Reference: [39] <author> David Maier and David M. Hansen. </author> <title> Bambi meets Godzilla: Object databases for scientific computing. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific 134 and Statistical Database Management, </booktitle> <pages> pages 176-184, </pages> <address> Charlottesville, Virginia, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: File format systems were designed primarily as data interchange vehicles and not as complete data management solutions. Database researchers are examining file format systems and investigating how to integrate them with database technology <ref> [39, 64] </ref>. 6.2 Log-structured file systems Log-structured file systems [53] optimize write operations to the file system by sequentially logging any modifications to existing data rather than overwriting data in place. Both log-structured file systems and Panda aim to achieve high performance through sequential i/o.
Reference: [40] <author> David Maier and Bennet Vance. </author> <title> A call to order. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 1-16, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A <ref> [20; 30; 40] </ref>::A [50; 60; 70]). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. <p> Unfortunately, the vast amount of work on declustering relational data on parallel machines is not relevant for scientific data access patterns. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk <ref> [40] </ref>. A few commercial DBMSes support multidimensional arrays, including Interbase [29], Orion [76], and Stratum [57]. Like the file format systems just described, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk. <p> Although the needs of scientific applications coincide with those of traditional database systems in this regard, today's commercial database management systems are not immediately applicable to scientific computing with large arrays, because they lack many characteristics and facilities needed in a computation-intensive environment <ref> [40] </ref>. Panda's notion of collective i/o has similarities to traditional databases which support bulk loading and indexing operations for improved performance. For example, loading a large database may be more efficient using an interface supporting a bulk load of many records simultaneously rather than using a record-at-a-time interface.
Reference: [41] <author> Udi Manber. </author> <title> A text compression scheme that allows fast searching directly in the compressed file. </title> <type> Technical Report 93-07, </type> <institution> Department of Computer Science, University of Ari-zona, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Compression has occasionally been integrated in data management systems [63, 27]. The MG retrieval system [3] uses compression for documents, indexes, and images [75] and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. <ref> [41, 42] </ref> also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [42] <author> Udi Manber and Sun Wu. GLIMPSE: </author> <title> A tool to search through a text compression scheme that allows fast searching entire file systems. </title> <type> Technical Report 93-34, </type> <institution> Department of Computer Science, University of Arizona, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Compression has occasionally been integrated in data management systems [63, 27]. The MG retrieval system [3] uses compression for documents, indexes, and images [75] and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. <ref> [41, 42] </ref> also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [43] <author> A.C. McKeller and E.G. Coffman Jr. </author> <title> Organizing matrices and matrix operations for paged virtual memory systems. </title> <journal> Communications of the ACM, </journal> <volume> 12(3) </volume> <pages> 153-156, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: Traditional array order is not always optimal for in-memory arrays. Studies have shown that page faulting in a virtual memory environment can sometimes be reduced by storing arrays in alternative orders, such as submatrix storage <ref> [43] </ref>. [20] extends this work by showing efficient strategies to reorganize an array from traditional order to submatrix order. [49] presents a balanced discussion of the issues surrounding in-memory tiling (submatrix storage). <p> Panda's design incorporates facilities for chunking, interleaving, subchunking, packing, disk-block alignment, overlaps, and compression. The default schema settings in Panda are dependent on the implementation architecture. 3.1 Chunking sections [68], blocks [72], and submatrices <ref> [43] </ref>. The chunks themselves form another multidi 19 mensional array of possibly lower rank, which this thesis refers to as the array of chunks. Figure 1.1 has a 6 fi 5 fi 4 array of chunks.
Reference: [44] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: The performance and portability of the server-directed i/o architecture are impressive in light of the modest software investment. Panda is roughly 5000 lines of C++ code, uses the MPI message passing library <ref> [44] </ref>, and is targeted for applications written in C++, C, and Fortran. Panda uses no custom file system or operating system features, which gives Panda a strong potential for easy portability to other parallel platforms and to network of workstation architectures. <p> The current strategy requires a single message between one client and one server, which will scale as the number of nodes increases and insulates the clients from needing to know the total number of i/o nodes. The current implementation uses MPI <ref> [44] </ref> for message passing and supports the Unix file system on each i/o node, requiring no parallel or distributed file system capabilities across the Panda servers running on the i/o nodes.
Reference: [45] <author> S. A. Moyer and V. S. Sunderam. </author> <title> A parallel i/o system for high -performance distributed computing. </title> <booktitle> In Proceedings of the IFIP WG10.3 Working Conference on Programming Environments for Massively Parallel Distributed Systems, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Vesta provides a user-definable view of parallel files that is applicable to array data. The application is given control over the stripe size. Panda provides a higher-level semantic interface than Vesta. PIOUS <ref> [46, 45] </ref> is a parallel file system architecture for a network computing environment. A prime focus of PIOUS is to provide transactions as a generalized concurrency control and fault tolerance mechanism. Two transaction types, stable and volatile, are provided to allow user-selectable fault tolerance.
Reference: [46] <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel i/o system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Vesta provides a user-definable view of parallel files that is applicable to array data. The application is given control over the stripe size. Panda provides a higher-level semantic interface than Vesta. PIOUS <ref> [46, 45] </ref> is a parallel file system architecture for a network computing environment. A prime focus of PIOUS is to provide transactions as a generalized concurrency control and fault tolerance mechanism. Two transaction types, stable and volatile, are provided to allow user-selectable fault tolerance.
Reference: [47] <author> Harish Nag. </author> <type> Personal Communication, </type> <month> March </month> <year> 1995. </year>
Reference-contexts: Experiments show that even for a fixed amount of data, CFS gives better performance if a compute node issues a single write request for its data than if the node divides the data into several write requests. This finding was confirmed by independent testing conducted at Intel <ref> [47] </ref>. A premature write of a partially-filled disk block can be particularly disastrous in CFS, which prefetches 7 disk blocks on every read [47], potentially ejecting data from the i/o node buffers before long contiguous writes have had time to accumulate. 79 array order and natural chunked schemas on disk, and <p> This finding was confirmed by independent testing conducted at Intel <ref> [47] </ref>. A premature write of a partially-filled disk block can be particularly disastrous in CFS, which prefetches 7 disk blocks on every read [47], potentially ejecting data from the i/o node buffers before long contiguous writes have had time to accumulate. 79 array order and natural chunked schemas on disk, and a BLOCK,BLOCK,BLOCK schema in memory. Throughput includes preallocation time. We conducted performance experiments to verify these hypotheses.
Reference: [48] <institution> National Center for Supercomputing Applications, University of Illinois. </institution> <note> NCSA HDF Reference Manual, Version 3.3, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems <ref> [48, 71] </ref>, database management systems [29, 57, 76], or hand-coded persistence. It is common for current scientific applications which access persistent arrays to utilize existing file system interfaces directly. Many file systems adopt the Unix file system model, even on parallel machines. <p> The problem is compounded when multiple arrays are stored in a file. Compression at the file level is particularly inappropriate in the common case of self-describing files <ref> [48, 71] </ref> for an application might have to decompress an entire file just to look at the metadata for the file and determine 27 whether it is of interest. Since Panda understands array chunks, compression in Panda is sup-ported at the array chunk level. <p> If high performance is achieved using this approach, it will have immediate and broad applicability. Machine independent data files are critical to real-world applications. Current scientific data management systems such as HDF <ref> [48] </ref> provide capabilities to support easy data migration between different platforms, such as machine independent data types and byte ordering facilities. Panda array files created on one hardware platform can, in theory, be migrated to another platform and still be accessed using Panda's API. <p> Now several file format systems in widespread use support storing multidimensional array data. Hierarchical Data Format (HDF) <ref> [48] </ref> is a self-describing, machine independent file format system developed at the National Center for Supercomputing Applications (NCSA) that supports storing multiple objects in a file, including multidimensional arrays.
Reference: [49] <author> Gary Newman. </author> <title> Organizing arrays for paged memory systems. </title> <journal> Communications of the ACM, </journal> <volume> 38(7) </volume> <pages> 93-103, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Studies have shown that page faulting in a virtual memory environment can sometimes be reduced by storing arrays in alternative orders, such as submatrix storage [43]. [20] extends this work by showing efficient strategies to reorganize an array from traditional order to submatrix order. <ref> [49] </ref> presents a balanced discussion of the issues surrounding in-memory tiling (submatrix storage). Image processing systems have also exploited alternative storage layouts for in-memory arrays [72]. 2 Traditional array order in a file is also not optimal for some applications. <p> memory to hold the working set of data at each time step. mapped files with different physical schemas. 103 Further experience is also needed to determine the performance impact of the cost of com-puting offsets with chunked physical schemas versus the simpler computation to compute offsets in traditional ordered arrays <ref> [49] </ref>. Logically, to compute the correct offset in a chunked physical schema one must first compute the offset in the array of chunks for the chunk containing the desired array element and then compute the correct offset within the chunk.
Reference: [50] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year> <month> 135 </month>
Reference-contexts: Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A [20; 30; 40]::A <ref> [50; 60; 70] </ref>). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. An emphasis on high-level interfaces, even within Panda itself, gives easy support for `strided' requests since clients and servers send logical requests for subchunks to each other. <p> The current implementation is optimized for write-centered operations such as checkpoint and time step output. Performance for restart is not critical and the current implementation contains no special optimizations for reading arrays. Table 5.3 details the characteristics of the Intel iPSC/860 at NAS <ref> [50] </ref>, a parallel machine with 128 compute nodes and 10 additional i/o nodes. Each i/o node has approximately a 1 MB buffer for files and a single attached disk. By default, files are striped across all 10 disks and the striping unit is the disk block. <p> As a basis for judging Panda's performance, note that the peak theoretical i/o throughput for the NAS iPSC/860's file system, Concurrent File System (CFS), is 10 MB/s, due to i/o node-to-disk hardware limitations, and the practical maximum that can be expected for writes is 7-8 MB/s <ref> [50] </ref>. Network contention is not a factor in i/o performance on the NAS iPSC/860, and the critical resource is i/o node buffer space [50]. CFS allows the user to preallocate space for a file, and tries to allocate sequential regions of disk for preallocated files. <p> system, Concurrent File System (CFS), is 10 MB/s, due to i/o node-to-disk hardware limitations, and the practical maximum that can be expected for writes is 7-8 MB/s <ref> [50] </ref>. Network contention is not a factor in i/o performance on the NAS iPSC/860, and the critical resource is i/o node buffer space [50]. CFS allows the user to preallocate space for a file, and tries to allocate sequential regions of disk for preallocated files. Preallocation greatly improves i/o throughput in CFS, so all the experiments use preallocated files. <p> The iPSC/860 is a busy machine, shared with other users; thus all throughputs reported are the best of 5 or more runs. The experiments also showed CFS to be very erratic and difficult to analyze. The performance study conducted in <ref> [50] </ref> reports that timings varied as much as 20% between runs when the machine was dedicated to one user. Preallocation is slow; in all experiments except Figure 5.24, preallocation is performed before timing begins. <p> Note that for all schemas in Figure 5.24, throughput first rises, then declines as the number of processors increases. Throughput rises until reaching 16 or 32 processors, because a lesser number of processors is insufficient to keep the i/o subsystem busy <ref> [50] </ref>. Throughput declines thereafter because the NAS iPSC/860's i/o system is not scalable; the first bottleneck visible in the system is the shortage of i/o node buffer space, as described earlier. <p> expect that greater throughput could be obtained by restricting the number of processors allowed to issue i/o requests simultaneously, so that longer contiguous writes would be possible once the buffer was filled. (In a truly scalable i/o system, such restrictions would bring no benefit.) These are known as grouped writes <ref> [50] </ref>. We conducted experiments to write twenty 10 MB or twenty 20 MB arrays to compare interleaved and non-interleaved schemas. These data sets, which total 200 MB and 400 MB respectively, only fit on 64 or 128 processors on the NAS iPSC/860. <p> In contrast, with large numbers of processors, the performance for non-interleaved schemas improves as array sizes are increased, and eventually approaches the performance of interleaved schemas as array sizes increase. This pattern is consistent with the findings in <ref> [50] </ref>. We also conducted experiments writing two 8 MB arrays comparing interleaved and non-interleaved schemas, as well as grouped writes using groups of 16 processors. <p> Subchunk size is chosen so that all the first subchunks from each processor will fit into the i/o node buffers, so they can be written out with a single disk access. A global synchronization after writing each subchunk will keep the processors approximately synchronized in their writes. <ref> [50] </ref> shows that the overhead cost of a global synchronization is minimal on the iPSC/860. We conducted experiments using subchunking to see if performance improves for non-interleaved chunks when the array being written is too big to fit into the i/o node buffers. <p> As the number of processors increases past 32 processors, throughput for uncompressed data begins to drop because the CFS file system does not cope well with large numbers of simultaneous i/o requests <ref> [50] </ref>. While uncompressed throughput is falling, compressed throughput is rising, because the overhead of compressing the chunks is divided among more and more processors. At some point between 8 and 64 processors, depending on the compression ratio, throughput is greater for compressed chunks than for uncompressed chunks. <p> For example, deliberate staggering of i/o requests has been found to increase performance of CFS i/o requests <ref> [50] </ref>. Experiments were conducted in which all processors were synchronized after compression and before writing, and found no significant change from the results in Figure 5.32.
Reference: [51] <author> Michael Paleczny, Ken Kennedy, and Charles Koelbel. </author> <title> Compiler support for out-of-core arrays on data parallel machines. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Panda is one example of a parallel i/o library that does support such alternatives. No performance data is presented in [8], so no comparison to Panda is possible. A compiler supporting out-of-core arrays is described in <ref> [51] </ref>. The compiler inserts explicit statements to move array data in and out of memory, guided by i/o distribution statements similar to Fortran D's in-memory data distribution statements. They report on early experiments (partially compiled by hand) which show superior performance compared to a program using virtual memory. <p> They propose two new directives to specify which arrays are to be out-of-core and how much memory is available in a processor to hold out-of-core array data. Ideas from [66] and <ref> [51] </ref> are combined in [5] which describes a run-time system employing the two-phase i/o approach for handling array i/o. <p> Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o <ref> [7, 8, 51, 5, 66] </ref>. In sum, the technology in Panda will be commonly used in future systems that handle persistent arrays.
Reference: [52] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The performance study in the previous 113 chapter showed that logical level disk-directed i/o offers good performance advantages. To the best of our knowledge, Panda is the first implementation of concepts from disk-directed i/o. Traditional parallel file systems provide a Unix-like interface for reading and writing to a file <ref> [52, 23] </ref>. In these systems, a file is simply a stream of bytes, often striped across multiple disks, and the file system has no built-in support for higher-level objects such as arrays. Several efforts are underway to construct scalable parallel file systems.
Reference: [53] <author> M. Rosenblum and J. K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1992. </year>
Reference-contexts: Using independent file pointers for different processors (mode 0 of CFS), either all processors must synchronize after compression to compute offsets for writing, or else use predetermined offsets that may waste a lot of space in the output file. Log structured files <ref> [53] </ref> (mode 1 of CFS) can be used to write out compressed chunks in a first-come-first-served order, as soon as each chunk is compressed. <p> File format systems were designed primarily as data interchange vehicles and not as complete data management solutions. Database researchers are examining file format systems and investigating how to integrate them with database technology [39, 64]. 6.2 Log-structured file systems Log-structured file systems <ref> [53] </ref> optimize write operations to the file system by sequentially logging any modifications to existing data rather than overwriting data in place. Both log-structured file systems and Panda aim to achieve high performance through sequential i/o.
Reference: [54] <author> J. S. Ryan and S. K. Weeratunga. </author> <title> Parallel computation of 3-D Navier-Stokes flowfields for supersonic vehicles. </title> <booktitle> In 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, NV, </address> <year> 1993. </year> <note> AIAA Paper 93-0064. </note>
Reference-contexts: A `flow solver,' usually a Fortran program that runs on a parallel computer system, takes the grid data and generates output data by solving systems of equations that govern fluid flow. The algorithmic details of one flow solver are given in <ref> [54] </ref>. The output data consists almost exclusively of multidimensional arrays [73] and is typically analyzed by scientific visualization tools that perform operations such as particle tracing [37, 36].
Reference: [55] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: In practice, a distributed file system may prove useful in managing the separate files on the i/o nodes after the application completes and data must be migrated to more permanent storage. Striping arrays across multiple i/o nodes allows them to be read and written in parallel <ref> [55, 25] </ref>. The striping unit in Panda is an array chunk, a distinguishing feature compared to related work where the arrays were written to disk in traditional (row-major or column-major) order [32, 33, 17] with the disk block as the striping unit.
Reference: [56] <author> Sunita Sarawagi and Michael Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <booktitle> In Proceedings of the 10th International Conference on Data Engineering, </booktitle> <pages> pages 328-336, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Like the file format systems just described, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chun-ked schemas <ref> [56] </ref>. There are three key distinctions between the work in POSTGRES and Panda. First, the POSTGRES work focuses on read-only applications on sequential machines that exploit the query capabilities of a DBMS, while the Panda work primarily examines write-intensive applications on parallel machines and scientific visualization applications. <p> Tertiary storage is a useful medium to store large scientific data sets at lower costs than magnetic disk. How does this research apply to tertiary storage? Tertiary storage was beyond the scope of this research. Other researchers have looked at tertiary storage in the context of chunking <ref> [56, 10] </ref>. (a) What are the proper high-level abstractions for tertiary storage? Can physical and logical data independence be provided? (b) What interface extensions may be required for Panda to effectively support tertiary storage? (c) How do the techniques of chunking and compression in Panda relate to data on tertiary storage?
Reference: [57] <institution> Scientific and Engineering Software, Austin, Texas. </institution> <note> Stratum Technical Reference Manual, </note> <month> October </month> <year> 1989. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [48, 71], database management systems <ref> [29, 57, 76] </ref>, or hand-coded persistence. It is common for current scientific applications which access persistent arrays to utilize existing file system interfaces directly. Many file systems adopt the Unix file system model, even on parallel machines. <p> Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [40]. A few commercial DBMSes support multidimensional arrays, including Interbase [29], Orion [76], and Stratum <ref> [57] </ref>. Like the file format systems just described, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chun-ked schemas [56].
Reference: [58] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server directed collective i/o in panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: This may result in corrupted data if a failure occurs during the checkpoint process. 6 The Panda server-directed i/o architecture <ref> [58, 59] </ref> is a prime example of an efficient under-lying implementation for array i/o. The server-directed i/o architecture is targeted for SPMD applications in distributed-memory environments that are closely synchronized during i/o (i.e., collective i/o) and read and write multidimensional arrays.
Reference: [59] <author> K. E. Seamons, Y. Chen M. WInslett, Y. Cho, S. Kuo, P. Jones, J. Jozwiak, and M. Sub-ramaniam. </author> <title> Fast and easy i/o for arrays in large-scale applications. </title> <booktitle> In Seventh IEEE Symposium on Parallel and Distributed Computing, Workshop on Modeling and Specification of I/O, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: This may result in corrupted data if a failure occurs during the checkpoint process. 6 The Panda server-directed i/o architecture <ref> [58, 59] </ref> is a prime example of an efficient under-lying implementation for array i/o. The server-directed i/o architecture is targeted for SPMD applications in distributed-memory environments that are closely synchronized during i/o (i.e., collective i/o) and read and write multidimensional arrays.
Reference: [60] <author> Kent E. Seamons and Marianne Winslett. </author> <title> An efficient abstract interface for multidimensional array i/o. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <address> Washington, D.C., </address> <month> November </month> <year> 1994. </year> <month> 136 </month>
Reference-contexts: This thesis presents a high-level interface for array i/o and three implementation architectures, embodied in the Panda (Persistence AND Arrays) array i/o library <ref> [61, 60] </ref>. The high-level interface contributes to application portability, by encapsulating unnecessary details and being easy to use. Panda encapsulates many physical storage details for application programmers, such as transparently mapping array data to files. Panda is designed for SPMD applications in distributed-memory environments and also for sequential applications. <p> Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A [20; 30; 40]::A <ref> [50; 60; 70] </ref>). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. An emphasis on high-level interfaces, even within Panda itself, gives easy support for `strided' requests since clients and servers send logical requests for subchunks to each other. <p> In general, postprocessors such as visualizers will run faster on chunked data than on unchunked data <ref> [60] </ref> because of the higher degree of locality. If chunks are compressed, however, an application must read an entire chunk even if it only needs a small portion of that chunk, so perhaps i/o costs would rise for applications that only looked at a small amount of data.
Reference: [61] <author> Kent E. Seamons and Marianne Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218-227, </pages> <address> Char-lottesville, Virginia, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: This thesis presents a high-level interface for array i/o and three implementation architectures, embodied in the Panda (Persistence AND Arrays) array i/o library <ref> [61, 60] </ref>. The high-level interface contributes to application portability, by encapsulating unnecessary details and being easy to use. Panda encapsulates many physical storage details for application programmers, such as transparently mapping array data to files. Panda is designed for SPMD applications in distributed-memory environments and also for sequential applications.
Reference: [62] <author> Kent E. Seamons and Marianne Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <address> McLean, Virginia, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: By 26 default, Panda outputs overlaps during checkpoints and ignores overlaps during other output operations. 3.7 Compression A promising high-level software technique for reducing array i/o time is data compression <ref> [62] </ref>. Compression has traditionally been used as a means to reduce disk space requirements. Like chunking, compression tends to concentrate the data of interest to an application on fewer disk pages, potentially reducing i/o requirements.
Reference: [63] <author> M. Stonebraker, E. Wong, P. Kreps, and G. </author> <title> Held. The design and implementation of INGRES. </title> <editor> In M. Stonebraker, editor, </editor> <booktitle> Readings in Database Systems, </booktitle> <pages> pages 37-53. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This benchmark has queries that include non-array data types and operations not applicable to Panda, so no direct performance comparison is possible with the benchmark results. Compression has occasionally been integrated in data management systems <ref> [63, 27] </ref>. The MG retrieval system [3] uses compression for documents, indexes, and images [75] and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. [41, 42] also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [64] <author> Michael Stonebraker. </author> <title> SEQUOIA 2000 a reflection on the first three years. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 108-116, </pages> <address> Charlottesville, Virginia, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: File format systems were designed primarily as data interchange vehicles and not as complete data management solutions. Database researchers are examining file format systems and investigating how to integrate them with database technology <ref> [39, 64] </ref>. 6.2 Log-structured file systems Log-structured file systems [53] optimize write operations to the file system by sequentially logging any modifications to existing data rather than overwriting data in place. Both log-structured file systems and Panda aim to achieve high performance through sequential i/o. <p> If compression is supported in the data management system, the data can be stored compressed in the file system and remain compressed until it is ready for display in the user's application, sometimes referred to as just-in time compression <ref> [64] </ref>. (e) Compression can be studied in the context of server-directed i/o. With natural chunking, compression can occur on the compute nodes similar to the parallel file system architecture.
Reference: [65] <author> Michael Stonebraker, Jim Frew, Kenn Gardels, and Jeff Meredith. </author> <title> The SEQUOIA 2000 storage benchmark. </title> <booktitle> In ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 2-11, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In the Paradise test application, queries are processed that require access to parts of an image. Performance degraded when tile sizes were large enough that unnecessary data was read and uncompressed. The performance of Paradise was compared to other GIS databases using the Sequoia 2000 benchmark <ref> [65] </ref> on a sequential workstation. This benchmark has queries that include non-array data types and operations not applicable to Panda, so no direct performance comparison is possible with the benchmark results. Compression has occasionally been integrated in data management systems [63, 27].
Reference: [66] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and runtime support for out-of-core HPF programs. </title> <booktitle> In Proceedings of the 8 th ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The compiler is able to optimize the i/o using knowledge about the computation. For example, in some cases the compiler can recognize instances where i/o and computation can be overlapped. 110 Research in compiler and run-time support for out-of-core HPF programs is reported in <ref> [66] </ref>. They propose two new directives to specify which arrays are to be out-of-core and how much memory is available in a processor to hold out-of-core array data. Ideas from [66] and [51] are combined in [5] which describes a run-time system employing the two-phase i/o approach for handling array i/o. <p> i/o and computation can be overlapped. 110 Research in compiler and run-time support for out-of-core HPF programs is reported in <ref> [66] </ref>. They propose two new directives to specify which arrays are to be out-of-core and how much memory is available in a processor to hold out-of-core array data. Ideas from [66] and [51] are combined in [5] which describes a run-time system employing the two-phase i/o approach for handling array i/o. <p> Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines. Panda is also an appropriate runtime system for compilers (both in-core and out-of-core) supporting array i/o <ref> [7, 8, 51, 5, 66] </ref>. In sum, the technology in Panda will be commonly used in future systems that handle persistent arrays.
Reference: [67] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel i/o. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: With a distributed view, each process requests only the part of the data it requires. Panda's high-level interfaces classify it as providing a global view. The Parallel And Scalable Software for Input-Output (PASSION) project <ref> [11, 67] </ref> at Syra-cuse is a broad effort to provide high performance parallel i/o at the language, compiler, runtime, and file system level.
Reference: [68] <author> Rajeev Thakur and Alok Choudhary. </author> <title> Accessing sections of out-of-core arrays using an extended two-phase method. </title> <type> Technical Report SCCS-685, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> January </month> <year> 1995. </year> <note> Submitted to the 1995 International Conference on Parallel Processing. 137 </note>
Reference-contexts: This approach has been labeled the direct method <ref> [68] </ref> or traditional caching [32] and is known 3 to perform poorly in many cases due to excess disk seeks and write-back errors. <p> Panda's design incorporates facilities for chunking, interleaving, subchunking, packing, disk-block alignment, overlaps, and compression. The default schema settings in Panda are dependent on the implementation architecture. 3.1 Chunking sections <ref> [68] </ref>, blocks [72], and submatrices [43]. The chunks themselves form another multidi 19 mensional array of possibly lower rank, which this thesis refers to as the array of chunks. Figure 1.1 has a 6 fi 5 fi 4 array of chunks. <p> During the second phase the compute nodes permute the data in memory to the required memory layout. The reverse strategy holds for writes. The two-phase strategy was extended to out-of-core computations in <ref> [68] </ref> to support reading and writing array sections (another term for chunks). In Panda's parallel file systems architecture, the need for a reorganization phase is avoided for writes by letting the in-memory array layout determine the array layout in a file.
Reference: [69] <author> Rajeev Thakur, Ewing Lusk, and William Gropp. </author> <title> I/o characterization of a portable astro-physics application on the ibm sp and intel paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1995. </year> <type> Preprint. </type>
Reference-contexts: These high-level operations are common in other domains besides computational fluid dynamics. An example of an astrophysics application with similar i/o needs is described in <ref> [69] </ref>. An example of an i/o library motivated by the same i/o operations is described in [24]. Two application groups at the National Center for Supercomputing Applications at the University of Illinois have i/o needs similar to those described earlier.
Reference: [70] <author> Michael Ubell. </author> <title> The Montage extensible Datablade architecture. </title> <booktitle> In ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> page 482, </pages> <address> Minneapolis, Minnesota, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Depending on the memory schema and disk schema, a client can receive a request for a subchunk of array data that is not contiguous in its memory (e.g., the client may hold A [1; 1; 1]::A [100; 200; 300] and receive a request for A [20; 30; 40]::A <ref> [50; 60; 70] </ref>). The client is responsible for any reorganization required to assemble the requested subchunk in a temporary buffer when this happens. An emphasis on high-level interfaces, even within Panda itself, gives easy support for `strided' requests since clients and servers send logical requests for subchunks to each other. <p> Concepts from Panda are appropriate for other systems that support multidimensional arrays. For example, any extended relational or object oriented database that supports an array data type will find this research applicable for handling large arrays. For instance, techniques from Panda would be applicable when building a Datablade <ref> [70] </ref> for arrays in the Illustra DBMS, the commercial version of POSTGRES. Any object-oriented database [35, 9] or object-oriented file system [31] will find techniques in Panda useful for creating array classes and methods, especially those intended for parallel machines.
Reference: [71] <institution> University Corporation for Atmospheric Research, Unidata Program Center. </institution> <note> NetCDF User's Guide, Version 2.0, </note> <month> October </month> <year> 1992. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems <ref> [48, 71] </ref>, database management systems [29, 57, 76], or hand-coded persistence. It is common for current scientific applications which access persistent arrays to utilize existing file system interfaces directly. Many file systems adopt the Unix file system model, even on parallel machines. <p> The problem is compounded when multiple arrays are stored in a file. Compression at the file level is particularly inappropriate in the common case of self-describing files <ref> [48, 71] </ref> for an application might have to decompress an entire file just to look at the metadata for the file and determine 27 whether it is of interest. Since Panda understands array chunks, compression in Panda is sup-ported at the array chunk level. <p> HDF was designated by NASA as the preferred file format for the Earth Observing System Data and Information System (EOSDIS) version 0. NetCDF <ref> [71] </ref> is an i/o library that is also used to store scientific information in self-describing, machine independent files. HDF was recently enhanced to support the NetCDF interface. Both packages organize fixed size arrays using traditional array ordering on disk. HDF and NetCDF support arrays of unlimited size along one dimension.
Reference: [72] <author> Ben Tsutom Wada. </author> <title> A virtual memory system for picture processing. </title> <journal> Communications of the ACM, </journal> <volume> 27(5) </volume> <pages> 444-454, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Image processing systems have also exploited alternative storage layouts for in-memory arrays <ref> [72] </ref>. 2 Traditional array order in a file is also not optimal for some applications. It is ideal for sequential applications that need to access a single array element, contiguous array elements along the fastest varying dimension, or an entire persistent array that fits in main memory. <p> Panda's design incorporates facilities for chunking, interleaving, subchunking, packing, disk-block alignment, overlaps, and compression. The default schema settings in Panda are dependent on the implementation architecture. 3.1 Chunking sections [68], blocks <ref> [72] </ref>, and submatrices [43]. The chunks themselves form another multidi 19 mensional array of possibly lower rank, which this thesis refers to as the array of chunks. Figure 1.1 has a 6 fi 5 fi 4 array of chunks.
Reference: [73] <author> P. Walatka, P. Buning, L. Pierce, and P. Elson. </author> <title> PLOT3D User's Manual, </title> <type> NASA Technical Memorandum 101067. </type> <institution> NASA Ames Research Center, Unidata Program Center, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The algorithmic details of one flow solver are given in [54]. The output data consists almost exclusively of multidimensional arrays <ref> [73] </ref> and is typically analyzed by scientific visualization tools that perform operations such as particle tracing [37, 36].
Reference: [74] <author> T. A. Welch. </author> <title> A technique for high performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17 </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: No specific information is provided describing how the tile sizes and shapes are determined in Paradise. Paradise supports compression of individual tiles for the same scalability reasons that Panda supports compression at the array chunk level. Paradise uses the basic LZW compression algorithm <ref> [74] </ref> and will support domain specific compression algorithms in the future. Compression experiments conducted using Paradise revealed the same phenomenon seen in Panda experiments, that tiles from the same array have different compression ratios.
Reference: [75] <author> I.H. Whitten, A. Moffat, </author> <title> and T.C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This benchmark has queries that include non-array data types and operations not applicable to Panda, so no direct performance comparison is possible with the benchmark results. Compression has occasionally been integrated in data management systems [63, 27]. The MG retrieval system [3] uses compression for documents, indexes, and images <ref> [75] </ref> and finds that retrieval performance is sustained and sometimes improved compared to systems providing no compression. [41, 42] also describes how compression can be used for space and i/o efficiency for text searching.
Reference: [76] <institution> XIDAK Inc., Palo Alto, California. Overview of Orion, </institution> <note> Version 2 Release 3(4), </note> <year> 1991. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [48, 71], database management systems <ref> [29, 57, 76] </ref>, or hand-coded persistence. It is common for current scientific applications which access persistent arrays to utilize existing file system interfaces directly. Many file systems adopt the Unix file system model, even on parallel machines. <p> Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [40]. A few commercial DBMSes support multidimensional arrays, including Interbase [29], Orion <ref> [76] </ref>, and Stratum [57]. Like the file format systems just described, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chun-ked schemas [56].
Reference: [77] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a language specification. </title> <type> Technical Report ICASE Interim Report 21, </type> <institution> MS 132c, ICASE, </institution> <year> 1992. </year>
Reference-contexts: In High Performance Fortran and related languages <ref> [21, 22, 28, 77] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
References-found: 77

