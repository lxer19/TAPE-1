URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/umsi-97-244.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/
Root-URL: http://www.cs.umn.edu
Title: Domain Decomposition and Multi-Level Type Techniques for General Sparse Linear Systems  
Author: Yousef Saad Maria Sosonkina Jun Zhang 
Keyword: Key words: Schur complement techniques; Incomplete LU factorization; Schwarz iterations; Multi-elimination; Multi-level ILU preconditioners; Krylov subspace methods.  
Date: February 12, 1998  
Abstract: Domain-decomposition and multi-level techniques are often formulated for linear systems that arise from the solution of elliptic-type Partial Differential Equations. In this paper, generalizations of these techniques for irregularly structured sparse linear systems are considered. An interesting common approach used to derive successful preconditioners is to resort to Schur complements. In particular, we discuss a multi-level domain decomposition-type algorithm for iterative solution of large sparse linear systems based on independent subsets of nodes. We also discuss a Schur complement technique that utilizes incomplete LU factorizations of local matrices.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Bank and C. Wagner. </author> <title> Multilevel ILU decomposition. </title> <type> Technical report, </type> <institution> Department of Mathematics, University of California at San Diego, </institution> <address> San-Diego, CA, </address> <year> 1997. </year>
Reference-contexts: In a recent report, some of these multi-level precon-ditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation [3]. Other multi-level preconditioning and domain decomposition techniques have also been developed in finite element analysis or for unstructured meshes <ref> [1, 5] </ref>. The ILUM preconditioner has been extended to a block version (BILUM) in which the B block in (1) is block-diagonal. This method utilizes independent sets of small clusters (or blocks), instead of single nodes [4, 21].
Reference: [2] <author> T. Barth, T. F. Chan, and W.-P. Tang. </author> <title> A parallel algebraic non-overlapping domain decomposition method for flow problems. </title> <type> Technical report, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <year> 1998. </year> <note> In preparation. </note>
Reference-contexts: The matrix of the related Schur complement system is also regarded as a distributed object and never formed explicitly. The main difference between our approach and the methods described in <ref> [2, 7] </ref>, is that we do not seek to compute an approximation to the Schur complement.
Reference: [3] <author> E. F. F. Botta, K. Dekker, Y. Notay, A. van der Ploeg, C. Vuik, F. W. Wubs, , and P. M. de Zeeuw. </author> <title> How fast the Laplace equation was solved in 1995. </title> <journal> Applied Numerical Mathematics, </journal> <volume> 24 </volume> <pages> 439-455, </pages> <year> 1997. </year>
Reference-contexts: Similar preconditioners have been designed and tested in [4] to show near grid-independent convergence for certain type of problems. In a recent report, some of these multi-level precon-ditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation <ref> [3] </ref>. Other multi-level preconditioning and domain decomposition techniques have also been developed in finite element analysis or for unstructured meshes [1, 5]. The ILUM preconditioner has been extended to a block version (BILUM) in which the B block in (1) is block-diagonal.
Reference: [4] <author> E. F. F. Botta and W. Wubs. MRILU: </author> <title> it's the preconditioning that counts. </title> <type> Technical Report W-9703, </type> <institution> Department of Mathematics, University of Groningen, </institution> <address> The Netherlands, </address> <year> 1997. </year>
Reference-contexts: Then the idea is applied recursively, computing a sequence of Schur complement (or reduced) systems. The last of these reduced systems is solved by an iterative solver. This recursively constructed preconditioner has a multi-level structure and a good degree of parallelism. Similar preconditioners have been designed and tested in <ref> [4] </ref> to show near grid-independent convergence for certain type of problems. In a recent report, some of these multi-level precon-ditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation [3]. <p> The ILUM preconditioner has been extended to a block version (BILUM) in which the B block in (1) is block-diagonal. This method utilizes independent sets of small clusters (or blocks), instead of single nodes <ref> [4, 21] </ref>. In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs [4] and Reusken [13] is in the choice of variables for the reduced system. <p> This method utilizes independent sets of small clusters (or blocks), instead of single nodes [4, 21]. In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs <ref> [4] </ref> and Reusken [13] is in the choice of variables for the reduced system. In [4] and [13], the nodes in the reduced system, i.e., the unknowns associated with the submatrix C in (1), are those nodes of the independent set itself and this leads to a technique which is akin <p> In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs <ref> [4] </ref> and Reusken [13] is in the choice of variables for the reduced system. In [4] and [13], the nodes in the reduced system, i.e., the unknowns associated with the submatrix C in (1), are those nodes of the independent set itself and this leads to a technique which is akin to an (algebraic) multigrid approach [14].
Reference: [5] <author> T. F. Chan, S. Go, , and J. Zou. </author> <title> Multilevel domain decomposition and multigrid methods for unstructured meshes: algorithms and theory. </title> <type> Technical Report 95-24, </type> <institution> Department of Mathematics, University of California at Los Angeles, </institution> <address> Los Angeles, CA, </address> <year> 1996. </year>
Reference-contexts: In a recent report, some of these multi-level precon-ditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation [3]. Other multi-level preconditioning and domain decomposition techniques have also been developed in finite element analysis or for unstructured meshes <ref> [1, 5] </ref>. The ILUM preconditioner has been extended to a block version (BILUM) in which the B block in (1) is block-diagonal. This method utilizes independent sets of small clusters (or blocks), instead of single nodes [4, 21].
Reference: [6] <author> A. Chapman, Y. Saad, and L. Wigton. </author> <title> High-order ILU preconditioners for CFD problems. </title> <type> Technical Report UMSI 96/14, </type> <institution> Minnesota Supercomputer Institute, </institution> <year> 1996. </year>
Reference-contexts: We point out that both tests yielded slow convergence for k = 5. Each row of the matrix WIGTO966 has 62 nonzeros on average. This matrix is very hard to solve by ILUT <ref> [6] </ref> which only worked with a large value of lf il, i.e., a large number of elements per row. Test results using ILUT with different lf il and droptol are given in Table 5. We note the large values for the efficiency ratio and the sparsity ratio.
Reference: [7] <author> V. Eijkhout and T. Chan. </author> <title> ParPre a parallel preconditioners package, reference manual for version 2.0.17. </title> <type> Technical Report CAM Report 97-24, </type> <institution> UCLA, </institution> <year> 1997. </year>
Reference-contexts: The matrix of the related Schur complement system is also regarded as a distributed object and never formed explicitly. The main difference between our approach and the methods described in <ref> [2, 7] </ref>, is that we do not seek to compute an approximation to the Schur complement. <p> This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables.
Reference: [8] <author> G. Golub and J. M. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing. </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: A typical example is the employment of singular value decomposition. Special blocks such as arrow-head matrices may also be constructed that has no additional fill-in in the inverse <ref> [8] </ref>. Furthermore, it is not necessary that all blocks should be of the same size. For unstructured matrices, blocks with variable sizes may be able to capture 14 Table 4: Characteristic parameters for solving the VENKAT50 matrix for different sizes of uniform blocks.
Reference: [9] <author> W. D. Gropp and B. Smith. </author> <title> User's manual for KSP: data-structure neutral codes implementing Krylov space methods. </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables.
Reference: [10] <author> Scott A. Hutchinson, John N. Shadid, and R. S. Tuminaro. </author> <title> Aztec user's guide. version 1.0. </title> <type> Technical Report SAND95-1559, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1995. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables.
Reference: [11] <author> M. T. Jones and P. E. Plassmann. </author> <title> BlockSolve95 users manual: Scalable library software for the solution of sparse linear systems. </title> <type> Technical Report ANL-95/48, </type> <institution> Argonne National Lab., Argonne, IL., </institution> <year> 1995. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables.
Reference: [12] <author> J. Mandel. </author> <title> Balancing domain decomposition. </title> <journal> Communications in Applied Numerical Methods, </journal> <volume> 9 </volume> <pages> 233-241, </pages> <year> 1993. </year>
Reference-contexts: In the successive Schur complement matrices obtained, each block contains the internal nodes of a subdomain. The inverse and application of all blocks on the same level can be done in parallel. One distinction with traditional domain decomposition methods <ref> [12] </ref> is that all subdomains are constructed algebraically and exploit no physical information. In addition, the reduced system (coarse grid acceleration) is solved by a multi-level recursive process akin to a multigrid technique. We define several measures to characterize the efficiency of BILUM (and other preconditioning techniques).
Reference: [13] <author> A. A. Reusken. </author> <title> Approximate cyclic reduction preconditioning. </title> <type> Technical Report RANA 97-02, </type> <institution> Department of Mathematics and Computing Science, Eindhoven University of Technology, </institution> <address> The Netherlands, </address> <year> 1997. </year>
Reference-contexts: In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs [4] and Reusken <ref> [13] </ref> is in the choice of variables for the reduced system. In [4] and [13], the nodes in the reduced system, i.e., the unknowns associated with the submatrix C in (1), are those nodes of the independent set itself and this leads to a technique which is akin to an (algebraic) <p> In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs [4] and Reusken <ref> [13] </ref> is in the choice of variables for the reduced system. In [4] and [13], the nodes in the reduced system, i.e., the unknowns associated with the submatrix C in (1), are those nodes of the independent set itself and this leads to a technique which is akin to an (algebraic) multigrid approach [14].
Reference: [14] <author> J W. Ruge and K Stuben. </author> <title> Efficient solution of finite difference and finite element equations. </title> <editor> In In D. J. Paddon and H. Holstein, editors, </editor> <title> Multigrid Methods for Integral and Differential Equations, </title> <address> pages 169-212, Oxford, 1985. </address> <publisher> Clarendon Press. </publisher> <pages> 17 </pages>
Reference-contexts: In [4] and [13], the nodes in the reduced system, i.e., the unknowns associated with the submatrix C in (1), are those nodes of the independent set itself and this leads to a technique which is akin to an (algebraic) multigrid approach <ref> [14] </ref>. An approximate inverse technique is used to invert the top-left submatrix B in (1) which is no longer diagonal or block-diagonal. In their implementations, these authors employ a simple approximate inverse technique which usually requires diagonal dominance in the B matrix.
Reference: [15] <author> Y. Saad. </author> <title> Parallel sparse matrix library (P SPARSLIB): The iterative solvers module. In Advances in Numerical Methods for Large Sparse Sets of Linear Equations, Number 10, Matrix Analysis and Parallel Computing, </title> <type> PCG 94, </type> <pages> pages 263-276, </pages> <address> Keio University, Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables. <p> In practice, we may use a double dropping strategy similar to the one just suggested, for the EB 1 and A 1 matrices, possibly with different parameters. 4 Numerical Experiments The Schur-ILU factorization has been implemented in the framework of the PSPARSLIB package <ref> [18, 20, 15] </ref> and was tested on a variety of machines. The experiments reported here have been performed on a CRAY T3E-900. ILUM and Block ILUM have been implemented and tested on sequential machines. Additional experiments with BILUM, specifically with small blocks, have been reported elsewhere, see [21].
Reference: [16] <author> Y. Saad. ILUM: </author> <title> a multi-elimination ILU preconditioner for general sparse matrices. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 17(4) </volume> <pages> 830-847, </pages> <year> 1996. </year>
Reference-contexts: Two techniques in this class are the Schur complement technique (Schur-ILU) developed in [19] and the point and block multi-elimination ILU preconditioners (ILUM, BILUM) discussed in <ref> [16, 21] </ref>. The framework of the Schur-ILU preconditioner is that of a distributed sparse linear system, in which equations are assigned to different processors according to a mapping determined by a graph partitioner. <p> This process can be continued for a few more levels and the last level system can be solved with a standard iterative method. This idea was exploited in <ref> [16] </ref> for blocks B i of the smallest possible size, namely one. In [21], the idea was generalized to blocks larger than one and a number of heuristics were suggested for selecting these blocks. <p> The last reduced system obtained is solved by a direct method or a preconditioned iterative method. Once the BILUM preconditioner is constructed, the solution process (application of BILUM) consists of the (block) forward and backward steps <ref> [16] </ref>. At each step (level), we partition the vector x j as x j = y j ! corresponding to the two-by-two block matrix (14) and perform the following steps: Copy the right-hand side vector b to x 0 . <p> The Speed-up and Efficiency plots in Figure 5 show the `adjusted' measures which factor out iteration increases. 4.2 Experiments with BILUM Standard implementations of ILUM and BILUM have been described in detail in <ref> [16, 21] </ref>. We used GMRES (10) as an accelerator for both the inner and outer iterations. The outer iteration process was preconditioned by BILUM with dual dropping strategy as was discussed earlier. The inner iteration process to solve the last reduced system approximately was preconditioned by ILUT [17].
Reference: [17] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: X j2N i The result of this multiplication affects only the local interface unknowns, which is indicated by a zero in the top part of the second term of the left-hand side of (4). 2.2 Schur complement systems This section gives a brief background on Schur complement systems; see e.g., <ref> [22, 17] </ref>, for additional details and references. The Schur complement system is obtained by eliminating the variable u i from the system (4). <p> These techniques, termed "induced preconditioners" (see, e.g., <ref> [17] </ref>), are based on a reordered version of the global system (2) in which all the internal vector components u = (u 1 ; : : : ; u p ) T are labeled first followed by all the interface vector components y, 0 B B B B 1 F 1 <p> Consider the block LU factorization E C = I 0 0 S ; (10) where S is the global Schur complement S = C EB 1 F: This Schur complement matrix is identical to the coefficient matrix of system (7) (see, e.g., <ref> [17] </ref>). The global system (9) can be preconditioned by an approximate LU factorization constructed such that L = I 0 0 M S (11) with M S being some approximation to S. <p> In one method considered in [19] the approximation ~ S i was extracted from an Incomplete LU factorization of A i . The idea is based on the following observation (see <ref> [17] </ref>). <p> , where L i = L B i 0 B i L S i and U i = U B i L 1 0 U S i : (13) Then, L S i U S i is equal to the Schur complement S i associated with the partitioning (4), see <ref> [17, 19] </ref>. <p> Our iterative algorithm consists of GMRES with a small restart value as an accelerator and BILUM as a preconditioner. The last reduced system is solved iteratively by another GMRES preconditioned by an ILUT preconditioner <ref> [17] </ref>. The dropping strategy that we used in [21] is the simplest one. Elements in EB 1 in the L factor and in the reduced system A 1 are dropped whenever their absolute values are less than a threshold tolerance droptol times the average value of the current row. <p> The sparsity ratio was doubled when the block size increased from 1 to 15. Such an uncontrolled large storage requirement may cause serious problems in large scale applications. Inspired by the dual threshold dropping strategy of ILUT <ref> [17] </ref>, we propose a similar dual threshold dropping for BILUM. We first apply the single dropping strategy as above to the EB 1 and A 1 matrices and keep only the largest lf il elements (absolute value) in each row. <p> In these test problems, the matrix rows followed by the columns were scaled by 2-norm. The initial guess was set to zero. A flexible variant of restarted GMRES (FGMRES) <ref> [17] </ref> with a subspace dimension of 20 has been used to solve these problems to reduce the residual norm by 10 6 . <p> We used GMRES (10) as an accelerator for both the inner and outer iterations. The outer iteration process was preconditioned by BILUM with dual dropping strategy as was discussed earlier. The inner iteration process to solve the last reduced system approximately was preconditioned by ILUT <ref> [17] </ref>. Exceptions are stated explicitly. The construction and application of the BILUM preconditioner was similar to those described in [21], except that here the dual dropping strategy was applied from the first level. The initial guess was random numbers.
Reference: [18] <author> Y. Saad and A. Malevsky. PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers. </title> <editor> In V. E. Malyshkin et al., editor, </editor> <booktitle> Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, Russia, </address> <month> Sept. </month> <year> 1995, 1995. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables. <p> In practice, we may use a double dropping strategy similar to the one just suggested, for the EB 1 and A 1 matrices, possibly with different parameters. 4 Numerical Experiments The Schur-ILU factorization has been implemented in the framework of the PSPARSLIB package <ref> [18, 20, 15] </ref> and was tested on a variety of machines. The experiments reported here have been performed on a CRAY T3E-900. ILUM and Block ILUM have been implemented and tested on sequential machines. Additional experiments with BILUM, specifically with small blocks, have been reported elsewhere, see [21].
Reference: [19] <author> Y. Saad and M. Sosonkina. </author> <title> Distributed Schur complement techniques for general sparse linear systems. </title> <type> Technical Report UMSI 97/159, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: Two techniques in this class are the Schur complement technique (Schur-ILU) developed in <ref> [19] </ref> and the point and block multi-elimination ILU preconditioners (ILUM, BILUM) discussed in [16, 21]. The framework of the Schur-ILU preconditioner is that of a distributed sparse linear system, in which equations are assigned to different processors according to a mapping determined by a graph partitioner. <p> For example, in (1) and (2) the linear system with B can be done either iteratively or directly, or approximately using just an ILU factorization for B. The choices for (2) are also numerous. One option considered in <ref> [19] </ref> starts by replacing (5) by an approximate system of the form, y i + ~ S 1 X E ij y j = ~ S 1 h i f i ; (12) in which ~ S i is some (local) approximation to the local Schur complement matrix S i . <p> The above system is then solved by an iterative accelerator such as GMRES requiring a solve with ~ S i at each step. In one method considered in <ref> [19] </ref> the approximation ~ S i was extracted from an Incomplete LU factorization of A i . The idea is based on the following observation (see [17]). <p> , where L i = L B i 0 B i L S i and U i = U B i L 1 0 U S i : (13) Then, L S i U S i is equal to the Schur complement S i associated with the partitioning (4), see <ref> [17, 19] </ref>. <p> In particular, the number of iterations required to converge increases substantially from 4 processors to 100 processors, as shown in Figure 5 (top-left plot). This contrasts with the earlier example and other tests seen in <ref> [19] </ref>. Recall that the problem is indefinite. The increase in the number of iterations adds to the deterioration of the achievable speed-up for larger numbers of processors.
Reference: [20] <author> Y. Saad and K. Wu. </author> <title> Design of an iterative solution module for a parallel sparse matrix library (P SPARSLIB). </title> <editor> In W. Schonauer, editor, </editor> <booktitle> Proceedings of IMACS conference, </booktitle> <address> Georgia, </address> <year> 1994, 1995. </year>
Reference-contexts: In practice, we may use a double dropping strategy similar to the one just suggested, for the EB 1 and A 1 matrices, possibly with different parameters. 4 Numerical Experiments The Schur-ILU factorization has been implemented in the framework of the PSPARSLIB package <ref> [18, 20, 15] </ref> and was tested on a variety of machines. The experiments reported here have been performed on a CRAY T3E-900. ILUM and Block ILUM have been implemented and tested on sequential machines. Additional experiments with BILUM, specifically with small blocks, have been reported elsewhere, see [21].
Reference: [21] <author> Y. Saad and J. Zhang. BILUM: </author> <title> Block versions of multi-elimination and multi-level ILU preconditioner for general sparse linear systems. </title> <type> Technical Report UMSI 97/126, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: Two techniques in this class are the Schur complement technique (Schur-ILU) developed in [19] and the point and block multi-elimination ILU preconditioners (ILUM, BILUM) discussed in <ref> [16, 21] </ref>. The framework of the Schur-ILU preconditioner is that of a distributed sparse linear system, in which equations are assigned to different processors according to a mapping determined by a graph partitioner. <p> The ILUM preconditioner has been extended to a block version (BILUM) in which the B block in (1) is block-diagonal. This method utilizes independent sets of small clusters (or blocks), instead of single nodes <ref> [4, 21] </ref>. In some difficult cases, the performance of this block version is substantially superior to that of the scalar version. The major difference between our approach and the approaches of Botta and Wubs [4] and Reusken [13] is in the choice of variables for the reduced system. <p> This approach is more akin to a domain decomposition technique and it is more generally applicable since it does not require diagonal dominance in either the B or C submatrix. One aim of the current paper is to further extend BILUM techniques of <ref> [21] </ref> to include blocks of large size and to treat related issues of keeping sparsity of the BILUM factors. Measurable parameters are introduced to characterize the efficiency of a preconditioner. Numerical results with some hard-to-solve problems are presented to demonstrate the merits of the new implementations. <p> This process can be continued for a few more levels and the last level system can be solved with a standard iterative method. This idea was exploited in [16] for blocks B i of the smallest possible size, namely one. In <ref> [21] </ref>, the idea was generalized to blocks larger than one and a number of heuristics were suggested for selecting these blocks. We recall that an independent set is a set of unknowns which are not coupled by an equation. <p> Unknowns within the same group may be coupled. ILUM and BILUM can be viewed as a recursive applications of domain decomposition in which the subdomains are all of small size. In ILUM the subdomains are all of size one and taken together they constitute an independent set. In BILUM <ref> [21] </ref>, this idea was slightly generalized by using block-independent sets, with groups (blocks) of size two or more instead of just one. As the blocks (subdomains) become larger, Schur complements become denser. <p> As the blocks (subdomains) become larger, Schur complements become denser. However, the resulting Schur complement systems are also smaller and they tend to be better conditioned as well. 3 Block Independent Sets with Large Blocks Heuristics based on local optimization arguments were introduced in <ref> [21] </ref> to find Block Independent Sets (BIS) having various properties. It has been shown numerically that selecting new subsets according to the lowest possible number of outgoing edges in the subgraph, usually yields better performance and frequently the smallest reduced system. These algorithms were devised for small independent sets. <p> The second undesirable consequence is the rapid increase in the amount of fill-ins in the LU factors and in the inverse of the block diagonal submatrix. As a result, the construction and application of a BILUM preconditioner associated with a BIS having large subsets tend to be expensive <ref> [21] </ref>. <p> The matrix C is square and of dimension l = n m. In <ref> [21] </ref>, a block ILU factorization of the form (11) is performed, i.e., E C I 0 ! 0 A 1 = L fi U: (15) Here A 1 = CEB 1 F is the Schur complement and I is the identity matrix. <p> Our iterative algorithm consists of GMRES with a small restart value as an accelerator and BILUM as a preconditioner. The last reduced system is solved iteratively by another GMRES preconditioned by an ILUT preconditioner [17]. The dropping strategy that we used in <ref> [21] </ref> is the simplest one. Elements in EB 1 in the L factor and in the reduced system A 1 are dropped whenever their absolute values are less than a threshold tolerance droptol times the average value of the current row. <p> Figure 2 shows the behavior of the four characteristic measures as the block size changes when an algorithm using this single dropping strategy is used to solve a system with the 5-POINT matrix described in <ref> [21] </ref> (some information about this matrix is given in Section 4). Here, a 20-level BILUM with single dropping strategy was used. The coarsest level solve was preconditioned by ILUT (10 4 ; 10). The iteration counts are 5 for block sizes 4, and 4 otherwise. <p> The experiments reported here have been performed on a CRAY T3E-900. ILUM and Block ILUM have been implemented and tested on sequential machines. Additional experiments with BILUM, specifically with small blocks, have been reported elsewhere, see <ref> [21] </ref>. We begin with experiments illustrating the Schur-ILU preconditioners. Some information on the test matrices is given in Table 1. The Raefsky matrix was supplied to us by H. Simon from Lawrence Berkeley National Laboratory. The Venkat matrix was supplied by V. <p> The Speed-up and Efficiency plots in Figure 5 show the `adjusted' measures which factor out iteration increases. 4.2 Experiments with BILUM Standard implementations of ILUM and BILUM have been described in detail in <ref> [16, 21] </ref>. We used GMRES (10) as an accelerator for both the inner and outer iterations. The outer iteration process was preconditioned by BILUM with dual dropping strategy as was discussed earlier. The inner iteration process to solve the last reduced system approximately was preconditioned by ILUT [17]. <p> The inner iteration process to solve the last reduced system approximately was preconditioned by ILUT [17]. Exceptions are stated explicitly. The construction and application of the BILUM preconditioner was similar to those described in <ref> [21] </ref>, except that here the dual dropping strategy was applied from the first level. The initial guess was random numbers. The numerical experiments were conducted on a Power-Challenge XL Silicon Graphics workstation equipped with 512 MB of main memory, two 190 MHZ R10000 processors, and 1 MB secondary cache.
Reference: [22] <author> B. Smith, P. Bjtrstad, and W. Gropp. </author> <title> Domain decomposition: Parallel multilevel methods for elliptic partial differential equations. </title> <publisher> Cambridge University Press, </publisher> <address> New-York, NY, </address> <year> 1996. </year>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables. <p> X j2N i The result of this multiplication affects only the local interface unknowns, which is indicated by a zero in the top part of the second term of the left-hand side of (4). 2.2 Schur complement systems This section gives a brief background on Schur complement systems; see e.g., <ref> [22, 17] </ref>, for additional details and references. The Schur complement system is obtained by eliminating the variable u i from the system (4).
Reference: [23] <author> B. Smith, W. D. Gropp, and L. C. McInnes. </author> <title> PETSc 2.0 user's manual. </title> <type> Technical Report ANL-95/11, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <month> July </month> <year> 1995. </year> <month> 18 </month>
Reference-contexts: This setting which is illustrated in Figure 1, is common to most packages for parallel iterative solution methods <ref> [15, 18, 10, 23, 9, 7, 22, 11] </ref>. 2.1 Distributed sparse linear systems The matrix assigned to a certain processor is split into two parts: the local matrix A i , which acts on the local variables and an interface matrix X i , which acts on the external variables.
References-found: 23

