URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn33.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Implementation of Strassen's Algorithm for Matrix Multiplication 1  
Author: Steven Huss-Lederman Elaine M. Jacobson Jeremy R. Johnson Anna Tsao Thomas Turnbull 
Address: 1210 W. Dayton St., Madi son, WI 53706,  17100 Science Dr., Bowie, MD 20715,  Philadelphia, PA 19104,  17100 Science Dr., Bowie, MD 20715,  17100 Science Dr., Bowie, MD 20715,  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison,  Center for Computing Sciences,  Department of Mathematics and Computer Science, Drexel University,  Center for Computing Sciences,  Center for Computing Sciences,  
Note: 1 This work was partially supported by the Applied and Computational Mathematics Program, Defense Advanced Research Projects Agency, under Contract  
Email: email: lederman@cs.wisc.edu  email: emj@super.org  email: jjohnson@king.mcs.drexel.edu  email: anna@super.org  email: turnbull@super.org  
Phone: 2  Phone: (608) 262-0664, FAX: (608) 262-9777,  3  Phone: (301) 805-7435, FAX: (301) 805-7602,  4  Phone: (215) 895-2893, FAX: (610) 647-8633,  5  Phone: (301) 805-7432, FAX: (301) 805-7602,  6  Phone: (301) 805-7358, FAX: (301) 805-7602,  
Date: August 1, 1996  
Web: P-95006.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference: [2] <author> D. H. Bailey. </author> <title> Extra high speed matrix multiplication on the Cray-2. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 9 </volume> <pages> 603-607, </pages> <year> 1988. </year>
Reference-contexts: A cutoff criterion is used to determine whether to apply Strassen's algorithm or to use DGEMM. The cutoff criterion uses parameters which can be set based on empirical performance measurements, allowing our code to be tuned to different machines. Previous implementations of Strassen's algorithm <ref> [2, 3, 8] </ref> offer similar functionality and performance characteristics, though our study and analysis is more thorough. Aspects of our work that are unique to our implementation are the following.
Reference: [3] <author> D. H. Bailey, K. Lee, and H. D. Simon. </author> <title> Using Strassen's Algorithm to Accelerate the Solution of Linear Systems. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(5) </volume> <pages> 357-371, </pages> <year> 1990. </year>
Reference-contexts: Therefore, at least theoretically, when considering rect-angular matrices, the cutoff criterion (7) should be used instead of the simpler condition, m 12 or k 12 or n 12, which has been used by others <ref> [3, 8] </ref>. Alternatively, instead of using the operation count model to predict the proper cutoff condition, one can empirically determine the appropriate cutoff in a manner very similar to the theoretical analysis. This will require a more complicated set of experiments for rectangular matrices than for square. <p> Using Strassen's original algorithm, Bailey, et al. <ref> [3] </ref> devised a straightforward scheme that reduces the total memory requirements to (mk + kn + mn)=3: Since Winograd's variant nests the additions/subtractions in stage (4) it is not clear that a similar reduction is possible. Below we discuss two computation schemes, both of which are used in our implementation. <p> A cutoff criterion is used to determine whether to apply Strassen's algorithm or to use DGEMM. The cutoff criterion uses parameters which can be set based on empirical performance measurements, allowing our code to be tuned to different machines. Previous implementations of Strassen's algorithm <ref> [2, 3, 8] </ref> offer similar functionality and performance characteristics, though our study and analysis is more thorough. Aspects of our work that are unique to our implementation are the following.
Reference: [4] <author> R. P. Brent. </author> <title> Algorithms for matrix multiplication. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1970. </year> <type> Technical Report CS 157, </type> <note> (Available from National Technical Information Service, # AD705509). </note>
Reference: [5] <author> J. Cohen and M. Roth. </author> <title> On the implementation of Strassen's fast multiplication algorithm. </title> <journal> Acta Informatica, </journal> <volume> 6 </volume> <pages> 341-355, </pages> <year> 1976. </year>
Reference: [6] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: introduction <ref> [6] </ref>. We remark that the asymptotic complexity does not depend on the number of additions/subtractions; however, reducing the number of additions/subtractions can have practical significance. Winograd's variant of Strassen's algorithm (credited to M. Paterson) uses 7 multiplications and 15 additions/subtractions [10].
Reference: [7] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference: [8] <author> C. Douglas, M. Heroux, G. Slishman, and R. M. Smith. GEMMW: </author> <title> A portable level 3 BLAS Winograd variant of Strassen's matrix-matrix multiply algorithm. </title> <journal> Journal of Computational Physics, </journal> <volume> 110 </volume> <pages> 1-10, </pages> <year> 1994. </year>
Reference-contexts: Therefore, at least theoretically, when considering rect-angular matrices, the cutoff criterion (7) should be used instead of the simpler condition, m 12 or k 12 or n 12, which has been used by others <ref> [3, 8] </ref>. Alternatively, instead of using the operation count model to predict the proper cutoff condition, one can empirically determine the appropriate cutoff in a manner very similar to the theoretical analysis. This will require a more complicated set of experiments for rectangular matrices than for square. <p> This approach to padding is called dynamic padding since padding occurs throughout the execution of Strassen's algorithm. A version of dynamic padding is used in <ref> [8] </ref>. Another approach, called dynamic peeling, deals with odd dimensions by stripping off the extra row and/or column as needed, and adding their contributions to the final result in a later round of fixup work. <p> To our knowledge, the dynamic peeling method had not been previously tested through actual implementation, and in fact its usefulness had been questioned <ref> [8] </ref>. However, our operation count analysis in [14] showed it to be superior to dynamic padding. This indicated that it could be competitive in practice with other approaches, and that further study was needed. Thus, we chose to use this technique in our implementation of Strassen's algorithm. <p> The first will demonstrate that an even lower memory requirement is attainable for the case where the input parameter fi = 0. This scheme is similar to the one used in the 8 implementation of Winograd's variant, DGEMMW, reported in Douglas, et al., <ref> [8] </ref>, where additional storage requirements when fi = 0 are slightly more than (m max (k; n) + kn)=3. Our technique requires (m max (k; n) + kn)=3 in this case. <p> Our first scheme for performing Strassen's algorithm, STRASSEN1, is a straightforward schedule for performing the computations as described in Section 2, and is similar to that used in <ref> [8] </ref>. See [14] for a detailed description. <p> This approach has been dismissed by others <ref> [8] </ref> because of the inefficiency of some of the required fixup operations. Our implementation partially deals with this concern by combining the required operations in (9) and computing them with BLAS routines. <p> There are two natural ways to create a rectangular criterion from the condition for square matrices: m o or k o or n o; (11) mkn o (nk + mn + mk)=3: (12) Condition (11), used in <ref> [8] </ref>, prevents Strassen's algorithm from being applied in certain situations where it would be beneficial. One situation where this can occur, as was illustrated in Section 2, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large. <p> We have observed that one can optimize the primitives and methods to typically get a several percent gain. To keep our code general, we have not included these machine-specific techniques in our code. Next, we compare to a public domain implementation from Douglas, et al. <ref> [8] </ref>, DGEMMW. We see in Figure 5 that, for general ff and fi on square matrices, there are matrix sizes where each code does better. The average ratio is 0.991, which shows that we are slightly better. <p> A cutoff criterion is used to determine whether to apply Strassen's algorithm or to use DGEMM. The cutoff criterion uses parameters which can be set based on empirical performance measurements, allowing our code to be tuned to different machines. Previous implementations of Strassen's algorithm <ref> [2, 3, 8] </ref> offer similar functionality and performance characteristics, though our study and analysis is more thorough. Aspects of our work that are unique to our implementation are the following.
Reference: [9] <author> P. C. Fischer. </author> <title> Further schemes for combining matrix algorithms, </title> <booktitle> in Automata, Languages and Programming, volume 14 of Lecture Notes in Computer Science, </booktitle> <pages> pages 428-436. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year>
Reference: [10] <author> P. C. Fischer and R. L. Probert. </author> <title> Efficient procedures for using matrix algorithms, </title> <booktitle> in Automata, Languages and Programming, volume 14 of Lecture Notes in Computer Science, </booktitle> <pages> pages 413-427. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year>
Reference-contexts: introduction [6]. We remark that the asymptotic complexity does not depend on the number of additions/subtractions; however, reducing the number of additions/subtractions can have practical significance. Winograd's variant of Strassen's algorithm (credited to M. Paterson) uses 7 multiplications and 15 additions/subtractions <ref> [10] </ref>.
Reference: [11] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the level 3 BLAS. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 352-368, </pages> <year> 1990. </year>
Reference-contexts: One situation where this can occur, as was illustrated in Section 2, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large. Condition (12), proposed by Higham in <ref> [11] </ref>, scales the theoretical condition (7) by (4=3)o , so that it reduces to the square condition (10) when m = k = n.
Reference: [12] <author> N. J. Higham. </author> <title> Stability of block algorithms with fast level 3 BLAS. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 </volume> <pages> 274-291, </pages> <year> 1992. </year> <month> 24 </month>
Reference: [13] <author> J. E. Hopcroft and L. R. Kerr. </author> <title> Some techniques for proving certain simple programs op-timal. </title> <booktitle> In Proceedings, Tenth Annual Symposium on Switching and Automata Theory, </booktitle> <pages> pages 36-45, </pages> <year> 1969. </year>
Reference-contexts: Further reduction in the number of multiplications and additions/subtractions of any Strassen-like algorithm based on 2 fi 2 matrices is not possible <ref> [13, 18] </ref>. In the remainder of this paper, unless otherwise specified, we will mean the Winograd variant described above when referring to Strassen's algorithm.
Reference: [14] <author> S. Huss-Lederman, E. M. Jacobson, J. R. Johnson, A. Tsao, and T. Turnbull. </author> <title> Strassen's algorithm for matrix multiplication: Modeling, analysis, and implementation. </title> <type> Technical report, </type> <institution> Center for Computing Sciences, </institution> <year> 1996. </year> <note> Technical Report CCS-TR-96-147. </note>
Reference-contexts: Returning to establishing the cutoff criteria, the situation is more complicated for rectangular matrices. For more details see <ref> [14] </ref>; here we illustrate with an example. If m = 6; k = 14; n = 86, (7) is not satisfied; thus recursion should be used when multiplying 6 fi 14 and 14 fi 86 matrices. <p> To our knowledge, the dynamic peeling method had not been previously tested through actual implementation, and in fact its usefulness had been questioned [8]. However, our operation count analysis in <ref> [14] </ref> showed it to be superior to dynamic padding. This indicated that it could be competitive in practice with other approaches, and that further study was needed. Thus, we chose to use this technique in our implementation of Strassen's algorithm. <p> Our first scheme for performing Strassen's algorithm, STRASSEN1, is a straightforward schedule for performing the computations as described in Section 2, and is similar to that used in [8]. See <ref> [14] </ref> for a detailed description. <p> With each computation step indicated, we show the temporary variable stored to, as well as the relationship to the variables used in the description of the algorithm in Section 2. Although STRASSEN2 performs some additional arithmetic, multiplication by ff and fi, and some accumulation operations, our analysis <ref> [14] </ref> and empirical results (Section 4) suggest that no time penalty is paid for these additional operations. <p> We can thus complete the computations using only three temporaries, R 1 , R 2 , and R 3 , the minimum number possible <ref> [14] </ref>. Notice that R 1 only holds subblocks of A, R 2 only holds subblocks of B, and R 3 only holds subblocks of C, so that their sizes are mk=4, nk=4, and mn=4, respectively. The total memory requirement for STRASSEN2 is thus bounded by (mn + km + kn)=3. <p> Further justification for our decision to use dynamic padding comes, as mentioned in Section 2, from our theoretical analysis in <ref> [14] </ref>, as well as what we believe is a simplified code structure, where no special cases are embedded in the routine that applies Strassen's algorithm, and no additional memory is needed when odd dimensions are encountered. 11 3.4 Setting Cutoff Criteria In Section 2 we determined the optimal cutoff criterion using <p> In practice operation count is not an accurate enough predictor of performance to be used to tune actual code. The interested reader is referred to <ref> [14] </ref> for a discussion of the limitations of operation count and development of other performance models that can more accurately predict performance parameters. In addition, performance of our code varies from machine to machine, so an effective cutoff criterion must be adaptable. <p> Overall, these results show that our DGEFMM code performs very well compared to other implementations. This is especially significant considering we use less memory in many cases. This also shows the dynamic peeling technique using rank-one updates is indeed a viable alternative. The reader is referred to <ref> [14] </ref>, where our enhanced models are given that quantitatively describe the behavior seen here. <p> A description of the models and their use in predicting performance and comparing design alternatives can be found in <ref> [14] </ref>.
Reference: [15] <author> S. Huss-Lederman, A. Tsao, and T. Turnbull. </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues. </title> <journal> SIAM J. Sci. Computing, </journal> <volume> 18(2), </volume> <year> 1997. </year> <note> (to appear). </note>
Reference-contexts: This eigensolver is based on the Invariant Subspace Decomposition Algorithm (ISDA) <ref> [15] </ref> and is part of the PRISM project. The ISDA uses matrix multiplication to apply a polynomial function to a matrix until a certain convergence criterion is met.
Reference: [16] <institution> IBM Engineering and Scientific Subroutine Library Guide and Reference, </institution> <year> 1992. </year> <title> Order No. </title> <publisher> SC23-0526. </publisher>
Reference: [17] <author> P. A. Knight. </author> <title> Fast rectangular matrix multiplication and QR decomposition. </title> <journal> Linear Algebra Appl., </journal> <volume> 221 </volume> <pages> 69-81, </pages> <year> 1995. </year>
Reference: [18] <author> R. L. Probert. </author> <title> On the additive complexity of matrix multiplication. </title> <journal> SIAM Journal of Computing, </journal> <volume> 5(6) </volume> <pages> 187-203, </pages> <year> 1976. </year>
Reference-contexts: Further reduction in the number of multiplications and additions/subtractions of any Strassen-like algorithm based on 2 fi 2 matrices is not possible <ref> [13, 18] </ref>. In the remainder of this paper, unless otherwise specified, we will mean the Winograd variant described above when referring to Strassen's algorithm.
Reference: [19] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math., </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year> <month> 25 </month>
References-found: 19

