URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR490.ps.Z
Refering-URL: http://www.cs.indiana.edu/ftp/techreports/index.html
Root-URL: http://www.cs.indiana.edu
Title: Dynamo: A Staged Compiler Architecture for Dynamic Program Optimization obstacles have prevented the widespread use
Author: Mark Leone and R. Kent Dybvig 
Note: September 1997 Project Summary Several  has been made in  
Abstract: Indiana University Computer Science Department Technical Report #490 Optimizing code at run time is appealing because run-time optimizations can make use of values and invariants that cannot be exploited statically. Dynamic optimization can yield code that is superior to statically optimal code. Recent research has shown that dynamic compilation can dramatically improve the performance of a wide range of applications including network packet demultiplexing, sparse matrix computations, pattern matching, and many forms of mobile code (such as "applets"). We intend to design and implement a compiler architecture, called Dynamo, that provides effective dynamic optimization with little programmer effort and low run-time overhead. An important characteristic of Dynamo will be its staged compilation model. To reduce the overhead of dynamic compilation, it is necessary to perform some analysis, translation, and optimization prior to execution. To achieve "lightweight" dynamic optimization, most compilation steps are performed statically, resulting in a low-level intermediate representation that admits a few simple dynamic optimizations. "Heavyweight" dynamic optimization employs a high-level intermediate representation and a wide range of optimizations. A staged 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Ashley. </author> <title> A Practical and Flexible Flow Analysis for Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Indiana University, Bloomington, </institution> <year> 1996. </year>
Reference-contexts: Relatively minor improvements in such regions can yield a large overall performance gain, and it is easier to amortize the cost of dynamically optimizing frequently executed code. The compiler will also employ static flow analysis <ref> [1] </ref> to gain a high-level understanding of a program's control flow. This allows the compiler to estimate execution frequency statically, and it also reveals opportunities for optimization.
Reference: [2] <author> J. Auslander, M. Philipose, C. Chambers, S. J. Eggers, and B. N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In PLDI'96 Conference on Programming Language Design and Implementation, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: More recent systems either rely on profiling information to guide dynamic recompilation [3, 10] or employ a combination of programmer hints and simple program analysis to determine which portions of a program to dynamically optimize <ref> [22, 2, 9] </ref>. The question of how to dynamically optimize code is also critical, since the overhead of ineffective optimizations harms performance. Ideally, static prediction or profiling information could be used to selectively apply only those dynamic optimizations that are likely to be beneficial, but this is not presently done.
Reference: [3] <author> R. G. Burger. </author> <title> Efficient Compilation and Profile-Driven Dynamic Recompilation in Scheme. </title> <type> PhD thesis, </type> <institution> Indiana University, Bloomington, </institution> <month> Feb </month> <year> 1997. </year>
Reference-contexts: Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. More recent systems either rely on profiling information to guide dynamic recompilation <ref> [3, 10] </ref> or employ a combination of programmer hints and simple program analysis to determine which portions of a program to dynamically optimize [22, 2, 9]. The question of how to dynamically optimize code is also critical, since the overhead of ineffective optimizations harms performance. <p> In the SPIN operating system, dynamic optimization is employed to create a specialized event dispatcher whenever a new event handler is registered [6]. Our prior research has included an investigation of the use of profiling information to improve program control flow through dynamic recompilation <ref> [4, 3] </ref>.
Reference: [4] <author> R. G. Burger and R. K. Dybvig. </author> <title> An infrastructure for profile-driven dynamic recompilation. </title> <note> In preparation. </note>
Reference-contexts: In the SPIN operating system, dynamic optimization is employed to create a specialized event dispatcher whenever a new event handler is registered [6]. Our prior research has included an investigation of the use of profiling information to improve program control flow through dynamic recompilation <ref> [4, 3] </ref>.
Reference: [5] <author> R. G. Burger, O. Waddell, and R. K. Dybvig. </author> <title> Register allocation using lazy saves, eager restores, and greedy shu*ing. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 130-138, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We plan to incorporate the results of our prior research in this area, including a fast register allocation algorithm <ref> [5] </ref> and efficient code generation techniques [12].
Reference: [6] <author> C. Chambers, S. J. Eggers, J. Auslander, M. Philipose, M. Mock, and P. Pardyak. </author> <title> Automatic dynamic compilation support for event dispatching in extensible systems. </title> <booktitle> In WCSSS'96 Workshop on Compiler Support for System Software, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Operat--ing systems and other interactive systems often employ dynamic dispatching to implement event handling. In the SPIN operating system, dynamic optimization is employed to create a specialized event dispatcher whenever a new event handler is registered <ref> [6] </ref>. Our prior research has included an investigation of the use of profiling information to improve program control flow through dynamic recompilation [4, 3]. <p> The emphasis in this stage is rapid generation of optimized native code. When no other dynamic optimizations are performed, Delta will yield code comparable to that produced by "template-based" run-time code generation systems <ref> [29, 6] </ref>, with comparable cost (20 to 100 cycles per dynamically generated instruction).
Reference: [7] <author> C. Chambers and D. Ungar. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <booktitle> In PLDI'89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-160, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time [31, 28, 20, 21, 25, 15, 14]. Some early dynamic optimization systems <ref> [11, 7] </ref> were not selective at all; they simply postponed all optimization until run time. <p> Compilers for such languages have employed dynamic optimization to eliminate this overhead by creating 6 customized code to directly invoke methods whose receiver class is known <ref> [11, 7] </ref>. Operat--ing systems and other interactive systems often employ dynamic dispatching to implement event handling. In the SPIN operating system, dynamic optimization is employed to create a specialized event dispatcher whenever a new event handler is registered [6].
Reference: [8] <author> B. Cmelik and D. Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on the Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: These fixed inputs are often tested in the innermost loop of the simulation, resulting in significant interpretive overhead. Dynamic optimization can eliminate this overhead by specializing a simulator to its inputs <ref> [8] </ref>. Interpreters are commonly used in large-scale systems to permit extensibility without compromising safety or reliability. For example, many Unix operating system kernels contain an extensible packet filter procedure that demultiplexes network packets and delivers them to user-level processes.
Reference: [9] <author> C. Consel and F. Noel. </author> <title> A general approach to run-time specialization and its application to C. </title> <booktitle> In POPL'96 Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145-156, </pages> <month> January </month> <year> 1996. </year> <month> 15 </month>
Reference-contexts: More recent systems either rely on profiling information to guide dynamic recompilation [3, 10] or employ a combination of programmer hints and simple program analysis to determine which portions of a program to dynamically optimize <ref> [22, 2, 9] </ref>. The question of how to dynamically optimize code is also critical, since the overhead of ineffective optimizations harms performance. Ideally, static prediction or profiling information could be used to selectively apply only those dynamic optimizations that are likely to be beneficial, but this is not presently done.
Reference: [10] <author> J. Dean and C. Chambers. </author> <title> Towards better inlining decisions using inlining trials. </title> <booktitle> In LFP'94 Conference on LISP and Functional Programming, </booktitle> <pages> pages 273-282, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. More recent systems either rely on profiling information to guide dynamic recompilation <ref> [3, 10] </ref> or employ a combination of programmer hints and simple program analysis to determine which portions of a program to dynamically optimize [22, 2, 9]. The question of how to dynamically optimize code is also critical, since the overhead of ineffective optimizations harms performance.
Reference: [11] <author> L. Deutsch and A. M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In POPL'84 Symposium on Principles of Programming Languages, </booktitle> <address> Salt Lake City, </address> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time [31, 28, 20, 21, 25, 15, 14]. Some early dynamic optimization systems <ref> [11, 7] </ref> were not selective at all; they simply postponed all optimization until run time. <p> Compilers for such languages have employed dynamic optimization to eliminate this overhead by creating 6 customized code to directly invoke methods whose receiver class is known <ref> [11, 7] </ref>. Operat--ing systems and other interactive systems often employ dynamic dispatching to implement event handling. In the SPIN operating system, dynamic optimization is employed to create a specialized event dispatcher whenever a new event handler is registered [6].
Reference: [12] <author> R. K. Dybvig, R. Hieb, and T. Butler. </author> <title> Destination-driven code generation. </title> <type> Technical Report 302, </type> <institution> Indiana University Computer Science Department, </institution> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: We plan to incorporate the results of our prior research in this area, including a fast register allocation algorithm [5] and efficient code generation techniques <ref> [12] </ref>. We also intend to adopt technology recently developed by researchers concentrating on "just-in-time" compilation of mobile code. 2 The final compilation stage, labeled Delta in Figure 2, takes a low-level, machine-dependent intermediate representation as input and performs "lightweight" optimizations such as code layout, constant propagation, and simple peephole optimizations.
Reference: [13] <author> M. Emami, R. Ghiya, and L. J. Hendren. </author> <title> Context-sensitive interprocedural points-to analysis in the presence of function pointers. </title> <booktitle> In PLDI'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 242-256. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1994. </year>
Reference-contexts: For example, if the compiler detects that an inner loop uses a variable whose value is fixed in an outer loop, dynamic optimization may yield improvements that are impossible to achieve with traditional loop-invariant removal. Other recent research on static program analyses, such as alias analysis <ref> [13] </ref> and shape analysis [17], has direct relevance to the problem of candidate selection. It is relatively straightforward to implement cost-benefit analysis using profiling techniques.
Reference: [14] <author> D. R. Engler, W. C. Hsieh, and M. F. Kaashoek. </author> <title> `C: A language for high-level, efficient, and machine-independent dynamic code generation. </title> <booktitle> In POPL'96 Symposium on Principles of Programming Languages, </booktitle> <pages> pages 131-144, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time.
Reference: [15] <author> D. R. Engler and T. A. Proebsting. </author> <title> DCG: An efficient, retargetable dynamic code generation system. </title> <booktitle> In Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 263-272. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time.
Reference: [16] <author> D. R. Engler, D. Wallach, and M. F. Kaashoek. </author> <title> Efficient, safe, application-specific message processing. </title> <type> Technical Memorandum MIT/LCS/TM533, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: But this approach has substantial overhead: the selection predicate is reinterpreted every time a packet is received. Our previous research [22] and related research <ref> [16] </ref> has demonstrated that dynamic optimization eliminates this overhead: a packet selection predicate can be rapidly compiled into trusted native code.
Reference: [17] <author> R. Ghiya and L. J. Hendren. </author> <title> Is it a tree, a DAG, or a cyclic graph? a shape analysis for heap-directed pointers in C. </title> <booktitle> In POPL'96 Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-15, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Other recent research on static program analyses, such as alias analysis [13] and shape analysis <ref> [17] </ref>, has direct relevance to the problem of candidate selection. It is relatively straightforward to implement cost-benefit analysis using profiling techniques.
Reference: [18] <author> J. Gosling and H. McGilton. </author> <title> The Java language environment: A white paper. </title> <type> Technical report, </type> <institution> Sun Microsystems, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Preprocessing is also used to achieve fast ("just-in-time") compilation 2 of mobile code written in Java: a Java compiler on the server side preprocesses Java source into Java bytecode, and a client's browser either interprets or compiles the Java bytecode on the fly <ref> [18] </ref>. A tradeoff exists, however, between the level of preprocessing and the eventual quality of the dynamically generated code: static compilation restricts the kinds of optimizations that can be performed dynamically. This occurs because compilation discards information that is useful for both high-level and low-level optimizations. <p> More expensive low-level optimizations, such as fine-grained instruction scheduling, can also be employed when the compiler (or the programmer) determines that their cost is repaid by improved performance. 2 Stage Gamma will be augmented with an interface that permits Java Virtual Machine code <ref> [18] </ref> to be optimized and compiled on the fly, and we anticipate implementing several network applications (including a simple Web browser) as benchmarks. 10 Optimization Model Static Stages Dynamic Stages Heavyweight: High cost, aggressive optimization Alpha Beta, Gamma, Delta Mid-weight: Moderate cost, significant optimization Alpha, Beta Gamma, Delta Lightweight: Low cost,
Reference: [19] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: Instead, each dynamic optimization will require a static analogue: given an optimization candidate and information about which values will be fixed, the analogue will estimate what kinds of code improvements can be made. In most cases this can be done with a simple dependency or binding-time analysis <ref> [19] </ref>. Achieving precise results from such an analysis is difficult, because some optimizations (such as constant propagation) enable additional optimizations (such as constant folding).
Reference: [20] <author> D. Keppel, S. J. Eggers, and R. R. Henry. </author> <title> A case for runtime code generation. </title> <type> Technical Report 91-11-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: For example, code can be statically compiled to a low-level intermediate representation (such as a machine-code template <ref> [20] </ref>) that can be quickly optimized at run time. <p> Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time.
Reference: [21] <author> D. Keppel, S. J. Eggers, and R. R. Henry. </author> <title> Evaluating runtime-compiled value-specific optimizations. </title> <type> Technical Report 93-11-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. <p> Dynamic optimization allows this common case to be exploited, while preserving correct behavior in the more general case. 2.3 Value-Specific Optimizations Postponing optimization until run-time makes a wide range of value-specific optimizations possible <ref> [21] </ref>. For example, our previous research demonstrated how a general-purpose matrix multiplication routine can be specialized to operate efficiently on sparse matrices using value-specific optimizations [22]. <p> Abstract syntax trees are used to represent expressions, but most high-level language constructs, such as object/record creation and array indexing, have been compiled away. A number of value-specific optimizations <ref> [21] </ref> are performed in stage Gamma, ranging from simple constant propagation to aggressive procedure cloning. When this stage is postponed until run time, these optimizations yield code that is specialized to specific run-time values and data structures; for example, a regular-expression matcher can be customized to a particular search pattern.
Reference: [22] <author> P. Lee and M. Leone. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In ACM Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137-148, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: More recent systems either rely on profiling information to guide dynamic recompilation [3, 10] or employ a combination of programmer hints and simple program analysis to determine which portions of a program to dynamically optimize <ref> [22, 2, 9] </ref>. The question of how to dynamically optimize code is also critical, since the overhead of ineffective optimizations harms performance. Ideally, static prediction or profiling information could be used to selectively apply only those dynamic optimizations that are likely to be beneficial, but this is not presently done. <p> A selection predicate is expressed in the abstract syntax of a "safe" or easily verified programming language, so that it can be trusted by the kernel. But this approach has substantial overhead: the selection predicate is reinterpreted every time a packet is received. Our previous research <ref> [22] </ref> and related research [16] has demonstrated that dynamic optimization eliminates this overhead: a packet selection predicate can be rapidly compiled into trusted native code. <p> For example, our previous research demonstrated how a general-purpose matrix multiplication routine can be specialized to operate efficiently on sparse matrices using value-specific optimizations <ref> [22] </ref>. Matrix multiplication is usually implemented as a triply nested loop, where the outer two loops select vectors from the matrices, e.g., a row and a column, and the innermost loop computes their inner product. <p> For example, the Synthesis kernel employed dynamic optimization to create "self-traversing" buffers and queues [25]. Our prior research has demonstrated that executable data structures can be automatically derived from ordinary code for common list operations, such as membership and association <ref> [22] </ref>. 2.4 Control Flow Optimizations Another common use of dynamic optimization is the elimination of dynamic dispatch. In purely object-oriented languages like Smalltalk and SELF, a method invocation often requires a dynamic type check to determine which method should be used. <p> Our previous research <ref> [24, 22] </ref> has demonstrated that this cost can be reduced by an order of magnitude by creating customized code-generators that are "hard wired" to optimize and generate native code for particular procedures or blocks of code. <p> The Dynamo compiler must avoid dynamically compiling program regions that do not benefit from run-time optimizations, and it should apply only those optimizations that will significantly improve code quality. Overly conservative strategies are not necessarily the answer, however, because they can result in missed optimization opportunities. Our previous research <ref> [22, 23] </ref> has demonstrated that syntactic features of programs (such as loop nests and certain kinds of procedure definitions) provide useful clues that help determine where to perform dynamic optimization. <p> We anticipate that explicit programmer control over dynamic optimization will continue to be useful: our previous research has indicated that domain-specific knowledge is necessary to selectively optimize some applications at run time <ref> [22] </ref>. We plan to adapt existing visualization tools [32] to provide the programmer with feedback on the costs and benefits of individual optimization directives. A dynamic optimization directive is a program annotation that may be attached to many different program constructs: procedure definitions, class definitions, loops, blocks, etc.
Reference: [23] <author> M. Leone. </author> <title> A Principled and Practical Approach to Run-Time Code Generation. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year> <note> In preparation. </note>
Reference-contexts: The Dynamo compiler must avoid dynamically compiling program regions that do not benefit from run-time optimizations, and it should apply only those optimizations that will significantly improve code quality. Overly conservative strategies are not necessarily the answer, however, because they can result in missed optimization opportunities. Our previous research <ref> [22, 23] </ref> has demonstrated that syntactic features of programs (such as loop nests and certain kinds of procedure definitions) provide useful clues that help determine where to perform dynamic optimization.
Reference: [24] <author> M. Leone and P. Lee. </author> <title> Lightweight run-time code generation. In PEPM 94 Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </title> <type> pages 97-106. Technical Report 94/9, </type> <institution> Department of Computer Science, University of Melbourne, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Our previous research <ref> [24, 22] </ref> has demonstrated that this cost can be reduced by an order of magnitude by creating customized code-generators that are "hard wired" to optimize and generate native code for particular procedures or blocks of code.
Reference: [25] <author> H. Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1992. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. <p> Pike and colleagues used dynamic optimization to generate common-case code on demand and achieved a significant speedup over statically optimized code. Dynamic optimization was used to perform common-case optimization of interrupt handling in the Synthesis kernel <ref> [25] </ref>. Rather than performing a full context switch on each interrupt, the Synthesis kernel dynamically synthesizes customized code to save and restore only those portions of the machine state that are used by a particular service routine. <p> The overhead of traversal can be amortized by compiling a data structure into code at run time. For example, the Synthesis kernel employed dynamic optimization to create "self-traversing" buffers and queues <ref> [25] </ref>. Our prior research has demonstrated that executable data structures can be automatically derived from ordinary code for common list operations, such as membership and association [22]. 2.4 Control Flow Optimizations Another common use of dynamic optimization is the elimination of dynamic dispatch.
Reference: [26] <author> S. McCanne and V. Jacobson. </author> <title> The BSD packet filter: A new architecture for user-level packet capture. </title> <booktitle> In Winter 1993 USENIX Conference, </booktitle> <pages> pages 259-269. </pages> <publisher> USENIX Association, </publisher> <month> January </month> <year> 1993. </year>
Reference-contexts: Many useless packets may be delivered as a result, with a consequent degradation of performance. A commonly adopted solution to this problem is to parameterize a packet filter by a selection predicate that is dynamically constructed by a user-level process <ref> [27, 26] </ref>. A selection predicate is expressed in the abstract syntax of a "safe" or easily verified programming language, so that it can be trusted by the kernel. But this approach has substantial overhead: the selection predicate is reinterpreted every time a packet is received.
Reference: [27] <author> J. C. Mogul, R. F. Rashid, and M. J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 39-51. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1987. </year> <note> An updated version is available as DEC WRL Research Report 87/2. </note>
Reference-contexts: Many useless packets may be delivered as a result, with a consequent degradation of performance. A commonly adopted solution to this problem is to parameterize a packet filter by a selection predicate that is dynamically constructed by a user-level process <ref> [27, 26] </ref>. A selection predicate is expressed in the abstract syntax of a "safe" or easily verified programming language, so that it can be trusted by the kernel. But this approach has substantial overhead: the selection predicate is reinterpreted every time a packet is received.
Reference: [28] <author> R. Pike, B. Locanthi, and J. Reiser. </author> <title> Hardware/software trade-offs for bitmap graphics on the Blit. </title> <journal> Software | Practice and Experience, </journal> <volume> 15(2) </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. <p> Dynamic optimization yields the benefits of common-case optimizations without either of these drawbacks because the optimization is performed on demand: the common case (s) can be determined dynamically, and code space can be reused if necessary. 5 Pike and colleagues <ref> [28] </ref> explored the problem of optimizing a bitblt procedure for the Blit, a graphics terminal with no special-purpose graphics hardware.
Reference: [29] <author> M. Poletto, D. R. Engler, and M. F. Kaashoek. tcc: </author> <title> A template-based compiler for `C. </title> <booktitle> In WCSSS'96 Workshop on Compiler Support for System Software, </booktitle> <pages> pages 1-7, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The emphasis in this stage is rapid generation of optimized native code. When no other dynamic optimizations are performed, Delta will yield code comparable to that produced by "template-based" run-time code generation systems <ref> [29, 6] </ref>, with comparable cost (20 to 100 cycles per dynamically generated instruction).
Reference: [30] <author> D. Tarditi, G. Morrisett, P. Cheng, C. Stone, B. Harper, and P. Lee. </author> <title> TIL: A type-directed optimizing compiler for ML. </title> <booktitle> In PLDI'96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 181-192, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Control-flow optimizations, such as loop unrolling and procedure inlining, are performed in stage Beta. Data representations are also optimized during this stage; such optimizations are critical to achieving good performance in advanced languages. We anticipate employing techniques similar to those used in the TIL compiler <ref> [30] </ref> to facilitate such optimizations. All of the analyses and optimizations performed in stage Beta can benefit significantly from dynamic information.
Reference: [31] <author> K. Thompson. </author> <title> Regular expression search algorithm. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 11(6) </volume> <pages> 419-422, </pages> <month> June </month> <year> 1968. </year>
Reference-contexts: Automatically determining where to dynamically optimize code is an open research problem. Until recently, selective dynamic optimizations could be obtained only manually, by writing code that explicitly generates code at run time <ref> [31, 28, 20, 21, 25, 15, 14] </ref>. Some early dynamic optimization systems [11, 7] were not selective at all; they simply postponed all optimization until run time. <p> For example, in 1968 Ken Thompson implemented a search algorithm that employed dynamic optimization, based on the observation that a regular expression is a program that specifies the behavior of a finite 4 state machine <ref> [31] </ref>. Previous search algorithms were based on a regular-expression-matching algorithm that was essentially an interpreter. However, repeatedly matching against a fixed regular expression involves duplicated effort, or interpretive overhead. Thompson found that compiling a regular expression at run-time into a native-code finite-state machine could amortize this overhead.
Reference: [32] <author> O. Waddell. </author> <title> The Scheme Widget Library User's Manual. </title> <institution> Indiana University, Bloom-ington, Indiana, </institution> <year> 1995. </year> <month> 17 </month>
Reference-contexts: We anticipate that explicit programmer control over dynamic optimization will continue to be useful: our previous research has indicated that domain-specific knowledge is necessary to selectively optimize some applications at run time [22]. We plan to adapt existing visualization tools <ref> [32] </ref> to provide the programmer with feedback on the costs and benefits of individual optimization directives. A dynamic optimization directive is a program annotation that may be attached to many different program constructs: procedure definitions, class definitions, loops, blocks, etc. <p> We intend to develop a set of analysis and profiling techniques that will automatically add dynamic optimization directives to ordinary code; these techniques will also be used to refine existing programmer directives. We also plan to adapt existing visualization and performance analysis tools <ref> [32] </ref> to provide the programmer with feedback on the costs and benefits of manually directed dynamic optimizations. Static program analysis and profiling are complementary techniques.
References-found: 32

