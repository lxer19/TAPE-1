URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/relevance.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/pubs.html
Root-URL: http://www.cs.cmu.edu
Email: (Avrim@cs.cmu.edu)  (Langley@isle.org)  
Title: Selection of Relevant Features and Examples in Machine Learning  
Author: Avrim L. Blum Pat Langley 
Note: Also affiliated with the  
Address: Pittsburgh, Pennsylvania 15213-3891  2164 Staunton Court, Palo Alto, California 94306  Center, 1510 Page Mill Road, Palo Alto, CA 94304.  
Affiliation: School of Computer Science, Carnegie Mellon University  Institute for the Study of Learning and Expertise  Intelligent Systems Laboratory, Daimler-Benz Research and Technology  
Abstract: In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1990). </year> <title> A study of instance-based algorithms for supervised learning tasks: Mathematical, empirical, and psychological evaluations . Doctoral dissertation, </title> <institution> Department of Information & Computer Science, University of California, Irvine. </institution>
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1996). </year> <title> A comparative evaluation of sequential feature selection algorithms. </title> <editor> In D. Fisher & J.-H. Lenz (Eds.), </editor> <booktitle> Artificial intelligence and statistics V. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Almuallim, H., & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 547-552). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <booktitle> Information and Computation, </booktitle> <pages> 75 , 87-106. </pages>
Reference: <author> Angluin, D., Hellerstein, L., & Karpinski, M. </author> <year> (1993). </year> <title> Learning read-once formulas with queries. </title> <journal> Journal of the ACM, </journal> <pages> 40 , 185-210. </pages> <note> 20 Armstrong, </note> <author> R., Freitag, D., Joachims, T., & Mitchell, T. </author> <year> (1995). </year> <title> Webwatcher: A learning ap-prentice for the world wide web. </title> <booktitle> AAAI Spring Symposium on Information Gathering from Heterogeneous Distributed Environments. </booktitle>
Reference: <author> Blum, A. </author> <year> (1992). </year> <title> Learning Boolean functions in an infinite attribute space. </title> <booktitle> Machine Learning, </booktitle> <pages> 9 , 373-386. </pages>
Reference: <author> Blum, A. </author> <year> (1995). </year> <title> Empirical support for Winnow and weighted-majority based algorithms: Results on a calendar scheduling domain. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 64-72). </pages> <address> Lake Tahoe, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Blum, A., Furst, M., Jackson, J., Kearns, M., Mansour, Y., & Rudich, S. </author> <year> (1994). </year> <title> Weakly learning DNF and Characterizing Statistical Query learning using Fourier analysis. </title> <booktitle> Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 253-262. </pages>
Reference-contexts: the exact leaning model, and a recent algorithm of Jackson (1994) learns the even larger class of general DNF formulas using membership queries, with respect to the uniform distribution. 18 "unusual" in the sense that the class has been proven impossible to learn in the statistical query model of Kearns <ref> (Blum et al., 1994) </ref>. Thus, issues of finding relevant features seem to be at the core of what makes those classes hard. As a practical matter, it is unclear how to experimentally test a proposed algorithm for this problem, since no distribution on the target functions is given.
Reference: <author> Blum, A., Hellerstein, L., & Littlestone, N. </author> <year> (1995). </year> <title> Learning in the presence of finitely or infinitely many irrelevant attributes. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> 50 , 32-40. </pages>
Reference: <author> Blum, A. & Kannan, R. </author> <year> (1993). </year> <title> Learning an intersection of k halfspaces over a uniform distribution. </title> <booktitle> Proceedings of the 34th Annual IEEE Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 312-320). </pages> <publisher> IEEE. </publisher>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <pages> 24 , 377-380. </pages>
Reference-contexts: For instance, theoretical results show that if, by focusing on only a small subset of features, an algorithm can significantly reduce the number of hypotheses under consideration, then there is a corresponding reduction in the sample size sufficient to guarantee good generalization <ref> (Blumer et al., 1987) </ref>. Somewhat in the middle of the above two extremes are feature-weighting methods that do not explicitly select subsets of features, but still aim to achieve good scaling behavior. We structure the remainder of this section as follows.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <pages> 36 , 929-965. </pages>
Reference-contexts: a given set of data is NP-hard (Garey & Johnson, 1979); a polynomial-time algorithm to find disjunctions only c log n times larger than the smallest for c &lt; 1=4 would place NP into quasi-polynomial time (Lund & Yannakakis, 1993). 7 form include learning intersections of halfspaces in constant-dimensional spaces <ref> (Blumer et al., 1989) </ref>, and algorithms for learning DNF formulas in n O (log n) time under the uniform distribution (Verbeurgt, 1990). <p> all examples on which their hypothesis is correct. 6 If one assumes that training data and test data are both taken from a single fixed distribution, then one can guarantee that with high probability, the data used for training will overall be relevant to the success criteria used for testing <ref> (Blumer et al., 1989) </ref>. As learning progresses, however, the learner's knowledge about certain parts of the input space increases, and examples in the "well-understood" portion of the space become less useful.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Similar operations for adding and removing features form the core of methods for inducing more complex logical concepts, but these methods also involve routines for combining features into richer descriptions. For example, recursive partitioning methods for induction, such as Quinlan's ID3 (1983) and C4.5 (1993), and CART <ref> (Breiman et al. 1984) </ref>, carry out a greedy search through the space of decision trees, at each stage using an evaluation function to select the attribute that has the best ability to discriminate among the classes.
Reference: <author> Bshouty, N. H. </author> <year> (1993). </year> <title> Exact learning via the monotone theory. </title> <booktitle> Proceedings of the IEEE Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 302-311). </pages> <publisher> IEEE: </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. A., & Freitag, D. </author> <year> (1994a). </year> <title> Greedy attribute selection. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 28-36). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. A., & Freitag, D. </author> <year> (1994b). </year> <title> How useful is relevance? Working Notes of the AAAI Fall Symposium on Relevance (pp. </title> <address> 25-29). New Orleans, LA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Catlett, J. </author> <year> (1992). </year> <title> Peepholing: Choosing attributes efficiently for megainduction. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 49-54). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cesa-Bianchi, N., Freund, Y., Helmbold, D. P., Haussler, D., Schapire, R. E., & Warmuth, M. K. </author> <year> (1993). </year> <title> How to use expert advice. </title> <booktitle> Proceedings of the Annual ACM Symposium on the Theory of Computing (pp. </booktitle> <pages> 382-391). </pages>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 , 261-284. </pages>
Reference: <author> Cohn, D. A., Ghahramani, Z., & Jordan, M. I. </author> <year> (1996). </year> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <pages> 4 , 129-145. </pages>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis: A new concept. </title> <booktitle> Signal Processing, </booktitle> <pages> 36 , 287-314. </pages>
Reference-contexts: Blum and Kannan (1993) describe theoretical guarantees for methods of this form, when the target function is an intersection of halfspaces and the examples are chosen from a sufficiently benign distribution. The related method of independent component analysis <ref> (Comon, 1994) </ref> incorporates similar ideas, but insists only that the new features be independent rather than orthogonal. 2.5 Wrapper Approaches to Feature Selection A third generic approach for feature selection also occurs outside the basic induction method but uses that method as a subroutine, rather than as a postprocessor.
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> 13 , 21-27. </pages>
Reference: <author> Daelemans, W., Gillis, S., & Durieux, G. </author> <year> (1994). </year> <title> The acquisition of stress: A data-oriented approach. </title> <note> Computational Linguistics , 20(3) 421-451. </note>
Reference: <author> Devijver, P. A., & Kittler, J. </author> <year> (1982). </year> <title> Pattern recognition: A statistical approach. </title> <address> New York: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: 1) as embedded and filter methods, but it evaluates alternative sets by running some induction algorithm on the 10 training data and using the estimated accuracy of the resulting classifier as its metric. 3 Actually, the wrapper scheme has a long history within the literature on statistics and pattern recognition <ref> (e.g., Devijver & Kittler, 1982) </ref>, where the problem of feature selection has long been an active research topic, but its use within machine learning is relatively recent.
Reference: <author> Dhagat, A., & Hellerstein, L. </author> <year> (1994). </year> <title> PAC learning with irrelevant attributes. </title> <booktitle> Proceedings of the IEEE Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 64-74). </pages> <publisher> IEEE. </publisher>
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature-selection methods and their application to computer security (Technical Report CSE-92-18). </title> <institution> Davis: University of California, Department of Computer Science. </institution>
Reference: <author> Drucker, H., Schapire, R., & Simard, P. </author> <year> (1992). </year> <title> Improving performance in neural networks using a boosting algorithm. </title> <booktitle> In Advances in neural information processing systems (Vol. 4). </booktitle> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Drucker, H., Cortes, C., Jackel, L. D., LeCun, Y., & Vapnik, V. </author> <year> (1994). </year> <title> Boosting and other machine learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> (pp. 53-61). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dyer, M., Frieze, A., & Kannan, R. </author> <year> (1989). </year> <title> A random polynomial time algorithm for approximating the volume of convex bodies. </title> <booktitle> Proceedings of the Annual ACM Symposium on the Theory of Computing (pp. </booktitle> <pages> 375-381). </pages>
Reference: <author> Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 202-216). </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Freund, Y. </author> <year> (1992). </year> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 391-398). </pages> <publisher> ACM Press. </publisher>
Reference: <author> Garey, M., & Johnson, D. </author> <year> (1979). </year> <title> Computers and intractability: A guide to the theory of NP-completeness. </title> <address> San Francisco: </address> <publisher> W. H. Freeman. </publisher>
Reference-contexts: In the other direction, finding the smallest disjunction consistent with a given set of data is NP-hard <ref> (Garey & Johnson, 1979) </ref>; a polynomial-time algorithm to find disjunctions only c log n times larger than the smallest for c &lt; 1=4 would place NP into quasi-polynomial time (Lund & Yannakakis, 1993). 7 form include learning intersections of halfspaces in constant-dimensional spaces (Blumer et al., 1989), and algorithms for learning
Reference: <author> Gil, Y. </author> <year> (1993). </year> <title> Efficient domain-independent experimentation. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 128-134). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gross, K. P. </author> <year> (1991). </year> <title> Concept acquisition through attribute evolution and experiment selection. </title> <type> Doctoral dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Haussler, D. </author> <year> (1986). </year> <title> Quantifying the inductive bias in concept learning. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 485-489). </pages> <address> Philadelphia: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used domains. </title> <booktitle> Machine Learning, </booktitle> <pages> 11 , 63-91. </pages>
Reference: <author> Jackson, J. </author> <year> (1994). </year> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> Proceedings of the IEEE Symposium on Foundations of Computer Science. IEEE. </booktitle>
Reference: <author> John, G. H., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 121-129). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. 22 John, </publisher> <editor> G. H., & Langley, P. </editor> <year> (1996). </year> <title> Static vs. dynamic sampling for data mining. </title> <booktitle> Proceedings of the Second International Conference of Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 367-370). </pages> <address> Portland: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal component analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Indeed, methods such as principal component analysis <ref> (Jolliffe, 1986) </ref> are commonly used as heuristics for finding these low-dimensional subspaces. 2.2 Feature Selection as Heuristic Search We now turn to discussing feature selection algorithms and, more generally, algorithms for dealing with data sets that contain large numbers of irrelevant attributes. <p> Another class of filter methods actually constructs higher-order features from the original ones, orders them in terms of the variance they explain, and selects the best such features. The statistical technique of principal components analysis <ref> (Jolliffe, 1986) </ref>, the best-known example of this approach, generates linear combinations of features whose vectors are orthogonal in the original space. Empirically, principal components has successfully reduced dimensionality on a variety of learning tasks.
Reference: <author> Johnson, D. S. </author> <year> (1974). </year> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> 9 , 256-278. </pages>
Reference: <author> Kearns, M. J., & Vazirani, U. V. </author> <year> (1994). </year> <title> An introduction to computational learning theory. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: each data point is an example paired with an associated label or classification (which might also be Boolean, multiple valued, or continuous). 2 Although the learning algorithm sees only the fixed sample S, it is often helpful to postulate two additional quantities, as is done in the PAC learning model <ref> (e.g., see Kearns & Vazirani, 1994) </ref>: a probability distribution D over the instance space, and a target function c from examples to labels. We then model the sample S as having been produced by repeatedly selecting examples from D and then labeling them according to the function c.
Reference: <author> Kira, K., & Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Stanfill (1987) and Ting (1994) describe filter-like methods that use conditional probability distributions to weight attributes for nearest neighbor. Daelemans et al. (1994) present a different weighting scheme that normalizes features based on an information-theoretic metric, and one could use the scores produced by Relief <ref> (Kira & Rendell, 1992) </ref> to the same end. Finally, Kohavi, Langley, and Yun (1997) have adapted the wrapper method to search through a discretized weight space that can be explored in much the same way as feature sets.
Reference: <author> Kivinen, J., & Warmuth, M. K. </author> <year> (1995). </year> <title> Additive versus exponentiated gradient updates for linear prediction. </title> <booktitle> Proceedings of the 27th Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 209-218). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> The power of decision tables. </title> <booktitle> Proceedings of the Eighth European Conference on Machine Learning. </booktitle>
Reference: <author> Kohavi, R., Langley, P., & Yun, Y. </author> <year> (1997). </year> <title> The utility of feature weighting in nearest-neighbor algorithms. </title> <booktitle> Proceedings of the Ninth European Conference on Machine Learning. </booktitle> <address> Prague: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Knobe, B., & Knobe, K. </author> <year> (1977). </year> <title> A method for inferring context-free grammars. </title> <note> Information and Control , 31 , 129-146. </note>
Reference: <author> Koller, D., & Sahami, M. </author> <year> (1996). </year> <title> Toward optimal feature selection. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of Relief. </title> <booktitle> Proceedings of the Seventh European Conference on Machine Learning. </booktitle>
Reference: <author> Kubat, M., Flotzinger, D., & Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering patterns in EEG signals: Comparative study of a few methods. </title> <booktitle> Proceedings of the 1993 European Conference on Machine Learning (pp. </booktitle> <pages> 367-371). </pages> <address> Vienna: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Kulkarni, D., & Simon, H. A. </author> <year> (1990). </year> <title> Experimentation in machine discovery. </title> <editor> In J. Shrager & P. Langley (Eds.), </editor> <title> Computational models of scientific discovery and theory formation. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., & Iba, W. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Chambery, France. </address>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994a). </year> <title> Oblivious decision trees and abstract cases. </title> <booktitle> Working Notes of the AAAI94 Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 113-117). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994b). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 399-406). </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1997). </year> <title> Scaling to domains with many irrelevant features. </title> <editor> In R. Greiner (Ed.), </editor> <booktitle> Computational learning theory and natural learning systems (Vol. 4). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <address> 23 Lewis, D. D. </address> <year> (1992a). </year> <title> Representation and learning in information retrieval . Doctoral dissertation, </title> <institution> Department of Computer Science, University of Massachusetts, Amherst. </institution> <note> Also available as Technical Report UM-CS-1991-093. </note>
Reference: <author> Lewis, D. D. </author> <year> (1992b). </year> <title> Feature selection and feature extraction for text categorization. </title> <booktitle> Proceedings of Speech and Natural Language Workshop (pp. </booktitle> <pages> 212-217). </pages> <address> San Francsico: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lewis, D. D., & Catlett, J. </author> <year> (1994). </year> <title> Heterogeneous uncertainty sampling. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 148-156). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lin, L. J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <booktitle> Machine Learning, </booktitle> <pages> 8 , 293-321. </pages>
Reference-contexts: For example, Scott and Markovitch (1991) adapt this idea to unsupervised learning situations, and many methods for reinforcement learning include a bias toward exploring unfamiliar parts of the state space <ref> (e.g., Lin, 1992) </ref>. Both approaches can considerably increase learning rates over random presentations. Most work on selecting and querying unlabeled data has used embedded methods, but Angluin et al. (1993) and Blum et al. (1995) describe theoretical results for a wrapper query method that can be applied to any algorithm.
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear threshold algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 285-318. </pages>
Reference: <author> Littlestone, N., Long, P. M., & Warmuth, M. K. </author> <year> (1991). </year> <title> On-line learning of linear functions. </title> <booktitle> In Proceedings of the Twenty-third Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 465-475). </pages> <address> New Orleans: </address> <publisher> ACM Press. </publisher>
Reference: <author> Littlestone, N., & Mesterharm, C. </author> <year> (1997). </year> <title> An apobayesian relative of Winnow. </title> <editor> In M. C. Mozer, M. I. Jordan, & T. Petsche (Eds.), </editor> <booktitle> Advances in neural information processing systems (Vol. 9). </booktitle> <address> Denver, </address> <publisher> CO: MIT Press. </publisher>
Reference-contexts: Experimental tests of Winnow and related multiplicative methods on natural domains have revealed good behavior (Armstrong et al., 1995; Blum, 1995), and studies with synthetic data show that they scale very well to domains with even thousands of irrelevant features <ref> (Littlestone & Mesterharm, 1997) </ref>. More generally, weighting methods are often cast as ways of merging advice from different knowledge sources that may themselves be generated through learning. In this light, the weighting process plays an interesting dual role with respect to the filter methods discussed earlier.
Reference: <author> Littlestone, N., & Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <booktitle> Information and Computation, </booktitle> <pages> 108 , 212-261. </pages>
Reference: <author> Lovasz, L., & Simonovits, M. </author> <year> (1992). </year> <title> On the randomized complexity of volume and diameter. </title> <booktitle> Proceedings of the IEEE Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 482-491). </pages> <publisher> IEEE. </publisher>
Reference: <author> Lund, C., & Yannakakis, M. </author> <year> (1993). </year> <title> On the hardness of approximating minimization problems. </title> <booktitle> Proceedings of the Annual ACM Symposium on the Theory of Computing (pp. </booktitle> <pages> 286-293). </pages>
Reference-contexts: In the other direction, finding the smallest disjunction consistent with a given set of data is NP-hard (Garey & Johnson, 1979); a polynomial-time algorithm to find disjunctions only c log n times larger than the smallest for c &lt; 1=4 would place NP into quasi-polynomial time <ref> (Lund & Yannakakis, 1993) </ref>. 7 form include learning intersections of halfspaces in constant-dimensional spaces (Blumer et al., 1989), and algorithms for learning DNF formulas in n O (log n) time under the uniform distribution (Verbeurgt, 1990).
Reference: <author> Matheus, C. J., & Rendell, L. A. </author> <year> (1989). </year> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1980). </year> <title> Pattern recognition as rule-guided inductive inference. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <pages> 2 , 349-361. </pages>
Reference: <author> Minsky, M., & Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An introduction to computational geometry. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: For instance, the most common is some form of gradient descent, in which training instances lead to simultaneous changes in all weights. Perhaps the best-known attribute-weighting method is the perceptron updating rule <ref> (Minsky & Papert, 1969) </ref>, which adds or subtracts weights on a linear threshold unit in response to errors on training instances.
Reference: <author> Mitchell, T. M. </author> <year> (1982). </year> <title> Generalization as search. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 18 , 203-226. </pages> <note> Reprinted in J. </note> <editor> W. Shavlik & T. G. Dietterich (Eds.) </editor> <booktitle> (1990), Readings in machine learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 190-198). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Norton, S. W. </author> <year> (1989). </year> <title> Generating better decision trees. </title> <booktitle> Proceedings of the Eleventh International Conference on Artificial Intelligence (pp. </booktitle> <pages> 800-805). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Embedded selection methods that rely 8 Some researchers have attempted to remedy these problems by replacing greedy search with lookahead techniques <ref> (e.g., Norton, 1989) </ref>, with some success. Of course, more extensive search carries with it a significant increase in computational cost.
Reference: <author> Pazzani, M. J., & Sarrett, W. </author> <year> (1992). </year> <title> A framework for the average case analysis of conjunctive learning algorithms. </title> <booktitle> Machine Learning, </booktitle> <pages> 9 , 349-372. </pages>
Reference: <author> Pagallo, G., & Haussler, D. </author> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 5 , 24 71-99. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rajamoney, S. </author> <year> (1990). </year> <title> A computational approach to theory revision. </title> <editor> In J. Shrager & P. Langley (Eds.), </editor> <title> Computational models of scientific discovery and theory formation. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rivest, R. L., & Schapire, R. E. </author> <year> (1993). </year> <title> Inference of finite automata using homing sequences. </title> <booktitle> Information and Computation, </booktitle> <pages> 103 , 299-347. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition (Vol. 1). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Perhaps the best-known attribute-weighting method is the perceptron updating rule (Minsky & Papert, 1969), which adds or subtracts weights on a linear threshold unit in response to errors on training instances. The least-mean squares algorithm (Widrow & Hoff, 1960) for linear units and backpropagation <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>, its generalization for multilayer neural networks, also involve additive changes to a set of weights to reduce error on the training set. 5 Baluja and Pomerleau, in this issue, discuss using a neural network approach in domains whose features have time-varying degrees of relevance.
Reference: <author> Sammut, C., & Banerji, R. B. </author> <year> (1986). </year> <title> Learning concepts by asking questions. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. 2). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <booktitle> Machine Learning, </booktitle> <pages> 5 , 197-227. </pages>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Efficiently inducing determinations: A complete and efficient search algorithm that uses optimal pruning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 284-290). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Scott, P. D., & Markovitz, S. </author> <year> (1991). </year> <title> Representation generation in an exploratory learning system. </title>
Reference: <author> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </author> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Seung, H. S., Opper, M., & Sompolinsky, H. </author> <year> (1992). </year> <title> Query by committee. </title> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 287-294). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference-contexts: This can be useful in scenarios where unlabeled data is plentiful, but where the labeling process is expensive. One generic approach to this problem, which can be embedded within an induction algorithm that maintains a set of hypotheses consistent with the training data, is called query by committee <ref> (Seung et al., 1992) </ref>. Given an unlabeled instance, the method selects two hypotheses at random from the consistent set and, if they make different predictions, requests the label for the instance.
Reference: <author> Shen, W. M., & Simon, H. A. </author> <year> (1989). </year> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 675-680). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sinclair, A., & Jerrum, M. </author> <year> (1989). </year> <title> Approximate counting, uniform generation and rapidly mixing Markov chains. </title> <booktitle> Information and Computation, </booktitle> <pages> 82 , 93-133. </pages>
Reference: <author> Singh, M., & Provan, G. M. </author> <year> (1995). </year> <title> A comparison of induction algorithms for selective and nonselective Bayesian classifiers. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 497-505). </pages> <address> Lake Tahoe, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, M., & Provan, G. M. </author> <year> (1996). </year> <title> Efficient learning of selective Bayesian network classifiers. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Skalak, D. B. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill-climbing algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 293-301). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stanfill, C. W. </author> <year> (1987). </year> <title> Memory-based reasoning applied to English pronunciation. </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 577-581). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. 25 Ting, </publisher> <editor> K. M. </editor> <year> (1994). </year> <title> Discretization of continuous-valued attributes and instance-based learning (Technical Report No. </title> <type> 491). </type> <institution> Sydney: University of Sydney, Basser Department of Computer Science. </institution>
Reference: <author> Townsend-Weber, T., & Kibler, D. </author> <year> (1994). </year> <title> Instance-based prediction of continuous values. </title> <booktitle> Working Notes of the AAAI94 Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 30-35). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Verbeurgt, K. </author> <year> (1990). </year> <title> Learning DNF under the uniform distribution in polynomial time. </title> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 314-325). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: n times larger than the smallest for c &lt; 1=4 would place NP into quasi-polynomial time (Lund & Yannakakis, 1993). 7 form include learning intersections of halfspaces in constant-dimensional spaces (Blumer et al., 1989), and algorithms for learning DNF formulas in n O (log n) time under the uniform distribution <ref> (Verbeurgt, 1990) </ref>. The above results for the greedy set-cover method are distribution free and worst case, but Pazzani and Sarrett (1992) report an average-case analysis of even simpler methods for conjunctive learning that imply logarithmic growth for certain product distributions.
Reference: <author> Vere, S. A. </author> <year> (1975). </year> <title> Induction of concepts in the predicate calculus. </title> <booktitle> Proceedings of the Fourth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 281-287). </pages> <address> Tbilisi, USSR: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vovk, V. </author> <year> (1990). </year> <title> Aggregating strategies. </title> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 371-383). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Widrow, B., & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> IRE WESCON Convention Record (pp. </booktitle> <pages> 96-104). </pages>
Reference-contexts: Perhaps the best-known attribute-weighting method is the perceptron updating rule (Minsky & Papert, 1969), which adds or subtracts weights on a linear threshold unit in response to errors on training instances. The least-mean squares algorithm <ref> (Widrow & Hoff, 1960) </ref> for linear units and backpropagation (Rumelhart, Hinton, & Williams, 1986), its generalization for multilayer neural networks, also involve additive changes to a set of weights to reduce error on the training set. 5 Baluja and Pomerleau, in this issue, discuss using a neural network approach in domains
Reference: <author> Winston, P. H. </author> <year> (1975). </year> <title> Learning structural descriptions from examples. </title> <editor> In P. H. Winston (Ed.), </editor> <booktitle> The psychology of computer vision. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher> <pages> 26 </pages>
Reference-contexts: This suggests a second broad type of relevance that concerns the examples themselves, and here we briefly consider techniques for their selection. Some work has assumed the presence of a benevolent tutor who gives informative instances, such as near misses, or provides ideal training sequences <ref> (Winston, 1975) </ref>. However, a more robust approach involves letting the learning system select or focus on training examples by itself. Researchers have proposed at least three reasons for selecting examples used during learning.
References-found: 95

