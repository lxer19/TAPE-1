URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9703.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: fpendrith,malcolmrg@cse.unsw.edu.au  
Title: Estimator Variance in Reinforcement Learning: Theoretical Problems and Practical Solutions  
Author: Mark D. Pendrith and Malcolm R.K. Ryan 
Note: Communicated by Claude Sammut  
Date: 13 MARCH 1997  
Address: Sydney 2052 Australia  
Affiliation: School of Computer Science and Engineering The University of New South Wales  
Pubnum: UNSW-CSE-TR-9703  
Abstract-found: 0
Intro-found: 1
Reference: <author> Brooks, R. </author> <year> (1991). </year> <title> Intelligence without reason. </title> <booktitle> In Proceedings of the 12 th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 569-595. </pages>
Reference-contexts: The robot was given a set of primitive "reflexes" in the spirit of Rodney Brooks' "subsumption" architecture <ref> (Brooks, 1991) </ref>.
Reference: <author> Kushner, H., & Clark, D. </author> <year> (1978). </year> <title> Stochastic approximation methods for constrained and unconstrained systems. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The resulting choice is usually a trade-off between fast adaptation (large fi) and low estimator variance (small fi). In RL, and in adaptive parameter estimation systems generally, there emerges a natural tension between the issues of convergence and adaptability. Stochastic convergence theory <ref> (Kushner & Clark, 1978) </ref> suggests that a 5 reducing fi series (such as fi i = 1=i) with the properties 1 X fi i = 1; and i=1 i &lt; 1 (8) may be used to adjust an estimator's value for successive returns; this will guarantee in-limit convergence under suitable conditions.
Reference: <author> Pendrith, M., & Ryan, M. </author> <year> (1996). </year> <title> Actual return reinforcement learning versus Temporal Differences: Some theoretical and experimental results. </title> <editor> In L.Saitta (Ed.), </editor> <booktitle> Machine Learning: Proc. of the Thirteenth Int. Conf. </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: We will also consider non-Markov Decision Processes (NMDPs) which are formally identical except that the Markov assumption is relaxed; for a variety of reasons NMDPs often better model complex real-world RL domains than do MDPs <ref> (Pendrith & Ryan, 1996) </ref>. Generally when faced with a decision process the problem is to discover a mapping S ! A (or policy) that maximises the expected total future discounted 1 reward R fl = P 1 t=0 fl t r t for some discount factor fl 2 [0; 1]. <p> The central conceptual importance of the CTR to RL techniques is that virtually all RL algorithms estimate the value function of the state/action pairs of the system using either single or multi-step CTRs directly, as in the case of QL or the C-Trace algorithm <ref> (Pendrith & Ryan, 1996) </ref>, or as returns that are equivalent to weighted sums of varying length n-step CTRs, such as the T D () return (Sutton, 1988; Watkins, 1989). 3 CTR Bias and Variance For RL in Markovian domains, the choice of length of CTR is usually viewed as a trade-off <p> Hence, the overall estimator variance for this NMDP is strictly greater at all stages of learning for 1-step CTRs than for any n-step CTRs n &gt; 1. In previously published work studying RL in noisy and non-Markovian domains <ref> (Pendrith & Ryan, 1996) </ref>, excessive estimator variance appeared to be causing problems for 1-step QL in domains where using Monte Carlo style returns improved matters. These unexpected experimental results did not (and still do not) fit well with the "folk wisdom" concerning estimator bias and variance in RL. <p> As well as being noisy, this domain was non-Markovian by virtue of the compact but coarse discretized state-space representation. This compact representation 4 meant learning was fast, but favoured an RL algorithm that did not rely heavily on the Markov assumption; in earlier work <ref> (Pendrith & Ryan, 1996) </ref> C-Trace had been shown to be well-suited for this domain. The robot was given a set of primitive "reflexes" in the spirit of Rodney Brooks' "subsumption" architecture (Brooks, 1991). <p> While one C-Trace variant, multiple-visit C-Trace, has been described in earlier work <ref> (Pendrith & Ryan, 1996) </ref> in application to this domain, the other, first-visit C-Trace, is a variant that has not been previously described (refer to figure 7 for pseudo-code).
Reference: <author> Puterman, M. </author> <year> (1994). </year> <title> Markov decision processes : discrete stochastic dynamic programming. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: This characterisation works well if the domain is well-modelled by a Markov Decision Process (MDP) <ref> (Puterman, 1994) </ref>. Formally, an MDP can be described as a quintuple hS; A; oe; T; aei. S is the set of process states, which may include a special terminal state.
Reference: <author> Singh, S., & Sutton, R. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <note> To Appear In: Machine Learning. </note>
Reference-contexts: domain. the positive correlation between returns is what prevents them being statistically independent. b) This raises the interesting possibility that the observed improved performance of "replacing traces" owes as much if not more to a reduction in estimator variance than to reduced estimator bias, which is the explanation proposed by <ref> (Singh & Sutton, 1996) </ref>. 15 The ccBeta algorithm has been presented as a practical example of an alter-native approach to managing estimator variance in RL.
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R., Barto, A., & Williams, R. </author> <year> (1992). </year> <title> Reinforcement learning is direct adaptive optimal control. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 12 (2), </volume> <pages> 19-22. </pages>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: the total future discounted reward is just the total reward R = P 1 t=0 r t where all future rewards are weighted equally; otherwise, rewards received sooner are weighted more heavily than those received later. 2.1 Q-learning as on-line value iteration If an RL method like 1-step Q-learning (QL) <ref> (Watkins, 1989) </ref> is used to find the optimal policy for an MDP, the method resembles an asynchronous, online form of the DP value iteration method.
Reference: <author> Watkins, C., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. 16 </pages>
Reference-contexts: Pendrith & Ryan, 1996) that the optimality properties of QL do not generalise to non-Markov systems. The original proof of QL convergence and optimality properties can be found in an expanded form in <ref> (Watkins & Dayan, 1992) </ref>. 2 as a limiting case. We note that if in (2) r (1) t were replaced with the actual return, then this would form the update rule for a Monte Carlo estimation procedure.
References-found: 9

