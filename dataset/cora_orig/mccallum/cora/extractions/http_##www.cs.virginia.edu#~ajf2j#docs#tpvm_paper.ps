URL: http://www.cs.virginia.edu/~ajf2j/docs/tpvm_paper.ps
Refering-URL: http://www.cs.virginia.edu/~ajf2j/tpvm.html
Root-URL: http://www.cs.virginia.edu
Email: ferrari@virginia.edu  vss@mathcs.emory.edu  
Title: TPVM: Distributed Concurrent Computing with Lightweight Processes  
Author: Adam Ferrari and V. S. Sunderam 
Note: Research supported by the Applied Mathematical Sciences program, Office of Basic Energy Sciences, U. S. Department of Energy, under Grant No. DE-FG05-91ER25105, the Office of Naval Research, under Grant No. N00014-93-1-0278, and the National Science Foundation, under Award Nos. CCR-9118787 and ASC-9214149.  
Address: Charlottesville, VA 22903, USA  Atlanta, GA 30322, USA  
Affiliation: Department of Computer Science University of Virginia,  Department of Mathematics and Computer Science Emory University,  
Abstract: The TPVM (Threads-oriented PVM) system is an experimental auxiliary subsystem for the PVM distributed system, which supports the use of lightweight processes or "threads" as the basic unit of parallelism and scheduling. TPVM provides a library interface which presents both a traditional, task based, explicit message passing model, as well as a data-driven scheduling model that enables straightforward specification of computation based on data dependencies. The TPVM system comprises three basic modules: a library interface that provides access to thread-based distributed concurrent computing facilities, a portable thread interface module which abstracts the required threads-related services, and a thread server module which performs scheduling and system 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.M. Butler and E.L. Lusk, </author> <title> "Monitors, Messages, and Clusters: The P4 Parallel Programming System", </title> <booktitle> Parallel Computing 20, </booktitle> <pages> pp. 417-444, </pages> <year> 1994. </year>
Reference-contexts: A plethora of such software systems have been developed over the past decade [16, 3]; however, a handful of these including PVM [14], Express [12], Linda [2], and P4/Parmacs <ref> [1] </ref> account for an overwhelming majority of those in current use.
Reference: [2] <author> N. Carriero and D. Galernter, </author> <title> "Linda in Context", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, No. 4, </volume> <month> pp.444-458 April </month> <year> 1989. </year>
Reference-contexts: A plethora of such software systems have been developed over the past decade [16, 3]; however, a handful of these including PVM [14], Express [12], Linda <ref> [2] </ref>, and P4/Parmacs [1] account for an overwhelming majority of those in current use. <p> A running PVM task may declare itself a potential "host" or "containing" pod for a given type of thread using the following library routine: char *name = "Type of thread to export"; void (*func)() = thread_entry_point; int flags = PvmDflowModel; /* Or PvmProcModel */ int nrules = 2; int rules <ref> [2] </ref> = - 1, 2 -; tpvm_export (name, func, flags, nrules, rules); The first parameter specifies the name of the declared thread, a well known label which may be used throughout the virtual machine to identify a given type of thread, 1 We use the term dataflow in a somewhat nonstandard
Reference: [3] <author> Doreen Y. Cheng, </author> <title> "A Survey of Parallel Programming Tools", </title> <type> Technical Report RND-91-005, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Since, by definition, distributed concurrent computing utilizes varied hardware, the mainstay of this technology has been the software infrastructure that manages underlying machine and network resources and presents a usable programming interface and model to applications. A plethora of such software systems have been developed over the past decade <ref> [16, 3] </ref>; however, a handful of these including PVM [14], Express [12], Linda [2], and P4/Parmacs [1] account for an overwhelming majority of those in current use.
Reference: [4] <author> S. Crane, </author> <title> "The Rex lightweight process library" Computer Science Technical Report, </title> <institution> Imperial College of Science and Technology, </institution> <address> London, England, gummo.doc.ic.ac.uk, </address> <year> 1993. </year>
Reference-contexts: These are fairly basic operations which can be supported by most typical thread systems with little implementation effort. Our prototype design has been implemented over two quite different thread systems. For use with SunOS 4.x, we implemented the thread interface over the Rex <ref> [4] </ref> thread system. This thread library operates completely in user space and is non-preemptive. Scheduling is performed only at the request of the user. Next, we implemented the thread interface over the Solaris threads library [15] which provides kernel supported, preemptive threads.
Reference: [5] <author> G. Fox et al., </author> <title> Solving Problems On Concurrent Processors, </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: It should also be noted that the relative costs of local and remote messages will be greatly affected by the load level of the underlying network. 5.2 Matrix Multiply Application As a test application, we implemented the matrix multiplication algorithm described in <ref> [5] </ref>. This algorithm decomposes the operand and result matrices into square sub-blocks, each assigned to a processing element. Communications are regular and occur in a mesh pattern. Due to the data partitioning scheme employed by this algorithm, it requires an even square number of processing entities.
Reference: [6] <author> G. A. Geist, A. Beguelin, et. al., </author> <title> PVM: Parallel Virtual Machine A Users' Guide and Tutorial for Networked Parallel Computing, </title> <publisher> The MIT Press, </publisher> <address> Cambridge. MA, </address> <year> 1994. </year>
Reference-contexts: following: buf = tpvm_recv (message_tag, -1); tpvm_bufinfo (buf, &source, &size, &tag); printf ("Message received from thread id %d"n", source); tpvm_upkint (&vecsize, 1, 1); tpvm_upkdouble (vector, vecsize, 1); A complete discussion of the details related to the use of PVM style message passing and buffer management primitives can be found in <ref> [6] </ref>. 3.2 Coarse-grained Dataflow TPVM Programming The TPVM coarse-grained dataflow programming model is intended to mask much of the inherent complexity of the message passing model. While straightforward in principle, message passing models typically force the programmer to explicitly manage all of task creation and synchronization in a program.
Reference: [7] <author> G. A. Geist and V. S. Sunderam, </author> <title> "Network Based Concurrent Computing on the PVM System", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <volume> 4(4), </volume> <pages> pp. 293-311, </pages> <month> June </month> <year> 1992. </year>
Reference: [8] <author> G. A. Geist and V. S. Sunderam, </author> <title> "The Evolution of the PVM Concurrent Computing System", </title> <booktitle> Proceedings 26th IEEE Compcon Symposium, </booktitle> <pages> pp. 471-478, </pages> <address> San Fransisco, </address> <month> February </month> <year> 1993. </year>
Reference: [9] <author> A.S. Grimshaw, </author> <title> "Easy-to-Use Object-Oriented Parallel Processing with Mentat", </title> <journal> Computer, </journal> <volume> Vol. 26 No.5, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: While a traditional dataflow model involves a fine-grained, static program graph, our dataflow model involves coarse-grained actors and a dynamically resolved program graph. This model, sometimes referred to as "macro dataflow" was introduced by the Mentat <ref> [9] </ref> system. 7 just as a "component name" is used in PVM. The second parameter indicates the code entry point in the calling process for the thread type named.
Reference: [10] <author> C. Hartley and V. S. Sunderam, </author> <title> "Concurrent Programming with Shared Objects in Networked Environments", </title> <booktitle> Proceedings 7th Intl. Parallel Processing Symposium, </booktitle> <pages> pp. 471-478, </pages> <address> Los Angeles, </address> <month> April </month> <year> 1993. </year>
Reference: [11] <author> Rusty Lusk, </author> <type> Personal Communication, </type> <month> December </month> <year> 1993. </year> <title> [12] "The Express Way to Distributed Processing", </title> <booktitle> Supercomputing Review, </booktitle> <address> pp.54-55, </address> <month> May </month> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Finally, new concepts such as active messages [17] and remote memory copy <ref> [11] </ref> are attractive but extremely cumbersome to implement within a process-oriented framework. In order to investigate the viability of a threads-based distributed concurrent computing model to address some of the above issues, we have designed and implemented an experimental system called TPVM (Threads-oriented PVM) which we describe in this paper.
Reference: [13] <author> B. Schmidt and V. S. Sunderam, </author> <title> "Empirical Analysis of Overheads in Cluster Environments", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <note> (to appear), </note> <year> 1993. </year>
Reference: [14] <author> V. S. Sunderam, </author> <title> "PVM : A Framework for Parallel Distributed Computing", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: A plethora of such software systems have been developed over the past decade [16, 3]; however, a handful of these including PVM <ref> [14] </ref>, Express [12], Linda [2], and P4/Parmacs [1] account for an overwhelming majority of those in current use.
Reference: [15] <institution> SunOS 5.3 Reference Manual, Sun Microsystems, Mountain View, </institution> <address> CA, </address> <year> 1993. </year>
Reference-contexts: In other words, a collection of TPVM threads that comprise an application are created and execute within the context of a (usually smaller) number of regular processes. These host processes or "pods" themselves do not contribute to the computation, as is typical with other threads systems e.g. SunOS LWP <ref> [15] </ref>. The TPVM system therefore requires that host processes are instantiated and established prior to the active computational entities, i.e. the threads, being started. 3 Host processes or pods are initiated via normal native PVM mechanisms, e.g. pvm spawn or a console command. <p> For use with SunOS 4.x, we implemented the thread interface over the Rex [4] thread system. This thread library operates completely in user space and is non-preemptive. Scheduling is performed only at the request of the user. Next, we implemented the thread interface over the Solaris threads library <ref> [15] </ref> which provides kernel supported, preemptive threads. Here, the use of semaphores to control scheduling was required. Given these two porting experiences, we were able to determine the necessary services required of two general classes thread systems to implement our portable thread interface.
Reference: [16] <author> Louis H. Turcotte, </author> <title> "A Survey of Software Environments for Exploiting Net-worked Computing Resources", </title> <institution> Engineering Research Center for Computational Field Simulation, Mississippi State, MS, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Since, by definition, distributed concurrent computing utilizes varied hardware, the mainstay of this technology has been the software infrastructure that manages underlying machine and network resources and presents a usable programming interface and model to applications. A plethora of such software systems have been developed over the past decade <ref> [16, 3] </ref>; however, a handful of these including PVM [14], Express [12], Linda [2], and P4/Parmacs [1] account for an overwhelming majority of those in current use.
Reference: [17] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, Klaus Erik Schauser, </author> <title> "Active Messages: a Mechanism for Integrated Communication and Computation" Proc. </title> <booktitle> Intl. Symposium on Computer Architecture pp.256-266, </booktitle> <month> May, </month> <year> 1992. </year>
Reference-contexts: Second, especially in heterogeneous networked environments, processors with multiple (2-8) CPU's and global memory are becoming commonplace; when collections of such machines are used as a parallel computing platform, the traditional process-oriented message passing model is probably suboptimal. Finally, new concepts such as active messages <ref> [17] </ref> and remote memory copy [11] are attractive but extremely cumbersome to implement within a process-oriented framework.
References-found: 16

