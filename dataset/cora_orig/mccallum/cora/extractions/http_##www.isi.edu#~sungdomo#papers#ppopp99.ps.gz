URL: http://www.isi.edu/~sungdomo/papers/ppopp99.ps.gz
Refering-URL: http://www.isi.edu/~sungdomo/publications.html
Root-URL: http://www.isi.edu
Email: fsungdomo,mhallg@isi.edu  
Title: Evaluation of Predicated Array Data-Flow Analysis for Automatic Parallelization  
Author: Sungdo Moon and Mary W. Hall 
Address: Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: This paper presents an evaluation of a new analysis for par-allelizing compilers called predicated array data-flow analysis. This analysis extends array data-flow analysis for par-allelization and privatization to associate predicates with data-flow values. These predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible. These conditions can be used both to enhance compile-time analysis and to introduce run-time tests that guard safe execution of a parallelized version of a computation. As compared to previous work that combines predicates with array data-flow analysis, our approach is distinguished by two features: (1) it derives low-cost, run-time paralleliza-tion tests; and, (2) it incorporates predicate embedding and predicate extraction, which translate between the domain of predicates and data-flow values to derive more precise analysis results. We present extensive experimental results across three benchmark suites and one additional program, demonstrating that predicated array data-flow analysis par-allelizes more than 40% of the remaining inherently parallel loops left unparallelized by the SUIF compiler and that it yields improved speedups for 5 programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Saman P. Amarasinghe. </author> <title> Parallelizing Compiler Techniques Based on Linear Inequalities. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering, Stanford University, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: In Figure 1 (d), help <ref> [1] </ref> may be upwards exposed if d &lt; 2, since in this case the first loop containing the writes to help would not execute. <p> proposed by Sharma et al. attempts to partially perform data-flow analysis at run time, using control-flow information derived during execution. 3 Previous Array Data-Flow Analysis Framework The SUIF compiler uses an interprocedural array data-flow analysis to determine which loops access independent memory locations, or for which privatization eliminates remaining dependences <ref> [1, 16, 19] </ref>. The analysis computes data-flow values for each program region PR in a hierarchical program representation called the region raph, where a program region is either a basic block, a loop body, a loop, a procedure call, or a procedure body. <p> Since ELPD is a run-time test, parallelization is guaranteed to be safe only for the input used. Collectively, these programs contain more than 4000 loops, and the base SUIF system parallelizes over 50% of them <ref> [1] </ref>. (Note that the base SUIF system used in this paper par-allelizes a few more loops than in previous work.) Of the remaining roughly 2000 loops, some are not candidates for parallelization because of read I/O or internal exits or because they are nested inside already parallelized loops; others do not
Reference: [2] <author> Glenn Ammons and James R. Larus. </author> <title> Improving data-flow analysis with path profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN '98 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 72-84, </pages> <address> Mon-treal, Canada, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [2, 6, 27] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [3] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 41-53, </pages> <address> Port-land, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [4] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jae-jin Lee, David Padua, Yunheung Paek, Bill Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Parallel programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stanford SUIF compiler <ref> [4, 18] </ref>. While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. We have been conducting an evaluation of the Stanford SUIF compiler to determine whether such programs have inherent loop-level parallelism that is being missed by the compiler [29].
Reference: [5] <author> William J. Blume. </author> <title> Symbolic Analysis Techniques for Effective Automatic Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [32], type analysis [30], symbolic analysis for paral-lelization <ref> [15, 5] </ref>, and the array data-flow analysis described above [14, 31]. <p> = help [j] else endfor endfor endfor . . . = help [j-1] endfor endfor endfor endfor endfor (a) improves compile-time (b) derives run-time test (c) benefits from (d) benefits from analysis predicate embedding predicate extraction Blume presents a method for combining control flow predicates with ranges of scalar variables <ref> [5] </ref>. The PIPS system incorporates integer constraints into its array region representations, as we do with predicate embedding, but it does not use predicates in its analysis in any other way [21].
Reference: [6] <author> Rastislav Bodik, Rajiv Gupta, and Mary Lou Soffa. </author> <title> In-terprocedural conditional branch elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-158, </pages> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [2, 6, 27] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [7] <author> Patrick Cousot and Radhia Cousot. </author> <title> Systematic design of program anaysis frameworks. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 269-282, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson <ref> [7, 23] </ref>. Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed [2, 6, 27].
Reference: [8] <author> Beatrice Creusillet and Francois Irigoin. </author> <title> Interproce-dural array region analyses. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-15, </pages> <address> Columbus, Ohio, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [9] <author> Paul Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 429-441, </pages> <address> Saint Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [10] <author> Paul Feautrier. </author> <title> Parametric integer programming. </title> <journal> RAIRO Recherche Operarionnelle, </journal> <volume> 22 </volume> <pages> 243-268, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [11] <author> Paul Feautrier. </author> <title> Dataflow analysis of scalar and array references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1) </volume> <pages> 23-53, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [12] <author> Gina Goff. </author> <title> Practical techniques to augment dependence analysis in the presence of symbolic terms. </title> <type> Technical Report TR92-194, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Thus, our approach, when applicable, leads to much more efficient tests than inspecting all of the array accesses within the loop body. Predicate extraction is also related to deriving breaking conditions from dependence testing, but breaking conditions have never been obtained in the context of general array data-flow analysis <ref> [12, 24] </ref>. There are some similarities between our approach and much earlier work on data-flow analysis frameworks. Hol-ley and Rosen describe a construction of qualified data-flow problems, but with only a fixed, finite, disjoint set of predicates [20].
Reference: [13] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, New Mexico, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible.
Reference: [14] <author> Junjie Gu, Zhiyuan Li, and Gyungho Lee. </author> <title> Experience with efficient array data-flow analysis for array priva-tization. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 157-167, </pages> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee <ref> [14] </ref>, but none of these analysis-based techniques use predicates to derive run-time tests [5, 14, 15, 20, 31, 32]. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. <p> A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> To make this argument more concrete, consider the examples in Figure 1. Figure 1 (a) shows an example that could be parallelized by previous array data-flow analysis techniques incorporating predicates <ref> [14, 31] </ref>. Figure 1 (c) shows how predicate embedding can also be used to improve the results of compile-time analysis. On the other hand, Figures 1 (b) and (d) show how our analysis derives run-time tests to parallelize more loops than these previous approaches. <p> Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [32], type analysis [30], symbolic analysis for paral-lelization [15, 5], and the array data-flow analysis described above <ref> [14, 31] </ref>.
Reference: [15] <author> Mohammad R. Haghighat. </author> <title> Symbolic Analysis for Par-allelizing Compilers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [32], type analysis [30], symbolic analysis for paral-lelization <ref> [15, 5] </ref>, and the array data-flow analysis described above [14, 31]. <p> Related to these array analysis techniques are approaches to enhance scalar symbolic analysis for parallelization. Haghighat describes an algebra on control flow predicates <ref> [15] </ref> while 2 for i = 1 to c for i = 1 to c for i = 1 to c for i = 1 to c if (x &gt; 5) then if (x &gt; 5) then help [j] = . . . help [j-1] = . . . help [j]
Reference: [16] <author> Mary W. Hall, Saman P. Amarasinghe, Jennifer M. An-derson, Brian R. Murphy, Shih-Wei Liao, and Monica S. Lam. </author> <title> Interprocedural parallelization analysis in SUIF. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <note> To appear, </note> <year> 1999. </year>
Reference-contexts: proposed by Sharma et al. attempts to partially perform data-flow analysis at run time, using control-flow information derived during execution. 3 Previous Array Data-Flow Analysis Framework The SUIF compiler uses an interprocedural array data-flow analysis to determine which loops access independent memory locations, or for which privatization eliminates remaining dependences <ref> [1, 16, 19] </ref>. The analysis computes data-flow values for each program region PR in a hierarchical program representation called the region raph, where a program region is either a basic block, a loop body, a loop, a procedure call, or a procedure body. <p> The Reshape calculation also uses Fourier-Motzkin projection to eliminate variables representing the dimensions of the formal parameter and replace them with corresponding values for variables representing the dimensions of the actual parameter. The existing array data-flow analysis framework is presented in detail elsewhere <ref> [16] </ref>. Dependence and Privatization Tests. The dependence and privatization tests, and the derivation of regions in priva-tizable arrays requiring initialization are presented below. These tests refer to the R, W and E components of the data-flow value of a loop for accesses to a particular array.
Reference: [17] <author> Mary W. Hall, Saman P. Amarasinghe, Brian R. Mur-phy, Shih-Wei Liao, and Monica S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural par-allelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, California, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Parallel code is generated in SUIF by encapsulating a parallel loop in a separate function and making parallel function calls. In previous experiments with SUIF, the function representing a parallel loop has been implemented by C code where the variables referenced by the function are accessed through pointers <ref> [17] </ref>, and by Fortran code where the variables in the parallel loop are accessed directly [18]. We chose the former approach because it is more straightforward, but we found that the resulting parallel programs ran much slower on 1 processor than their sequential counterparts.
Reference: [18] <author> Mary W. Hall, Jennifer M. Anderson, Saman P. Ama-rasinghe, Brian R. Murphy, Shih-Wei Liao, Edouard Bugnion, and Monica S. Lam. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Parallelizing compilers are becoming increasingly successful at exploiting coarse-grain parallelism in scientific computations, as evidenced by recent experimental results from both the Polaris system at University of Illinois and from the Stanford SUIF compiler <ref> [4, 18] </ref>. While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. We have been conducting an evaluation of the Stanford SUIF compiler to determine whether such programs have inherent loop-level parallelism that is being missed by the compiler [29]. <p> In previous experiments with SUIF, the function representing a parallel loop has been implemented by C code where the variables referenced by the function are accessed through pointers [17], and by Fortran code where the variables in the parallel loop are accessed directly <ref> [18] </ref>. We chose the former approach because it is more straightforward, but we found that the resulting parallel programs ran much slower on 1 processor than their sequential counterparts.
Reference: [19] <author> Mary W. Hall, Brian R. Murphy, Saman P. Amaras-inghe, Shih-Wei Liao, and Monica S. Lam. </author> <title> Interproce-dural analysis for parallelization. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 61-80, </pages> <address> Columbus, Ohio, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The SUIF automatic parallelization system, upon which we base our implementation, performs a single array data-flow analysis to test for both independence and privatization, as will be discussed in Section 3 <ref> [19] </ref>. <p> proposed by Sharma et al. attempts to partially perform data-flow analysis at run time, using control-flow information derived during execution. 3 Previous Array Data-Flow Analysis Framework The SUIF compiler uses an interprocedural array data-flow analysis to determine which loops access independent memory locations, or for which privatization eliminates remaining dependences <ref> [1, 16, 19] </ref>. The analysis computes data-flow values for each program region PR in a hierarchical program representation called the region raph, where a program region is either a basic block, a loop body, a loop, a procedure call, or a procedure body.
Reference: [20] <author> L. Howard Holley and Barry K. Rosen. </author> <title> Qualified data flow problems. </title> <booktitle> In Conference Record of the Seventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 68-82, </pages> <address> Las Vegas, Nevada, </address> <month> January </month> <year> 1980. </year>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> There are some similarities between our approach and much earlier work on data-flow analysis frameworks. Hol-ley and Rosen describe a construction of qualified data-flow problems, but with only a fixed, finite, disjoint set of predicates <ref> [20] </ref>. Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson [7, 23].
Reference: [21] <author> Francois Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In Proceedings of the NSF-CNRS Workshop on Environment and Tools for Parallel Scientific Programming, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: The PIPS system incorporates integer constraints into its array region representations, as we do with predicate embedding, but it does not use predicates in its analysis in any other way <ref> [21] </ref>. <p> Predicate embedding and predicate extraction fit nicely with array region representations based on integer linear programming techniques such as in SUIF and the PIPS system <ref> [21] </ref>. In a framework that supports both predicate embedding and extraction, it is equivalent for integer constraints to appear either in the predicate or in the data-flow value. For predicate embedding, additional integer constraints are integrated into the integer linear inequalities representing an array region.
Reference: [22] <author> Sungdo Moon, Mary W. Hall, and Brian R. Murphy. </author> <title> Predicated array data-flow analysis for run-time par-allelization. </title> <booktitle> In Proceedings of the 1998 ACM International Conference on Supercomputing, </booktitle> <pages> pages 204-211, </pages> <address> Melbourne, Australia, </address> <month> July </month> <year> 1998. </year> <month> 11 </month>
Reference-contexts: data-flow analysis derives much simpler run-time tests that focus on the cause of the dependence (for example, evaluating control flow paths that will be taken or testing for boundary conditions). 1 A previous paper presented an overview of predicated array data-flow analysis with some early experimental results on 3 programs <ref> [22] </ref>. This paper makes two distinct contributions that go beyond our previous work: * We present a comprehensive evaluation across 30 programs, demonstrating that predicated array data-flow analysis parallelizes additional outer loops in 9 programs. <p> Predicates are derived in two ways. Prior to performing the predicated array data-flow analysis, predicates can be derived via a forward interprocedural data-flow analysis that forms the conjunction of all the tests along the control-flow paths reaching the current program point <ref> [22] </ref>. Additional predicates can be extracted during predicated analysis using predicate extraction, described in Section 5. 4.2 Extensions to Array Data-Flow Analysis Operations The formulation of array data-flow analysis is changed in only a few ways by incorporation of predicated data-flow values. <p> Currently, extraction is used only at two points in our analysis. It is performed during PredSubtract as in the example above. It is also used during translation across procedure boundaries, where arrays are delinearized from a calling procedure to a called procedure <ref> [22] </ref>. In this latter case, an entire array is written if the problem size is divisible by one of the dimension sizes in the callee. The Reshape operation returns two predicated data-flow values; an optimistic data-flow value guarded by a symbolic predicate ensuring this property, and the default value.
Reference: [23] <author> Flemming Nielson. </author> <title> Expected forms of data flow anal-ysis. </title> <editor> In Harald Ganzinger and Neil D Jones, editors, </editor> <title> Programs as Data Objects, </title> <booktitle> volume 217 of Lecture Notes on Computer Science, </booktitle> <pages> pages 172-191. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1986. </year>
Reference-contexts: Cousot and Cousot describe a theoretical construction of a reduced cardinal power of two data-flow frameworks, in which a data-flow analysis is performed on the lattice of functions between the two original data-flow lattices, and this technique has been refined by Nielson <ref> [7, 23] </ref>. Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed [2, 6, 27].
Reference: [24] <author> William Pugh and David Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 140-151, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Predicated data-flow analysis associates a predicate with each data-flow value; analysis interprets these predicates as describing a relationship on the data-flow value when the predicate evaluates to true. When applied to the problem of array data-flow analysis <ref> [3, 8, 9, 10, 11, 13, 24] </ref> | the central analysis component of a modern parallelizing compiler | these predicates can be used to derive conditions under which dependences can be eliminated or privatization is possible. <p> Thus, our approach, when applicable, leads to much more efficient tests than inspecting all of the array accesses within the loop body. Predicate extraction is also related to deriving breaking conditions from dependence testing, but breaking conditions have never been obtained in the context of general array data-flow analysis <ref> [12, 24] </ref>. There are some similarities between our approach and much earlier work on data-flow analysis frameworks. Hol-ley and Rosen describe a construction of qualified data-flow problems, but with only a fixed, finite, disjoint set of predicates [20].
Reference: [25] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 218-232, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Our approach is fundamentally different from previous techniques for run-time parallelization testing, most of which are based on an inspector/executor model <ref> [25, 26] </ref>. An inspector/executor introduces several auxiliary arrays per array possibly involved in a dependence, and run-time overhead on the order of the aggregate size of the arrays. <p> Some previous work in run-time parallelization uses specialized techniques not based on data-flow analysis. An inspector/executor technique inspects array accesses at run time immediately prior to execution of the loop <ref> [25, 26] </ref>. The inspector decides whether to execute a parallel or sequential version of the loop. Predicated data-flow analysis instead derives run-time tests based on values of scalar variables that can be tested prior to loop execution. <p> To determine the remaining inherently parallel loops, we performed an experiment where accesses to all arrays reported by the compiler as being involved in a dependence were instrumented with a run-time parallelization and pri-vatization test called the Extended Lazy Privatizing Doall Test (ELPD) <ref> [25, 28, 29] </ref>. The ELPD test instruments all the candidate loops in a program left unparallelized by the compiler; thus, the loops reported to be parallelizable by the ELPD test represent all the remaining loops in the program that could safely be parallelized directly or after pri-vatization.
Reference: [26] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Our approach is fundamentally different from previous techniques for run-time parallelization testing, most of which are based on an inspector/executor model <ref> [25, 26] </ref>. An inspector/executor introduces several auxiliary arrays per array possibly involved in a dependence, and run-time overhead on the order of the aggregate size of the arrays. <p> Some previous work in run-time parallelization uses specialized techniques not based on data-flow analysis. An inspector/executor technique inspects array accesses at run time immediately prior to execution of the loop <ref> [25, 26] </ref>. The inspector decides whether to execute a parallel or sequential version of the loop. Predicated data-flow analysis instead derives run-time tests based on values of scalar variables that can be tested prior to loop execution.
Reference: [27] <author> Shamik D. Sharma, Anurag Acharya, and Joel Saltz. </author> <title> Defered data-flow analysis: Algorithms, proofs and applications. </title> <type> Technical Report UMD-CS-TR-3845, </type> <institution> Dept. of Computer Science, University of Maryland, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Neither of the latter two prior works were designed with predicates as one of the data-flow analysis frameworks, and none of the three techniques derives run-time tests. Recently, additional approaches that, in some way, exploit control-flow information in data-flow analysis have been proposed <ref> [2, 6, 27] </ref>. Ammons and Larus's approach improves the precision of data-flow analysis along frequently taken control flow paths, called hot paths, by using profile information. Bodik et al. describe a demand-driven interprocedural correlation analysis that eliminates some branches by path specialization.
Reference: [28] <author> Byoungro So, Sungdo Moon, and Mary W. Hall. </author> <title> Evaluating automatic parallelization in SUIF. </title> <journal> IEEE Transaction on Parallel and Distributed Systems, </journal> <note> To appear. </note>
Reference-contexts: In Proceedings of the 7th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP'99), Atlanta, Georgia, May 1999 ploiting a substantial amount of the available parallelism in these programs <ref> [28] </ref>. However, the results indicate that there is still room for improving parallelizing compiler technology, primarily in the following two areas: * more precise compile-time analysis by taking condi tional control flow into account * derivation of focused, low-cost run-time parallelization tests from analysis results. <p> To determine the remaining inherently parallel loops, we performed an experiment where accesses to all arrays reported by the compiler as being involved in a dependence were instrumented with a run-time parallelization and pri-vatization test called the Extended Lazy Privatizing Doall Test (ELPD) <ref> [25, 28, 29] </ref>. The ELPD test instruments all the candidate loops in a program left unparallelized by the compiler; thus, the loops reported to be parallelizable by the ELPD test represent all the remaining loops in the program that could safely be parallelized directly or after pri-vatization. <p> In the table, granularity and coverage numbers are omitted for loops nested inside other loops also parallelized by predicated analysis, as SUIF only exploits a single level of parallelism. The next column in the table provides a category for the loop corresponding to classifications defined in <ref> [28] </ref>.
Reference: [29] <author> Byoungro So, Sungdo Moon, and Mary W. Hall. </author> <title> Measuring the effectiveness of automatic parallelization in SUIF. </title> <booktitle> In Proceedings of the 1998 ACM International Conference on Supercomputing, </booktitle> <pages> pages 212-219, </pages> <address> Mel-bourne, Australia, </address> <month> July </month> <year> 1998. </year>
Reference-contexts: While these results are impressive overall, some of the programs presented achieve little or no speedup when executed in parallel. We have been conducting an evaluation of the Stanford SUIF compiler to determine whether such programs have inherent loop-level parallelism that is being missed by the compiler <ref> [29] </ref>. Results on three benchmark suites, Specfp95, NAS sample benchmarks and Perfect, show that overall, SUIF is ex fl This work has been supported by DARPA Contract DABT63-95-C-0118 and NSF Contract ACI-9721368. <p> To determine the remaining inherently parallel loops, we performed an experiment where accesses to all arrays reported by the compiler as being involved in a dependence were instrumented with a run-time parallelization and pri-vatization test called the Extended Lazy Privatizing Doall Test (ELPD) <ref> [25, 28, 29] </ref>. The ELPD test instruments all the candidate loops in a program left unparallelized by the compiler; thus, the loops reported to be parallelizable by the ELPD test represent all the remaining loops in the program that could safely be parallelized directly or after pri-vatization.
Reference: [30] <author> Robert E. Strom and Daniel M. Yellin. </author> <title> Extending type-state checking using conditional liveness analysis. </title> <journal> IEEE Transaction on Software Engineering, </journal> <volume> 19(5) </volume> <pages> 478-485, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [32], type analysis <ref> [30] </ref>, symbolic analysis for paral-lelization [15, 5], and the array data-flow analysis described above [14, 31].
Reference: [31] <author> Peng Tu. </author> <title> Automatic Array Privatization and Demand-driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> To make this argument more concrete, consider the examples in Figure 1. Figure 1 (a) shows an example that could be parallelized by previous array data-flow analysis techniques incorporating predicates <ref> [14, 31] </ref>. Figure 1 (c) shows how predicate embedding can also be used to improve the results of compile-time analysis. On the other hand, Figures 1 (b) and (d) show how our analysis derives run-time tests to parallelize more loops than these previous approaches. <p> Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation [32], type analysis [30], symbolic analysis for paral-lelization [15, 5], and the array data-flow analysis described above <ref> [14, 31] </ref>. <p> Tu and Padua present a limited sparse approach on a gated SSA graph that is demand based, only examining predicates if they might assist in loop bounds or subscript values for parallelization, a technique that appears to be no more powerful than that of Gu, Li and Lee <ref> [31] </ref>. Related to these array analysis techniques are approaches to enhance scalar symbolic analysis for parallelization.
Reference: [32] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 180-210, </pages> <month> April </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: A few existing data-flow analyses incorporate predicates, particularly guarded array data-flow analysis by Gu, Li and Lee [14], but none of these analysis-based techniques use predicates to derive run-time tests <ref> [5, 14, 15, 20, 31, 32] </ref>. Because predicates in previous work are used only during compile-time analysis, they are restricted to some domain of values that the compiler understands. By contrast, our approach can derive more general predicates, run-time evalu-able predicates consisting of arbitrary program statements. <p> Both versions of the loop can then be parallelized. 2.2 Relationship to Previous Work Analysis techniques exploiting predicates have been developed for specific data-flow problems including constant propagation <ref> [32] </ref>, type analysis [30], symbolic analysis for paral-lelization [15, 5], and the array data-flow analysis described above [14, 31].
References-found: 32

