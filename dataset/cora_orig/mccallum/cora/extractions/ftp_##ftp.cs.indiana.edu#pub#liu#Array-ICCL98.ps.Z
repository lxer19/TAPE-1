URL: ftp://ftp.cs.indiana.edu/pub/liu/Array-ICCL98.ps.Z
Refering-URL: http://www.cs.indiana.edu/~liu/papers/Array-ICCL98.html
Root-URL: http://www.cs.indiana.edu
Title: Loop optimization for aggregate array computations  
Author: Yanhong A. Liu and Scott D. Stoller 
Abstract: An aggregate array computation is a loop that computes accumulated quantities over array elements. Such computations are common in programs that use arrays, and the array elements involved in such computations often overlap, especially across iterations of loops, resulting in significant redundancy in the overall computation. This paper presents a method and algorithms that eliminate such overlapping aggregate array redundancies and shows both analytical and experimental performance improvements. The method is based on incrementalization, i.e., updating the values of aggregate array computations from iteration to iteration rather than computing them from scratch in each iteration. This involves maintaining additional information not maintained in the original program. We reduce various analysis problems to solving inequality constraints on loop variables and array subscripts, and we apply results from work on array data dependence analysis. Incrementalizing aggregate array computations produces drastic program speedup compared to previous optimizations. Previous methods for loop optimizations of arrays do not perform incremental-ization, and previous techniques for loop incremental-ization do not handle arrays. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. H. Allan, R. B. Jones, R. M. Lee, and S. J. Allan. </author> <title> Software pipelining. </title> <journal> ACM Comput. Surv., </journal> <volume> 27(3) </volume> <pages> 366-432, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: The programs were compiled using Sun Mi-crosystems' f77 compiler, with optimization flags -O4 and -fast. 6.1 Partial sum Partial sum is a simple but interesting and illustrative example. Given an array a [1::n] of numbers, for each index i (line <ref> [1] </ref>), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (18) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (18) It can be optimized using our algorithm. <p> Given an array a [1::n] of numbers, for each index i (line <ref> [1] </ref>), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (18) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (18) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. Now, consider the outer loop. Step 1. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (19). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (19). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (19). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (19). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining <ref> [1] </ref>, and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [2] <author> F. E. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In R. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers, </booktitle> <pages> pages 1-30. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1971. </year>
Reference-contexts: Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines <ref> [2] </ref> to [4]). The straightforward program (18) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (18) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion <ref> [2, 3, 11, 26, 58] </ref>, deforestation [57], and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [3] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1983. </year>
Reference-contexts: Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (18) takes O (n 2 ) time. [1] for i := 1 to n do <ref> [3] </ref> for j := 1 to i do (18) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. Now, consider the outer loop. Step 1. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion <ref> [2, 3, 11, 26, 58] </ref>, deforestation [57], and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [4] <author> J. Auslander, M. Philipose, C. Chambers, S. J. Eggers, and B. N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In PLDI 1996 [47], </booktitle> <pages> pages 149-159. </pages>
Reference-contexts: Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines [2] to <ref> [4] </ref>). The straightforward program (18) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (18) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. <p> These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35], run-time specialization and code generation [13, 36], and dynamic compilation and code generation <ref> [4, 16] </ref>, have been used in program optimizations and achieved certain large speedups. These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [5] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [18, 19, 41, 42, 50, 51, 52] and source to-source transformation <ref> [5, 34, 42, 49, 54] </ref> that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [5, 34, 42, 49, 54] </ref>, pipelining [1], and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [6] <author> F. L. Bauer, B. Moller, H. Partsch, and P. Pepper. </author> <title> Formal program construction by transformations|Computer-aided, </title> <journal> intuition-guided programming. IEEE Trans. Softw. Eng., </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [10, 33] can be used for optimization, as demonstrated in projects like CIP <ref> [6, 9, 46] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53].
Reference: [7] <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization.
Reference: [8] <author> M. Bromley, S. Heller, T. McNerney, and G. L. Steele Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 145-156. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs <ref> [8, 17] </ref>, etc. For example, in image processing, computing information about local neighborhoods is common [20, 32, 60, 62, 64, 65]. The local summation problem above is a simple but typical example [62, 64]. <p> In particular, we have used tools developed by Pugh's group [50, 51, 52]. Interestingly, ideas of incremental-ization are used for optimizations in serializing parallel programs <ref> [8, 17] </ref>. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods. Besides achieving optimizations not previously possible, our techniques fall out of one general approach, rather than simply being yet another new but ad hoc method.
Reference: [9] <author> M. Broy. </author> <title> Algebraic methods for program construction: The project CIP. </title> <editor> In P. Pepper, editor, </editor> <booktitle> Program Transformation and Programming Environments, </booktitle> <pages> pages 199-222. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [10, 33] can be used for optimization, as demonstrated in projects like CIP <ref> [6, 9, 46] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53].
Reference: [10] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> J. ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions. Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations <ref> [10, 33] </ref> can be used for optimization, as demonstrated in projects like CIP [6, 9, 46]. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations.
Reference: [11] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion <ref> [2, 3, 11, 26, 58] </ref>, deforestation [57], and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [12] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Commun. ACM, </journal> 20(11) 850-856, Nov. 1977. 
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> As expected, the running time for the optimized program is approximately independent of m. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine [27]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [12, 28, 56] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [15, 22, 23, 43, 45].
Reference: [13] <author> C. Consel and F. Noel. </author> <title> A general approach for run-time specialization and its application to C. </title> <note> In POPL 1996 [48]. </note>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35], run-time specialization and code generation <ref> [13, 36] </ref>, and dynamic compilation and code generation [4, 16], have been used in program optimizations and achieved certain large speedups. These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [14] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <booktitle> Prentice-Hall Series in Automatic Computation. </booktitle> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, N.J., </address> <year> 1976. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [14, 29, 30, 53] </ref>. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam [20, 21, 32], to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions.
Reference: [15] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> J. Comput. Lang., </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [15, 22, 23, 43, 45] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [63], which allows programs to be written using operations on bags, rather than sets.
Reference: [16] <author> D. R. Engler. </author> <title> VCODE: A retragetable, extensible, very fast dynamic code generation system. </title> <booktitle> In PLDI 1996 [47], </booktitle> <pages> pages 160-170. </pages>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35], run-time specialization and code generation [13, 36], and dynamic compilation and code generation <ref> [4, 16] </ref>, have been used in program optimizations and achieved certain large speedups. These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [17] <author> M. D. Ernst. </author> <title> Serializing parallel programs by removing redundant computation. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> August </month> <year> 1992, </year> <note> Revised August 1994. </note>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs <ref> [8, 17] </ref>, etc. For example, in image processing, computing information about local neighborhoods is common [20, 32, 60, 62, 64, 65]. The local summation problem above is a simple but typical example [62, 64]. <p> In particular, we have used tools developed by Pugh's group [50, 51, 52]. Interestingly, ideas of incremental-ization are used for optimizations in serializing parallel programs <ref> [8, 17] </ref>. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods. Besides achieving optimizations not previously possible, our techniques fall out of one general approach, rather than simply being yet another new but ad hoc method.
Reference: [18] <author> P. Feautrier. </author> <title> Parametric integer programming. </title> <journal> Opera tionnelle/Operations Research, </journal> <volume> 22(3) </volume> <pages> 243-268, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [19] <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1), </volume> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [20] <author> A. L. Fisher and P. T. Highnam. </author> <title> Communication and code optimization in SIMD programs. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64]. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53]. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [20, 21, 32] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [21] <author> A. L. Fisher, J. Leon, and P. T. Highnam. </author> <title> Design and performance of an optimizing SIMD compiler. </title> <booktitle> In Frontiers of Massively Parallel Computation, </booktitle> <year> 1990. </year>
Reference-contexts: Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53]. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [20, 21, 32] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [22] <author> A. C. Fong. </author> <title> Inductively computable constructs in very high level languages. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 21-28. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [22, 23, 44, 45] </ref>. Changes in hardware design have reduced the importance of strength reduction on arithmetic operations, but the ability to incremental-ize aggregate computations remains essential. Compared to work on parallelizing compilers, our method demonstrates a powerful alternative that is both orthogonal and correlated. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [15, 22, 23, 43, 45] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [63], which allows programs to be written using operations on bags, rather than sets.
Reference: [23] <author> A. C. Fong and J. D. Ullman. </author> <title> Inductive variables in very high level languages. </title> <booktitle> In Conference Record of the 3rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104-112. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1976. </year>
Reference-contexts: This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [22, 23, 44, 45] </ref>. Changes in hardware design have reduced the importance of strength reduction on arithmetic operations, but the ability to incremental-ize aggregate computations remains essential. Compared to work on parallelizing compilers, our method demonstrates a powerful alternative that is both orthogonal and correlated. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [15, 22, 23, 43, 45] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [63], which allows programs to be written using operations on bags, rather than sets.
Reference: [24] <author> O. I. Franksen. </author> <title> Mr. Babbage's Secret : The Tale of a Cypher and APL. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1985. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [24, 31, 61] </ref>. The same basic idea underlies techniques such as fusion [2, 3, 11, 26, 58], deforestation [57], and transformation of series expressions [59].
Reference: [25] <author> V. K. Garg and J. R. Mitchell. </author> <title> An efficient algorithm for detecting conjunctions of general global predicates. </title> <type> Technical Report TR-PDS-1996-005, </type> <institution> University of Texas at Austin, </institution> <year> 1996. </year>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection <ref> [25] </ref>, serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common [20, 32, 60, 62, 64, 65]. The local summation problem above is a simple but typical example [62, 64].
Reference: [26] <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 53-62. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion <ref> [2, 3, 11, 26, 58] </ref>, deforestation [57], and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [27] <author> H. H. Goldstine. </author> <title> Charles Babbage and his analytical engine. In The Computer from Pascal to von Neumann, </title> <booktitle> chapter 2, </booktitle> <pages> pages 10-26. </pages> <publisher> Princeton University Press, </publisher> <address> Prince-ton, New Jersey, </address> <year> 1972. </year>
Reference-contexts: Running times for programs (1) and (24) are shown in Figure 3. As expected, the running time for the optimized program is approximately independent of m. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine <ref> [27] </ref>. Strength reduction is the first realization of this idea in optimizing compilers [12, 28, 56]. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations.
Reference: [28] <author> A. A. Grau, U. Hill, and H. Langmaac. </author> <title> Translation of ALGOL 60, volume 1 of Handbook for automatic computation. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1967. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> As expected, the running time for the optimized program is approximately independent of m. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine [27]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [12, 28, 56] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [15, 22, 23, 43, 45].
Reference: [29] <editor> D. Gries. </editor> <booktitle> The Science of Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [14, 29, 30, 53] </ref>. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam [20, 21, 32], to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions.
Reference: [30] <author> D. Gries. </author> <title> A note on a standard strategy for developing loop invariants and loops. </title> <journal> Sci. Comput. Program., </journal> <volume> 2 </volume> <pages> 207-214, </pages> <year> 1984. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [14, 29, 30, 53] </ref>. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam [20, 21, 32], to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions.
Reference: [31] <author> L. Guibas and K. Wyatt. </author> <title> Compilation and delayed evaluation in APL. </title> <booktitle> In Conference Record of the 5th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-8. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1978. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [24, 31, 61] </ref>. The same basic idea underlies techniques such as fusion [2, 3, 11, 26, 58], deforestation [57], and transformation of series expressions [59].
Reference: [32] <author> P. T. Highnam. </author> <title> Systems and Programming Issues in the Design and Use of a SIMD Linear Array for Image Processing. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylva-nia, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64]. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53]. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [20, 21, 32] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [33] <author> S. Katz. </author> <title> Program optimization using invariants. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-4(5):378-389, </volume> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions. Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations <ref> [10, 33] </ref> can be used for optimization, as demonstrated in projects like CIP [6, 9, 46]. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations.
Reference: [34] <author> W. Kelly and W. Pugh. </author> <title> Finding legal reordering transformations using mappings. </title> <booktitle> In Proceedings of the 7th Annual Workshop on Programming Languages and Compilers for Parallel Computing, volume 892 of Lecture Notes in Computer Science, </booktitle> <address> Ithaca, New York, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [18, 19, 41, 42, 50, 51, 52] and source to-source transformation <ref> [5, 34, 42, 49, 54] </ref> that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [5, 34, 42, 49, 54] </ref>, pipelining [1], and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [35] <author> T. B. Knoblock and E. Ruf. </author> <title> Data specialization. </title> <note> In PLDI 1996 [47]. </note>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization <ref> [35] </ref>, run-time specialization and code generation [13, 36], and dynamic compilation and code generation [4, 16], have been used in program optimizations and achieved certain large speedups. These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [36] <author> M. Leone and P. Lee. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In PLDI 1996 [47], </booktitle> <pages> pages 137-148. </pages>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35], run-time specialization and code generation <ref> [13, 36] </ref>, and dynamic compilation and code generation [4, 16], have been used in program optimizations and achieved certain large speedups. These optimizations allow subcomputations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [37] <author> Y. A. Liu. </author> <title> Principled strength reduction. </title> <booktitle> In Proceedings of the IFIP TC2 Working Conference on Algorithmic Languages and Calculi. </booktitle> <publisher> Chapman & Hall, </publisher> <address> London, U.K., </address> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Methods of explicit incrementalization [40], cache and-prune [39], and use of auxiliary information [38] were first formulated for a functional language. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction <ref> [37] </ref>. This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL [22, 23, 44, 45].
Reference: [38] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In POPL 1996 [48], </booktitle> <pages> pages 157-170. </pages>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization [40], cache and-prune [39], and use of auxiliary information <ref> [38] </ref> were first formulated for a functional language. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [37]. This paper extends that work to handle imperative programs that use arrays. <p> l] 6 s [i] := s [i1]; for l := n 2 1 downto 0 do s [i] := s [i] a [i1; l]; s [i] := s [i] + a [i1 + m; l] 4 Maintaining additional information Additional information often needs to be maintained for efficient incremental computation <ref> [38, 39] </ref>. Such information often comes from intermediate results computed in the middle of the original computation [39]. It may also come from auxiliary information that is not computed at all in the original computation [38]. The central issues are how to find, use, and maintain appropriate information. <p> Such information often comes from intermediate results computed in the middle of the original computation [39]. It may also come from auxiliary information that is not computed at all in the original computation <ref> [38] </ref>. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [38, 39]. Here we apply them to AACs, using a variant of the cache-and-prune method [39]. <p> It may also come from auxiliary information that is not computed at all in the original computation [38]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language <ref> [38, 39] </ref>. Here we apply them to AACs, using a variant of the cache-and-prune method [39].
Reference: [39] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Static caching for incremental computation. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 20(2), </volume> <month> March </month> <year> 1998. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization [40], cache and-prune <ref> [39] </ref>, and use of auxiliary information [38] were first formulated for a functional language. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [37]. This paper extends that work to handle imperative programs that use arrays. <p> l] 6 s [i] := s [i1]; for l := n 2 1 downto 0 do s [i] := s [i] a [i1; l]; s [i] := s [i] + a [i1 + m; l] 4 Maintaining additional information Additional information often needs to be maintained for efficient incremental computation <ref> [38, 39] </ref>. Such information often comes from intermediate results computed in the middle of the original computation [39]. It may also come from auxiliary information that is not computed at all in the original computation [38]. The central issues are how to find, use, and maintain appropriate information. <p> Such information often comes from intermediate results computed in the middle of the original computation <ref> [39] </ref>. It may also come from auxiliary information that is not computed at all in the original computation [38]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [38, 39]. <p> It may also come from auxiliary information that is not computed at all in the original computation [38]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language <ref> [38, 39] </ref>. Here we apply them to AACs, using a variant of the cache-and-prune method [39]. <p> The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [38, 39]. Here we apply them to AACs, using a variant of the cache-and-prune method <ref> [39] </ref>. <p> Stage III analyzes dependencies in the incremental-ized computation and prunes out useless information and the associated computations. The analysis starts with the uses of such information in computing the original accumulating variables and follows dependencies back to the definitions of such information. The dependencies are transitive <ref> [39] </ref> and can be used to compute all the information that is useful. Pruning then eliminates useless data and code, saving both space and time. 4.4 Forming incrementalized loops The incrementalized loop is formed as in Section 3.3, but using the AACs that have been extended with useful additional information.
Reference: [40] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <journal> Sci. Comput. Program., </journal> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization <ref> [40] </ref>, cache and-prune [39], and use of auxiliary information [38] were first formulated for a functional language. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [37]. This paper extends that work to handle imperative programs that use arrays.
Reference: [41] <author> V. Maslov. </author> <title> Lazy array data-flow dependence analysis. </title> <booktitle> In Conference Record of the 21st Annual ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <address> New York, </address> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [42] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <address> New York, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [18, 19, 41, 42, 50, 51, 52] and source to-source transformation <ref> [5, 34, 42, 49, 54] </ref> that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [5, 34, 42, 49, 54] </ref>, pipelining [1], and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [43] <author> R. Paige. </author> <title> Transformational programming|Applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 73-87. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1983. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [15, 22, 23, 43, 45] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [63], which allows programs to be written using operations on bags, rather than sets.
Reference: [44] <author> R. Paige. </author> <title> Symbolic finite differencing|Part I. </title> <booktitle> In Proceedings of the 3rd European Symposium on Programming, volume 432 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36-56. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [22, 23, 44, 45] </ref>. Changes in hardware design have reduced the importance of strength reduction on arithmetic operations, but the ability to incremental-ize aggregate computations remains essential. Compared to work on parallelizing compilers, our method demonstrates a powerful alternative that is both orthogonal and correlated.
Reference: [45] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [22, 23, 44, 45] </ref>. Changes in hardware design have reduced the importance of strength reduction on arithmetic operations, but the ability to incremental-ize aggregate computations remains essential. Compared to work on parallelizing compilers, our method demonstrates a powerful alternative that is both orthogonal and correlated. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [15, 22, 23, 43, 45] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [63], which allows programs to be written using operations on bags, rather than sets.
Reference: [46] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [10, 33] can be used for optimization, as demonstrated in projects like CIP <ref> [6, 9, 46] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [14, 29, 30, 53].
Reference: [47] <institution> Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation. ACM, </institution> <address> New York, </address> <month> May </month> <year> 1996. </year>
Reference: [48] <institution> Conference Record of the 23rd Annual ACM Symposium on Principles of Programming Languages. ACM, </institution> <address> New York, </address> <month> Jan. </month> <year> 1996. </year>
Reference: [49] <author> W. Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [18, 19, 41, 42, 50, 51, 52] and source to-source transformation <ref> [5, 34, 42, 49, 54] </ref> that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [5, 34, 42, 49, 54] </ref>, pipelining [1], and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [50] <author> W. Pugh. </author> <title> The Omega Test: A fast and practical integer programming algorithm for dependence analysis. </title> <journal> Com-mun. ACM, </journal> <volume> 31(8), </volume> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> 1 + k; l] and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use the methods and tools developed by Pugh et al. in the Omega project <ref> [50, 51, 52] </ref> to simplify the constraints. <p> Simplify the constraints in S using Omega <ref> [50, 51, 52] </ref>. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [50, 51, 52] </ref>. Interestingly, ideas of incremental-ization are used for optimizations in serializing parallel programs [8, 17]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [51] <author> W. Pugh and D. Wonnacott. </author> <title> Going beyond integer pro-ramming with the omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> Dec. </month> <year> 1992. </year> <note> An earlier version of this paper appeared at the ACM SIGPLAN '92 Conference on PLDI. </note>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> 1 + k; l] and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use the methods and tools developed by Pugh et al. in the Omega project <ref> [50, 51, 52] </ref> to simplify the constraints. <p> Simplify the constraints in S using Omega <ref> [50, 51, 52] </ref>. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [50, 51, 52] </ref>. Interestingly, ideas of incremental-ization are used for optimizations in serializing parallel programs [8, 17]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [52] <author> W. Pugh and D. Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Proceedings of the 6th Annual Workshop on Programming Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <address> Portland, Oregon, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> and source to-source transformation [5, 34, 42, 49, 54] that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> 1 + k; l] and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use the methods and tools developed by Pugh et al. in the Omega project <ref> [50, 51, 52] </ref> to simplify the constraints. <p> Simplify the constraints in S using Omega <ref> [50, 51, 52] </ref>. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [5, 34, 42, 49, 54], pipelining [1], and array data dependence analysis <ref> [18, 19, 41, 42, 50, 51, 52] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [50, 51, 52] </ref>. Interestingly, ideas of incremental-ization are used for optimizations in serializing parallel programs [8, 17]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [53] <author> J. C. Reynolds. </author> <title> The Craft of Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [14, 29, 30, 53] </ref>. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam [20, 21, 32], to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputa-tions.
Reference: [54] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [18, 19, 41, 42, 50, 51, 52] and source to-source transformation <ref> [5, 34, 42, 49, 54] </ref> that were developed for parallelizing compilers. Comparing the straightforward program (1) with the optimized program (24) on page 8, one can see that performing the optimizations by hand is tedious and error-prone. <p> Also, programs must be written using direc-tionals to take advantage of their optimizations; this is inconvenient when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [5, 34, 42, 49, 54] </ref>, pipelining [1], and array data dependence analysis [18, 19, 41, 42, 50, 51, 52] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [55] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program develop ment system. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization.
Reference: [56] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 4th International Joint Conference on TAPSOFT, volume 494 of Lecture Notes in Computer Science, </booktitle> <pages> pages 394-415. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: As expected, the running time for the optimized program is approximately independent of m. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine [27]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [12, 28, 56] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [15, 22, 23, 43, 45].
Reference: [57] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <booktitle> In Proceedings of the 2nd European Symposium on Programming, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 344-358. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion [2, 3, 11, 26, 58], deforestation <ref> [57] </ref>, and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [58] <author> J. Warren. </author> <title> A hierarchical basis for reordering transformations. </title> <booktitle> In Conference Record of the 11th Annual ACM Symposium on POPL, </booktitle> <pages> pages 272-282. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [24, 31, 61]. The same basic idea underlies techniques such as fusion <ref> [2, 3, 11, 26, 58] </ref>, deforestation [57], and transformation of series expressions [59]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [59] <author> R. C. Waters. </author> <title> Automatic transformation of series expressions into loops. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 13(1) </volume> <pages> 52-98, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: The same basic idea underlies techniques such as fusion [2, 3, 11, 26, 58], deforestation [57], and transformation of series expressions <ref> [59] </ref>. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [60] <author> J. A. Webb. </author> <title> Steps towards architecture-independent image processing. </title> <journal> IEEE Computer, </journal> <volume> 25(2) </volume> <pages> 21-31, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64].
Reference: [61] <author> B. Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [24, 31, 61] </ref>. The same basic idea underlies techniques such as fusion [2, 3, 11, 26, 58], deforestation [57], and transformation of series expressions [59].
Reference: [62] <author> W. M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64]. <p> For example, in image processing, computing information about local neighborhoods is common [20, 32, 60, 62, 64, 65]. The local summation problem above is a simple but typical example <ref> [62, 64] </ref>. Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking.
Reference: [63] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Trans. Program. Lang. and Syst., </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking. Optimizations similar to incrementalization have been studied for various language features <ref> [7, 12, 28, 38, 39, 40, 43, 45, 44, 55, 63] </ref>, but no systematic technique han dles aggregate computations on arrays. At the same time, many optimizations have been studied for arrays [1, 2, 3, 5, 24, 26, 31, 34, 42, 49, 54, 58], but none of them achieves incrementalization. <p> Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [15, 22, 23, 43, 45]. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC <ref> [63] </ref>, which allows programs to be written using operations on bags, rather than sets.
Reference: [64] <author> R. Zabih. </author> <title> Individuating Unknown Objects by Combining Motion and Stereo. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64]. <p> For example, in image processing, computing information about local neighborhoods is common [20, 32, 60, 62, 64, 65]. The local summation problem above is a simple but typical example <ref> [62, 64] </ref>. Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking.
Reference: [65] <author> R. Zabih and J. Woodfill. </author> <title> Non-parametric local transforms for computing visual correspondence. </title> <editor> In J.-O. Eklundh, editor, </editor> <booktitle> Proceedings of the 3rd European Conference on Computer Vision, volume 801 of Lecture Notes in Computer Science, </booktitle> <pages> pages 151-158. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [25], serializing parallel programs [8, 17], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [20, 32, 60, 62, 64, 65] </ref>. The local summation problem above is a simple but typical example [62, 64].
References-found: 65

