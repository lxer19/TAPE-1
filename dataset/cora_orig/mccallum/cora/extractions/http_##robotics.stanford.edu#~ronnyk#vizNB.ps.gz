URL: http://robotics.stanford.edu/~ronnyk/vizNB.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fbecker,ronnyk,sommdag@engr.sgi.com  
Title: Issues in the Integration of Data Mining and Data Visualization Visualizing the Simple Bayesian Classifier  
Author: Barry Becker Ron Kohavi Dan Sommerfield 
Keyword: Classification, simple/naive-Bayes, visualization.  
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  
Note: Appears in the KDD 1997 Workshop on  
Abstract: The simple Bayesian classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class. The model was previously shown to be surprisingly robust to obvious violations of this independence assumption, yielding accurate classification models even when there are clear conditional dependencies. The SBC can serve as an excellent tool for initial exploratory data analysis when coupled with a visualizer that makes its structure comprehensible. We describe such a visual representation of the SBC model that has been successfully implemented. We describe the requirements we had for such a visualization and the design decisions we made to satisfy them. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cover, T. M. & Thomas, J. A. </author> <year> (1991), </year> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The pie's height is proportional to this number. Pointing to the items in the legend on the right pane, shows the numerical probabilities corresponding to the slice size. 3 order of importance computed as the conditional entropy <ref> (Cover & Thomas 1991) </ref> of each attribute and the label. The left pane will switch to a bars representation of evidence for a label once a specific label is selected. <p> The additive evidence can be interpreted as the information content in bits <ref> (Cover & Thomas 1991) </ref>. High evidence values will increase the class posterior probability more. This evidence can be summed in order to determine which class is being predicted by the model (unlike probabilities, which must be multiplied). This is analogous to a race between runners, each representing a class. <p> To allow the user to see datasets with hundreds of attributes, we compute the importance of each attribute and display them in the scene ordered by this measure. Technically, the importance value is conditional entropy <ref> (Cover & Thomas 1991) </ref> of each attribute and the label. Attribute with low conditional entropy have little effect on the posterior probability. A slider button on the bottom right (see Figure 1) allows users to remove lower-ranked attributes from the visualization.
Reference: <author> Domingos, P. & Pazzani, M. </author> <year> (1996), </year> <title> Beyond independence: conditions for the optimality of the simple Bayesian classifier, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 105-112. </pages>
Reference-contexts: The probabilities in the above formulas must be estimated from the training set. This model is very robust and continues to perform well even in the face of obvious violations of this independence assumption <ref> (Domingos & Pazzani 1996, Kohavi & Sommerfield 1995) </ref>. We begin with a discussion of our motivation and requirements for the SBC visualization.
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: Simple models are especially useful if the model is to be understood by non-experts in machine learning. The simple Bayesian classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class <ref> (Good 1965, Duda & Hart 1973, Langley, Iba & Thompson 1992) </ref>.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in `The First International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 192-197. </pages>
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of Bayesian classifiers, </title> <booktitle> in `Proceedings of the tenth national conference on artificial intelligence', </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference: <author> Madigan, D., Mosurski, K. & Almond, R. G. </author> <year> (1997), </year> <title> `Graphical explanation in belief networks', </title> <journal> J. Comp. and Graphical Statistics p. </journal> <note> to appear. </note>
Reference: <author> Spiegelhalter, D. J. & Knill-Jones, R. P. </author> <year> (1984), </year> <title> `Statistical and knowledge-based approaches to clinical decision-support systems, with an application in gastroenterology', </title> <journal> Journal of the Royal Statistical Society A 147, </journal> <pages> 35-77. </pages>
Reference-contexts: The alternate representation, using bars, is more useful when the user is interested in properties of a specific class, and its use during interactive classification closely parallels the use of an evidence balance sheet to explain a result <ref> (Spiegelhalter & Knill-Jones 1984) </ref>. The use of log-probabilities for the bars coupled with subtracting the minimum bar height from all bars makes this representation ideal for understanding the effect each attribute value has on the final prediction.
Reference: <author> Tufte, E. R. </author> <year> (1983), </year> <title> The Visual Display of Quantitative Information, </title> <publisher> Graphics Press, </publisher> <address> Cheschire, CT. </address> <month> 9 </month>
Reference-contexts: Because the pie charts are of the same size and laid out on a line, they are similar to charts used by Consumer Reports to represent product quality through a set of circles filled with varying amounts of green and black to signify good and bad aspects respectively. Tufte <ref> (Tufte 1983, p. 174) </ref> lauded this approach as "a particularly ingenious mix of table and graphic." The use of the third dimension allows us to show the number of records underlying a particular distribution as the height (z coordinate) of the pie chart.
References-found: 9

