URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-621.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: Investigation of a Preemptive Network Architecture  
Author: by Christopher James Lefelhocz 
Degree: Submitted to the Department of Electrical Engineering and Computer Science In Partial Fulfillment of the Requirements for the Degree of Master of Science in Electrical Engineering and Computer Science at the  
Address: Cambridge, Massachusetts 02139  
Note: c Massachusetts Institute of Technology,  
Date: May, 1994  1994  
Affiliation: Massachusetts Institute of Technology  Massachusetts Institute of Technology Laboratory for Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dimitri Bertsekas and Robert Gallager. </author> <title> Data Networks, Second Edition, chapter Three. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: This differs from my work which is to explore the trade-off of required processing power versus the performance of a system. A number of sources exist which cover priority queueing. The preemptive architecture described above is classified as a preemptive resume queueing strategy. Both <ref> [1] </ref> and [10] cover how a M/G/1 system performs with a preemptive resume strategy of two priority classes. Our system is slightly more complex than either of the M/G/1 models presented. This work may be used to help in attempting to characterize the architecture performance in a closed form.
Reference: [2] <author> Herwig Bruneel and Byung G. Kim. </author> <title> Discrete-Time Models for Communication Systems Including ATM, chapter Two. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: ATM uses the small cell architecture with each cell being 53 bytes (currently) in length. ATM can be considered a preemptive resume system if the correct algorithm is used to decide how cells are sent. Such a scheme is discussed in <ref> [2] </ref> where a discrete model of preemption is presented. ATM is not used in this research for two reasons. The first is that our characterizations 13 can be more general and not restricted to ATM.
Reference: [3] <author> David D. Clark and David L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In ACM SIGCOMM'90 Symposium, </booktitle> <year> 1990. </year>
Reference-contexts: This transfer unit is built out of one or more NDUs. We call this transfer unit an Application Data Unit (ADU). 24 2.3.2 Application Data Units An ADU is defined in <ref> [3] </ref> as a "suitable aggregate of data from an application for which frame boundaries are preserved by the lower levels of the network." For our purposes we define it as the data supplied to the network by some source for transmission and is expected as the same unit by some sink.
Reference: [4] <author> M. W. Garrett and M. Vetterli. </author> <title> Congestion control strategies for packet video. </title> <booktitle> In Fourth International Workshop on Packet Video, </booktitle> <address> Kyoto, Japan August 1991, </address> <year> 1991. </year>
Reference-contexts: These approximations were seen in the simulations run. Note that the acknowledgment ADUs are processed by the same processor complex even though they are being sent down a different link. To generate the VBR video source traffic, <ref> [4] </ref> was used. The coding is of the movie "Star Wars" and was nicknamed as such. A set of data files representing the ADU size for each frame in the picture were read and then sent through the network.
Reference: [5] <author> J. N. Giacopelli, W. D. Sincoskie, and M. Littlewood. Sunshine: </author> <title> A high performance self routing broadband packet switch architecture. </title> <booktitle> In Proc. of the Inernational Switching Symposium, </booktitle> <year> 1990. </year>
Reference-contexts: The processor complex is the switch processing power since it is an array of processors. The ADU data store has two functions: 1) storage of blocked ADUs and 2)switching the ADUs for transmission. The expected complexity for the memory and switching fabric is similar to those presented in <ref> [5] </ref>, [7], and [11] since that is their primary function. Thus we should examine the destructor and constructor more closely to understand their complexities.
Reference: [6] <author> V. Jacobson. </author> <title> Congestion avoidance and control. </title> <booktitle> In ACM SIGCOMM '88 Symposium, </booktitle> <year> 1988. </year>
Reference-contexts: The acknowledgment ADUs flow from the sinks to the sources. The priority of the acknowledgment ADUs is low. The VBR video sources are clearly a real-time traffic source. These sources received high priority in the network. The TCP sources use a simple TCP model containing slow-start <ref> [6] </ref>. The ADU size is 500 bytes. Ideally, the number of acknowledgment ADUs sent would be one acknowledgment per round trip time. However, in this model an almost one to one mapping occurred.
Reference: [7] <author> Tony T. Lee. </author> <title> A modular architecture for very large packet switches. </title> <journal> In IEEE Transactions on Communications, </journal> <year> 1990. </year>
Reference-contexts: The processor complex is the switch processing power since it is an array of processors. The ADU data store has two functions: 1) storage of blocked ADUs and 2)switching the ADUs for transmission. The expected complexity for the memory and switching fabric is similar to those presented in [5], <ref> [7] </ref>, and [11] since that is their primary function. Thus we should examine the destructor and constructor more closely to understand their complexities.
Reference: [8] <author> Jorg Liebeherr, Ian F. Akyildiz, and N. Tantawi Asser. </author> <title> An effective scheme for pre-emptive priorities in dual bus metropolitan area networks. </title> <type> Technical report, </type> <institution> IBM Research Division, </institution> <year> 1992. </year> <month> 92 </month>
Reference-contexts: Another area where a preemptive approach has been used is in the shared medium network. These approaches are used to gain better performance for the expected delay of real-time traffic. The two mediums are a slotted ring [9] and distributed queue dual bus <ref> [8] </ref>. Since this is a shared medium, there is no required processing power unless we estimate the processing power necessary at each connection point. This work is important since it may be an input to an actual implementation.
Reference: [9] <author> Sarit Mukherjee, Debanjan Saha, and Satish K Tripathi. </author> <title> Performance evalua-tion of a preemptive protocol for voice-data integration in ring-based lan/man. </title> <type> Technical report, </type> <institution> University of Maryland, College Park, MD, </institution> <year> 1993. </year>
Reference-contexts: Another area where a preemptive approach has been used is in the shared medium network. These approaches are used to gain better performance for the expected delay of real-time traffic. The two mediums are a slotted ring <ref> [9] </ref> and distributed queue dual bus [8]. Since this is a shared medium, there is no required processing power unless we estimate the processing power necessary at each connection point. This work is important since it may be an input to an actual implementation.
Reference: [10] <author> Ronald W. Wolff. </author> <title> Stochastic Modeling and the Theory of Queues. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: This differs from my work which is to explore the trade-off of required processing power versus the performance of a system. A number of sources exist which cover priority queueing. The preemptive architecture described above is classified as a preemptive resume queueing strategy. Both [1] and <ref> [10] </ref> cover how a M/G/1 system performs with a preemptive resume strategy of two priority classes. Our system is slightly more complex than either of the M/G/1 models presented. This work may be used to help in attempting to characterize the architecture performance in a closed form.
Reference: [11] <author> Y. S. Yeh, M. G. Hluchyj, and A. S. Acampora. </author> <title> The knockout switch: A simple modular architecture for high performance packet switching. </title> <booktitle> In Proc. of the International Switching Symposium, </booktitle> <year> 1987. </year>
Reference-contexts: The ADU data store has two functions: 1) storage of blocked ADUs and 2)switching the ADUs for transmission. The expected complexity for the memory and switching fabric is similar to those presented in [5], [7], and <ref> [11] </ref> since that is their primary function. Thus we should examine the destructor and constructor more closely to understand their complexities.
Reference: [12] <author> Lixia Zhang, Scott Shenker, and David D. Clark. </author> <title> Observations on the dynamics of a congestion control algorithm: The effects of two-way traffic. </title> <booktitle> In SIGCOMM '91 Conference on Communications, Architectures, and Protocols, </booktitle> <year> 1991. </year> <month> 93 </month>
Reference-contexts: Figure 4-13 shows this phenomenon when the small ADU priority is low. By compress we mean to say that the small ADUs will bundle together and blast the link at once. This is similar to the effect seen with TCP and ACK-compression in <ref> [12] </ref>. At the next switch, the compression causes an overload in the amount of ADUs to be processed in a short interval of time.
References-found: 12

