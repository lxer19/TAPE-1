URL: http://www.cs.ucsd.edu/classes/wi98/cse255/naivebayes.ps
Refering-URL: http://www.cs.ucsd.edu/classes/wi98/cse255/
Root-URL: http://www.cs.ucsd.edu
Email: elkan@cs.ucsd.edu  
Title: NAIVE BAYESIAN LEARNING  Adapted from  
Author: Charles Elkan 
Note: First version  
Date: September 1997.  May 1997.  
Address: La Jolla, California 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Pubnum: Technical Report No. CS97-557,  
Abstract-found: 0
Intro-found: 1
Reference: [ Domingos and Pazzani, 1996 ] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 105112. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference-contexts: Although this assumption is almost always violated in practice, recent work has shown that naive Bayesian learning is remarkably effective in practice and difficult to improve upon systematically <ref> [ Domingos and Pazzani, 1996 ] </ref> . On many real-world example datasets naive Bayesian learning gives better test set accuracy than any other known method, including backpropagation and C4.5 decision trees. Also, these classifiers can be learned very efficiently.
Reference: [ Dougherty et al., 1995 ] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, pages 194202. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The simplest quantization method is to divide the range of the attribute into a fixed number M of equal-width bins. In the experiments described below M = 10 is chosen arbitrarily. Previous experimental work has shown that the benefits of more complex quantization methods are small at best <ref> [ Dougherty et al., 1995 ] </ref> . Using bins of equal width tends to work well because they allow good non-parametric approximation of skewed, multimodal, and/or heavy-tailed probability distributions. Let each A j be a numerical (integer-valued or real-valued) attribute.
Reference: [ Minsky and Papert, 1969 ] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons; an introduction to computational geometry. </title> <publisher> MIT Press, </publisher> <year> 1969. </year> <month> 4 </month>
References-found: 3

