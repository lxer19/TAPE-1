URL: http://suif.stanford.edu/papers/scales94.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Email: fscales,lamg@cs.stanford.edu  
Title: The Design and Evaluation of a Shared Object System for Distributed Memory Machines  
Author: Daniel J. Scales and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Note: To Appear in the First Symposium on Operating Systems Design and Implementation  
Abstract: This paper describes the design and evaluation of SAM, a shared object system for distributed memory machines. SAM is a portable run-time system that provides a global name space and automatic caching of shared data. SAM incorporates mechanisms to address the problem of high communication overheads on distributed memory machines; these mechanisms include tying synchronization to data access, chaotic access to data, prefetching of data, and pushing of data to remote processors. SAM has been implemented on the CM-5, Intel iPSC/860 and Paragon, IBM SP1, and networks of workstations running PVM. SAM applications run on all these platforms without modification. This paper provides an extensive analysis of several complex scientific algorithms written in SAM on a variety of hardware platforms. We find that the performance of these SAM applications depends fundamentally on the scalability of the underlying parallel algorithm, and whether the algorithm's communication requirements can be satisfied by the hardware. Our experience suggests that SAM is successful in allowing programmers to use distributed memory machines effectively with much less programming effort than required today. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber [11], Prelude [28], Orca <ref> [1] </ref>, Midway [3], and Linda [5, 7] communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages.
Reference: [2] <author> J. E. Barnes and P. Hut. </author> <title> A Hierarchical O(NlogN) Force-Calculation Algorithm. </title> <journal> Nature, </journal> <volume> 324(6096) </volume> <pages> 446-449, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: limits, but at much larger numbers of processors. 3 On the CM-5, we do not use the four vector units at each node to enhance floating-point performance, because significant additional programming is required to utilize them. (a) (right) for (a) BCSSTK15 and (b) D1000 4.2 Barnes-Hut Algorithm The Barnes-Hut algorithm <ref> [2] </ref> is a fast algorithm for simulating the evolution of a system of astronomical bodies as they interact with each other via the gravitational force (the n-body problem).
Reference: [3] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In COMPCON 1993, </booktitle> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber [11], Prelude [28], Orca [1], Midway <ref> [3] </ref>, and Linda [5, 7] communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages.
Reference: [4] <author> R. Bisiani and A. Forin. </author> <title> Multilanguage Parallel Programming of Heterogeneous Machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 930-945, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Using a local, possibly older copy is sometimes sufficient and can reduce the total execution time of these algorithms. A number of systems have supported the notion of chaotic access. Agora <ref> [4] </ref> supports a memory model in which all accesses are chaotic, since all modifications to shared data structures are allowed to complete before holders of cached copies have been notified.
Reference: [5] <author> R. D. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber [11], Prelude [28], Orca [1], Midway [3], and Linda <ref> [5, 7] </ref> communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages. Amber and Prelude primarily provide access to shared objects by moving tasks to the processor containing the data.
Reference: [6] <author> B. </author> <title> Buchberger. Grobner Basis: an Algorithmic Method in Polynomial Ideal Theory. </title> <editor> In N. K. Bose, editor, </editor> <booktitle> Multidimensional Systems Theory, chapter 6, </booktitle> <pages> pages 184-232. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: The SAM versions on the iPSC/860 and SP1 have the lowest speedups, because of the high cost of sending and receiving messages on these machines. 4.3 Grobner Basis Algorithm We have used SAM to parallelize an important algorithm from symbolic algebra systems. The algorithm computes the Grobner basis <ref> [6] </ref> of a set of polynomials, which is used to solve systems of non-linear equations and determine implicit forms for parametric equations. This algorithm has been previously implemented on the CM-5 by Chakrabarti [8].
Reference: [7] <author> N. Carriero. </author> <title> Implementation of Tuple Space Machines. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber [11], Prelude [28], Orca [1], Midway [3], and Linda <ref> [5, 7] </ref> communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages. Amber and Prelude primarily provide access to shared objects by moving tasks to the processor containing the data. <p> Sophisticated compiler analysis is typically required to analyze and optimize the communication in a Linda program. We do not know of a distributed-memory Linda implementation that provides dynamic caching of tuples (except for one which broadcasts all tuples to all processors <ref> [7] </ref>). 2.2 Design of SAM SAM takes one step further than Orca and Midway in providing user control over communication on a distributed memory machine. SAM also communicates data at the level of user-defined data types.
Reference: [8] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an Irregular Application on a Distributed Memory Multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices and Parallel Programming, </booktitle> <pages> pages 169-179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Both the SAM and DASH versions of the Barnes-Hut application are derived from the original serial code. Warren and Salmon's message-passing Barnes-Hut code [27] is a completely separate code not based at all on the serial Barnes-Hut code. Chakrabarti's message-passing version of the Grobner basis algorithm for the CM-5 <ref> [8] </ref> is based on the original serial code, as is the SAM version. The line counts indicate the size of the applications and give a rough comparison of the difficulty in programming the different versions, which we will address further below. <p> The algorithm computes the Grobner basis [6] of a set of polynomials, which is used to solve systems of non-linear equations and determine implicit forms for parametric equations. This algorithm has been previously implemented on the CM-5 by Chakrabarti <ref> [8] </ref>. The basic structure of the algorithm is to start with the initial basis equal to the input set of polynomials. <p> This effect increases with larger numbers of processors and with longer communication latencies. Our implementation of the algorithm appears to have better heuristics for setting the priority of tasks than the Chakrabarti's message-passing implementation for the CM-5 <ref> [8] </ref>. In consequence, we have better absolute serial and parallel run times on the majority of the available polynomial benchmarks. Our uniprocessor and 10-processor times are better than those of the message-passing implementation for eight of the nine input sets whose timings are reported in [8]. <p> message-passing implementation for the CM-5 <ref> [8] </ref>. In consequence, we have better absolute serial and parallel run times on the majority of the available polynomial benchmarks. Our uniprocessor and 10-processor times are better than those of the message-passing implementation for eight of the nine input sets whose timings are reported in [8]. For one input set, our 10-processor time is substantially slower than our 1-processor time, because our task-ordering heuristic happens not to work well for that input set. Our speedups are comparable to those in [8] for several of the benchmarks, but smaller for the other benchmarks because of our faster <p> the message-passing implementation for eight of the nine input sets whose timings are reported in <ref> [8] </ref>. For one input set, our 10-processor time is substantially slower than our 1-processor time, because our task-ordering heuristic happens not to work well for that input set. Our speedups are comparable to those in [8] for several of the benchmarks, but smaller for the other benchmarks because of our faster uniprocessor times. As shown in Figure 2, the line count for the SAM version is not much larger than the line count of the serial version. <p> Many recent scientific algorithms use complex data structures and dynamic task placement and can benefit substantially from dynamic caching of remote data. Often, such caching functionality is implemented in an application-specific way in message-passing programs (for example, in <ref> [8] </ref>), at the cost of much programmer effort.
Reference: [9] <author> K. M. Chandy and C. Kesselman. </author> <title> Composition C++: Compositional Parallel Programming. </title> <booktitle> In Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124-144, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The concept of single-assignment values have been used in a variety of parallel languages <ref> [10, 9, 13, 14, 20] </ref> as a way of exposing parallelism and synchronizing independent tasks. Such systems often must deal with problems of excessive memory usage and data copying as shared data items are created and modified.
Reference: [10] <author> K. M. Chandy and S. Taylor. </author> <title> The Composition of Concurrent Programs. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, Nevada, </address> <month> Nov. </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: The concept of single-assignment values have been used in a variety of parallel languages <ref> [10, 9, 13, 14, 20] </ref> as a way of exposing parallelism and synchronizing independent tasks. Such systems often must deal with problems of excessive memory usage and data copying as shared data items are created and modified.
Reference: [11] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems, </booktitle> <pages> pages 147-158, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber <ref> [11] </ref>, Prelude [28], Orca [1], Midway [3], and Linda [5, 7] communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages.
Reference: [12] <author> A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Raja-mony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In addition, Treadmarks attempts to alleviate the problems of excess consistency messages by implementing lazy release consistency, in which the modifications or invalidations to data are not sent until a processor acquires a lock that requires that it see those modifications. Cox et al. <ref> [12] </ref> compare the performance of several applications on a network of eight workstations running Treadmarks and on a hardware shared-memory multiprocessor.
Reference: [13] <author> M. J. Feeley and H. M. Levy. </author> <title> Distributed Shared Memory with Versioned Objects. </title> <booktitle> In 1992 ACM SIG-PLAN Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The concept of single-assignment values have been used in a variety of parallel languages <ref> [10, 9, 13, 14, 20] </ref> as a way of exposing parallelism and synchronizing independent tasks. Such systems often must deal with problems of excessive memory usage and data copying as shared data items are created and modified.
Reference: [14] <author> J. T. Feo, D. C. Cann, and R. R. Oldeheoft. </author> <title> A Report on the SISAL Language Project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: The concept of single-assignment values have been used in a variety of parallel languages <ref> [10, 9, 13, 14, 20] </ref> as a way of exposing parallelism and synchronizing independent tasks. Such systems often must deal with problems of excessive memory usage and data copying as shared data items are created and modified.
Reference: [15] <author> J.B. Carter and J.K. Bennett and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Processes use lock and unlock operations to synchronize with each others' data accesses. The system must guarantee that all memory updates preceding a synchronization operation are observable by all processors when the synchronization completes, though most updates may be relevant to only a few processors. Munin <ref> [15] </ref> and Treadmarks [16] address the problem of false sharing by allowing multiple processors to write to a page and merging changes at the next synchronization point.
Reference: [16] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Processes use lock and unlock operations to synchronize with each others' data accesses. The system must guarantee that all memory updates preceding a synchronization operation are observable by all processors when the synchronization completes, though most updates may be relevant to only a few processors. Munin [15] and Treadmarks <ref> [16] </ref> address the problem of false sharing by allowing multiple processors to write to a page and merging changes at the next synchronization point.
Reference: [17] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: to the basis until no further polynomials need to be added. serial SAM DASH msg-pass. code code code code Block Cholesky NA 6713 6813 NA Barnes-Hut 1959 2896 2232 3973 Grobner Basis 3757 4082 NA 5747 Cholesky application is derived from the original code written for the DASH shared-memory multiprocessor <ref> [17] </ref>. Both the SAM and DASH versions of the Barnes-Hut application are derived from the original serial code. Warren and Salmon's message-passing Barnes-Hut code [27] is a completely separate code not based at all on the serial Barnes-Hut code.
Reference: [18] <author> K. Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages II 94-101, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Shared virtual memory (SVM) systems such as Ivy <ref> [18] </ref> apply the concepts of hardware caching to virtual memory pages. Such systems transparently support the view of a single address space across processors.
Reference: [19] <author> R. G. Minnich and D. J. Farber. </author> <title> Reducing Host Load, Network Load, and Latency in a Distributed Shared Memory. </title> <booktitle> In Tenth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 468-475, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A number of systems have supported the notion of chaotic access. Agora [4] supports a memory model in which all accesses are chaotic, since all modifications to shared data structures are allowed to complete before holders of cached copies have been notified. Mether <ref> [19] </ref> and Clouds [22] support operations for accessing a read-only copy of a page (or segment) that will not be kept coherent even if the contents of the page (or segment) are changed by another processor. Finally, besides minimizing communication, SAM also provides primitives for optimizing communication.
Reference: [20] <author> R. S. Nikhil. </author> <title> The Parallel Programming Language Id and Its Compilation for Parallel Machines. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 5(2) </volume> <pages> 171-223, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The concept of single-assignment values have been used in a variety of parallel languages <ref> [10, 9, 13, 14, 20] </ref> as a way of exposing parallelism and synchronizing independent tasks. Such systems often must deal with problems of excessive memory usage and data copying as shared data items are created and modified.
Reference: [21] <author> K. Pingali and K. Ekanadham. </author> <title> Accumulators: New Logic Variable Abstractions for Functional Languages. </title> <journal> Theoretical Computer Science, </journal> <volume> 81(2) </volume> <pages> 201-221, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Values make it simple to express producer-consumer relationships or precedence constraints; any read of a value must wait for the creation of the value. accumulators <ref> [21] </ref>, whose data accesses are mutually exclusive. Data are migrated automatically in turn to processors which request mutually exclusive access.
Reference: [22] <author> U. Ramachandran, M. Yousef, and A. Khalidi. </author> <title> An Implementation of Distributed Shared Memory. </title> <journal> Software Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 443-464, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A number of systems have supported the notion of chaotic access. Agora [4] supports a memory model in which all accesses are chaotic, since all modifications to shared data structures are allowed to complete before holders of cached copies have been notified. Mether [19] and Clouds <ref> [22] </ref> support operations for accessing a read-only copy of a page (or segment) that will not be kept coherent even if the contents of the page (or segment) are changed by another processor. Finally, besides minimizing communication, SAM also provides primitives for optimizing communication.
Reference: [23] <author> E. Rothberg and A. Gupta. </author> <title> An Efficient Block-Oriented Approach to Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 503-512, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: By analyzing the performance of these applications on a variety of machines with different characteristics, we get a better understanding of the hardware and software factors that affect their performance. 4.1 Block Sparse Cholesky Factorization The block Cholesky application <ref> [23] </ref> performs a Cholesky factorization of a sparse, symmetric matrix in parallel. It decomposes the sparse matrix into blocks and assigns work to processors at the granularity of updates to blocks. Such updates typically involve using two source blocks to update one destination block.
Reference: [24] <author> D. J. Scales and M. S. Lam. </author> <title> An Efficient Shared Memory System for Distributed Memory Machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: All these optimizations are well integrated into SAM's shared memory model. 3 SAM Overview In this section, we present a brief overview of SAM mainly via several examples. A more detailed description is provided in <ref> [24] </ref>. 3.1 Basic Primitives In SAM, all shared data are represented by either a value or an accumulator. (SAM deals only with the management and communication of shared data; data that are completely local to a processor can be managed by any appropriate method.) In Figure 1, we show several common
Reference: [25] <author> J. P. Singh. </author> <title> Parallel Hierarchical N-body Methods and Their Implications for Multiprocessors. </title> <type> Technical Report CSL-TR-93-565, </type> <institution> Stanford University, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: However, the work of the force calculation phase can be partitioned so that there is extensive locality in each processor's access to the tree nodes <ref> [25] </ref>.
Reference: [26] <author> V. Sunderam. </author> <title> PVM: a Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: SAM has been implemented as a C library on a variety of platforms: on the CM-5, Intel iPSC/860, Intel Paragon, and IBM SP1, and on heterogeneous networks of workstations using PVM <ref> [26] </ref>. To evaluate the effectiveness of SAM, we have chosen a number of complex scientific applications that have previously been developed by other researchers either for machines with hardware shared memory support or for distributed memory machines using the message-passing style.
Reference: [27] <author> M. Warren and J. Salmon. </author> <title> An O(NlogN) Hypercube N-body Integrator. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages II 971-975, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Both the SAM and DASH versions of the Barnes-Hut application are derived from the original serial code. Warren and Salmon's message-passing Barnes-Hut code <ref> [27] </ref> is a completely separate code not based at all on the serial Barnes-Hut code. Chakrabarti's message-passing version of the Grobner basis algorithm for the CM-5 [8] is based on the original serial code, as is the SAM version. <p> We have also included performance numbers (labeled in the graph as MP-iPSC) for the message-passing Barnes-Hut code by Warren and Salmon <ref> [27] </ref>. We have ported this message-passing code to the iPSC/860 and run it on an identical problem. 4 For these performance figures, tree blocking is used for the runs on the iPSC/860, Paragon, and SP1.
Reference: [28] <author> W. Weihl et al. </author> <title> Prelude: A System for Portable Parallel Software. </title> <type> Technical Report MIT/LCS/TR-519, </type> <institution> MIT, </institution> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: It remains undemonstrated as to how these approaches perform for applications with more demanding communication patterns. To provide the user with control over the granularity of communication and avoid false sharing, systems such as Amber [11], Prelude <ref> [28] </ref>, Orca [1], Midway [3], and Linda [5, 7] communicate data at the level of user-defined data types (or objects), rather than virtual-memory pages. Amber, Prelude, and Orca provide access to shared data in the context of object-oriented languages.
References-found: 28

