URL: http://www.tns.lcs.mit.edu/~djw/library/ivan_paper.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/
Root-URL: 
Email: email mtam@gradient.cis.upenn.edu farber@linc.cis.upenn.edu  
Title: CapNet A Gigabit Wide Area Backplane That Supports Mobile Data Objects  
Author: Ming-Chit Tam David J. Farber 
Note: This work was supported by the National Science Foundation and the Advanced research projects Agency under Cooperative Agreement NCR-89190.5 with the Corporation for National Research Initiatives.  
Date: July 12, 1994  
Address: Pennsylvania  
Affiliation: Distributed Systems Laboratory Department of Computer And Information Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [Bennette 90] <author> J. Bennette, J. Carter and W. Zwaenepoel, "Munin: </author> <title> Distributed Shared Memory Based On Type-Specific Memory Coherence", </title> <booktitle> PROC 1990 Conf. Principles And Practice Of Parallel Programming. </booktitle> <publisher> ACM Press. </publisher> <address> New York, N.Y. </address> <year> 1990, </year> <month> pp168-176. </month>
Reference-contexts: CapNet extends this concept into wide area networks. 1.2 Distributed Shared Memory [Tam 90], [Nitzberg 90] surveyed a number of research efforts in DSM. While there has been a lot of research in relaxing cache consistency requirements, e.g., Munin <ref> [Bennette 90] </ref> and Mether [Minnich 90], speeding up shared memory access in a wide area environment remains largely unexplored. In almost all of the DSM implementations, shared memory objects are distributed among processors and are transferred from one processor to another on demand.
Reference: [Delp 88] <author> Gary Delp. </author> <title> "The Architecture and Implementation of Memnet : A high-speed Shared memory Computer Communication Network", </title> <type> PH.D thesis, </type> <institution> University of Deleware, </institution> <year> 1988. </year>
Reference-contexts: From the viewpoint of a process, the network is abstracted as a shared object space [Farber 92][Delp 91]. The nature of distribution is transparent, i.e., processes do not communicate with each other explicitly, communication is done by reading and writing shared data objects. MemNet <ref> [Delp 88] </ref> is a hardware implementation of this concept in a local environment using a high speed token ring. CapNet extends this concept into wide area networks. 1.2 Distributed Shared Memory [Tam 90], [Nitzberg 90] surveyed a number of research efforts in DSM.
Reference: [Delp 91] <author> Gary Delp, David J. Farber, Ronald Minnich, Jonathan Smith, Ming-Chit Tam. </author> <title> "Memory As A Network Abstract". </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1991. </year>
Reference: [Farber 92] <author> David J. Farber. </author> <title> "A Tale of Two Major Networking Problems one Organizational and one Technical" Harvard Information Quarterly, </title> <month> Fall </month> <year> 1989. </year>
Reference-contexts: From the viewpoint of a process, the network is abstracted as a shared object space <ref> [Farber 92] </ref>[Delp 91]. The nature of distribution is transparent, i.e., processes do not communicate with each other explicitly, communication is done by reading and writing shared data objects. MemNet [Delp 88] is a hardware implementation of this concept in a local environment using a high speed token ring.
Reference: [Katz 85] <author> R. H. Katz et al. </author> <title> "Implementing a cache consistency protocol." </title> <booktitle> Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283. </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: A process reading a memory object for the first time after a write is now forced to fetch an updated copy from the writer. Following the convention of the Berkeley ownership protocol <ref> [Katz 85] </ref> , we call the most recent writer of an object its owner. A read fault occurs when a process reads a shared memory object but does not have the read right.
Reference: [Kleinrock 92] <author> Leonard Kleinrock. </author> <title> "The Latency/Bandwidth Tradeoff in Gigabit Network". </title> <journal> IEEE Communication Magazine, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: However, recent advances in high speed networks [Partridge 94] have significantly reduced the packet insertion time into a network and the queuing delay at the switches, leaving the light wave propagation time as the only major delay <ref> [Kleinrock 92] </ref>. This makes it interesting to consider what interprocess communication (IPC) paradigms are appropriate for wide area distributed systems. A distributed shared memory (DSM) [Li 86] is an IPC paradigm in which a page based, segment based or object based virtual memory space is shared among distributed processes.
Reference: [Li 86] <author> Kai Li. </author> <title> "Shared Virtual Memory on Loosely Coupled Multiprocessors". </title> <type> Ph.D Thesis, </type> <institution> Yale University, </institution> <year> 1986. </year>
Reference-contexts: This makes it interesting to consider what interprocess communication (IPC) paradigms are appropriate for wide area distributed systems. A distributed shared memory (DSM) <ref> [Li 86] </ref> is an IPC paradigm in which a page based, segment based or object based virtual memory space is shared among distributed processes. <p> To distribute the load, the directory is often distributed among a number of processors by partitioning it in the object space. We can further classify distributed directory approaches into 1) DDRO (Distributed Directory with Read Ownership), e.g., DASH [Lenoski 90], and 2) DBD (Distributed Basic Directory), e.g., IVY <ref> [Li 86] </ref>. A DDRO directory keeps track of both the owner and the copy holders of an object. All read and write faults are sent to a directory. A directory adds the requesting host onto the object's copy list when it handles a read fault.
Reference: [Lenoski 90] <author> Lenoski, D., J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor" Proc. </title> <booktitle> 17th Int. Symp. on Computer Architecture. </booktitle> <pages> pp. 148-159, </pages> <year> 1990. </year> <note> CapNet Ivan Tam, David Farber 21 </note>
Reference-contexts: To distribute the load, the directory is often distributed among a number of processors by partitioning it in the object space. We can further classify distributed directory approaches into 1) DDRO (Distributed Directory with Read Ownership), e.g., DASH <ref> [Lenoski 90] </ref>, and 2) DBD (Distributed Basic Directory), e.g., IVY [Li 86]. A DDRO directory keeps track of both the owner and the copy holders of an object. All read and write faults are sent to a directory.
Reference: [Minnich 90] <author> R. Minnich, D. Farber, </author> <title> "The Mether System Distributed Shared Memory for SunOS 4.0", </title> <booktitle> Distributed Computer Systems Conference,, </booktitle> <year> 1990. </year>
Reference-contexts: CapNet extends this concept into wide area networks. 1.2 Distributed Shared Memory [Tam 90], [Nitzberg 90] surveyed a number of research efforts in DSM. While there has been a lot of research in relaxing cache consistency requirements, e.g., Munin [Bennette 90] and Mether <ref> [Minnich 90] </ref>, speeding up shared memory access in a wide area environment remains largely unexplored. In almost all of the DSM implementations, shared memory objects are distributed among processors and are transferred from one processor to another on demand.
Reference: [Nitzberg 91] <author> Bill Nitzberg and Virginia Lo, </author> <title> University Of Oregon "Distributed Shared Memory: A Survey Of Issues And Algorithms", </title> <journal> IEEE, Computer Magazine, </journal> <month> August </month> <year> 1991. </year>
Reference: [Partridge 94] <author> Craig Partridge. </author> <title> "Gigabit Networking", </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction 1.1 A Nationwide Multiprocessor Due to large communication delays, distributed systems over wide area networks had not been common. However, recent advances in high speed networks <ref> [Partridge 94] </ref> have significantly reduced the packet insertion time into a network and the queuing delay at the switches, leaving the light wave propagation time as the only major delay [Kleinrock 92]. This makes it interesting to consider what interprocess communication (IPC) paradigms are appropriate for wide area distributed systems.
Reference: [Smith 88] <author> Jonathan M. Smith, </author> <title> "A Survey Of Process Migration", </title> <journal> ACM Operating Systems Review, </journal> <month> July </month> <year> 1988, </year> <month> pp.28-40 </month>
Reference-contexts: The advantages of DSM over message passing and remote procedure call include: 1) better support for complex data structures, 2) well-understood programming model, and CapNet Ivan Tam, David Farber 2 3) easier process migration <ref> [Smith 88] </ref>. From the viewpoint of a process, the network is abstracted as a shared object space [Farber 92][Delp 91]. The nature of distribution is transparent, i.e., processes do not communicate with each other explicitly, communication is done by reading and writing shared data objects.
Reference: [Tam 90] <author> Ming-Chit Tam and David J. Farber. </author> <title> "CapNet An Alternate Approach to Ultra-high Speed Networks" Proceedings of International Communication Conference '90, </title> <address> Atlanta, Georgia, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: MemNet [Delp 88] is a hardware implementation of this concept in a local environment using a high speed token ring. CapNet extends this concept into wide area networks. 1.2 Distributed Shared Memory <ref> [Tam 90] </ref>, [Nitzberg 90] surveyed a number of research efforts in DSM. While there has been a lot of research in relaxing cache consistency requirements, e.g., Munin [Bennette 90] and Mether [Minnich 90], speeding up shared memory access in a wide area environment remains largely unexplored. <p> A read request is directly replied by the nearest copy holder, thus besides requiring only two rounds of messages, the propagation time of messages is reduced to a minimal. Figure 1 and 2 (c) show the message exchanged during a read fault and a write fault in CapNet. <ref> [Tam 90] </ref> described some of the ideas in section 2. This paper extends the idea much further and reports our simulation results. The rest of the paper is organized as follows. Section 2 introduces the basic idea of CapNet by showing how object owners are tracked in the network.
Reference: [Tam 94] <author> Ming-Chit Tam. </author> <title> "CapNet Using Gigabit Network As a High Speed Backplane". </title> <type> Ph.D Thesis, </type> <institution> Department of Computer and Information Science, University Of Pennsylvania, </institution> <year> 1994. </year>
Reference-contexts: In the location network approach, if every site is equally likely to own an object, the average cost of object search paths would simply be the average internodal distance of the tree, hence suggesting a minimum internodal distance (MIND) tree for our purpose. Our experiments <ref> [Tam 94] </ref> shows that the MIND tree approach generates shorter object search paths than the shortest path approach. <p> In addition to implementation, we have also considered issues such as communication reliability, network failures and object table size. We will describe them briefly in here but the details can be found in <ref> [Tam 94] </ref>. The reliability of host to host communications in CapNet are provided by a transport level protocol such as TCP. Host to mobile object communications, e.g., memory requests, follows a time-based protocol to recover from network errors.
References-found: 14

