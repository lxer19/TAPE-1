URL: http://www.cs.berkeley.edu/~alanm/CP/blumrich.hoti2.94.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Two Virtual Memory Mapped Network Interface Designs  
Author: Matthias A. Blumrich, Cezary Dubnicki, Edward W. Felten, Kai Li, Malena R. Mesarina 
Address: Princeton NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Note: In the Hot Interconnects II Symposium Record, August, 1994, pp. 134-142.  
Abstract: In existing multicomputers, software overhead dominates the message-passing latency cost. Our research on the SHRIMP project at Princeton indicates that appropriate network interface support can significantly reduce this software overhead. We have designed two network interfaces for the SHRIMP multicomputer. Both support virtual memory mapped communication allowing user processes to communicate without doing expensive buffer management, and without using system calls to cross the protection boundary separating user processes from the operating system kernel. This paper describes and compares the two network interfaces, and discusses performance tradeoffs between them. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ramune Arlauskas. </author> <title> iPSC/2 system: A second generation hypercube. </title> <booktitle> In Concurrent Supercomputing, the Second Generation, </booktitle> <pages> pages 9-13. </pages> <publisher> Intel Corporation, </publisher> <year> 1988. </year>
Reference-contexts: Since both SHRIMP network interfaces support traditional message passing and virtual memory-mapped communication, they allow user programs to optimize for common cases. 7 Related Work The traditional network interface design is based on DMA data transfer. Recent examples include the NCUBE [16], iPSC/2 <ref> [1] </ref> and iPSC/860 [11]. In this scheme an application sends messages by making operating system calls to initiate DMA data transfers.
Reference: [2] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Received messages are transferred directly to memory, reducing the receive software overhead to only a few instructions in the common case. Our second design 1 Physical space memory spaces memory spaces memory space memory Physical Virtual Virtual NODE A NODE B INTERCONNECT implements virtual memory mapping completely in hardware <ref> [2] </ref>. <p> The deliberate update mode requires a few user-level instructions to send up to a page of data. Table 1 shows the overhead components of message passing on three kinds of network interfaces: traditional, SHRIMP-I, and SHRIMP-II <ref> [2] </ref>. The SHRIMP-II network interface implements virtual memory mapping translation in hardware so that the send operation can be performed at user level. <p> The cost of mapping on both SHRIMP-I and SHRIMP-II is similar to that of passing a small message using csend and crecv in NX/2. For applications that have static communication patterns, the amortized overhead of creating a mapping can be negligible <ref> [2] </ref>. We should point out that the semantics of the csend/crecv primitives of NX/2 are richer than the virtual memory-mapped communication supported by SHRIMP family of interfaces. Our comparison shows that rich semantics comes with substantial overhead.
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peter-son, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [3, 8, 4] </ref>. Writing and reading these registers queues and de-queues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [4] <author> William J. Dally. </author> <title> The J-Machine system. </title> <editor> In P.H. Winston and S.A. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <pages> pages 550-580. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [3, 8, 4] </ref>. Writing and reading these registers queues and de-queues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [5] <author> Cezary Dubnicki, Kai Li, and Malena Mesarina. </author> <title> Network interface support for user-level buffer management. In Workshop on Parallel Computer Routing and Communication. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: In our first design, we explored how to do minimal modifications to the traditional DMA-based network interface design, while implementing virtual memory mapping in software <ref> [5] </ref>. Our design requires a system call to initiate outgoing data transfer, but its virtual memory mapped communication can reduce the send latency overhead by up to 78%. Received messages are transferred directly to memory, reducing the receive software overhead to only a few instructions in the common case.
Reference: [6] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <month> August </month> <year> 1993. </year> <note> Available as technical report 93-09-09. </note>
Reference-contexts: Recent studies and analyses indicate that moving communication buffer management out of the kernel to the user level can greatly reduce the software overhead of message passing. By using a compiled, application-tailored runtime library, the latency of multicomputer message passing can be improved by about 30% <ref> [6] </ref>. In addition, virtual memory mapped communication takes advantage of the protection provided by virtual memory systems. Since mappings are established at the virtual memory level, virtual address translation hardware guarantees that an application can only use mappings created by itself.
Reference: [7] <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. An alternative network interface design approach employs memory-mapped network interface FIFOs <ref> [13, 7] </ref>. In this scheme, the controller has no DMA capability. Instead, the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. This approach results in good latency for short messages.
Reference: [8] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [3, 8, 4] </ref>. Writing and reading these registers queues and de-queues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [9] <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 interconnect elan elite design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: On the Intel Delta multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to time on the wire [14]. Other recent multicomputers such as the Intel Paragon [12], Meiko CS-2 <ref> [9] </ref>, and TMC CM-5 [18] have lower message-passing latencies than Delta, but not much lower. <p> One solution to the problem of software overhead is to add a separate processor on every node just for message passing [15, 10]. Recent examples of this approach are the Intel Paragon [12] and Meiko CS-2 <ref> [9] </ref>. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [10] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> A message passing coprocessor for distributed memory multicom-puters. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: The main disadvantage of traditional network interfaces is that message passing costs are usually thousands of CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [15, 10] </ref>. Recent examples of this approach are the Intel Paragon [12] and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: [11] <institution> Intel Corporation. </institution> <note> iPSC/860 Technical Reference Manual, </note> <year> 1991. </year>
Reference-contexts: Since both SHRIMP network interfaces support traditional message passing and virtual memory-mapped communication, they allow user programs to optimize for common cases. 7 Related Work The traditional network interface design is based on DMA data transfer. Recent examples include the NCUBE [16], iPSC/2 [1] and iPSC/860 <ref> [11] </ref>. In this scheme an application sends messages by making operating system calls to initiate DMA data transfers. The network interface initiates an incoming DMA data transfer when a message arrives and interrupts the local processor when the transfer has finished so that it can dispatch the arrived message.
Reference: [12] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: On the Intel Delta multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to time on the wire [14]. Other recent multicomputers such as the Intel Paragon <ref> [12] </ref>, Meiko CS-2 [9], and TMC CM-5 [18] have lower message-passing latencies than Delta, but not much lower. <p> One solution to the problem of software overhead is to add a separate processor on every node just for message passing [15, 10]. Recent examples of this approach are the Intel Paragon <ref> [12] </ref> and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [13] <author> C.E. Leiserson, Z.S. Abuhamdeh, D.C. Douglas, C.R. Feynman, M.N. Ganmukhi, J.V. Hill, D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, D.S. Wells, M.C. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year> <month> 8 </month>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. An alternative network interface design approach employs memory-mapped network interface FIFOs <ref> [13, 7] </ref>. In this scheme, the controller has no DMA capability. Instead, the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. This approach results in good latency for short messages.
Reference: [14] <author> Richard J. Littlefield. </author> <title> Characterizing and tuning communications performance for real applications. </title> <booktitle> In Proceedings of the First Intel DELTA Applications Workshop, </booktitle> <pages> pages 179-190, </pages> <month> February </month> <year> 1992. </year> <note> Proceedings also available as Caltech Technical Report CCSF-14-92. </note>
Reference-contexts: On the Intel Delta multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to time on the wire <ref> [14] </ref>. Other recent multicomputers such as the Intel Paragon [12], Meiko CS-2 [9], and TMC CM-5 [18] have lower message-passing latencies than Delta, but not much lower.
Reference: [15] <author> R.S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The main disadvantage of traditional network interfaces is that message passing costs are usually thousands of CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [15, 10] </ref>. Recent examples of this approach are the Intel Paragon [12] and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: [16] <author> John Palmer. </author> <title> The NCUBE family of high-performance parallel computer systems. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 845-851, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Since both SHRIMP network interfaces support traditional message passing and virtual memory-mapped communication, they allow user programs to optimize for common cases. 7 Related Work The traditional network interface design is based on DMA data transfer. Recent examples include the NCUBE <ref> [16] </ref>, iPSC/2 [1] and iPSC/860 [11]. In this scheme an application sends messages by making operating system calls to initiate DMA data transfers.
Reference: [17] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The network interfaces of existing multicomputers and workstation networks require a significant amount of software overhead to implement message-passing protocols. In fact, message-passing primitives on many multicomputers, such as the csend/crecv of Intel's NX/2 <ref> [17] </ref>, often execute more than a thousand instructions to send and receive a message; by comparison, the hardware overhead of data transfer is negligible.
Reference: [18] <institution> The connection machine CM-5 technical summary, </institution> <year> 1991. </year>
Reference-contexts: On the Intel Delta multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to time on the wire [14]. Other recent multicomputers such as the Intel Paragon [12], Meiko CS-2 [9], and TMC CM-5 <ref> [18] </ref> have lower message-passing latencies than Delta, but not much lower. The main reason for such high software overheads is that these multicomputers use network interfaces that require a significant number of instructions at the operating system and user levels to provide protection, buffer management, and message-passing protocols.
Reference: [19] <author> Roger Traylor and Dave Dunning. </author> <title> Routing chip set for Intel Paragon parallel supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year> <month> 9 </month>
Reference-contexts: The resulting network interface supports the traditional DMA-based model, and can optionally be used to implement virtual memory-mapped communication with some software assistance. transactions to interface between the EISA bus of a Pentium PC and a NIC (Network Interface Chip) connected to an Intel Paragon routing network <ref> [19] </ref>. DMA transactions are limited to the size of a memory page and cannot cross page boundaries, since pages are the unit of protection.
References-found: 19

