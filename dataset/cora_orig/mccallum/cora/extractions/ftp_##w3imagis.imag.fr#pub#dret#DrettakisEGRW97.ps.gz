URL: ftp://w3imagis.imag.fr/pub/dret/DrettakisEGRW97.ps.gz
Refering-URL: http://w3imagis.imag.fr/Publications/index_fr.html
Root-URL: http://www.imag.fr
Title: Interactive Common Illumination for Computer Augmented Reality  
Author: George Drettakis Luc Robert Sylvain Bougnoux iMAGIS/GRAVIR-INRIA, ROBOTVIS 
Abstract: The advent of computer augmented reality (CAR), in which computer generated objects mix with real video images, has resulted in many interesting new application domains. Providing common illumination between the real and synthetic objects can be very beneficial, since the additional visual cues (shadows, interreflections etc.) are critical to seamless real-synthetic world integration. Building on recent advances in computer graphics and computer vision, we present a new framework to resolving this problem. We address three specific aspects of the common illumination problem for CAR: (a) simplification of camera calibration and modeling of the real scene; (b) efficient update of illumination for moving CG objects and (c) efficient rendering of the merged world. A first working system is presented for a limited sub-problem: a static real scene and camera with moving CG objects. Novel advances in computer vision are used for camera calibration and user-friendly modeling of the real scene, a recent interactive ra-diosity update algorithm is adapted to provide fast illumination update and finally textured polygons are used for display. This approach allows interactive update rates on mid-range graphics workstations. Our new framework will hopefully lead to CAR systems with interactive common illumination without restrictions on the movement of real or synthetic objects, lights and cameras. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <editor> K.B. Atkinson, editor. </editor> <title> Close Range Photogrammetry and Machine Vision. </title> <publisher> Whittles Publishing, </publisher> <year> 1996. </year>
Reference-contexts: The photogrammetry approach mostly focuses on accuracy problems, and the derived techniques produce three-dimensional models of high quality <ref> [1] </ref>. However, they generally require significant human interaction. Some commercial products, such as Photomodeler, already integrate these techniques. In computer vision, a number of automatic techniques exist for computing structure from stereo or motion (e.g., [7, 20, 10]). <p> From this initial estimate, we then run a non-linear minimisation process known as bundle adjustment in photogrammetry <ref> [1] </ref>, which refines the estimate of the rotations and translations.
Reference: 2. <author> Ronald T. Azuma. </author> <title> A survey of augmented reality. In Presence: Teleoperators and Virtual Environments (to appear) (earlier version in Course Notes #9: Developing Advanced Virtual Reality Applications, </title> <booktitle> ACM SIGGRAPH (LA, </booktitle> <year> 1995), </year> <note> 20-1 to 20-38, 1997. http://www.cs.unc.edu/azuma/azuma AR.html. </note>
Reference-contexts: The combination of virtual or synthetic environments with real video images (RVI) has lead to many new and exciting applications. The core research in this area concentrates on the problems related to registration and calibration for real-time systems (see for example <ref> [2, 3] </ref>). Since many of these problems are still largely unresolved, little attention has been given to the problems of the interaction of illumination between the real and synthetic scenes. Pioneering work in this domain has been performed by Fournier et al. [14]. <p> We follow this last approach in this paper. 2.2 Computer Augmented Reality Much work has recently been performed in the domain of computer augmented reality. The main body of this research concentrates on the requirements of real-time systems <ref> [2] </ref>. In terms of illumination, these systems provide little, if any, common lighting information. Examples of work including some form of shadowing between real and synthetic objects are presented in [26] and [19]. Common illumination requires full 3D information, and thus should use explicit modeling of the real world objects. <p> The wealth of excellent research in this domain will undoubtedly be central in the future work in common illumination (see Section 6.1). For now however, we concentrate on the issues directly related to illumination. The reader interested in an in-depth survey should refer to <ref> [2] </ref>. 2.3 Radiosity and Common Illumination for CAR In what follows, we consider the following configuration: we have an image I , which we call the target image, and, using techniques developed below, a set of geometric elements approximating the scene. <p> The ultimate goal is to have seamless, real-time mixing of real and synthetic scenes, with shared realistic illumination. There is a lot of work to be done before this goal can be achieved, much of which is related to hardware, vision, registration and sensing (see <ref> [2] </ref> for more detail). We concentrate here on the issues directly or indirectly related to common illumination and display. 6.1 Future Work The first restriction to lift is that of a static camera.
Reference: 3. <author> D. E. Breen, R. T. Whitaker, E. Rose, and M. Tuceryan. </author> <title> Interactive occlusion and automatic object placement for augmented reality. </title> <journal> Computer Graphics Forum, </journal> <volume> 15(3):C11-C22, </volume> <month> September </month> <year> 1996. </year>
Reference-contexts: The combination of virtual or synthetic environments with real video images (RVI) has lead to many new and exciting applications. The core research in this area concentrates on the problems related to registration and calibration for real-time systems (see for example <ref> [2, 3] </ref>). Since many of these problems are still largely unresolved, little attention has been given to the problems of the interaction of illumination between the real and synthetic scenes. Pioneering work in this domain has been performed by Fournier et al. [14]. <p> Common illumination requires full 3D information, and thus should use explicit modeling of the real world objects. Similar requirements exist for the resolution of occlusion between real and virtual objects (e.g., <ref> [3] </ref>). The wealth of excellent research in this domain will undoubtedly be central in the future work in common illumination (see Section 6.1). For now however, we concentrate on the issues directly related to illumination.
Reference: 4. <author> Shenchang Eric Chen and Lance Williams. </author> <title> View interpolation for image synthesis. </title> <editor> In James T. Kajiya, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '93 Proceedings), </booktitle> <volume> volume 27, </volume> <pages> pages 279-288, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The removal of real objects is also an interesting challenge, and will require the use of some of the techniques developed for the treatment of the texture de-warping problem, in particular to effect a removal of a real object. Image-based rendering approaches <ref> [4] </ref> may prove useful here as well. The move to real-time video acquisition and common illumination will be the greatest challenge of all.
Reference: 5. <author> Michael F. Cohen, Shenchang Eric Chen, John R. Wallace, and Donald P. Greenberg. </author> <title> A progressive refinement approach to fast radiosity image generation. </title> <journal> Computer Graphics, </journal> <volume> 22(4) </volume> <pages> 75-84, </pages> <month> August </month> <year> 1988. </year> <booktitle> Proceedings SIGGRAPH '88 in Atlanta, </booktitle> <address> USA. </address>
Reference-contexts: The average reflectivity of the scene ^ae is selected arbitrarily. This can also be set as the average pixel value. Once a value for ^ae is set, the overall reflectivity factor R is defined as: R = 1 ^ae The concept of ambient radiosity <ref> [5] </ref>, ^ B A is then used, permitting a first estimation of the exitance values E i of the sources: ^ B A = P P ; (2) where E i is the exitance of each object i and A i its area.
Reference: 6. <author> P.E. Debevec, C.J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIG-GRAPH, </booktitle> <pages> pages 11-20, </pages> <address> New Orleans, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Some recent approaches have been proposed to reduce the effort in the production of explicit 3D models of high quality, either by imposing constraints on the modeled scene <ref> [6] </ref>, or by combining automatic computer vision processes with human interaction [12]. We follow this last approach in this paper. 2.2 Computer Augmented Reality Much work has recently been performed in the domain of computer augmented reality.
Reference: 7. <author> Umesh R. Dhond and J.K. Aggarwal. </author> <title> Structure from stereo a review. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(6) </volume> <pages> 1489-1510, </pages> <year> 1989. </year>
Reference-contexts: However, they generally require significant human interaction. Some commercial products, such as Photomodeler, already integrate these techniques. In computer vision, a number of automatic techniques exist for computing structure from stereo or motion (e.g., <ref> [7, 20, 10] </ref>). With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images.
Reference: 8. <author> George Drettakis and Francois Sillion. </author> <title> Interactive update of global illumination using a line-space hierarchy. </title> <editor> In Turner Whitted, editor, </editor> <booktitle> SIGGRAPH 97 Conference Proceedings (Los Angeles, CA), Annual Conference Series. ACM SIGGRAPH, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: This method allows us to model the real scene to the level of detail required, and to extract camera parameters simply and automatically. We use fast hierarchical [16, 24, 25] and incremental update <ref> [8] </ref> techniques for radiosity, permitting interaction with virtual objects in the CAR environment. Interactive update rates (a few seconds per frame) of the mixed real/synthetic environment, including common illumination is achieved by using a texture-based rendering approach on suitable hardware. <p> This guarantees that the final synthesized images have approximately the same level of detail as the initial ones. 4 A Fast Hierarchical Method for Common Illumination Recent advances in global illumination technology allow us to calculate the lighting efficiently, using hierarchical radiosity [16], clustering [24, 25] and incremental update methods <ref> [8] </ref>. To initialise the system, the calculation of certain basic parameters is required. We adopt many of the conventions used by Fournier et al. [14], adapting them appropriately to the application and the requirements at hand. <p> The value ~ B i is a (relative) representation of the illumination due to real sources. The next step is the addition of computer generated objects. This is performed by adapting the methods described in <ref> [8] </ref>. We thus group the synthetic objects into natural clusters (i.e. a chair or a desk lamp) and we add them into the scene. <p> modulate positively and negatively, so we must coherently re-scale this ratio to always lie between zero and one. 4.4 Interactive Common Illumination of CG Objects in a Real Scene Using an implicit hierarchical description of the line segment space contained between hierarchical elements, we can rapidly identify the links modified <ref> [8] </ref>. This is achieved by keeping a hierarchy of shaft structures [15] associated with the links and inactive (or passive) refined links. In the case of the CAR application, special treatment is required to ensure that light-ing effects created by CG objects are well represented. <p> Thus an obstacle elimination scheme will have to be developed, using known vision techniques enhanced with the available 3D and lighting information. A different restriction to overcome is to permit motion of CG light sources. This requires an adaptation of the incremental update method <ref> [8] </ref>, most notably in what concerns the representation of direct lighting shadows and corresponding refinement. The motion of real sources will result in similar problems. Moving real objects is also an important challenge.
Reference: 9. <author> Olivier Faugeras. </author> <title> On the evolution of simple curves of the real projective plane. </title> <type> Technical Report 1998, </type> <institution> INRIA, </institution> <year> 1993. </year>
Reference-contexts: We thus construct a geometric model of the room by computer-vision assisted, image-based interaction. Finally, textures are extracted and de-warped automatically. The whole process took approximately 4 hours for the scene shown in Figure 1. 3.1 Camera Calibration Using a Target The intrinsic parameters of the camera (see <ref> [9] </ref> for more details about the imaging geometry of cameras) are computed using the calibration technique described in [22]. We need to take one image of a non-planar calibration pattern, i.e., a real object with visible features of known geometry. <p> The output of the process is a 3 fi 4 matrix, which is decomposed as the product of a matrix of intrinsic parameters and a 4 fi 4 displacement (rotation, translation) matrix (computation described in <ref> [9] </ref>). 3.2 Image Acquisition and Mosaicing Though the minimum number of viewpoints for stereo reconstruction is two, we acquired images from four distinct viewpoints for better accuracy of the reconstructed 3D geometry. The viewpoints lie approximately at the vertices of a 1-meter-wide vertical square in one corner of the room.
Reference: 10. <author> Olivier Faugeras. </author> <title> Three-Dimensional Computer Vision: a Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: However, they generally require significant human interaction. Some commercial products, such as Photomodeler, already integrate these techniques. In computer vision, a number of automatic techniques exist for computing structure from stereo or motion (e.g., <ref> [7, 20, 10] </ref>). With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images.
Reference: 11. <author> Olivier Faugeras and Stephane Laveau. </author> <title> Representing three-dimensional data as a collection of images and fundamental matrices for image synthesis. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 689-691, </pages> <address> Jerusalem, Israel, October 1994. </address> <publisher> Computer Society Press. </publisher>
Reference-contexts: With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images. With image interpolation techniques <ref> [11, 21, 23] </ref>, the scene is represented as a depth field, or equivalently, as a set of feature correspondences across two reference images. Although these implicit 3D representations are suited to rendering, they are not adapted to our framework since we need complete 3D data to perform radiosity computation.
Reference: 12. <author> Olivier Faugeras, Stephane Laveau, Luc Robert, Gabriella Csurka, Cyril Zeller, Cyrille Gauclin, and Imed Zoghlami. </author> <title> 3-d reconstruction of urban scenes from image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Some recent approaches have been proposed to reduce the effort in the production of explicit 3D models of high quality, either by imposing constraints on the modeled scene [6], or by combining automatic computer vision processes with human interaction <ref> [12] </ref>. We follow this last approach in this paper. 2.2 Computer Augmented Reality Much work has recently been performed in the domain of computer augmented reality. The main body of this research concentrates on the requirements of real-time systems [2].
Reference: 13. <author> Olivier Faugeras, Tuan Luong, and Steven Maybank. </author> <title> Camera self-calibration: theory and experiments. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Proc 2nd ECCV, volume 588 of Lecture Notes in Computer Science, </booktitle> <pages> pages 321-334, </pages> <address> Santa Margherita Ligure, Italy, May 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference: 14. <author> Alain Fournier, Atjeng S. Gunawan, and Chris Romanzin. </author> <title> Common illumination between real and computer generated scenes. </title> <booktitle> In Proceedings Graphics Interface '93, </booktitle> <pages> pages 254-263. </pages> <publisher> Morgan Kaufmann publishers, </publisher> <year> 1993. </year>
Reference-contexts: Since many of these problems are still largely unresolved, little attention has been given to the problems of the interaction of illumination between the real and synthetic scenes. Pioneering work in this domain has been performed by Fournier et al. <ref> [14] </ref>. This work (see Section 2.3 for a brief review), has shown how the computation of common illumination between the real and synthetic scene results in a greatly improved graphical environment with which the user can interact. <p> All quantities related to the image will be noted ^: . The most closely related previous research in common illumination is that of Fournier et al. <ref> [14] </ref>. We will be adopting many of the conventions and approximations used in that approach. In [14] many basic quantities are defined in a rather ad-hoc manner using information taken from image I. The average reflectivity of the scene ^ae is selected arbitrarily. <p> All quantities related to the image will be noted ^: . The most closely related previous research in common illumination is that of Fournier et al. <ref> [14] </ref>. We will be adopting many of the conventions and approximations used in that approach. In [14] many basic quantities are defined in a rather ad-hoc manner using information taken from image I. The average reflectivity of the scene ^ae is selected arbitrarily. This can also be set as the average pixel value. <p> In our approach, we improve the ease of modeling, as well as the lighting update and final display speeds compared to <ref> [14] </ref>. Nonetheless, to achieve these improvements, we sacrifice certain advantages of Fournier et al.'s system: we currently can only handle a static camera and real scene, and the quality of rendering may be slightly degraded compared to that obtained by ray-traced correction to a real image. <p> To initialise the system, the calculation of certain basic parameters is required. We adopt many of the conventions used by Fournier et al. <ref> [14] </ref>, adapting them appropriately to the application and the requirements at hand. <p> and reflectance for the real video image (RVI) objects, and (b) the creation of a full hierarchical radiosity system, including the cluster hierarchy and line-space hierarchy of links and shafts required for the incremental solution. 4.1 Initialising the Basic Parameters As discussed in Section 2.3 the basic approximations proposed in <ref> [14] </ref> can be used to estimate the set of parameters required to create a hierarchical representation of the (real) light transfer in the CAR scene. <p> The calculation of ^ B i is performed by rendering the polygon textured with the corresponding part of the target (real) image 3 This is easier to calculate than the neighbourhood in <ref> [14] </ref>. I into an offscreen buffer and averaging the resulting pixel values. Once the new ^ B i is computed for the child element, the value ^ae i is updated. <p> As a result all patches now have a (possibly modified) radiosity value B i . 4.3 Display To achieve interactive update rates, we need to display the result of the combination of real and synthetic environments at interactive rates. This requirement precludes the use of the ray-casting approach of <ref> [14] </ref>. Our solution is to exploit the real-time texture capacities currently available on mid- and high-range graphics workstations. <p> We have developed a working system for the restricted case of a static camera and static real environment. The prototype system we present shows that it is possible to create convincing CAR environments in which CG objects can be manipulated interactively. Compared to previous work in common illumination (notably <ref> [14] </ref>), our framework allows easier modeling and calibration, faster illumination updates and rapid display of CAR scenes. Nonetheless, much more remains to be done.
Reference: 15. <author> Eric A. Haines. </author> <title> Shaft culling for efficient ray-traced radiosity. </title> <editor> In Brunet and Jansen, editors, </editor> <booktitle> Photorealistic Rendering in Comp. Graphics, </booktitle> <pages> pages 122-138. </pages> <publisher> Springer Verlag, </publisher> <year> 1993. </year> <booktitle> Proc. 2nd EG Workshop on Rendering (Barcelona, </booktitle> <year> 1991). </year>
Reference-contexts: This is achieved by keeping a hierarchy of shaft structures <ref> [15] </ref> associated with the links and inactive (or passive) refined links. In the case of the CAR application, special treatment is required to ensure that light-ing effects created by CG objects are well represented.
Reference: 16. <author> Pat Hanrahan, David Saltzman, and Larry Aupperle. </author> <title> A rapid hierarchical radiosity algorithm. </title> <journal> Computer Graphics, </journal> <volume> 25(4) </volume> <pages> 197-206, </pages> <month> August </month> <year> 1991. </year> <note> SIGGRAPH '91 Las Vegas. </note>
Reference-contexts: By using advanced vision techniques, we have replaced the tedious and inaccurate manual modeling process with a flexible and precise vision-based approach. This method allows us to model the real scene to the level of detail required, and to extract camera parameters simply and automatically. We use fast hierarchical <ref> [16, 24, 25] </ref> and incremental update [8] techniques for radiosity, permitting interaction with virtual objects in the CAR environment. Interactive update rates (a few seconds per frame) of the mixed real/synthetic environment, including common illumination is achieved by using a texture-based rendering approach on suitable hardware. <p> This guarantees that the final synthesized images have approximately the same level of detail as the initial ones. 4 A Fast Hierarchical Method for Common Illumination Recent advances in global illumination technology allow us to calculate the lighting efficiently, using hierarchical radiosity <ref> [16] </ref>, clustering [24, 25] and incremental update methods [8]. To initialise the system, the calculation of certain basic parameters is required. We adopt many of the conventions used by Fournier et al. [14], adapting them appropriately to the application and the requirements at hand.
Reference: 17. <author> R.I. </author> <title> Hartley. In defence of the 8-point algorithm. </title> <booktitle> In Proceedings of the 5th International Conference on Computer Vision, </booktitle> <pages> pages 1064-1070, </pages> <address> Boston, MA, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: From F 1i and the intrinsic parameters, we then derive, using the technique described in <ref> [17] </ref>, the rotation R 1i and translation t 1i . In fact, each translation is known only up to a scale factor, which corresponds to choosing an arbitrary unit for distances in space.
Reference: 18. <author> M. Irani, P. Anandan, and S. Hsu. </author> <title> Mosaic based representations of video sequences and their applications. </title> <booktitle> In Proceedings of the 5th International Conference on Computer Vision, </booktitle> <pages> pages 605-611, </pages> <address> Boston, MA, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The viewpoints lie approximately at the vertices of a 1-meter-wide vertical square in one corner of the room. To enlarge the field-of-view, we built panoramic images using mosaicing <ref> [27, 18] </ref>. At each viewpoint, we took three left-to-right images with an overlap of approximately 50% between two consecutive images. During this process, we were very careful at each viewpoint not to translate the camera but restrict motion to rotation.
Reference: 19. <author> P. Jancene, F. Neyret, X. Provot, J-P. Tarel, J-M. Vezien, C. Meilhac, and A. Verroust. </author> <title> Res: computing the interactions between real and virtual objects in video sequences. </title> <booktitle> In 2nd IEEE Workshop on networked Realities, </booktitle> <pages> pages 27-40, </pages> <address> Boston, Ma (USA), </address> <month> October </month> <year> 1995. </year> <note> http://www-rocq.inria.fr/syntim/textes/nr95-eng.html. </note>
Reference-contexts: The main body of this research concentrates on the requirements of real-time systems [2]. In terms of illumination, these systems provide little, if any, common lighting information. Examples of work including some form of shadowing between real and synthetic objects are presented in [26] and <ref> [19] </ref>. Common illumination requires full 3D information, and thus should use explicit modeling of the real world objects. Similar requirements exist for the resolution of occlusion between real and virtual objects (e.g., [3]).
Reference: 20. <author> C.P. Jerian and R. Jain. </author> <title> Structure from motion. a critical analysis of methods. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 572-587, </pages> <year> 1991. </year>
Reference-contexts: However, they generally require significant human interaction. Some commercial products, such as Photomodeler, already integrate these techniques. In computer vision, a number of automatic techniques exist for computing structure from stereo or motion (e.g., <ref> [7, 20, 10] </ref>). With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images.
Reference: 21. <author> L. McMillan. </author> <title> Acquiring immersive visual environments with an uncalibrated camera. </title> <type> Technical Report TR95-006, </type> <institution> University of North Carolina, </institution> <year> 1995. </year>
Reference-contexts: With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images. With image interpolation techniques <ref> [11, 21, 23] </ref>, the scene is represented as a depth field, or equivalently, as a set of feature correspondences across two reference images. Although these implicit 3D representations are suited to rendering, they are not adapted to our framework since we need complete 3D data to perform radiosity computation.
Reference: 22. <author> L Robert. </author> <title> Camera calibration without feature extraction. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 63(2) </volume> <pages> 314-325, </pages> <month> March </month> <year> 1995. </year> <note> also INRIA Technical Report 2204. </note>
Reference-contexts: The whole process took approximately 4 hours for the scene shown in Figure 1. 3.1 Camera Calibration Using a Target The intrinsic parameters of the camera (see [9] for more details about the imaging geometry of cameras) are computed using the calibration technique described in <ref> [22] </ref>. We need to take one image of a non-planar calibration pattern, i.e., a real object with visible features of known geometry. With minimal interaction (the user only needs to click the approximate position in the image of 6 reference points), an estimate of the camera parameters is computed.
Reference: 23. <author> S.M. Seitz and C.R. Dyer. </author> <title> Physically-valid view synthesis by image interpolation. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <pages> pages 18-25, </pages> <address> Cambridge, Mas-sachusetts, USA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: With these techniques, the three-dimensional models are produced much more easily, but they are less accurate, potentially containing a small fraction of gross errors. Alternate representations have been proposed for realistic rendering from images. With image interpolation techniques <ref> [11, 21, 23] </ref>, the scene is represented as a depth field, or equivalently, as a set of feature correspondences across two reference images. Although these implicit 3D representations are suited to rendering, they are not adapted to our framework since we need complete 3D data to perform radiosity computation.
Reference: 24. <author> Francois Sillion. </author> <title> A unified hierarchical algorithm for global illumination with scattering volumes and object clusters. </title> <journal> IEEE Trans. on Vis. and Comp. Graphics, </journal> <volume> 1(3), </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: By using advanced vision techniques, we have replaced the tedious and inaccurate manual modeling process with a flexible and precise vision-based approach. This method allows us to model the real scene to the level of detail required, and to extract camera parameters simply and automatically. We use fast hierarchical <ref> [16, 24, 25] </ref> and incremental update [8] techniques for radiosity, permitting interaction with virtual objects in the CAR environment. Interactive update rates (a few seconds per frame) of the mixed real/synthetic environment, including common illumination is achieved by using a texture-based rendering approach on suitable hardware. <p> This guarantees that the final synthesized images have approximately the same level of detail as the initial ones. 4 A Fast Hierarchical Method for Common Illumination Recent advances in global illumination technology allow us to calculate the lighting efficiently, using hierarchical radiosity [16], clustering <ref> [24, 25] </ref> and incremental update methods [8]. To initialise the system, the calculation of certain basic parameters is required. We adopt many of the conventions used by Fournier et al. [14], adapting them appropriately to the application and the requirements at hand.
Reference: 25. <author> Brian Smits, James Arvo, and Donald P. Greenberg. </author> <title> A clustering algorithm for radios-ity in complex environments. </title> <editor> In Andrew S. Glassner, editor, </editor> <booktitle> SIGGRAPH 94 Conference Proceedings (Orlando, FL), Annual Conference Series, </booktitle> <pages> pages 435-442. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: By using advanced vision techniques, we have replaced the tedious and inaccurate manual modeling process with a flexible and precise vision-based approach. This method allows us to model the real scene to the level of detail required, and to extract camera parameters simply and automatically. We use fast hierarchical <ref> [16, 24, 25] </ref> and incremental update [8] techniques for radiosity, permitting interaction with virtual objects in the CAR environment. Interactive update rates (a few seconds per frame) of the mixed real/synthetic environment, including common illumination is achieved by using a texture-based rendering approach on suitable hardware. <p> This guarantees that the final synthesized images have approximately the same level of detail as the initial ones. 4 A Fast Hierarchical Method for Common Illumination Recent advances in global illumination technology allow us to calculate the lighting efficiently, using hierarchical radiosity [16], clustering <ref> [24, 25] </ref> and incremental update methods [8]. To initialise the system, the calculation of certain basic parameters is required. We adopt many of the conventions used by Fournier et al. [14], adapting them appropriately to the application and the requirements at hand.
Reference: 26. <author> Andrei State, Gentaro Hirota, David T. Chen, Bill Garrett, and Mark Livingston. </author> <title> Superior augmented reality registration by integrating landmark tracking and magnetic tracking. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIGGRAPH 96 Conference Proceedings (New Orleans, LO), Annual Conference Series, </booktitle> <pages> pages 429-438. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: The main body of this research concentrates on the requirements of real-time systems [2]. In terms of illumination, these systems provide little, if any, common lighting information. Examples of work including some form of shadowing between real and synthetic objects are presented in <ref> [26] </ref> and [19]. Common illumination requires full 3D information, and thus should use explicit modeling of the real world objects. Similar requirements exist for the resolution of occlusion between real and virtual objects (e.g., [3]).
Reference: 27. <author> Richard Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The viewpoints lie approximately at the vertices of a 1-meter-wide vertical square in one corner of the room. To enlarge the field-of-view, we built panoramic images using mosaicing <ref> [27, 18] </ref>. At each viewpoint, we took three left-to-right images with an overlap of approximately 50% between two consecutive images. During this process, we were very careful at each viewpoint not to translate the camera but restrict motion to rotation.
Reference: 28. <author> Zhengyou Zhang, Rachid Deriche, Olivier Faugeras, and Quang-Tuan Luong. </author> <title> A robust technique for matching two uncalibrated images through the recovery of the unknown epipo-lar geometry. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 78(1-2):87-119, </volume> <year> 1994. </year>
Reference-contexts: In most cases these points indeed represent the same object as the reference point. If not, the user can manually correct the errors. Based on the point correspondences, we compute the fundamental matrices F 1i (see appendix 7) using the non-linear method described in <ref> [28] </ref>. The minimum number of (a) (b) (c) ... Fig. 1. Three original images, and the resulting mosaic (see text and also Colour Section). ... ...

References-found: 28

