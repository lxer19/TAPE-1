URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94417.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Performance Complexity of LU Factorization with Efficient Pipelining and Overlap on a Multiprocessor  
Author: F. Desprez and B. Tourancheau yz J. J. Dongarra 
Date: February 8, 1994  
Address: 46, Allee d'Italie 107, Ayres Hall 69364 Lyon Cedex 07 Knoxville, TN 37996-1301 France USA  107 Ayres Hall P.O. Box 2008, Bldg. 6012 Knoxville, TN 37996-1301 Oak Ridge, TN 37831-6367 USA USA  
Affiliation: LIP ENS Lyon The University of Tennessee URA 1398 du CNRS Computer Science Department  The University of Tennessee Oak Ridge National Laboratory Department of Computer Science Mathematical Sciences Section  
Abstract: In this paper, we make efficient use of pipelining on LU decomposition with pivoting and a column-scattered data decomposition to derive precise variations of the computational complexities. We then compare these results with experiments on the Intel iPSC/860 and Paragon machines.
Abstract-found: 1
Intro-found: 1
Reference: [BDL91] <author> R.H. Bisseling, L. Daniel, and J.C. Loyens. </author> <title> Towards Peak Parallel LINPACK Performance on 400 Transputers. Supercomputer, </title> <address> VIII-5(45):20-27, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: In [RTV89], the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations [RT88, DO90]. Blockwise distribution is introduced in [CDW92], and <ref> [BDL91, DGW92] </ref> show that the communication can be reduced by using a block-wrapped data decomposition of the data. To understand the effectiveness of these various methods and to be able to predict performance on a target machine, we must carry out an analytical analysis.
Reference: [Cap87] <author> P.R. Capello. </author> <title> Gaussian Elimination on a Hypercube Automaton. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 288-308, </pages> <year> 1987. </year>
Reference-contexts: If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [CDW92] <author> J. Choi, J.J. Dongarra, and D.W. Walker. </author> <title> The Design of Scalable Software Libraries for Distributed Memory Concurrent Computers. </title> <editor> In J.J. Dongarra and B. Tourancheau, editors, </editor> <booktitle> Environments and Tools For Parallel Scientific Computing. </booktitle> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: In [RTV89], the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations [RT88, DO90]. Blockwise distribution is introduced in <ref> [CDW92] </ref>, and [BDL91, DGW92] show that the communication can be reduced by using a block-wrapped data decomposition of the data. To understand the effectiveness of these various methods and to be able to predict performance on a target machine, we must carry out an analytical analysis. <p> Note that because we are not using a blocked version of the LU decomposition, we not could use the assembly-coded Level 3 BLAS routines and reach the performance of Choi et al. <ref> [CDW92] </ref>. 9 Conclusion and Future Work We have described an approach for analyzing the complexity of the parallel LU decomposition with a scattered-column data distribution in both synchronous and asynchronous communication protocols cases. We have presented a complexity analysis for each algorithm.
Reference: [CG87] <author> E. Chu and A. George. </author> <title> Gaussian Elimination with Partial Pivoting and Load Balancing on a Multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 65-74, </pages> <year> 1987. </year>
Reference-contexts: For example, <ref> [CG87] </ref> advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [CRT89] <author> M. Cosnard, Y. Robert, and B. Tourancheau. </author> <title> Evaluating speedups on distributed memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 10 </volume> <pages> 247-253, </pages> <year> 1989. </year>
Reference-contexts: If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [CTV87] <author> M. Cosnard, B. Tourancheau, and G. Villard. </author> <title> Gaussian elimination on message passing architecture. </title> <booktitle> In International Conference on Supercomputing, Patras, </booktitle> <address> Grece, June 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [DCDH90] <author> J.J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A Set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transaction on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: paper we call fi scal and t scal the parameters for the cost of the inversion of the pivot element, the search for the maximum of the vector, the interchange of these two elements, and the scale of the vector by a scalar using the Level 1 BLAS DSCAL function <ref> [DCDH90] </ref>.
Reference: [DFT93] <author> F. Desprez, P. Fraigniaud, and B. Tourancheau. </author> <title> Successive broadcasts on hypercube. </title> <type> Technical Report CS-93-210, </type> <institution> University of Tennessee, CS dept., </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: On the other hand, the synchronous theoretical timings are slightly worse than the experiment, 13 probably because of an overlap between communication and idle time resulting of the successive communication calls (an example of such an effect for spanning binomial tree braodcast in hypercube is studied in <ref> [DFT93] </ref>). The results were less accurate on the Paragon machine, because we were using version 1.1 of the multitasking operating system.
Reference: [DGW92] <author> J.J. Dongarra, R. Van De Geijn, and D.W. Walker. </author> <title> A Look at Dense Linear Algebra Libraries. </title> <type> Technical Report ORNL/TM-12126, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: In [RTV89], the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations [RT88, DO90]. Blockwise distribution is introduced in [CDW92], and <ref> [BDL91, DGW92] </ref> show that the communication can be reduced by using a block-wrapped data decomposition of the data. To understand the effectiveness of these various methods and to be able to predict performance on a target machine, we must carry out an analytical analysis. <p> Our experimental results also indicate that our method is efficient: we achieved 0.42 Gflops on the iPSC860 and 0.47 Gflops on the Paragon for a 7K matrix on 64 processors, compared with 1.34 Gflops of the ScaLAPACK implementation of the LU decomposition in the LINPACK benchmark of the iPSC860 <ref> [DGW92] </ref>. 15
Reference: [DO90] <author> J.J. Dongarra and S. Ostrouchov. </author> <title> LAPACK Working Note: LAPACK Block Factorization Algorithms on the Intel iPSC/860. </title> <type> Technical Report 24, </type> <institution> Department of Computer Science University of Tennessee, </institution> <year> 1990. </year>
Reference-contexts: If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage. <p> In [RTV89], the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations <ref> [RT88, DO90] </ref>. Blockwise distribution is introduced in [CDW92], and [BDL91, DGW92] show that the communication can be reduced by using a block-wrapped data decomposition of the data.
Reference: [Dun90] <author> T.H. Dunigan. </author> <title> Performance of the Intel iPSC/860 Hypercube. </title> <type> Technical Report TM-11491, </type> <institution> Oak Ridge National Laboratory, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: We use a linear model (fi+mt ) for its performance on vector of size m. In all our experiments we use 64-bit double precision floating-point arithmetic. This model corresponds roughly to the currently available commercial machines, such as the Intel iPSC/860 <ref> [Dun90] </ref> and Paragon [EK93, Int93].
Reference: [EK93] <author> R. Esser and R; Knetcht. </author> <title> Intel Paragon XP/S Architecture and Software Environment. </title> <type> Technical Report KFA-ZAM-IB-9305, </type> <institution> Zentralinstitut fur Angewandte Mathematik - Forschungszentrum Julich, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: We use a linear model (fi+mt ) for its performance on vector of size m. In all our experiments we use 64-bit double precision floating-point arithmetic. This model corresponds roughly to the currently available commercial machines, such as the Intel iPSC/860 [Dun90] and Paragon <ref> [EK93, Int93] </ref>.
Reference: [GL89] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computation. </title> <publisher> The John Hopkins University Press, </publisher> <year> 1989. </year> <note> Second edition. </note>
Reference-contexts: Hence, many systems can be solved by two triangular solves, Ly = b and U x = y. There are many versions of the LU algorithm depending on the way the three internal loops are nested <ref> [Rob90, GL89] </ref>.
Reference: [Int93] <institution> Intel Corporation - SSD, Beaverton, OR. </institution> <note> PARAGON OSF/1 - User's guide, </note> <month> April </month> <year> 1993. </year>
Reference-contexts: We use a linear model (fi+mt ) for its performance on vector of size m. In all our experiments we use 64-bit double precision floating-point arithmetic. This model corresponds roughly to the currently available commercial machines, such as the Intel iPSC/860 [Dun90] and Paragon <ref> [EK93, Int93] </ref>.
Reference: [LC89] <author> G. Li and T.F. Coleman. </author> <title> A New Method for Solving Triangular Systems on Distributed Memory Message Passing Multiprocessors. </title> <journal> SIAM Journal on Science and Statistical Computing, </journal> <volume> 10 </volume> <pages> 382-396, </pages> <year> 1989. </year> <month> 16 </month>
Reference-contexts: For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but <ref> [LC89] </ref> shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [LKK83] <author> R.E. Lord, J.S. Kowalik, and S.P. Kumar. </author> <title> Solving Linear Algebraic Equations on a MIMD Computer. </title> <journal> Journal of the ACM, </journal> <volume> 30(1) </volume> <pages> 103-117, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: In this paper, we study the critical path of the LU decomposition algorithm with column-scattered data distribution. For our path analysis, we use the notation of <ref> [LKK83] </ref>, where a task is an indivisible unit of computational activity and the precedence binary relation between tasks is denoted by . This relation is nonreflexive, antisymmetric, and transitive.
Reference: [PBKP92] <author> B.V. Purushotham, A. Basu, P.S. Kumar, and L.M. Patnaik. </author> <title> Performance Estimation of LU Factorisation on Message Passing Multiprocessors. </title> <journal> Parallel Processing Letters, </journal> <volume> 2(1) </volume> <pages> 51-60, </pages> <year> 1992. </year>
Reference-contexts: We study the kji, or "right looking," form, which is most suitable for parallel implementation (since the matrix can be distributed by columns, the pivoting is done easily inside each processor without communication <ref> [PBKP92] </ref>). * Task S k comprises the pivot search, interchange, and scaling of the elimination column. * Task U kj comprises the interchange of pivot elements and the updating of the remaining column j. (Note: the multipliers, L, are saved in the array A in place of the elements that would <p> This relation is nonreflexive, antisymmetric, and transitive. If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see <ref> [PBKP92] </ref> and the related works of [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90]). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage. <p> Hence, on most machines the complexity analysis will correspond to case 2. The execution time of the PR algorithm is then computed following the critical path and taking into account the idle times (note that we do not find the same total time of <ref> [PBKP92] </ref> since we take into account the idle times introduced by the pipeline strategy). As the pivot column needs to be transmitted to the next processor (which holds the next column with our scattered data distribution), the T com time is increased by a factor of two. <p> This overlap is realized inside a processor; it does not correspond to the overlap between processors steps described in <ref> [PBKP92] </ref>, which is the effect of pipeline algorithms. We will see that the execution time of the complete network can be obtained with the pipeline algorithm on a ring.
Reference: [Rob90] <author> Y. Robert. </author> <title> The Impact of Vector and Parallel Architectures on the Gaussian Elimination Algorithm. </title> <publisher> Manchester University Press - Manchester - UK, </publisher> <year> 1990. </year>
Reference-contexts: Hence, many systems can be solved by two triangular solves, Ly = b and U x = y. There are many versions of the LU algorithm depending on the way the three internal loops are nested <ref> [Rob90, GL89] </ref>. <p> of the remaining column j. (Note: the multipliers, L, are saved in the array A in place of the elements that would become zero in task S k .) 3 Parallel kji Version of the LU Algorithm with Column-scattered Data Distribution Parallel versions of the LU algorithm are described in <ref> [Saa86a, Saa86b, RTV89, Rob90] </ref>. Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. <p> Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. Two well-known methods for broadcast are the minimum spanning tree broadcast and the pipeline ring (unidirectional broadcast along the ring) algorithms <ref> [Saa86a, RTV89, Rob90] </ref>. <p> If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [RT88] <author> Y. Robert and B. Tourancheau. </author> <title> Block gaussian elimination on a hypercube vector multiprocessor. </title> <journal> Revista de Mathematicas Aplicadas, </journal> <volume> 10 </volume> <pages> 77-89, </pages> <year> 1988. </year> <institution> Universidad de Chile. </institution>
Reference-contexts: If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage. <p> In [RTV89], the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations <ref> [RT88, DO90] </ref>. Blockwise distribution is introduced in [CDW92], and [BDL91, DGW92] show that the communication can be reduced by using a block-wrapped data decomposition of the data.
Reference: [RTV89] <author> Y. Robert, B. Tourancheau, and G. Villard. </author> <title> Data allocation strategies for the gauss and jordan algorithms on a ring of processors. </title> <journal> Information Processing Letters, </journal> <volume> 31 </volume> <pages> 21-29, </pages> <year> 1989. </year>
Reference-contexts: of the remaining column j. (Note: the multipliers, L, are saved in the array A in place of the elements that would become zero in task S k .) 3 Parallel kji Version of the LU Algorithm with Column-scattered Data Distribution Parallel versions of the LU algorithm are described in <ref> [Saa86a, Saa86b, RTV89, Rob90] </ref>. Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. <p> Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. Two well-known methods for broadcast are the minimum spanning tree broadcast and the pipeline ring (unidirectional broadcast along the ring) algorithms <ref> [Saa86a, RTV89, Rob90] </ref>. <p> For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage. In <ref> [RTV89] </ref>, the panel-wrapped column-distribution method is proved to be efficient because it leads to a good load-balancing between computation and communication; and, in practice, this method has given good results with blocked computations [RT88, DO90].
Reference: [Saa86a] <author> Y. Saad. </author> <title> Communication Complexity of the Gaussian Elimination Algorithm on Multiprocessors. </title> <journal> Linear Algebra and Applications, </journal> <volume> 77 </volume> <pages> 315-340, </pages> <year> 1986. </year>
Reference-contexts: of the remaining column j. (Note: the multipliers, L, are saved in the array A in place of the elements that would become zero in task S k .) 3 Parallel kji Version of the LU Algorithm with Column-scattered Data Distribution Parallel versions of the LU algorithm are described in <ref> [Saa86a, Saa86b, RTV89, Rob90] </ref>. Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. <p> Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. Two well-known methods for broadcast are the minimum spanning tree broadcast and the pipeline ring (unidirectional broadcast along the ring) algorithms <ref> [Saa86a, RTV89, Rob90] </ref>.
Reference: [Saa86b] <author> Y. Saad. </author> <title> Gaussian Elimination on Hypercubes. </title> <editor> In M. Cosnard, Y. Robert, P. Quinton, and M. Tchuente, editors, </editor> <booktitle> Parallel Algorithms and Architectures. </booktitle> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: of the remaining column j. (Note: the multipliers, L, are saved in the array A in place of the elements that would become zero in task S k .) 3 Parallel kji Version of the LU Algorithm with Column-scattered Data Distribution Parallel versions of the LU algorithm are described in <ref> [Saa86a, Saa86b, RTV89, Rob90] </ref>. Depending on the topology, the difference between the methods is the manner in which the elimination column is sent to all the processors. <p> If we let A and B belong to the set of tasks, then A B means that task A must complete execution before B commences execution. 5 Related Work Numerous methods have been proposed for LU factorization (see [PBKP92] and the related works of <ref> [Saa86b, Cap87, CTV87, RT88, CRT89, DO90, Rob90] </ref>). For example, [CG87] advocates partial pivoting and load balancing in rowwise methods with a straightforward parallel triangular solve algorithm, but [LC89] shows that the parallel triangular solve algorithm can have the same performance with columnwise storage.
Reference: [SS89] <author> Y. Saad and M.H. Schultz. </author> <title> Data Communication in Parallel Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 115-135, </pages> <year> 1989. </year> <month> 17 </month>
Reference-contexts: The time needed to communicate a message of size m between two neighboring processors is modeled as the startup time fi com plus the length m times the throughput t com of the communication channel <ref> [SS89] </ref>. We assume that the messages are sent in one block. For arithmetic operations, the arithmetic logical unit of the CPU is assumed to be superscalar. We use a linear model (fi+mt ) for its performance on vector of size m.
References-found: 23

