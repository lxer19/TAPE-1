URL: http://www.cs.rochester.edu/u/meira/canpc97/paper.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/meira/canpc97/
Root-URL: 
Title: Understanding the Performance of DSM Applications  
Author: Wagner Meira Jr. Thomas J. LeBlanc Nikolaos Hardavellas Claudio Amorim 
Keyword: Alphas connected by a DEC Memory Channel network.  
Note: From Proceeedings of the Workshop on Communication and Architectural Support for Network-based Parallel Computing (CANPC' 97),  
Address: Rochester, Rochester NY 14627  Brazil 21945-970  San Antonio, TX,  
Affiliation: 1 Department of Computer Science, University of  COPPE Systems Engineering, UFRJ, Rio de Janeiro,  
Email: fmeira,leblanc,nikolaosg@cs.rochester.edu  amorim@cos.ufrj.br  
Phone: 2  
Date: February 97.  
Abstract: Carnival is a performance measurement and analysis tool that assists users in understanding the performance of DSM applications and protocols. Using traces of program executions, Carnival presents performance data as a hierarchy of execution profiles. During analysis, Carnival automates the inference process that relates performance phenomena to specific causes in the source code or DSM protocol using techniques that focus on the two most important sources of overhead in DSM systems: waiting time analysis identifies the causes of synchronization overhead, and produces an explanation for each source of waiting time in the program; communication analysis identifies the sequence of requests that result in invalidations, and produces an explanation for each source of communication. We describe these techniques and their implementation in TreadMarks, and show how to use waiting time analysis and communication analysis to improve the running time of two programs from the SPLASH application suite when executed on DEC 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. Treadmarks: </author> <title> shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: We instrumented the Treadmarks DSM system, and use Carnival for presentation and visualization. Treadmarks <ref> [1] </ref> is a DSM system for Unix systems developed at Rice University that uses a lazy release consistency pro-tocol [8] to reduce communication and false sharing. Carnival is a performance analysis and visualization tool developed at the University of Rochester.
Reference: 2. <author> R. Bianchini, L. Kontothanassis, R. Pinto, M. De Maria, M. Abud, and C. Amorim. </author> <title> Hiding communication latency and coherence overhead in software DSMs. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston,MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level [8, 7, 9] and the architecture level <ref> [5, 2, 3] </ref> have made DSM a practical and cost effective approach to parallel computing. Nonetheless, synchronization and communication are still major sources of performance degradation in DSM systems. ? This research was supported by NSF grant CCR-9510173, an NSF CISE Institutional Infrastructure Grant No.
Reference: 3. <author> M. Blumrich, C. Dubnicki, E. Felten, K. Li, and M. Mesarina. </author> <title> Virtual-memory--mapped network interfaces. </title> <journal> IEEE Micro, </journal> <volume> 15(2) </volume> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level [8, 7, 9] and the architecture level <ref> [5, 2, 3] </ref> have made DSM a practical and cost effective approach to parallel computing. Nonetheless, synchronization and communication are still major sources of performance degradation in DSM systems. ? This research was supported by NSF grant CCR-9510173, an NSF CISE Institutional Infrastructure Grant No.
Reference: 4. <author> T. Chilimbi, T. Ball, S. Eick, and J. Larus. Stormwatch: </author> <title> A tool for visualizing memory system protocols. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: There are two tools that focus on cause-and-effect relationships. Rajamony and Cox [14] implemented a performance debugger that automatically detects unnecessary and excessive synchronization by verifying data accesses between synchronization intervals. StormWatch <ref> [4] </ref> is a visualization tool for memory system protocols that presents multiple views of memory access operations, including performance slices that capture relationships between individual memory events, exposing causality in memory operations.
Reference: 5. <author> R. Gillett. </author> <title> Memory channel network for PCI. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 12-18, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level [8, 7, 9] and the architecture level <ref> [5, 2, 3] </ref> have made DSM a practical and cost effective approach to parallel computing. Nonetheless, synchronization and communication are still major sources of performance degradation in DSM systems. ? This research was supported by NSF grant CCR-9510173, an NSF CISE Institutional Infrastructure Grant No. <p> To use Carnival with Treadmarks, we only had to implement a global clock within Treadmarks (for recording timestamps) and define the relevant protocol events for our analysis. We implemented a global clock by broadcasting one processor's clock value using the DEC Memory Channel <ref> [5] </ref>. The accuracy of this global clock is on the order of tens of microseconds. The relevant events in Treadmarks include lock operations, barriers, page requests, and garbage collection. <p> All experiments were performed on a cluster of eight DEC Alpha Server 2100 nodes connected by a DEC Memory Channel <ref> [5] </ref>. Each Alpha Server node has four 233 MHz Alpha processors with 256 Mb of memory. Applications are linked to an instrumented implementation of Tread-marks (version 0.9.6), which employs DEC's implementation of TCP/IP on the Memory Channel. 4.1 Excessive synchronization in Water Fig. 1.
Reference: 6. <author> A. Goldberg and J. Hennessy. </author> <title> MTool:an integrated system for performance debugging shared memory multiprocessor applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 28-40, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: There are many tools that help the programmer in understanding and tuning the performance of parallel applications. Many tools identify the location of performance problems. For example, Paradyn [12] measures performance bottlenecks, and presents the resulting performance information in an abstraction hierarchy. MTool <ref> [6] </ref> measures the time spent by processors waiting for memory requests to be satisfied, and relates memory behavior to code segments. Mem-Spy [10] identifies the data structures that cause remote memory references, and classifies the misses into various categories, such as invalidation misses and replacement misses.
Reference: 7. <author> L. Iftode, C. Dubnicki, E. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computing Architecture. IEEE, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Software DSM (distributed shared memory) systems offer the simplicity of the shared-memory programming model on cost-effective distributed-memory architectures (including networks of workstations). Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level <ref> [8, 7, 9] </ref> and the architecture level [5, 2, 3] have made DSM a practical and cost effective approach to parallel computing.
Reference: 8. <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Software DSM (distributed shared memory) systems offer the simplicity of the shared-memory programming model on cost-effective distributed-memory architectures (including networks of workstations). Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level <ref> [8, 7, 9] </ref> and the architecture level [5, 2, 3] have made DSM a practical and cost effective approach to parallel computing. <p> We instrumented the Treadmarks DSM system, and use Carnival for presentation and visualization. Treadmarks [1] is a DSM system for Unix systems developed at Rice University that uses a lazy release consistency pro-tocol <ref> [8] </ref> to reduce communication and false sharing. Carnival is a performance analysis and visualization tool developed at the University of Rochester.
Reference: 9. <author> L. Kontothanassis and M. Scott. </author> <title> High performance software coherence for current and future architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29 </volume> <pages> 179-195, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Software DSM (distributed shared memory) systems offer the simplicity of the shared-memory programming model on cost-effective distributed-memory architectures (including networks of workstations). Although early DSM systems could only provide good performance for a limited class of applications, recent advances at both the protocol level <ref> [8, 7, 9] </ref> and the architecture level [5, 2, 3] have made DSM a practical and cost effective approach to parallel computing. <p> We are continuing to analyze applications using these techniques, to better understand the limits of our techniques, and to improve the way in which performance information is presented to the user by Carnival. Furthermore, we plan to compare the performance of applications under Treadmarks and Cashmere <ref> [9] </ref> (a DSM system under development at Rochester), and consider how best to apply our techniques to understanding tradeoffs in the protocols. Acknowledgements We would like to thank Sandhya Dwarkadas for her comments on this work.
Reference: 10. <author> M. Martonosi, A. Gupta, and T. Anderson. Memspy: </author> <title> Analyzing memory system bottlenecks in programs. Performance Evaluation Review, </title> <address> 20(1):1 - 12, </address> <month> June </month> <year> 1992. </year> <note> Reprint of a paper presented in Sigmetrics' 92. </note>
Reference-contexts: Many tools identify the location of performance problems. For example, Paradyn [12] measures performance bottlenecks, and presents the resulting performance information in an abstraction hierarchy. MTool [6] measures the time spent by processors waiting for memory requests to be satisfied, and relates memory behavior to code segments. Mem-Spy <ref> [10] </ref> identifies the data structures that cause remote memory references, and classifies the misses into various categories, such as invalidation misses and replacement misses.
Reference: 11. <author> W. Meira Jr., T. LeBlanc, and A. Poulos. </author> <title> Waiting time analysis and performance visualization in Carnival. </title> <booktitle> In Proceedings of SPDT96: SIGMETRICS Symposium on Parallel and Distributed Tools, </booktitle> <pages> pages 1-10, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: These differences may represent code segments that were executed by one processor but not the other, or communication operations that were required by one processor but not the other. Waiting time analysis is an automated technique that generates explanations for waiting time in an execution. (See <ref> [11] </ref> for a detailed description of waiting time analysis and its use in message-passing systems.) The implementation analyzes execution trace files, recording each occurrence of waiting time, and the set of basic blocks traversed by each processor leading up to a synchronization point. <p> Performance analysis with Carnival consists of four steps: (i) instrumentation, (ii) program execution, (iii) automated analysis, and (iv) visualization. During the instrumentation phase a preprocessor uses static information <ref> [11] </ref> or user hints, which identify the portions of the code where computation is replicated, to insert instrumentation calls into the application code, Each call records the occurrence of an important event, a timestamp, and the basic block (or data structure) in the source code where the event occurred. <p> The results of this analysis (both profiles and explanations) are examined via a Tcl/Tk [13] interface (see figures 1 and 2). More details about the visualization resources provided by Carnival can be found in <ref> [11] </ref>. The instrumentation library is the only architecture-dependent code in Carnival. To use Carnival with Treadmarks, we only had to implement a global clock within Treadmarks (for recording timestamps) and define the relevant protocol events for our analysis. <p> Users can quickly identify places in the code where the most time is spent by scrolling down the line numbers looking for the darkest portion of the scale. Carnival also provides other profiling displays, as described in <ref> [11] </ref>. Two pop-up windows are used for waiting time analysis.
Reference: 12. <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall. </author> <title> The Paradyn parallel performance measurement tool. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 37-46, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: There are many tools that help the programmer in understanding and tuning the performance of parallel applications. Many tools identify the location of performance problems. For example, Paradyn <ref> [12] </ref> measures performance bottlenecks, and presents the resulting performance information in an abstraction hierarchy. MTool [6] measures the time spent by processors waiting for memory requests to be satisfied, and relates memory behavior to code segments.
Reference: 13. <author> John K. Ousterhout. </author> <title> Tcl and Tk Toolkit. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: After execution, the Carnival preprocessor analyzes the trace files, producing a hierarchy of performance profiles, and explanations for various performance phenomena. The results of this analysis (both profiles and explanations) are examined via a Tcl/Tk <ref> [13] </ref> interface (see figures 1 and 2). More details about the visualization resources provided by Carnival can be found in [11]. The instrumentation library is the only architecture-dependent code in Carnival.
Reference: 14. <author> R. Rajamony and A. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the 4th International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of COmputer and Telecommunication Systems (MASCOTS), </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: There are two tools that focus on cause-and-effect relationships. Rajamony and Cox <ref> [14] </ref> implemented a performance debugger that automatically detects unnecessary and excessive synchronization by verifying data accesses between synchronization intervals.
Reference: 15. <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Applications are linked to an instrumented implementation of Tread-marks (version 0.9.6), which employs DEC's implementation of TCP/IP on the Memory Channel. 4.1 Excessive synchronization in Water Fig. 1. Carnival visualization of Water Our first example examines Water, a molecular dynamics simulation from the Splash suite <ref> [15] </ref> that is distributed as part of the Treadmarks release. Wa ter evaluates forces and potentials that occur over time in a system of water molecules. It uses one large, shared array to represent the molecules being simulated.
Reference: 16. <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <title> ACM. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: This modification, which reduces significantly the number of lock acquire operations and was al-ready incorporated into Water in the Splash2 suite <ref> [16] </ref>, improves the execution time by a factor of 17 on four processors. The original version of Water was written for a shared-memory machine, where lock operations are relatively cheap and excessive synchronization is a small price to pay for simplicity. <p> Carnival visualization of Ocean Our second example examines Ocean, an application in the Splash2 suite <ref> [16] </ref> that models large-scale ocean movements based on eddy and boundary currents. The original Splash2 code was ported to Treadmarks by colleagues at the Federal University in Rio de Janeiro without any changes to the data layout scheme. <p> An examination of the two loops reveals that the boundary conditions do not overlap among processors, and therefore we attribute the MPMC behavior to false sharing. The Splash2 implementation of Ocean adopts a tiling allocation policy to improve the communication-to-computation ratio <ref> [16] </ref>. Under this allocation, less than two percent of all accesses are to boundary entries shared with another processor. However, using a tiling allocation of sub-matrices of 500K each, coupled with the 8K page size in Treadmarks, means that every access to multi under Treadmarks is a shared access.
References-found: 16

