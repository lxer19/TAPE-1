URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/tcm/www/tcm_papers/inf_mem_isca96.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: 
Email: horowitz@ee.stanford.edu martonosi@princeton.edu tcm@eecg.toronto.edu smith@eecs.harvard.edu  
Title: Informing Memory Operations: Providing Memory Performance Feedback in Modern Processors  
Author: Mark Horowitz Margaret Martonosi Todd C. Mowry Michael D. Smith 
Affiliation: Computer Systems Department of Department of Electrical Division of Laboratory Electrical Engineering and Computer Engineering Applied Sciences Stanford University Princeton University University of Toronto Harvard University  
Abstract: Memory latency is an important bottleneck in system performance that cannot be adequately solved by hardware alone. Several promising software techniques have been shown to address this problem successfully in specific situations. However, the generality of these software approaches has been limited because current architectures do not provide a fine-grained, low-overhead mechanism for observing and reacting to memory behavior directly. To fill this need, we propose a new class of memory operations called informing memory operations, which essentially consist of a memory operation combined (either implicitly or explicitly) with a conditional branch-and-link operation that is taken only if the reference suffers a cache miss. We describe two different implementations of informing memory operationsone based on a cache-outcome condition code and another based on low-overhead trapsand find that modern in-order-issue and out-of-order-issue superscalar processors already contain the bulk of the necessary hardware support. We describe how a number of software-based memory optimizations can exploit informing memory operations to enhance performance, and look at cache coherence with fine-grained access control as a case study. Our performance results demonstrate that the runtime overhead of invoking the informing mechanism on the Alpha 21164 and MIPS R10000 processors is generally small enough to provide considerable exibility to hardware and software designers, and that the cache coherence application has improved performance compared to other current solutions. We believe that the inclusion of informing memory operations in future processors may spur even more innovative performance optimizations. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 95] <author> A. Agarwal, R. Bianchini, D. Chaiken, et al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> Proc. 22nd Annual Intl. Symp. on Computer Architecture. </booktitle> <month> Jun, </month> <year> 1995. </year>
Reference: [ACC+90] <author> R. Alverson, D. Callahan, D. Cummings, et al. </author> <title> The Tera Computer System. </title> <booktitle> Intl. Conference Supercomputing. </booktitle> <pages> pp 1-6. </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: Since we will want to tailor a prefetching response to its context within the program, the miss handler address is likely to change frequently. 4.1.3 Software-Controlled Multithreading Multithreading tolerates memory latency by switching from one thread (or context) to another at the start of a cache miss <ref> [Smi81, AKK+93, LGH94, TE94, ACC+90] </ref>. Multithreading implementations to date have generally relied upon hardware to manage and switch between threads. However, informing memory operations enable a software-based approach where a single miss handler could save and restart threads (all under software control) upon cache misses.
Reference: [AKK+93] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, et al. Sparcle: </author> <title> 11 An Evolutionary Processor Design for Large-Scale Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pp 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Since we will want to tailor a prefetching response to its context within the program, the miss handler address is likely to change frequently. 4.1.3 Software-Controlled Multithreading Multithreading tolerates memory latency by switching from one thread (or context) to another at the start of a cache miss <ref> [Smi81, AKK+93, LGH94, TE94, ACC+90] </ref>. Multithreading implementations to date have generally relied upon hardware to manage and switch between threads. However, informing memory operations enable a software-based approach where a single miss handler could save and restart threads (all under software control) upon cache misses.
Reference: [AKL79] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic Program Transformations for Virtual Memory Computers. </title> <booktitle> Proc. 1979 National Computer Conf. </booktitle> <pages> pp 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference: [BLA+94] <author> M. A. Blumrich, K. Li, R. Alpert, et al. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> Proc. 21st Intl. Symp. on Computer Architecture. </booktitle> <pages> pp 141-153. </pages> <month> Apr., </month> <year> 1994. </year>
Reference-contexts: When remote protocol operations are needed, a handler running on one processor will need to induce an action (a cache invalidation) at another node. In our experiments, we assume remote operations are accomplished without interrupting the remote processore.g., using a user-level DMA engine and network interface per compute node <ref> [BLA+94] </ref>. 7 4.3.2 Results We present performance results for an informing memory operations implementation of access control, assuming the low-overhead cache miss trap scheme described in Sections 2 and 3.
Reference: [BLRC94] <author> B. N. Bershad, D. Lee, T. H. Romer, and J. B. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> Proc. 6th Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp 158170, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: With memory system observation becoming more important, machine designers are providing at least limited hardware support for it. For example, hardware bus monitors have been used by some applications to gather statistics about references visible on an external bus <ref> [CDV+94, SG94, BLRC94, BM89] </ref>. A bus monitors architectural independence allows for exible implementations, but also can result in high overhead to access the monitoring hardware. Furthermore, such monitors only observe memory behavior beyond the second or even third level cache.
Reference: [BM89] <author> H. Burkhart and R. Millen. </author> <title> Performance-Measurement Tools in a Multiprocessor Environment. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38(5):725737, </volume> <month> May </month> <year> 1989. </year> <note> [CDV + 94] R. </note> <author> Chandra, S. Devine, B. Verghese, et al. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> Proc. Sixth Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp 1224, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: With memory system observation becoming more important, machine designers are providing at least limited hardware support for it. For example, hardware bus monitors have been used by some applications to gather statistics about references visible on an external bus <ref> [CDV+94, SG94, BLRC94, BM89] </ref>. A bus monitors architectural independence allows for exible implementations, but also can result in high overhead to access the monitoring hardware. Furthermore, such monitors only observe memory behavior beyond the second or even third level cache. <p> We address both of these issues in our discussion of each technique, and we quantify their impact on performance in Section 4.2. 4.1.1 Performance Monitoring Performance monitoring tools collect detailed information to guide either the programmer or the compiler in identifying and eliminating memory performance bottlenecks <ref> [BM89, GH93, LW94, MGA95] </ref>. A major difficulty with such tools is how to collect sufficiently detailed information quickly and without perturbing the monitored program.
Reference: [CMCH91] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> Proc. </booktitle> <volume> Microcomputing 24, </volume> <year> 1991. </year>
Reference-contexts: The actual number of dynamic instructions can vary depending on whether the hash probe hits. 4.1.2 Software-Controlled Prefetching Software-controlled prefetching tolerates memory latency by moving data lines into the cache before they are needed <ref> [MLG92, Por89, CMCH91] </ref>. A major challenge with prefetching is predicting which dynamic references are likely to miss, since indiscriminately prefetching all the time results in too much overhead [MLG92]. Informing memory operations can address this problem in two ways.
Reference: [CMM+88] <author> R. C. Covington, S. Madala, V. Mehta, et al. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> Proc. 1988 ACM Sigmetrics Conf. on Measurement and Modeling of Computer Systems. </booktitle> <pages> pp 4-11. </pages> <month> May, </month> <year> 1988. </year>
Reference: [Dix92] <author> Kaivalya M. Dixit. </author> <title> New CPU Benchmark Suites from SPEC. </title> <booktitle> Proc. COMPCON, </booktitle> <month> Spring </month> <year> 1992. </year>
Reference-contexts: The parameters of our two machine models are shown in Table 1. We simulated fourteen SPEC92 benchmarks (five integer and nine oating-point) <ref> [Dix92] </ref>, all of which were compiled with -O2 using the standard MIPS compilers under IRIX 5.3. We simulate 1, 10, and 100-instruction generic miss handlers, and we pessimistically assume that all instructions within the handlers are data-dependent on each other (hence a 10-instruction handler requires 10 cycles to execute).
Reference: [DBKF90] <author> J. Dongarra, O. Brewer, J. A. Kohl and S. Fineberg. </author> <title> A Tool to Aid in the Design, Implementation, and Understanding of Matrix Algorithms for Parallel Processors. </title> <booktitle> Jour. of Parallel and Distributed Computing, </booktitle> <pages> pp 185-202. </pages> <month> Jun, </month> <year> 1990. </year>
Reference: [DEC92] <institution> Digital Equipment Corp. </institution> <note> DECChip 21064 RISC Microprocessor Preliminary Data Sheet. Technical report, </note> <year> 1992. </year>
Reference-contexts: For example, the Pentium processor has several performance counters, including reference and In: Proceedings of the 23rd Annual International Symposium on Computer Architecture. May, 1996. 2 cache miss counters, and the MIPS R10000 and Alpha 21064 and 21164 also include memory performance counters <ref> [JHei95, DEC92, ERB+95, Mat94] </ref>. Compared to bus monitors, on-chip counters allow more fine-grained views of cache memory behavior. Unfortunately, it is still difficult to use these counters to determine if a particular reference hits or misses in the cache.
Reference: [ECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, K. E. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> Proc. 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pp 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: User level operations cause DMAs at another processor, and these induce invalidations of data cached at the remote node. Similar functionality could be implemented using a message passing machine and active messages <ref> [ECGS92] </ref>. TABLE 2. Machine and experiment parameters for different access control methods.
Reference: [ERB+95] <author> J. H. Edmonson, P. I. Rubenfeld, P. J. Bannon, et al. </author> <title> Internal Organization of the Alpha 21164, a 300 MHz 64-bit Quad-issue CMOS RISC Microprocessor. Digital Tech. </title> <journal> Journal, </journal> <volume> 7(1): </volume> <pages> 119-135, </pages> <year> 1995. </year>
Reference-contexts: For example, the Pentium processor has several performance counters, including reference and In: Proceedings of the 23rd Annual International Symposium on Computer Architecture. May, 1996. 2 cache miss counters, and the MIPS R10000 and Alpha 21064 and 21164 also include memory performance counters <ref> [JHei95, DEC92, ERB+95, Mat94] </ref>. Compared to bus monitors, on-chip counters allow more fine-grained views of cache memory behavior. Unfortunately, it is still difficult to use these counters to determine if a particular reference hits or misses in the cache. <p> (using the Alpha 21164 as an example) and then describe the implementation for an out-of-order machine (such as MIPS R10000). 3.1 In-order-issue Machines To be more concrete about the required hardware, this section describes how low-overhead cache miss traps could be added to the 21164 implementation of the Alpha architecture <ref> [ERB+95] </ref>. The 21164 is a superscalar machine that can execute up to 4 instructions per cycle. Its pipeline is shown in Figure 1; integer operations complete in 6 stages. The machine uses an interesting stall model. <p> Address; Begin Dcache read End DCache read S8 S10S9 S11 S12 Use DCache data (hit); Write store data to DCache. L2 Cache tag access L2 Cache data access DCache Fill Use L2 Cache Data FIGURE 1. Alpha 21164 integer and memory access pipeline stages <ref> [ERB+95] </ref>. 6 load is squashed (for either reason listed above), the address in the MSHR is used to invalidate the line in the cache (i.e., change the tag state) before the MSHR is freed for reuse.
Reference: [FJ94] <author> K. Farkas and N. Jouppi. </author> <title> Complexity/Performance Tradeoffs with Non-Blocking Loads, </title> <booktitle> Proc. 21st Annual Intl. Symp. on Computer Architecture. </booktitle> <pages> pp 211-222. </pages> <month> April, </month> <year> 1994. </year>
Reference-contexts: To implement our invalidate mechanism, we extend the lifetime of the Miss Status Handling Registers (MSHR), the data structure used to track the outstanding misses in a lockup-free cache <ref> [FJ94] </ref>. This structure contains the address of the miss, the destination register, and other bookkeeping information.
Reference: [GH93] <author> Aaron J. Goldberg and John L. Hennessy. </author> <title> Mtool: An Integrated System for Performance Debugging Shared Memory Multiprocessor Applications. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <pages> pp 2840, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: We address both of these issues in our discussion of each technique, and we quantify their impact on performance in Section 4.2. 4.1.1 Performance Monitoring Performance monitoring tools collect detailed information to guide either the programmer or the compiler in identifying and eliminating memory performance bottlenecks <ref> [BM89, GH93, LW94, MGA95] </ref>. A major difficulty with such tools is how to collect sufficiently detailed information quickly and without perturbing the monitored program.
Reference: [GJMS87] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The Impact of Hierarchical Memory Systems on Linear Algebra Algorithm Design. </title> <type> Technical Report UIUCSRD 625, </type> <institution> Univ. of Illinois, </institution> <year> 1987. </year>
Reference: [HMMS95] <author> M. Horowitz, M. Martonosi, T. C. Mowry, and M. D. Smith. </author> <title> Informing Loads: Enabling Software to Observe and React to Memory Behavior. </title> <type> Stanford CSL Technical Report CSL-TR-95-673. </type> <institution> Stanford University. </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: By allowing the branch-and-link operation to test the value of this state bit, we provide the software with a mechanism to react to memory system activity. Though simple, this mechanism offers quicker control transfers than current cache miss counters. Our second method (evaluated more fully in an earlier study <ref> [HMMS95] </ref>) removes the explicit user state for the hit/miss information, but retains the explicit dispatch instruction. In this case, the machine notifies software that the informing operation was a cache hit by squashing the instruction in the issue slot following that informing operation. <p> A previous study demonstrated that informing memory operations can be used to collect precise per-reference miss rates with low runtime overheads (less than 25%) and tolerable data cache perturbations <ref> [HMMS95] </ref>. This tool uses a single miss handler containing roughly 10 instructions 3 to increment a hash table entry based on the branch-and-link return address (available in the MHRR), thus distinguishing all static references. <p> All of these approaches were evaluated in a previous study and were shown to be useful in improving prefetching performance <ref> [HMMS95] </ref>. The size of a miss handler for prefetching is likely to be small (less than 10 instructions) since it is either launching a handful of prefetches or else recording some simple statistics.
Reference: [JHei95] <author> Joe Heinrich. </author> <title> MIPS R10000 Microprocessor Users Manual. </title> <year> 1995. </year>
Reference-contexts: For example, the Pentium processor has several performance counters, including reference and In: Proceedings of the 23rd Annual International Symposium on Computer Architecture. May, 1996. 2 cache miss counters, and the MIPS R10000 and Alpha 21064 and 21164 also include memory performance counters <ref> [JHei95, DEC92, ERB+95, Mat94] </ref>. Compared to bus monitors, on-chip counters allow more fine-grained views of cache memory behavior. Unfortunately, it is still difficult to use these counters to determine if a particular reference hits or misses in the cache.
Reference: [Jou90] <author> Norm Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> Proc. 17th Annual Intl. Symposium on Computer Architecture, </booktitle> <pages> pp 364373, </pages> <month> May </month> <year> 1990. </year> <note> [KOH + 94] J. </note> <author> Kuskin, D. Ofelt, M. Heinrich, et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> Proc. 21st Annual Intl. Symposium on Computer Architecture, </booktitle> <pages> pp 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To cope with memory latency, most computer systems today rely on their cache hierarchy to reduce the effective memory access time. While caches are an important step toward addressing this problem, neither they nor other purely hardware-based mechanisms (e.g., stream buffers <ref> [Jou90] </ref>) are complete solutions. In addition to hardware mechanisms, a number of promising software techniques have been proposed to avoid or tolerate memory latency. These software techniques have resorted to a variety of different approaches for gathering information and reasoning about memory performance.
Reference: [LGH94] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A Multithreading Technique Targeting Multiprocessors and Workstations. </title> <booktitle> Sixth Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp 308318, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Since we will want to tailor a prefetching response to its context within the program, the miss handler address is likely to change frequently. 4.1.3 Software-Controlled Multithreading Multithreading tolerates memory latency by switching from one thread (or context) to another at the start of a cache miss <ref> [Smi81, AKK+93, LGH94, TE94, ACC+90] </ref>. Multithreading implementations to date have generally relied upon hardware to manage and switch between threads. However, informing memory operations enable a software-based approach where a single miss handler could save and restart threads (all under software control) upon cache misses.
Reference: [LW94] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <booktitle> IEEE Computer, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We address both of these issues in our discussion of each technique, and we quantify their impact on performance in Section 4.2. 4.1.1 Performance Monitoring Performance monitoring tools collect detailed information to guide either the programmer or the compiler in identifying and eliminating memory performance bottlenecks <ref> [BM89, GH93, LW94, MGA95] </ref>. A major difficulty with such tools is how to collect sufficiently detailed information quickly and without perturbing the monitored program.
Reference: [Mat94] <author> Terje Mathison. </author> <title> Pentium Secrets. </title> <journal> Byte, </journal> <pages> pp 191-192, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: For example, the Pentium processor has several performance counters, including reference and In: Proceedings of the 23rd Annual International Symposium on Computer Architecture. May, 1996. 2 cache miss counters, and the MIPS R10000 and Alpha 21064 and 21164 also include memory performance counters <ref> [JHei95, DEC92, ERB+95, Mat94] </ref>. Compared to bus monitors, on-chip counters allow more fine-grained views of cache memory behavior. Unfortunately, it is still difficult to use these counters to determine if a particular reference hits or misses in the cache.
Reference: [MGA95] <author> M. Martonosi, A. Gupta, and T. E. Anderson. </author> <title> Tuning Memory Performance of Sequential and Parallel Programs. </title> <booktitle> IEEE Computer, </booktitle> <pages> pp 32-40. </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: We address both of these issues in our discussion of each technique, and we quantify their impact on performance in Section 4.2. 4.1.1 Performance Monitoring Performance monitoring tools collect detailed information to guide either the programmer or the compiler in identifying and eliminating memory performance bottlenecks <ref> [BM89, GH93, LW94, MGA95] </ref>. A major difficulty with such tools is how to collect sufficiently detailed information quickly and without perturbing the monitored program.
Reference: [MLG92] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proc. 5th Intl Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp 6273, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In addition to hardware mechanisms, a number of promising software techniques have been proposed to avoid or tolerate memory latency. These software techniques have resorted to a variety of different approaches for gathering information and reasoning about memory performance. Compiler-based techniques, such as cache blocking [AKL79,GJMS87,WL91] and prefetching <ref> [MLG92, Por89] </ref> use static program analysis to predict which references are likely to suffer misses. Memory performance tools have relied on sampling or simulation-based approaches to gather memory statistics [CMM+88,DBKF90,GH93,LW94,MGA95]. Operating systems have used coarse-grained system information to reduce latencies by adjusting page coloring and migration strategies [BLRC94,CDV + 94]. <p> The actual number of dynamic instructions can vary depending on whether the hash probe hits. 4.1.2 Software-Controlled Prefetching Software-controlled prefetching tolerates memory latency by moving data lines into the cache before they are needed <ref> [MLG92, Por89, CMCH91] </ref>. A major challenge with prefetching is predicting which dynamic references are likely to miss, since indiscriminately prefetching all the time results in too much overhead [MLG92]. Informing memory operations can address this problem in two ways. <p> A major challenge with prefetching is predicting which dynamic references are likely to miss, since indiscriminately prefetching all the time results in too much overhead <ref> [MLG92] </ref>. Informing memory operations can address this problem in two ways. The first option is to recompile for a subsequent run based on a detailed memory profile captured from earlier runs.
Reference: [NAB + 94] <author> A. Nowatzyk, G. Aybay, M. Browne, et al. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> Proc. 27th Hawaii Intl. Conf. on System Sciences Vol. I: Architecture. </booktitle> <pages> pp 144-53. </pages> <month> Jan, </month> <year> 1994. </year>
Reference: [Pau94] <author> Richard Paul. </author> <title> SPARC Architecture, Assembly Language Programming, </title> & <address> C. </address> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Second, the overhead of saving and restoring register state could be minimized through compiler optimizations (e.g., statically partition the register set amongst threads, only save or restore registers that are live, etc.), or perhaps through hardware support (e.g., something similar to the SPARC register windows <ref> [Pau94] </ref>).
Reference: [Por89] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: In addition to hardware mechanisms, a number of promising software techniques have been proposed to avoid or tolerate memory latency. These software techniques have resorted to a variety of different approaches for gathering information and reasoning about memory performance. Compiler-based techniques, such as cache blocking [AKL79,GJMS87,WL91] and prefetching <ref> [MLG92, Por89] </ref> use static program analysis to predict which references are likely to suffer misses. Memory performance tools have relied on sampling or simulation-based approaches to gather memory statistics [CMM+88,DBKF90,GH93,LW94,MGA95]. Operating systems have used coarse-grained system information to reduce latencies by adjusting page coloring and migration strategies [BLRC94,CDV + 94]. <p> The actual number of dynamic instructions can vary depending on whether the hash probe hits. 4.1.2 Software-Controlled Prefetching Software-controlled prefetching tolerates memory latency by moving data lines into the cache before they are needed <ref> [MLG92, Por89, CMCH91] </ref>. A major challenge with prefetching is predicting which dynamic references are likely to miss, since indiscriminately prefetching all the time results in too much overhead [MLG92]. Informing memory operations can address this problem in two ways.
Reference: [RLW94] <author> S. K. Reinhardt, J. R. Larus and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> Proc. 21st Intl. Symp. on Computer Architecture. </booktitle> <pages> pp 325-337. </pages> <month> Apr., </month> <year> 1994. </year> <note> [SFL + 94] I. </note> <author> Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-Grain Access Control for Distributed Shared Memory. </title> <booktitle> Proc 6th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems. </booktitle> <pages> pp 297-306. </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Our informing memory approach does not rely on hardware specialized for multi-process access control, and for this reason, we compare ourselves to two published approaches that similarly do not rely on special access control hardware. Systems with hardware support for cache coherence [KOH + 94] or access control <ref> [RLW94] </ref>, offer better perfor mance, but at the cost of more (and more specialized) hardware. FIGURE 4. Normalized execution times for three access control methods. 5 Conclusions Informing memory operations are a general primitive for allowing software to observe and react to its own memory referencing behavior.
Reference: [SG94] <author> Ashok Singhal and Aaron J. Goldberg. </author> <title> Architectural Support for Performance Tuning: A Case Study on the SPARCcenter 2000. </title> <booktitle> Proc. 21st Annual Intl. Symp. on Computer Architecture. </booktitle> <pages> pp 325-337. </pages> <month> April, </month> <year> 1994. </year>
Reference-contexts: With memory system observation becoming more important, machine designers are providing at least limited hardware support for it. For example, hardware bus monitors have been used by some applications to gather statistics about references visible on an external bus <ref> [CDV+94, SG94, BLRC94, BM89] </ref>. A bus monitors architectural independence allows for exible implementations, but also can result in high overhead to access the monitoring hardware. Furthermore, such monitors only observe memory behavior beyond the second or even third level cache.
Reference: [Smi81] <author> B. J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> SPIE Real-Time Signal Processing IV, </booktitle> <volume> Vol. 298, </volume> <year> 1981. </year>
Reference-contexts: Since we will want to tailor a prefetching response to its context within the program, the miss handler address is likely to change frequently. 4.1.3 Software-Controlled Multithreading Multithreading tolerates memory latency by switching from one thread (or context) to another at the start of a cache miss <ref> [Smi81, AKK+93, LGH94, TE94, ACC+90] </ref>. Multithreading implementations to date have generally relied upon hardware to manage and switch between threads. However, informing memory operations enable a software-based approach where a single miss handler could save and restart threads (all under software control) upon cache misses.
Reference: [TE94] <author> R. Thekkath and S. J. Eggers. </author> <title> The Effectiveness of Multiple Hardware Contexts. </title> <booktitle> 6th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp 328-337, </pages> <year> Oct.1994. </year>
Reference-contexts: Since we will want to tailor a prefetching response to its context within the program, the miss handler address is likely to change frequently. 4.1.3 Software-Controlled Multithreading Multithreading tolerates memory latency by switching from one thread (or context) to another at the start of a cache miss <ref> [Smi81, AKK+93, LGH94, TE94, ACC+90] </ref>. Multithreading implementations to date have generally relied upon hardware to manage and switch between threads. However, informing memory operations enable a software-based approach where a single miss handler could save and restart threads (all under software control) upon cache misses.
Reference: [TL94] <author> C. A. Thekkath and H. M. Levy. </author> <title> Hardware and Software Support for Efficient Exception Handling. </title> <booktitle> Proc 6th Intl Conf. on Architectural Support for Programming Languages and Operating Systems. </booktitle> <pages> pp 110-119. </pages> <month> Oct. </month> <year> 1994. </year>
Reference: [WL91] <author> M. E. Wolf and M. S. Lam. </author> <title> A Data Locality Optimization Algorithm. </title> <booktitle> Proc. SIGPLAN 91 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp 30-44, </pages> <month> June </month> <year> 1991. </year>
References-found: 34

