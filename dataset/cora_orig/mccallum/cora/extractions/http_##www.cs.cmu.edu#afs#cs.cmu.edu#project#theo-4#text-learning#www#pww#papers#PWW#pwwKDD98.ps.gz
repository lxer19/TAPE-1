URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/papers/PWW/pwwKDD98.ps.gz
Refering-URL: 
Root-URL: 
Email: E-mail: Marko.Grobelnik@ijs.si, Dunja.Mladenic@ijs.si  
Phone: Phone: (+386)(61) 1773 272, Fax: (+386)(61) 1258-158  
Title: Fast text categorization  
Author: Marko Grobelnik and Dunja Mladenic 
Date: March 27, 1998  
Address: Jamova 39, 1111 Ljubljana, Slovenia  
Affiliation: Department of Intelligent Systems, J.Stefan Institute,  
Abstract: We present an approach to text categorization using machine learning/data mining techniques. The approach is developed and tested on large text hierarchy named Yahoo available on the Web. The goal is to classify an arbitrary text document as accurate and as fast as possible to the right category within Yahoo hierarchy. To achieve this, we have to handle large number of features (words, word sequences) and training examples (items in the Yahoo nodes) by taking into account hierarchical structure of examples and using feature subset selection adapted for large text data. In our previous work we show that a rather high quality of classification can be achieved. Here our main concern is to classify as fast as possible while keeping classification quality still high. Classification of a document is performed by collecting votes from the category nodes. To achieve fast classification the number of category nodes involved in the voting process should be reduced. Our experiments are performed using naive Bayesian classifier on text data using feature-vector document representation. Experimental evaluation on five domains taken from the Yahoo hierarchy shows that the reduction to few percent of all candidate categories results in either an improvement or no loss in the system performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Verkamo, A.I., </author> <year> 1996. </year> <title> Fast Discovery of Association Rules, </title> <editor> In Fayyad, U.M., Piatetsky-Shapiro, G., Smyth, P., Uthurusamy, R. (eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining AAAI Press/The MIT Press, </booktitle> <pages> pp. 307|328. </pages>
Reference-contexts: Each new pass generates features of length i only from the candidate features (of length i1) generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm <ref> [1] </ref>. 3 3 Learning text classifier In order to handle large number of examples and features we divide the whole problem into subproblems as suggested in [5] for hierarchical document classification. For a new document, the classifier returns probability distribution over categories included in the hierarchy.
Reference: [2] <author> Domingos, P., Pazzani, M., </author> <year> 1997. </year> <title> On the Optimality of the Simple Bayesian Classifier under Zero-One Loss, Machine Learning 29, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 103|130. </pages>
Reference-contexts: The assumption about feature independence used in naive Bayesian classifier is here incorrect, especially when features representing several words are added. According to <ref> [2] </ref> this does not necessary mean that classifier will have poor performance because of that.
Reference: [3] <author> Filo, D., Yang, J., </author> <year> 1997. </year> <institution> Yahoo! Inc., </institution> <year> 1997. </year> <note> http://www.yahoo.com/docs/pr/ </note>
Reference: [4] <author> Grobelnik, M., Mladenic, D., </author> <title> Learning Machine: design and impelmentation, </title> <type> Technical Report IJS-DP-7824, </type> <institution> Department for Intelligent Systems, J.Stefan Institute, </institution> <year> 1998. </year>
Reference-contexts: all the selected features) to 0.4 (meaning consider as many best features as needed that their summary score represents 40% of the summary score assigned to all the selected features). 4 Experimental results Experimental evaluation of the developed approach is performed using our recently developed machine learning system Learning Machine <ref> [4] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and collecting data from the Web. We get our categorization results from the set of independent classifiers, each potentially having different number of features.
Reference: [5] <author> Koller, D., Sahami, M., </author> <title> Hierarchically classifying documents using very few words, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 170|178, </pages> <year> 1997. </year>
Reference-contexts: This process is similar to the large k-itemset generation used in association rules algorithm [1]. 3 3 Learning text classifier In order to handle large number of examples and features we divide the whole problem into subproblems as suggested in <ref> [5] </ref> for hierarchical document classification. For a new document, the classifier returns probability distribution over categories included in the hierarchy.
Reference: [6] <author> Lewis, D.D. </author> <year> (1995). </year> <title> Evaluating and optimizating autonomous text classification systems. </title> <booktitle> Proceedings of the 18th Annual International ACM-SIGIR Conference on Recsearch and Development in Information Retrieval (pp.246-254). </booktitle>
Reference-contexts: The vector size is varied from including 0.25 to 11 times the number of features that occur in positive examples. We report F 2 -measure on the keywords. F 2 -measure <ref> [6] </ref> is a combination of precision and recall commonly used in information retrieval, where recall is twice as important as precision: F 2 = (1+2 2 )P recisionfiRecall 2 2 P recision+Recall .
Reference: [7] <author> Mitchell, </author> <title> T.M., Machine Learning, </title> <publisher> The McGraw-Hill Companies, Inc., </publisher> <year> 1997. </year>
Reference-contexts: The final result of learning is a set of specialized classifiers. Classification of a new example includes consulting a specialized classifier for each of the categories. Learning algorithm used in our experiments is based on naive Bayesian classifier on feature-vector as described in <ref> [7] </ref>. In this approach documents are represented with frequency vectors where a feature is defined for each word position in the document having word at that position as its value.
Reference: [8] <author> Mladenic, D., </author> <title> Feature subset selection in text-learning, </title> <booktitle> Proc. of the 10th European Conference on Machine Learning ECML98, </booktitle> <year> 1998. </year> <editor> [9] van Rijsbergen, C.J,. Harper, D.J., Porter, M.F., </editor> <title> The selection of good search terms, </title> <booktitle> Information Processing & Management, 17, </booktitle> <address> pp.77|91, </address> <year> 1981. </year>
Reference-contexts: In this way we can capture some characteristic word combinations but also increase the number of features (eg. in whole Yahoo hierarchy from 69,280 features for 1-grams to 255,602 features for 5-grams). We additionally apply feature subset selection as commonly used on text data eg. <ref> [8] </ref>, [11]. In this approach a score is assigned to each feature independently, features are sorted according to the assigned score and a predefined number of the best features is taken to form the solution feature subset.
Reference: [10] <author> Shaw Jr, W.M., </author> <title> Term-relevance computations and perfect retrieval performance, </title> <booktitle> Information Processing & Management, 31(4), </booktitle> <address> pp.491|498, </address> <year> 1995. </year>
Reference-contexts: We handle singularities as proposed in <ref> [10] </ref>. The process of feature generation is performed in n passes over documents, where i-grams are generated in the i-th pass. At the end of each pass over documents all infrequent features are deleted (here we check for frequency &lt; 4).
Reference: [11] <author> Yang, Y., Pedersen, J.O., </author> <title> A Comparative Study on Feature Selection in Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 412|420, </pages> <year> 1997. </year>
Reference-contexts: In this way we can capture some characteristic word combinations but also increase the number of features (eg. in whole Yahoo hierarchy from 69,280 features for 1-grams to 255,602 features for 5-grams). We additionally apply feature subset selection as commonly used on text data eg. [8], <ref> [11] </ref>. In this approach a score is assigned to each feature independently, features are sorted according to the assigned score and a predefined number of the best features is taken to form the solution feature subset.
References-found: 10

