URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/SaVa94.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Email: (vivek sarkar@vnet.ibm.com (lvazquez@buevm2.vnet.ibm.com)  
Title: Automatic Localization for Distributed-Memory Multiprocessors Using a Shared-Memory Compilation Framework  
Author: Vivek Sarkar Lelia A. Vazquez 
Address: Argentina, CRAAG 555 Bailey Avenue Ing. Butty 275, piso 7 San Jose, California 95141 (1300) Buenos Aires, Argentina  
Affiliation: IBM Santa Teresa Laboratory IBM  
Abstract: In this paper, we outline an approach for compiling for distributed-memory multiprocessors that is inherited from compiler technologies for shared-memory multiprocessors. We believe that this approach to compiling for distributed-memory machines is promising because it is a logical extension of the shared-memory parallel programming model, a model that is easier for programmers to work with, and that has been studied in great detail as a target for parallelizing and optimizing compilers. In particular, the paper focuses on the localization step, and presents optimal localization algorithms for a single global DOALL, and for special-case structures of multiple global DOALLs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and its Application to Program Transformation. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1983. </year>
Reference-contexts: SUPERB [33], Kali [24], DINO [26], 1 In this paper, the term "variables" is used to denote the smallest granularity of data elements that can be accessed by the program. For example, an array A [1 : M ] consists of M distinct variables, A <ref> [1] </ref>; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning [30]. 3. <p> We present solutions for the problem for different cases, based on the structure of the loop-level data dependence graph [13]. The nodes of the dependence graph are the DOALL statements. The edges represent loop-independent data dependences <ref> [1] </ref>, and can be further classified as flow, anti, and output [32]. 4.1 Algorithm for Fan-in Trees In this subsection, we provide an optimal localization algorithm for the case when the dependence graph is a fan-in tree of flow dependences.
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The Computation Alignment and Computation Partitioning steps are performed so as to obtain the best trade-off between optimizing parallelism and optimizing locality [27, 28]. Locality optimizations result in reduced interprocessor communication regardless of whether the target is a shared-memory or a distributed-memory multiprocessor <ref> [31, 2] </ref>. We assume that the number of virtual processors equals the number of physical processors so that the output of Computation Partitioning can capture the trade-off between locality and parallelism as accurately as possible 2 .
Reference: [3] <author> V. Bala and J. Ferrante. </author> <title> Explicit data placement (XDP) : A methodology for compile time representation and optimization of data movement (extended abstract). </title> <booktitle> In Proceedings of the Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The owner-computes rule dictates that each processor should only compute values for variables that it owns [33, 25, 18]. This is an ad hoc rule that simplifies the task of deriving the computation mapping. Recently, some researchers have identified a need for relaxing this rule in certain cases <ref> [3, 5] </ref>. 4. Communication Extraction | identify and optimize the necessary interprocessor communication to resolve accesses to nonlocal variables. The compiler examines each reference to a nonlocal variable and generates the necessary in-terprocessor communication to resolve nonlocal accesses [33, 18].
Reference: [4] <author> Vasanth Balasundaram. </author> <title> A Mechanism for Keeping Useful Internal Information in Parallel Programming Tools: The Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: There have been many techniques discussed in the literature for summarizing variable accesses in this way, with varying degrees of algorithmic complexity and of precision in the summary e.g. see <ref> [4] </ref>. * read B (i) = set of variables read by iteration i * write B (i) = set of variables written by iteration i * ref B (i) = read B (i) S write B (i) = set of variables referenced by iteration i * writeonly B (i) = write
Reference: [5] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> In Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <year> 1988. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors. <p> The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows [33, 22, 5, 17, 18, 24, 15]: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain <ref> [5, 21, 12, 15] </ref>. 2. Data Partitioning | partition the alignment template elements among physical processors. Data Alignment and Data Partitionining together define the Data Mapping. <p> The owner-computes rule dictates that each processor should only compute values for variables that it owns [33, 25, 18]. This is an ad hoc rule that simplifies the task of deriving the computation mapping. Recently, some researchers have identified a need for relaxing this rule in certain cases <ref> [3, 5] </ref>. 4. Communication Extraction | identify and optimize the necessary interprocessor communication to resolve accesses to nonlocal variables. The compiler examines each reference to a nonlocal variable and generates the necessary in-terprocessor communication to resolve nonlocal accesses [33, 18]. <p> This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model <ref> [33, 5, 25, 18] </ref>. This step includes the translation of global names to local names, the generation of architecture-specific message-passing code, and insertion of runtime resolution code for data references that are not analyzable at compile-time.
Reference: [6] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multilevel Caches. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: This is accomplished by using a DISTRIBUTE statement with ADD and TRANSFER attributes. 2. Readwrite and writefirst variables are redistributed by deleting their previous distributions. This is accomplished by using a DISTRIBUTE statement with the REDIST attribute. This strategy is analogous to the invalidate-on-write mechanism for maintaining cache consistency <ref> [6] </ref>, and avoids the need for communication at the end of the DOALL to make the distributed state consistent (which would be analogous to update-on-write [6]). Also, an important observation is that write--first variables can be distributed using the NOTRANSFER attribute because their values do not need to be communicated. <p> This is accomplished by using a DISTRIBUTE statement with the REDIST attribute. This strategy is analogous to the invalidate-on-write mechanism for maintaining cache consistency <ref> [6] </ref>, and avoids the need for communication at the end of the DOALL to make the distributed state consistent (which would be analogous to update-on-write [6]). Also, an important observation is that write--first variables can be distributed using the NOTRANSFER attribute because their values do not need to be communicated.
Reference: [7] <author> Barbara Chapman, Piyush Mehrotra, and Hans Zima. </author> <title> Vienna Fortran A FORTRAN language extension for distributed memory multiprocessors. Contract No. </title> <institution> NAS1-18605, Nasa Langley Reaserch Center, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran <ref> [7] </ref>, High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning [30]. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. <p> This step will have to generate communication for dynamic DISTRIBUTE statements inserted during localization; these statements are similar to those proposed in the Fortran D [12], Vienna Fortran <ref> [7] </ref>, and High Performance Fortran [16] languages, and will have to be supported in code generation for those languages as well. <p> The add/redist attribute identifies the kind of distribution statement specified; it defines how mapping f should be integrated into the existing data distribution [10]. The transfer/notransfer mode indicates whether or not any data values needs to be moved as part of the distribute statement <ref> [7] </ref>. For localization, we are interested in the problem of changing a data distribution while keeping the evaluation of the state (~ s function) unchanged. This can be accomplished by using distribute statements.
Reference: [8] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <booktitle> In Proceedings of the Second Workshop on Languages, Compilers and Runtime Environments for Distributed Memory Multiprocessors, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Our algorithm is similar to the dynamic programming algorithm presented in <ref> [8] </ref>, though the program models and cost functions used are very different just like the differences between our approach in subsection 4.1.1 and the algorithm presented in [14]. <p> In [9], Chatterjee, Gilbert, Schreiber and Teng develop some heuristic algorithms to obtain static alignments of internal array values in an expression dag as as to minimize communication. Their data alignment model does not allow for data replication. In <ref> [8] </ref>, the same authors extend the results in [14] to other interconnection networks, and also consider expressions involving array sections of different sizes. The dynamic programming algorithm presented in [8] is similar to the algorithm discussed in subsection 4.1.2 of this paper. <p> Their data alignment model does not allow for data replication. In <ref> [8] </ref>, the same authors extend the results in [14] to other interconnection networks, and also consider expressions involving array sections of different sizes. The dynamic programming algorithm presented in [8] is similar to the algorithm discussed in subsection 4.1.2 of this paper. Apart from the differences in program models and cost functions, a significant difference is that the complexity of the algorithm in [8] depends on Dists, the number of possible data alignments for an array, whereas the complexity of <p> The dynamic programming algorithm presented in <ref> [8] </ref> is similar to the algorithm discussed in subsection 4.1.2 of this paper. Apart from the differences in program models and cost functions, a significant difference is that the complexity of the algorithm in [8] depends on Dists, the number of possible data alignments for an array, whereas the complexity of our algorithm depends on P erms, the number of possible p2v mappings (which is much smaller than Dists, as discussed earlier). 6 Conclusions and Future Work In this paper, we outline an approach for
Reference: [9] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM/SIGACT/SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments <ref> [21, 9, 23] </ref>, and on using a strip-mining technique to automatically perform data partitioning [30]. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. <p> Our algorithm is more tractable because it instead explores different p2v mappings which is theoretically bounded and tends to be quite small in practice. The reason for this difference is that, in our approach, data distributions are automatically derived from the p2v mappings using dynamic data redistributions. In <ref> [9] </ref>, Chatterjee, Gilbert, Schreiber and Teng develop some heuristic algorithms to obtain static alignments of internal array values in an expression dag as as to minimize communication. Their data alignment model does not allow for data replication.
Reference: [10] <author> P. Elustondo, L. Vazquez, O. Nestares, J. Sanchez Avalos, G. Alvarez, C.-T. Ho, and J. L. C. Sanz. </author> <title> Data parallel fortran. </title> <type> Technical report rj8690(78252), </type> <institution> IBM Almaden Research Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: In this case, we will call this function ~ s : V ars ! V als <ref> [10] </ref>. <p> The add/redist attribute identifies the kind of distribution statement specified; it defines how mapping f should be integrated into the existing data distribution <ref> [10] </ref>. The transfer/notransfer mode indicates whether or not any data values needs to be moved as part of the distribute statement [7]. For localization, we are interested in the problem of changing a data distribution while keeping the evaluation of the state (~ s function) unchanged. <p> No read-write conflicts are allowed among distinct DOALL iterations i.e. 8i; j 2 f1; : : : ; N g s.t. i 6= j, read B (i) T write B (j) = ;. 2.5 Properties of local DOALLs A DOALL is said to be local <ref> [10] </ref> (written as L-DOALL) if the following conditions are satisfied with respect to the input state, s, and physical-to-virtual processor mapping, p2v: 1. <p> In general, this condition admits the possibility of a read and write being performed concurrently for the same variable, since the operations will only be performed on the local state <ref> [10] </ref>. However, in this paper we will confine our discussions to local DOALLs obtained from global DOALLs, which have no read-write conflicts. Each iteration of a L-DOALL can execute independently using only its local state.
Reference: [11] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The Program Dependence Graph and its Use in Optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: There are two main directions for future work. One is to extend the localization techniques presented in this paper so that they work for more general schemes, such as nested DOALLs, DOALLs inside sequential loops, DOALLs within conditionals, program dependence graphs <ref> [11] </ref>, and parallel program graphs [29]. The other is to implement this strategy in a prototype compiler system, and evaluate its effectiveness in comparison to the predominant strategy of deriving the computation mapping from the data mapping.
Reference: [12] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M.-Y. Wu. </author> <title> Fortran D language specification. </title> <institution> TR:90-141, Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows [33, 22, 5, 17, 18, 24, 15]: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain <ref> [5, 21, 12, 15] </ref>. 2. Data Partitioning | partition the alignment template elements among physical processors. Data Alignment and Data Partitionining together define the Data Mapping. <p> For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D <ref> [12] </ref>, Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning [30]. 3. <p> This step is similar to the code generation step in the previous approach, and the techniques used there are equally applicable to this case. This step will have to generate communication for dynamic DISTRIBUTE statements inserted during localization; these statements are similar to those proposed in the Fortran D <ref> [12] </ref>, Vienna Fortran [7], and High Performance Fortran [16] languages, and will have to be supported in code generation for those languages as well.
Reference: [13] <author> Guang R. Gao, Russell Olsen, Vivek Sarkar, and Radhika Thekkath. </author> <title> Collective Loop Fusion by Array Contraction. </title> <type> Technical Report YALEU/DCS/RR-915, </type> <institution> Yale University, Department of Computer Science, </institution> <month> August </month> <year> 1992. </year> <booktitle> Conference Record of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <institution> Yale University, </institution> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: We present solutions for the problem for different cases, based on the structure of the loop-level data dependence graph <ref> [13] </ref>. The nodes of the dependence graph are the DOALL statements.
Reference: [14] <author> J. Gilbert and R. Schreiber. </author> <title> Optimal expression evaluation for data parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 58-64, </pages> <year> 1991. </year>
Reference-contexts: The complexity of the algorithm is O (k fi P erms), where k is the number of loop nests and P erms is the number of p2v permutations considered for a single loop nest. Our algorithm is similar to the dynamic programming algorithm presented in <ref> [14] </ref> though the program models and cost functions used are quite different, as discussed in Section 5. Theorem 4.1 The algorithm presented in Figure 1 computes an assignment of p2v permutations that minimizes the total communication cost. <p> Our algorithm is similar to the dynamic programming algorithm presented in [8], though the program models and cost functions used are very different just like the differences between our approach in subsection 4.1.1 and the algorithm presented in <ref> [14] </ref>. <p> Alignment analysis is based on identity, conformance and control preferences between scalars or arrays. Communication is necessary at the virtual processor level for unhonored preferences. A stripmining phase is used to judiciously choose a template partition that reduces communication at the physical processor level. In <ref> [14] </ref>, Gilbert and Schreiber develop an optimal algorithm that uses dynamic programming to stat-icly align the internal array values in a Fortran 90 expression tree, when the alignment of the operands is given as input. The communication cost functions they consider define "robust" metrics that represent different interconnection network costs. <p> The communication cost functions they consider define "robust" metrics that represent different interconnection network costs. They also show that the problem of optimally aligning internal array values is NP-complete for the more general case of expression dags. The dynamic programming algorithm presented in <ref> [14] </ref> is similar to the algorithm discussed in subsection 4.1.1 of this paper. However, the program models and cost functions used are quite different. The algorithm in [14] was designed to select alignments for array values in a Fortran 90 array expression, so that each tree edge corresponds to the communication <p> The dynamic programming algorithm presented in <ref> [14] </ref> is similar to the algorithm discussed in subsection 4.1.1 of this paper. However, the program models and cost functions used are quite different. The algorithm in [14] was designed to select alignments for array values in a Fortran 90 array expression, so that each tree edge corresponds to the communication of a single array value. <p> The distance function is a metric. Our approach is different, since in our model, the cost of an edge is the number of data items that must really be moved; this cost is not a metric. Another important difference is that the complexity of the algorithm in <ref> [14] </ref> is O (k fi Dists), where Dists is the number of possible data alignments for an array variable which is theoretically unbounded and is also very large in practice. <p> In [9], Chatterjee, Gilbert, Schreiber and Teng develop some heuristic algorithms to obtain static alignments of internal array values in an expression dag as as to minimize communication. Their data alignment model does not allow for data replication. In [8], the same authors extend the results in <ref> [14] </ref> to other interconnection networks, and also consider expressions involving array sections of different sizes. The dynamic programming algorithm presented in [8] is similar to the algorithm discussed in subsection 4.1.2 of this paper.
Reference: [15] <author> P. Hatcher and M. Quinn. </author> <title> Introduction to Data Parallel C. In The MIT Press, editor, Data-Parallel Programming on MIMD Computers, </title> <month> November </month> <year> 1991. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors. <p> The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows [33, 22, 5, 17, 18, 24, 15]: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain <ref> [5, 21, 12, 15] </ref>. 2. Data Partitioning | partition the alignment template elements among physical processors. Data Alignment and Data Partitionining together define the Data Mapping. <p> For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C <ref> [15] </ref>, Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning [30]. 3.
Reference: [16] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran. Language Specification Version 0.4, </title> <month> December </month> <year> 1992. </year>
Reference-contexts: For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran <ref> [16] </ref>. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning [30]. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. <p> This step will have to generate communication for dynamic DISTRIBUTE statements inserted during localization; these statements are similar to those proposed in the Fortran D [12], Vienna Fortran [7], and High Performance Fortran <ref> [16] </ref> languages, and will have to be supported in code generation for those languages as well. <p> So far, there has been little enthusiasm in the compiler community for supporting dynamic data distributions, even though they are included in most programming languages proposed for distributed-memory multiprocessors. For example, dynamic data distributions are excluded from the High Performance Fortran subset recommended for initial implementations <ref> [16] </ref>. However, robust support for dynamic data distributions can actually simplify the compiler implementation by providing a single con-struct for expressing data mappings, communication, and ownership. This is one of the main strengths of our approach. The rest of the paper is organized as follows. <p> In theory, P erms can be a very large number ( N !), but in practice we restrict the possible p2v's to a much smaller set of parameterized mappings such as block, cyclic, and block-cyclic <ref> [16] </ref>.
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. CRPC-TR91132, Center for Research on Parallel Computation, </title> <institution> Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors.
Reference: [18] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluation of compiler optimizations for fortran D on mimd distributed-memory machines. </title> <booktitle> In Proceedings of the ACM 1992 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors. <p> Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. The owner-computes rule dictates that each processor should only compute values for variables that it owns <ref> [33, 25, 18] </ref>. This is an ad hoc rule that simplifies the task of deriving the computation mapping. Recently, some researchers have identified a need for relaxing this rule in certain cases [3, 5]. 4. <p> Communication Extraction | identify and optimize the necessary interprocessor communication to resolve accesses to nonlocal variables. The compiler examines each reference to a nonlocal variable and generates the necessary in-terprocessor communication to resolve nonlocal accesses <ref> [33, 18] </ref>. This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model [33, 5, 25, 18]. <p> This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model <ref> [33, 5, 25, 18] </ref>. This step includes the translation of global names to local names, the generation of architecture-specific message-passing code, and insertion of runtime resolution code for data references that are not analyzable at compile-time. <p> If a data reference is unanalyzable, then it becomes necessary to generate runtime resolution code <ref> [25, 18] </ref>.
Reference: [19] <author> Alan H. Karp and Vivek Sarkar. </author> <title> Data Merging for Shared-Memory Multiprocessors. </title> <booktitle> Proc. Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1993. </year> <note> (to appear). Also available as Technical Report 03.454, </note> <institution> Santa Teresa Laboratory, IBM Corporation, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: In this case, we will call this function ~ s : V ars ! V als [10]. An important property of DOALLs is that the distributed state only needs to be consistent at the entry and exit of a DOALL <ref> [19] </ref>. 2.2 Data Distribution Statements The general syntax of a data distribution statement is assumed to be as follows: DISTRIBUTE WITH f, [ADD, REDIST], [TRANSFER, NOTRANSFER] where f : P Es ! 2 V ars is a data mapping.
Reference: [20] <author> K. Knobe, J. Lukas, and W. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for parallel computers, </booktitle> <month> July </month> <year> 1992. </year> <institution> Austrian Center for Parallel Computation, Vienna, Austria. </institution>
Reference-contexts: The heuristic algorithm they present is based on traversing the graph of preferences and deriving a new alignment based on previously selected alignments. A minimum spanning tree is used to traverse cyclic graphs. In <ref> [20] </ref>, Knobe, Lukas and Dally propose a compilation model consisting of data alignment, stripmining, and processor-local optimizations. As in [21], they allow for dynamic realignments. Alignment analysis is based on identity, conformance and control preferences between scalars or arrays. Communication is necessary at the virtual processor level for unhonored preferences.
Reference: [21] <author> K. Knobe, J. Lukas, and G. Steele. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 102-118, </pages> <year> 1990. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows [33, 22, 5, 17, 18, 24, 15]: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain <ref> [5, 21, 12, 15] </ref>. 2. Data Partitioning | partition the alignment template elements among physical processors. Data Alignment and Data Partitionining together define the Data Mapping. <p> For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments <ref> [21, 9, 23] </ref>, and on using a strip-mining technique to automatically perform data partitioning [30]. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. <p> They provide some heuristic algorithm for solving this NP-hard problem. This work does not take into account the data dependence structure of the program, and their data distribution model does not allow for data replication or for dynamic realignments. In <ref> [21] </ref>, Knobe, Lukas and Steele perform automatic alignment with the constraint that distinct elements of an array must be allocated onto different virtual processors. Also, data replication of array elements is not permitted. They do allow for dynamic realignment, by potentially selecting a different alignment for each array reference. <p> A minimum spanning tree is used to traverse cyclic graphs. In [20], Knobe, Lukas and Dally propose a compilation model consisting of data alignment, stripmining, and processor-local optimizations. As in <ref> [21] </ref>, they allow for dynamic realignments. Alignment analysis is based on identity, conformance and control preferences between scalars or arrays. Communication is necessary at the virtual processor level for unhonored preferences. A stripmining phase is used to judiciously choose a template partition that reduces communication at the physical processor level.
Reference: [22] <author> C. Koelbel and P. Mehrotra. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors.
Reference: [23] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M]. Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments <ref> [21, 9, 23] </ref>, and on using a strip-mining technique to automatically perform data partitioning [30]. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. <p> In section 1, we presented a broad overview of related work. In this section, we present some detailed comparisons with those references that appear to be most closely related to this paper. In <ref> [23] </ref>, Li and Chen formulate the static alignment problem and provide a heuristic solution. They define an alignment of two arrays as a function from the index domain of one of them to the index domain of the other.
Reference: [24] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <editor> In Pitman/MIT Press, editor, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors. <p> Data Alignment and Data Partitionining together define the Data Mapping. The simplest way of selecting data mappings is to rely on the programmer to provide them; indeed, there have been several proposals for programming language extensions to specify data mappings e.g. SUPERB [33], Kali <ref> [24] </ref>, DINO [26], 1 In this paper, the term "variables" is used to denote the smallest granularity of data elements that can be accessed by the program. For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M].
Reference: [25] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of Conference on Programming Language Design and Implementation, ACM SIGPLAN, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. The owner-computes rule dictates that each processor should only compute values for variables that it owns <ref> [33, 25, 18] </ref>. This is an ad hoc rule that simplifies the task of deriving the computation mapping. Recently, some researchers have identified a need for relaxing this rule in certain cases [3, 5]. 4. <p> This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model <ref> [33, 5, 25, 18] </ref>. This step includes the translation of global names to local names, the generation of architecture-specific message-passing code, and insertion of runtime resolution code for data references that are not analyzable at compile-time. <p> If a data reference is unanalyzable, then it becomes necessary to generate runtime resolution code <ref> [25, 18] </ref>.
Reference: [26] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The Dino parallel programming language. </title> <institution> TR:CU-CS-457-90, Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1990. </year>
Reference-contexts: Data Alignment and Data Partitionining together define the Data Mapping. The simplest way of selecting data mappings is to rely on the programmer to provide them; indeed, there have been several proposals for programming language extensions to specify data mappings e.g. SUPERB [33], Kali [24], DINO <ref> [26] </ref>, 1 In this paper, the term "variables" is used to denote the smallest granularity of data elements that can be accessed by the program. For example, an array A [1 : M ] consists of M distinct variables, A [1]; : : : ; A [M].
Reference: [27] <author> Vivek Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> Pitman, London and The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1989. </year> <booktitle> In the series, Research Monographs in Parallel and Distributed Computing. </booktitle>
Reference-contexts: The results of the paper can be extended to other parallel constructs. 2. Computation Partitioning | partition execution units (threads/tasks) onto a fixed set of virtual processors. The Computation Alignment and Computation Partitioning steps are performed so as to obtain the best trade-off between optimizing parallelism and optimizing locality <ref> [27, 28] </ref>. Locality optimizations result in reduced interprocessor communication regardless of whether the target is a shared-memory or a distributed-memory multiprocessor [31, 2].
Reference: [28] <author> Vivek Sarkar. </author> <title> Automatic Partitioning of a Program Dependence Graph into Parallel Tasks. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(5/6), </volume> <year> 1991. </year>
Reference-contexts: The results of the paper can be extended to other parallel constructs. 2. Computation Partitioning | partition execution units (threads/tasks) onto a fixed set of virtual processors. The Computation Alignment and Computation Partitioning steps are performed so as to obtain the best trade-off between optimizing parallelism and optimizing locality <ref> [27, 28] </ref>. Locality optimizations result in reduced interprocessor communication regardless of whether the target is a shared-memory or a distributed-memory multiprocessor [31, 2].
Reference: [29] <author> Vivek Sarkar. </author> <title> A Concurrent Execution Semantics for Parallel Program Graphs and Program Dependence Graphs (Extended Abstract). </title> <type> Technical Report YALEU/DCS/RR-915, </type> <institution> Yale University, Department of Computer Science, </institution> <month> August </month> <year> 1992. </year> <booktitle> Conference Record of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <institution> Yale University, </institution> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: There are two main directions for future work. One is to extend the localization techniques presented in this paper so that they work for more general schemes, such as nested DOALLs, DOALLs inside sequential loops, DOALLs within conditionals, program dependence graphs [11], and parallel program graphs <ref> [29] </ref>. The other is to implement this strategy in a prototype compiler system, and evaluate its effectiveness in comparison to the predominant strategy of deriving the computation mapping from the data mapping.
Reference: [30] <author> M. Weiss. </author> <title> Strip mining on simd architectures. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Dataparallel C [15], Fortran D [12], Vienna Fortran [7], High Performance Fortran [16]. But there has also been work done on automatic selection of data alignments [21, 9, 23], and on using a strip-mining technique to automatically perform data partitioning <ref> [30] </ref>. 3. Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. The owner-computes rule dictates that each processor should only compute values for variables that it owns [33, 25, 18].
Reference: [31] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimization Algorithm. </title> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The Computation Alignment and Computation Partitioning steps are performed so as to obtain the best trade-off between optimizing parallelism and optimizing locality [27, 28]. Locality optimizations result in reduced interprocessor communication regardless of whether the target is a shared-memory or a distributed-memory multiprocessor <ref> [31, 2] </ref>. We assume that the number of virtual processors equals the number of physical processors so that the output of Computation Partitioning can capture the trade-off between locality and parallelism as accurately as possible 2 .
Reference: [32] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, London and The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <booktitle> In the series, Research Monographs in Parallel and Distributed Computing. </booktitle>
Reference-contexts: This compilation approach is organized as follows: 1. Computation Alignment | specify a mapping of program computations onto an unbounded number of abstract execution units, such as threads or tasks. This step defines a shared-memory parallel program using constructs like DOALL <ref> [32] </ref>, fork-join, cobegin-coend, etc. The shared-memory parallel program may be explicitly written by the programmer based on some parallel algorithm, or may be automatically extracted by a compiler from a sequential program written in a language like Fortran 90. <p> appropriate for execution on distributed-memory multiprocessors. * readfirst B = readwrite B S readonly B = set of variables whose first access may be a read 2.4 Properties of global DOALLs A global (non-localized) DOALL loop (written as G-DOALL) is known to have no read-write, write-read, or write-write loop-carried dependences <ref> [32] </ref>, and hence must satisfy the following properties: 1. No write-write conflicts are allowed among distinct DOALL iterations i.e. 8i; j 2 f1; : : : ; N g s.t. i 6= j, write B (i) T write B (j) = ;. <p> We present solutions for the problem for different cases, based on the structure of the loop-level data dependence graph [13]. The nodes of the dependence graph are the DOALL statements. The edges represent loop-independent data dependences [1], and can be further classified as flow, anti, and output <ref> [32] </ref>. 4.1 Algorithm for Fan-in Trees In this subsection, we provide an optimal localization algorithm for the case when the dependence graph is a fan-in tree of flow dependences.
Reference: [33] <author> Hans P. Zima, Heinz-J. Bast, and Michael Gerndt. </author> <title> Superb: A tool for semi-automatic mimd/simd parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The predominant approach in compiling a program for execution on a distributed-memory multiprocessor is as follows <ref> [33, 22, 5, 17, 18, 24, 15] </ref>: 1. Data Alignment | select a data distribution for each global variable onto a decomposition template i.e. onto an abstract index domain [5, 21, 12, 15]. 2. Data Partitioning | partition the alignment template elements among physical processors. <p> Data Alignment and Data Partitionining together define the Data Mapping. The simplest way of selecting data mappings is to rely on the programmer to provide them; indeed, there have been several proposals for programming language extensions to specify data mappings e.g. SUPERB <ref> [33] </ref>, Kali [24], DINO [26], 1 In this paper, the term "variables" is used to denote the smallest granularity of data elements that can be accessed by the program. <p> Computation Mapping | use the owner-computes rule to derive the computation mapping from the data mapping. The owner-computes rule dictates that each processor should only compute values for variables that it owns <ref> [33, 25, 18] </ref>. This is an ad hoc rule that simplifies the task of deriving the computation mapping. Recently, some researchers have identified a need for relaxing this rule in certain cases [3, 5]. 4. <p> Communication Extraction | identify and optimize the necessary interprocessor communication to resolve accesses to nonlocal variables. The compiler examines each reference to a nonlocal variable and generates the necessary in-terprocessor communication to resolve nonlocal accesses <ref> [33, 18] </ref>. This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model [33, 5, 25, 18]. <p> This step is simplified when obeying the owner-computes rule since it implies that only read accesses are nonlocal. 5. Code Generation | generate a node program to be executed on each processor based on the Single Program Multiple Data (SPMD) model <ref> [33, 5, 25, 18] </ref>. This step includes the translation of global names to local names, the generation of architecture-specific message-passing code, and insertion of runtime resolution code for data references that are not analyzable at compile-time.
References-found: 33

