URL: ftp://ftp.csd.abdn.ac.uk/pub/terry/Publications/proposal.ps
Refering-URL: http://www.csd.abdn.ac.uk/~terry/Publications/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-Mail: terry@csd.abdn.ac.uk  
Title: Dimensionality Reduction for Agent-Based Learning  
Author: Terry R. Payne 
Date: December 9, 1996  
Address: King's College,  Aberdeen, AB24 3UE Scotland.  
Affiliation: Department of Computing Science  University of Aberdeen,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1992). </year> <title> Tolerating Noisy, Irrelevant and Novel Attributes in Instance-Based Learning Algorithms. </title> <journal> International Journal of Man-Machine Studies 36, </journal> <pages> 267-287. </pages>
Reference-contexts: The inclusion of irrelevant or redundant attributes can diminish the performance of a learning technique. For example, nearest neighbour algorithms are susceptible to the inclusion of irrelevant attributes, as the metrics used calculate an average similarity measure across all of the attributes <ref> (Aha, 1992) </ref>.
Reference: <author> Aha, D. and Bankert, R. </author> <year> (1994). </year> <title> A Comparative Evaluation of Sequential Feature Selection Algorithms. </title> <booktitle> In Proceedings of the 5th International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pp. 1-7. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference-contexts: Additional datasets will be constructed from USENET news articles to evaluate the algorithm extensions on instances containing sets of values. 4 Both datasets taken from the UCI Repository of Machine Learning Databases <ref> (Murphy & Aha, 1994) </ref>. 14 5.3.1 An Investigation of Attribute Selection Approaches (6 Jan 97 - 3 Mar 97) Section 4 outlined several attribute selection approaches. These methods identify a subset of relevant attributes from the set of all attributes, which can then be presented to the learning algorithm.
Reference: <author> Aha, D., Kibler, D., and Albert, M. </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 37-66. </pages>
Reference: <author> Almuallim, H. and Dietterich, T. </author> <year> (1991). </year> <title> Learning With Many Irrelevant Features. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pp. 547-552. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Armstrong, R., Freitag, D., Joachims, T., and Mitchell, T. </author> <year> (1995). </year> <title> WebWatcher: A Learning Apprentice for the World Wide Web. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments. </booktitle> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference-contexts: However, values extracted from the smaller fields, such as the From header field, appear in the majority of rules. This is due to 3 these values appearing more frequently in the training set; for example, the value journal appears in all 6 instances in Figure 2. WebWatcher <ref> (Armstrong et al., 1995) </ref> assists users by indicating which hypertext links to follow when searching for a technical paper. Terms from Web documents are grouped into three different categories according to where they appear within the document: hypertext links, headings, and sentences. <p> For each category, all the terms found in the training documents are ranked by measuring their mutual information with respect to correctly classifying the training data; see <ref> (Armstrong et al., 1995) </ref> for details. The top N ranked terms from each category are selected to create Boolean subvectors. Each element in the subvector corresponds to one of the ranked terms.
Reference: <author> Bala, J., Huang, J., Vafaie, H., DeJong, K., and Wechsler, H. </author> <year> (1995). </year> <title> Hybrid Learning Using Genetic Algorithms and Decision Trees for Pattern Classification. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pp. 719-724. </pages> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: Both Magi and WebWatcher preserve some document structure by mapping terms found in different fields or hypertext categories to different attributes or sets of attributes. However, some systems ignore this structure. For example, LIRA <ref> (Balabanovic et al., 1995) </ref> selects the ten highest ranking words from Web documents, to create a weighted vector. The ranking is determined by using the tfidf weighting scheme as proposed by Salton & McGill (1983).
Reference: <author> Balabanovic, M., Shoham, Y., and Yun, Y. </author> <year> (1995). </year> <title> An Adaptive Agent for Automated Web Browsing. Journal of Image Representation and Visual Communication 6 (4). </title>
Reference-contexts: Both Magi and WebWatcher preserve some document structure by mapping terms found in different fields or hypertext categories to different attributes or sets of attributes. However, some systems ignore this structure. For example, LIRA <ref> (Balabanovic et al., 1995) </ref> selects the ten highest ranking words from Web documents, to create a weighted vector. The ranking is determined by using the tfidf weighting scheme as proposed by Salton & McGill (1983).
Reference: <author> Biberman, Y. </author> <year> (1995). </year> <title> The Role of Prototypicality in Exemplar-Based Learning. </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning, </booktitle> <pages> pp. 77-91. </pages> <address> Berlin, Germany:Springer-Verlag. </address>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using Decision Trees to Improve Case-Based Learning. </title> <booktitle> In Proceedings of the 10th International Conference on Machine Learning, </booktitle> <pages> pp. 25-32. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. and Freitag, D. </author> <year> (1994). </year> <title> Greedy Attribute Selection. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. 28-36. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Cherkauer, K. and Shavlik, D. </author> <year> (1996). </year> <title> Growing Simpler Decision Trees to Facilitate Knowledge Discovery. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 315-318. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 Induction Algorithm. </title> <booktitle> Machine Learning 3, </booktitle> <pages> 261-283. </pages>
Reference-contexts: This study demonstrated that it was possible to learn rules by observing users performing a mail filtering task, and that these rules could be used to automate that task. However, no conclusions were drawn about the suitability of the learning algorithm (CN2 <ref> (Clark & Niblett, 1989) </ref>) for the task, or the data representation used. For this reason, a more detailed study was conducted to investigate this representation and the different rules generated by CN2 (Payne & Edwards, in press).
Reference: <author> Cohen, W. </author> <year> (1995). </year> <title> Fast Effective Rule Induction. </title> <booktitle> In Proceedings of the 12th International Machine Learning Conference (ML95), </booktitle> <pages> pp. 115-123. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: When calculating the decision metric (such as ID3's Information Gain (Quinlan, 1986) or the Distance-Based Gain Ratio (De Mantaras, 1991)), the set membership test can be performed for symbolic values. The greedy rule induction algorithm, RIPPER <ref> (Cohen, 1995) </ref> was modified in this way. RIPPER constructs a set of rules by repeatedly adding rules to an initially empty set until all the positive instances of a concept are covered.
Reference: <author> Cohen, W. </author> <year> (1996). </year> <title> Learning Trees and Rules with Set-valued Features. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pp. 709-716. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <booktitle> Machine Learning 10, </booktitle> <pages> 57-78. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1991). </year> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> Los Alamitos, California:IEEE Computer Society Press. 18 Data, </publisher> <editor> P. and Kibler, D. </editor> <year> (1995). </year> <title> Learning Prototypical Concept Descriptions. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pp. 158-166. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: Year 1997 Pages 1-32 Volume 11 (1) Notes "In Press" the original database entry. A number of learning techniques (such as nearest neighbour learning algorithms <ref> (Dasarathy, 1991) </ref>) rely on only being presented with data in the training instances that is relevant to the learning task. If additional, perhaps irrelevant or redundant data is provided, the performance of these learning algorithms can deteriorate (the terms relevant and redundant in this context are defined in Section 4).
Reference: <author> Davis, L. </author> <year> (1991). </year> <title> The Handbook of Genetic Algorithms. </title> <address> New York, </address> <publisher> NY:Van Nostrand Reinhold. </publisher>
Reference-contexts: To date, a feature selection system based on the wrapper model has been implemented, which uses IBPL to evaluate different subsets of attributes (Section 5.2). Two greedy search algorithms have been implemented, namely variants of forward and backward selection. Additional stochastic search algorithms will be implemented: a genetic algorithm <ref> (Davis, 1991) </ref>, and a simulated annealing algorithm (Kirkpatrick et al., 1983). The basic wrapper system will initially be used to compare these different search algorithms.
Reference: <author> De Mantaras, R. </author> <year> (1991). </year> <title> A Distance-Based Attribute Selection Measure for Decision Tree Induction. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 81-92. </pages>
Reference-contexts: When calculating the decision metric (such as ID3's Information Gain (Quinlan, 1986) or the Distance-Based Gain Ratio <ref> (De Mantaras, 1991) </ref>), the set membership test can be performed for symbolic values. The greedy rule induction algorithm, RIPPER (Cohen, 1995) was modified in this way. <p> However, in the absence of such background knowledge, automatic techniques are required to identify such attributes. Rule induction algorithms have been developed which use a variety of metrics (such as the Information Gain metric (Quinlan, 1986) or the Distance-Based Gain Ratio <ref> (De Mantaras, 1991) </ref>) to select relevant attributes when building decision trees. Various weighting techniques have been investigated in an attempt to reduce the contribution of irrelevant attributes in nearest neighbour algorithms (Wettschereck et al., 1995). John et al. (1994) have attempted to define the notion of `relevance'.
Reference: <author> Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. </author> <year> (1990). </year> <title> Indexing by Latent Semantic Analysis. </title> <journal> Journal of the American Society for Information Science 41 (6), </journal> <pages> 391-407. </pages>
Reference-contexts: To date, two search algorithms have been implemented (greedy variants of forward and backward selection) and preliminary testing with IBPL has been performed. An information retrieval technique, known as Latent Semantic Indexing (LSI) <ref> (Deerwester et al., 1990) </ref>, uses singular value decomposition (SVD) to project documents into a new space with reduced dimensionality. SVD is an algebraic tool used by correspondence analysis (Greenacre, 1984) to identify the principal components of a given problem.
Reference: <author> Edwards, P., Bayer, D., Green, C., and Payne, T. </author> <year> (1996). </year> <title> Experience with Learning Agents which Manage Internet-Based Information. In Machine Learning in Information Access: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pp. 31-40. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference-contexts: class distribution could be generated and assigned to these values. a (t a ; a ) = i2t a j2 a distance (i; j) sizeof (t a ) (2) To date, IBPL has been utilised by a number of different agent systems, including: Magi (Payne & Edwards, in press), IAN <ref> (Green & Edwards, 1996) </ref>, and LAW (Edwards et al., 1996). However, the performance of IBPL can deteriorate if redundant or irrelevant values are included in the sets. <p> assigned to these values. a (t a ; a ) = i2t a j2 a distance (i; j) sizeof (t a ) (2) To date, IBPL has been utilised by a number of different agent systems, including: Magi (Payne & Edwards, in press), IAN (Green & Edwards, 1996), and LAW <ref> (Edwards et al., 1996) </ref>. However, the performance of IBPL can deteriorate if redundant or irrelevant values are included in the sets. The current agent systems utilise a separate pre-processing step to select a subset of values for each of the sets (described in Section 2). <p> In contrast to these models, a number of nearest neighbour methods utilise weights to identify irrelevant attributes. We propose that these techniques should be thought of as a third approach to attribute selection, one which we will refer to as the weighted model <ref> (Payne & Edwards, 1996) </ref>. 8 The filter model (Figure 8) utilises an independent search criterion to find the appropriate attribute subset. This subset is then used to generate a reduced data set which in turn is used by a machine learning algorithm. <p> Table 2 lists some of the attribute selection systems that use the wrapper model. The Search column again represents the type of search used. These search methods are described in detail in <ref> (Payne & Edwards, 1996) </ref>. The Control column refers to the control mechanism used when evaluating the attribute subsets. The last column refers to the final learning algorithm which was also used as part of the attribute subset search. <p> A survey of existing techniques has been performed <ref> (Payne & Edwards, 1996) </ref>, and a system that uses the wrapper model to select attributes has been implemented. To date, two search algorithms have been implemented (greedy variants of forward and backward selection) and preliminary testing with IBPL has been performed.
Reference: <author> Green, C. and Edwards, P. </author> <year> (1996). </year> <title> Using Machine Learning to Enhance Software Tools for Internet Information Management. </title> <booktitle> In Proceedings of the AAAI-96 Workshop on Internet-based Information Systems. </booktitle>
Reference-contexts: class distribution could be generated and assigned to these values. a (t a ; a ) = i2t a j2 a distance (i; j) sizeof (t a ) (2) To date, IBPL has been utilised by a number of different agent systems, including: Magi (Payne & Edwards, in press), IAN <ref> (Green & Edwards, 1996) </ref>, and LAW (Edwards et al., 1996). However, the performance of IBPL can deteriorate if redundant or irrelevant values are included in the sets.
Reference: <author> Greenacre, M. </author> <year> (1984). </year> <title> Theory and Applications of Correspondence Analysis. </title> <publisher> London, UK:Academic Press. </publisher>
Reference-contexts: An information retrieval technique, known as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), uses singular value decomposition (SVD) to project documents into a new space with reduced dimensionality. SVD is an algebraic tool used by correspondence analysis <ref> (Greenacre, 1984) </ref> to identify the principal components of a given problem. A recent study utilised this property to reduce the dimensionality of protein sequence data for presentation to neural networks (Wu et al., 1995).
Reference: <author> Hart, P. </author> <year> (1968). </year> <title> The Condenced Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Information Theory 4, </journal> <pages> 515-516. </pages>
Reference: <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant Features and the Subset Selection Problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. 121-129. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Kibler, D. and Aha, D. </author> <year> (1988). </year> <title> Comparing Instance-Averaging with Instance-Filtering Learning Algorithms. </title> <booktitle> In Proceedings of the 3rd European Working Session on Learning, EWSL88, </booktitle> <pages> pp. 63-88. </pages>
Reference-contexts: redundant instances; instance-based learning techniques (Aha et al., 1991; Salzberg, 1991) in which those instances that identify class boundaries are preserved whilst the remaining instances are discarded; and techniques which identify prototypical instances for each class, such as simply averaging the instances of a class to find the centroid point <ref> (Kibler & Aha, 1988) </ref>, 7 or performing more complex operations to generate a small number of prototype points (Skalak, 1994; Biberman, 1995; Data & Kibler, 1995).
Reference: <author> Kira, K. and Rendell, L. </author> <year> (1992). </year> <title> A Practical Approach to Feature Selection. </title> <booktitle> In Proceedings of the 9th International Workshop on Machine Learning, </booktitle> <pages> pp. 249-256. </pages> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: The Evaluation column refers to the way attribute weights are updated during the evaluation phase. The last column refers either to the type of algorithm used during attribute selection, or in the case of RELIEF <ref> (Kira & Rendell, 1992) </ref> and the extensions to RELIEF (Kononenko, 1994), to the learning algorithm used once the attributes have been selected. 4.2 Summary The three models described above differ in the way they identify subsets of attributes.
Reference: <author> Kirkpatrick, S., Gelatt, C., and Vecchi, P. </author> <year> (1983). </year> <title> Optimization by Simulated Annealing. </title> <booktitle> Science 220, </booktitle> <pages> 671-680. </pages>
Reference-contexts: Two greedy search algorithms have been implemented, namely variants of forward and backward selection. Additional stochastic search algorithms will be implemented: a genetic algorithm (Davis, 1991), and a simulated annealing algorithm <ref> (Kirkpatrick et al., 1983) </ref>. The basic wrapper system will initially be used to compare these different search algorithms.
Reference: <author> Kohavi, R. </author> <year> (1994). </year> <title> Feature Subset Selection as Search with Probabilistic Estimates. </title> <booktitle> In AAAI Fall Symposium on Relevance, </booktitle> <pages> pp. 121-126. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995a). </year> <title> A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pp. 1137-1145. </pages> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: However, it performs induction at every state visited. As the number of attributes selected increases, the complexity of the induction process increases accordingly. This is further compounded by the control mechanism used to evaluate the state. Many systems utilise a k-fold cross validation approach <ref> (Kohavi, 1995a) </ref>, which involves partitioning the training data into k folds, training with k-1 folds, and then evaluating the performance of the learning algorithm when classifying the k'th fold. This process is repeated for each of the k folds in turn. <p> An attribute contributes towards a classifications if the VDM distance (Eq. 1) is smaller than a contribution threshold (Figure 11, Line 16). A leave-one-out cross validation <ref> (Kohavi, 1995a) </ref> will be used to update the weights for each of the instances in the training set. Attributes will then be 15 selected if their corresponding weights are greater than a selection threshold.
Reference: <author> Kohavi, R. </author> <year> (1995b). </year> <title> The Power of Decision Tables. </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning (ECML95), </booktitle> <pages> pp. 174-189. </pages> <address> Berlin, Germany:Springer-Verlag. </address>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating Attributes: Analysis and Extensions of RELIEF. </title> <booktitle> In Proceedings of the 7th European Conference on Machine Learning, </booktitle> <pages> pp. 171-182. </pages> <address> Berlin, Heidelberg:Springer-Verlag. </address>
Reference-contexts: The Evaluation column refers to the way attribute weights are updated during the evaluation phase. The last column refers either to the type of algorithm used during attribute selection, or in the case of RELIEF (Kira & Rendell, 1992) and the extensions to RELIEF <ref> (Kononenko, 1994) </ref>, to the learning algorithm used once the attributes have been selected. 4.2 Summary The three models described above differ in the way they identify subsets of attributes. The filter and wrapper models perform a search through a space of possible attribute subsets.
Reference: <author> Kubat, M., Flotzinger, D., and Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering Patterns in EEG-Signals: Comparative Study of a Few Methods. </title> <booktitle> In Proceedings of the 6th European Conference on Machine Learning, </booktitle> <pages> pp. 366-371. </pages> <address> Berlin, Heidelberg:Springer-Verlag. 19 Lang, K. </address> <year> (1995). </year> <title> NewsWeeder: Learning to Filter Netnews. </title> <booktitle> In Proceedings of the 12th International Machine Learning Conference (ML95), </booktitle> <pages> pp. 331-339. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Langley, P. </author> <year> (1996). </year> <title> Induction of Condensed Determinations. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 327-330. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference: <author> Langley, P. and Sage, S. </author> <year> (1994a). </year> <title> Induction of Selective Bayesian Classifiers. </title> <booktitle> In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 399-406. </pages> <address> Seattle, </address> <publisher> WA:Morgan Kaufmann. </publisher>
Reference: <author> Langley, P. and Sage, S. </author> <year> (1994b). </year> <title> Oblivious Decision Trees and Abstract Cases. </title> <booktitle> In Working Notes of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> pp. 113-117. </pages> <address> Seattle, </address> <publisher> WA:AAAI Press. </publisher>
Reference-contexts: train/ Bayesian (K2-AS) evaluation partitions Networks Skalak (1994) Random mutation search k-fold cross 1-NN (RMHC-PF1) validation Terano & Ishino (1996) Genetic algorithm - C4.5 c (SIBILE) Vafaie & De Jong (1994) Genetic algorithm Equal size train/ AQ15 evaluation partitions a "...equivalent to a nearest neighbour scheme that ignores some attributes..." <ref> (Langley & Sage, 1994b) </ref> b A Determination Table classifies unseen instances in a similar manner to a nearest neighbour scheme except that if no identical match can be found then the majority class is used. c Decision trees induced by C4.5 are evaluated and rated by a domain expert.
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm. </title> <booktitle> Machine Learning 2, </booktitle> <pages> 285-318. </pages>
Reference: <author> Liu, H. and Setiono, R. </author> <year> (1996). </year> <title> A Probabilistic Approach to Feature Selection A Filter Solution. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pp. 319-327. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. and Lee, M. </author> <year> (1994). </year> <title> Efficient Algorithms for Minimizing Cross Validation Error. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. 190-198. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> UCI Repository of Machine Learning Databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. [http://www.ics.uci.edu/~mlearn/MLRepository.html]. </address>
Reference-contexts: Additional datasets will be constructed from USENET news articles to evaluate the algorithm extensions on instances containing sets of values. 4 Both datasets taken from the UCI Repository of Machine Learning Databases <ref> (Murphy & Aha, 1994) </ref>. 14 5.3.1 An Investigation of Attribute Selection Approaches (6 Jan 97 - 3 Mar 97) Section 4 outlined several attribute selection approaches. These methods identify a subset of relevant attributes from the set of all attributes, which can then be presented to the learning algorithm.
Reference: <author> Payne, T. </author> <year> (1994). </year> <title> Learning Email Filtering Rules with Magi, A Mail Agent Interface. </title> <type> MSc Thesis, </type> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: This section begins by summarising work which has already been performed to identify issues that arise when a set-valued attribute representation is used, before describing a number of extensions to the learning algorithm, IBPL, to investigate these issues. 5.1 Work To Date A previous study <ref> (Payne, 1994) </ref> investigated the various issues involved in developing an agent system for filtering electronic mail. An approach was developed for inducing rules from mail messages, by creating multiple instances for each message (Section 2).
Reference: <author> Payne, T. and Edwards, P. </author> <title> Interface Agents that Learn: An Investigation of Learning Issues in a Mail Agent Interface. </title> <booktitle> Applied Artificial Intelligence 11 (1). </booktitle> <publisher> In Press. </publisher>
Reference: <author> Payne, T. and Edwards, P. </author> <year> (1995). </year> <title> Learning Mechanisms for Information Filtering Agents. </title> <type> Technical Report AUCS/TR9509, </type> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scot-land. </address> <booktitle> Presented at the BCS-SGES Intelligent Agents Workshop, </booktitle> <publisher> Oxford Brookes University, </publisher> <month> November </month> <year> 1995. </year>
Reference-contexts: The algorithm, IBPL, was implemented (described in Section 3), and its performance compared to that of CN2. Within the IBPL algorithm, two different mechanisms have been explored to compare sets of values <ref> (Payne & Edwards, 1995) </ref>; both are based on the Value Difference Metric. The first approach averages all distances between the values in each set; the second averages the smallest distances found for each of the values (Section 3, Eq. 2).
Reference: <author> Payne, T. and Edwards, P. </author> <year> (1996). </year> <title> A Survey of Feature Selection Methods. </title> <type> Unpublished Draft. </type>
Reference-contexts: In contrast to these models, a number of nearest neighbour methods utilise weights to identify irrelevant attributes. We propose that these techniques should be thought of as a third approach to attribute selection, one which we will refer to as the weighted model <ref> (Payne & Edwards, 1996) </ref>. 8 The filter model (Figure 8) utilises an independent search criterion to find the appropriate attribute subset. This subset is then used to generate a reduced data set which in turn is used by a machine learning algorithm. <p> Table 2 lists some of the attribute selection systems that use the wrapper model. The Search column again represents the type of search used. These search methods are described in detail in <ref> (Payne & Edwards, 1996) </ref>. The Control column refers to the control mechanism used when evaluating the attribute subsets. The last column refers to the final learning algorithm which was also used as part of the attribute subset search. <p> A survey of existing techniques has been performed <ref> (Payne & Edwards, 1996) </ref>, and a system that uses the wrapper model to select attributes has been implemented. To date, two search algorithms have been implemented (greedy variants of forward and backward selection) and preliminary testing with IBPL has been performed.
Reference: <author> Payne, T., Edwards, P., and Green, C. </author> <title> Experience with Rule Induction and k-Nearest Neighbour Methods for Interface Agents that Learn. </title> <journal> IEEE Transactions on Knowledge and Data Engineering. </journal> <note> In Press. Presented at the ML95 Workshop on Agents that Learn from Other Agents. </note>
Reference: <author> Porter, M. </author> <year> (1980). </year> <title> An Algorithm for Suffix Stripping. </title> <booktitle> Program (Automated Library and Information Systems) 14 (3), </booktitle> <pages> 130-137. </pages>
Reference-contexts: Domain knowledge can be used to identify the individual fields in the database record. Fields containing text can be parsed to identify terms, remove punctuation, stem the terms (i.e. remove suffixes such as `ing' or `ed' <ref> (Porter, 1980) </ref>), and sort the terms in dictionary order. Figure 1 1 shows an illustrative entry that has been retrieved from a database and processed in this way. This data now has to be mapped to a training example for presentation to the learning algorithm.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: When calculating the decision metric (such as ID3's Information Gain <ref> (Quinlan, 1986) </ref> or the Distance-Based Gain Ratio (De Mantaras, 1991)), the set membership test can be performed for symbolic values. The greedy rule induction algorithm, RIPPER (Cohen, 1995) was modified in this way. <p> However, in the absence of such background knowledge, automatic techniques are required to identify such attributes. Rule induction algorithms have been developed which use a variety of metrics (such as the Information Gain metric <ref> (Quinlan, 1986) </ref> or the Distance-Based Gain Ratio (De Mantaras, 1991)) to select relevant attributes when building decision trees. Various weighting techniques have been investigated in an attempt to reduce the contribution of irrelevant attributes in nearest neighbour algorithms (Wettschereck et al., 1995).
Reference: <author> Rachlin, J., Kasif, S., Salzberg, S., and Aha, D. </author> <year> (1994). </year> <title> Towards a Better Understanding of Memory-Based Reasoning Systems. </title> <booktitle> In Proceedings of the 11th International Machine Learning Conference (ML94), </booktitle> <pages> pp. 242-250. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Richeldi, M. and Lanzi, P. </author> <year> (1996). </year> <title> Performing Effective Feature Selection by Investigating the Deep Structure of the Data. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 379-382. </pages> <address> Menlo Park, </address> <note> CA:AAAI Press. 20 Salton, </note> <author> G. and McGill, M. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A Nearest Hyperrectangle Learning Method. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 251-276. </pages>
Reference: <author> Salzberg, S. </author> <year> (1992). </year> <title> Improving Classification Methods via Feature Selection. </title> <type> Technical Report TR JHU-92/12, </type> <institution> Department of Computer Science, Johns Hopkins University, </institution> <address> Baltimore, MD 21218. </address>
Reference: <author> Singh, M. and Provan, G. </author> <year> (1995). </year> <title> A Comparison of Induction Algorithms for Selective and nonSelective Bayesian Classifiers. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pp. 497-505. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Singh, M. and Provan, G. </author> <year> (1996). </year> <title> Efficient Learning of Selective Bayesian Network Classifiers. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pp. 453-461. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. 293-301. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> Toward Memory-Based Reasoning. </title> <booktitle> Communications of the ACM 29 (12), </booktitle> <pages> 1213-1228. </pages>
Reference: <author> Terano, T. and Ishino, Y. </author> <year> (1996). </year> <title> Interactive Knowledge Discovery from Marketing Questionnaire Using Simulated Breeding and Inductive Learning Methods. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 279-282. </pages> <address> Menlo Park, </address> <publisher> CA:AAAI Press. </publisher>
Reference: <author> Vafaie, H. and De Jong, K. </author> <year> (1994). </year> <title> Improving a Rule Induction System using Genetic Algorithms. </title>
Reference: <editor> In R. Michalski and G. Tecuci (Eds.), </editor> <booktitle> Machine Learning: A Multistrategy Approach. </booktitle> <volume> Vol 4, </volume> <pages> pp. 453-469. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference: <author> Wettschereck, D., Aha, D., and Mohri, T. </author> <year> (1995). </year> <title> A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms. </title> <type> Technical Report AIC-95-012, </type> <note> NRL NCARAI. To Appear in an AI Review Special Issue on Lazy Learning. </note>
Reference-contexts: Various weighting techniques have been investigated in an attempt to reduce the contribution of irrelevant attributes in nearest neighbour algorithms <ref> (Wettschereck et al., 1995) </ref>. John et al. (1994) have attempted to define the notion of `relevance'. <p> Various studies have demonstrated that the wrapper model finds better subsets than the filter model. However, Section 4.2 concluded that the weighted model could successfully eliminate irrelevant attributes, but often fails to eliminate redundant ones <ref> (Wettschereck et al., 1995) </ref>. For this reason, a novel approach to attribute selection is proposed here, which combines the weighted and wrapper models to reduce the time required when selecting a subset of relevant attributes for nearest neighbour learning algorithms. It utilises weights to identify and eliminate irrelevant attributes.
Reference: <author> Wu, C., Berry, M., Shivakumar, S., and McLarty, J. </author> <year> (1995). </year> <title> Neural Networks for Full-Scale Protein Sequence Classification: Sequence Encoding with Singular Value Decomposition. </title> <booktitle> Machine Learning 21, </booktitle> <pages> 177-193. 21 </pages>
Reference-contexts: SVD is an algebraic tool used by correspondence analysis (Greenacre, 1984) to identify the principal components of a given problem. A recent study utilised this property to reduce the dimensionality of protein sequence data for presentation to neural networks <ref> (Wu et al., 1995) </ref>. A k-nearest neighbour learning algorithm that utilises SVD to reduce the number of dimensions when performing classifications is currently under development.
References-found: 59

