URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/ml92_def.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/index.html
Root-URL: http://www.cs.berkeley.edu
Email: aml@ic.berkeley.edu  alberto@ic.berkeley.edu  
Title: Constructive Induction Using a Non-Greedy Strategy for Feature Selection  
Author: Arlindo L. Oliveira Alberto Sangiovanni-Vincentelli 
Address: Berkeley CA 94720  Berkeley CA 94720  
Affiliation: Dept. of EECS UC Berkeley  Dept. of EECS UC Berkeley  
Abstract: We present a method for feature construction and selection that finds a minimal set of conjunctive features that are appropriate to perform the classification task. For problems where this bias is appropriate, the method outperforms other constructive induction algorithms and is able to achieve higher classification accuracy. The application of the method in the search for minimal multi-level boolean expressions is presented and analyzed with the help of some examples.
Abstract-found: 1
Intro-found: 1
Reference: [Almuallim & Dietterich 1990] <author> H. Almuallim & T. G. </author> <title> Di-etterich "Learning with many irrelevant features", </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 547-552, </pages> <address> 1990, Anaheim, CA, </address> <publisher> AAAI Press. </publisher>
Reference-contexts: These greedy methods may fail to select important features in the cases where important information can only be obtained by looking simultaneously at a set of several features. On the other hand, feature selection can be done in a non-greedy way like, for instance, in <ref> [Almuallim & Dietterich 1990] </ref>. This method may, in principle, lead to a better selection but is computationally more expensive. To illustrate this problem, consider again the xor problem. <p> The algorithm derives a minimal set of features that are sufficient to distinguish between all the positive and negative examples in the training set. A detailed analysis of a restriction of a similar bias was carried out in <ref> [Almuallim & Dietterich 1990] </ref>. In that work the authors study the properties of learning algorithms that implement the following bias: between two classification functions that are consistent with the training data, choose the one that depends on the smaller number of input features. <p> We performed two experiments: 5.1 Experiment A This is the same experiment described as experiment 3 in <ref> [Almuallim & Dietterich 1990] </ref>. Specifically, we selected 50 arbitrary concepts each depending on at most 5 (out of n) input attributes. For each of these concepts and for different values of n we measured the dependence of the accuracy of the hypothesis returned while successively increasing the sample size. <p> For each of these concepts and for different values of n we measured the dependence of the accuracy of the hypothesis returned while successively increasing the sample size. We compared the performance of MIFES with the performance of ID3 [Quinlan 1986], FRINGE [Pagallo & Haussler 1990] and FOCUS <ref> [Almuallim & Dietterich 1990] </ref> under two different conditions: 1. MIFES was instructed to select only features consisting of conjunctions of no more than 1 literal.
Reference: [Brayton et Al. 1984] <author> R. K. Brayton, G. D. Hachtel, C. T. McMullen & A. L. </author> <title> Sangiovanni-Vincentelli "Logic Minimization Algorithms for VLSI Synthesis", </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1984. </year>
Reference-contexts: This approach was used successfully in several two-level minimization packages, of which ESPRESSO <ref> [Brayton et Al. 1984] </ref> is the most successful example. However, when no good rules are available for the process of candidate selection, this reduction in the size of the problem cannot take place and the covering problem cannot be solved using reasonable resources.
Reference: [Brayton et Al. 1990] <author> R. K. Brayton, G. D. Hachtel & A. L. </author> <booktitle> Sangiovanni-Vincentelli "Multilevel Logic Synthesis" Proceedings of the IEEE, </booktitle> <volume> Vol. 78:2, </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: In the logic synthesis community, the problem of finding adequate intermediate features is known as the factorization problem <ref> [Brayton et Al. 1990] </ref>. In this paper we describe an algorithm for feature selection (or boolean factorization) that does not have the limitations that are present when a greedy feature selection is performed.
Reference: [Blumer et Al. 1986] <author> A. Blumer, A. Ehrenfeucht, D. Haussler & M. K. </author> <title> Warmuth "Classifying Learnable Geometric Concepts with the Vapnik-Chervonenkis Dimension", </title> <booktitle> Proceedings of the Eighteenth Annual ACM Symposium of Theory of Computing, </booktitle> <pages> pp 273-282, </pages> <address> Berkeley, CA, </address> <year> 1986. </year>
Reference-contexts: This family of functions has a VC-dimension of 2 k , and, since smaller VC-dimension means that one is able to learn using a smaller number of examples <ref> [Blumer et Al. 1986] </ref> we should usually be better off by using as few features as possible. This simplified analysis did not consider the contribution to the VC-dimension given by the ability of the classifier to choose the features. <p> This 1 The V Cdimension is a simple combinatorial property of a set of functions that is related with the ability of a function on that set to fit an arbitrary set of data points. See <ref> [Blumer et Al. 1986] </ref> for details. feature will have the value 1 for some examples in the train-ing set and the value 0 for the remaining ones.
Reference: [Drastal et Al. 1989] <author> G. Drastal, G. Czako & S. </author> <title> Raatz "Induction in an Abstraction Space: A Form of Constructive Induction", </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 708-712, </pages> <address> Detroit, MI, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Chosing an appropriate set of auxiliary features to use when performing classification is a difficult problem, usually described as constructive induction. The approaches developed so far can be viewed as belonging to one of two classes. Analytical approaches <ref> [Drastal et Al. 1989, Utgoff 1986] </ref> generate new features by using a domain specific theory. The usefulness of the features derived depends on the amount of appropriate domain knowledge available. If there exists good domain knowledge this may be an effective method to generate an adequate set of auxiliary features.
Reference: [Garey & Johnson 1979] <author> M. R. Garey & D. S. </author> <title> Johnson "Computers and Intractability: A Guide to the Theory of NP-Completeness", </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Fran-cisco, </address> <year> 1979. </year>
Reference-contexts: It is known <ref> [Garey & Johnson 1979] </ref> that finding minimum 2 DNF or CNF representations are NP-complete problems 3 .
Reference: [Michalski et Al. 1986] <author> R. S. Michalski, I. Mozetic, J. Hong, & N. </author> <title> Lavrac "The multipurpose incremental learning system AQ15 and its testing application to three medical domains", </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 1041-1045, </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: New features are usually selected from the available pool by a greedy method where a new feature is chosen at a time, like, for example in ID3 [Quinlan 1986] and AQ15 <ref> [Michalski et Al. 1986] </ref>. These greedy methods may fail to select important features in the cases where important information can only be obtained by looking simultaneously at a set of several features.
Reference: [Oliveira & Sangiovanni 1991] <author> A. L. Oliveira & A. </author> <title> San-giovanni Vincentelli "LSAT An Algorithm for the Synthesis of Two-Level Threshold Gate Networks", </title> <booktitle> Proceedings of the International Conference on Computer Aided Design, </booktitle> <pages> pp. 130-133, </pages> <address> 1991, Santa Clara, CA, </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In [Wnek & Michalski 1991] similar results are obtained using the latest version of the AQ family. Methods based on the use of logic synthesis techniques to minimize two-level networks have also been used to efficiently derive the minimal DNF representation <ref> [Oliveira & Sangiovanni 1991] </ref> for this type of problems. For many concepts of interest, neither a compact DNF nor a compact CNF exists, but they can be represented by a compact factored form. <p> However, when no good rules are available for the process of candidate selection, this reduction in the size of the problem cannot take place and the covering problem cannot be solved using reasonable resources. An alternative method for solving a covering problem was proposed in <ref> [Oliveira & Sangiovanni 1991] </ref>.
Reference: [Pagallo & Haussler 1990] <author> G. Pagallo & D. </author> <title> Haussler "Boolean Feature Discovery in Empirical Learning", </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> pp. 71-99, </pages> <year> 1990. </year>
Reference-contexts: If there exists good domain knowledge this may be an effective method to generate an adequate set of auxiliary features. Empirical approaches, on the other hand, usually start by using the original features and recursively constructing more complex ones <ref> [Pagallo & Haussler 1990, Schilmer & Granger 1986] </ref>. Given a set of either primitive or composite features the problem of selecting a set of features that is adequate for the classification task at hand is still difficult. <p> Despite the limitations of greedy selection, efficient algorithms for constructive induction applicable to problems where the concepts accept a compact DNF (Disjoint Normal Form) representations have been proposed. For example, FRINGE <ref> [Pagallo & Haussler 1990] </ref> is able to select the appropriate conjunctions to be used as the features for a set of concepts that accept a compact DNF representation. <p> For each of these concepts and for different values of n we measured the dependence of the accuracy of the hypothesis returned while successively increasing the sample size. We compared the performance of MIFES with the performance of ID3 [Quinlan 1986], FRINGE <ref> [Pagallo & Haussler 1990] </ref> and FOCUS [Almuallim & Dietterich 1990] under two different conditions: 1. MIFES was instructed to select only features consisting of conjunctions of no more than 1 literal.
Reference: [Quinlan 1986] <author> J. R. </author> <title> Quinlan "Induction of Decision Trees" Machine Learning, </title> <type> 1, </type> <institution> pp.81-106, </institution> <year> 1986. </year>
Reference-contexts: New features are usually selected from the available pool by a greedy method where a new feature is chosen at a time, like, for example in ID3 <ref> [Quinlan 1986] </ref> and AQ15 [Michalski et Al. 1986]. These greedy methods may fail to select important features in the cases where important information can only be obtained by looking simultaneously at a set of several features. <p> For each of these concepts and for different values of n we measured the dependence of the accuracy of the hypothesis returned while successively increasing the sample size. We compared the performance of MIFES with the performance of ID3 <ref> [Quinlan 1986] </ref>, FRINGE [Pagallo & Haussler 1990] and FOCUS [Almuallim & Dietterich 1990] under two different conditions: 1. MIFES was instructed to select only features consisting of conjunctions of no more than 1 literal.
Reference: [Schilmer & Granger 1986] <author> J. C. Schilmer & R. H. Granger Jr. </author> <title> "Incremental Learning from Noisy Data", </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> pp. 317-354. </pages>
Reference-contexts: If there exists good domain knowledge this may be an effective method to generate an adequate set of auxiliary features. Empirical approaches, on the other hand, usually start by using the original features and recursively constructing more complex ones <ref> [Pagallo & Haussler 1990, Schilmer & Granger 1986] </ref>. Given a set of either primitive or composite features the problem of selecting a set of features that is adequate for the classification task at hand is still difficult.
Reference: [Utgoff 1986] <author> P. E. </author> <title> Utgoff "Shift of Bias for Inductive Concept Learning" in Machine Learning: </title> <booktitle> An Artificial Intelligence Approach, 1986, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Chosing an appropriate set of auxiliary features to use when performing classification is a difficult problem, usually described as constructive induction. The approaches developed so far can be viewed as belonging to one of two classes. Analytical approaches <ref> [Drastal et Al. 1989, Utgoff 1986] </ref> generate new features by using a domain specific theory. The usefulness of the features derived depends on the amount of appropriate domain knowledge available. If there exists good domain knowledge this may be an effective method to generate an adequate set of auxiliary features.
Reference: [Wnek & Michalski 1991] <author> J. Wnek & R. S. Michalski, </author> <title> "Hypothesis-driven Constructive Induction in AQ17: </title>
Reference-contexts: A generalization of the FRINGE heuristics proposed in [Yang et Al. 1991] is able to build either conjunctive or disjunctive features that make it possible to learn concepts with a compact DNF or a compact CNF (Conjunctive Normal Form) representation. In <ref> [Wnek & Michalski 1991] </ref> similar results are obtained using the latest version of the AQ family. Methods based on the use of logic synthesis techniques to minimize two-level networks have also been used to efficiently derive the minimal DNF representation [Oliveira & Sangiovanni 1991] for this type of problems.
References-found: 13

