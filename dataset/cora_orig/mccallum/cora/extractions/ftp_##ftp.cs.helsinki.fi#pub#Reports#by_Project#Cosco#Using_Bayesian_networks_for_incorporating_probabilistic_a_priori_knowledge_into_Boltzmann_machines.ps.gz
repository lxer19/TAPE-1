URL: ftp://ftp.cs.helsinki.fi/pub/Reports/by_Project/Cosco/Using_Bayesian_networks_for_incorporating_probabilistic_a_priori_knowledge_into_Boltzmann_machines.ps.gz
Refering-URL: 
Root-URL: 
Title: Using Bayesian networks for incorporating probabilistic a priori knowledge into Boltzmann machines  
Author: Petri Myllymaki 
Address: P.O.Box 26, FIN-00014 University of Helsinki, Finland  
Affiliation: University of Helsinki, Department of Computer Science  
Date: March 1994), 97-102.  
Note: Proceedings of SOUTHCON'94 (Orlando,  
Abstract: We present a method for automatically determining the structure and the connection weights of a Boltzmann machine corresponding to a given Bayesian network representation of a probability distribution on a set of discrete variables. The resulting Boltzmann machine structure can be implemented efficiently on massively parallel hardware, since the structure can be divided into two separate clusters where all the nodes in one cluster can be updated simultaneously. The updating process of the Boltzmann machine approximates a Gibbs sampling process of the original Bayesian network in the sense that the Boltzmann machine converges to the same final state as the Gibbs sampler does. The mapping from a Bayesian network to a Boltzmann machine can be seen as a method for incorporating probabilistic a priori information into a neural network architecture, which can then be trained further with existing learning algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aarts, E., and Korst, J., </author> <title> Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing. </title> <publisher> John Wiley & Sons, </publisher> <address> Chich-ester, </address> <year> 1989. </year>
Reference-contexts: Gibbs distribution P f~s t g = e C (~s t )=T (t) : It follows that in principle the BM model can be used as a massively parallel tool for finding the maximum of the consensus function, provided that the generating probability matrix used fulfills the general requirements in <ref> [1, Th. 3.3] </ref>. However, the acceptance probability (6) sets here implicitly some additional requirements to the generation probabilities, since the difference in energy is calculated by 99 keeping all nodes except one constant. <p> Using this kind of clustered BM models we can maintain some parallelism and update all the nodes in one cluster at the same time, while a convergence theorem can be proved <ref> [1, p. 139] </ref>. Obviously, the degree of parallelism depends on the number of clusters in the network. Unfortunately, the problem of finding a minimal set of clusters in a given network is NP-complete [1, p. 141]. <p> Obviously, the degree of parallelism depends on the number of clusters in the network. Unfortunately, the problem of finding a minimal set of clusters in a given network is NP-complete <ref> [1, p. 141] </ref>. In the next section we deal with a special class of BM architectures which has by definition only two clusters, being in this sense an optimal BM architecture. V.
Reference: [2] <author> Baum, E.B., </author> <title> Towards practical 'neural' computation for combinatorial optimization problems. Pp. </title> <booktitle> 53-58 in Proceedings of the AIP Conference 151: Neural Networks for Computing (Snowbird, </booktitle> <address> UT, </address> <year> 1986), </year> <title> edited by J.Denker. </title> <booktitle> American Institute of Physics, </booktitle> <address> New York, NY, </address> <year> 1986. </year>
Reference: [3] <author> Barker, A.A., </author> <title> Monte Carlo calculations of the radial distribution functions for a proton-electron plasma. Aust. </title> <journal> J. Phys. </journal> <volume> 18 (1965), </volume> <pages> 119-133. </pages>
Reference: [4] <author> Cooper, </author> <title> G.F., The computational complexity of probabilistic inference using Bayesian belief networks. </title> <booktitle> Artificial Intelligence 42 (1990) 2-3 (March), </booktitle> <pages> 393-405. </pages>
Reference: [5] <author> Geffner, H., and Pearl, J., </author> <title> On the probabilistic semantics of connectionist networks. </title> <type> Technical report R-84, </type> <institution> UCLA Computer Science Department. </institution> <address> Los Angeles, CA, </address> <year> 1987. </year>
Reference-contexts: The corresponding BM has N units, one for each random variable in the Bayesian network <ref> [8, 5] </ref>. However, in this case the parallelism of the resulting BM is lost, as no two adjacent units (variables) can be updated at the same time. <p> For an accurate representation of the general case, it seems at a first glance that binary connections are not sufficient, but higher-order connections (arcs connecting three or more nodes to each other) are necessary <ref> [5] </ref>. We avoid the need to generalize the BM model by introducing hidden units, i.e. units that do not directly correspond to any of the random variables U i , but to combinations of these variables.
Reference: [6] <author> Geman, S., and Geman, D., </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 6 (1984), </journal> <pages> 721-741. </pages> <note> [7] de Gloria, </note> <author> A., Faraboschi, P., and Olivieri, M., </author> <title> Clustered Boltzmann Machines: Massively parallel architectures for constrained optimization problems. </title> <booktitle> Parallel Computing 19 (1993), </booktitle> <pages> 163-175. </pages>
Reference: [8] <author> Hinton, G.E., and Sejnowski, T.J., </author> <title> Optimal perceptual inference. Pp. </title> <booktitle> 448-453 in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Washington DC, </booktitle> <address> June 1983). </address> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference-contexts: The corresponding BM has N units, one for each random variable in the Bayesian network <ref> [8, 5] </ref>. However, in this case the parallelism of the resulting BM is lost, as no two adjacent units (variables) can be updated at the same time.
Reference: [9] <author> Hinton, G.E., and Sejnowski, T.J., </author> <title> Learning and relearning in Boltzmann machines. </title> <note> Pp. 282-317 in: </note> <editor> Rumelhart, D.E., and McClelland, J.L. (eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1. </volume> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [10] <author> Hopfield, J.J., and Tank, D.W., </author> <title> Neural computation of decisions in optimization problems. </title> <booktitle> Biological Cybernetics 52 (1985), </booktitle> <pages> 141-152. </pages>
Reference: [11] <author> Hrycej, T., </author> <title> Gibbs sampling in Bayesian networks. </title> <booktitle> Artificial Intelligence 46 (1990), </booktitle> <pages> 351-363. </pages>
Reference: [12] <author> Laskey, </author> <title> K.B., Adapting connectionist learning to Bayesian networks. </title> <journal> International Journal of Approximate Reasoning 4 (1990), </journal> <pages> 261-282. </pages>
Reference: [13] <author> Lauritzen, S.L. and Spiegelhalter, </author> <title> D.J., Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Stat. Soc., Ser. </journal> <volume> B 50 (1988) 2, </volume> <pages> 157-224. </pages> <note> Reprinted as pp. 415-448 in Readings in Uncertain Reasoning, </note> <editor> edited by G.Shafer and J.Pearl. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [14] <author> Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, M.N. and Teller, E., </author> <title> Equations of state calculations by fast computing machines. </title> <journal> Journal of Chem. Phys. </journal> <volume> 21 (1953), </volume> <pages> 1087-1092. </pages>
Reference: [15] <author> Myllymaki, P., </author> <title> Bayesian Reasoning by Stochastic Neural Networks. Ph.Lic. </title> <type> Thesis, </type> <institution> Department of Computer Science, University of Helsinki. Report C-1993-67, University of Helsinki, </institution> <month> December </month> <year> 1993. </year>
Reference: [16] <author> Myllymaki, P., and Orponen, P., </author> <booktitle> Programming the Harmonium. </booktitle> <pages> Pp. </pages> <booktitle> 671-677 in Proceedings of the International Joint Conference on Neural Networks (Singapore, November 1991), </booktitle> <volume> Vol. 1. </volume> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference: [17] <author> Neal, </author> <title> R.M., Connectionist learning of belief networks. </title> <booktitle> Artificial Intelligence 56 (1992), </booktitle> <pages> 71-113. </pages>
Reference: [18] <author> Neapolitan, R.E., </author> <title> Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [19] <author> Orponen, P., Floreen, P., Myllymaki, P. and Tirri, H., </author> <title> A neural implementation of conceptual hierarchies with Bayesian reasoning. Pp. </title> <booktitle> 297-303 in Proceedings of the International Joint Conf. on Neural Networks (San Diego, </booktitle> <address> CA, </address> <month> June </month> <year> 1990), </year> <note> Vol. </note> <editor> I. </editor> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [20] <author> Pearl, J., </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Moreover, this type of a Bayesian network is a tree, which means that there is no need for approximate stochastic methods since polynomial time exact algorithms for solving the MLE task exist <ref> [20] </ref>. For a general Bayesian network structure, a similar N -unit construction is possible, if one uses a noisy-OR approximation (see [20, p. 184]) of the probability distribution instead of the accurate form (1)[17]. <p> For a general Bayesian network structure, a similar N -unit construction is possible, if one uses a noisy-OR approximation (see <ref> [20, p. 184] </ref>) of the probability distribution instead of the accurate form (1)[17]. For an accurate representation of the general case, it seems at a first glance that binary connections are not sufficient, but higher-order connections (arcs connecting three or more nodes to each other) are necessary [5].
Reference: [21] <author> Spiegelhalter, </author> <title> D.J., Probabilistic reasoning in predictive expert systems. Pp. </title> <booktitle> 47-67 in Uncertainty in Artificial Intelligence, edited by L.N.Kanal and J.F.Lemmer. </booktitle> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <address> Amsterdam, </address> <year> 1986. </year>
References-found: 20

