URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:expand.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/research/starfish/papers.html
Root-URL: http://www.cs.dartmouth.edu
Email: dfk@cs.dartmouth.edu  
Title: Expanding the Potential for Disk-Directed I/O  
Author: David Kotz 
Address: College  
Affiliation: Department of Computer Science Dartmouth  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:expand.ps.Z  URL ftp://ftp.cs.dartmouth.edu/TR/TR95-254.ps.Z  
Note: Copyright 1995 by IEEE. Appeared in the Symposium on Parallel and Distributed Processing, October 1995, pp. 490-495. Available at  Earlier version available as Dartmouth PCS-TR95-254 at  
Abstract: As parallel computers are increasingly used to run scientific applications with large data sets, and as processor speeds continue to increase, it becomes more important to provide fast, effective parallel file systems for data storage and for temporary files. In an earlier work we demonstrated that a technique we call disk-directed I/O has the potential to provide consistent high performance for large, collective, structured I/O requests. In this paper we expand on this potential by demonstrating the ability of a disk-directed I/O system to read irregular subsets of data from a file, and to filter and distribute incoming data according to data-dependent functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Kotz and Nils Nieuwejaar, </author> <title> Dynamic file-access characteristics of a production parallel scientific workload, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 640-649. </pages>
Reference-contexts: When they find a multiprocessor that is configured with sufficient parallel-I/O hardware (unfortunately, many are not) they often discover that the file system software is not designed to meet their needs <ref> [1] </ref>, or has poor performance [2]. As a result, there are several proposals for new interfaces, run-time libraries, compilers, languages, and file systems to support parallel applications on parallel computers.
Reference: [2] <author> Bill Nitzberg, </author> <title> Performance of the iPSC/860 Concurrent File System, </title> <type> Tech. Rep. </type> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: When they find a multiprocessor that is configured with sufficient parallel-I/O hardware (unfortunately, many are not) they often discover that the file system software is not designed to meet their needs [1], or has poor performance <ref> [2] </ref>. As a result, there are several proposals for new interfaces, run-time libraries, compilers, languages, and file systems to support parallel applications on parallel computers.
Reference: [3] <author> David Kotz, </author> <title> Disk-directed I/O for MIMD multiprocessors, </title> <booktitle> in Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 61-74, </pages> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: The focus of this paper is on a file-system technique called disk-directed I/O, which can dramatically improve the performance of reading and writing a large, regular data structure (like a matrix) between memory that is distributed across many processors and a file that is distributed across many disks <ref> [3] </ref>. There are two ways to look at this paper. One view is that we are exploring the ability of disk-directed I/O to accommodate three extensions: data-dependent distribution This research was funded by NSF under grant number CCR-9404919, and by NASA Ames under agreement number NCC 2-849. <p> This sieve is not data-dependent, and is used to allow the library to make larger, more efficient requests to the file system. Disk-directed I/O. Disk-directed I/O is a technique for optimizing data transfer given a high-level, collective inter face <ref> [3] </ref>. <p> The buckets are then filtered to extract the items of interest, and (in a parallel application) distributed among memories of the multiprocessor according to the application's needs. Clearly this application has different I/O needs from those imagined for the disk-directed-I/O system in <ref> [3] </ref>. It reads an irregular, discontiguous subset of data from the file. It filters out and discards some of the data it reads, after examining the data. Finally, it distributes the data among the memories in a data-dependent manner. <p> Finally, it distributes the data among the memories in a data-dependent manner. In the remainder of the paper, we show how the concept of disk-directed I/O can also include these unusual requirements. 3 Data-dependent distributions In the disk-directed I/O system described in <ref> [3] </ref>, matrices could be read from the file into memory according to one of a variety of distribution patterns. <p> As each block was read from disk (in whatever order was convenient for the disk), the records within that block were sent to the appropriate location in the appropriate memory, based on the distribution function. In <ref> [3] </ref> the distribution function was independent of the data; of course, a data-dependent distribution function could easily be used for the same purpose. A traditional file system, however, is quite different. <p> Of course, for the purpose of this experiment it matters little what distribution function we actually use even a data-independent function would do. We used a cyclic distribution (rc in <ref> [3] </ref>). Thus, the disk-directed system needed no change for this experiment. In the traditional caching system, the compute processors each looped reading blocks from the file, and for each record within each block, sent the record on to the appropriate destination processor. <p> Logically, it made no difference which blocks were read by which processor, since most records would be redistributed anyway. For best I/O performance on contiguous layouts <ref> [3] </ref>, we chose to have compute processors read the blocks in a cyclic distribution. We ran these experiments on our simulator from [3], configured as shown in Table 1. <p> Logically, it made no difference which blocks were read by which processor, since most records would be redistributed anyway. For best I/O performance on contiguous layouts <ref> [3] </ref>, we chose to have compute processors read the blocks in a cyclic distribution. We ran these experiments on our simulator from [3], configured as shown in Table 1. In all cases a 10 Mbyte file was striped across disks, block by block, using one of two block layouts within each disk: contiguous or random. <p> The main point here is that disk-directed I/O was perfectly capable of dealing with irregular access patterns as well as regular access patterns. 6 Conclusions While our earlier work demonstrated the value of disk-directed I/O for large, collective requests, the examples were limited to regular structures with a data-independent distribution <ref> [3] </ref>. In this paper we show that disk-directed I/O could accommodate irregularly structured requests, data-dependent distributions, and data-dependent filtering, with no loss in performance.
Reference: [4] <author> Thomas H. Cormen and David Kotz, </author> <title> Integrating theory and practice in parallel file systems, </title> <booktitle> in Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993, </year> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> pp. 64-74. </note>
Reference-contexts: view is that we are applying one idea from disk-directed I/O (shifting control from the compute nodes to the I/O nodes) to new situations (data-dependent distribution and data-dependent filtering), although our implementation of those techniques is in a disk-directed-I/O system. 2 Background There are many different parallel file systems (see <ref> [4, 5] </ref> for a partial survey). Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read or write.
Reference: [5] <author> Dror G. Feitelson, Peter F. Corbett, Yarson Hsu, and Jean-Pierre Prost, </author> <title> Parallel I/O systems and interfaces for parallel computers, in Multiprocessor Systems Design and Integration, </title> <editor> C.-L. Wu, Ed. </editor> <publisher> World Scientific, </publisher> <year> 1995, </year> <note> To appear. </note>
Reference-contexts: view is that we are applying one idea from disk-directed I/O (shifting control from the compute nodes to the I/O nodes) to new situations (data-dependent distribution and data-dependent filtering), although our implementation of those techniques is in a disk-directed-I/O system. 2 Background There are many different parallel file systems (see <ref> [4, 5] </ref> for a partial survey). Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read or write.
Reference: [6] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary, </author> <title> Design and evaluation of primitives for parallel I/O, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 452-461. </pages>
Reference: [7] <author> Susanne Englert, Jim Gray, Terrye Kocher, and Pra-ful Shah, </author> <title> A benchmark of NonStop SQL Release 2 demonstrating near-linear speedup and scaleup on large databases, </title> <booktitle> in Proceedings of the 1990 ACM Sig-metrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990, </year> <pages> pp. 245-246. </pages>
Reference-contexts: Most, though not all, of the contemporary parallel file systems are designed for machines with an architecture of this type. Some database machines have a data-dependent tuple-filtering function on the IOPs, e.g., Tandem NonStop <ref> [7] </ref>. The Super Database Computer [8] has a load-dependent data-distribution mechanism, in which disk controllers continuously produce tasks that are consumed and processed by CPs.
Reference: [8] <author> Masaru Kitsuregawa, Satoshi Hirano, Masanobu Harada, Minoru Nakamura, and Mikio Takagi, </author> <title> The Super Database Computer (SDC): System architecture, algorithm and preliminary evaluation, </title> <booktitle> in Proceedings of the Twenty-Fifth Annual Hawaii International Conference on System Sciences, 1992, </booktitle> <volume> vol. I, </volume> <pages> pp. 308-319. </pages>
Reference-contexts: Most, though not all, of the contemporary parallel file systems are designed for machines with an architecture of this type. Some database machines have a data-dependent tuple-filtering function on the IOPs, e.g., Tandem NonStop [7]. The Super Database Computer <ref> [8] </ref> has a load-dependent data-distribution mechanism, in which disk controllers continuously produce tasks that are consumed and processed by CPs.
Reference: [9] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh, </author> <title> PASSION runtime library for parallel I/O, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <month> Oct. </month> <year> 1994, </year> <pages> pp. 119-128. </pages>
Reference-contexts: The PASSION library for scientific applications can read submatrices that can be represented as a large contiguous region with some holes of unwanted data, by reading the full region of data and then sieving out the undesired data <ref> [9] </ref>. This sieve is not data-dependent, and is used to allow the library to make larger, more efficient requests to the file system. Disk-directed I/O. Disk-directed I/O is a technique for optimizing data transfer given a high-level, collective inter face [3].
Reference: [10] <author> John F. Karpovich, James C. French, and Andrew S. Grimshaw, </author> <title> High performance access to radio astronomy data: A case study, </title> <booktitle> in Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <month> Sept. </month> <year> 1994, </year> <note> Also available as Univ. of Virginia TR CS-94-25. </note>
Reference-contexts: Compared to a traditional system with caches at the I/O processors, this strategy optimizes the disk accesses, uses less memory (no cache at the I/O processors), and has less CPU and message-passing overhead. An interesting application. Karpovich et al. <ref> [10] </ref> describe the problem of storing and retrieving radioastronomy data sets. The read-mostly data set is large and multi-dimensional: each data point represents an astronomical reading at some time at some frequency on some instrument pointed at some region of the sky.
Reference: [11] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary, </author> <title> Improved parallel I/O via a two-phase run-time access strategy, </title> <booktitle> in IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <year> 1993, </year> <pages> pp. 56-70, </pages> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: With a data-independent distribution, each processor independently computes the locations of the records it requires from the file, and reads those records. With a data-dependent distribution, however, there is no way for processors to request their own set of data. A reasonable solution is similar to two-phase I/O <ref> [11] </ref>: each processor reads some convenient subset of data from the file, examines each record to compute the distribution function, and then sends the data to the appropriate processor. <p> Consider the 64-byte records, which present an interesting picture. Despite nearly doubling the amount of message traffic, traditional caching with data-dependent redistribution was faster than the direct-access version. Here we were essentially using a pipelined form of two-phase I/O <ref> [11] </ref>. Compute processors requested whole blocks from the I/O nodes, rather than small records. The larger requests reduced the overhead at the I/O nodes, which in this case more than offset the extra work at the compute nodes and in the network.
Reference: [12] <author> Nils Nieuwejaar and David Kotz, </author> <title> Low-level interfaces for high-level parallel I/O, </title> <booktitle> in IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <month> Apr. </month> <year> 1995, </year> <pages> pp. 47-62. </pages>
Reference-contexts: How might disk-directed I/O support these kinds of applications? 5.1 Experiments We simulate applications that request an irregular subset of blocks from the file, although we do assume that they could specify the entire subset in a list (e.g., in a batch request <ref> [12] </ref>). The list was in logical sorted order. In the We compare normal DDIO with a DDIO that filtered 90% of all records.
Reference: [13] <author> David Kotz and Carla Schlatter Ellis, </author> <title> Practical prefetching techniques for multiprocessor file systems, </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 33-51, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: In the contiguous layout those mistakes made little difference because those prefetches were quickly completed by the drive's own prefetching. A production system would, of course, use smarter prefetching policies (although something like the Unix prefetching policy does not adapt well to parallel access patterns) <ref> [13, 14] </ref>. Prefetching aside, there were no unusual differences between disk-directed I/O and traditional caching that could be attributed to the irregularity of access.
Reference: [14] <author> R. Hugo Patterson and Garth A. Gibson, </author> <title> Exposing I/O concurrency with informed prefetching, </title> <booktitle> in Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <month> Sept. </month> <year> 1994, </year> <pages> pp. 7-16. </pages>
Reference-contexts: In the contiguous layout those mistakes made little difference because those prefetches were quickly completed by the drive's own prefetching. A production system would, of course, use smarter prefetching policies (although something like the Unix prefetching policy does not adapt well to parallel access patterns) <ref> [13, 14] </ref>. Prefetching aside, there were no unusual differences between disk-directed I/O and traditional caching that could be attributed to the irregularity of access.
Reference: [15] <author> David Kotz, </author> <title> Expanding the potential for disk-directed I/O, </title> <type> Tech. Rep. </type> <institution> PCS-TR95-254, Dept. of Computer Science, Dartmouth College, </institution> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: In particular, how does the user (or compiler) tell the I/O processor about its distribution function, filtration function, and which file data to read? We are beginning to study this issue. Availability Our simulator, the full technical-report version of this paper <ref> [15] </ref>, and many of the papers below, are available at http://www.cs.dartmouth.edu/dfk/.
References-found: 15

