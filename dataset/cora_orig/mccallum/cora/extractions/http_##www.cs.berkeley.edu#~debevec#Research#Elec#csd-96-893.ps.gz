URL: http://www.cs.berkeley.edu/~debevec/Research/Elec/csd-96-893.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~debevec/Research/
Root-URL: 
Email: debevec@cs.berkeley.edu camillo@cs.berkeley.edu malik@cs.berkeley.edu  
Phone: (510) 642 9940 (510) 642 5029 (510) 642 7597  (510) 642 5775 (fax)  
Title: Modeling and Rendering Architecture from Photographs: A hybrid geometry- and image-based approach  
Author: Paul E. Debevec Camillo J. Taylor Jitendra Malik 
Keyword: Image-based modeling, image-based rendering, interactive modeling systems, photogrammetry, reconstruction, view-dependent texture mapping, view interpolation, model-based stereo  
Note: See also: http://www.cs.berkeley.edu/~debevec/Research/  
Affiliation: Computer Science Division, University of California at Berkeley  
Address: 545 Soda Hall 485 Soda Hall 725 Soda Hall  Berkeley, CA 94720-1776  
Abstract: Technical Report UCB//CSD-96-893 January 19, 1996 Abstract We present an approach for creating realistic synthetic views of existing architectural scenes from a sparse set of still photographs. Our approach, which combines both geometry-based and image-based modeling and rendering techniques, has two components. The first component is an easy-to-use photogrammetric modeling system which facilitates the recovery of a basic geometric model of the photographed scene. The modeling system is effective and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo approach can robustly recover accurate depth from image pairs with large baselines. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. As an intermediate result, we present view-dependent texture mapping, a method of better simulating geometric detail on basic models. Our approach can recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach's abilty to create realistic renderings of architectural scenes from viewpoints far from the original photographs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kurt Akeley. </author> <title> Realityengine graphics. </title> <booktitle> In SIGGRAPH '93, </booktitle> <pages> pages 109-116, </pages> <year> 1993. </year>
Reference-contexts: In this case, corresponding points must be computed for a dense sampling of image pixels, a job too tedious to assign to a human, but feasible for a computer to perform using model-based stereo. Of course, many computer graphics systems (e.g. <ref> [1] </ref>) and applications (e.g. Doom by Id Software) already make use of a well-known technique to simulate detail in coarse geometric models: texture-mapping.
Reference: [2] <author> A. Appel. </author> <title> Some techniques for shading machine renderings of solids. </title> <booktitle> In Proceedings of the Spring Joint Computer Conference, </booktitle> <pages> pages 37-45, </pages> <year> 1968. </year> <month> 30 </month>
Reference-contexts: When the model is not convex, it is possible that some parts of the model will shadow others with respect to the camera. Such shadowed regions could be determined using an object-space visible surface algorithm, such as [5], or an image-space ray casting algorithm, such as in <ref> [2] </ref>. We use an image-space shadow map algorithm based on [40] since such an algorithm can be implemented efficiently using standard polygon rendering hardware. Fig. 12, upper left, shows the results of mapping a single image onto the high school building model.
Reference: [3] <author> H. H. Baker and T. O. Binford. </author> <title> Depth from edge and intensity based stereo. </title> <booktitle> In Proceedings of the Seventh IJCAI, Vancouver, BC, </booktitle> <pages> pages 631-636, </pages> <year> 1981. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar. <p> The advantage of this assumption is that it often helps the stereo alogorithm avoid bogus correspondences. In addition, it can be implemented effciently using a dynamic programming technique <ref> [3] </ref>. The disadvantage is that this constraint is violated in situtations where a sufficiently narrow object close to the camera, such as a pole, is observed in front of a larger object further away. <p> The similarity of regions in the key and warped offset images is evaluated using normalized correlation, and a dynamic programming approach is used to enforce the ordering constraint on the resulting disparity map <ref> [3] </ref>. After a disparity map has been computed for the epipolar line, a smoothing procedure is invoked which attempts to merge neighboring regions with similar disparities in order to produce a smoother disparity map. This process also fills in half-occluded regions in the key image with appropriate disparity values.
Reference: [4] <author> P. Besl. </author> <title> Active optical imaging sensors. </title> <editor> In J. Sanz, editor, </editor> <booktitle> Advances in Machine Vision: Architectures and Applications. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: able to successfully determine stereo correspondences, even with baselines that are large with respect to the distance of the objects in the scene. 2.3 Modeling from Range Images Instead of the anthropomorphic approach of using multiple images to reconstruct scene structure, an alternative technique is to use range imaging sensors <ref> [4] </ref> to directly measure depth to various points in the scene. Early versions of these sensors were slow, cumbersome and expensive. Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes.
Reference: [5] <author> F. C. Crow. </author> <title> Shadow algorithms for computer graphics. </title> <booktitle> In SIGGRAPH '77, </booktitle> <pages> pages 242-247, </pages> <year> 1977. </year>
Reference-contexts: When the model is not convex, it is possible that some parts of the model will shadow others with respect to the camera. Such shadowed regions could be determined using an object-space visible surface algorithm, such as <ref> [5] </ref>, or an image-space ray casting algorithm, such as in [2]. We use an image-space shadow map algorithm based on [40] since such an algorithm can be implemented efficiently using standard polygon rendering hardware.
Reference: [6] <author> James L. Crowley, Patrick Stelmaszyk, Thomas Skordas, and Pierre Puget. </author> <title> Measurement and integration of 3-D structures by tracking edge lines. </title> <journal> International Journal of Computer Vision, </journal> <volume> 8(1) </volume> <pages> 29-52, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Attention has turned to using more than two views with image stream methods such as [35] or recursive approaches <ref> [39, 6] </ref>. When a scaled orthography approximation is appropriate, [35] has shown excellent results, but a general solution for the case of perspective projection remains elusive.
Reference: [7] <editor> D.J.Fleet, A.D.Jepson, and M.R.M. Jenkin. </editor> <booktitle> Phase-based disparity measurement. CVGIP: Image Understanding, </booktitle> <volume> 53(2) </volume> <pages> 198-210, </pages> <year> 1991. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [8] <author> O.D. Faugeras, Q.-T. Luong, and S.J. Maybank. </author> <title> Camera self-calibration: theory and experiments. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 321-34, </pages> <year> 1982. </year>
Reference-contexts: A camera's extrinsic parameters, in contrast, relate the camera's frame of reference to the world coordinate system by a 3D rigid transformation. Camera calibration is a well-studied problem both in photogrammetry and computer vision; successful methods include [36] and [9]. Recent results in computer vision <ref> [8] </ref> recover projective structure without calibrated cameras, but can not recover all aspects of affine and euclidean structure without calibration information.
Reference: [9] <author> O.D. Faugeras and G. Toscani. </author> <title> The calibration problem for stereo. </title> <booktitle> In Proceedings IEEE CVPR 86, </booktitle> <pages> pages 15-20, </pages> <year> 1986. </year>
Reference-contexts: A camera's extrinsic parameters, in contrast, relate the camera's frame of reference to the world coordinate system by a 3D rigid transformation. Camera calibration is a well-studied problem both in photogrammetry and computer vision; successful methods include [36] and <ref> [9] </ref>. Recent results in computer vision [8] recover projective structure without calibrated cameras, but can not recover all aspects of affine and euclidean structure without calibration information.
Reference: [10] <author> Olivier Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: More recently, the problem has been studied vigorously in computer vision under the title of structure from motion. Both its mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman [38] and Longuet-Higgins [20], in the early 1980s. Faugeras's book <ref> [10] </ref> provides a synthesis of the state of the art as of 1992. <p> We demonstrate our approach's successful use in an interactive system for building architectural models from photographs. 2.1.1 Camera Calibration Determining structure from multiple views is a simpler problem when the cameras are calibrated, that is, their intrinsic parameters (see <ref> [10] </ref>) are known. A camera's intrinsic parameters do not depend on its position in space and include the focal length, the center of projection, and the aspect ratio of the image pixels. <p> The stereo algorithm that we have developed makes use of this fact by dividing the key image into a series of epipolar lines and determining correspondences along each of these lines independently <ref> [10, 20] </ref>. The similarity of regions in the key and warped offset images is evaluated using normalized correlation, and a dynamic programming approach is used to enforce the ordering constraint on the resulting disparity map [3].
Reference: [11] <author> W. E. L. </author> <title> Grimson. From Images to Surface. </title> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [12] <author> A. Gross and T. Boult. </author> <title> Recovery of generalized cylinders from a single intensity view. </title> <booktitle> In Proceedings of the Image Understanding Workshop, </booktitle> <pages> pages 319-330, </pages> <year> 1990. </year>
Reference: [13] <author> Paul S. Heckbert. </author> <title> Survey of texture mapping. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 6(11) </volume> <pages> 56-67, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Because of self-shadowing, not every point on the model within the camera's viewing frustum is mapped. The original image has been resampled using bilinear interpolation; schemes less prone to aliasing are surveyed in <ref> [13] </ref>. 5.2 Compositing Multiple Images In general, each photograph will view only a piece of the model. Thus, it is usually necessary to use multiple images in order to render the entire model from a novel point of view.
Reference: [14] <author> H. Hoppe, T. DeRose, T. DUchamp, M. Halstead, H. Jin, J. McDonald, J. Schweitzer, and W. Stuetzle. </author> <title> Piecewise smooth surface reconstruction. </title> <booktitle> In ACM SIGGRAPH 94 Proc., </booktitle> <pages> pages 295-302, </pages> <year> 1994. </year>
Reference-contexts: Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes. Algorithms for combining multiple range images from different viewpoints have been developed both in computer vision [42, 30, 28] and in computer graphics <ref> [14, 37] </ref>. In many ways, range image based techniques and photographic techniques are complementary and have their relative advantages and disadvantages.
Reference: [15] <author> William Jepson, Robin Liggett, and Scott Friedman. </author> <title> An environment for real-time urban simulation. </title> <booktitle> In Proceedings of the Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 165-166, </pages> <year> 1995. </year>
Reference-contexts: Large-scale efforts have pushed the campuses of Iowa State University, California State University - Chico, and swaths of downtown Los Angeles <ref> [15] </ref> through the graphics pipeline. Unfortunately, the modeling methods employed in such projects are very labor-intensive. They typically involve surveying the site, locating and digitizing architectural plans (if available), and converting existing CAD data (again, if available).
Reference: [16] <author> D. Jones and J. Malik. </author> <title> Computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <journal> Image and Vision Computing, </journal> <volume> 10(10) </volume> <pages> 699-708, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [17] <author> Craig Kolb, Don Mitchell, and Pat Hanrahan. </author> <title> A realistic camera model for computer graphics. </title> <booktitle> In SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: A camera's intrinsic parameters do not depend on its position in space and include the focal length, the center of projection, and the aspect ratio of the image pixels. Real camera lenses can also introduce significant nonlinear radial distortion into images (an effect modeled in <ref> [17] </ref>), which is usually well-approximated by a simple parametric mapping. A camera's extrinsic parameters, in contrast, relate the camera's frame of reference to the world coordinate system by a 3D rigid transformation. Camera calibration is a well-studied problem both in photogrammetry and computer vision; successful methods include [36] and [9].
Reference: [18] <author> E. Kruppa. </author> <title> Zur ermittlung eines objectes aus zwei perspektiven mit innerer orientierung. </title> <journal> Sitz.-Ber. Akad. Wiss., Wien, Math. Naturw. Kl., Abt. Ila., </journal> <volume> 122 </volume> <pages> 1939-1948, </pages> <year> 1913. </year>
Reference-contexts: This problem has been studied in the area of photogrammetry, the discipline of measuring the world using photographic images, mainly for producing topographic maps. Back in 1913, Kruppa <ref> [18] </ref> proved the fundamental result that given two views of five distinct points, one could recover the rotation and translation between the two camera positions as well as the 3D locations of the points (up to a scale factor).
Reference: [19] <author> J. Liu, J. Mundy, D. Forsyth, and C. Rothwell. </author> <title> Efficient recognition of rotationally symmetric surfaces and straight homogeneous generalized cylinders. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. Recog., </booktitle> <pages> pages 123-128, </pages> <year> 1993. </year>
Reference-contexts: surfaces of revolution (such as domes, columns, and minarets) represent an important component of architecture that should be handled more effectively in the Fa~cade modeling system. (The copper flames atop the pinnacles in Fig. 2 are approximated by polyhedra with 48 faces each.) Fortunately, there has been much work ([12], <ref> [19] </ref> [43]) that presents methods of recovering such structures from photographs. Curved objects are also entirely consistent with our approach to recovering additional detail through model-based stereo: the image warping process and the epipolar constraint in Observation 1 are valid for any model geometry.
Reference: [20] <author> H.C. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <month> September </month> <year> 1981. </year> <month> 31 </month>
Reference-contexts: More recently, the problem has been studied vigorously in computer vision under the title of structure from motion. Both its mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman [38] and Longuet-Higgins <ref> [20] </ref>, in the early 1980s. Faugeras's book [10] provides a synthesis of the state of the art as of 1992. <p> The stereo algorithm that we have developed makes use of this fact by dividing the key image into a series of epipolar lines and determining correspondences along each of these lines independently <ref> [10, 20] </ref>. The similarity of regions in the key and warped offset images is evaluated using normalized correlation, and a dynamic programming approach is used to enforce the ordering constraint on the resulting disparity map [3].
Reference: [21] <author> D. Marr and T. Poggio. </author> <title> A computational theory of human stereo vision. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> 204 </volume> <pages> 301-328, </pages> <year> 1979. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [22] <author> Leonard McMillan and Gary Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: Recently, creating models directly from photographs has received increased interest in computer graphics. Since real images are used as input, such an image-based system has an advantage in producing photorealistic renderings as output. Some of the most promising of these systems <ref> [31, 22] </ref> rely on the computer vision technique of computational stereopsis to automatically determine the structure of the scene from the multiple photographs available. As a consequence, however, these systems are only as strong as the underlying stereo algorithms which recover the structure of the scene. <p> Because of this, the image-based techniques presented have used images that were taken a short distance apart relative to the depth of objects in the scene, and in the case of <ref> [22] </ref>, have also employed significant amounts of user input (see section 2.4) per image pair. These concessions to the weakness of stereo algorithms bode poorly for creating large-scale, freely navigable virtual environments from photographs. <p> Using this property, [41] demonstrated how regularly spaced synthetic images (with their computed depth maps) could be warped and composited in real time to produce a virtual environment. Recently, <ref> [22] </ref> presented a real-time image-based rendering system that used panoramic photographs with depth computed, in part, from stereo correspondence. The system successfully demonstrated how a single real image could be re-rendered from nearby points of view. <p> In comparison, we found that naive correlation-based approaches were unable to compute reasonable depth estimates for more than half of the points. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [41, 31, 22] </ref>. Furthermore, when several images and their corresponding depth maps are available, we use the weighting function developed in the view-dependent texture-mapping method of section 5 to composite multiple renderings.
Reference: [23] <author> Eric N. Mortensen and William A. Barrett. </author> <title> Intelligent scissors for image composition. </title> <booktitle> In SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: Fa~cade is able to compute accurate reconstructions with only a portion of the visible edges marked in any particular image, particularly when the user has embedded constraints in the model. We have also investigated a gradient-based technique as in <ref> [23] </ref> to further assist the user. With the edges marked, the user needs to specify which features in the model correspond to which features in the images. This is accomplished by clicking on an edge in the model and then clicking on the corresponding edge in one of the images.
Reference: [24] <author> Michael Naimark. </author> <title> Displacements. </title> <booktitle> San Francisco Museum of Modern Art, </booktitle> <year> 1984. </year>
Reference-contexts: However, the images will not necessarily agree if there is unmodeled geometric detail in the building, or if the surfaces of the building are not perfectly Lambertian 2 . In this 1 In the art exhibit Displacements <ref> [24] </ref>, Michael Naimark performed such a replacement literally.
Reference: [25] <author> H. K. Nishihara. </author> <title> Practical real-time imaging stereo matcher. </title> <journal> Optical Engineering, </journal> <volume> 23(5) </volume> <pages> 536-545, </pages> <year> 1984. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [26] <author> S. B. Pollard, J. E. W. Mayhew, and J. P. </author> <title> Frisby. A stereo correspondence algorithm using a disparity gradient limit. </title> <journal> Perception, </journal> <volume> 14 </volume> <pages> 449-470, </pages> <year> 1985. </year>
Reference-contexts: Disparity refers to the difference in image location between corresponding features in the two images, which is projectively related to the depth of the feature in the scene. Solving the correspondence problem with computer algorithms has proved hard in spite of years of research on the problem <ref> [3, 7, 11, 16, 21, 25, 26] </ref>. The major sources of difficulty include: 1. Foreshortening. Surfaces in the scene viewed from different positions will be foreshortened differently in the images, causing the image neighborhoods of corresponding pixels to appear dissimilar.
Reference: [27] <author> Harpreet S. Sawhney. </author> <title> Simplifying motion and structure analysis using planar parallax and image warping. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Warping with respect to a single plane (as opposed to an entire model) has also been used to simplify image motion analysis in <ref> [27] </ref>. 6.1 Establishing Stereo Correspondences Once the offset image is warped, the stereo algorithm is ready to begin comparing pixel neighborhoods in order to establish stereo correspondences.
Reference: [28] <author> H. Shum, M. Hebert, K. Ikeuchi, and R. Reddy. </author> <title> An integral approach to free-formed object modeling. </title> <booktitle> ICCV, </booktitle> <pages> pages 870-875, </pages> <year> 1995. </year>
Reference-contexts: Early versions of these sensors were slow, cumbersome and expensive. Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes. Algorithms for combining multiple range images from different viewpoints have been developed both in computer vision <ref> [42, 30, 28] </ref> and in computer graphics [14, 37]. In many ways, range image based techniques and photographic techniques are complementary and have their relative advantages and disadvantages.
Reference: [29] <author> Steven Smith. </author> <title> Geometric Optimization Methods for Adaptive Filtering. </title> <type> PhD thesis, </type> <institution> Harvard University, Division of Applied Sciences, </institution> <address> Cambridge MA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: This non-linear objective function is minimized using a variant of the Newton-Raphson method described in [32]. Our optimization technique is also similar to the one developed in <ref> [29] </ref>. The minimization procedure involves calculating the gradient and Hessian of the objective function with respect to the parameters of the camera and the model. As we have shown, it is simple to construct symbolic expressions for m in terms of the unknown model parameters.
Reference: [30] <author> M. Soucy and D. Lauendeau. </author> <title> Multi-resolution surface modeling from multiple range views. </title> <booktitle> In Proc. IEEE Computer Vision and Pattern Recognition, </booktitle> <pages> pages 348-353, </pages> <year> 1992. </year>
Reference-contexts: Early versions of these sensors were slow, cumbersome and expensive. Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes. Algorithms for combining multiple range images from different viewpoints have been developed both in computer vision <ref> [42, 30, 28] </ref> and in computer graphics [14, 37]. In many ways, range image based techniques and photographic techniques are complementary and have their relative advantages and disadvantages.
Reference: [31] <author> R. Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <booktitle> In IEEE Computer Graphics and Applications, </booktitle> <year> 1996. </year>
Reference-contexts: Recently, creating models directly from photographs has received increased interest in computer graphics. Since real images are used as input, such an image-based system has an advantage in producing photorealistic renderings as output. Some of the most promising of these systems <ref> [31, 22] </ref> rely on the computer vision technique of computational stereopsis to automatically determine the structure of the scene from the multiple photographs available. As a consequence, however, these systems are only as strong as the underlying stereo algorithms which recover the structure of the scene. <p> In comparison, we found that naive correlation-based approaches were unable to compute reasonable depth estimates for more than half of the points. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [41, 31, 22] </ref>. Furthermore, when several images and their corresponding depth maps are available, we use the weighting function developed in the view-dependent texture-mapping method of section 5 to composite multiple renderings.
Reference: [32] <author> Camillo J. Taylor and David J. Kriegman. </author> <title> Minimization on the lie group so(3) and related manifolds. </title> <type> Technical Report 9405, </type> <institution> Center for Systems Science, Dept. of Electrical Engineering, Yale University, </institution> <address> New Haven, CT, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: This non-linear objective function is minimized using a variant of the Newton-Raphson method described in <ref> [32] </ref>. Our optimization technique is also similar to the one developed in [29]. The minimization procedure involves calculating the gradient and Hessian of the objective function with respect to the parameters of the camera and the model.
Reference: [33] <author> Camillo J. Taylor and David J. Kriegman. </author> <title> Structure and motion from line segments in multiple images. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> 17(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: In general, linear algorithms for the problem fail to make use of all available information while nonlinear minimization methods are prone to difficulties arising from local minima in the parameter space. An alternative formulation of the problem tailored to structured environments <ref> [33] </ref> has used line segments instead of points for the as image features, but the previously stated concerns were shown to remain largely valid. <p> O = P Err i where Err i represents the disparity computed for edge feature i. Estimates for the unknown model parameters and camera positions are obtained by minimizing the objective function with respect to these variables. Our system uses the the error function Err i presented in <ref> [33] </ref>, which is described below. 14 Fig. 7 shows how a straight line in the model projects onto the image plane of a camera.
Reference: [34] <author> S. J. Teller, Celeste Fowler, Thomas Funkhouser, and Pat Hanrahan. </author> <title> Partitioning and ordering large radiosity computations. </title> <booktitle> In SIGGRAPH '94, </booktitle> <pages> pages 443-450, </pages> <year> 1994. </year>
Reference-contexts: For complex models where most images are entirely occluded for the typical view, it can be very inefficient to project every original photograph to the novel viewpoint. Some efficient techniques to determine such visibility a priori in architectural scenes through spatial partitioning are presented in <ref> [34] </ref>. 6 Detail Recovery with Model-Based Stereo The modeling system described in Section 4 allows the user to create a basic model of a scene, but in general the scene will have additional geometric detail not captured in the model.
Reference: [35] <author> Carlo Tomasi and Takeo Kanade. </author> <title> Shape and motion from image streams under orthography: a factorization method. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Attention has turned to using more than two views with image stream methods such as <ref> [35] </ref> or recursive approaches [39, 6]. When a scaled orthography approximation is appropriate, [35] has shown excellent results, but a general solution for the case of perspective projection remains elusive. <p> Attention has turned to using more than two views with image stream methods such as <ref> [35] </ref> or recursive approaches [39, 6]. When a scaled orthography approximation is appropriate, [35] has shown excellent results, but a general solution for the case of perspective projection remains elusive. In general, linear algorithms for the problem fail to make use of all available information while nonlinear minimization methods are prone to difficulties arising from local minima in the parameter space.
Reference: [36] <author> Roger Tsai. </author> <title> A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 3(4) </volume> <pages> 323-344, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: A camera's extrinsic parameters, in contrast, relate the camera's frame of reference to the world coordinate system by a 3D rigid transformation. Camera calibration is a well-studied problem both in photogrammetry and computer vision; successful methods include <ref> [36] </ref> and [9]. Recent results in computer vision [8] recover projective structure without calibrated cameras, but can not recover all aspects of affine and euclidean structure without calibration information.
Reference: [37] <author> Greg Turk and Marc Levoy. </author> <title> Zippered polygon meshes from range images. </title> <booktitle> In SIGGRAPH '94, </booktitle> <pages> pages 311-318, </pages> <year> 1994. </year>
Reference-contexts: Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes. Algorithms for combining multiple range images from different viewpoints have been developed both in computer vision [42, 30, 28] and in computer graphics <ref> [14, 37] </ref>. In many ways, range image based techniques and photographic techniques are complementary and have their relative advantages and disadvantages.
Reference: [38] <author> S. Ullman. </author> <title> The Interpretation of Visual Motion. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1979. </year>
Reference-contexts: More recently, the problem has been studied vigorously in computer vision under the title of structure from motion. Both its mathematical and algorithmic aspects have been explored starting from the fundamental work of Ullman <ref> [38] </ref> and Longuet-Higgins [20], in the early 1980s. Faugeras's book [10] provides a synthesis of the state of the art as of 1992.
Reference: [39] <author> Thierry Vieville and Olivier Faugeras. </author> <title> Feed-forward recovery of motion and structure from a sequence of 2d-lines matches. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> page 517. </pages> <publisher> IEEE, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: Attention has turned to using more than two views with image stream methods such as [35] or recursive approaches <ref> [39, 6] </ref>. When a scaled orthography approximation is appropriate, [35] has shown excellent results, but a general solution for the case of perspective projection remains elusive.
Reference: [40] <author> L Williams. </author> <title> Casting curved shadows on curved surfaces. </title> <booktitle> In SIGGRAPH '78, </booktitle> <pages> pages 270-274, </pages> <year> 1978. </year>
Reference-contexts: Such shadowed regions could be determined using an object-space visible surface algorithm, such as [5], or an image-space ray casting algorithm, such as in [2]. We use an image-space shadow map algorithm based on <ref> [40] </ref> since such an algorithm can be implemented efficiently using standard polygon rendering hardware. Fig. 12, upper left, shows the results of mapping a single image onto the high school building model. The recovered camera position for the projected image is indicated in the lower left corner of the image.
Reference: [41] <author> Lance Williams and Eric Chen. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIGGRAPH '93, </booktitle> <year> 1993. </year>
Reference-contexts: A principal attraction of image-based rendering is that it offers a method of rendering arbitrarily complex scenes with a constant amount of computation required per pixel. Using this property, <ref> [41] </ref> demonstrated how regularly spaced synthetic images (with their computed depth maps) could be warped and composited in real time to produce a virtual environment. Recently, [22] presented a real-time image-based rendering system that used panoramic photographs with depth computed, in part, from stereo correspondence. <p> The lower right picture shows the results of compositing renderings of all twelve original images. Some pixels near the front edge of the roof not seen in any image have been filled in with the hole-filling algorithm from <ref> [41] </ref>. 21 case, the best image to use is clearly the one with the viewing angle closest to that of the rendered view. However, using the image closest in angle at every pixel means that neighboring rendered pixels may be sampled from different original images. <p> Any regions in the composite image which are occluded in every projected image are filled in using the hole-filling method from <ref> [41] </ref>. In the discussion so far, projected image weights are computed at every pixel of every projected rendering. Since the weighting function is smooth (though not constant) across flat surfaces, it is not generally not necessary to compute it for every pixel of every face of the model. <p> In comparison, we found that naive correlation-based approaches were unable to compute reasonable depth estimates for more than half of the points. Once a depth map has been computed for a particular image, we can rerender the scene from novel viewpoints using the methods described in <ref> [41, 31, 22] </ref>. Furthermore, when several images and their corresponding depth maps are available, we use the weighting function developed in the view-dependent texture-mapping method of section 5 to composite multiple renderings.
Reference: [42] <author> Y.Chen and G. Medioni. </author> <title> Object modeling from multiple range images. </title> <journal> Image and Vision Computing, </journal> <volume> 10(3) </volume> <pages> 145-155, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Early versions of these sensors were slow, cumbersome and expensive. Altough many improvements have been made, the most convenient application currently is for human scale objects and not for architectural scenes. Algorithms for combining multiple range images from different viewpoints have been developed both in computer vision <ref> [42, 30, 28] </ref> and in computer graphics [14, 37]. In many ways, range image based techniques and photographic techniques are complementary and have their relative advantages and disadvantages.
Reference: [43] <author> Mourad Zerroug and Ramakant Nevatia. </author> <title> Segmentation and recovery of shgcs from a real intensity image. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 319-330, </pages> <year> 1994. </year> <month> 33 </month>
Reference-contexts: of revolution (such as domes, columns, and minarets) represent an important component of architecture that should be handled more effectively in the Fa~cade modeling system. (The copper flames atop the pinnacles in Fig. 2 are approximated by polyhedra with 48 faces each.) Fortunately, there has been much work ([12], [19] <ref> [43] </ref>) that presents methods of recovering such structures from photographs. Curved objects are also entirely consistent with our approach to recovering additional detail through model-based stereo: the image warping process and the epipolar constraint in Observation 1 are valid for any model geometry.
References-found: 43

