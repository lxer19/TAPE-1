URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-77-97.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: martin@idsia.ch  nic@idsia.ch  juergen@idsia.ch  
Title: Predictability Minimization  
Author: Martin Eldracher Nicol N. Schraudolph Jurgen Schmidhuber 
Date: August 31, 1997 revised December 19, 1997  
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36, CH-6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Note: Processing Images by Semi-Linear  
Abstract: Technical Report IDSIA-77-97 Abstract In the predictability minimization approach (Schmidhuber, 1992), input patterns are fed into a system consisting of adaptive, initially unstructured feature detectors. There are also adaptive predictors constantly trying to predict current feature detector outputs from other feature detector outputs. Simultaneously, however, the feature detectors try to become as unpredictable as possible, resulting in a co-evolution of predictors and feature detectors. This report describes the implementation of a visual processing system trained by semi-linear predictability minimization, and presents many experiments that examine its response to artificial and real-world images. In particular, we observe that under a wide variety of conditions, predictability minimization results in the development of well-known visual feature detectors. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. 757-763. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Atick, J. J., Li, Z., and Redlich, A. N. </author> <year> (1992). </year> <title> Understanding retinal color coding from first principles. </title> <journal> Neural Computation, </journal> <volume> 4 (4), </volume> <pages> 559-572. </pages>
Reference: <author> Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. </author> <year> (1989). </year> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1 (3), </volume> <pages> 412-423. </pages>
Reference: <author> Barrow, H. G. </author> <year> (1987). </year> <title> Learning receptive fields. </title> <booktitle> In Proceedings of the IEEE 1st Annual Conference on Neural Networks, </booktitle> <volume> Vol. IV, </volume> <pages> pp. 115-121. </pages> <publisher> IEEE. </publisher>
Reference: <author> Bell, A. J., and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7 (6), </volume> <pages> 1129-1159. </pages>
Reference-contexts: In other words, we obtain a distributed rather than local encoding. * Unlike a number of other approaches, there is no need to perform expensive, non-local calculations, such as computing the derivatives of determinants of covariance matrices (Linsker, 1988), or inverting a weight matrix <ref> (Bell and Sejnowski, 1995) </ref>. * Unlike e.g.
Reference: <author> Cardoso, J.-F., and Souloumiac, A. </author> <year> (1993). </year> <title> Blind beamforming for non Gaussian signals. </title> <journal> IEE Proceedings-F, </journal> <volume> 140 (6), </volume> <pages> 362-370. </pages>
Reference: <author> Deco, G., and Parra, L. </author> <year> (1997). </year> <title> Non-linear feature extraction by redundancy reduction in an unsupervised stochastic neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 10 (4), </volume> <pages> 683-691. </pages>
Reference: <author> Field, D. J. </author> <year> (1994). </year> <title> What is the goal of sensory coding?. </title> <journal> Neural Computation, </journal> <volume> 6 (4), </volume> <pages> 559-601. </pages>
Reference: <author> Foldiak, P. </author> <year> (1990). </year> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64, </volume> <pages> 165-170. </pages>
Reference: <author> Hochreiter, S., and Schmidhuber, J. H. </author> <year> (1998). </year> <title> Feature extraction through LOCOCODE. Neural Computation, </title> <publisher> in press. </publisher>
Reference-contexts: For efficiency reasons, most statistical classifiers (e.g., Bayesian pattern classi fiers) assume statistical independence of their input variables. * Like Lococode <ref> (Hochreiter and Schmidhuber, 1998) </ref>, but unlike most ICA methods, PM does not need to know the number of statistically independent sources in the input a priori. 48 * Like Lococode and the symplectic map approach (Parra, Deco, and Miesbach, 1996), PM has a potential to discover non-linear redundancy in the input
Reference: <author> Lindstadt, S. </author> <year> (1993). </year> <title> Comparison of two unsupervised neural network models for redundancy reduction. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S. (Eds.), </editor> <booktitle> Proc. 1993 Connectionist Models Summer School, </booktitle> <pages> pp. 308-315. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21, </volume> <pages> 105-117. </pages>
Reference-contexts: In other words, we obtain a distributed rather than local encoding. * Unlike a number of other approaches, there is no need to perform expensive, non-local calculations, such as computing the derivatives of determinants of covariance matrices <ref> (Linsker, 1988) </ref>, or inverting a weight matrix (Bell and Sejnowski, 1995). * Unlike e.g.
Reference: <author> Miller, K. D. </author> <year> (1994). </year> <title> A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between on-and off-center inputs. </title> <journal> Journal of Neuroscience, </journal> <volume> 14 (1), </volume> <pages> 409-441. </pages>
Reference: <author> Molgedey, L., and Schuster, H. G. </author> <year> (1994). </year> <title> Separation of independent signals using time-delayed correlations. </title> <journal> Phys. Reviews Letters, </journal> <volume> 72 (23), </volume> <pages> 3634-3637. </pages>
Reference: <author> Parra, L., Deco, G., and Miesbach, S. </author> <year> (1996). </year> <title> Statistical independence and novelty detection with information preserving nonlinear maps. </title> <journal> Neural Computation, </journal> <volume> 8 (2), </volume> <pages> 260-269. </pages>
Reference-contexts: (e.g., Bayesian pattern classi fiers) assume statistical independence of their input variables. * Like Lococode (Hochreiter and Schmidhuber, 1998), but unlike most ICA methods, PM does not need to know the number of statistically independent sources in the input a priori. 48 * Like Lococode and the symplectic map approach <ref> (Parra, Deco, and Miesbach, 1996) </ref>, PM has a potential to discover non-linear redundancy in the input data, and to generate appropriate redundancy-free codes. * Like binary information gain optimization (Schraudolph and Sejnowski, 1993), but unlike most other "neural" methods (see references above), existing variants of PM create binary rather than continuous
Reference: <author> Rubner, J., and Schulten, K. </author> <year> (1990). </year> <title> Development of feature detectors by self-organization: A network model. </title> <journal> Biological Cybernetics, </journal> <volume> 62, </volume> <pages> 193-199. </pages>
Reference-contexts: But how can this goal be achieved in a massively parallel, local, efficient, and biologically plausible way? PM's relationship to other methods was already discussed in Schmidhuber et al. (1996); here we briefly recapitulate its potential advantages over related approaches: * Unlike certain inherently sequential methods <ref> (e.g. Rubner and Schulten, 1990) </ref>, PM can be implemented in parallel. * Unlike e.g. Barrow (1987), there may be many simultaneously active code units (multiple "winners" instead of just one), as long as they represent different aspects of the environment.
Reference: <author> Rubner, J., and Tavan, P. </author> <year> (1989). </year> <title> A self-organization network for principal-component analysis. </title> <journal> Europhysics Letters, </journal> <volume> 10, </volume> <pages> 693-698. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4 (6), </volume> <pages> 863-879. </pages>
Reference-contexts: This conditional expectation will typically be different from the actual activation of the code unit. The code units, however, are trained (in our experiments also via the online delta rule) to maximize essentially the same objective <ref> (Schmidhuber, 1992) </ref>: V C = i;p p p In this way predictors and code units are caught in an evolutionary arms race forcing them to co-evolve by struggling against each other.
Reference: <author> Schmidhuber, J. H. </author> <year> (1993). </year> <institution> Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Habilita-tionsschrift, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. H. </author> <year> (1998). </year> <title> Neural predictors for detecting and removing redundant information. </title>
Reference-contexts: For efficiency reasons, most statistical classifiers (e.g., Bayesian pattern classi fiers) assume statistical independence of their input variables. * Like Lococode <ref> (Hochreiter and Schmidhuber, 1998) </ref>, but unlike most ICA methods, PM does not need to know the number of statistically independent sources in the input a priori. 48 * Like Lococode and the symplectic map approach (Parra, Deco, and Miesbach, 1996), PM has a potential to discover non-linear redundancy in the input
Reference: <editor> In Cruse, H., Dean, J., and Ritter, H. (Eds.), </editor> <title> Adaptive Behavior and Learning. </title> <type> Kluwer. </type> <note> In preparation. </note>
Reference: <author> Schmidhuber, J. H., Eldracher, M., and Foltin, B. </author> <year> (1996). </year> <title> Semilinear predictability minimization produces well-known feature detectors. </title> <journal> Neural Computation, </journal> <volume> 8 (4), </volume> <pages> 773-786. </pages>
Reference: <author> Schraudolph, N. N., and Sejnowski, T. J. </author> <year> (1993). </year> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 499-506. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 50 </month>
Reference-contexts: know the number of statistically independent sources in the input a priori. 48 * Like Lococode and the symplectic map approach (Parra, Deco, and Miesbach, 1996), PM has a potential to discover non-linear redundancy in the input data, and to generate appropriate redundancy-free codes. * Like binary information gain optimization <ref> (Schraudolph and Sejnowski, 1993) </ref>, but unlike most other "neural" methods (see references above), existing variants of PM create binary rather than continuous codes.
References-found: 23

