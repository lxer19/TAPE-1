URL: http://www.cs.cmu.edu/~lafferty/ps/smts.ps
Refering-URL: http://www.cs.cmu.edu/~lafferty/publications.html
Root-URL: 
Title: Statistical Models for Text Segmentation  
Author: DOUG BEEFERMAN ADAM BERGER JOHN LAFFERTY 
Address: Pittsburgh, PA 15213 USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model by identifying features correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric enables a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Beeferman, A. Berger, and J. Lafferty. </author> <title> A model of lexical attraction and repulsion. </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> Madrid, Spain, </address> <year> 1997. </year>
Reference-contexts: Another approach, using maximum entropy methods, introduces parameters for trigger pairs of mutually informative words, so that occurrences of certain words in recent context boost the probabilities of the words that they trigger [16]. STATISTICAL MODELS FOR TEXT SEGMENTATION 11 The method we use here, described in <ref> [1] </ref>, starts with a trigram model as a prior, or default distribution, and tacks onto the model a set of features to account for the long-range lexical properties of language.
Reference: 2. <author> A. Berger, S. Della Pietra, and V. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71, </pages> <year> 1996. </year>
Reference-contexts: A comparison of these techniques appears in [24]. 3. A Feature-Based Approach Using Exponential Models Our approach to the segmentation problem is based on a statistical framework that we call feature induction for random fields and exponential models <ref> [2, 5] </ref>. The idea is to construct a model that assigns a probability to the end of every sentence| the probability that that there exists a boundary between that sentence and the next. <p> Similar effects appear in the feature induction results for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers <ref> [2, 5] </ref>. The discussion in [5] also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting.
Reference: 3. <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: The parameters of this algorithm were optimized on the test set to give the lowest possible error rate. the CART approach for inducing decision trees <ref> [3] </ref>, using entropy as the impurity function to evaluate questions. <p> These parameters are trained using the EM algorithm on heldout data [15]. Our experience has been that this smoothing procedure compares favorably to CART pruning algorithms <ref> [3] </ref> that are used to reduce the size of the tree, and thereby improve the robustness of the distributions at the leaves.
Reference: 4. <author> M. Christel, T. Kanade, M. Mauldin, R. Reddy, M. Sirbu, S. Stevens, and H. Wactlar. </author> <title> Informedia digital video library. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 57-58, </pages> <year> 1995. </year>
Reference-contexts: This can manifest itself in quite unfortunate ways. For example, a video-on-demand application (such as the one described in <ref> [4] </ref>) responding to a query about a recent news event may provide the user with a news clip related to the event, followed or preceded by part of an unrelated story or even a commercial. We take a feature-based approach to the problem of detecting segment boundaries.
Reference: 5. <author> S. Della Pietra, V. Della Pietra, and J. Lafferty. </author> <title> Inducing features of random fields. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(4) </volume> <pages> 380-393, </pages> <month> April, </month> <year> 1997. </year>
Reference-contexts: A comparison of these techniques appears in [24]. 3. A Feature-Based Approach Using Exponential Models Our approach to the segmentation problem is based on a statistical framework that we call feature induction for random fields and exponential models <ref> [2, 5] </ref>. The idea is to construct a model that assigns a probability to the end of every sentence| the probability that that there exists a boundary between that sentence and the next. <p> This formula makes clear that the algorithm is choosing the model so that the features' expected values with respect to the model are the same as their expected values with respect to the data. One can also employ the "improved iterative scaling" algorithm <ref> [5] </ref>, which uses a slightly different update procedure, to achieve faster convergence. <p> Similar effects appear in the feature induction results for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers <ref> [2, 5] </ref>. The discussion in [5] also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting. <p> Similar effects appear in the feature induction results for the segmentation problem in the following sections. For the details on feature induction and other examples of it in action, we refer to the papers [2, 5]. The discussion in <ref> [5] </ref> also explains how the feature induction algorithm generalizes decision trees. While decision trees recursively partition the training data, the features in an exponential model can be overlapping, so that the scheme is much less prone to overfitting.
Reference: 6. <editor> Topic Detection and Tracking Workshop. </editor> <booktitle> Unpublished workshop notes, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: Their cue phrases are drawn from an empirically selected list of words [10], while in our approach we allow all of the words in a fixed vocabulary to participate as candidate features. 2.3. TDT pilot study The Topic Detection and Tracking (TDT) pilot study <ref> [6] </ref> carried out during 1997 led to the development of several new and complementary approaches to the segmentation problem, and these approaches were quantitatively evaluated using the metric described in Section 8. <p> BEEFERMAN, A. BERGER, AND J. LAFFERTY the static trigram model, as a function of relative position within a segment. The data was collected on Reuters stories from the TDT corpus <ref> [6] </ref>. In this plot the position labeled 0 on the x-axis corresponds to a boundary and the position labeled 100 (100) corresponds to 100 words after (before) the beginning of a segment. <p> A more reasonable choice for D, which focuses the probability mass on small distances, is D = E , an exponential distribution with mean 1 , fixed at the mean document length for the domain. Another, offered by Doddington <ref> [6] </ref>, is to let D = D k have all its probability mass at a fixed distance k. The computation of the metric can then be visualized as two probes, a fixed distance apart, sweeping across the corpus (Figure 11). <p> The default segmentation models, described in the text, are also presented. The quantitative results for the TDT models are collected in Figure 13. These results are part of a much more extensive study carried out by several research groups in the course of the TDT project <ref> [6, 24, 19] </ref>. The exponential model evaluated here was the result of inducing 100 features on a training corpus of two million words of CNN transcriptions, and evaluated on the CNN portion of the TDT corpus. No batch selection or event discarding was used to speed up training.
Reference: 7. <author> A. Gelman, J. Carlin, H. Stern, and D. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: For instance, a 200; 000-word sample containing 11; 303 training events harbored only 322 positive examples. The statistical technique of importance sampling <ref> [7] </ref> typically used in performing Monte Carlo estimation of an integral, suggests an optimization: bias our event sampling to include more positive examples so that the feature induction algorithm has a better chance to "learn" the features which suggest the presence of a topic boundary.
Reference: 8. <author> M. A. Hearst. </author> <title> Multi-paragraph segmentation of expository text. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1994. </year>
Reference-contexts: The TextTiling algorithm, introduced by Hearst <ref> [8] </ref>, is a simple, domain-independent technique that assigns a score to each topic boundary candidate (inter-sentence gap) based on a cosine similarity measure between chunks of words appearing to the left and right of the candidate.
Reference: 9. <author> M. A. Hearst. TextTiling: </author> <title> Segmenting text into multi-paragraph subtopic passages. </title> <journal> Computational Linguistics, </journal> <volume> 23(1) </volume> <pages> 33-64, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: However, TextTiling is designed for a slightly different problem than the one addressed in this study. Since it is designed to identify the subtopics within a single text and not to find breaks between consecutive documents <ref> [9] </ref>, a STATISTICAL MODELS FOR TEXT SEGMENTATION 3 comparison of TextTiling with the system we propose is difficult. Furthermore, TextTiling segments at the paragraph level, while this work doesn't assume the presence of explicit paragraph boundaries. <p> The effect of the topicality features is essentially the same when we use only self triggers or non-self triggers. A comparison to the TextTiling approach was also made, using the "blocks" version of TextTiling <ref> [9] </ref> run with parameters (w; k; n; s) optimized on the test set data. Since paragraph boundaries are absent in the broadcast news data, each inter-sentence gap in the data was a potential boundary candidate.
Reference: 10. <author> J. Hirschberg and D. J. Litman. </author> <title> Disambiguation of cue phrases. </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 501-530, </pages> <month> September </month> <year> 1993. </year> <title> STATISTICAL MODELS FOR TEXT SEGMENTATION 35 </title>
Reference-contexts: Passoneau and Litman's approach, like ours, chooses from a space of candidate features, some of which are similar to the cue-word features we employ. Their cue phrases are drawn from an empirically selected list of words <ref> [10] </ref>, while in our approach we allow all of the words in a fixed vocabulary to participate as candidate features. 2.3.
Reference: 11. <author> F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. </author> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 293-295, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: One means of injecting long-range awareness into a language model is by retaining a cache of the most recently seen n-grams which is combined (typically by linear interpolation) with the static model; see for example <ref> [11, 14] </ref>. Another approach, using maximum entropy methods, introduces parameters for trigger pairs of mutually informative words, so that occurrences of certain words in recent context boost the probabilities of the words that they trigger [16].
Reference: 12. <author> S. Katz. </author> <title> Estimation of probabilities from sparse data for the langauge model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: Typically, one computes the parameters of a trigram model using a modified maximum-likelihood approach, such as that described in <ref> [12] </ref>. For the purposes of this study, we constructed 10 D. BEEFERMAN, A. BERGER, AND J. LAFFERTY two different trigram models. The parameters of the first, especially suited for financial newswire text, were tuned to approximately 38 million words of archived Wall Street Journal (henceforth WSJ) articles.
Reference: 13. <author> H. Kozima. </author> <title> Text segmentation based on similarity between words. </title> <booktitle> In Proceedings of the ACL (Student Session), </booktitle> <year> 1993. </year>
Reference-contexts: Another approach, introduced by Reynar [20], is a graphically motivated segmentation technique called dotplotting. This technique depends exclusively on word repetition to find tight regions of topic similarity. Instead of focusing on strict lexical repetition, Kozima <ref> [13] </ref> uses a semantic network to track cohesiveness of a document in a lexical cohesion profile.
Reference: 14. <author> R. Kuhn and R. de Mori. </author> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 570-583, </pages> <year> 1990. </year>
Reference-contexts: One means of injecting long-range awareness into a language model is by retaining a cache of the most recently seen n-grams which is combined (typically by linear interpolation) with the static model; see for example <ref> [11, 14] </ref>. Another approach, using maximum entropy methods, introduces parameters for trigger pairs of mutually informative words, so that occurrences of certain words in recent context boost the probabilities of the words that they trigger [16].
Reference: 15. <author> J. Lafferty. </author> <title> Barking up the right tree: Estimating 's for decision trees using the EM algorithm. </title> <institution> IBM Research Report, </institution> <year> 1993. </year>
Reference-contexts: The resulting model is naturally thought of as being comprised of a series of Hidden Markov Models, one for each path from root to leaf, with shared parameters. These parameters are trained using the EM algorithm on heldout data <ref> [15] </ref>. Our experience has been that this smoothing procedure compares favorably to CART pruning algorithms [3] that are used to reduce the size of the tree, and thereby improve the robustness of the distributions at the leaves.
Reference: 16. <author> R. Lau, R. Rosenfeld, and S. Roukos. </author> <title> Adaptive language modeling using the maximum entropy principle. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pages 108-113. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Another approach, using maximum entropy methods, introduces parameters for trigger pairs of mutually informative words, so that occurrences of certain words in recent context boost the probabilities of the words that they trigger <ref> [16] </ref>. STATISTICAL MODELS FOR TEXT SEGMENTATION 11 The method we use here, described in [1], starts with a trigram model as a prior, or default distribution, and tacks onto the model a set of features to account for the long-range lexical properties of language.
Reference: 17. <author> D. J. Litman and R. J. Passonneau. </author> <title> Combining multiple knowledge sources for discourse segmentation. </title> <booktitle> In Proceedings of the ACL, </booktitle> <year> 1995. </year>
Reference-contexts: current image and an image near the last segment boundary? * Are there blank video frames nearby? * Is there a sharp change in the audio stream in the next utterance? The idea of using features is a natural one, and indeed other recent work on segmentation adopts this approach <ref> [17] </ref>. Our approach differs in how we collect and incorporate the information provided by the features, as described below. 3.1.
Reference: 18. <author> R. J. Passoneau and D. J. Litman. </author> <title> Discourse segmentation by human and automated means. </title> <journal> Computational Linguistics, </journal> <volume> 23(1) </volume> <pages> 103-139, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: Kozima generalizes lexical cohesiveness to apply to a window of text, and plots the cohesiveness of successive text windows in a document, identifying the valleys in the measure as segment boundaries. 2.2. Combining features with decision trees Passoneau and Litman <ref> [18] </ref> present an algorithm for identifying topic boundaries that uses decision trees to combine multiple linguistic features extracted from corpora of spoken text.
Reference: 19. <author> J. Ponte and W. B. Croft. </author> <title> Text segmentation by topic. </title> <booktitle> In Proceedings of the First European Conference Research and Advanced Technology for on Digital Libraries, </booktitle> <pages> pages 120-129, </pages> <year> 1997. </year>
Reference-contexts: In this approach, finding story boundaries is equivalent to finding topic transitions, and the stories are generated using unigram language models that depend on the hidden class of the segment. Ponte <ref> [19] </ref> developed an approach based on information retrieval methods such as local context analysis [23], a technique 4 D. BEEFERMAN, A. BERGER, AND J. LAFFERTY that uses co-occurrence data to map a query text into semantically related words and phrases. A comparison of these techniques appears in [24]. 3. <p> The default segmentation models, described in the text, are also presented. The quantitative results for the TDT models are collected in Figure 13. These results are part of a much more extensive study carried out by several research groups in the course of the TDT project <ref> [6, 24, 19] </ref>. The exponential model evaluated here was the result of inducing 100 features on a training corpus of two million words of CNN transcriptions, and evaluated on the CNN portion of the TDT corpus. No batch selection or event discarding was used to speed up training.
Reference: 20. <author> J. C. Reynar. </author> <title> An automatic method of finding topic boundaries. </title> <booktitle> In Proceedings of the ACL (Student Session), </booktitle> <year> 1994. </year>
Reference-contexts: Applications such as video retrieval may use speech recognition transcripts or closed captions that lack structural markup. Since TextTiling is widely used and implemented, we examine its behavior on our task in Section 9. Another approach, introduced by Reynar <ref> [20] </ref>, is a graphically motivated segmentation technique called dotplotting. This technique depends exclusively on word repetition to find tight regions of topic similarity. Instead of focusing on strict lexical repetition, Kozima [13] uses a semantic network to track cohesiveness of a document in a lexical cohesion profile. <p> It is natural to expect that in a segmenter, close should count for something. One suggestion <ref> [20] </ref> is to redefine correct to mean "hypothesized within some constant-sized window of units away from a reference boundary," but this approach seems too forgiving; after all, a "right on the nose" segmenter should outscore an "always close" segmenter. 24 D. BEEFERMAN, A. BERGER, AND J.
Reference: 21. <author> R. Rosenfeld. </author> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 10 </volume> <pages> 187-228, </pages> <year> 1996. </year>
Reference-contexts: The features are trigger pairs, automatically discovered by analyzing a corpus of text using a mutual information heuristic described in <ref> [21] </ref>. Figure 2 contains a sample of the (s; t) trigger pairs used in the BN long-range model.
Reference: 22. <author> G. Salton, J. Allan, and C. Buckley. </author> <title> Approaches to passage retrieval in full text information systems. </title> <booktitle> In Proceedings of the 16th Annual International ACM/SIGIR Conference, </booktitle> <pages> pages 49-58, </pages> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference: 23. <author> J. Xu and W.B. Croft. </author> <title> Query expansion using local and global document analysis. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 4-11, </pages> <year> 1996. </year>
Reference-contexts: In this approach, finding story boundaries is equivalent to finding topic transitions, and the stories are generated using unigram language models that depend on the hidden class of the segment. Ponte [19] developed an approach based on information retrieval methods such as local context analysis <ref> [23] </ref>, a technique 4 D. BEEFERMAN, A. BERGER, AND J. LAFFERTY that uses co-occurrence data to map a query text into semantically related words and phrases. A comparison of these techniques appears in [24]. 3.
Reference: 24. <author> J. Yamron. </author> <title> Topic detection and tracking segmentation task. In Broadcast News Transcription and Understanding Workshop, </title> <address> Lansdowne, VA, </address> <year> 1998. </year>
Reference-contexts: Ponte [19] developed an approach based on information retrieval methods such as local context analysis [23], a technique 4 D. BEEFERMAN, A. BERGER, AND J. LAFFERTY that uses co-occurrence data to map a query text into semantically related words and phrases. A comparison of these techniques appears in <ref> [24] </ref>. 3. A Feature-Based Approach Using Exponential Models Our approach to the segmentation problem is based on a statistical framework that we call feature induction for random fields and exponential models [2, 5]. <p> The default segmentation models, described in the text, are also presented. The quantitative results for the TDT models are collected in Figure 13. These results are part of a much more extensive study carried out by several research groups in the course of the TDT project <ref> [6, 24, 19] </ref>. The exponential model evaluated here was the result of inducing 100 features on a training corpus of two million words of CNN transcriptions, and evaluated on the CNN portion of the TDT corpus. No batch selection or event discarding was used to speed up training. <p> In Figure 13 we also list the performance of Dragon's HMM approach, which was run on the identical test data set <ref> [24] </ref>. On this particular data set our approach using exponential models performed better than the HMM, with accuracies 9.5% and 16.7% respectively, but the exponential model performed worse on the portion of the TDT corpus comprised of Reuters newswire articles, where the accuracies were 15.5% and 12.3% [24]. <p> test data set <ref> [24] </ref>. On this particular data set our approach using exponential models performed better than the HMM, with accuracies 9.5% and 16.7% respectively, but the exponential model performed worse on the portion of the TDT corpus comprised of Reuters newswire articles, where the accuracies were 15.5% and 12.3% [24]. An additional set of models was built on broadcast news data in order to evaluate the relative contributions made by the different types of features.

References-found: 24

