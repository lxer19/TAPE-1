URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-290.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Title: SHAPE: A Parallelization Tool for Sparse Matrix Computations  
Author: Sesh Venugopal Vijay K. Naik T. J. 
Address: New Brunswick, NJ 08903.  Yorktown Heights, NY 10598.  
Affiliation: Dept. of Computer Science Rutgers University  IBM Research Division  Watson Research Center  
Abstract: We describe the design, implementation and performance of a Sparse Hybrid Automatic Parallelization Environment (SHAPE). SHAPE partitions and schedules sparse matrix computations for Cholesky factorization with the goal of achieving good performance at low cost, while providing flexibility for use as an experimental tool. It employs efficient parallelization algorithms which reduce the communication cost without adversely affecting the load balance by using a hybrid mixture of column and block partitions. Through several parameters, SHAPE aims for portability across a diverse range of sparse matrix structures and message-passing multiprocessors with different communication cost parameters. We present preliminary timing results on the iPSC/860 and compare the performance of SHAPE with that of a commonly used column-based method. The results show that SHAPE significantly reduces computation time, number of messages, and overall communication time for a variety of test matrices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. L. Alvarado. </author> <title> The sparse matrix manipulation system users manual. </title> <type> Technical report, </type> <institution> University of Wisconsin, Madison, Wisc., </institution> <year> 1990. </year>
Reference-contexts: To understand the details of the Cholesky factorization algorithm, we show the element-level for j = 1 to N for each k 2 <ref> [1; j 1] </ref> s.t. L j;k 6= 0 for each i 2 [j; N ] s.t. L i;k 6= 0 L i;j L i;j L i;k fl L j;k endfor endfor L j;j L j;j for each i 2 [j + 1; N ] s.t. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; <ref> [1; 3] </ref>; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; <ref> [1; 2] </ref>g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval. <p> The entry "nonzeros in factor" refers to the number of nonzeros in the lower triangular factor including the elements on the diagonal. All these matrices were reordered using Liu's modified multiple minimum degree ordering scheme [9]. We used SPARSKIT [13] and the Wisconsin Sparse Matrix Manipulation System <ref> [1] </ref> for generating and converting the test matrices into various formats, and for ordering and symbolically factoring the matrices. 9.2 Performance of numerical factorization For each test matrix, for both the block and column cases, we first ran the appropriate symbolic scheduler immediately followed by the numerical factorization code.
Reference: [2] <author> C. Ashcraft. </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices. </title> <type> Technical Report ECA-TR-148, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, Washington, </address> <year> 1990. </year>
Reference-contexts: On message passing multiprocessors, characterized by relatively high message initialization (i.e., latency) and data transmission costs, the column methods usually suffer from poor performance. To alleviate the communication overheads associated with the column-based methods, block-based partitioning and scheduling schemes have been proposed by [12] and <ref> [2] </ref> for parallel Cholesky factorization. Here the manipulation units for partitioning and scheduling are contiguous blocks of nonzero elements which span across multiple columns, but may not contain all the nonzeros from any single column. In these schemes, the communication overhead is reduced by taking advantage of locality. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; <ref> [2; 5] </ref>; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; <ref> [2; 3] </ref>; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; <ref> [1; 2] </ref>g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; <p> These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8].
Reference: [3] <author> C. Ashcraft, S. C. Eisenstat, and J. W. H. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: In the case of direct solution methods, such as Cholesky factorization, some of the difficulties have been overcome in the past by partitioning and scheduling the sparse matrices using columns as the manipulation units [5], <ref> [3] </ref>, [8]. In these schemes, partitions assigned to processors consist of collections of entire columns and the individual computations scheduled on each processor are in the form of operations on an entire columns. We refer to these as column-based partitioning and scheduling methods. <p> For instance, Lindex <ref> [3] </ref> = 5 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex [4] = 8 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; <ref> [3; 7] </ref>; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; <ref> [1; 3] </ref>; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; <ref> [2; 3] </ref>; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; <p> These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [4] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15-1:1-14, </volume> <year> 1989. </year>
Reference-contexts: For instance, Lindex [3] = 5 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex <ref> [4] </ref> = 8 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; <ref> [4; 5] </ref>; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval. <p> All timings reported in this section are in milliseconds. 9.1 Test Matrices The test matrices were chosen from a variety of applications. A brief description of each test case appears in Table 2. The first five cases are from the Harwell-Boeing sparse matrix collection <ref> [4] </ref>. These matrices are highly unstructured. The remaining four test cases arise in structured 2-D and 3-D grid problems. The entry "nonzeros in factor" refers to the number of nonzeros in the lower triangular factor including the elements on the diagonal.
Reference: [5] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a hypercube. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 129-156, </pages> <year> 1989. </year>
Reference-contexts: In the case of direct solution methods, such as Cholesky factorization, some of the difficulties have been overcome in the past by partitioning and scheduling the sparse matrices using columns as the manipulation units <ref> [5] </ref>, [3], [8]. In these schemes, partitions assigned to processors consist of collections of entire columns and the individual computations scheduled on each processor are in the form of operations on an entire columns. We refer to these as column-based partitioning and scheduling methods. <p> The algebraic details of Cholesky factorization can be found in [7]. For efficient application of this algorithm to sparse matrices, a few additional steps are required. The details of these steps have been extensively described in the literature; see, for example, <ref> [5] </ref> and the references therein. In the following, we bring out the dependencies in Cholesky factorization. Understanding these dependencies is crucial to understanding the functionality of SHAPE. <p> For instance, Lindex [3] = 5 is the index in Lval and Lstruct at which the first non-zero element for column 3 is stored and Lindex [4] = 8 is the index in Lval and Lstruct at which the first non-zero for column 4 is stored. Thus, Lval <ref> [5] </ref> through Lval [7] store the non-zero values in column 3 and Lstruct [5] through Lstruct [7] hold the corresponding row indices - 3, 4, and 6 are the respective row numbers of the non-zeros in column 3. <p> Thus, Lval <ref> [5] </ref> through Lval [7] store the non-zero values in column 3 and Lstruct [5] through Lstruct [7] hold the corresponding row indices - 3, 4, and 6 are the respective row numbers of the non-zeros in column 3. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; <ref> [2; 5] </ref>; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; <ref> [4; 5] </ref>; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> These intervals are [1; 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> are [1; 8]; <ref> [2; 5] </ref>; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 8]; [2; 5]; [3; 7] and <ref> [4; 5] </ref>, stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval. <p> column j which is updateable by k do add cmod (j,k) to task q if (all updates to j are done) add j to done q endif endfor endwhile endwhile Output task q Parallel column symbolic scheduler. a version of the fan-out algorithm for parallel distributed sparse Cholesky factorization in <ref> [5] </ref>, with some differences in the way the computation and communication on a processor are interleaved. A version of this code also appears in [15].
Reference: [6] <author> J. A. George and J. W. H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliff, NJ, </address> <year> 1981. </year>
Reference-contexts: Further details on these steps can be found in any reference book on sparse matrix computations (see, for example, <ref> [6] </ref>). Following these two steps, the actual numerical factorization step is per formed using the algorithm shown in Section 2.1. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; <ref> [6; 8] </ref>; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g.
Reference: [7] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1983. </year>
Reference-contexts: Cholesky factorization is a well-known method for factoring symmetric positive definite matrices. The algebraic details of Cholesky factorization can be found in <ref> [7] </ref>. For efficient application of this algorithm to sparse matrices, a few additional steps are required. The details of these steps have been extensively described in the literature; see, for example, [5] and the references therein. In the following, we bring out the dependencies in Cholesky factorization. <p> Thus, Lval [5] through Lval <ref> [7] </ref> store the non-zero values in column 3 and Lstruct [5] through Lstruct [7] hold the corresponding row indices - 3, 4, and 6 are the respective row numbers of the non-zeros in column 3. <p> Thus, Lval [5] through Lval <ref> [7] </ref> store the non-zero values in column 3 and Lstruct [5] through Lstruct [7] hold the corresponding row indices - 3, 4, and 6 are the respective row numbers of the non-zeros in column 3. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; <ref> [3; 7] </ref>; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; <p> These intervals are [1; 8]; [2; 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 5]; <ref> [3; 7] </ref> and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [8] <author> M. T. Heath, E. Ng, and B. W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: In the case of direct solution methods, such as Cholesky factorization, some of the difficulties have been overcome in the past by partitioning and scheduling the sparse matrices using columns as the manipulation units [5], [3], <ref> [8] </ref>. In these schemes, partitions assigned to processors consist of collections of entire columns and the individual computations scheduled on each processor are in the form of operations on an entire columns. We refer to these as column-based partitioning and scheduling methods. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5]; [6; 8]; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The node list of a node v is stored as as two sequences: the ordered list of left endpoints and the ordered list of right endpoints. S = f [1; 8]; [2; 5]; [3; 7]; [4; 5]; <ref> [6; 8] </ref>; [1; 3]; [2; 3]; [1; 2]g with respect to the universe U = f1; 2; 3; 4; 5; 6; 7g. <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; <p> The intervals stored at this node are all and only those intervals of S which contain the split value. These intervals are <ref> [1; 8] </ref>; [2; 5]; [3; 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order [1; 8]; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. <p> 7] and [4; 5], stored in two sequences: one sequence stores them in order of left endpoints, i.e., in the order <ref> [1; 8] </ref>; [2; 5]; [3; 7]; [4; 5] and the other sequence stores them in order of right endpoints, i.e., in the order [2; 5]; [4; 5]; [3; 7]; [1; 8]. These sequences (represented by the left and right endpoints, respectively) are shown to the left and right of the root node in the figure. 24 Let jU j = N and let I = [x o ; y o ] be a query interval.
Reference: [9] <author> J. W. H. Liu. </author> <title> Modification of the minimum-degree algorithm by multiple elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 </volume> <pages> 141-153, </pages> <year> 1985. </year> <month> 47 </month>
Reference-contexts: The remaining four test cases arise in structured 2-D and 3-D grid problems. The entry "nonzeros in factor" refers to the number of nonzeros in the lower triangular factor including the elements on the diagonal. All these matrices were reordered using Liu's modified multiple minimum degree ordering scheme <ref> [9] </ref>.
Reference: [10] <author> J. W. H. Liu. </author> <title> Computational models and task scheduling for parallel sparse cholesky factor-ization. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: The column-based Cholesky algorithm used here is similar to the one presented in <ref> [10] </ref>. To present a fair comparison, we have modified that algorithm so that the tasks on each processor are scheduled ahead of the numerical factorization using a symbolic scheduler, just as is done in the case of block sparse Cholesky factorization. The two algorithms are presented in the following.
Reference: [11] <author> K. Mehlhorn. </author> <title> Data Structures and Algorithms, Vol. 3 Multidimensional Searching and Computational Geometry. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: A can be computed in time O (log N + jAj). For a detailed description of the interval tree and its uses, refer to <ref> [11] </ref>. Grouping and storage of intervals Clusters are partitioned and the dependencies involving blocks in the cluster are computed by proceeding left to right along the matrix, processing clusters one by one. <p> Steps 2 and 3 build and store the intervals in the interval tree. For details on how to store the intervals in an interval tree, refer <ref> [11] </ref>.
Reference: [12] <author> V. Naik and M. Patrick. </author> <title> Data traffic reduction schemes for cholesky factorization on asynchronous multiprocessor systems. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1989. </year> <note> Also available as IBM Research Report RC 14500. </note>
Reference-contexts: On message passing multiprocessors, characterized by relatively high message initialization (i.e., latency) and data transmission costs, the column methods usually suffer from poor performance. To alleviate the communication overheads associated with the column-based methods, block-based partitioning and scheduling schemes have been proposed by <ref> [12] </ref> and [2] for parallel Cholesky factorization. Here the manipulation units for partitioning and scheduling are contiguous blocks of nonzero elements which span across multiple columns, but may not contain all the nonzeros from any single column. In these schemes, the communication overhead is reduced by taking advantage of locality.
Reference: [13] <author> Y. Saad. Sparskit: </author> <title> a basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> RIACS, NASA Ames Research Center, </institution> <year> 1990. </year>
Reference-contexts: The entry "nonzeros in factor" refers to the number of nonzeros in the lower triangular factor including the elements on the diagonal. All these matrices were reordered using Liu's modified multiple minimum degree ordering scheme [9]. We used SPARSKIT <ref> [13] </ref> and the Wisconsin Sparse Matrix Manipulation System [1] for generating and converting the test matrices into various formats, and for ordering and symbolically factoring the matrices. 9.2 Performance of numerical factorization For each test matrix, for both the block and column cases, we first ran the appropriate symbolic scheduler immediately
Reference: [14] <author> S. Venugopal and V. K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference-contexts: For large matrices encountered in real-life applications, any type of systematic block partitioning by hand is simply impractical. In an effort to simplify the block partitioning task, in a previous paper <ref> [14] </ref>, we proposed and described an automatic, 1 general purpose block-based partitioning and scheduling scheme. There, the sparse matrix was partitioned into several blocks of various sizes by taking maximum advantage of locality.
Reference: [15] <author> S. Venugopal, V. K. Naik, and J. Saltz. </author> <title> Performance of distributed sparse cholesky factorization with pre-scheduling. </title> <note> Submitted to Supercomputing '92. 48 </note>
Reference-contexts: The profile consists of the order and the sizes of blocks to be fetched and received, the processors to which each block is to be sent and other information which is useful in reducing the run-time communication overheads during the numerical factorization phase. In <ref> [15] </ref> we describe how the communication profile can be used to improve the performance of sparse column Cholesky factorization. <p> A version of this code also appears in <ref> [15] </ref>. There are two main data structures used in the scheduling process : (a) done q : a queue of columns to which all updates have been done and (b) update q : a queue of columns which are to be used to update local columns.
References-found: 15

