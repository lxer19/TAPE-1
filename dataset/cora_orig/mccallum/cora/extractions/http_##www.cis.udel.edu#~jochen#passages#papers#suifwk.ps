URL: http://www.cis.udel.edu/~jochen/passages/papers/suifwk.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: ffenwick,pollockg@cis.udel.edu  
Phone: (302) 831-1953  
Title: Implementing an Optimizing Linda Compiler using SUIF  
Author: James B. Fenwick Jr. Lori L. Pollock 
Address: 19716  
Affiliation: High Performance Computing Software Laboratory Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1986. </year>
Reference-contexts: This multi-level representation supports varying types of analyses, some of which work on the tree form and others on the list format. The SUIF system consists of a variety of standard optimization passes, some of which utilize the control flow graph (CFG) <ref> [1] </ref>. Thus, SUIF provides a CFG class to construct and manipulate a control flow graph. Our work utilizes these intermediate forms in building the representations needed for our analyses. 3.1 Computing Data Dependences We compute scalar data dependences by performing reaching definition data flow analysis on the CFG. <p> We have implemented this data flow framework in SUIF. The actual data flow algorithm is the standard, well known iterative technique <ref> [1] </ref>. In our reaching definitions data flow analysis, we stored the data flow information in the nodes themselves. For this data flow analysis, we wanted to experiment with the annotations feature of SUIF by storing the data flow information as SUIF immediate lists in an annotation on the instructions.
Reference: [2] <author> Mauricio Arango and Donald Berndt. Tsnet: </author> <title> A linda implementation for networks of unix-based computers. </title> <type> Technical Report YALEU/DCS/TR-739, </type> <institution> Yale University, Department of Computer Science, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp <ref> [8, 18, 3, 2, 4] </ref>. Moreover, Deshpande and Schultz [9] demonstrated that Linda programs can be efficient on distributed-memory machines.
Reference: [3] <author> Robert D. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp <ref> [8, 18, 3, 2, 4] </ref>. Moreover, Deshpande and Schultz [9] demonstrated that Linda programs can be efficient on distributed-memory machines. <p> However, it is also these properties of associative access and uncoupled communication that cause inefficiencies. Compile-time analysis has been developed to structure tuplespace which significantly reduces the associative search cost [8], and run-time strategies have been developed to counteract inefficiencies due to the uncoupling property <ref> [3] </ref>. Carriero and Gelernter discuss potential compile-time analysis to reduce the impact of uncoupled communication [5]. Our work on optimizing Linda programs is based on the observations presented by them.
Reference: [4] <author> Nicholas Carriero, Eric Freeman, and David Gelernter. </author> <title> Adaptive parallelism on multiprocessors: Preliminary experience with piranha on the cm-5. </title> <type> Technical Report YALEU/DCS/TR-969, </type> <institution> Yale University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp <ref> [8, 18, 3, 2, 4] </ref>. Moreover, Deshpande and Schultz [9] demonstrated that Linda programs can be efficient on distributed-memory machines.
Reference: [5] <author> Nicholas Carriero and David Gelernter. </author> <title> A foundation for advanced compile-time analysis of linda programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 389-404. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Compile-time analysis has been developed to structure tuplespace which significantly reduces the associative search cost [8], and run-time strategies have been developed to counteract inefficiencies due to the uncoupling property [3]. Carriero and Gelernter discuss potential compile-time analysis to reduce the impact of uncoupled communication <ref> [5] </ref>. Our work on optimizing Linda programs is based on the observations presented by them. This paper describes our implementation of the program representations which lay the foundation for our Linda optimizer, and the implementation of our initial analysis passes used to gather information for communication optimization.
Reference: [6] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs, A First Course. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year> <month> 15 </month>
Reference-contexts: We conclude the paper by describing the lessons we've learned thus far using SUIF and indicate some future directions. 2 Linda Linda is a coordination language consisting of a small number of primitives which are added into existing sequential languages <ref> [12, 6] </ref>. These operations perform the communication and synchronization necessary for parallel programming. Communication between processes is achieved through tuples in an associative, global memory known as tuplespace. The tuples in tuplespace are manipulated by the Linda operations: out, eval, in, rd, inp, rdp.
Reference: [7] <author> Nicholas Carriero and David Gelernter. </author> <title> Learning from our successes. </title> <editor> In Janusz S. Kowalik and Lucio Grandinetti, editors, </editor> <booktitle> Software for Parallel Computation, volume 106 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 37-45. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In support, there are a number of Linda production codes to perform tasks as diverse as image raytracing to medical monitoring to pricing financial instruments <ref> [7] </ref>. Our goal is to use compile-time analysis and optimization to increase the efficiency of Linda programs on distributed-memory systems, in order to make Linda an even more attractive choice given the cost effective availability of parallelism in network clusters.
Reference: [8] <author> Nicholas John Carriero, Jr. </author> <title> Implementation of Tuple Space Machines. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp <ref> [8, 18, 3, 2, 4] </ref>. Moreover, Deshpande and Schultz [9] demonstrated that Linda programs can be efficient on distributed-memory machines. <p> However, it is also these properties of associative access and uncoupled communication that cause inefficiencies. Compile-time analysis has been developed to structure tuplespace which significantly reduces the associative search cost <ref> [8] </ref>, and run-time strategies have been developed to counteract inefficiencies due to the uncoupling property [3]. Carriero and Gelernter discuss potential compile-time analysis to reduce the impact of uncoupled communication [5]. Our work on optimizing Linda programs is based on the observations presented by them. <p> part of our project [21]; even in its infancy, this tool has proved useful. 4 Optimization Analysis 4.1 Partitioning of Tuplespace Early efforts in using compile-time analysis to improve Linda programs was focused on partitioning tuplespace and implementing each partition with a data structure that makes searching the partition fast <ref> [8] </ref>. An initial classification is performed on the number and types of fields of the tuples and templates. These initial partitions are then further subdivided using pre-match rules that are similar to the rules for matching tuples and templates in tuplespace at runtime.
Reference: [9] <author> Ashish Deshpande and Martin Schultz. </author> <title> Efficient parallel programming with linda. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 238-244, </pages> <address> Minneapolis, Minnesota, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp [8, 18, 3, 2, 4]. Moreover, Deshpande and Schultz <ref> [9] </ref> demonstrated that Linda programs can be efficient on distributed-memory machines. We believe that Linda's simplicity, explicit parallelism, and integration with existing languages makes it an attractive choice for scientists of non-computer science disciplines who want to increase performance of codes through parallelism.
Reference: [10] <author> James B. Fenwick Jr. and Lori L. Pollock. </author> <title> Identifying tuple usage patterns in an optimizing linda compiler. </title> <type> Technical report, </type> <institution> University of Delaware, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Our results have been encouraging <ref> [10] </ref>. When faced with the problem of needing a compiler infrastructure on which to validate and evaluate our optimization analysis and transformation, we decided upon SUIF [13] which is a compiler project undertaken by Stanford University researchers and utilized by many academic and industry groups. <p> To attempt to answer this question in a conservative way at compile time, we have developed a distributive data flow analysis framework that answers the question, "For each tuple partition, may there ever be more than one tuple in that partition?" The data flow framework is described in detail in <ref> [10] </ref>. We have implemented this data flow framework in SUIF. The actual data flow algorithm is the standard, well known iterative technique [1]. In our reaching definitions data flow analysis, we stored the data flow information in the nodes themselves.
Reference: [11] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The system dependence graph developed by Horwitz, Reps, and Binkley [15] provides us with a good basis for extension to explicitly parallel Linda programs. In particular, it unifies both control and data dependences into a single representation due to its basis on the program dependence graph (PDG) <ref> [11] </ref>, and also models interprocedural effects. Thus, our initial implementation has included construction of a system dependence graph (SDG) representation of the input program as a new pass in SUIF. Our SDG construction consists of the following steps: 1. Computation of data dependences. 2. <p> This gives us precise array reference data dependence information for "regular" FOR loops. Because SUIF is distributed with this dependence analyzer, we were able to obtain this complex information with minimal programming effort. 3.2 Program Dependence Graph Construction The program dependence graph (PDG) <ref> [11] </ref> is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling [14, 20], register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a <p> Because SUIF is distributed with this dependence analyzer, we were able to obtain this complex information with minimal programming effort. 3.2 Program Dependence Graph Construction The program dependence graph (PDG) <ref> [11] </ref> is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling [14, 20], register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in <p> 3.2 Program Dependence Graph Construction The program dependence graph (PDG) <ref> [11] </ref> is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling [14, 20], register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in the program. The nodes of the graph are statements and predicate expressions that occur in the program. <p> The program dependence graph (PDG) <ref> [11] </ref> is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling [14, 20], register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in the program. The nodes of the graph are statements and predicate expressions that occur in the program. An edge represents either a control dependence or a data dependence among program components. <p> Figure 2 shows the relevant aspects of this class. Future methods can then be added to access the object and perform various analyses. Figure 3 outlines our method for constructing a PDG represention for a given function. Our construction follows the method described in <ref> [11] </ref> and is patterned after SUIF's construction of the 6 CFG. The first step is to construct the CFG from the SUIF instruction list intermediate form. As stated previously, this is trivial because SUIF already contains a CFG class. The nodes of the CFG are wrappers for the individual instructions. <p> Thus, when building the CDG, we wrap the CFG nodes inside of PDG nodes and link together the PDG nodes to form the CDG. This initial PDG contains three types of nodes: an entry node, predicate nodes, and statement nodes. We then use the two step method of <ref> [11] </ref> to insert region nodes. Note that the region nodes do not serve as wrappers for CFG nodes. To represent control dependence information and ease the retrieval and update of this information, the CDset class was developed. <p> To represent control dependence information and ease the retrieval and update of this information, the CDset class was developed. This class is a component of each PDG node and is manipulated during region insertion. This method results in a standard program dependence graph as described by <ref> [11] </ref>. We augment this standard PDG with two additional types of nodes. Parameter nodes are created to serve as definitions for each of the procedure's formal parameters.
Reference: [12] <author> David Gelernter. </author> <title> Generative communication in linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Our goal is to use compile-time analysis and optimization to increase the efficiency of Linda programs on distributed-memory systems, in order to make Linda an even more attractive choice given the cost effective availability of parallelism in network clusters. Linda is based upon the unique generative communication model <ref> [12] </ref> called tuplespace. Two distinguishing characteristics of tuplespace give rise to its power and flexibility. Communication uncoupling refers to the tuple producer's lack of concern for the consumer (s) of the tuple. <p> We conclude the paper by describing the lessons we've learned thus far using SUIF and indicate some future directions. 2 Linda Linda is a coordination language consisting of a small number of primitives which are added into existing sequential languages <ref> [12, 6] </ref>. These operations perform the communication and synchronization necessary for parallel programming. Communication between processes is achieved through tuples in an associative, global memory known as tuplespace. The tuples in tuplespace are manipulated by the Linda operations: out, eval, in, rd, inp, rdp.
Reference: [13] <author> Stanford SUIF Compiler Group. </author> <title> The SUIF Parallelizing Compiler Guide. </title> <institution> Stanford University, </institution> <year> 1994. </year> <note> Version 1.0. </note>
Reference-contexts: Our results have been encouraging [10]. When faced with the problem of needing a compiler infrastructure on which to validate and evaluate our optimization analysis and transformation, we decided upon SUIF <ref> [13] </ref> which is a compiler project undertaken by Stanford University researchers and utilized by many academic and industry groups. The SUIF system is based upon a well-documented, multi-level intermediate program representation and consists of a core library, support libraries, and a variety of passes.
Reference: [14] <author> Rajiv Gupta and Mary Lou Soffa. </author> <title> Region scheduling: An approach for detecting and redistributing parallelism. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 421-431, </pages> <year> 1990. </year>
Reference-contexts: Because SUIF is distributed with this dependence analyzer, we were able to obtain this complex information with minimal programming effort. 3.2 Program Dependence Graph Construction The program dependence graph (PDG) [11] is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling <ref> [14, 20] </ref>, register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in the program.
Reference: [15] <author> Susan Horwitz, Thomas Reps, and David Binkley. </author> <title> Interprocedural slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: However, the interprocedural analysis has a different twist than interprocedural analysis of sequential programs, due to the multiplicity of processes and the parallelism created at the task spawning site, namely the eval site in Linda. The system dependence graph developed by Horwitz, Reps, and Binkley <ref> [15] </ref> provides us with a good basis for extension to explicitly parallel Linda programs. In particular, it unifies both control and data dependences into a single representation due to its basis on the program dependence graph (PDG) [11], and also models interprocedural effects. <p> Similarly we add a data dependence from the return node of the called procedure to the result operand, if any, of the call site instruction. We have currently not implemented the transitive edges detailed in <ref> [15] </ref>. A simple program and its corresponding SDG is shown in Figure 4. An important side effect of our SDG construction is the construction of the interprocedural control flow graph (ICFG) [17].
Reference: [16] <editor> N.D. Jones and S.S. Muchnick. </editor> <title> A flexible approach to interprocedural data flow analysis and programs with recursive data structures. </title> <booktitle> In Conference Recordof the Ninth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 66-74, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: The data flow is interprocedural and is performed on the ICFG. Because of the unrealizable paths 4 that exist within the ICFG, an encoding of the runtime stack <ref> [16] </ref> is necessary. Our current encoding has the effect that an update of data flow information at a point within a procedure is simultaneously updating information for this point in all of the procedure's possible invocations.
Reference: [17] <author> William Landi and Barbara G. Ryder. </author> <title> A safe approximation algorithm for interprocedural pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 235-248. </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: We have currently not implemented the transitive edges detailed in [15]. A simple program and its corresponding SDG is shown in Figure 4. An important side effect of our SDG construction is the construction of the interprocedural control flow graph (ICFG) <ref> [17] </ref>. <p> Annotations can lead to a simple, modular style of code development. Rather than cramming several passes over the intermediate representation into one phase in order to perform some transformation, one can use several single-pass phases and use annotations to store information needed in subsequent phases. 4 An unrealizable path <ref> [17] </ref> is a path from a call site, through the called procedure, but returning to a different call site. 14 This is possible because the annotations are stored as part of the intermediate representation.
Reference: [18] <author> Jerrold Sol Leichter. </author> <title> Shared Tuple Memories, Shared Memories, Buses and LAN's - Linda Implementations Across the Spectrum of Connectivity. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Linda has been implemented on several architecture platforms such as massively parallel processors, distributed-memory hypercubes, networks of workstations and shared-memory machines, and has been combined with a number of base languages including C, Fortran, and Lisp <ref> [8, 18, 3, 2, 4] </ref>. Moreover, Deshpande and Schultz [9] demonstrated that Linda programs can be efficient on distributed-memory machines. <p> At that time, the tuple becomes a passive tuple, like those created using the out operation. New processes 2 The troublesome, unpredictable nature of these predicates in distributed-memory implementations was noted by <ref> [18] </ref>. 3 are created to evaluate the fields of an eval operation 3 . This is how the programmer explicitly creates parallelism.
Reference: [19] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation over the program dependence graph. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: It has been used to detect parallelism [11], perform instruction scheduling [14, 20], register allocation <ref> [19] </ref> as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in the program.
Reference: [20] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation sensitive region scheduling. </title> <booktitle> In PACT `95: International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Limassol, Cyprus, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Because SUIF is distributed with this dependence analyzer, we were able to obtain this complex information with minimal programming effort. 3.2 Program Dependence Graph Construction The program dependence graph (PDG) [11] is a versatile program representation. It has been used to detect parallelism [11], perform instruction scheduling <ref> [14, 20] </ref>, register allocation [19] as well as numerous standard optimizations including code motion, loop peeling and unrolling, loop fusion, and branch deletion [11]. The program dependence graph [11] for a program is a directed graph that represents the relevant control and data dependences between statements in the program.
Reference: [21] <author> Amie L. Souter. </author> <title> Flexible algorithms for displaying large graphical program representations. </title> <institution> Bucknell University, Senior Honor's Thesis, </institution> <year> 1995. </year>
Reference-contexts: In addition, we would 10 11 like some way of selectively eliding parts of the graph from the display, in some hierarchical fashion based on the program structure. Such a tool is currently being developed as part of our project <ref> [21] </ref>; even in its infancy, this tool has proved useful. 4 Optimization Analysis 4.1 Partitioning of Tuplespace Early efforts in using compile-time analysis to improve Linda programs was focused on partitioning tuplespace and implementing each partition with a data structure that makes searching the partition fast [8].
Reference: [22] <author> Michael Wolfe. </author> <note> http://www.cse.ogi.edu/ mwolfe/. 16 </note>
Reference-contexts: Then the cycle would begin anew. It did not take too many iterations of this process to realize that an automatic graph viewing tool was needed. Michael Wolfe has developed a fast, but limited, version of such a tool <ref> [22] </ref>. We would like more information than what is currently provided by this tool. For example, we would like the various PDG nodes to have different shapes or colors so as to quickly discern them from other nodes.
References-found: 22

