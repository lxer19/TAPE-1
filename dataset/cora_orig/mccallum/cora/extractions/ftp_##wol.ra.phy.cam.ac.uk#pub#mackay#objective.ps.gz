URL: ftp://wol.ra.phy.cam.ac.uk/pub/mackay/objective.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Searching for `optimal' inputs with an empirical regression model  
Author: David J.C. MacKay 
Keyword: Process software package when optimizing the input variables.  
Date: November 25, 1997 Version 1.2  
Address: Madingley Road, Cambridge CB3 0HE  
Affiliation: University of Cambridge Cavendish Laboratory,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Gibbs, M. N., and MacKay, D. J. C., </author> <title> (1996) Efficient implementation of Gaussian processes for interpolation. </title> <note> in preparation. </note>
Reference: <author> Kimeldorf, G. S., and Wahba, G. </author> <title> (1970) A correspondence between Bayesian estimation of stochastic processes and smoothing by splines. </title> <journal> Annals of Mathematical Statistics 41 (2): </journal> <pages> 495-502. </pages>
Reference-contexts: Examples of non-parametric models are cubic splines interpolation, and Gaussian processes. Splines correspond to a model in which the prior probability distribution over the function y (x) <ref> (Kimeldorf and Wahba 1970) </ref> is: 1 log P (y (x)jff; H 1 ) = 2 Z dx [y (p) (x)] 2 + const; (2) where y (p) denotes the pth derivative of y, and p = 2 for cubic splines. 1 Strictly this prior is improper since addition of an arbitrary
Reference: <author> MacKay, D. J. C., </author> <title> (1991) Bayesian Methods for Adaptive Models. </title> <institution> California Institute of Technology dissertation. </institution>
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) The evidence framework applied to classification networks. </title> <booktitle> Neural Computation 4 (5): </booktitle> <pages> 698-714. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1992c) Information based objective functions for active data selection. </title> <booktitle> Neural Computation 4 (4): </booktitle> <pages> 589-603. </pages>
Reference-contexts: What should the utility function be? 4.1 Utility functions for experimental design I have studied the problem of active learning in neural networks a few years ago and derived a few experimental design objective functions <ref> (MacKay 1992c) </ref>. I learnt two things from this work: 1. The choice of objective function must be made carefully to avoid getting silly results.
Reference: <author> MacKay, D. J. C. </author> <title> (1992d) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks. Network: </title> <booktitle> Computation in Neural Systems 6: </booktitle> <pages> 469-505. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1996) Bayesian non-linear modelling for the 1993 energy prediction competition. In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. </editor> <booktitle> Heidbreder, </booktitle> <pages> pp. 221-234, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: These unknown lengthscales can then be inferred from the data. This `automatic relevance determination' method is available when we use either neural networks or Gaussian processes. 3 Implementation method There are deterministic and Monte Carlo implementation methods <ref> (Neal 1993) </ref> for Bayesian models. Both have advantages. 4 Decision problems In order to make decisions, we need to have not only a probabilistic model but also a utility function.
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <booktitle> Number 118 in Lecture Notes in Statistics. </booktitle> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Williams, C. K. I., </author> <title> (1995) Regression with Gaussian processes. </title> <note> To appear in Annals of Mathematics and Artificial Intelligence. </note>
Reference: <author> Williams, C. K. I., and Rasmussen, C. E. </author> <title> (1996) Gaussian processes for regression. </title> <booktitle> In Advances in Neural Information Processing Systems 8 , ed. </booktitle> <editor> by D. S. Touretzky, M. </editor> <address> C. </address>
Reference: <author> Mozer, and M. E. </author> <title> Hasselmo. </title> <publisher> MIT Press. </publisher> <pages> 7 </pages>
References-found: 14

