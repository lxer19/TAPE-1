URL: http://mnemosyne.itc.it:1024/blanzieri/aiia95bgk.ps.Z
Refering-URL: http://mnemosyne.itc.it:1024/blanzieri/conference-papers-list.html
Root-URL: 
Email: e-mail: blanzier@psych.unito.it  e-mail: attilio@di.unito.it  
Phone: 2  
Title: Mapping Symbolic Knowledge into Locally Receptive Field Networks  
Author: Enrico Blanzieri and Attilio Giordana 
Address: Via Lagrange 3, 10100 Torino, Italy  C.so Svizzera 185, 10149 Torino, Italy  
Affiliation: 1 Centro di Scienza Cognitiva, Universita di Torino,  Dipartimento di Informatica, Universita di Torino,  
Abstract: This paper investigates Locally Receptive Field Networks, a broad class of neural networks including Probabilistic Neural Networks and Radial Basis Function Networks, which naturally exhibit symbolic properties. Moreover, specific attention is given to the sub-class of Fac-torizable Radial Basis Function Networks whose architecture can be directly translated into a propositional theory and viceversa. Exploiting this characteristics, symbolic and numeric algorithms can be easely integrated for automating network synthesis. Several methods including classification and regression trees, and statistical clustering are evaluated on a classification task in a difficult medical domain. The obtained results show that the considered network class is able to achieve a high accuracy, while conserving a symbolic readability.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H.R. Berenji. </author> <title> Fuzzy logic controllers. In R.R. </title> <editor> Yager and L.A. Zadeh, editors, </editor> <title> An Introduction to Fuzzy Logic Applications in Intelligent Systems. </title> <publisher> Kluver Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: A step towards the integration between this architecture and symbolic algorithms is made in the work of Sethi [11]. In this paper, we will show that the Locally Receptive Field Networks (LRFN), a broad family including Probabilistic Neural Networks [12], Radial Basis Function Networks [10, 9], Fuzzy Controllers <ref> [1] </ref> and some others, naturally exhibits the same symbolic properties obtained by Shawlik without requiring sophisticated translation algorithms. Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. [16]. <p> define a more precise network architecture which has straightforward interpretation in term of propositional logics. 2.1 An F-RBFN Architecture The specific network architecture we will investigate in this paper is a hybrid between the Factorized Radial Basis Function Networks (F-RBFNs), mentioned in [10] and the fuzzy/neural networks introduced by Berenji <ref> [1] </ref> for implementing fuzzy controllers capable of learning from a reinforcement signal. It is similar to the architecture proposed by Tresp et al. [16]. Figure 1 describes the basic network topology. <p> The consequence is under-generalization in the classification tasks. This problem can be avoided by introducing a normalization term in the output activation function: ^ Y = j w j r j j r j This function is frequently used for fuzzy controller architectures <ref> [1] </ref>. In this case, one obtains a network biased toward over-generalization in a similar way as it happen for the multi-layer perceptron. Depending on the application, under-generalization or over-generalization can be preferable.
Reference: 2. <author> M. Botta and A. Giordana. </author> <title> SMART+: A multi-strategy learning tool. </title> <booktitle> In IJCAI-93, Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, volume 2, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The first algorithm is the well known CART [3] which can work both for classification and for regression. In both cases the trees produced by CART can be simply translated into rules and then used to construct a network. The second algorithm is SMART+ <ref> [2] </ref> which has the peculiarity of combining inductive and deductive techniques. In this way it is possible to force the system to exploit a domain theory even when it is imperfect.
Reference: 3. <author> L. Breiman, J.H. Friedman, R.A. Ohlsen, and C.J. Stone. </author> <title> Classification And Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: In the present case we focused on two specific algorithms, not because they are considered the best for this specific purpose, but because they where immediately available to the authors. The first algorithm is the well known CART <ref> [3] </ref> which can work both for classification and for regression. In both cases the trees produced by CART can be simply translated into rules and then used to construct a network. The second algorithm is SMART+ [2] which has the peculiarity of combining inductive and deductive techniques.
Reference: 4. <author> L.N. Cooper D.L. Reilly and C. Elbaum. </author> <title> A neural model for category learning. </title> <journal> Biological Cybernatics, </journal> <volume> 45 </volume> <pages> 35-41, </pages> <year> 1982. </year>
Reference-contexts: For instance, multidimensional Gaussian functions are used in Probabilistic Neural Networks (PNN) [12] and in Radial Basis Function Networks (RBFNs) [10], pyramidal or trapezoidal functions are used for fuzzy controllers, and cylindric functions in the Restricted Coulomb Energy model <ref> [4] </ref>. As a consequence, MLP and LRFNs encode the target function in a totally different way.
Reference: 5. <author> A. Giordana, M. Kaiser, and M. Nuttin. </author> <title> Robot controller synthesis: How to reduce costs. </title> <booktitle> In Proceedings of the Workshop on Learning Robots, </booktitle> <address> Torino, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. [16]. The symbolic proprieties of Fuzzy Controllers are straightforward and prior knowledge is widely used for their syn-thesis. Giordana et al. <ref> [5] </ref> already shown how to exploit their symbolic readability for integrating symbolic learning methods and rule-based knowledge insertion.
Reference: 6. <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feed-forward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: Boolean input and boolean outputs can be represented as threshold functions on continuous spaces. Therefore, we will restrict our attention to networks with an arbitrary set of inputs, but a single output, as it is usually done for RBFNs. As it has been proven in <ref> [6] </ref>, a universal function approximator can be constructed using at least three layers of nodes (input, hidden and output), provided the activation function in the hidden layer be non-linear. In the backpropagation network family, derived from the Multi-Layer Perceptron (MLP), the most frequently used activation function is the sigmoid.
Reference: 7. <author> F. Saulnier J.R. Le Gall, S. Lemeshow. </author> <title> A new simplified acute physiology score (saps ii) based on a european/north american multicenter study. </title> <journal> JAMA, </journal> <volume> 270(24) </volume> <pages> 2957-2963, </pages> <year> 1993. </year>
Reference-contexts: The state-of-the-art method currently used for predicting the outcome, from this physiological variables, is based on the Simplified Acute Physiology Score (SAPS-II) <ref> [7] </ref>, a statistical method for evaluating the severity of the patient status. SAPS-II has developed and validated using logistic regression analysis on more then 8,000 patients and it can be converted to a probability of death.
Reference: 8. <author> J.R. Millan. </author> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <booktitle> In Proceedings of the third International Conference on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: More in general, this locality property of the LRFN allows the network layout to be incrementally constructed (see for instance <ref> [8] </ref>) adjusting the existing neurons and/or adding new ones. As every change has a local effect, the knowledge encoded in the other parts of the network is not lost; so, it will not be necessary to go through a global revision process.
Reference: 9. <author> J. Park and W. Sandberg. </author> <title> Universal approximation using radial-basis functions. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 246-257, </pages> <year> 1991. </year>
Reference-contexts: A step towards the integration between this architecture and symbolic algorithms is made in the work of Sethi [11]. In this paper, we will show that the Locally Receptive Field Networks (LRFN), a broad family including Probabilistic Neural Networks [12], Radial Basis Function Networks <ref> [10, 9] </ref>, Fuzzy Controllers [1] and some others, naturally exhibits the same symbolic properties obtained by Shawlik without requiring sophisticated translation algorithms. Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. [16].
Reference: 10. <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A step towards the integration between this architecture and symbolic algorithms is made in the work of Sethi [11]. In this paper, we will show that the Locally Receptive Field Networks (LRFN), a broad family including Probabilistic Neural Networks [12], Radial Basis Function Networks <ref> [10, 9] </ref>, Fuzzy Controllers [1] and some others, naturally exhibits the same symbolic properties obtained by Shawlik without requiring sophisticated translation algorithms. Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. [16]. <p> On the contrary, LRFN family, while similar in the topological structure, makes use of activation functions having axial symmetry. For instance, multidimensional Gaussian functions are used in Probabilistic Neural Networks (PNN) [12] and in Radial Basis Function Networks (RBFNs) <ref> [10] </ref>, pyramidal or trapezoidal functions are used for fuzzy controllers, and cylindric functions in the Restricted Coulomb Energy model [4]. As a consequence, MLP and LRFNs encode the target function in a totally different way. <p> The greatest contribution of a neuron to the output value Y comes essentially from this region. From a mathematical point of view, Poggio and Girosi <ref> [10] </ref> have shown that LRFNs (more specifically RBFNs) tend to reproduce a serial development of the target function in terms of Green's functions. However, LRFNs exhibit properties substantially different with respect to both the learning algorithms and the semantic interpretation. <p> In the following we will define a more precise network architecture which has straightforward interpretation in term of propositional logics. 2.1 An F-RBFN Architecture The specific network architecture we will investigate in this paper is a hybrid between the Factorized Radial Basis Function Networks (F-RBFNs), mentioned in <ref> [10] </ref> and the fuzzy/neural networks introduced by Berenji [1] for implementing fuzzy controllers capable of learning from a reinforcement signal. It is similar to the architecture proposed by Tresp et al. [16]. Figure 1 describes the basic network topology. <p> In the following we will briefly overview several of this methods, which will be experimentally evaluated in Section 4. 3.1 Statistical Clustering The usual methods for learning from examples Radial Basis Function networks <ref> [10] </ref> are based on a two step learning procedure. First a statistical clustering algorithm, such as k-Means is used to determine the centers and the amplitude of the activation functions. Then, the weights on the links to the output neuron are determined by computing the coefficients of the pseudo-inverse matrix [10]. <p> <ref> [10] </ref> are based on a two step learning procedure. First a statistical clustering algorithm, such as k-Means is used to determine the centers and the amplitude of the activation functions. Then, the weights on the links to the output neuron are determined by computing the coefficients of the pseudo-inverse matrix [10]. The method is usually faster than gradient descent. On the other hand, this last is sometime preferable, because it is more simple to implement and because it is suitable for on-line learning.
Reference: 11. <author> I.K. Sethi. </author> <title> Entropy nets: From decison trees to neural networks. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1605-1613, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: However, the seminal works of Towel and Shavlik, mostly relates to backpropagation networks based on Multilayer Perceptron. A step towards the integration between this architecture and symbolic algorithms is made in the work of Sethi <ref> [11] </ref>. In this paper, we will show that the Locally Receptive Field Networks (LRFN), a broad family including Probabilistic Neural Networks [12], Radial Basis Function Networks [10, 9], Fuzzy Controllers [1] and some others, naturally exhibits the same symbolic properties obtained by Shawlik without requiring sophisticated translation algorithms.
Reference: 12. <author> D.F. Specht. </author> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 109-118, </pages> <year> 1990. </year>
Reference-contexts: A step towards the integration between this architecture and symbolic algorithms is made in the work of Sethi [11]. In this paper, we will show that the Locally Receptive Field Networks (LRFN), a broad family including Probabilistic Neural Networks <ref> [12] </ref>, Radial Basis Function Networks [10, 9], Fuzzy Controllers [1] and some others, naturally exhibits the same symbolic properties obtained by Shawlik without requiring sophisticated translation algorithms. Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. [16]. <p> On the contrary, LRFN family, while similar in the topological structure, makes use of activation functions having axial symmetry. For instance, multidimensional Gaussian functions are used in Probabilistic Neural Networks (PNN) <ref> [12] </ref> and in Radial Basis Function Networks (RBFNs) [10], pyramidal or trapezoidal functions are used for fuzzy controllers, and cylindric functions in the Restricted Coulomb Energy model [4]. As a consequence, MLP and LRFNs encode the target function in a totally different way.
Reference: 13. <author> G. Towell and J.W. Shavlik. </author> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101, </pages> <year> 1993. </year>
Reference-contexts: The claimed benefits deriving from this form of integration of symbolic and subsymbolic knowledge, are first of all, a speed up in neural network training and a general improvement of the performances. Second, the possibility of mapping back a neural network into symbolic knowledge <ref> [13] </ref> offers a good interface toward the end user which frequently refuses to trust a "black box". However, the seminal works of Towel and Shavlik, mostly relates to backpropagation networks based on Multilayer Perceptron. <p> ^ 26:90 &lt; HCO3 ^HCO3 &lt; 29:47 ^ 4:58 &lt; GCS ^ GCS &lt; 8:65 5 Conclusions In this paper, we investigated a class of neural networks, called Locally Receptive Field Networks, which is particularly suitable for integrating the symbolic and connectionist paradigms, in the line of Shavlik and Towell <ref> [15, 13] </ref>. More specifically, we shows how classical symbolic learning algorithms can be used in order to synthesize a network which can still be refined by performing the gradient descent. After refinement, the network tends to conserve its symbolic readability.
Reference: 14. <author> G. Towell and J.W. Shavlik. </author> <title> Knowledge based artificial neural networks. </title> <journal> Artficial Intelligence, </journal> <volume> 70(4) </volume> <pages> 119-166, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of converting a body of symbolic knowledge into a neural network and viceversa has been already investigated by several authors. A major contribution comes by Towell and Shavlik <ref> [15, 14] </ref> which tackled this problem among the firsts. The claimed benefits deriving from this form of integration of symbolic and subsymbolic knowledge, are first of all, a speed up in neural network training and a general improvement of the performances.
Reference: 15. <author> G.G. Towell, J.W. Shavlik, and M.O. Noordwier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the 8 th National Conference on Artificial Intelligence AAAI'90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The problem of converting a body of symbolic knowledge into a neural network and viceversa has been already investigated by several authors. A major contribution comes by Towell and Shavlik <ref> [15, 14] </ref> which tackled this problem among the firsts. The claimed benefits deriving from this form of integration of symbolic and subsymbolic knowledge, are first of all, a speed up in neural network training and a general improvement of the performances. <p> Section 3 describes several learning methods for synthesising a F-RBFN from examples and a domain theory, and Section 4 presents an experimental evaluation. Finally, the approach is compared to the one of Shavlik <ref> [15] </ref> in Section 5. 2 Locally Receptive Field Networks In the following of this paper we will restrict our interest to the problem of learning a function (called the target function in the following) from an n-dimensional continuous domain D R n to a co-domain Y R, being n the number <p> ^ 26:90 &lt; HCO3 ^HCO3 &lt; 29:47 ^ 4:58 &lt; GCS ^ GCS &lt; 8:65 5 Conclusions In this paper, we investigated a class of neural networks, called Locally Receptive Field Networks, which is particularly suitable for integrating the symbolic and connectionist paradigms, in the line of Shavlik and Towell <ref> [15, 13] </ref>. More specifically, we shows how classical symbolic learning algorithms can be used in order to synthesize a network which can still be refined by performing the gradient descent. After refinement, the network tends to conserve its symbolic readability.
Reference: 16. <author> V. Tresp, J. Hollatz, and S. Ahmad. </author> <title> Network structuring and training using rule-based knowledge. </title> <booktitle> In Advances in Neural Information Processing Systems 5 (NIPS-5). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Insertion, refinement and extraction of rule-based knowledge in a Basis Function Network was proposed by Tresp et al. <ref> [16] </ref>. The symbolic proprieties of Fuzzy Controllers are straightforward and prior knowledge is widely used for their syn-thesis. Giordana et al. [5] already shown how to exploit their symbolic readability for integrating symbolic learning methods and rule-based knowledge insertion. <p> It is similar to the architecture proposed by Tresp et al. <ref> [16] </ref>. Figure 1 describes the basic network topology. The activation function used in an F-RBFN with n input units is defined as the product of n unidimensional radial functions, each one associated to one of the input features. <p> In fact it is easy to control the overlap by means of direct monitoring and freeezing of the centers. Alternatively, it is possible to adopt one the strategy proposed by Tresp et al. <ref> [16] </ref> in order to preserve the knowledge inserted in the network. 4 Experimental Evaluation In the following we will test the learning methods described in the previous section on a classification task in a real medical domain using data collected from the Italian National Research Council.
References-found: 16

