URL: http://www.cs.vt.edu/~chitra/docs/nrgpub/gdiss.ps.gz
Refering-URL: http://www.cs.vt.edu/~chitra/docs/nrgpub/
Root-URL: http://www.cs.vt.edu
Title: Analysis and Modeling of World Wide Web Traffic  
Author: Ghaleb Abdulla Edward A. Fox, Chair Marc Abrams Ali Nayfeh Osman Balci Dennis Kafura 
Degree: Dissertation submitted to the Faculty of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science  
Keyword: Proxy, Caching, Log analysis, World Wide Web, Scalability, Modeling, Time Series  
Note: Copyright 1998, Ghaleb Abdulla  
Address: Virginia  
Date: May, 1998  
Affiliation: Virginia Polytechnic Institute and State University  Blacksburg,  
Abstract-found: 0
Intro-found: 1
Reference: [AFA97] <author> G. Abdulla, E. A. Fox, and M. Abrams. </author> <title> Shared User Behavior on the World Wide Web. </title> <booktitle> In Proc. of WebNet97, </booktitle> <address> Toronto, Canada, </address> <month> November </month> <year> 1997. </year> <pages> AACE. </pages>
Reference-contexts: The work described in this dissertation was one of the major sources that helped in answering some of the questions raised by the W3C protocol design group <ref> [AFAW97, AFA97, ANF97, Pit98] </ref>. Braun and Claffy pointed out the difficulty of tracking Web statistics on a large scale | e.g., the entire Internet [BC94]. A more reasonable approach is to study Web statistics collected from representative Web objects which are responsible for initiating, responding to, and carrying Web transactions. <p> We note that accesses to unique servers and URLs are a small portion 13 WWW Proxy Traffic Characterization with Application to Caching 14 of the total. Earlier versions of the work described in this chapter were reported in <ref> [AFA97, AFAW97] </ref>. The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. <p> By using our earlier work to characterize other important parameters of the proxy traffic <ref> [AFAW97, AFA97] </ref> we are ready to generate synthetic proxy log files for simulation studies. Barford and Crovella [BC98] use statistical techniques to model on/off periods of arrival rates from clients at Boston University using data collected in 1994 and 1995. <p> Our approach is actually to create synthetic proxy access log files for simulation studies. Using the results from this work and the results in Chapter 3 and <ref> [AFAW97, AFA97] </ref> we have all necessary parameters to generate a synthetic proxy workload. There are three main contributions in this chapter. First, we introduce a new approach to model traffic that has oscillatory or periodic autocorrelation and is self-similar. <p> With respect to accessed file types the behavior between the two groups is not really different. In both cases graphics and text (which includes HTML) are responsible for 95 percent of the references to the EI server. This is consistent with the identified invariant for accesses to proxies in <ref> [AFA97] </ref> and Digital Library for Computer Science Courses 64 Digital Library for Computer Science Courses 65 Digital Library for Computer Science Courses 66 Table 5.2: File type distribution on the EI server, remote clients vs. local clients Local clients Remote clients File type %Refs %Bytes %Refs %Bytes Graphics 45.95 40.94 44.93
Reference: [AFAW97] <author> G. Abdulla, E. A. Fox, M. Abrams, and Stephen Williams. </author> <title> WWW Proxy Traffic Characterization with Application to Caching. </title> <type> Technical Report TR-97-03, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: The work described in this dissertation was one of the major sources that helped in answering some of the questions raised by the W3C protocol design group <ref> [AFAW97, AFA97, ANF97, Pit98] </ref>. Braun and Claffy pointed out the difficulty of tracking Web statistics on a large scale | e.g., the entire Internet [BC94]. A more reasonable approach is to study Web statistics collected from representative Web objects which are responsible for initiating, responding to, and carrying Web transactions. <p> We note that accesses to unique servers and URLs are a small portion 13 WWW Proxy Traffic Characterization with Application to Caching 14 of the total. Earlier versions of the work described in this chapter were reported in <ref> [AFA97, AFAW97] </ref>. The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. <p> They have focused on inter-arrival time and discovered that the traffic is periodic on large time scales (hourly, daily and weekly). This is similar to what we have noticed and reported earlier in <ref> [AFAW97] </ref>. Arlitt and Williamson [AW96b] used six different log files to characterize accesses to WWW servers. From these logs the authors identified ten different invariants for Web server work-loads. The invariants are important since they aim to represent universal truths for all Internet Web servers. <p> By using our earlier work to characterize other important parameters of the proxy traffic <ref> [AFAW97, AFA97] </ref> we are ready to generate synthetic proxy log files for simulation studies. Barford and Crovella [BC98] use statistical techniques to model on/off periods of arrival rates from clients at Boston University using data collected in 1994 and 1995. <p> Our approach is actually to create synthetic proxy access log files for simulation studies. Using the results from this work and the results in Chapter 3 and <ref> [AFAW97, AFA97] </ref> we have all necessary parameters to generate a synthetic proxy workload. There are three main contributions in this chapter. First, we introduce a new approach to model traffic that has oscillatory or periodic autocorrelation and is self-similar. <p> Note, however, that the steps to model the deterministic part also have been Modeling Proxy Web Traffic Using Fourier Analysis 40 tested successfully on other collected proxy workloads. Second, by examining the auto-correlation plots for Web traffic, we uncover and explain the long-range dependency noticed in the literature <ref> [PF94, AFAW97] </ref>. The auto-correlation function reveals periodic long-range dependency in the examined data. Tests over several workloads show that this periodicity is not arbitrary; it is common to all tested workloads and can be explained in terms of daily and weekly cyclic behavior of Web users. <p> and thus are important for building scalable architectures; see for example [ASA + 95]. 4.2 Background and discussion 4.2.1 Network traffic and long-range dependency Long-range dependency in network traffic was noticed in the Ethernet measurement for a local area network (LAN) at Bellcore and for Web proxy and server traffic <ref> [AFAW97] </ref>. Leland et al. [LTWW94, WTSW97] show how to model this data with a self-similar process. Beran [Ber94] lists common features that distinguish a long-range dependent process, including both qualitative and quantitative features, and provides additional details on this subject. <p> They trace the causes of the existence of self-similarity to the basic characteristics of information organization and distribution. Accordingly, they claim that changes in the protocols or machine architecture will not affect self-similarity in Web traffic. Abdulla et al. <ref> [AFAW97] </ref> identified the self-similarity property as an invariant for a set of proxy workloads. However, to our knowledge, no one have done a complete study to model proxy Web traffic in a way that the models can be used in simulation studies.
Reference: [AHF97] <author> G. Abdulla, W. S. Heagy, and E. A. Fox. </author> <title> Quantitative analysis and visualization regarding interactive learning with a digital library in computer science. </title> <booktitle> In Proc. of 2nd ACM International Conference on Digital Libraries, Poster Session, </booktitle> <address> Philadelphia, PA, </address> <month> July </month> <year> 1997. </year> <note> ACM. </note>
Reference-contexts: along with its causes; * a characterization of usage of the Web for information retrieval as a sample task that is performed by Web users; * a characterization of interactions with a digital library server that is used to deliver Computer Science course material from Virginia Tech over the Web <ref> [AHF97, Hea98] </ref>; and * a simple WWW model with scenarios to show how we can help scale up the current Web architecture. The core of the work in this dissertation deals with workload characterization. Network monitoring and collecting data on the internet scale is a major challenge. <p> Key concepts of the EI project are to improve CS education by increasing interactivity and use of digital library technology <ref> [FB94, AHF97] </ref>. Currently the server hosts the home pages and class material for over 40 Computer Science courses.
Reference: [AKMS95] <author> K. Andrews, F. Kappe, H. Maurer, and K. Schmaranz. </author> <booktitle> On second generation hypermedia systems. In Proc. ED-MEDIA 95, World Conference on Educational Multimedia and Hypermedia, </booktitle> <address> Graz, Austria, </address> <month> June </month> <year> 1995. </year> <note> &lt;URL: http:- //www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/claffy/main.html&gt;. </note>
Reference-contexts: The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. Examples of such studies are systems and simulations using server <ref> [AW96b, BC94, KMR95a, AKMS95] </ref> and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96].
Reference: [ALSF97] <author> G. Abdulla, B. Liu, R. Saad, and E. A. Fox. </author> <title> Characterizing World Wide Web queries. </title> <type> Technical Report TR-97-04, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Browsing rapidly and aimlessly through large multimedia files can result in transferring huge numbers of bytes and slowing the access of someone else. Searching if done effectively can reduce the amount of browsing required for a person with a specific information need <ref> [ALSF97] </ref>. Effective search can be achieved by having effective Web Information Retrieval Systems and by teaching users to submit effective queries. 2.2.3 HTTP protocol enhancement The HTTP protocol suffers from several problems; it was not designed for optimized interaction [BC94, Get95].
Reference: [ANF97] <author> G. Abdulla, A. H. Nayfeh, and E. A. Fox. </author> <title> Modeling Correlated Proxy Web Traffic Using Fourier Analysis. </title> <type> Technical Report TR-97-19, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: The work described in this dissertation was one of the major sources that helped in answering some of the questions raised by the W3C protocol design group <ref> [AFAW97, AFA97, ANF97, Pit98] </ref>. Braun and Claffy pointed out the difficulty of tracking Web statistics on a large scale | e.g., the entire Internet [BC94]. A more reasonable approach is to study Web statistics collected from representative Web objects which are responsible for initiating, responding to, and carrying Web transactions.
Reference: [Arl96] <author> M. F. Arlitt. </author> <title> A performance study of internet web servers. </title> <type> Master's thesis, </type> <institution> University of Saskatchewan, Saskatoon, Saskatchewan, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The analysis done here shows that the data is quasiperiodic; however, for simplicity we always use the term periodic instead of quasiperiodic. 4.2.3 Web traffic characterization and self-similarity The main objective of the study reported by Arlitt <ref> [Arl96] </ref> and Arlitt and Williamson [AW96b] was to identify workload invariants across a set of log files collected from different locations. The authors identified 10 invariants in all collected workloads. Self-similarity was not reported as an invariant across all workloads, however some of the tested workloads appeared to be self-similar.
Reference: [ASA + 95] <author> M. Abrams, C. R. Standridge, G. Abdulla, S. Williams, and E. A. Fox. </author> <title> Caching proxies: Limitations and potentials. </title> <booktitle> In 4th International World-wide Web Conference, </booktitle> <pages> pages 119-133, </pages> <address> Boston, </address> <month> December </month> <year> 1995. </year> <note> &lt;URL: http://ei.cs.vt.edu/- ~succeed/WWW4/WWW4.html&gt;. </note>
Reference-contexts: Using the percentage of compressible files and the average entropy in each of those types of files we can estimate how much bandwidth we can save through compression. 2.2.5 Delta encoding The idea of caching is very appealing and experiments showed that it can save network bandwidth <ref> [ASA + 95] </ref>. Caching usually assumes that the entire document should be retrieved Background and sources of Web traffic 9 whenever it changes. <p> Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic <ref> [ASA + 95, WAS + 96, Smi94, O'C95] </ref>. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic. <p> We encourage caching using proxy servers because it captures locality of reference within the user community. Studies show that proxies have good potential for reducing network loads and thus are important for building scalable architectures; see for example <ref> [ASA + 95] </ref>. 4.2 Background and discussion 4.2.1 Network traffic and long-range dependency Long-range dependency in network traffic was noticed in the Ethernet measurement for a local area network (LAN) at Bellcore and for Web proxy and server traffic [AFAW97].
Reference: [AW96a] <author> M. Abrams and Stephen Williams. </author> <title> Complementing surveying and demographics with automated network monitoring. World Wide Web, </title> <booktitle> 1(3) </booktitle> <pages> 101-119, </pages> <month> July </month> <year> 1996. </year> <title> 103 Conclusions and Future Work 104 </title>
Reference-contexts: between the proxy p and remote servers rs Using the definitions in Table 2.1, T LAN = T c;p [ T c;rs [ T c;s [ T rc;s [ T p;rs (2.1) To capture this traffic we use tcpdump [Ste94] with filters that we have developed to extract relevant data <ref> [AW96a] </ref>. Server traffic To simplify the analysis we will split the server traffic into two sets, traffic that results from accesses by local clients, T c;s , and by remote clients, T rc;s .
Reference: [AW96b] <author> M. F. Arlitt and C. L. Williamson. </author> <title> Web server workload characterization: The search for invariants. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <pages> pages 126-137, </pages> <address> Philadelphia, PA, </address> <month> April </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. Examples of such studies are systems and simulations using server <ref> [AW96b, BC94, KMR95a, AKMS95] </ref> and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. <p> In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads <ref> [AW96b, BC96] </ref>. There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs. One of the earliest studies to characterize proxy traffic was done by Glassman [Gla94]. <p> They have focused on inter-arrival time and discovered that the traffic is periodic on large time scales (hourly, daily and weekly). This is similar to what we have noticed and reported earlier in [AFAW97]. Arlitt and Williamson <ref> [AW96b] </ref> used six different log files to characterize accesses to WWW servers. From these logs the authors identified ten different invariants for Web server work-loads. The invariants are important since they aim to represent universal truths for all Internet Web servers. <p> Table 3.1 summarizes the workloads used in this study, showing dates of collection, numbers of accesses represented, and total bytes transferred. 3.3 Proxy workload invariants In this section we identify invariants that hold true across the workloads studied. We follow closely the work done in <ref> [AW96b] </ref> in establishing the invariants. However, since our workloads WWW Proxy Traffic Characterization with Application to Caching 16 are for a different class of HTTP traffic, namely traffic seen at a caching-proxy server, we expect that though our set of invariants will overlap with the set identified in [AW96b], they will <p> done in <ref> [AW96b] </ref> in establishing the invariants. However, since our workloads WWW Proxy Traffic Characterization with Application to Caching 16 are for a different class of HTTP traffic, namely traffic seen at a caching-proxy server, we expect that though our set of invariants will overlap with the set identified in [AW96b], they will not be the same. Our workload invariants for proxy-server traffic are listed in Table 3.2. These invariants are discussed in detail below. <p> This appears in Table 3.2 as an invariant. The mean file size ranges between 7K to 27K; this is consistent with the invariant for average server file size found in <ref> [AW96b] </ref>. This is our second invariant and also appears in Table 3.2. <p> Graphics files are the most accessed type in all workloads. However HTML and graphics account for less than 90% of the total accesses which is different from the servers invariants in <ref> [AW96b] </ref>. By comparing the results from the different workloads we notice that BU (G), BU (U) and Korea follow the results reported in [AW96b]. However, in the other workloads HTML and graphics represent less than 89% of the accesses. <p> However HTML and graphics account for less than 90% of the total accesses which is different from the servers invariants in <ref> [AW96b] </ref>. By comparing the results from the different workloads we notice that BU (G), BU (U) and Korea follow the results reported in [AW96b]. However, in the other workloads HTML and graphics represent less than 89% of the accesses. This is especially true in the DEC data which has a no type category. <p> However servers get hits from all over the world and the cyclic or periodic behavior will be weaker. This could be the reason why self-similarity was not identified as one of the invariants for the server workload in <ref> [AW96b] </ref>. In this chapter we only try to show that there might be other sources for self-similarity and that it is dependent on users' behavior and schedule, protocol, and Web document structure in addition to the sources identified in [CB96]. <p> The analysis done here shows that the data is quasiperiodic; however, for simplicity we always use the term periodic instead of quasiperiodic. 4.2.3 Web traffic characterization and self-similarity The main objective of the study reported by Arlitt [Arl96] and Arlitt and Williamson <ref> [AW96b] </ref> was to identify workload invariants across a set of log files collected from different locations. The authors identified 10 invariants in all collected workloads. Self-similarity was not reported as an invariant across all workloads, however some of the tested workloads appeared to be self-similar. <p> In this chapter we analyze accesses to the Educational Infrastructure (EI) server as an example of a courseware digital library server. We compare the general statistical results with the results published in the literature. Especially, we try to assess the applicable invariants identified for Web servers in <ref> [AW96b] </ref>. We define metrics derived from examining the log files to assess the value of using the EI digital library server. <p> server, remote clients vs. local clients Local clients Remote clients File type %Refs %Bytes %Refs %Bytes Graphics 45.95 40.94 44.93 57.97 Text 50.80 48.30 52.80 40.50 Script or Map 0.11 0.01 0.03 0.00 Audio 0.04 0.03 0.01 0.04 Video 0.04 6.68 0.01 0.50 Other 3.07 3.72 2.11 0.90 servers in <ref> [AW96b] </ref>. As expected, the numbers of references to audio and video files are very small. This is due to the low percentage of audio and video files in the server collection, and because network performance does not encourage accessing this type of data. <p> Statistical tests that we performed confirmed that a lognormal or Weibull distribution can be used to model file sizes for our digital library server. This is consistent with identified invariants for other servers <ref> [AW96b] </ref>. 5.2.5 Inter-arrival time The inter-arrival time of accesses to the EI server was calculated from the log files. Also the cumulative distribution function for the inter-arrival time was calculated. Figure 5.5 shows a histogram plot for a sample of the inter-arrival times.
Reference: [AWA + 95] <author> M. Abrams, S. Williams, G. Abdulla, S. Patel, R. Ribler, and E. A. Fox. </author> <title> Multimedia traffic analysis using Chitra95. </title> <booktitle> In Proc. ACM Multimedia '95, </booktitle> <pages> pages 267-276, </pages> <address> San Francisco, </address> <month> November </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: a s : audio file transfer rate needed to achieve real time playback. r s : average transfer rate for the rest of the files that makes accessing the Web acceptable for the rest of the users. c m : data compression ratio. h r : proxy cache hit rate <ref> [AWA + 95] </ref>. c r : client cache hit rate. C t : traffic generated from one client. I c : speed of the enterprise connection to the Internet. <p> The value for r s was chosen by noticing that most Web users are happy when they get a transfer rate of 5 KB/sec. The value for the proxy cache hit rate was measured from our log files, ranging between 30%-60% <ref> [AWA + 95] </ref>. Similar numbers have been reported in the literature [MLB95]. In this chapter we use the value 40% for the network cache hit rate.
Reference: [BC94] <author> H. Braun and K. Claffy. </author> <title> Web traffic characterization: An assessment of the impact of caching documents from NCSA's Web server. </title> <booktitle> In Proc. 2nd Int. WWW Conference, </booktitle> <address> Chicago, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Educators are using the Web to post course notes, syllabi, homework assignments, and even exams and quizzes. Companies are using the Web for advertising, publicity, and to sell products. Recent literature shows that the number of clients and servers is increasing rapidly and that Web traffic displays exponential growth <ref> [BC94] </ref>. As a result, the Web has changed the fundamental dynamics of network usage. <p> The rapid growth of WWW usage often is not accompanied by an overall understanding of models of information resources and their deployment strategies [BCC + 94]. Consequently, the current Web architecture is vulnerable and lacks optimized interaction between applications and network protocols <ref> [BC94] </ref>. Performance and reliability are two major concerns for Web users. Although the Web enhances accessibility, users still demand faster response [WA97]. Performance problems can result from poor protocol design [PM94, Spe94], or inadequate servers, clients, or network speed. In addition, the popularity of the Web created other unexpected problems. <p> Effective search can be achieved by having effective Web Information Retrieval Systems and by teaching users to submit effective queries. 2.2.3 HTTP protocol enhancement The HTTP protocol suffers from several problems; it was not designed for optimized interaction <ref> [BC94, Get95] </ref>. One problem with the original protocol is the use of a new connection for every single document retrieved over the network. This creates extra packets over the network and causes network congestion. <p> Braun and Claffy pointed out the difficulty of tracking Web statistics on a large scale | e.g., the entire Internet <ref> [BC94] </ref>. A more reasonable approach is to study Web statistics collected from representative Web objects which are responsible for initiating, responding to, and carrying Web transactions. <p> The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. Examples of such studies are systems and simulations using server <ref> [AW96b, BC94, KMR95a, AKMS95] </ref> and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. <p> In some other cases multiple instances of the browser might be running on a multiuser machine. In such cases, accesses from this client might be due to several users running multiple instances of the browser. Sessions are very hard to characterize <ref> [BC94] </ref>. In this paper we define sessions with respect to clients not users.
Reference: [BC95] <author> A. Bestavros and C. Cunha. </author> <title> A prefetching protocol using client speculation for the WWW, May 1995. </title> <address> http://www.cs.bu.edu/students/grads/carro/. </address>
Reference-contexts: An important feature of models created for Web traffic simulation studies is that they should have a period of several weeks or months in order to capture the cache behavior and predict user accesses for studies such as forward caching or pre-fetching <ref> [BC95, Lee96] </ref>. Forward caching is when documents are pushed to the places where it is anticipated that they will be accessed. Collecting traces that satisfy this requirement is a major challenge for technical and privacy reasons.
Reference: [BC96] <author> A. Bestavros and C. Cunha. </author> <title> Server-initiated document dissemination for the WWW. IEEE Data Engineering Bulletin, </title> <month> September </month> <year> 1996. </year> <note> http://www.cs.bu.edu/students/grads/carro/. </note>
Reference-contexts: In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads <ref> [AW96b, BC96] </ref>. There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs. One of the earliest studies to characterize proxy traffic was done by Glassman [Gla94].
Reference: [BC98] <author> P. Barford and M. Crovella. </author> <title> An Architecture for a WWW Workload Generator. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: In Table 3.4 we give a suitable statistical model of the file size for each workload. We used ExpertFit to derive the model in each case. The results for the Boston University workloads (BU (G) and BU (U)) were consistent with findings in <ref> [BC98] </ref>. In all workloads the best model suggested by ExpertFit was either a Weibull or a Lognormal distribution. With the huge size of the tested samples, fitting the data to a statistical distribution can become very difficult and involves artistic as well as scientific judgment. <p> By using our earlier work to characterize other important parameters of the proxy traffic [AFAW97, AFA97] we are ready to generate synthetic proxy log files for simulation studies. Barford and Crovella <ref> [BC98] </ref> use statistical techniques to model on/off periods of arrival rates from clients at Boston University using data collected in 1994 and 1995. The work presented in this chapter is different from their paper in three ways.
Reference: [BCC + 94] <author> R. C. Berwick, J. M. Carroll, C. Connolly, J. Foley, E. A. Fox, T. Imielinski, and V. S. Subrahmanian. </author> <title> Research priorities for the World-wide Web. Report of the NSF Workshop Sponsored by the Information, </title> <journal> Robotics, and Intelligent Systems Division, </journal> <month> October </month> <year> 1994. </year> <note> &lt;URL:http://www.cc.gatech.edu/gvu/nsf-ws/report/- Report.html&gt;. </note>
Reference-contexts: The rapid growth of WWW usage often is not accompanied by an overall understanding of models of information resources and their deployment strategies <ref> [BCC + 94] </ref>. Consequently, the current Web architecture is vulnerable and lacks optimized interaction between applications and network protocols [BC94]. Performance and reliability are two major concerns for Web users. Although the Web enhances accessibility, users still demand faster response [WA97].
Reference: [Ber94] <author> J. Beran. </author> <title> Statistics for Long-Memory Process. </title> <publisher> Chapman and Hall, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: These graphs can be used to test for the appearance of self-similarity in the data, however that does not prove that the data has this property. Definitive tests for self-similarity in the collected data showed that these workloads are self-similar. Table 3.9 shows the estimated Hurst or H parameter <ref> [Ber94] </ref> for tested workloads. <p> The value obtained for the Hurst parameter from the R/S plot was 0.86 and from the normalized variance plot was 0.82. For 0:5 H 1 the correlation decays to zero slowly so that 1 X (k) = 1 and the process has long memory <ref> [Ber94] </ref>. As a result we expect that the access rate for the VT-CS data will have a strong correlation. To test for this we extracted accesses/minute, accesses/hour, accesses/day, and accesses/week. We then generated a time series and plotted the autocorrelation function with different lags. <p> Leland et al. [LTWW94, WTSW97] show how to model this data with a self-similar process. Beran <ref> [Ber94] </ref> lists common features that distinguish a long-range dependent process, including both qualitative and quantitative features, and provides additional details on this subject.
Reference: [BHH96] <author> Christine L. Borgman, Sandra G. Hirsh, and John Hiller. </author> <title> Rethinking online monitoring methods for information retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 47(7) </volume> <pages> 568-583, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: To help users locate information on the Web, special search tools and cataloging systems were developed. We call these tools Web Information Retrieval Systems (IRS). They have evolved rapidly since 1994, and have been used by millions who thereby had their first experience with search engines. Borgman et al. <ref> [BHH96] </ref> observed that "The end users who now dominate searching are using systems with exploratory interfaces, under less time pressure, and have less clear retrieval goals than do skilled search intermediaries." As part of our effort to characterize WWW traffic so as to support modeling, planning and prediction, and to help
Reference: [Bla95] <author> M. Blakeley. </author> <title> Webstone performance analysis: Sun netrai20. </title> <address> &lt;URL:http://www.sgi.com/Products/WebFORCE/WebStone/sun-ss20/sun-ss20.html&gt;, </address> <month> December </month> <year> 1995. </year> <pages> SGI. </pages>
Reference-contexts: This number might be very high compared to the client request rate since it represents the aggregation of requests by all clients. For a Web dedicated workstation, such as the SGI WebFORCE, the benchmarks show that it will sustain up to 96 requests/sec <ref> [Bla95] </ref>. Note that there 86,400 seconds per day, so this type of workstation could support 8.3 million requests per day if they are uniformly distributed, and 345 K requests per hour (if one considers peak times most important). <p> We will include compression as a factor, and will assume that the compression ratio cm is 0.5 for data types other than video or audio data types. We will start by assuming that the server can support 96 requests/sec with no errors <ref> [Bla95] </ref>. For this scenario we will use two cases from the previous scenarios as base cases. The first base case is the same base case for scenario 1, where we concluded that 1942 simultaneous clients can be supported. This case represents the currently measured values for the previous parameters.
Reference: [Blo76] <author> P. Bloomfield. </author> <title> Fourier Analysis of Time Series: an Introduction. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: These plots were made after the data mean was extracted. (We show how to characterize the mean in the next section.) The plot shows half of the data points because the second half is a mirror image of the first half due to aliasing <ref> [Blo76] </ref>. Again we clearly see the peaks that correspond to daily and weekly cycles. We now show how to find the frequencies associated with these peaks. <p> Before building a complete model, we need to determine the amplitudes and periods of all of the periodic components. Finding all of the periods can be done by using Fourier analysis, as described in the previous section. Determining the amplitudes can be done using linear regression <ref> [Blo76] </ref>. The details are given below. Determining the frequencies Frequencies, single or multiple, can be determined using Fourier spectra or autoregressive spectra. <p> This function is quadratic so we cannot use linear regression to find an R that minimizes *. To solve this problem, we let a = R cos and b = R sin <ref> [Blo76] </ref>. <p> can capture the changing mean, the periodic part, and the random part: X (t) = (t) + i 4.6 Measuring periodicity in the data To find the strength of our periodic signal with respect to the random part, we use a measure that is called signal to noise ratio (SNR) <ref> [Blo76] </ref>, defined as, SN R = mean square value (M SV ) of the signal mean square value (M SV ) of the random part Modeling Proxy Web Traffic Using Fourier Analysis 57 For the periodic signal M SV = (y s y s ) 2 Where y s is the
Reference: [BR94] <author> George E. P. Box and Gwilym M. Jenkins Gregory C. Reinsel. </author> <title> Time Series Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> London, </address> <year> 1994. </year> <title> Conclusions and Future Work 105 </title>
Reference-contexts: CS b- Korea c- Library d- Engineering 4.4.2 Auto-correlation function The autocorrelation function R xx (t ) is a measure of correlation between x (t) and x (t + t ), where t is a time delay <ref> [NB95, BR94, Sta97] </ref>. For random signals, R xx (t ) decays to zero as t increases. However for a signal with a periodic component, R xx (t ) does not decay to zero Modeling Proxy Web Traffic Using Fourier Analysis 45 and is oscillatory.
Reference: [CB96] <author> M. E. Crovella and A. Bestavros. </author> <title> Self-similarity in World Wide Web traffic evidence and possible causes. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <pages> pages 160-169, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads <ref> [CB96, CBC95, Cun97] </ref> and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs. <p> The invariants in the study were used to identify two strategies for cache design and to determine the bounds on performance improvement due to each strategy. A study to test if those invariants hold true over time and across location is necessary. In <ref> [CB96] </ref> and [CBC95] the data were collected from a group of clients accessing the Web. Cunha et al. [CBC95] showed that many characteristics of the WWW can be modeled using power-law distributions. Crovella and Bestavros [CB96] showed that the Web traffic has characteristics that are consistent with self-similarity. <p> In <ref> [CB96] </ref> and [CBC95] the data were collected from a group of clients accessing the Web. Cunha et al. [CBC95] showed that many characteristics of the WWW can be modeled using power-law distributions. Crovella and Bestavros [CB96] showed that the Web traffic has characteristics that are consistent with self-similarity. They traced the reasons for Web self-similarity to the basic characteristics of information organization and retrieval. Finally, Pitkow [Pit98] comprehensively summarizes work done in the field of workload characterization. <p> Access rate is defined as the number of accesses that a proxy gets per unit time. Bytes rate is defined as the number of bytes that a proxy sends per unit time. We used the R/S statistics and the normalized variance tests to estimate the H value <ref> [CB96] </ref>. For self-similar traffic H should be between 0.5 and 1. The closer H is to one, the stronger the self-similarity in the tested traffic. The Boston traffic has been proven previously to be self-similar [CB96]. <p> We used the R/S statistics and the normalized variance tests to estimate the H value <ref> [CB96] </ref>. For self-similar traffic H should be between 0.5 and 1. The closer H is to one, the stronger the self-similarity in the tested traffic. The Boston traffic has been proven previously to be self-similar [CB96]. Self-similarity is an invariant and it is listed in Table 3.2. 3.3.7 Invariants Table 3.2 shows a list of the identified invariants across the workloads. Some of the identified invariants can be linked directly to caching and can be used to estimate if caching will help or not. <p> We then generated a time series and plotted the autocorrelation function with different lags. Figure 3.7, which is the ACF plot for accesses/hour, shows that the correlation is strong and cyclic. The daily cycle is very clear in the figure; every 24 hours we get a peak. In <ref> [CB96] </ref> the authors tried to identify sources of self-similarity in the Web traffic by showing that transmission times may be heavy-tailed, primarily due to the distribution of Web file sizes. The transmission times correspond to the on times and user think time corresponds to the off time. <p> In this chapter we only try to show that there might be other sources for self-similarity and that it is dependent on users' behavior and schedule, protocol, and Web document structure in addition to the sources identified in <ref> [CB96] </ref>. Figure 3.7 shows that there is a significant correlation between accesses from one hour to another. This leads us to hypothesize that a source for Web traffic burstiness and hence self-similarity is the way users interact with Web browsers. <p> Self similarity also was found to be weaker in the bytes rate data (see Table 3.9). This suggests that including file sizes might have a negative effect on self-similarity contrary to what was suggested by <ref> [CB96] </ref>. 3.7 Conclusions In this chapter we identified a set of invariants that hold true across different proxy workloads. One of the identified invariants was self-similarity. We also showed that some of the identified invariants hold over time for the VT-CS workload. <p> Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption <ref> [CB96, LTWW94, PF94, Pax97] </ref>. Leland and Wilson [LW91] show that simulations using real network traffic produce different results from simulations of synthetic traffic generated using traditional approaches (that ignore long-range dependency in network traffic). <p> The authors identified 10 invariants in all collected workloads. Self-similarity was not reported as an invariant across all workloads, however some of the tested workloads appeared to be self-similar. Crovella et al. <ref> [CB96] </ref> used traces collected from Web clients to test for the existence of self-similarity in the client-based Web traffic and to explain the causes for this phenomenon. They conclude that traffic due to WWW transfers shows characteristics that are approximately consistent with self-similarity.
Reference: [CBC95] <author> C. R. Cunha, A. Bestavros, and M. E. Crovella. </author> <title> Characteristics of WWW client-based traces. </title> <type> Technical Report BT-CS-95-010, </type> <institution> Dept. of Comp. Sci., Boston Univ., </institution> <address> Boston, MA 02215, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads <ref> [CB96, CBC95, Cun97] </ref> and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs. <p> The invariants in the study were used to identify two strategies for cache design and to determine the bounds on performance improvement due to each strategy. A study to test if those invariants hold true over time and across location is necessary. In [CB96] and <ref> [CBC95] </ref> the data were collected from a group of clients accessing the Web. Cunha et al. [CBC95] showed that many characteristics of the WWW can be modeled using power-law distributions. Crovella and Bestavros [CB96] showed that the Web traffic has characteristics that are consistent with self-similarity. <p> A study to test if those invariants hold true over time and across location is necessary. In [CB96] and <ref> [CBC95] </ref> the data were collected from a group of clients accessing the Web. Cunha et al. [CBC95] showed that many characteristics of the WWW can be modeled using power-law distributions. Crovella and Bestavros [CB96] showed that the Web traffic has characteristics that are consistent with self-similarity. They traced the reasons for Web self-similarity to the basic characteristics of information organization and retrieval.
Reference: [CR92] <author> J. M. Carroll and M. B. Rosson. </author> <title> Getting around the task-artifact cycle: How to make claims and design by scenario. </title> <journal> TOIS, </journal> <volume> 10(2) </volume> <pages> 181-212, </pages> <year> 1992. </year>
Reference-contexts: We think that Web IRS should help correct such mistakes for users. Another study could look into the session characterization problem in more detail. Chapter 7 Network Traffic is not the Sum of its Components 7.1 Introduction In this chapter we define scenarios <ref> [CR92, JRC97] </ref> that can be used to test the WWW and distributed information systems' scalability with emphasis on the educational systems.
Reference: [Cun97] <author> C. R. Cunha. </author> <title> Trace Analysis and Its Application to Performance Enhancements of Distributed Information. </title> <type> PhD thesis, </type> <institution> Boston University, Computer Science, </institution> <year> 1997. </year> <note> http://www.cs.bu.edu/students/grads/carro/. </note>
Reference-contexts: Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads <ref> [CB96, CBC95, Cun97] </ref> and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs.
Reference: [DH93] <author> P. B. Danzig and R. S. Hall. </author> <title> A case for caching file objects inside the in-ternetworks. </title> <type> Technical Report CU-CS-642-93, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1993. </year>
Reference-contexts: The convenience of Web browsers made misuse of HTTP inevitable. Nevertheless, in many cases, caching can alleviate bandwidth consumption. For example, see the discussion on FTP caching in <ref> [DH93] </ref>. The Web design, not inherently scalable, can be helped by greater use of proxies in concert with widespread migration (e.g., push) of document copies from servers to points closer to users.
Reference: [DJ91] <author> P. B. Danzig and S. Jamin. tcplib: </author> <title> A library of TCP intenetwork traffic characteristics. </title> <type> Technical Report USC-CS-91-495, </type> <institution> Computer Science Department, University of Southern California, </institution> <year> 1991. </year>
Reference-contexts: Chapter 4 Modeling Proxy Web Traffic Using Fourier Analysis 4.1 Introduction Proper and correct models of network and WWW traffic are important for simulation studies and for understanding the nature of interactions that appear over the Internet and the Web <ref> [DJ91, PF94, Pax97] </ref>, as aids to planning network infrastructure. Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption [CB96, LTWW94, PF94, Pax97].
Reference: [FB94] <author> E. A. Fox and N. D. Barnette. </author> <title> Improving education through a computer science digital library with three types of WWW servers. </title> <note> In Second World-wide Web Conference '94: Mosaic and the Web, 1994. &lt;URL: http://ei.cs.vt.edu/papers/WWW94.html&gt;. </note>
Reference-contexts: Key concepts of the EI project are to improve CS education by increasing interactivity and use of digital library technology <ref> [FB94, AHF97] </ref>. Currently the server hosts the home pages and class material for over 40 Computer Science courses.
Reference: [GB97] <author> S. D. Gribble and E. A. Brewer. </author> <title> System design issues for internet middleware services: Deduction from a large client trace. </title> <booktitle> In Usenix Symposium on Internet Technologies and System, </booktitle> <address> Monterey, California, </address> <year> 1997. </year>
Reference-contexts: Gwertzman and Seltzer [GS96] have used several server and proxy traffic sources to characterize MIME types and the average life span of a Web document. A study that characterizes accesses of dial-in users with modem connections was done by Gribble and Brewer <ref> [GB97] </ref>. They have focused on inter-arrival time and discovered that the traffic is periodic on large time scales (hourly, daily and weekly). This is similar to what we have noticed and reported earlier in [AFAW97]. Arlitt and Williamson [AW96b] used six different log files to characterize accesses to WWW servers. <p> In other words, self-similar models by themselves might not be enough to capture the long-range dependency in the Web traffic especially if they are constructed from pure statistical on/off signals. A daily cycle was observed by other researchers, see for example Gribble and Brewer <ref> [GB97] </ref>. To our knowledge, however, no one has succeeded in using this phenomena to model the traffic. Gribble et al. describe the daily cycle and argue that internet service providers should be ready to deal with this behavior.
Reference: [Get95] <author> J. Gettys. </author> <title> HTTP problem statement. </title> <address> &lt;URL: http://www.w3.org/- Protocols/HTTP-NG/951005Problem.html&gt;, </address> <year> 1995. </year> <month> W3C. </month>
Reference-contexts: Although the Web enhances accessibility, users still demand faster response [WA97]. Performance problems can result from poor protocol design [PM94, Spe94], or inadequate servers, clients, or network speed. In addition, the popularity of the Web created other unexpected problems. Scalability, latency, bandwidth, and disconnected operations <ref> [Get95] </ref> are some of the important issues that should be considered to make up for the rate of growth in Web usage. <p> The WWW Consortium (W3C) realized these issues and they have launched an effort to design a new protocol that will be able to support future demands 1 Chapter 1. Introduction 2 <ref> [Get95] </ref>. Spero [Spe94] did a careful analysis of the HTTP protocol and showed that it is not optimized for fast interactions. <p> This huge success leads to new challenges that almost every Web user is willing to help solve so as to keep the Web alive. Major challenges to the Web are scalability, latency, bandwidth, and disconnected operations <ref> [Get95] </ref>. Scaling issues are a major concern with the current Web architecture. To help with such issues the protocol that carries Web interactions should be redesigned to be able to meet the new challenges. <p> Effective search can be achieved by having effective Web Information Retrieval Systems and by teaching users to submit effective queries. 2.2.3 HTTP protocol enhancement The HTTP protocol suffers from several problems; it was not designed for optimized interaction <ref> [BC94, Get95] </ref>. One problem with the original protocol is the use of a new connection for every single document retrieved over the network. This creates extra packets over the network and causes network congestion.
Reference: [Gla94] <author> S. Glassman. </author> <title> A caching relay for the World-wide Web. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2) </volume> <pages> 165-173, </pages> <year> 1994. </year> <note> &lt;URL: http://www1.cern.ch/PapersWWW94/steveg.ps&gt;. </note>
Reference-contexts: There are, however, fewer studies to fully characterize proxy traffic. This is due to the difficulty of collecting proxy log files from different sources due to the sensitivity of such logs. One of the earliest studies to characterize proxy traffic was done by Glassman <ref> [Gla94] </ref>. Characterization included parameters such as document popularity, cache misses, cache hits, and rate of change for Web pages. Sedayao [Sed94] studied and characterized the distribution of several parameters.
Reference: [GS96] <author> J. Gwertzman and M. Seltzer. </author> <title> World Wide Web cache consistency. </title> <booktitle> In Proc. of the 1996 Usenix Technical Conference, </booktitle> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: One of the earliest studies to characterize proxy traffic was done by Glassman [Gla94]. Characterization included parameters such as document popularity, cache misses, cache hits, and rate of change for Web pages. Sedayao [Sed94] studied and characterized the distribution of several parameters. Gwertzman and Seltzer <ref> [GS96] </ref> have used several server and proxy traffic sources to characterize MIME types and the average life span of a Web document. A study that characterizes accesses of dial-in users with modem connections was done by Gribble and Brewer [GB97].
Reference: [Hea98] <author> W. Heagy. </author> <title> WWW server log data visualization and evaluation. </title> <type> Master's thesis, </type> <institution> Computer Sci. Dept., Virginia Tech, Blacksburg, </institution> <address> VA 24061-0106, </address> <month> Anticpated August </month> <year> 1998. </year> <title> Conclusions and Future Work 106 </title>
Reference-contexts: along with its causes; * a characterization of usage of the Web for information retrieval as a sample task that is performed by Web users; * a characterization of interactions with a digital library server that is used to deliver Computer Science course material from Virginia Tech over the Web <ref> [AHF97, Hea98] </ref>; and * a simple WWW model with scenarios to show how we can help scale up the current Web architecture. The core of the work in this dissertation deals with workload characterization. Network monitoring and collecting data on the internet scale is a major challenge.
Reference: [HP97] <author> A. V. Hoff and J. Payne. </author> <title> Generic Diff Format Specification. </title> <journal> &lt;URL: </journal> <note> http://www.w3.org/TR/NOTE-gdiff-19970901&gt;, 1997. W3C. </note>
Reference-contexts: The work reported by Mogul et al. [MDFK97] shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. The research in this field triggered recommendations to extend the HTTP protocol to include delta encoding in its future versions <ref> [MDFK97, Wil97, HP97] </ref>. 2.3 Web traffic, data collection and analysis Characterizing interactions and kinds of tasks that the Web is used for is an essential step to help design a better protocol.
Reference: [Jai91] <author> R. Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: In the data under examination. The histogram shows that the distribution is asymmetric and the right-hand side of the distribution contains more values than the left-hand side. This implies that the distribution might be a long-tail distribution. The quantile-quantile plot <ref> [Jai91] </ref> confirms that the distribution is a long-tail distribution from the right-hand side. We use a Weibull distribution to model the residual. To generate a synthetic set of data, we used the UnifitII software [VL92]. The parameters for the distribution were estimated from the data.
Reference: [Jan97] <author> B. Janssen. </author> <note> PARC HTTP-NG project. &lt;URL: http://www.parc.xerox.com/- istl/projects/http-ng/&gt;, 1997. Xerox. </note>
Reference-contexts: We need to look at the long term behavior of Web users, and find similarities and differences in the way different communities use the Web. The results obtained can be used in generating scenarios for the new HTTP protocol design <ref> [Jan97] </ref>, for building simulation models, and for better understanding Web growth and usage. The W3C realizes the importance of studying and understanding the system under consideration before attempting to optimize it [NG97]. We need to characterize the way the Web is being used.
Reference: [Joh97] <author> T. Johnson. Webwatcher. &lt;http://www.cs.vt.edu/ chitra/webwatcher.html&gt;, </author> <year> 1997. </year> <institution> Computer Sci. Dept., Virginia Tech. </institution>
Reference-contexts: Data files grow in size rapidly and have to be transferred to the main file archive as quickly as possible. 99 Conclusions and Future Work 100 This led us to come up with a more automated logging process <ref> [Joh97] </ref> where we use scripts to move the files from the collection machines to the main store every week. Maintaining the hardware running was another problem that required immediate attention.
Reference: [JRC97] <author> G. Chin Jr., M. B. Rosson, and J. M. Carroll. </author> <title> Participatory analysis: Shared development of requirements from scenarios. </title> <booktitle> In CHI, </booktitle> <pages> pages 162-169, </pages> <year> 1997. </year>
Reference-contexts: We think that Web IRS should help correct such mistakes for users. Another study could look into the session characterization problem in more detail. Chapter 7 Network Traffic is not the Sum of its Components 7.1 Introduction In this chapter we define scenarios <ref> [CR92, JRC97] </ref> that can be used to test the WWW and distributed information systems' scalability with emphasis on the educational systems.
Reference: [KMR95a] <author> T. Kwan, R. McGrath, and D. Reed. </author> <title> NCSA's World Wide Web server: Design and performance. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 68-74, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The next section relates this study to some of the relevant literature. 3.1.1 Related work Most of the research that uses proxy traces aims to reduce network latency, enhance response time and conserve network bandwidth. Examples of such studies are systems and simulations using server <ref> [AW96b, BC94, KMR95a, AKMS95] </ref> and proxy traffic [ASA + 95, WAS + 96, Smi94, O'C95]. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96].
Reference: [KMR95b] <author> T. T. Kwan, R. E. McGrath, and D. A. Reed. </author> <title> User access patterns to NCSA's World-Wide Web server. </title> <type> Technical Report UIUCDCS-R-95-1934, </type> <institution> Dept. of Comp. Sci., Univ. of IL, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: In this chapter we discuss methods and techniques that will help in scaling up the Web. We start by identifying techniques that improve scalability other than the brute force solutions of increasing network bandwidth and server throughput. These techniques are: use of caching on the server side (e.g., <ref> [KMR95b] </ref>), on the client side (e.g., caches built into Web browsers), and in the network (known as "Proxy caching") (e.g., [LA94]); creating better Web citizens; protocol enhancement; compression; and finally delta encoding. 2.2.1 Caching One impediment to scalability is use of the wrong protocol for a given type of document delivery. <p> Caching on the server side reflects overall interest in server content. It can be implemented by replicating the file system and the HTTP server and connecting the replicated servers with a high speed network <ref> [KMR95b] </ref>. This is similar to server mirroring, where the data on the server is copied into several other servers to reduce the load on the network and the original server. However in server mirroring the servers are not placed in one location; they can be separated by long distances. <p> This solution was adopted by NCSA to scale up their popular server <ref> [KMR95b] </ref>.
Reference: [LA94] <author> A. Luotonen and K. Altis. </author> <title> World-Wide Web proxies. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2), </volume> <year> 1994. </year> <note> &lt;URL: http://www1.cern.ch/PapersWWW94/- luotonen.ps&gt;. </note>
Reference-contexts: These techniques are: use of caching on the server side (e.g., [KMR95b]), on the client side (e.g., caches built into Web browsers), and in the network (known as "Proxy caching") (e.g., <ref> [LA94] </ref>); creating better Web citizens; protocol enhancement; compression; and finally delta encoding. 2.2.1 Caching One impediment to scalability is use of the wrong protocol for a given type of document delivery.
Reference: [LAJF98] <author> B. Liu, G. Abdulla, T. Johnson, and E. A. Fox. </author> <title> Web response time and proxy caching. </title> <type> Technical Report TR-98-07, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: One of our findings is that the locality of reference within a certain community is high, hence caching can be an effective tool to help reduce network traffic and so help in solving the scalability problem. Recent tests and studies by our group, however, showed that proxies increase latency <ref> [Liu98, LAJF98] </ref>, so some type of engineering balance is needed involving network traffic quantity and time for response latency. Chapter 1. <p> Conclusions and Future Work 101 In a study to characterize WWW latency <ref> [Liu98, LAJF98] </ref> we found that latency also displays a periodic behavior and its peaks coincide with the peaks of request rate to proxies from our server. This suggests that the latency is due to the network not servers.
Reference: [Lee96] <author> D. C. Lee. </author> <title> Improving user response time for the World-wide Web using pre-fetch data caching. </title> <type> Master's thesis, </type> <institution> Electrical Eng. Dept., Virginia Tech, Blacksburg, </institution> <address> VA 24061-0106, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: An important feature of models created for Web traffic simulation studies is that they should have a period of several weeks or months in order to capture the cache behavior and predict user accesses for studies such as forward caching or pre-fetching <ref> [BC95, Lee96] </ref>. Forward caching is when documents are pushed to the places where it is anticipated that they will be accessed. Collecting traces that satisfy this requirement is a major challenge for technical and privacy reasons.
Reference: [Liu98] <author> B. Liu. </author> <title> World Wide Web latency and proxy caching. </title> <type> Master's thesis, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <year> 1998. </year> <note> Thesis in Progress. </note>
Reference-contexts: One of our findings is that the locality of reference within a certain community is high, hence caching can be an effective tool to help reduce network traffic and so help in solving the scalability problem. Recent tests and studies by our group, however, showed that proxies increase latency <ref> [Liu98, LAJF98] </ref>, so some type of engineering balance is needed involving network traffic quantity and time for response latency. Chapter 1. <p> Conclusions and Future Work 101 In a study to characterize WWW latency <ref> [Liu98, LAJF98] </ref> we found that latency also displays a periodic behavior and its peaks coincide with the peaks of request rate to proxies from our server. This suggests that the latency is due to the network not servers.
Reference: [LK91] <author> A. M. Law and W. D. </author> <title> Kelton. Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, 2nd edition, </address> <year> 1991. </year>
Reference-contexts: This means that the main source for the correlation is the periodic behavior of the Web users. Modeling Proxy Web Traffic Using Fourier Analysis 53 In the rest of this section, we attempt to model the third component of the data, namely the random part using techniques described in <ref> [LK91] </ref>. Fitting the residual We use quantile summaries and histogram plots to study the distribution for the residual. In the data under examination. The histogram shows that the distribution is asymmetric and the right-hand side of the distribution contains more values than the left-hand side.
Reference: [LTWW94] <author> W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson. </author> <title> On the self-similar nature of ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption <ref> [CB96, LTWW94, PF94, Pax97] </ref>. Leland and Wilson [LW91] show that simulations using real network traffic produce different results from simulations of synthetic traffic generated using traditional approaches (that ignore long-range dependency in network traffic). <p> New methods and approaches that capture the characteristics of Web traffic should be developed. There are several efforts to define methods to generate self-similar traffic for network simu 38 Modeling Proxy Web Traffic Using Fourier Analysis 39 lation purposes. Leland et al. <ref> [LTWW94, WTSW97] </ref> describe a methodology for generating bursty and hence self-similar traffic by multiplexing a large number of on/off sources with heavy tailed period lengths. <p> Leland et al. <ref> [LTWW94, WTSW97] </ref> show how to model this data with a self-similar process. Beran [Ber94] lists common features that distinguish a long-range dependent process, including both qualitative and quantitative features, and provides additional details on this subject. <p> We demonstrate the modeling approach with an example and we validate each step by comparing the generated data to the original. In Appendix A we show that the periodic behavior exists for the ethernet traffic used in an important Bellcore study <ref> [LTWW94, Pax97, WTSW97] </ref>, however, the complete modeling is beyond the scope of the work described here. As part of characterizing specific WWW activities we provided a characterization of our courseware digital library server and accesses to Web IRS.
Reference: [LV95] <author> Averill M. Law and Stephen Vincent. </author> <title> ExpertFit User's Guide. </title> <editor> Averill M. </editor> <publisher> Law and Associates, </publisher> <address> Tucson, AZ 85717, </address> <month> August </month> <year> 1995. </year> <title> Conclusions and Future Work 107 </title>
Reference-contexts: In all workloads we notice that we have a small percentage of clients which are very active followed by less active clients. The number of accesses decays rapidly and shows an asymptotic behavior. By fitting the data to distributions using ExpertFit <ref> [LV95] </ref> we found out that they follow a "Weibull" distribution [Mey70] with ff = 0:44 for the Korea workload, ff = 0:51 for the library workload, ff = 0:37 for the CS workload, and ff = 0:48 for the AUB workload.
Reference: [LW91] <author> W. E. Leland and D. V. Wilson. </author> <title> High time-resolution measurement and analysis of LAN traffic: Implications for LAN interconnection. </title> <booktitle> In Proc. of IEEE Infocomm '91, </booktitle> <pages> pages 1360-1366, </pages> <address> Bar Harbour, FL, 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption [CB96, LTWW94, PF94, Pax97]. Leland and Wilson <ref> [LW91] </ref> show that simulations using real network traffic produce different results from simulations of synthetic traffic generated using traditional approaches (that ignore long-range dependency in network traffic).
Reference: [Mat97] <author> Mathworks. </author> <title> Matlab Users's Manual, Version 5.1. </title> <publisher> Mathworks Inc., </publisher> <address> Natick, MA 01760, </address> <year> 1997. </year>
Reference-contexts: The files were analysed using perl scripts. The script extracts the times of access and then creates a time series of arrival rates per hour for each log file. The time series is further examined using Splus [Sta97] and Matlab <ref> [Mat97] </ref>. The workloads, except those from Korea and Boston, were considered when collecting the WWW LAN traffic and using Figure 2.1 and equations 2.1 and 2.3 we extract the proxy traffic.
Reference: [Mau96] <author> H. Maurer. </author> <title> Hyper-G now Hyperwave, the Next Generation Web Solution. </title> <publisher> Ad-dison Wesley, 2725 Sand Hall Road, </publisher> <address> Menlo Park, CA 94025, 1st edition, </address> <year> 1996. </year>
Reference-contexts: This phenomenon was termed the "flash crowd" by Jakob Nielsen <ref> [Mau96] </ref>, when winter sports fans tried to access the latest results of the 1994 Winter Olympics in Lillehammer that were posted on the Web by the Norwegian Oslonett.
Reference: [MDFK97] <author> J. C. Mogul, F. Douglis, A. Feldman, and B. Krishnamurthy. </author> <title> Potential benefits of delta encoding and data compression for HTTP. In Proceedings of SIG-COMM, </title> <address> Cannes, France, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: The idea behind delta encoding is to retrieve only the parts of the document which have changed [Wil97]. Using delta encoding over the Web was suggested in [WAS + 96]. The work reported by Mogul et al. <ref> [MDFK97] </ref> shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. <p> The work reported by Mogul et al. [MDFK97] shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. The research in this field triggered recommendations to extend the HTTP protocol to include delta encoding in its future versions <ref> [MDFK97, Wil97, HP97] </ref>. 2.3 Web traffic, data collection and analysis Characterizing interactions and kinds of tasks that the Web is used for is an essential step to help design a better protocol.
Reference: [Mey70] <author> P. L. Meyer. </author> <title> Introductory Probability and Statistical Applications. </title> <publisher> Addison Wesley, 2725 Sand Hall Road, </publisher> <address> Menlo Park, CA 94025, 2nd edition, </address> <year> 1970. </year>
Reference-contexts: The number of accesses decays rapidly and shows an asymptotic behavior. By fitting the data to distributions using ExpertFit [LV95] we found out that they follow a "Weibull" distribution <ref> [Mey70] </ref> with ff = 0:44 for the Korea workload, ff = 0:51 for the library workload, ff = 0:37 for the CS workload, and ff = 0:48 for the AUB workload. This indicates that a small percentage of the clients are responsible for most of the accesses.
Reference: [MLB95] <author> R. Malpani, J. Lorch, and D. Berger. </author> <title> Making World Wide Web caching servers cooperate. </title> <booktitle> In 4th International World-wide Web Conference, </booktitle> <pages> pages 107-117, </pages> <address> Boston, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The effectiveness of the network cache can increase by placing it where we know that a group of users has a high degree of similarity of interest (or locality of reference), and by implementing multiple <ref> [MLB95] </ref> or hierarchical proxy caching. Multiple proxy caching is when many clients share many caches and a cache that misses can query other caches. In two-level caching we have several network caches connected to another parent or first level network cache with a larger cache size. <p> The value for the proxy cache hit rate was measured from our log files, ranging between 30%-60% [AWA + 95]. Similar numbers have been reported in the literature <ref> [MLB95] </ref>. In this chapter we use the value 40% for the network cache hit rate.
Reference: [MS97] <author> S. Manley and M. Seltzer. </author> <title> Web facts and fantasy. </title> <booktitle> In Proceedings of the 1997 USENIX Symposium on Internet Technologies and Systems, </booktitle> <address> Monterey, Canada, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: Measuring and characterizing the file sizes and distributions should continue since the distribution might change over time in accord with the change of technology. There is a study, however, that suggests that the number of dynamically generated documents does not increase in time <ref> [MS97] </ref> which is an accurate statement about our server, however, it was not true for the proxy traffic case which we demonstrated in Chapter 3.
Reference: [NB95] <author> A. H. Nayfeh and B. Balachandran. </author> <title> Applied Nonlinear Dynamics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: CS b- Korea c- Library d- Engineering 4.4.2 Auto-correlation function The autocorrelation function R xx (t ) is a measure of correlation between x (t) and x (t + t ), where t is a time delay <ref> [NB95, BR94, Sta97] </ref>. For random signals, R xx (t ) decays to zero as t increases. However for a signal with a periodic component, R xx (t ) does not decay to zero Modeling Proxy Web Traffic Using Fourier Analysis 45 and is oscillatory.
Reference: [NG97] <author> H. F. Nielsen and J. Gettys. </author> <title> HTTP-NG the next generation. </title> <journal> &lt;URL: </journal> <note> http://www.w3.org/Protocols/HTTP-NG/Activity.html&gt;, 1997. W3C. </note>
Reference-contexts: The results obtained can be used in generating scenarios for the new HTTP protocol design [Jan97], for building simulation models, and for better understanding Web growth and usage. The W3C realizes the importance of studying and understanding the system under consideration before attempting to optimize it <ref> [NG97] </ref>. We need to characterize the way the Web is being used. Characterization deals with: tasks, documents, access times (e.g., arrival rate of GET commands), file sizes, and other factors that will be discussed in detail in this dissertation.
Reference: [NGBS + 97] <author> H. F. Nielsen, J. Gettys, A. Baird-Smith, E. Prud'hommeaux, a. Wium Lie, and C. Lilley. </author> <title> Network performance effects of HTTP/1.1, CSS1, </title> <booktitle> and PNG. In Proc. </booktitle> <address> SIGCOMM97, Cannes, French Riviera, FRANCE, </address> <month> September </month> <year> 1997. </year> <note> ACM. </note>
Reference-contexts: This introduces the overhead of starting a new communication protocol. The HTTP protocol could be designed to utilize available functionalities in such protocols for faster delivery. Work to enhance the HTTP protocol is done by several groups; see for example Nielsen et. al. <ref> [NGBS + 97] </ref>. 2.2.4 Compression Compression works well with text documents but can be less effective with binary files especially if they are already compressed (such as JPEG or GIF).
Reference: [O'C95] <author> D. O'Callaghan. </author> <title> A central caching proxy server for WWW users at the University of Melbourne. </title> <booktitle> In First Australian World-wide Web Conference, </booktitle> <institution> University of Melbourne, Australia, </institution> <year> 1995. </year>
Reference-contexts: Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic <ref> [ASA + 95, WAS + 96, Smi94, O'C95] </ref>. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic.
Reference: [Pax97] <author> V. Paxson. </author> <title> Fast, approximate synthesis of fractional gaussian noise for generating self-similar network traffic. </title> <journal> In Computer Communications Review, </journal> <note> submitted for publication, </note> <year> 1997. </year>
Reference-contexts: Chapter 4 Modeling Proxy Web Traffic Using Fourier Analysis 4.1 Introduction Proper and correct models of network and WWW traffic are important for simulation studies and for understanding the nature of interactions that appear over the Internet and the Web <ref> [DJ91, PF94, Pax97] </ref>, as aids to planning network infrastructure. Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption [CB96, LTWW94, PF94, Pax97]. <p> Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption <ref> [CB96, LTWW94, PF94, Pax97] </ref>. Leland and Wilson [LW91] show that simulations using real network traffic produce different results from simulations of synthetic traffic generated using traditional approaches (that ignore long-range dependency in network traffic). <p> Leland et al. [LTWW94, WTSW97] describe a methodology for generating bursty and hence self-similar traffic by multiplexing a large number of on/off sources with heavy tailed period lengths. Paxson <ref> [Pax97] </ref> introduces "a fast Fourier transform method for synthesizing approximate self-similar sample paths for one type of self-similar process." However he warns that "One must use caution in assuming that traffic sources are well modeled using self-similar processes." In Section 6 of the same article he questions "even if network traffic <p> That is, we validate our analysis and modeling by showing our generated traffic is similar to the real traffic. This way we address the concern raised by Paxson in <ref> [Pax97] </ref>, section 6, which was quoted earlier. We use proxy workloads because proxies are the most attractive solutions for scaling information distribution and copyright management [Win97]. We encourage caching using proxy servers because it captures locality of reference within the user community. <p> We demonstrate the modeling approach with an example and we validate each step by comparing the generated data to the original. In Appendix A we show that the periodic behavior exists for the ethernet traffic used in an important Bellcore study <ref> [LTWW94, Pax97, WTSW97] </ref>, however, the complete modeling is beyond the scope of the work described here. As part of characterizing specific WWW activities we provided a characterization of our courseware digital library server and accesses to Web IRS.
Reference: [PB94] <author> J. E. Pitkow and K. Bharat. WEBVIZ: </author> <title> A tool for World Wide Web access log visualization. </title> <booktitle> In Proc. of the First International World-wide Web Conference, </booktitle> <address> Amsterdam, 1994. </address> <publisher> Elsevier. </publisher> <address> &lt;URL: </address> <note> http://www.elsevier.nl/cgi-bin/WWW94link/31/overview &gt;. Conclusions and Future Work 108 </note>
Reference: [PF94] <author> V. Paxson and S. Floyd. </author> <title> Wide-area traffic: The failure of Poisson modeling. </title> <booktitle> In Proc. SIGCOMM94, </booktitle> <pages> pages 257-268, </pages> <address> London, </address> <month> August </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Chapter 4 Modeling Proxy Web Traffic Using Fourier Analysis 4.1 Introduction Proper and correct models of network and WWW traffic are important for simulation studies and for understanding the nature of interactions that appear over the Internet and the Web <ref> [DJ91, PF94, Pax97] </ref>, as aids to planning network infrastructure. Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption [CB96, LTWW94, PF94, Pax97]. <p> Recent studies to understand and model network traffic challenge the traditional Poisson distribution assumption <ref> [CB96, LTWW94, PF94, Pax97] </ref>. Leland and Wilson [LW91] show that simulations using real network traffic produce different results from simulations of synthetic traffic generated using traditional approaches (that ignore long-range dependency in network traffic). <p> Note, however, that the steps to model the deterministic part also have been Modeling Proxy Web Traffic Using Fourier Analysis 40 tested successfully on other collected proxy workloads. Second, by examining the auto-correlation plots for Web traffic, we uncover and explain the long-range dependency noticed in the literature <ref> [PF94, AFAW97] </ref>. The auto-correlation function reveals periodic long-range dependency in the examined data. Tests over several workloads show that this periodicity is not arbitrary; it is common to all tested workloads and can be explained in terms of daily and weekly cyclic behavior of Web users.
Reference: [PH96] <author> Annabel Pollock and Andrew Hockley. </author> <title> What's wrong with internet searching. In Designing for the Web Empirical Studies. </title> <publisher> Microsoft, </publisher> <month> October </month> <year> 1996. </year> <note> &lt;URL: http://www.microsoft.com/usability/webconf.htm &gt;. </note>
Reference-contexts: The first one is the sensitivity of the search engine in finding useful information. The second one is how conservative or liberal the search engine is in determining which sites to include in the search results. The authors conclude that the current search systems exhibit poor performance. Pollock <ref> [PH96] </ref> demonstrates that user background does not affect the search process or results. The author demonstrates this through an experiment that compares naive and non-naive user searches. The author concludes that there are misconceptions and problems with Web searching due to the design of available search tools. <p> Tables 6.2 and 6.3 show that Web users do use Web IRS to locate information. However the degree of computer knowledge does not appear to predict the percentage of accesses that are queries, for our workloads. This is consistent with the findings in <ref> [PH96] </ref>. However, the task effect is very clear in two cases, the library and the high school (AUB). Table 6.4 shows the distribution of accesses to different Web IRS. One thing that is common between all workloads except AOL is that Yahoo is the top used IRS.
Reference: [Pit98] <author> J. E. Pitkow. </author> <title> Summary of WWW characterization. </title> <booktitle> In Proc. of the 7th Int. WWW Conf., </booktitle> <address> Brisbone, Australia, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: The work described in this dissertation was one of the major sources that helped in answering some of the questions raised by the W3C protocol design group <ref> [AFAW97, AFA97, ANF97, Pit98] </ref>. Braun and Claffy pointed out the difficulty of tracking Web statistics on a large scale | e.g., the entire Internet [BC94]. A more reasonable approach is to study Web statistics collected from representative Web objects which are responsible for initiating, responding to, and carrying Web transactions. <p> Crovella and Bestavros [CB96] showed that the Web traffic has characteristics that are consistent with self-similarity. They traced the reasons for Web self-similarity to the basic characteristics of information organization and retrieval. Finally, Pitkow <ref> [Pit98] </ref> comprehensively summarizes work done in the field of workload characterization. WWW Proxy Traffic Characterization with Application to Caching 15 Table 3.1: Workloads used in this study. <p> Peak and minimum usage hours can be identified for upgrades, price commercials, and other time dependent jobs. The results of this characterization were used by Motorola and the W3C who described our proxy characterization as a comprehensive study for proxies <ref> [Pit98] </ref>. The fact that Web traffic has a significant deterministic and cyclic component is very important for internet service providers.
Reference: [PM94] <author> V. N. Padmanabhan and J. C. Mogul. </author> <title> Improving HTTP latency. </title> <note> In Second World-wide Web Conference '94: Mosaic and the Web, 1994. http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/mogul/HTTPLatency.html. </note>
Reference-contexts: Consequently, the current Web architecture is vulnerable and lacks optimized interaction between applications and network protocols [BC94]. Performance and reliability are two major concerns for Web users. Although the Web enhances accessibility, users still demand faster response [WA97]. Performance problems can result from poor protocol design <ref> [PM94, Spe94] </ref>, or inadequate servers, clients, or network speed. In addition, the popularity of the Web created other unexpected problems. Scalability, latency, bandwidth, and disconnected operations [Get95] are some of the important issues that should be considered to make up for the rate of growth in Web usage.
Reference: [PR94] <author> J. E. Pitkow and M. M. Recker. </author> <title> A simple yet robust caching algorithm based on dynamic access patterns. </title> <booktitle> In Proc. 2nd Int. WWW Conf., </booktitle> <pages> pages 1039-1046, </pages> <address> Chicago, </address> <month> October </month> <year> 1994. </year>
Reference: [Rao90] <author> T. S. Rao. </author> <title> Analysis of nonlinear time series (and chaos) by bispectral methods. </title> <booktitle> In Workshop on Nonlinear Modeling and Forecasting, volume Studies in the Science of Complexity, </booktitle> <pages> pages 199-226, </pages> <address> Santa Fe, NM, </address> <month> September </month> <year> 1990. </year> <institution> Santa Fe Institute. </institution>
Reference: [RP94] <author> M. M. Recker and J. E. Pitkow. </author> <title> Predicting document access in large, multimedia repositories. </title> <type> Technical Report VU-GIT-94-35, </type> <institution> Graphics, Visualization, and Usability Center, Georgia Tech, </institution> <month> August </month> <year> 1994. </year> <note> &lt;URL: ftp://ftp.gvu.gatech.edu/pub/gvu/tech-reports/94-35a.ps.Z. </note>
Reference: [Sed94] <author> J. Sedayao. </author> <title> Mosaic will kill my network. In Second World-wide Web Conference '94: Mosaic and the Web, </title> <address> Chicago, IL, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: One of the earliest studies to characterize proxy traffic was done by Glassman [Gla94]. Characterization included parameters such as document popularity, cache misses, cache hits, and rate of change for Web pages. Sedayao <ref> [Sed94] </ref> studied and characterized the distribution of several parameters. Gwertzman and Seltzer [GS96] have used several server and proxy traffic sources to characterize MIME types and the average life span of a Web document.
Reference: [Smi94] <author> N. Smith. </author> <title> What can archives offer the World-wide Web. </title> <booktitle> In Proc. 1st World-wide Web Conference, </booktitle> <address> Geneva, </address> <month> May </month> <year> 1994. </year> <note> &lt;URL: http://www.hensa.ac.uk/www94&gt;. </note>
Reference-contexts: Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic <ref> [ASA + 95, WAS + 96, Smi94, O'C95] </ref>. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic.
Reference: [SN96] <author> Carsten Schlichting and Erik Nilsen. </author> <title> Signal detection analysis of WWW search engines. In Designing for the Web Empirical Studies. </title> <publisher> Microsoft, </publisher> <month> October </month> <year> 1996. </year> <note> &lt;URL: http://www.microsoft.com/usability/webconf.htm &gt;. </note>
Reference-contexts: Characterizing Users' Searches and Sessions 75 Table 6.1: Workloads used in this study Workload Period Number of (K) accesses Korea 9/2/95-9/26/95 1,682 Library (VT) 9/19/96-11/20/96 128 CS (VT) 10/9/96-11/10/96 92 AOL 11/96 (one day) 897 6.1.3 Related work In <ref> [SN96] </ref> the authors compare several WWW search engines by using five faculty members to search for certain subjects and then judge the relevance of the returned results. Evaluation of the search engines is done using "signal detection analysis." The authors provide two measures to evaluate search engines.
Reference: [Spe94] <author> S. E. Spero. </author> <title> Analysis of HTTP performance problems, </title> <month> July </month> <year> 1994. </year> <note> http://elanor.oit.unc.edu/http-prob.html. </note>
Reference-contexts: Consequently, the current Web architecture is vulnerable and lacks optimized interaction between applications and network protocols [BC94]. Performance and reliability are two major concerns for Web users. Although the Web enhances accessibility, users still demand faster response [WA97]. Performance problems can result from poor protocol design <ref> [PM94, Spe94] </ref>, or inadequate servers, clients, or network speed. In addition, the popularity of the Web created other unexpected problems. Scalability, latency, bandwidth, and disconnected operations [Get95] are some of the important issues that should be considered to make up for the rate of growth in Web usage. <p> The WWW Consortium (W3C) realized these issues and they have launched an effort to design a new protocol that will be able to support future demands 1 Chapter 1. Introduction 2 [Get95]. Spero <ref> [Spe94] </ref> did a careful analysis of the HTTP protocol and showed that it is not optimized for fast interactions.
Reference: [Sta97] <author> StatSci. </author> <note> S-PLUS Users's Manual, Version 3.2. </note> <institution> MathSoft Inc., </institution> <address> Seattle, Wash-ington, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: The files were analysed using perl scripts. The script extracts the times of access and then creates a time series of arrival rates per hour for each log file. The time series is further examined using Splus <ref> [Sta97] </ref> and Matlab [Mat97]. The workloads, except those from Korea and Boston, were considered when collecting the WWW LAN traffic and using Figure 2.1 and equations 2.1 and 2.3 we extract the proxy traffic. <p> CS b- Korea c- Library d- Engineering 4.4.2 Auto-correlation function The autocorrelation function R xx (t ) is a measure of correlation between x (t) and x (t + t ), where t is a time delay <ref> [NB95, BR94, Sta97] </ref>. For random signals, R xx (t ) decays to zero as t increases. However for a signal with a periodic component, R xx (t ) does not decay to zero Modeling Proxy Web Traffic Using Fourier Analysis 45 and is oscillatory.
Reference: [Ste94] <author> W. Richard Stevens. </author> <title> TCP/IP Illustrated, Volume I: The Protocols. </title> <publisher> Addison Wesley, 2725 Sand Hall Road, </publisher> <address> Menlo Park, CA 94025, </address> <year> 1994. </year> <note> Bibliography 109 </note>
Reference-contexts: and local server s T p;rs the set of Web transactions between the proxy p and remote servers rs Using the definitions in Table 2.1, T LAN = T c;p [ T c;rs [ T c;s [ T rc;s [ T p;rs (2.1) To capture this traffic we use tcpdump <ref> [Ste94] </ref> with filters that we have developed to extract relevant data [AW96a]. Server traffic To simplify the analysis we will split the server traffic into two sets, traffic that results from accesses by local clients, T c;s , and by remote clients, T rc;s .
Reference: [UoH96] <author> Open University and University of Hull. </author> <title> Evaluation methods and procedures for studying learners' use of media. </title> <note> http://www-iet.open.ac.uk/iet/PLUM/plum.html, 1996. </note>
Reference-contexts: It is useful here to discuss two methods that are used to evaluate multimedia educational material. The two methods are formative and summative evaluation <ref> [UoH96] </ref>. During the formative process, data is collected from users who are using the current system or material. Small numbers of students, sometimes working in pairs, are monitored and data is collected using surveys, video recordings, and/or logs. <p> For Web courseware designers the main objective of this evaluation is to make sure that students will get the full benefit from deploying the course material over the Web. In the summative evaluation, the success of the program is tested <ref> [UoH96] </ref>. The process of summative evaluation is done over a long period of time and it should include analysis of all the material presented. The results should give us an idea about the long term benefits for education that will result from using the new technology.
Reference: [VL92] <author> S. G. Vincent and A. M. </author> <title> Law. UniFit II: Total support for simulation input modeling. </title> <booktitle> In Proc. Winter Simulation Conference, </booktitle> <pages> pages 371-376, </pages> <address> Arlington, VA, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: This implies that the distribution might be a long-tail distribution. The quantile-quantile plot [Jai91] confirms that the distribution is a long-tail distribution from the right-hand side. We use a Weibull distribution to model the residual. To generate a synthetic set of data, we used the UnifitII software <ref> [VL92] </ref>. The parameters for the distribution were estimated from the data. We followed the modeling process used in the previous chapter to validate the model. In Figure 4.11 we show the original data, its Fourier spectrum, the newly generated or synthetic data, and its Fourier spectrum.
Reference: [WA97] <author> R. Wooster and M. Abrams. </author> <title> Proxy Caching that Estimates Page Load Delays. </title> <booktitle> In 6th International World-wide Web Conference, </booktitle> <address> Santa Clara, California, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Consequently, the current Web architecture is vulnerable and lacks optimized interaction between applications and network protocols [BC94]. Performance and reliability are two major concerns for Web users. Although the Web enhances accessibility, users still demand faster response <ref> [WA97] </ref>. Performance problems can result from poor protocol design [PM94, Spe94], or inadequate servers, clients, or network speed. In addition, the popularity of the Web created other unexpected problems.
Reference: [WAS + 96] <author> S. Williams, M. Abrams, C. R. Standridge, G. Abdulla, and E. A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In ACM SIGCOMM'96 Conference, </booktitle> <pages> pages 293-305, </pages> <institution> Stanford University, California, </institution> <month> August </month> <year> 1996. </year> <note> &lt;URL: http://ei.cs.vt.edu/~succeed/96sigcomm/96sigcomm.html&gt;. </note>
Reference-contexts: The idea behind delta encoding is to retrieve only the parts of the document which have changed [Wil97]. Using delta encoding over the Web was suggested in <ref> [WAS + 96] </ref>. The work reported by Mogul et al. [MDFK97] shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. <p> Examples of such studies are systems and simulations using server [AW96b, BC94, KMR95a, AKMS95] and proxy traffic <ref> [ASA + 95, WAS + 96, Smi94, O'C95] </ref>. In those studies the authors have shown that caching will reduce network traffic. There have been several studies to characterize client workloads [CB96, CBC95, Cun97] and server workloads [AW96b, BC96]. There are, however, fewer studies to fully characterize proxy traffic. <p> We ran all the workloads through a trace driven simulation of a proxy server with infinite cache <ref> [WAS + 96] </ref>. After examining Figures 3.2 and 3.3 we expect the DEC workloads to have the lowest hit rates due to their short duration and Table 3.16 confirms this result. <p> Table 7.1 lists the parameters and their values. We choose the values for pv, pa, and ps based on the results obtained from log files we collected and reported on <ref> [WAS + 96] </ref>. The value of U was calculated from the log files of a PC machine that is used by several students in a research lab, by dividing the number of hours the machine was used on average by 24.
Reference: [Wil97] <author> S. Williams. </author> <note> HTTP: Delta-Encoding Notes, January 1997. &lt;URL: http:- //ei.cs.vt.edu/ williams/DIFF/prelim.html&gt;. </note>
Reference-contexts: However, some documents might change because of a typo or a minor revision and retrieving the whole new document will be a waste of bandwidth when an older copy is at hand. The idea behind delta encoding is to retrieve only the parts of the document which have changed <ref> [Wil97] </ref>. Using delta encoding over the Web was suggested in [WAS + 96]. The work reported by Mogul et al. [MDFK97] shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. <p> The work reported by Mogul et al. [MDFK97] shows that delta encoding can provide remarkable improvements in response size and delay for certain types of Web documents. The research in this field triggered recommendations to extend the HTTP protocol to include delta encoding in its future versions <ref> [MDFK97, Wil97, HP97] </ref>. 2.3 Web traffic, data collection and analysis Characterizing interactions and kinds of tasks that the Web is used for is an essential step to help design a better protocol.
Reference: [Win97] <author> T. Winograd. </author> <title> The proxy is where it's at. </title> <type> Technical Report CS-TN-97-51, </type> <institution> Computer Sci. Dept., Stanford Univ., Stanford Univ., </institution> <address> CA., </address> <month> February </month> <year> 1997. </year>
Reference-contexts: In our early work we focused on proxies since they provide a good medium for caching, filtering information, payment methods, and copyright management <ref> [Win97] </ref>. We collected data from our environment over a period of over two years. We also collected data from other sources such as schools, information service providers, and commercial sites. Sampling time ranges from days to years. <p> This way we address the concern raised by Paxson in [Pax97], section 6, which was quoted earlier. We use proxy workloads because proxies are the most attractive solutions for scaling information distribution and copyright management <ref> [Win97] </ref>. We encourage caching using proxy servers because it captures locality of reference within the user community.
Reference: [WTSW97] <author> W. Willinger, M. S. Taqqu, R. Sherman, and D. V. Wilson. </author> <title> Self-similarity through high-variability: Statistical analysis of ethernet lan traffic at the source level. </title> <journal> IEEE Trans. on Networking, </journal> <volume> 5 </volume> <pages> 71-86, </pages> <year> 1997. </year>
Reference-contexts: New methods and approaches that capture the characteristics of Web traffic should be developed. There are several efforts to define methods to generate self-similar traffic for network simu 38 Modeling Proxy Web Traffic Using Fourier Analysis 39 lation purposes. Leland et al. <ref> [LTWW94, WTSW97] </ref> describe a methodology for generating bursty and hence self-similar traffic by multiplexing a large number of on/off sources with heavy tailed period lengths. <p> Leland et al. <ref> [LTWW94, WTSW97] </ref> show how to model this data with a self-similar process. Beran [Ber94] lists common features that distinguish a long-range dependent process, including both qualitative and quantitative features, and provides additional details on this subject. <p> We demonstrate the modeling approach with an example and we validate each step by comparing the generated data to the original. In Appendix A we show that the periodic behavior exists for the ethernet traffic used in an important Bellcore study <ref> [LTWW94, Pax97, WTSW97] </ref>, however, the complete modeling is beyond the scope of the work described here. As part of characterizing specific WWW activities we provided a characterization of our courseware digital library server and accesses to Web IRS.
References-found: 80

