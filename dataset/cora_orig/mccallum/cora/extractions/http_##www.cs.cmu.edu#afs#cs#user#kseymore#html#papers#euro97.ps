URL: http://www.cs.cmu.edu/afs/cs/user/kseymore/html/papers/euro97.ps
Refering-URL: http://gs213.sp.cs.cmu.edu/prog/findhome/?query=Roni.Rosenfeld
Root-URL: 
Email: kseymore@cs.cmu.edu, roni@cs.cmu.edu  
Title: USING STORY TOPICS FOR LANGUAGE MODEL ADAPTATION  
Author: Kristie Seymore and Ronald Rosenfeld 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: The subject matter of any conversation or document can typically be described as some combination of elemental topics. We have developed a language model adaptation scheme that takes a piece of text, chooses the most similar topic clusters from a set of over 5000 elemental topics, and uses topic specific language models built from the topic clusters to rescore N-best lists. We are able to achieve a 15% reduction in perplexity and a small improvement in WER by using this adaptation. We also investigate the use of a topic tree, where the amount of training data for a specific topic can be judiciously increased in cases where the elemental topic cluster has too few word tokens to build a reliably smoothed and representative language model. Our system is able to fine-tune topic adaptation by interpolating models chosen from thousands of topics, allowing for adaptation to unique, previously unseen combinations of subjects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Seymore, S. Chen, M. Eskenazi and R. Rosenfeld, </author> <title> Language and Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation, </title> <booktitle> Proc. of the 1997 ARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Previously unseen combinations of topics occur frequently in domains such as Broadcast News, where current events dictate the contents of each article. For details of our earlier work in topic adaptation, see <ref> [1] </ref>. 2. <p> The hypothesis is then reevaluated according to the language scores of the interpolated language models. Even when the word error rate of the decoder hypothesis is significant, topic detection will still perform reasonably well <ref> [1] </ref>. As long as the word errors in the hypothesis are not significantly topic-correlated, the correct content words in the hypothesis will provide enough weight for the selection of appropriate clusters. 3. EXPERIMENTS The training data used in these experiments is the Broadcast News corpus obtained from Primary Source Media. <p> In all cases, twenty leaf clusters were chosen per story. Rescoring consisted of using the original acoustic score, the new language model score, and a word insertion penalty. Filled pauses were predicted from manually set unigram probabilities <ref> [1] </ref>. For the development set, the first-pass WER with no rescoring was 40.2%. The lowest N-best WER, found by using the reference transcripts to choose the N-best hypotheses with the lowest error, was 34.6%. The lowest N-best WER represents an upper bound on the performance of N-best rescoring. <p> On both the development and evaluation sets, using a Kneser-Ney smoothed general trigram model to rescore results in a lower WER than the topic models <ref> [1] </ref>. A Kneser-Ney model results in a WER of 39.4% on the development set and 34.9% on the evaluation set. Future work in topic adaptation must include better smoothing techniques for models built from small amounts of training data. 4.
Reference: [2] <author> R. Iyer, M. Ostendorf, </author> <title> Modeling Long Distance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models, </title> <booktitle> Proc. ICSLP, </booktitle> <volume> vol. 1, </volume> <year> 1996, </year> <pages> pp. 236-239. </pages>
Reference-contexts: Each cluster is then a candidate to be used in future adaptation. Topic trees can be built by treating the topic clusters as leaves and iteratively merging the topics together to form a tree. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [2, 3] </ref>. In these cases, training data was partitioned into a relatively small set of topic clusters, which was used for adaptation.
Reference: [3] <author> P. Clarkson, A. Robinson, </author> <title> Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache, </title> <booktitle> Proc. ICASSP, </booktitle> <volume> vol. 2, </volume> <year> 1997, </year> <pages> pp. 799-802. </pages>
Reference-contexts: Each cluster is then a candidate to be used in future adaptation. Topic trees can be built by treating the topic clusters as leaves and iteratively merging the topics together to form a tree. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [2, 3] </ref>. In these cases, training data was partitioned into a relatively small set of topic clusters, which was used for adaptation.
Reference: [4] <author> B. Carlson, </author> <title> Unsupervised Topic Clustering of Switchboard Speech Messages, </title> <booktitle> Proc. ICASSP , 1996, </booktitle> <pages> pp. 315-318. </pages>
Reference-contexts: Language models built at various nodes along the active paths can be combined to best model the current document. The use of topic trees has also been explored in the Switchboard domain by Carlson <ref> [4] </ref>. Automatic topic clustering does not always result in optimal clustering decisions. We are investigating semiautomatic methods, where the system asks for cues whenever its confidence in its clustering decision is weak.
Reference: [5] <author> G. Salton, </author> <title> Developments in Automatic Text Retrieval, </title> <journal> Science, </journal> <volume> Vol. 253, </volume> <year> 1991, </year> <pages> pp. 974-980. </pages>
Reference-contexts: Topic Detection Once we have a set of topic clusters, we can use topic detection to determine the most topicsimilar clusters to a new piece of text. We consider two topic detection methods: the TFIDF classifier and the nave Bayes classifier. The TFIDF measure <ref> [5] </ref> assigns a weight to each unique word in a document representing how topic-specific that word is to its document or cluster.
Reference: [6] <author> K. Seymore, R. Rosenfeld, </author> <title> Large-scale Topic Detection and Language Model Adaptation, </title> <institution> Carnegie Mellon University Technical Report, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: The resulting similarity measure is a value between zero and one, with zero indicating no topic correlation, and one meaning an identical match. A Nave Bayes classifier calculates the probability of a topic given the words in a new document. In comparing these two classifiers <ref> [6] </ref>, we found that the nave Bayes classifier consistently outperforms the TFIDF classifier in both precision and recall on a Broadcast News test set where the manually assigned keywords indicate the correct classifications. See [6] for details. 2.3 Model Interpolation In the speech recognition paradigm, each time a new story is <p> In comparing these two classifiers <ref> [6] </ref>, we found that the nave Bayes classifier consistently outperforms the TFIDF classifier in both precision and recall on a Broadcast News test set where the manually assigned keywords indicate the correct classifications. See [6] for details. 2.3 Model Interpolation In the speech recognition paradigm, each time a new story is decoded an initial hypothesis transcription is produced. We then feed the hypothesis transcription to the classifier, which chooses the most similar leaf clusters. <p> Furthermore, the semantic landscape of Broadcast News has been mapped out in two different topic trees. Future work may find these structures helpful in more complex topic detection and adaptation systems. For a more detailed presentation of this work, see <ref> [6] </ref>. 5. ACKNOWLEDGEMENTS We would like to thank Richard Schwartz, Yiming Yang, Stanley Chen and Bin Zhou for their contributions and help with this work. This research was sponsored by the Department of the Navy, Naval Research Laboratory under Grant No.
Reference: [7] <editor> P. Placeway et al., </editor> <booktitle> The 1996 Hub-4 Sphinx-3 System, Proc. of the 1997 ARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: Perplexity Reduction In order to determine the best way to interpolate topic specific language models, we varied the number of topic specific models and measured development set perplexity. First, topic detection was run using the TFIDF and nave Bayes classifiers on errorful first-pass Sphinx-3 <ref> [7] </ref> recognition hypotheses from each of the 57 stories from the development set. The word error rate (WER) of the development set was 40%. A 51k vocabulary general trigram backoff language model was built from the Linguistic Data Consortium's (LDC) release of the Broadcast News corpus.
Reference: [8] <author> S. Katz, </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing , vol. </journal> <volume> ASSP-35, no. 3, </volume> <month> March </month> <year> 1987, </year> <pages> pp. 400-401. </pages>
Reference-contexts: The word error rate (WER) of the development set was 40%. A 51k vocabulary general trigram backoff language model was built from the Linguistic Data Consortium's (LDC) release of the Broadcast News corpus. Good-Turing discounted trigram backoff language models <ref> [8] </ref> were built from each of the 20 most similar topic clusters chosen by the classifiers for each development set story.
References-found: 8

