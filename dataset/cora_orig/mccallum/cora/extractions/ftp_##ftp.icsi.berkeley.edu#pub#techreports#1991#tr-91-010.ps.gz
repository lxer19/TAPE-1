URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-010.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: How Receptive Field Parameters Affect Neural Learning  
Author: Bartlett W. Mel Stephen M. Omohundro 
Address: Pasadena, CA 91125  1947 Center St., Suite 600 Berkeley, CA 94704  
Affiliation: CNS Program Caltech, 216-76  ICSI  
Abstract: We identify the three principle factors affecting the performance of learning by networks with localized units: unit noise, sample density, and the structure of the target function. We then analyze the effect of unit receptive field parameters on these factors and use this analysis to propose a new learning algorithm which dynamically alters receptive field properties during learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baldi, P. & Heiligengerg, W. </author> <title> How sensory maps could enhance resolution through ordered arrangements of broadly tuned receptors. </title> <journal> Biol. Cybern., 1988, </journal> <volume> 59, </volume> <pages> 313-318. </pages>
Reference-contexts: Several workers have analyzed the effect of receptive field size, shape, and overlap on representation accuracy: <ref> (Baldi, 1988) </ref>, (Ballard, 1987), and (Hinton, 1986). This paper investigates the additional interactions introduced by the task of function learning. <p> This type of error falls off as the 5th power of d, where d is the spacing of the receptive fields. In a similar result, <ref> (Baldi and Heilegenberg, 1988) </ref> show that approximations to both linear and quadratic functions improve exponentially fast with increasing density of Gaussian receptive fields. 2.2.2 MISMATCH OF SPATIAL SCALE A more general source of error in fitting target functions occurs when receptive fields are either too broad or too widely spaced relative
Reference: <author> Ballard, D.H. </author> <title> Interpolation coding: a representation for numbers in neural models. </title> <journal> Biol. Cybern., 1987, </journal> <volume> 57, </volume> <pages> 389-402. </pages>
Reference-contexts: Several workers have analyzed the effect of receptive field size, shape, and overlap on representation accuracy: (Baldi, 1988), <ref> (Ballard, 1987) </ref>, and (Hinton, 1986). This paper investigates the additional interactions introduced by the task of function learning.
Reference: <author> Broomhead, D.S. & Lowe, D. </author> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, 1988, </journal> <volume> 2, </volume> <pages> 321-355. </pages>
Reference-contexts: We will see that the structure of the function being learned may also be advantageously taken into account. Function learning using radial basis functions (RBF's) is currently a popular technique <ref> (Broomhead and Lowe, 1988) </ref> and serves as an adequate framework for our discussion. Because we are interested in constraints on biological systems, we must explictly consider the effects of unit noise. The goal is to choose the layout of receptive fields so as to minimize average performance error.
Reference: <author> Hinton, G.E. </author> <title> (1986) Distributed representations. </title> <booktitle> In Parallel distributed processing: explorations in the microstructure of cognition, </booktitle> <volume> vol. 1, </volume> <editor> D.E. Rumelhart, </editor> <publisher> J.L. </publisher>
Reference-contexts: Several workers have analyzed the effect of receptive field size, shape, and overlap on representation accuracy: (Baldi, 1988), (Ballard, 1987), and <ref> (Hinton, 1986) </ref>. This paper investigates the additional interactions introduced by the task of function learning. Previous studies which have considered learning have for the most part restricted attention to the use of the input probability distribution to determine receptive field layout (Kohonen, 1984) and (Moody and Darken, 1989).
Reference: <editor> McClelland, (Eds.), </editor> <publisher> Bradford, </publisher> <address> Cambridge. </address>
Reference: <author> Kohonen, T. </author> <title> Self organization and associative memory. </title> <publisher> Springer-Verlag: </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: This paper investigates the additional interactions introduced by the task of function learning. Previous studies which have considered learning have for the most part restricted attention to the use of the input probability distribution to determine receptive field layout <ref> (Kohonen, 1984) </ref> and (Moody and Darken, 1989). We will see that the structure of the function being learned may also be advantageously taken into account. Function learning using radial basis functions (RBF's) is currently a popular technique (Broomhead and Lowe, 1988) and serves as an adequate framework for our discussion.
Reference: <author> MacKay, D. Hyperacuity and coarse-coding. </author> <note> In preparation. </note>
Reference: <author> Moody, J. & Darken, C. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation, 1989, </booktitle> <volume> 1, </volume> <pages> 281-294. </pages>
Reference-contexts: This paper investigates the additional interactions introduced by the task of function learning. Previous studies which have considered learning have for the most part restricted attention to the use of the input probability distribution to determine receptive field layout (Kohonen, 1984) and <ref> (Moody and Darken, 1989) </ref>. We will see that the structure of the function being learned may also be advantageously taken into account. Function learning using radial basis functions (RBF's) is currently a popular technique (Broomhead and Lowe, 1988) and serves as an adequate framework for our discussion.
References-found: 8

