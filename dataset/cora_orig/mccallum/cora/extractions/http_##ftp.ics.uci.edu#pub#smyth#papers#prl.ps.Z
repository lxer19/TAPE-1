URL: http://ftp.ics.uci.edu/pub/smyth/papers/prl.ps.Z
Refering-URL: http://www.ics.uci.edu/~smyth/
Root-URL: 
Email: smyth@ics.uci.edu  
Title: Belief Networks, Hidden Markov Models, and Markov Random Fields: a Unifying View  
Author: Padhraic Smyth 
Date: March 20, 1998  
Address: CA 92697-3425.  
Affiliation: Information and Computer Science Department University of California, Irvine  
Abstract: The use of graphs to represent independence structure in multivariate probability models has been pursued in a relatively independent fashion across a wide variety of research disciplines since the beginning of this century. This paper provides a brief overview of the current status of such research with particular attention to recent developments which have served to unify such seemingly disparate topics as probabilistic expert systems, statistical physics, image analysis, genetics, decoding of error-correcting codes, Kalman filters, and speech recognition with Markov models.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berzuini, C. and Larizza, C. </author> <year> (1996). </year> <title> A unified approach for modeling longitudinal and failure time data, with application in medical monitoring. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(2), </volume> <pages> 109-123. </pages>
Reference: <author> Besag, J. </author> <year> (1986). </year> <title> On the statistical analysis of dirty pictures. </title> <journal> J. R. Statist. Soc. B, </journal> <volume> 48(3), </volume> <pages> 259-302. </pages>
Reference-contexts: For example, the use of "mean-field" approximations are motivated by physical arguments on the nature of cumulative long-range particle interactions, 8 and the popular Iterative Conditional Modes algorithm for image analysis relies on greedy local maximization of the posterior probability of the pixel labels given the observed data <ref> (Besag, 1986) </ref>. It is also worth noting that many directed graphical models of practical interest have sufficiently dense structure to not admit efficient exact solutions.
Reference: <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 159-225. </pages> <note> 9 Buntine, </note> <author> W. </author> <year> (1996). </year> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 8(2), </volume> <pages> 195-210. </pages>
Reference-contexts: In a related context, learning of multivariate regression and classification models such as neural networks, can also be treated profitably within a graphical model framework <ref> (Buntine, 1994) </ref>. 4.2 Hidden Markov models as ADGs The well-known "first-order" hidden Markov model (HMM) (as widely used in speech recognition) is a particularly simple probability model and has a direct representation as a graphical model (Figure 1).
Reference: <author> Dawid, A. P. </author> <year> (1992). </year> <title> Applications of a general propagation algorithm for probabilistic expert systems. </title> <journal> Statistics and Computing, </journal> <volume> 2, </volume> <pages> 25-36. </pages>
Reference: <author> Geman, S. and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 6, </volume> <pages> 721-741. </pages>
Reference-contexts: This work, linked with related ideas in statistics (Isham, 1981), motivated a whole sub-discipline of image analysis based on Markov random fields <ref> (Geman and Geman, 1984) </ref> and related work in neural network modeling using Boltzmann machines (Hinton and Sejnowski, 1986). The fact that all of these models are closely related is relatively well-known although not always explicitly referred to in the literature.
Reference: <author> Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. </author> <title> (editors) (1996). Markov Chain Monte Carlo in Practice, </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data, </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 197-244. </pages>
Reference-contexts: Belief networks have gained widespread acceptance and application within AI in areas such as diagnosis, planning, robotics, computer vision, and so forth <ref> (see, for example, Heckerman, Wellman, and Mamdani, 1995) </ref>. From an AI perspective the well-defined semantics of an ADG, where each node is a direct descendant of its "causal" parents, provide a useful and practical language for knowledge elicitation.
Reference: <author> Heckerman, D., Wellman, M. and Mamdani A. </author> <title> (editors) (1995). </title> <journal> Communications of the ACM: Special Issue on Uncertainty in AI, </journal> <volume> 38(3). </volume>
Reference-contexts: Belief networks have gained widespread acceptance and application within AI in areas such as diagnosis, planning, robotics, computer vision, and so forth <ref> (see, for example, Heckerman, Wellman, and Mamdani, 1995) </ref>. From an AI perspective the well-defined semantics of an ADG, where each node is a direct descendant of its "causal" parents, provide a useful and practical language for knowledge elicitation.
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> Rumelhart D. E., McClelland J. L., and the PDP Research Group, editors, </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <address> v.1, ch. </address> <month> 7. </month>
Reference-contexts: This work, linked with related ideas in statistics (Isham, 1981), motivated a whole sub-discipline of image analysis based on Markov random fields (Geman and Geman, 1984) and related work in neural network modeling using Boltzmann machines <ref> (Hinton and Sejnowski, 1986) </ref>. The fact that all of these models are closely related is relatively well-known although not always explicitly referred to in the literature. Less well known are recent realizations that the "extended family" of graphical models also encompasses some very well-known and widely used techniques in engineering.
Reference: <author> Jensen, F. V. </author> <year> (1996). </year> <title> An Introduction to Bayesian Networks, </title> <publisher> University College London Press, London. </publisher>
Reference-contexts: ADGs can be reduced to an equivalent UG structure in a relatively 4 straightforward manner, although the corresponding UG may be less efficient at representing the same probability distribution as the original ADG, i.e., have more edges. The "canonical" graphical form for computation is the "clique tree" <ref> (Jensen, 1996) </ref>, which is constructed from the UG representation via triangulation. Inference simply consists of local message passing in the clique tree. The "clique tree inference algorithms" (Jensen, 1996) are quite general and subsume the earlier more specialized inference algorithms such as those proposed by Pearl (1988). <p> The "canonical" graphical form for computation is the "clique tree" <ref> (Jensen, 1996) </ref>, which is constructed from the UG representation via triangulation. Inference simply consists of local message passing in the clique tree. The "clique tree inference algorithms" (Jensen, 1996) are quite general and subsume the earlier more specialized inference algorithms such as those proposed by Pearl (1988).
Reference: <author> Jensen, F. V., Lauritzen, S. L. and Olesen, K. G. </author> <year> (1990). </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly, </journal> <volume> 4, </volume> <pages> 269-282. </pages>
Reference: <author> Kenley, C. R. </author> <year> (1986). </year> <title> Influence Diagram Models with Continuous Variables. </title> <type> PhD. Thesis Dissera-tion, </type> <institution> Department of Engineering-Economic Systems, Stanford University. </institution>
Reference-contexts: Thus, it should not be surprising to the reader at this point to learn that such models can also be represented within the graphical model family, again as ADGs due to the causal nature of temporal processes <ref> (Kenley, 1986) </ref>.
Reference: <author> Kindermann, R., and Snell, J. L. </author> <year> (1980). </year> <title> Markov Random Fields and their Applications, </title> <publisher> American Mathematical Society. </publisher>
Reference: <author> Kschischang, F. R. and Frey, B. J. </author> <title> (in press). Iteratve decoding of compound codes by probability propagation in graphical models. </title> <journal> IEEE Journal on Selected Areas in Communications. </journal>
Reference: <author> Levy, B. C., Benveniste, A., Nikoukhah, R. </author> <year> (1996). </year> <title> High-level primitives for recursive maximum likelihood estimation. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 41(8), </volume> <pages> 1125-1145. </pages> <note> 10 Lauritzen, </note> <author> S. L. </author> <year> (1996). </year> <title> Graphical Models, </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: Specifically, hidden Markov models (including the forward-backward algorithm) as used in speech recognition can be viewed as special cases of graphical models (Smyth, Heckerman and Jordan, 1997), Kalman filtering equations and models can be profitably viewed from a graphical model context <ref> (Levy, Benveniste, and Nikoukhah, 1996) </ref>, and a variety of well-known algorithms for decoding error-correcting codes turn out to be special cases of more general graphical model algorithms (MacKay, McEliece, and Cheng, in press ). The purpose of this paper is to briefly review some of these connections. <p> More recently there have been significant extensions which have proceeded by showing the direct equivalence of the standard Kalman prediction/smoothing equations to graphical model inference algorithms, and by then exploiting the generality of graphical models to propose novel extensions to standard Kalman filters within a unified framework <ref> (Levy, Benveniste, and Nikoukah, 1996) </ref>. In the context of more general time-series modeling, graphical models can also play a useful role.
Reference: <author> Lauritzen, S. L. and Spiegelhalter D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. B, </journal> <volume> 50, </volume> <pages> 157-224. </pages>
Reference-contexts: Computational inference methods are often based on undirected representations <ref> (Lauritzen and Spiegelhalter, 1988) </ref>. ADGs can be reduced to an equivalent UG structure in a relatively 4 straightforward manner, although the corresponding UG may be less efficient at representing the same probability distribution as the original ADG, i.e., have more edges.
Reference: <author> MacKay, D. J. C, McEliece, R. J., and Cheng, J-F. </author> <title> (in press). Turbo-decoding as an instance of Pearl's belief propagation algorithm. </title> <journal> IEEE Journal on Selected Areas in Communications. </journal>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Rabiner, L. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <pages> 257-285. </pages>
Reference-contexts: Speech recognition systems take advantage of the fact that there exist efficient algorithms (linear in the length of the Markov chain) for solving the inference and MAP problems associated with recognition. Inference is solved by the forward-backward algorithm and the MAP problem is handled by the Viterbi algorithm <ref> (Rabiner, 1989) </ref>. Since we can represent a HMM as a simple graphical model, it follows that the inference and MAP problems can be solved by the standard algorithms developed by Pearl (1988), Lauritzen and Spiegelhalter (1988), and subsequent refinements (Jensen, Lauritzen and Olesen (1990).
Reference: <author> Saul, L. K., and Jordan, M. I. </author> <year> (1996). </year> <title> Exploiting tractable substructures in intractable networks. </title>
Reference: <editor> In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Smyth, P., Heckerman, D., Jordan, M. I. </author> <year> (1997). </year> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 9(2), </volume> <pages> 227-269. </pages>
Reference-contexts: Less well known are recent realizations that the "extended family" of graphical models also encompasses some very well-known and widely used techniques in engineering. Specifically, hidden Markov models (including the forward-backward algorithm) as used in speech recognition can be viewed as special cases of graphical models <ref> (Smyth, Heckerman and Jordan, 1997) </ref>, Kalman filtering equations and models can be profitably viewed from a graphical model context (Levy, Benveniste, and Nikoukhah, 1996), and a variety of well-known algorithms for decoding error-correcting codes turn out to be special cases of more general graphical model algorithms (MacKay, McEliece, and Cheng, in
Reference: <author> Whittaker, J. </author> <year> (1990). </year> <title> Graphical Models in Applied Multivariate Statistics, </title> <publisher> John Wiley and Sons: </publisher> <address> Chichester, UK. </address>
Reference: <author> Wright, S. </author> <year> (1921). </year> <title> Correlation and causation. </title> <journal> Journal of Agricultural Research, </journal> <volume> 20 </volume> <pages> 557-85. 11 </pages>
References-found: 24

