URL: http://theory.lcs.mit.edu/~rivest/BergmanRivest-PickingTheBestExpertFromASequence.ps
Refering-URL: http://theory.lcs.mit.edu/~rivest/publications.html
Root-URL: 
Title: Picking the Best Expert from a Sequence  
Author: Ruth Bergman Ronald L. Rivest 
Date: March 17, 1995  
Address: Cambridge, MA 02139  Cambridge, MA 02139  
Affiliation: Laboratory for Artificial Intelligence Massachusetts Institute of Technology  Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: We examine the problem of finding a good expert from a sequence of experts. Each expert has an "error rate"; we wish to find an expert with a low error rate. However, each expert's error rate is unknown and can only be estimated by a sequence of experimental trials. Moreover, the distribution of error rates is also unknown. Given a bound on the total number of trials, there is thus a tradeoff between the number of experts examined and the accuracy of estimating their error rates. We present a new expert-finding algorithm and prove an upper bound on the expected error rate of the expert found. A second approach, based on the sequential ratio test, gives another expert-finding algorithm that is not provably better but which performs better in our empirical studies.
Abstract-found: 1
Intro-found: 1
Reference: <author> Drescher, G. L. </author> <year> (1989), </year> <title> Made-Up Minds: A Constructivist Approach to Artificial Intelligence, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference-contexts: A rule has the form precondition ! action ! postcondition with the meaning that if the preconditions are true in the current state and the action is taken, then the postcondition will be true in the next state. These are predictive rules as in <ref> (Drescher 1989) </ref>, as opposed to the prescriptive rules in reinforcement learning (Watkins 1989, Holland 1985) or operators in Soar (Laird, Newell & Rosenbloom 1978). An algorithm to learn rules uses triples of previous state, S, action, A, and current state to learn.
Reference: <author> Holland, J. H. </author> <year> (1985), </year> <title> Properties of the bucket brigade algorithm, </title> <booktitle> in `First International Conference on Genetic Algorithms and Their Applications', </booktitle> <address> Pittsburg, PA, </address> <pages> pp. 1-7. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> Technical Report TR-90-04, </type> <institution> Teleos Re search. </institution>
Reference: <author> Laird, J. E., Newell, A. & Rosenbloom, P. S. </author> <year> (1978), </year> <title> `SOAR: An Architecture for General Intelli gence', </title> <booktitle> Artificial Intelligence 33, </booktitle> <pages> 1-64. </pages>
Reference-contexts: These are predictive rules as in (Drescher 1989), as opposed to the prescriptive rules in reinforcement learning (Watkins 1989, Holland 1985) or operators in Soar <ref> (Laird, Newell & Rosenbloom 1978) </ref>. An algorithm to learn rules uses triples of previous state, S, action, A, and current state to learn. It may isolate a postcondition, P , in the current state, and generate preconditions that explain the postcondition from the previous state and action.
Reference: <author> Rice, J. A. </author> <year> (1988), </year> <title> Mathematical Statistics and Data Analysis, </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, CA. </address>
Reference-contexts: The ratio test is reject if f m (p 0 + q 2m )m accept otherwise 1 We choose the ratio test since it has the most power, i.e., for a given ff, i.e. it gives the least fi (probability of accepting when the hypothesis H 0 is wrong (see <ref> (Rice 1988) </ref>.) 3 3.2 An Algorithm for Finding a Good Expert We know how to test if a coin is good given a threshold defining a good error rate, but when we do not know the error-rate distribution we can not estimate the lowest error rate b t that we can
Reference: <author> Sutton, R. S. </author> <year> (1990), </year> <title> First Results with DYNA, an Integrated Architecture for Learning, Planning, and Reacting, </title> <booktitle> in `Proceedings, AAAI-90', </booktitle> <address> Cambridge, Massachusetts. </address>
Reference: <author> Sutton, R. S. </author> <year> (1991), </year> <title> Reinforcement Learning Architectures for Animats, </title> <booktitle> in `First International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Wald, A. </author> <year> (1947), </year> <title> Sequential Analysis, </title> <publisher> John Wiley & Sons, Inc., Chapman & Hall, LTD., </publisher> <address> London. 8 Watkins, C. </address> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> King's College. </institution> <month> 9 </month>
Reference-contexts: Section 3 contains the main result of this paper: our algorithm finds an expert whose error rate is close to the error rate of the best expert you can expect to find given the same resources. Section 4 presents a similar expert-finding algorithm that uses the sequential ratio test <ref> (Wald 1947) </ref> rather than the ratio test. Wald (1947) shows empirically that the sequential ratio test is twice as efficient as the ratio test when the test objects are normally distributed.
References-found: 8

