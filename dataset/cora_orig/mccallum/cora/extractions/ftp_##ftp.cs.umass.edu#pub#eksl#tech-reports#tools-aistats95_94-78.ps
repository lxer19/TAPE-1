URL: ftp://ftp.cs.umass.edu/pub/eksl/tech-reports/tools-aistats95_94-78.ps
Refering-URL: http://www.cs.washington.edu/homes/carlson/research.html
Root-URL: 
Title: Tools for Empirically Analyzing AI Programs  
Author: Scott D. Anderson, David M. Hart, David L. Westbrook, Paul R. Cohen and Adam Carlson 
Address: Box 34610  Amherst, MA 01003-4610  
Affiliation: Computer Science  Experimental Knowledge Systems Laboratory Computer Science Department,  Lederle Graduate Research Center University of Massachusetts  
Note: In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, pp. 35-41.  This research is supported by ARPA/Rome Laboratory under contract #'s F30602-91-C-0076 and F30602-93-C 0100.  
Pubnum: Technical Report 94-78  
Abstract: The paper describes two separate but synergistic tools for running experiments on large Lisp systems such as Artificial Intelligence planning systems, by which we mean systems that produce plans and execute them in some kind of simulator. The first tool, called Clip (Common Lisp Instrumentation Package), allows the researcher to define and run experiments, including experimental conditions (parameter values of the planner or simulator) and data to be collected. The data are written out to data files that can be analyzed by statistics software. The second tool, called Clasp (Common Lisp Analytical Statistics Package), allows the researcher to analyze data from experiments by using graphics, statistical tests, and various kinds of data manipulation. Clasp has a graphical user interface (using Clim, the Common Lisp Interface Manager, Version 2.0) and also allows data to be directly processed by Lisp functions. Clip and Clasp form the foundation of a larger set of specialized tools we are building for the empirical analysis of AI programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert St. Amant and Paul R. Cohen. </author> <title> Preliminary system design for an EDA assistant. </title> <booktitle> In Proceedings of the Fifth International Workshop on AI and Statistics, </booktitle> <year> 1995. </year>
Reference-contexts: We are building a module that assists the user in this effort by employing EDA techniques <ref> [1] </ref>. These techniques can partition data to distinguish different modes of behavior and generate functional descriptions of interactions between factors. Through detailed exploration of experimental data the user can gain a more complete picture of system behavior. <p> We envision a new generation of statistical software in which knowledge and heuristics will guide the application of exploratory data analysis procedures. We are currently at work on such a system which we call Aide, automated intelligent data exploration <ref> [1] </ref>. 5 Related Work An alternative to Clip is the Meters system, developed by Bolt, Beranek and Newman, Inc., for use in the ARPA/RL Planning Initiative's Common Prototyping Environment [4]. Meters is particularly useful for collecting and filtering time-series data from distributed systems.
Reference: [2] <author> Scott D. Anderson, Adam Carlson, David L. Westbrook, David M. Hart, and Paul R. Co-hen. Clasp/Clip: </author> <title> Common Lisp Analytical Statistics Package/Common Lisp Instrumentation Package. </title> <type> Technical Report 93-55, </type> <institution> University of Massachusetts at Amherst, Computer Science Department, </institution> <year> 1993. </year>
Reference-contexts: For example, you might want to collect data every day of the simulation, with the average being 2 This article is no substitute for the Clip/Clasp manual <ref> [2] </ref>, where everything is rigorously explained. 3 This requirement may be lifted in future versions of Clip, but the impact is minor. Most multi-threaded Lisps provide a process-wait function, which can be used to make the simulator seem like a single piece of code. 1 written to the data file. <p> Clip also lets you run only part of the experiment, which facilitates breaking the experiment into parts to run on different machines. These facilities are all explained at length in the Clip/Clasp documentation <ref> [2] </ref>. 3 Data Analysis The idea of Clasp began when we wanted to run a t-test on some experiment data without having to write out the data to a file in some tab-delimited format, move the code to another machine, run a statistics program, and load the data.
Reference: [3] <author> Scott D. Anderson, Adam Carlson, David L. Westbrook, David M. Hart, and Paul R. Co-hen. </author> <title> Tools for experiments in planning. </title> <booktitle> In Proceedings of the Sixth International Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 615-623, </pages> <address> Los Alamitos, CA, 1994. </address> <publisher> IEEE, IEEE Computer Society Press. </publisher>
Reference-contexts: In this paper we will introduce two tools that we have developed to aid in running and analyzing experiments: Clip and Clasp (Common Lisp Instrumentation Package and Common Lisp Analytical Statistics Package). These tools are described in more detail in Anderson et al. ( <ref> [3] </ref>), where we give examples of their use with a planning system for a transportation planning problem. They form the substrate for a larger toolbox we are developing that is specifically designed for analyzing AI programs.
Reference: [4] <author> Bolt Beranek and Newman, Inc. and ISX Corporation. </author> <title> Common prototyping environment testbed release 1.0: User's guide, </title> <booktitle> 1993. BBN Systems and Technologies, </booktitle> <address> 10 Moulton Street, Cambridge, MA 02138. </address>
Reference-contexts: We are currently at work on such a system which we call Aide, automated intelligent data exploration [1]. 5 Related Work An alternative to Clip is the Meters system, developed by Bolt, Beranek and Newman, Inc., for use in the ARPA/RL Planning Initiative's Common Prototyping Environment <ref> [4] </ref>. Meters is particularly useful for collecting and filtering time-series data from distributed systems. XlispStat [11] provides a richer set of statistical and graphical capabilities than the current version of Clasp, but is not as tightly integrated into an environment for instrumentation and analysis as is Clasp.
Reference: [5] <author> Paul R. Cohen. </author> <booktitle> Empirical Methods in Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> Forthcoming. 6 </month>
Reference-contexts: Case studies using these and other empirical techniques to analyze AI programs are included in a forthcoming textbook on empirical methods for AI research <ref> [5] </ref>. It is possible that the major contribution of Clip/Clasp will not be as a standalone instrumentation and analysis package, but rather as a platform for the integration of more powerful techniques such as those described above.
Reference: [6] <author> Paul R. Cohen, Adam Carlson, Lisa Balles--teros, and Robert St. Amant. </author> <title> Automating path analysis for building causal models from data. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 57-64. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This requires that we understand the underlying causal relationships among the factors influencing its behavior. We are developing a module that uses path analytic techniques to build causal models from data <ref> [6] </ref>, and have incorporated into it several new causal induction algorithms [7]. Case studies using these and other empirical techniques to analyze AI programs are included in a forthcoming textbook on empirical methods for AI research [5].
Reference: [7] <author> Paul R. Cohen, Dawn Gregory, Lisa Balles-teros, and Robert St. Amant. </author> <title> Two algorithms for inducing structural equation models from data. </title> <booktitle> In Proceedings of the Fifth International Workshop on AI and Statistics, </booktitle> <year> 1995. </year>
Reference-contexts: This requires that we understand the underlying causal relationships among the factors influencing its behavior. We are developing a module that uses path analytic techniques to build causal models from data [6], and have incorporated into it several new causal induction algorithms <ref> [7] </ref>. Case studies using these and other empirical techniques to analyze AI programs are included in a forthcoming textbook on empirical methods for AI research [5].
Reference: [8] <author> Bradley Efron and Gail Gong. </author> <title> A leisurely look at the bootstrap, the jackknife, and cross-validation. </title> <journal> The American Statistician, </journal> <volume> 37(1) </volume> <pages> 36-48, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Describe This menu contains the most com monly used descriptive statistics. Test This menu contains a variety of inferential procedures, including t-test, confidence intervals, analysis of variance, chi-square and regression. In the near future, we plan to implement bootstrap variants on most com mon statistical functions <ref> [8] </ref>. Manipulate Clasp provides several ways to extract subsets from a dataset through data manipulation operations such as partitioning. Other operations on this menu allow you to create new datasets. Transform This menu has commands that produce new variables from old ones (for exam ple, by sorting a variable).
Reference: [9] <author> Adele E. Howe. </author> <title> Finding dependencies in event streams using local search. </title> <booktitle> In Proceedings of the Fifth International Workshop on AI and Statistics, </booktitle> <year> 1995. </year>
Reference-contexts: Program actions often interact in unforeseen and deleterious ways. We employ a technique we call dependency detection, analyzing program execution traces with a statistical filter to find significant dependencies among interacting actions <ref> [10, 9, 12] </ref>. Causal Induction Having explored the data and/or identified dependencies among interacting factors, the user next tries to build a predictive model of the program's behavior. We would like for such a model to tell us how to change the program to improve or modify its behavior.
Reference: [10] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Understanding planner behavior. </title> <journal> Artificial Intelligence. </journal> <note> To appear. </note>
Reference-contexts: Program actions often interact in unforeseen and deleterious ways. We employ a technique we call dependency detection, analyzing program execution traces with a statistical filter to find significant dependencies among interacting actions <ref> [10, 9, 12] </ref>. Causal Induction Having explored the data and/or identified dependencies among interacting factors, the user next tries to build a predictive model of the program's behavior. We would like for such a model to tell us how to change the program to improve or modify its behavior.
Reference: [11] <institution> Luke Tierney. XlispStat. School of Statistics Report #528, University of Minnesota, </institution> <year> 1988. </year>
Reference-contexts: Meters is particularly useful for collecting and filtering time-series data from distributed systems. XlispStat <ref> [11] </ref> provides a richer set of statistical and graphical capabilities than the current version of Clasp, but is not as tightly integrated into an environment for instrumentation and analysis as is Clasp.
Reference: [12] <author> Dawn Gregory Tim Oates and Paul R. Co-hen. </author> <title> Detecting complex dependencies in categorical data. </title> <booktitle> In Proceedings of the Fifth International Workshop on AI and Statistics, </booktitle> <year> 1995. </year> <month> 7 </month>
Reference-contexts: Program actions often interact in unforeseen and deleterious ways. We employ a technique we call dependency detection, analyzing program execution traces with a statistical filter to find significant dependencies among interacting actions <ref> [10, 9, 12] </ref>. Causal Induction Having explored the data and/or identified dependencies among interacting factors, the user next tries to build a predictive model of the program's behavior. We would like for such a model to tell us how to change the program to improve or modify its behavior.
References-found: 12

