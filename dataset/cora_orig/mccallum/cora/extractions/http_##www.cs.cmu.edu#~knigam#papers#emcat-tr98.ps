URL: http://www.cs.cmu.edu/~knigam/papers/emcat-tr98.ps
Refering-URL: http://www.cs.cmu.edu/~knigam/
Root-URL: http://www.cs.cmu.edu/~jr6b
Title: Using EM to Classify Text from Labeled and Unlabeled Documents  
Author: Kamal Nigam Andrew McCallum Sebastian Thrun Tom Mitchell 
Note: This research has been supported in part by the DARPA HPKB program under research contract F30602-97-1-0215.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: May 11, 1998  
Abstract: This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is significant because in many important text classification problems obtaining classification labels is expensive, while large quantities of unlabeled documents are readily available. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text, based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%. 
Abstract-found: 1
Intro-found: 1
Reference: [ Castelli and Cover, 1995 ] <author> V. Castelli and T. </author> <title> Cover. On the exponential value of labeled samples. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 </volume> <pages> 105-111, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For this knowledge to aid classification, we have to exclude the two extreme cases: jD l j = 0 and jD l j = 1. If there is no labeled data, unlabeled data cannot improve classification, as shown in <ref> [ Castelli and Cover, 1995 ] </ref> . If there is infinite amounts of labeled data, all parameters can be recovered with probability 1 from the labeled data and the resulting classifier is Bayes-optimal [ McLachlan and Basford, 1988 ] ; thus, further unlabeled data cannot improve the classification accuracy. <p> Thus, the estimation problem reduces to the problem of learning a permutation matrix, which assigns labels to the different mixture components. Without any labeled data, this permutation cannot be found, and thus, although the parameters are known, classification error is not reduced from random guessing. As shown in <ref> [ Castelli and Cover, 1995 ] </ref> , with infinite unlabeled data, the classification error approaches the Bayes optimal solution at an exponential rate in the number of labeled examples given. <p> Unfortunately, their analysis assumes that unlabeled data alone is sufficient to estimate both parameter vectors; thus, they assume that the target concept can be recovered without any target labels. This assumption is unrealistic. As shown in <ref> [ Castelli and Cover, 1995 ] </ref> , unlabeled data does not improve the classification results in the absence of labeled data.
Reference: [ Castelli and Cover, 1996 ] <author> V. Castelli and T. </author> <title> Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6) </volume> <pages> 2101-2117, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Thus, if infinite amounts of unlabeled data are available, the convergence rate of learning from labeled data is changed by an exponential factor. * Trade-off. As shown in <ref> [ Castelli and Cover, 1996 ] </ref> , labeled data can be exponentially more valuable than unlabeled data in reducing the probability of classification error by non-degenerate Bayesian classifiers.
Reference: [ Cheeseman and Stutz, 1996 ] <author> Peter Cheeseman and John Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, editor, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <year> 1996. </year>
Reference-contexts: When the data is accurately modeled, gains from using EM are readily seen. One obvious question is how to select the best model representation. Cheeseman and Stutz <ref> [ Cheeseman and Stutz, 1996 ] </ref> investigate this for clustering tasks with no labeled data, and explicitly compare the probability of the data for different models, and select the best match, with a prior that prefers smaller models. <p> Ghahramani and Jordan have used EM with mixture models to fill in missing values [ Ghahramani and Jordan, 1994 ] . The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> [ Cheeseman and Stutz, 1996; Hanson et al., 1991 ] </ref> investigated the combination of the EM algorithm with an underlying model of a naive Bayes classifier. The emphasis of their research was the discovery of novel clusterings for unsupervised learning over unlabeled data.
Reference: [ Cohen and Singer, 1997 ] <author> W. Cohen and Y. Singer. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of ACM SIGIR Conference, </booktitle> <year> 1997. </year>
Reference: [ Cover and Thomas, 1991 ] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: of encoding words from class c j using a code that is optimal for the distribution of words in :c j ; the sum of this quantity over all words is the Kullback-Leibler divergence between the distribution of words in c j and the distribution of words in :c j <ref> [ Cover and Thomas, 1991 ] </ref> . averaged over 20 trials per data point. With small amounts of labeled documents, EM helps, but in the limit, it degrades performance slightly, indicating a misfit between the data and the assumed generative model. WebKB data for two different amounts of labeled data.
Reference: [ Craven et al., 1998 ] <author> M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slat-tery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of AAAI-98, </booktitle> <year> 1998. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles [ Lewis and Gale, 1994; Joachims, 1997b ] and web pages <ref> [ Craven et al., 1998 ] </ref> , automatically learn the reading interests of users [ Pazzani et al., 1996; Lang, 1995 ] , and automatically sort electronic mail [ Lewis and Knowles, 1997 ] . <p> Best performance was obtained with no feature selection, and by normalizing word counts by document length. Accuracy results are reported as averages of ten test/train splits, with 20% of the documents randomly selected for placement in the test set. The WebKB data set <ref> [ Craven et al., 1998 ] </ref> contains 8145 web pages gathered from university computer science departments. For four departments, all web pages were included; additionally, there are many pages from an assortment of other universities. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. <p> We did not use stemming or a stoplist; we found that using a stoplist actually hurt performance because, for example, "my" is the fourth-ranked word by information gain, and is an excellent indicator of a student homepage. As done previously <ref> [ Craven et al., 1998 ] </ref> , we use only the 2000 most informative words, as measured by average mutual information with the class variable. This feature selection method is commonly used for text [ Yang and Pederson, 1997; Koller and Sahami, 1997; Joachims, 1997a ] .
Reference: [ Dempster et al., 1977 ] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier [ Lewis and Ringuette, 1994 ] and the Expectation Maximization (EM) algorithm <ref> [ Dempster et al., 1977 ] </ref> . The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. <p> Dempster <ref> [ Dempster et al., 1977 ] </ref> uses this insight in the Expectation Maximization algorithm, which finds a local maximum likelihood ^ by an iterative procedure that recomputes the expected value of z and the maximum likelihood parameterization given z. Note that for the labeled documents z i is already known.
Reference: [ Devroye et al., 1996 ] <author> L. Devroye, L. Gyofri, and G. Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: In certain asymptotic cases, the relative value of unlabeled data in learning classification is well-understood. * No unlabeled data. First consider the efficiency of estimating from a pool of labeled data only, D l . According to <ref> [ Devroye et al., 1996 ] </ref> , estimating using the maximum likelihood estimator is subject to the following error bound: P (j ^ j &gt; ") 2 m e jD l j" 2 =2 (16) Here ^ DjC and ^ C are the maximum likelihood estimates for . <p> Unfortunately, if this assumption is violated, estimators such as the maximum likelihood estimator may generate poor results. As shown in <ref> [ Devroye et al., 1996 ] </ref> , under such conditions the maximum likelihood estimator can easily fail to minimize the classification error on the training set. 8.
Reference: [ Domingos and Pazzani, 1997 ] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: We will then use this to introduce the classifier and show that unlabeled data can be used to improve classification. The framework follows commonly used assumptions <ref> [ Lewis and Ringuette, 1994; Domingos and Pazzani, 1997 ] </ref> about the data|(1) that our text is produced by a mixture model, and (2) that there is a one-to-one correspondence between mixture components and classes. <p> Thus, the motivational result from the previous section still holds|that unlabeled documents can be beneficial. Naive Bayes makes the additional assumption that the probability of seeing a word in a document is independent of its context and its position <ref> [ Lewis and Ringuette, 1994; Domingos and Pazzani, 1997 ] </ref> . The learning task is to use a set of training documents in order to form estimates for the parameters of the generative model. <p> This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high <ref> [ Domingos and Pazzani, 1997; Friedman, 1997 ] </ref> . The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document.
Reference: [ Friedman, 1997 ] <author> Jerome H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <booktitle> 1 </booktitle> <pages> 55-77, </pages> <year> 1997. </year>
Reference-contexts: This paradox is explained by the fact that classification estimation is only a function of the sign (in binary cases) of the function estimation; the function approximation can still be poor while classification accuracy remains high <ref> [ Domingos and Pazzani, 1997; Friedman, 1997 ] </ref> . The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document.
Reference: [ Ghahramani and Jordan, 1994 ] <author> Zoubin Ghahramani and Michael Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 6). </booktitle> <publisher> Morgan Kauffman Publishers, </publisher> <year> 1994. </year>
Reference-contexts: This section describes how to use EM within the probabilistic framework of the previous section. This is a special case of the more general missing values formulation, as presented by <ref> [ Ghahramani and Jordan, 1994 ] </ref> . While the theory of why EM works is not particularly simple, the resulting algorithm is very straightforward. Our algorithm is outlined in Table 1. <p> Our work is an example of applying EM to fill in missing values|the missing values are the class labels of the unlabeled training examples. Ghahramani and Jordan have used EM with mixture models to fill in missing values <ref> [ Ghahramani and Jordan, 1994 ] </ref> . The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data.
Reference: [ Hanson et al., 1991 ] <author> Hanson, Cheeseman, and Stutz. </author> <title> Bayesian classification theory. </title> <type> Technical Report Technical Report FIA-90-12-7-01, </type> <institution> NASA AMES Research Center, </institution> <year> 1991. </year>
Reference-contexts: Ghahramani and Jordan have used EM with mixture models to fill in missing values [ Ghahramani and Jordan, 1994 ] . The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> [ Cheeseman and Stutz, 1996; Hanson et al., 1991 ] </ref> investigated the combination of the EM algorithm with an underlying model of a naive Bayes classifier. The emphasis of their research was the discovery of novel clusterings for unsupervised learning over unlabeled data.
Reference: [ Joachims, 1997a ] <author> Thorsten Joachims. </author> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year>
Reference-contexts: We present experimental results with three different text corpora from the domains of UseNet news articles (20 Newsgroups), web pages (WebKB), and newswire articles (Reuters). 3 Datasets and Protocol The 20 Newsgroups data set <ref> [ Joachims, 1997a ] </ref> , collected by Ken Lang, consists of 20,017 articles divided almost evenly among 20 different UseNet discussion groups. When words from a stoplist of common short words are removed, there are 62,258 unique words that occur more than once. <p> These numbers are presented at the best vocabulary size for each task, indicated in parentheses. Classifiers for different categories performed best with widely varying vocabulary sizes. This variance of optimal vocabulary size is unsurprising. As previously noted <ref> [ Joachims, 1997a ] </ref> , categories like 12 Category NB 1 EM 1 EM 5 EM 20 EM 40 Diff acq 75.9 (19371) 39.5 (19371) 87.2 (19371) 88.4 (5000) 88.9 (5000) +13.0 corn 40.5 (100) 21.1 (100) 36.6 (100) 39.8 (100) 39.1 (200) -0.7 crude 60.7 (19371) 27.8 (100) 55.3 (100)
Reference: [ Joachims, 1997b ] <author> Thorsten Joachims. </author> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <type> Technical Report LS8-Report, </type> <institution> University of Dortmund, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> [ Lewis and Gale, 1994; Joachims, 1997b ] </ref> and web pages [ Craven et al., 1998 ] , automatically learn the reading interests of users [ Pazzani et al., 1996; Lang, 1995 ] , and automatically sort electronic mail [ Lewis and Knowles, 1997 ] . <p> Other examples include TFIDF/Rocchio [ Salton, 1991; Rocchio, 1971 ] , regression models [ Yang and Chute, 1993 ] , k-nearest-neighbor [ Yang and Pederson, 1997 ] and Support Vector Machines <ref> [ Joachims, 1997b ] </ref> . EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data. <p> Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. Following several other studies <ref> [ Joachims, 1997b; Liere and Tadepalli, 1997 ] </ref> we use the 10 most populous classes and build binary classifiers for each class.
Reference: [ Kalt and Croft, 1996 ] <author> T. Kalt and W. B. Croft. </author> <title> A new probabilistic model of text classification and retrieval. </title> <type> Technical Report IR-78, </type> <institution> University of Massachusetts Center for Intelligent Information Retrieval, </institution> <year> 1996. </year> <note> http://ciir.cs.umass.edu/publications/index.shtml. </note>
Reference: [ Koller and Sahami, 1997 ] <author> Daphne Koller and Mehran Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference: [ Lang, 1995 ] <author> Ken Lang. Newsweeder: </author> <title> Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <pages> pages 331-339, </pages> <year> 1995. </year>
Reference-contexts: These text classification algorithms have been used to automatically catalog news articles [ Lewis and Gale, 1994; Joachims, 1997b ] and web pages [ Craven et al., 1998 ] , automatically learn the reading interests of users <ref> [ Pazzani et al., 1996; Lang, 1995 ] </ref> , and automatically sort electronic mail [ Lewis and Knowles, 1997 ] . <p> Take, for example, the task of learning which newsgroup articles are of interest to a person reading UseNet news, as examined by Lang <ref> [ Lang, 1995 ] </ref> . After reading and classifying about 1000 articles, precision of the learned classifier was about 50% for the top 10% of documents ranked by the classifier.
Reference: [ Larkey and Croft, 1996 ] <author> Leah S. Larkey and W. Bruce Croft. </author> <title> Combining classifiers in text categorization. </title> <booktitle> In SIGIR-96, </booktitle> <year> 1996. </year>
Reference: [ Lewis and Gale, 1994 ] <author> D. Lewis and Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR Conference, </booktitle> <year> 1994. </year>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> [ Lewis and Gale, 1994; Joachims, 1997b ] </ref> and web pages [ Craven et al., 1998 ] , automatically learn the reading interests of users [ Pazzani et al., 1996; Lang, 1995 ] , and automatically sort electronic mail [ Lewis and Knowles, 1997 ] . <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling <ref> [ Lewis and Gale, 1994; Lewis, 1995 ] </ref> , and a "Query By Committee" approach [ Liere and Tadepalli, 1997 ] .
Reference: [ Lewis and Knowles, 1997 ] <author> David D. Lewis and Kimberly A. Knowles. </author> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 33(2) </volume> <pages> 209-217, </pages> <year> 1997. </year>
Reference-contexts: text classification algorithms have been used to automatically catalog news articles [ Lewis and Gale, 1994; Joachims, 1997b ] and web pages [ Craven et al., 1998 ] , automatically learn the reading interests of users [ Pazzani et al., 1996; Lang, 1995 ] , and automatically sort electronic mail <ref> [ Lewis and Knowles, 1997 ] </ref> . One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately.
Reference: [ Lewis and Ringuette, 1994 ] <author> David Lewis and Marc Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier <ref> [ Lewis and Ringuette, 1994 ] </ref> and the Expectation Maximization (EM) algorithm [ Dempster et al., 1977 ] . The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. <p> We will then use this to introduce the classifier and show that unlabeled data can be used to improve classification. The framework follows commonly used assumptions <ref> [ Lewis and Ringuette, 1994; Domingos and Pazzani, 1997 ] </ref> about the data|(1) that our text is produced by a mixture model, and (2) that there is a one-to-one correspondence between mixture components and classes. <p> Thus, the motivational result from the previous section still holds|that unlabeled documents can be beneficial. Naive Bayes makes the additional assumption that the probability of seeing a word in a document is independent of its context and its position <ref> [ Lewis and Ringuette, 1994; Domingos and Pazzani, 1997 ] </ref> . The learning task is to use a set of training documents in order to form estimates for the parameters of the generative model.
Reference: [ Lewis, 1992 ] <author> David D. Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In SIGIR-92, </booktitle> <year> 1992. </year>
Reference: [ Lewis, 1995 ] <author> David D. Lewis. </author> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <journal> SIGIR Forum, </journal> <volume> 29(2) </volume> <pages> 13-19, </pages> <year> 1995. </year>
Reference-contexts: Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling <ref> [ Lewis and Gale, 1994; Lewis, 1995 ] </ref> , and a "Query By Committee" approach [ Liere and Tadepalli, 1997 ] .
Reference: [ Li and Yamanishi, 1997 ] <author> Hang Li and Kenji Yamanishi. </author> <title> Document classification using a finite mixture model. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: For unlabeled data to carry information about the parameters , it is sufficient that 1. the learning task is not degenerate, that is, 1 This assumption will be relaxed in Section 6 by making this a one-to-many correspondence. Other work <ref> [ Li and Yamanishi, 1997 ] </ref> relaxes this assumption in a many-to-one fashion. 2 9d i ; c j ; ; 0 : P (D = d i jC = c j ; ) 6= P (D = d i jC = c j ; 0 ) where D is a
Reference: [ Liere and Tadepalli, 1997 ] <author> Liere and Tadepalli. </author> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97, </booktitle> <year> 1997. </year> <month> 17 </month>
Reference-contexts: Accuracy results presented below are an average of twenty test/train splits, again randomly holding out 20% of the documents for testing. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 articles and 90 topic categories from the Reuters newswire. Following several other studies <ref> [ Joachims, 1997b; Liere and Tadepalli, 1997 ] </ref> we use the 10 most populous classes and build binary classifiers for each class. <p> Approaches differ in their methods for selecting the unlabeled example to request a label. Three such examples are relevance sampling and uncertainty sampling [ Lewis and Gale, 1994; Lewis, 1995 ] , and a "Query By Committee" approach <ref> [ Liere and Tadepalli, 1997 ] </ref> .
Reference: [ McCallum and Nigam, 1998a ] <author> Andrew McCallum and Kamal Nigam. </author> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization, </booktitle> <year> 1998. </year> <note> http://www.cs.cmu.edu/~mccallum. </note>
Reference-contexts: The above formulation of naive Bayes assumes a generative model that accounts for the number of times a word appears in a document. This is equivalent to a multinomial event model (without the factorial terms that account for event ordering) <ref> [ McCallum and Nigam, 1998a ] </ref> . This formulation has been used by numerous practitioners of naive Bayes text classification [ Lewis and Gale, 1994; Kalt and Croft, 1996; Joachims, 1997a; Li and Yamanishi, 1997; Mitchell, 1997; McCallum et al., 1998 ] . <p> Empirical comparisons show that the multinomial formulation yields higher-accuracy classifiers <ref> [ McCallum and Nigam, 1998a ] </ref> . 5. Using EM to Incorporate Unlabeled Data When naive Bayes is given just a small set of labeled training data, classification accuracy will suffer because variance in the parameter estimates of the generative model will be high.
Reference: [ McCallum and Nigam, 1998b ] <author> Andrew McCallum and Kamal Nigam. </author> <title> Employing EM and pool-based active learning for text classification. </title> <booktitle> In ICML-98, </booktitle> <year> 1998. </year>
Reference-contexts: Two other learning task formulations could also benefit from using EM: (1) an active learning approach that uses an explicit model of unlabeled data could incorporate EM iterations at every stage to improve its classification, and to better select for which data to request class labels from a labeler <ref> [ McCallum and Nigam, 1998b ] </ref> ; (2) an incremental learning algorithm that re-trains throughout the testing phase could use the unlabeled test data received early in the testing phase in order to improve performance on the later test data.
Reference: [ McCallum et al., 1998 ] <author> Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and Andrew Ng. </author> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In ICML-98, </booktitle> <year> 1998. </year>
Reference: [ McLachlan and Basford, 1988 ] <author> G.J. McLachlan and K.E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: If there is no labeled data, unlabeled data cannot improve classification, as shown in [ Castelli and Cover, 1995 ] . If there is infinite amounts of labeled data, all parameters can be recovered with probability 1 from the labeled data and the resulting classifier is Bayes-optimal <ref> [ McLachlan and Basford, 1988 ] </ref> ; thus, further unlabeled data cannot improve the classification accuracy. Note that our argument does not immediately motivate an algorithm for extracting the information from the unlabeled data. Additionally, it not show that better parameter estimation will yield better classification. <p> From this it follows that the parameter estimation error j ^ j converges to zero at the rate O (1= p * Infinite unlabeled data. If infinite amounts of unlabeled data are available, however, the parameters of the mixture components can be recovered from the unlabeled data <ref> [ McLachlan and Basford, 1988 ] </ref> , but not the assignment of mixture components to classes. Thus, the estimation problem reduces to the problem of learning a permutation matrix, which assigns labels to the different mixture components.
Reference: [ Miller and Uyar, 1997 ] <author> David J. Miller and Hasan S. Uyar. </author> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 9), </booktitle> <year> 1997. </year>
Reference-contexts: Previous supervised algorithms for learning to classify from text do not incorporate unlabeled data. A similar approach was used by Miller and Uyar <ref> [ Miller and Uyar, 1997 ] </ref> for non-text data sources. We adapt this approach for the naive Bayes text classifier and conduct a thorough empirical analysis. <p> As shown in [ Devroye et al., 1996 ] , under such conditions the maximum likelihood estimator can easily fail to minimize the classification error on the training set. 8. Related Work Two other studies have used EM to combine labeled and unlabeled data for classification <ref> [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] </ref> . Instead of naive Bayes, Shahshahani and Landgrebe use a mixture of Gaussians; Miller and Uyar use Mixtures of Experts. They demonstrate experimental results on non-text data sets with up to 40 features.
Reference: [ Mitchell, 1997 ] <author> Tom M. Mitchell. </author> <title> Machine Learning. </title> <address> WCB/McGraw-Hill, </address> <year> 1997. </year>
Reference: [ Pazzani et al., 1996 ] <author> M. J. Pazzani, J. Muramatsu, and D. Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In AAAI-96, </booktitle> <year> 1996. </year>
Reference-contexts: These text classification algorithms have been used to automatically catalog news articles [ Lewis and Gale, 1994; Joachims, 1997b ] and web pages [ Craven et al., 1998 ] , automatically learn the reading interests of users <ref> [ Pazzani et al., 1996; Lang, 1995 ] </ref> , and automatically sort electronic mail [ Lewis and Knowles, 1997 ] .
Reference: [ Robertson and Sparck-Jones, 1976 ] <author> S. E. Robertson and K. Sparck-Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27 </volume> <pages> 129-146, </pages> <year> 1976. </year>
Reference: [ Rocchio, 1971 ] <author> J. Rocchio. </author> <title> Relevance feedback in information retrieval. In The SMART Retrieval System:Experiments in Automatic Document Processing, </title> <booktitle> chapter 14, </booktitle> <pages> pages 313-323. </pages> <publisher> Prentice Hall, </publisher> <year> 1971. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio <ref> [ Salton, 1991; Rocchio, 1971 ] </ref> , regression models [ Yang and Chute, 1993 ] , k-nearest-neighbor [ Yang and Pederson, 1997 ] and Support Vector Machines [ Joachims, 1997b ] . EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: [ Salton, 1991 ] <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-979, </pages> <year> 1991. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio <ref> [ Salton, 1991; Rocchio, 1971 ] </ref> , regression models [ Yang and Chute, 1993 ] , k-nearest-neighbor [ Yang and Pederson, 1997 ] and Support Vector Machines [ Joachims, 1997b ] . EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: [ Shahshahani and Landgrebe, 1994 ] <author> B. Shahshahani and D. Landgrebe. </author> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing, </journal> <volume> 32(5) </volume> <pages> 1087-1095, </pages> <month> Sept </month> <year> 1994. </year>
Reference-contexts: This result, however, assumes that the parameters of the individual mixture components are known; little is known for the more general case, where unlabeled data can be used to estimate those. Shahshahani and Landgrebe <ref> [ Shahshahani and Landgrebe, 1994 ] </ref> investigates the utility of unlabeled data in supervised learning, with quite different results. They analyze the convergence rate under the assumption that unbiased estimators are available for , for both the labeled and the unlabeled data. <p> As shown in [ Devroye et al., 1996 ] , under such conditions the maximum likelihood estimator can easily fail to minimize the classification error on the training set. 8. Related Work Two other studies have used EM to combine labeled and unlabeled data for classification <ref> [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] </ref> . Instead of naive Bayes, Shahshahani and Landgrebe use a mixture of Gaussians; Miller and Uyar use Mixtures of Experts. They demonstrate experimental results on non-text data sets with up to 40 features.
Reference: [ Vapnik, 1982 ] <author> V. Vapnik. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year>
Reference-contexts: To calculate the probability of a word given a class, w t jc j , simply count the fraction of times that word occurs in the data for that class, and augment this fraction with Bayes optimal smoothing that primes the count for each word with a "pseudo-occurrence" of one <ref> [ Vapnik, 1982 ] </ref> . This smoothing is sometimes referred to as the Laplacean prior, and is necessary to prevent probability zero probabilities for infrequently occurring words.
Reference: [ Yang and Chute, 1993 ] <author> Yiming Yang and Christopher G. Chute. </author> <title> An application of least squares fit mapping to text information retrieval. </title> <booktitle> In Proceedings of the Sixteenth Annual International ACM SIGIR Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [ Salton, 1991; Rocchio, 1971 ] , regression models <ref> [ Yang and Chute, 1993 ] </ref> , k-nearest-neighbor [ Yang and Pederson, 1997 ] and Support Vector Machines [ Joachims, 1997b ] . EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: [ Yang and Pederson, 1997 ] <author> Yiming Yang and Jan Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year> <month> 18 </month>
Reference-contexts: The naive Bayes algorithm is one of a class of statistical text classifiers that uses word frequencies as features. Other examples include TFIDF/Rocchio [ Salton, 1991; Rocchio, 1971 ] , regression models [ Yang and Chute, 1993 ] , k-nearest-neighbor <ref> [ Yang and Pederson, 1997 ] </ref> and Support Vector Machines [ Joachims, 1997b ] . EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
References-found: 39

