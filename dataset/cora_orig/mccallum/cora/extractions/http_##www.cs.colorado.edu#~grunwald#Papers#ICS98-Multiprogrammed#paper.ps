URL: http://www.cs.colorado.edu/~grunwald/Papers/ICS98-Multiprogrammed/paper.ps
Refering-URL: http://www.cs.colorado.edu/~grunwald/Papers/ICS98-Multiprogrammed/
Root-URL: http://www.cs.colorado.edu
Email: suvas@lanl.gov  grunwald@cs.colorado.edu  
Title: Dependence Driven Execution for Multiprogrammed Multiprocessor Figure 1: Performance Degradation in Multiprogrammed Multiprocessors Others have
Author: Suvas Vajracharya CIC/ACL Dirk Grunwald 
Keyword: Run-time systems, multiprogramming, loop scheduling, dependence-driven execution, barrier synchronization, coarse-grain dataflow.  
Note: c fl1998 ACM $3.50  
Address: Los Alamos, NM, U.S.A.  Boulder, CO, U.S.A.  
Affiliation: Los Alamos National Laboratory  Department of Computer Science University of Colorado  
Abstract: Barrier synchronizations can be very expensive on multiprogramming environment because no process can go past a barrier until all the processes have arrived. If a process participating at a barrier is swapped out by the operating system, the rest of participating processes end up waiting for the swapped-out process. This paper presents a compile-time/run-time system that uses a dependence-driven execution to overlap the execution of computations separated by barriers so that the processes do not spend most of the time idling at the synchronization point. The parallel execution of a sequence of loop nests is typically broken into phases, each phase consisting of a simple loop separated by a barrier synchronization to ensure that the execution respects data dependencies between phases of collective operations. A barrier synchronization insists that all the participating processes be collectively done with a particular phase of a computation before the next phase is begun. This is undesirable in that the barrier synchronization causes the computation to stall while waiting for the slowest process. On multiprogrammed multiprocessors, a participating process may be swapped out to run a process from a different job, making the problem worse. Figure 1 shows that the swapped out processes effectively lengthen the time required to achieve a barrier. Much of the work on multiprogrammed environment has been done on the operating system or combination of user space/operating system. Several researchers [1, 2, 10, 11] have focused on ways to establish some co-operation between the application and the operating system kernel. By extending the kernel to communicate with the application, the kernel can avoid preempting a task that is in a critical section. However, because these methods require modification to the operating system and more importantly, because they require that the applications handshake with the O.S. scheduler, many commercial operating systems do not support scheduler activations and scheduler-conscious synchronization. Permissions to make digital/hard copy of part or all this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. ICS'98 Melbourne, Austrailia We take an approach that is not incompatible with the above methods. We are presenting a user-level loop scheduling method that adapts to dynamic increases or decreases in the available number of CPUs without requiring modifications to the operating system. This makes the runtime system that we are proposing portable and easy to use, and makes no assumptions about the policies or the kinds of extra services that the operating system supports. The loop scheduling method proposed here extends traditional loop scheduling methods such as static, affinity and dynamic loop schedulers by using symbolic data dependence information. Having dependence information allows the run-time system to schedule the entire loop structure consisting of several simple loops. To illustrate, let us turn to figure 1 once more. Using a dependence-driven execution, some of the processors can begin computing phase 2 while the other processors are still computing phase 1. This allows us to do useful computation while waiting for processes that the operating system has swapped out. However, overlapping executions of different phases is only legal if the computation respects the data dependencies between the two phases. In our companion papers [19, 18], we discussed how such a runtime fl This work was the while Suvas Vajracharya was a graduate student at the University of Colorado
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 10(1):52 79, </volume> <month> February </month> <year> 1992. </year>
Reference: [2] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1):132, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: By putting together and specializing these objects, the user specializes the system to create a software systolic array for the application at hand. This object-oriented model is based on AWESIME [6] and the Chores <ref> [2] </ref> runtime systems. The following is a list of objects in DUDE: * Data Descriptor: Data Descriptors describe a sub-region of the data space. For example, the system can divide a matrix into sub-matrices with each sub-matrix being defined by a data descriptor.
Reference: [3] <author> D. Gannon and J.K. Lee. </author> <title> Object oriented parallelism. </title> <booktitle> In Proceedings of 1991 Japan Society for Parallel Processing, </booktitle> <pages> pages 1323, </pages> <year> 1991. </year>
Reference-contexts: The user chooses the decomposition method such as by BLOCK or CYCLIC similar to decomposition and distribution utilities available in HPF [9] and pC++ <ref> [3] </ref>. One important difference, however, is that in the process of data decomposition, DUDE takes flat data and creates objects or Iterates, which are tuples consisting of both data and operation. 1.8 Dynamic Scheduling In dynamic scheduling, the data decomposition and data distribution is determined at run-time by the scheduler.
Reference: [4] <author> A. S. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 3951, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A very fine granularity of asynchronous computation requires a significantly different design than for a coarse-grain computation. For example, dataflow computers such as the Manchester Dataflow Machine [7, 8] relied on special hardware to orchestrate parallelism at the level arithmetic operations. In Mentat <ref> [5, 4] </ref>, the unit of parallelism were functions. In the autoscheduling work by Moreira and Polychronopoulos [13], the unit was a task, which may consist of a entire loop.
Reference: [5] <author> A. S. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <booktitle> IEEE Parallel and Distributed Technology: Systems and Applications, </booktitle> <pages> pages 3347, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A very fine granularity of asynchronous computation requires a significantly different design than for a coarse-grain computation. For example, dataflow computers such as the Manchester Dataflow Machine [7, 8] relied on special hardware to orchestrate parallelism at the level arithmetic operations. In Mentat <ref> [5, 4] </ref>, the unit of parallelism were functions. In the autoscheduling work by Moreira and Polychronopoulos [13], the unit was a task, which may consist of a entire loop.
Reference: [6] <author> Dirk Grunwald. </author> <title> A user's guide to a.w.e.s.i.m.e: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: By putting together and specializing these objects, the user specializes the system to create a software systolic array for the application at hand. This object-oriented model is based on AWESIME <ref> [6] </ref> and the Chores [2] runtime systems. The following is a list of objects in DUDE: * Data Descriptor: Data Descriptors describe a sub-region of the data space. For example, the system can divide a matrix into sub-matrices with each sub-matrix being defined by a data descriptor.
Reference: [7] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The manchester prototype dataflow computer. </title> <journal> Communication of the ACM, </journal> <volume> 28:3452, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: One issue that distinguishes the various works is the unit or the granularity of parallelism. A very fine granularity of asynchronous computation requires a significantly different design than for a coarse-grain computation. For example, dataflow computers such as the Manchester Dataflow Machine <ref> [7, 8] </ref> relied on special hardware to orchestrate parallelism at the level arithmetic operations. In Mentat [5, 4], the unit of parallelism were functions. In the autoscheduling work by Moreira and Polychronopoulos [13], the unit was a task, which may consist of a entire loop.
Reference: [8] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The Manchester Dataflow Computing System, </title> <publisher> pages 516,517,519,520,529. North-Holland, </publisher> <year> 1987. </year>
Reference-contexts: One issue that distinguishes the various works is the unit or the granularity of parallelism. A very fine granularity of asynchronous computation requires a significantly different design than for a coarse-grain computation. For example, dataflow computers such as the Manchester Dataflow Machine <ref> [7, 8] </ref> relied on special hardware to orchestrate parallelism at the level arithmetic operations. In Mentat [5, 4], the unit of parallelism were functions. In the autoscheduling work by Moreira and Polychronopoulos [13], the unit was a task, which may consist of a entire loop.
Reference: [9] <author> High Performance Fortran Forum HPFF. </author> <title> Draft high performance fortran specificition, </title> <note> version 0.4. In Proceedings of 1991 Japan Society for Parallel Processing, page Available at http://www.crpc.rice.edu/HPFF, 1992. </note>
Reference-contexts: One of the parameters used in instantiating a StaticCollection or an AffinityCollection is the decomposition method, which determines how the system divides the data between the different Iterates. The user chooses the decomposition method such as by BLOCK or CYCLIC similar to decomposition and distribution utilities available in HPF <ref> [9] </ref> and pC++ [3].
Reference: [10] <author> L. Kontothanassis and R. Wisniewski. </author> <title> Using scheduler information to achieve optimal barrier synchronization performance. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference: [11] <author> L. Kontothanassis, R. Wisniewski, and Michael L. Scott. </author> <title> Scheduler-conscious synchronization. </title> <journal> ACM TOCS, </journal> <volume> 15(1), </volume> <year> 1997. </year>
Reference: [12] <author> E.P Markatos and T. J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared Memory Multiprocessors. </title> <booktitle> In Intl. Conference on Parallel Processing, </booktitle> <pages> pages 258257, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The advantage of affinity scheduling is that the scheduler can adapt to the dynamic fluctuation of the workload by distributing Iterates to idle processors while trying to maintain data locality <ref> [12] </ref>. One of the parameters used in instantiating a StaticCollection or an AffinityCollection is the decomposition method, which determines how the system divides the data between the different Iterates.
Reference: [13] <author> Jose Moreira and Constantine Polychronopoulos. </author> <title> Au-toscheduling in a shared memory multiprocessor. </title> <booktitle> In Proceedings of the IEEE/USP International Workshop on High Performance Computing Compilers and Tools for Parallel Processing, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: For example, dataflow computers such as the Manchester Dataflow Machine [7, 8] relied on special hardware to orchestrate parallelism at the level arithmetic operations. In Mentat [5, 4], the unit of parallelism were functions. In the autoscheduling work by Moreira and Polychronopoulos <ref> [13] </ref>, the unit was a task, which may consist of a entire loop. These loops might be scheduled by guided-self scheduling across the available processors so that the iterations of a doall loop can be run in parallel. <p> Put differently, the nodes in the dataflow graph is at the granularity of entire loops. The granularity of parallelism in the DUDE run-time system is somewhere between the fine grain computation in dataflow machines and the coarse grain macro-dataflow approach in the work by Moreira and Polychronopoulos <ref> [13] </ref>. Another dimension along which to compare the different work is the extent to which the user is involved in synchronizing the units of computation.
Reference: [14] <author> John Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of Distributed Computing Systems, </booktitle> <pages> pages 2230, </pages> <month> Oct </month> <year> 1990. </year>
Reference: [15] <author> C. D. Polochronopoulous and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12):1425 1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: We focus a little on the Chores runtime system, which is most similar to our own work along the dimensions mentioned. In the Chores runtime system, a per-processor worker (a user-level thread) grabs chunks of work from a central queue using the guided self-scheduling <ref> [15] </ref> method. To use guided self-scheduling to schedule loops, the system requires that a multi-dimensional iteration space be collapsed into a single dimension.
Reference: [16] <author> J. Torres, E. Ayguadi, J. Labarta, and M. Valero. </author> <title> Loop parallelization: Revisiting framework of unimodular transformations. </title> <booktitle> In Proceedings of the Fourth Euromicro Workshop on Parallel and Distributed Processing, IEEE Computer Society, </booktitle> <pages> pages 420427, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Methods also vary in the extent to which the system relies on the optimizations using dependence information are applied at compile-time. Strictly compile-time methods <ref> [20, 16] </ref> restructure the loop and generate new code to improve locality and parallelism.
Reference: [17] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 159166, </pages> <month> Dec </month> <year> 1989. </year>
Reference: [18] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Dependence-driven run-time system. </title> <booktitle> In Proceedings of Language and Compilers for Parallel Computing, </booktitle> <pages> pages 168176, </pages> <year> 1996. </year>
Reference-contexts: The inter-Iterate order, however, is determined by the dependence-driven execution. DUDE system consists of three types of dependence-driven loop scheduling methods: static, affinity, and dynamic. A description of these schedulers in DUDE can also be found in our paper <ref> [18] </ref>. Depending on the application, or based on runtime conditions, the user can choose one of these methods by instantiating the appropriate IterateCollection template, which determines the scheduling behavior. <p> In DUDE, we use spatial data-structures based on the principle of recursive decomposition called quadtrees ( or its three-dimensional version, octree) breakdown and coalesce neighboring Iterates to adjust the granularity during runtime. A more complete description of our octree scheduling method can be found in a different paper <ref> [18] </ref>. 2 Related Work Using dependence information to increase parallelism is not a new idea. One issue that distinguishes the various works is the unit or the granularity of parallelism. A very fine granularity of asynchronous computation requires a significantly different design than for a coarse-grain computation. <p> While compile-time methods have little overheads, they are limited in that optimizations cannot be applied to loops that have array sub-expressions that are non-linear with respect to the induction variable A more complete comparison of our work with other related works can be found in <ref> [18, 19] </ref> which studied the locality and par allelism of the DUDE runtime system. This paper describes the performance of a dependence-driven execution in multiprogrammed environment where there are more processes than processors.
Reference: [19] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Loop re-ordering and pre-fetching at runtime. </title> <booktitle> In High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: While compile-time methods have little overheads, they are limited in that optimizations cannot be applied to loops that have array sub-expressions that are non-linear with respect to the induction variable A more complete comparison of our work with other related works can be found in <ref> [18, 19] </ref> which studied the locality and par allelism of the DUDE runtime system. This paper describes the performance of a dependence-driven execution in multiprogrammed environment where there are more processes than processors.
Reference: [20] <author> Michael Edward Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Methods also vary in the extent to which the system relies on the optimizations using dependence information are applied at compile-time. Strictly compile-time methods <ref> [20, 16] </ref> restructure the loop and generate new code to improve locality and parallelism.
Reference: [21] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Fortunately, affine subscript expressions are also the most common form of subscripts in loops <ref> [21] </ref>. For linear subscript expressions, we can derive the macro-dependence rule directly from the sub-expressions: we simply replace the iteration index i which ranges over 1::N , with an Iterate index which ranges over 1::N=g, where g is the granularity of Iterates.
References-found: 21

