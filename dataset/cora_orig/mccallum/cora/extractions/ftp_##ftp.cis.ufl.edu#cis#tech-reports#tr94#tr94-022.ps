URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-022.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu, nemo@cis.ufl.edu  
Title: A Comparison of Fast and Low Overhead Distributed Priority Locks  
Author: Theodore Johnson and Richard Newman-Wolfe 
Address: Gainesville, Fl 32611-2024  
Affiliation: Dept. of CIS, University of Florida  
Abstract: Distributed synchronization is necessary to coordinate the diverse activities of a distributed system. Priority synchronization is needed for real time systems, or to improve the performance of critical tasks. Practical synchronization techniques require fast response and low overhead. In this paper, we present three priority synchronization algorithms that send O(log n) messages per critical section request, and use O(log n) bits of storage per processor. Two of the algorithms are based on Li and Hudak's path compression techniques, and the third algorithm uses Raymond's fixed-tree structure. Since each of the algorithms have the same theoretical complexity, we make a performance comparison to determine which of the algorithms is best under different loads and different request priority distributions. We find that when the request priority distribution is stationary, the path-compression algorithm that uses a singly-linked list is best overall, but the the fixed-tree algorithm requires fewer messages when the number of processors is small and the load is high (100% or greater). When the request priority distribution is non-stationary, the fixed-tree algorithm is requires the fewest messages when the load is 100% or greater. The double-link algorithm is better when the load is low (less than 100%), or if minimizing execution time overhead is more important than minimizing message overhead.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <year> 1991. </year>
Reference-contexts: In our example, the last message from A to B tells B that it is in the waiting ring, and the first message from C to B is the token. Considerable work has been done on implementing causal communications <ref> [1] </ref>, but this work requires that all messages are broadcast, which in general requires O (n) messages. The messages that need to be processed in-order involve the waiting ring maintenance.
Reference: [2] <author> O.S.F. Carvalho and G. Roucairol. </author> <title> On mutual exclusion in computer networks. </title> <journal> Comm. of the ACM, </journal> <volume> 26(2) </volume> <pages> 146-147, </pages> <year> 1983. </year>
Reference-contexts: Lamport [11] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [18] and Carvalho and Roucairol <ref> [2] </ref> improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [19] introduces the idea of quorum consensus for distributed synchronization.
Reference: [3] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <year> 1985. </year>
Reference-contexts: While global time does not exist in an asynchronous distributed system, we can view the events in the system as being totally ordered using Lamport timestamps [11], and view a point in time as being a consistent cut <ref> [3] </ref>. We note that all processors that are not requesting the token lie on a path that leads to a processor that either holds or is requesting the token. This property can be seen by induction. We assume the property holds initially (and this is required for correctness).
Reference: [4] <author> Y.I. Chang, M. Singhal, and M.T. Liu. </author> <title> An improved o(log(n) mutual exclusion algorithm for distribtued systems. </title> <booktitle> In Int'l Conf. on Parallel Processing, </booktitle> <pages> pages III295-302, </pages> <year> 1990. </year>
Reference-contexts: When the system is quiescent, the pointers form a tree that is rooted at the current page owner. Trehel and Naimi [21, 20] present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu <ref> [4] </ref> present an improvement to the Trehel and Naimi [20] that reduces the average number of messages required per critical section entry. When a processor faults on a non-resident page, it sends a request to the pointed-to processor. Eventually, the page is returned and the faulting processor unblocks. <p> The recent contention-free priority locks are based on the Mellor-Crummey and Scott lock. Recent distributed synchronization algorithms by Naimi and Trehel [21], Neilsen and Mizuno [16], and by Chang, Singhal and Liu <ref> [4] </ref> make use of simple distributed lists to improve the performance of their algorithms and avoid O (n) storage.
Reference: [5] <author> T.S. Craig. </author> <title> Queuing spin lock alternatives to support timing predictability. </title> <type> Technical report, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc [14], Craig <ref> [5] </ref>, and Johnson and Harathi [8]. Two of our priority synchronization algorithm use the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block processor externally instead of internally.
Reference: [6] <author> D. Ginat, D.D. Sleator, and R. Tarjan. </author> <title> A tight amortized bound for path reversal. </title> <journal> Information Processing Letters, </journal> <volume> 31 </volume> <pages> 3-5, </pages> <year> 1989. </year>
Reference-contexts: Previous analyses show that the number of hops to find the waiting processes is O (log n) <ref> [6] </ref>. The number of hops that a request makes around the waiting structure is difficult to determine, but we can estimate that the number of hops is proportional to the number of waiting processors.
Reference: [7] <author> A. Goscinski. </author> <title> Two algorithms for mutual exclusion in real-time distributed computer systems. </title> <journal> The Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 77-82, </pages> <year> 1990. </year> <note> 28 stationary priorities. </note>
Reference-contexts: Li and Hudak show that their path compression algorithm requires O (n + K log q) messages if only q of the n processors use the page. Some work has been done to develop prioritized critical section algorithms. Goscinski <ref> [7] </ref> has proposed a fully distributed priority synchronization algorithm. However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request.
Reference: [8] <author> K. Harathi and T. Johnson. </author> <title> A priority synchronization algorithm for multiprocessors. </title> <type> Technical Report tr93.005, </type> <institution> UF, </institution> <year> 1993. </year> <note> available at ftp.cis.ufl.edu:cis/tech-reports. </note>
Reference-contexts: However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc [14], Craig [5], and Johnson and Harathi <ref> [8] </ref>. Two of our priority synchronization algorithm use the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block processor externally instead of internally.
Reference: [9] <author> D.V. James, A.T. Laundrie, S. Gjessing, and G.S. Sohi. </author> <title> Scalable coherent interface. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 74-77, </pages> <year> 1990. </year>
Reference-contexts: The third algorithm uses a fixed tree, as in Raymond's approach. Some work has been done on distributed lists, primarily in the context of directory-based cache coherence algorithms. For example, the Scalable Coherent Interface (SCI) <ref> [9] </ref> uses a distributed queue to chain together all of the processors that are requesting access to a memory block. The algorithm is greatly simplified because the pointer to the head of the list is stored in a standard place (the home memory block).
Reference: [10] <author> A. Kumar. </author> <title> Hierarchical quorum consensus: A new algorithm for managing replicated data. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(9) </volume> <pages> 994-1004, </pages> <year> 1991. </year>
Reference-contexts: Maekawa [13] presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar <ref> [10] </ref> presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak [12] present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system.
Reference: [11] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-564, </pages> <year> 1978. </year>
Reference-contexts: Since all three of the protocols have similar theoretical performance, we implemented a simulation of the algorithms and made a performance study. Considerable attention has been paid to the problem of distributed synchronization. Lamport <ref> [11] </ref> proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. <p> We loosely refer to events as occurring at a point in `time'. While global time does not exist in an asynchronous distributed system, we can view the events in the system as being totally ordered using Lamport timestamps <ref> [11] </ref>, and view a point in time as being a consistent cut [3]. We note that all processors that are not requesting the token lie on a path that leads to a processor that either holds or is requesting the token. This property can be seen by induction.
Reference: [12] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <year> 1989. </year>
Reference-contexts: Kumar [10] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak <ref> [12] </ref> present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system. In DSVM, a page of memory in a processor is treated as a cached version of a globally shared memory page.
Reference: [13] <author> M. Maekawa. </author> <title> A sqrt(n) algorithm for mutual exlcusion in decentralized systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 145-159, </pages> <year> 1985. </year>
Reference-contexts: The number of votes that are required to obtain the token can be reduced by observing that the only requirement for mutual exclusion is that any pair of processors require a vote from the same processor. Maekawa <ref> [13] </ref> presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar [10] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm.
Reference: [14] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Multiprocessor synchronization primitives with priorities. </title> <type> Technical report, </type> <institution> University of Rochester, </institution> <year> 1991. </year>
Reference-contexts: Goscinski [7] has proposed a fully distributed priority synchronization algorithm. However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc <ref> [14] </ref>, Craig [5], and Johnson and Harathi [8]. Two of our priority synchronization algorithm use the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block processor externally instead of internally.
Reference: [15] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Trans. Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <year> 1991. </year>
Reference-contexts: We note that Li and Hudak found that their path compression algorithm had far superior performance to algorithms that required fixed site managers. A shared memory synchronization algorithm that is quite similar in nature to the SCI algorithm is the contention-free lock of Mellor-Crummey and Scott <ref> [15] </ref>. The recent contention-free priority locks are based on the Mellor-Crummey and Scott lock.
Reference: [16] <author> M.L. Neilsen and M. Mizuno. </author> <title> A dag-based neilsen for distributed mutual exclusion. </title> <booktitle> In International Conference on Distributed Computer Systems, </booktitle> <pages> pages 354-360, </pages> <year> 1991. </year> <month> 29 </month>
Reference-contexts: The algorithm organizes the participating processors in a fixed tree. The execution of the algorithm is similar to that of the Li and Hudak algorithm. Neilsen and Mizuno <ref> [16] </ref> present an improved version of Raymond's algorithm that requires fewer messages because it passes tokens directly between 2 processors instead of through the tree. Woo and Newman-Wolfe [22] use a fixed tree based on a Huffman code. <p> A shared memory synchronization algorithm that is quite similar in nature to the SCI algorithm is the contention-free lock of Mellor-Crummey and Scott [15]. The recent contention-free priority locks are based on the Mellor-Crummey and Scott lock. Recent distributed synchronization algorithms by Naimi and Trehel [21], Neilsen and Mizuno <ref> [16] </ref>, and by Chang, Singhal and Liu [4] make use of simple distributed lists to improve the performance of their algorithms and avoid O (n) storage.
Reference: [17] <author> K. Raymond. </author> <title> A tree-based algorithm for distributed mutual exclusion. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 61-77, </pages> <year> 1989. </year>
Reference-contexts: However, the blocking that the algorithm requires incurs a O (n) space overhead, to store the identities of the blocked requests. Raymond <ref> [17] </ref> has proposed a simple synchronization algorithm that can be configured to require O (log n) storage per processor and O (log n) messages per critical section request. The algorithm organizes the participating processors in a fixed tree. <p> If the token is in a subtree rooted at the processor, currentdir points to the child whose subtree contains the token. If the token is not in the subtree rooted at the processor, currentdir points to the processor's parent. This structure is illustrated in Figure 7. Raymond <ref> [17] </ref> presents a simple distributed non-prioritized synchronization algorithm with guaranteed O (log n) performance by using this technique. Raymond's algorithm can be fairly easily modified to permit prioritized synchronization. In addition to keeping currentdir, each processor keeps a priority queue of the requests that it has received from its neighbors.
Reference: [18] <author> G. Ricart and A.K. Agrawala. </author> <title> An optimal algorithm for mutual exclusion in computer networks. </title> <journal> Comm. of the ACM, </journal> <volume> 24(1) </volume> <pages> 9-17, </pages> <year> 1981. </year>
Reference-contexts: Lamport [11] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala <ref> [18] </ref> and Carvalho and Roucairol [2] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [19] introduces the idea of quorum consensus for distributed synchronization.
Reference: [19] <author> R. H. Thomas. </author> <title> A majority consensus approach to concurrency control for multiple copy databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <year> 1979. </year>
Reference-contexts: A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [18] and Carvalho and Roucairol [2] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas <ref> [19] </ref> introduces the idea of quorum consensus for distributed synchronization. When a processor requests the token, it sends a vote request to all of the other processors in the system. A processor will vote for the critical section entry of at most one processor at a time.
Reference: [20] <author> M. Trehel and M. Naimi. </author> <title> A distributed algorithm for mutual exclusion based on data structures and fault tolerance. </title> <booktitle> In IEEE Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 36-39, </pages> <year> 1987. </year>
Reference-contexts: Instead, every processor associates a pointer with each globally shared page. This pointer is a guess about the current location of the page. When the system is quiescent, the pointers form a tree that is rooted at the current page owner. Trehel and Naimi <ref> [21, 20] </ref> present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [4] present an improvement to the Trehel and Naimi [20] that reduces the average number of messages required per critical section entry. <p> Trehel and Naimi [21, 20] present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [4] present an improvement to the Trehel and Naimi <ref> [20] </ref> that reduces the average number of messages required per critical section entry. When a processor faults on a non-resident page, it sends a request to the pointed-to processor. Eventually, the page is returned and the faulting processor unblocks.
Reference: [21] <author> M. Trehel and M. Naimi. </author> <title> An improvement of the log(n) distributed algorithm for mutual exclusion. </title> <booktitle> In Proc. IEEE Intl. Conf. on Distributed Computer Systems, </booktitle> <pages> pages 371-375, </pages> <year> 1987. </year>
Reference-contexts: Instead, every processor associates a pointer with each globally shared page. This pointer is a guess about the current location of the page. When the system is quiescent, the pointers form a tree that is rooted at the current page owner. Trehel and Naimi <ref> [21, 20] </ref> present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [4] present an improvement to the Trehel and Naimi [20] that reduces the average number of messages required per critical section entry. <p> A shared memory synchronization algorithm that is quite similar in nature to the SCI algorithm is the contention-free lock of Mellor-Crummey and Scott [15]. The recent contention-free priority locks are based on the Mellor-Crummey and Scott lock. Recent distributed synchronization algorithms by Naimi and Trehel <ref> [21] </ref>, Neilsen and Mizuno [16], and by Chang, Singhal and Liu [4] make use of simple distributed lists to improve the performance of their algorithms and avoid O (n) storage.
Reference: [22] <author> T.K. Woo and R. Newman-Wolfe. </author> <title> Huffman trees as a basis for a dynamic mutual exclusion algorithm for distributed systems. </title> <booktitle> In Proceedings of the 12th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 126-133, </pages> <year> 1992. </year> <type> 30 non-stationary priorities. 31 non-stationary priorities. non-stationary priorities. 32 non-stationary priorities. 33 </type>
Reference-contexts: The execution of the algorithm is similar to that of the Li and Hudak algorithm. Neilsen and Mizuno [16] present an improved version of Raymond's algorithm that requires fewer messages because it passes tokens directly between 2 processors instead of through the tree. Woo and Newman-Wolfe <ref> [22] </ref> use a fixed tree based on a Huffman code. Because the tree is fixed, however, it does not adapt to the pattern of requests in the system. Often, only a small population of the processors make requests for the token.
References-found: 22

