URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-97-25.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Decimation of Tetrahedral Grids With Error Control UCSC-CRL-97-25  
Author: Vivek Verma Allen Van Gelder 
Date: June 23, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Mar ia-Elena Algorri and Francis Schmitt. </author> <title> Mesh simplification. </title> <booktitle> In Computer Graphics Forum, Proceedings of EUROGRAPHICS'96, </booktitle> <volume> volume 15, </volume> <pages> pages 77-86, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Algorri and Schmitt have proposed a similar method where vertices are grouped into clusters, which are well characterized regions that can successfully accept simplification operations <ref> [1] </ref>. Ciampalini et al. [5] have proposed a decimation method based on Schroder's work [51]. Following Schroder, they also classify vertices based on local geometry and topology, but the decimation criterion for choosing which vertices to delete is based on a global approximation of the error.
Reference: [2] <author> G. G. Cameron and P.E. Undrill. </author> <title> Rendering volumetric medical image data on a SIMD-architecture computer. </title> <booktitle> In Proceedings of the Third Eurographics Workshop on Rendering, </booktitle> <pages> pages 135-145, </pages> <address> Bristol, UK, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The shear-warp factorization was first used to simplify data communication patterns in volume rendering algorithms for SIMD machines <ref> [2, 50] </ref>. Lacroute and Levoy [34] have used the shear-warp factorization in their volume rendering algorithms which are very fast. They are able to achieve almost interactive speeds.
Reference: [3] <author> Andrew Certain, Jovan Popovic, Tony DeRose, Tom Duchamp, David Salesin, and Werner Stuetzle. </author> <title> Interactive multiresolution surface viewing. </title> <booktitle> In Proceedings of SIGGRAPH'96, </booktitle> <pages> pages 91-98, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The parameterization is such that all faces fit together seamlessly. Finally, the triangle faces are recursively split to the desired level of detail. Certain et al. extended multi-resolution analysis to represent both color and geometry in multi-resolution meshes which provide interactive speeds <ref> [3] </ref>. Puppo [43] has introduced Multi-Triangulation (MT), a triangle based multiresolution decomposition to represent terrain surfaces at variable resolution. Floriani et al. [20] have extended the MT model to represent generic meshes of triangles in 3-D.
Reference: [4] <author> Xin Chen and Francis Schmitt. </author> <booktitle> Modeling in computer graphics: Methods and applications. </booktitle> <pages> 95-113. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: A method that uses features in the data to do constrained identification is proposed by Chen and Francis <ref> [4] </ref>. They describe methods to detect the features, as those mentioned above, both manually and automatically. Scarlatos and Pavlidis describe a method to construct hierarchical triangulations using terrain features [49]. Floriani has defined a hierarchical model of a 2 1 2 dimensional surface based on a Delaunay triangulation [19].
Reference: [5] <author> A. Ciampalini, P. Cignoni, C. Montani, and R. Scopigno. </author> <title> Multiresolution decimation based on global error. </title> <journal> The Visual Computer, </journal> <volume> 13(5) </volume> <pages> 228-246, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: This is a very desirable characteristic, for example, when a user flies over a terrain, the mesh needs to be fully detailed only near the viewer, and only within the field of view. Ciampalini et al. <ref> [5] </ref> have classified the surface simplification methods based on how the different methods reduce the vertices of the mesh and the error criteria used. <p> Algorri and Schmitt have proposed a similar method where vertices are grouped into clusters, which are well characterized regions that can successfully accept simplification operations [1]. Ciampalini et al. <ref> [5] </ref> have proposed a decimation method based on Schroder's work [51]. Following Schroder, they also classify vertices based on local geometry and topology, but the decimation criterion for choosing which vertices to delete is based on a global approximation of the error. <p> This method seeks to preserve not only the geometry of the mesh surface, but also its overall appearance, as defined by the discrete and scalar attributes (like texture) associated with its surface. Unlike Ciampalini's method (described in <ref> [5] </ref>), the Progressive Mesh construction algorithm does not guarantee maximum error bounds. The Progressive Mesh representation also supports selective refinement, whereby detail is added only to the selected areas. He has also introduced a framework for selectively refining an arbitrary Progressive Mesh according to changing viewing parameters in real-time [30].
Reference: [6] <author> Paolo Cignoni, Liela De Floriani, Claudio Montani, Enrico Puppo, and Roberto Scopigno. </author> <title> Multireso-lution modeling and visualization of volume data based on simplical complexes. </title> <booktitle> In Proceedings of 1994 Symposium on Volume Visualization, </booktitle> <pages> pages 19-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: form (2 N + 1), hence the volume may need to be padded with extra slices of zeros. irregular grids Cignoni et al. treat a tetrahedral grid as a sampled version of a scalar field defined over a 3-D domain, whose graph is a hypersurface embedded in a 4-D space <ref> [6] </ref>. Their method is a refinement based one and starts from an initial tetrahedralization built using a small number of vertices, and iteratively adds new vertices until the desired precision is achieved.
Reference: [7] <author> Paolo Cignoni, Claudio Montani, Enrico Puppo, and Roberto Scopigno. </author> <title> Multiresolution representation and visualization of volume data. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <volume> 3(4), </volume> <month> December </month> <year> 1997. </year> <note> (In press). </note>
Reference-contexts: The method is fast but suffers from all the problems associated with subsampling problems, 11 and also it does not provide an estimate of the error. It is an output simplification approach rather than data simplification approach. Cignoni et al. <ref> [7] </ref> have developed algorithms to simplify curvilinear as well as tetrahedral grids. To simplify curvilinear datasets they use a refinement approach and for tetrahedral grids they have proposed a decimation approach.
Reference: [8] <author> Jonathan Cohen, Amitabh Varshney, Dinesh Manocha, Greg Turk, Hans Weber, Pankaj Agarwal, Fred-erick Brooks, and William Wright. </author> <title> Simplification envelopes. </title> <booktitle> In Proceedings of Visualization'96, </booktitle> <pages> pages 119-128, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: A surface simplification method that uses quadratic error metric is described by Garland and Heckbert [23]. Cohen et al. have proposed the idea of simplification envelopes for generating a hierarchy of level-of-detail approximations for a given polygonal model <ref> [8] </ref>. Their's is a general framework in which a large number of existing simplification algorithms can run with the guarantee that all points in an approximation lie within a user-specifiable distance * from the original model, while preserving global topology and sharp edges.
Reference: [9] <author> J. Danskin and Pat Hanrahan. </author> <title> Fast algorithms for volume ray tracing. </title> <booktitle> In Proceedings of 1992 Workshop on Volume Visualization, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: These methods are known to have a poor performance due to large number of cache misses. Although several optimizations have been suggested <ref> [9, 37] </ref>, this class of methods is still very slow.
Reference: [10] <author> T. D. DeRose, M. Lounsbery, and J. Warren. </author> <title> Multiresolution analysis for surfaces of arbitrary topological type. </title> <type> Technical Report 93-10-05, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, WA, </address> <year> 1993. </year>
Reference-contexts: Popovic and Hoppe [42] have extended the progressive meshes method to allow the genus to change during simplification of arbitrary polygonal models. A very different approach to providing many levels of detail from a complex model is described by DeRose et al. <ref> [10] </ref>. They provide a theoretically sound framework for multi-resolution models. A multi-resolution model consists of a simple base mesh, and a series of local correction terms called wavelet coefficients. These wavelets capture the details of the original mesh at various resolutions.
Reference: [11] <author> D. H. Douglas. </author> <title> Experiments to locate ridges and channels to create a new type of digital elevation model. </title> <journal> Cartographica, </journal> <volume> 23(4) </volume> <pages> 29-61, </pages> <year> 1986. </year>
Reference-contexts: Garland and Heckbert [22] investigated an importance measure based on a curvature measure. They reported their importance measure to be unsuccessful for curves because it generated poor approximations, however they did not test the curvature measure for surfaces. Douglas <ref> [11] </ref> and Southard [52] have also proposed methods that use feature identification and preservation. Lindstrom et al. [38] describe a real-time continuous level of detail algorithm for rendering height fields.
Reference: [12] <author> Robert A. Dreblin, Loren Carpenter, and Pat Hanrahan. </author> <title> Volume rendering. </title> <booktitle> In Proceedings of SIG-GRAPH'88, </booktitle> <volume> volume 22, </volume> <pages> pages 65-74, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Samples are taken along each ray to determine the color and opacity which are then integrated to compute the ray's contribution to the pixel. Each ray can be assumed to be a light ray that accumulates color and opacity as it passes through the volume's medium <ref> [48, 56, 12] </ref>. Image order methods are generally very expensive because they do not access the volume data in the storage order and spend most of their time in calculating the location of the sample points.
Reference: [13] <author> Mark Duchaineau, Murray Wolinsky, David E. Sigeti, Mark C. Miller, Charles Aldrich, and Mark B. Mineev-Weinstein. </author> <title> Roaming terrain: Real-time optimally adapting meshes. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 81-88, </pages> <month> October </month> <year> 1997. </year> <month> 49 </month>
Reference-contexts: Puppo [43] has introduced Multi-Triangulation (MT), a triangle based multiresolution decomposition to represent terrain surfaces at variable resolution. Floriani et al. [20] have extended the MT model to represent generic meshes of triangles in 3-D. Duchaineau et al. <ref> [13] </ref> have developed a terrain visualization system called ROAM, which depends on predefined multiresolution terrain representation used to build adaptive triangle meshes for each frame in an interactive fly-through. The triangles are always right-isosceles and hence the problem of slivers is reduced.
Reference: [14] <author> Nira Dyn and Davin Levin. </author> <title> Data dependent triangulations for piecewise linear interpolation. </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 10(1) </volume> <pages> 137-154, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation uses the height of the points in addition to their x and y coordinates. It can achieve lower error approximations than Delaunay triangulation, but the resulting triangulation has more sliver triangles <ref> [14, 45] </ref>. Some common approaches for approximating height fields are decimation, refinement, and multireso-lution. The decimation methods begin with a triangulation of all the input points and simplify the approximation by iteratively deleting vertices from the triangulation.
Reference: [15] <author> Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. </author> <title> Multiresolution analysis of arbitrary meshes. </title> <booktitle> In Proceedings of SIGGRAPH'95, </booktitle> <pages> pages 173-182, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: These wavelets capture the details of the original mesh at various resolutions. DeRose's technique can only be used on a mesh which can be obtained from a base mesh by 4-to-1 splits. Eck et al. present a three-step approach to multi-resolution analysis which overcomes this limitation <ref> [15] </ref>. A mesh is partitioned into triangular regions first. Then a parameterization of each face is created using a wavelet decomposition. The parameterization is such that all faces fit together seamlessly. Finally, the triangle faces are recursively split to the desired level of detail.
Reference: [16] <author> H. Edelsbrunner. </author> <title> Dynamic data structures for orthogonal intersection queries. </title> <type> Technical Report F59, </type> <institution> Inst. Informationsverarb,, Tech. Univ. Graz, Graz, Austria, </institution> <year> 1980. </year>
Reference-contexts: They have proposed two global error estimates and define a joint global error by combining the two estimates by choosing the larger of the two. They also use a multiresolution representation in an interval tree data structure <ref> [16] </ref> using which they are able to extract a mesh representation efficiently at any specified error tolerance. Klien et al. have presented a way to achieve global error bounds using Hausdroff distance between the original and the simplified mesh as a meaningful geometric error value [33].
Reference: [17] <author> Jihad El-Sana and Amitabh Varshney. </author> <title> Controlled simplification of genus for polygonal models. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 403-410, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: One of the restrictions of a method like this is that it works only for closed surfaces. They have suggested that open surface meshes can be closed by dummy patches prior to using their method. El-Sana and Varshney <ref> [17] </ref> have proposed a scheme to compute approximations of polygonal CAD models by simplifying the genus of the models during simplification. Popovic and Hoppe [42] have extended the progressive meshes method to allow the genus to change during simplification of arbitrary polygonal models.
Reference: [18] <author> Francine Evans, Steven Skiena, and Amitabh Varshney. </author> <title> Optimizing triangle strips for fast rendering. </title> <booktitle> In Proceedings of Visualization '96, </booktitle> <pages> pages 319-326, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: They use a greedy algorithm to generate triangle strips at every frame and found that their algorithm produces strips of adequate length. Evans et al. have reported a method to construct triangle strips from partially triangulated models <ref> [18] </ref>. Rossignac and Borrel present a vertex clustering method which is independent of the original topology of the mesh [47]. The model is represented by a vertex table and a polygon table which contains indices into the vertex table.
Reference: [19] <author> Leila De Floriani. </author> <title> A pyramidal structure for triangle-based surface description. </title> <journal> IEEE Computer Graphics & Applications, </journal> <volume> 9(2) </volume> <pages> 67-78, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: They describe methods to detect the features, as those mentioned above, both manually and automatically. Scarlatos and Pavlidis describe a method to construct hierarchical triangulations using terrain features [49]. Floriani has defined a hierarchical model of a 2 1 2 dimensional surface based on a Delaunay triangulation <ref> [19] </ref>. Her model, called a Delaunay Pyramid, consists of a sequence of Delaunay triangulations of a given set of points linked together, to form a pyramid. The pyramid represents a surface at increasingly finer levels of detail.
Reference: [20] <author> Leila De Floriani, Paola Magilo, and Enrico Puppo. </author> <title> Building and traversing a surface at variable resolution. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 103-110, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: Certain et al. extended multi-resolution analysis to represent both color and geometry in multi-resolution meshes which provide interactive speeds [3]. Puppo [43] has introduced Multi-Triangulation (MT), a triangle based multiresolution decomposition to represent terrain surfaces at variable resolution. Floriani et al. <ref> [20] </ref> have extended the MT model to represent generic meshes of triangles in 3-D. Duchaineau et al. [13] have developed a terrain visualization system called ROAM, which depends on predefined multiresolution terrain representation used to build adaptive triangle meshes for each frame in an interactive fly-through.
Reference: [21] <author> R. L. Fowler and J. J Little. </author> <title> Automatic extraction of irregular network digital terrain models. </title> <booktitle> In Proceedings of SIGGRAPH' 79, </booktitle> <volume> volume 13, </volume> <pages> pages 199-207, </pages> <month> August </month> <year> 1979. </year>
Reference-contexts: Simply subsampling grid elevation values produces a coarser level of detail model, whereas the triangulated irregular network models like those described by Garland [22] and Fowler <ref> [21] </ref> require complete retriangulation in order to generate multiple levels of detail. Lindstrom's method produces continuous changes between different levels of detail.
Reference: [22] <author> Michael Garland and Paul S. Heckbert. </author> <title> Fast polygonal approximation of terrains and height fields. </title> <type> Technical Report CMU-CS-95-181, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Height field simplification 2. Surface simplification 3. Volume simplification 2.1 Height Field Simplification A height field is a set of height samples over a planar domain <ref> [22] </ref>. The height samples are mostly terrain data used in applications such as flight simulation and ground vehicle simulators. The height samples could also represent range data acquired by stereo and laser range finders and is used in computer vision. <p> Also, since their method solves an optimization problem, the simplification times are long. Their method falls under the category of decimation methods. Garland and Heckbert's survey paper on height field approximation <ref> [22] </ref> is an excellent reference. They have adopted a refinement approach to height field data simplification and found that the most successful method, among those that they examined, was a greedy insertion algorithm. <p> In addition to the approaches described above there are methods that are called feature methods. They are one pass methods which select a set of important feature points (such as peaks, ridges, pits, and valleys for terrain data) and use them as vertex set for triangulation. Garland and Heckbert <ref> [22] </ref> investigated an importance measure based on a curvature measure. They reported their importance measure to be unsuccessful for curves because it generated poor approximations, however they did not test the curvature measure for surfaces. Douglas [11] and Southard [52] have also proposed methods that use feature identification and preservation. <p> The advantage of using regular grid representations is that they are more compact, and allow for easier construction of a multiple level of detail hierarchy. Simply subsampling grid elevation values produces a coarser level of detail model, whereas the triangulated irregular network models like those described by Garland <ref> [22] </ref> and Fowler [21] require complete retriangulation in order to generate multiple levels of detail. Lindstrom's method produces continuous changes between different levels of detail.
Reference: [23] <author> Michael Garland and Paul S. Heckbert. </author> <title> Surface simplification using quadratic error metrics. </title> <booktitle> In Proceedings of SIGGRAPH'97, </booktitle> <pages> pages 209-216, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Klien et al. have presented a way to achieve global error bounds using Hausdroff distance between the original and the simplified mesh as a meaningful geometric error value [33]. A surface simplification method that uses quadratic error metric is described by Garland and Heckbert <ref> [23] </ref>. Cohen et al. have proposed the idea of simplification envelopes for generating a hierarchy of level-of-detail approximations for a given polygonal model [8].
Reference: [24] <author> Tran S. Gieng, Bernd Hamann, Kenneth I. Joy, Gregroy L. Schussman, and Issac J. Trotts. </author> <title> Smooth hierarchical surface triangulations. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 379-386, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: HDS operates dynamically, retessellating the scene continuously as the user's view position shifts, and adaptively, processing the entire database without first decomposing the environment into individual objects. This allows real-time interaction with the CAD model. Gieng et al. <ref> [24] </ref> have presented a method to compute multiresolution representation of surfaces meshes using an error metric based on approximation of the local curvature at the centroid of each triangle in the mesh.
Reference: [25] <author> Leonidas Guibas and Jorge Stolfi. </author> <title> Primitives for the manipulation of general subdivisions and the computation of voronoi diagrams. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 4(2) </volume> <pages> 75-123, </pages> <year> 1985. </year>
Reference-contexts: The terrain is most often represented as a triangular mesh because almost all graphics hardware uses the triangle as the most fundamental primitive for object description. A height field can be triangulated by using a scheme like Delaunay triangulation <ref> [25] </ref>. This approach leads to an explosion in the number of triangles that need to be rendered. Even with state of the art graphics hardware it soon becomes prohibitively expensive to render a terrain of any reasonable size. <p> Most methods employ general triangulations like Delaunay triangulation or data dependent triangulation. Delaunay triangulation is a purely two dimensional method and it uses only the xy projection of the input points. Delaunay triangulations maximize the minimum angle of all triangles, among all triangulations of a given point set <ref> [25] </ref>. This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation uses the height of the points in addition to their x and y coordinates. It can achieve lower error approximations than Delaunay triangulation, but the resulting triangulation has more sliver triangles [14, 45].
Reference: [26] <author> Taosong He, Lichan Hong, and Amitabh Varshney. </author> <title> Controlled topology simplification. </title> <journal> IEEE Transactions on visualization and Computer Graphics, </journal> <volume> 2(2) </volume> <pages> 171-183, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Furthermore, all these methods work by iteratively modifying the mesh by edge collapse/vertex split operations. He et al. have presented a method based on converting an object into multi-resolution volume rasters using a controlled filtering and sampling technique <ref> [26] </ref>. It is sometimes desirable to simplify the topology of a mesh. Consider, for example, a fly-through CAD model. A tiny hole on the surface of a mechanical part will gradually disappear as the viewer moves away from the part. <p> A topology preserving method would limit the amount the simplification that can be achieved in such a situation. Another disadvantage is that rendering of a simplified object retaining high frequency features will increase aliasing problems. He et al. <ref> [26] </ref> have proposed a voxel-based topology simplification algorithm for the generation of a multi-resolution object hierarchy. They first convert the input closed surface into 3-D volume rasters by applying a controlled filtering and sampling technique. A surface fitting technique like Marching Cubes can then be applied to produce simplified meshes.
Reference: [27] <author> Paul Hinker and Charles Hansen. </author> <title> Geometric optimization. </title> <booktitle> In Proceedings of Visualization'93, </booktitle> <pages> pages 189-195, </pages> <year> 1993. </year>
Reference-contexts: The vertices in the simplified mesh are a subset of the original. Simplification envelopes can be constructed only for orientable surfaces. Hinker and Hansel presented a method called geometric optimization <ref> [27] </ref> which attempts to construct sets of adjacent polygons that are nearly co-planar. These polygons can then be replaced with one large polygon. They have three approaches for building these sets. The first is to bucket sort the polygons based on their normals.
Reference: [28] <author> Hughes Hoppe, Tony De Rose, TomDuchamp, John McDonald, and Werner Stuetzle. </author> <title> Mesh optimization. </title> <booktitle> In Proceedings of SIGGRAPH' 93, </booktitle> <pages> pages 19-26, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The original points are removed one at a time, making local adjustments to preserve the topology of the surface. Hoppe et al. formulate mesh simplification as an optimization problem with an energy function that directly measures deviation of the final mesh from the original <ref> [28] </ref>. The energy function consists of three terms that address different characteristics of the mesh. The first term is an energy function that measures the distance of the solution from the original mesh. The next term minimizes the number of vertices used in the final representation of the mesh. <p> The vertices for the large polygons are a subset of the vertices of all the polygons in the set that they replace. Hoppe et al. have reported a mesh optimization method <ref> [28] </ref>. Their method was discussed in the context of height field simplification (section 2.1) and also works for more complex surface meshes. This technique attempts to minimize a surface mesh based on an energy function as discussed earlier. Their method does not guarantee a minimum. <p> An arbitrary triangle mesh can be simplified into the coarse mesh and a series of detail records by using a version of mesh optimization method as described in his earlier work <ref> [28] </ref>. This method seeks to preserve not only the geometry of the mesh surface, but also its overall appearance, as defined by the discrete and scalar attributes (like texture) associated with its surface. Unlike Ciampalini's method (described in [5]), the Progressive Mesh construction algorithm does not guarantee maximum error bounds.
Reference: [29] <author> Hugues Hoppe. </author> <title> Progressive meshes. </title> <booktitle> In Proceedings of SIGGRAPH'96, </booktitle> <pages> pages 99-108, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: This technique attempts to minimize a surface mesh based on an energy function as discussed earlier. Their method does not guarantee a minimum. Hoppe later refined his technique and introduced a mesh representation which he calls Progressive Meshes <ref> [29] </ref>. In this form a detailed mesh is stored as a coarse mesh, and a sequence of detail records, which describe how to transform the coarse mesh into detailed mesh.
Reference: [30] <author> Hugues Hoppe. </author> <title> View-dependent refinement of progressive meshes. </title> <booktitle> In Proceedings of SIGGRAPH'97, </booktitle> <pages> pages 189-198, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The Progressive Mesh representation also supports selective refinement, whereby detail is added only to the selected areas. He has also introduced a framework for selectively refining an arbitrary Progressive Mesh according to changing viewing parameters in real-time <ref> [30] </ref>. This latter work is based on Xia and Varshney's work on view-dependent simplification of polygonal models [65]. Xia and Varshney have introduced the notion of image-space-guided simplification. <p> Selective refinement is applied based on viewing direction, lighting, and visibility. They maintain an active list of vertices and triangles which is updated using frame-to-frame coherence to achieve real-time performance. Since most graphics hardware can render triangle strips very fast, Hoppe <ref> [30] </ref> generates triangle strips at every frame. Since their incremental refining scheme is dynamic, it is not possible for them to precompute optimal triangle strips. They use a greedy algorithm to generate triangle strips at every frame and found that their algorithm produces strips of adequate length.
Reference: [31] <author> Alan D. Kalvin and Russell H. Taylor. Superfaces: </author> <title> polygonal mesh simplification with bounded error. </title> <journal> IEEE Computer Graphics & Applications, </journal> <volume> 16(3) </volume> <pages> 64-77, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: One of the drawbacks of using such decimation criteria is that the distances are measured relative to intermediate mesh approximations, not the original mesh. The simplified mesh has vertices which are a subset of the original ones. A general decimation, called superfaces, is proposed by Kalvin and Taylor <ref> [31] </ref>. Superfaces works for any planar polygon and not just triangles. It preserves the topological features of the original mesh. It works by 7 grouping polygons into a set of superface patches. These superface patches are non-planar polygons whose vertices are the boundary vertices of the polygon mesh it represents.
Reference: [32] <author> F. Klein and O. Kubler. </author> <title> A prebuffer algorithm for instant display of volume data. </title> <booktitle> In Proceedings of SPIE (Architectures and Algorithms for Digital Image Processing), </booktitle> <volume> volume 596, </volume> <pages> pages 54-58, </pages> <year> 1985. </year>
Reference-contexts: Software implementation of the scan conversion is expensive, but the polygon scan conversion can be accelerated using graphics hardware available on current workstations. 1.2 Shear Warp Volume rendering algorithms based on shear-warp factorization combine the advantages of image-order and object-order algorithms <ref> [32, 67] </ref>. The general method in this class of algorithms factorizes the viewing matrix into a 3D shear parallel to the slices of the volume data. The sheared volume is then projected on a plane parallel to the plane of the voxel slices to form an intermediate distorted image.
Reference: [33] <author> Reinhard Klein, Gunther Liebich, and W. Straer. </author> <title> Mesh reduction with error control. </title> <booktitle> In Proceedings of Visualization '96, </booktitle> <pages> pages 311-318, </pages> <month> October </month> <year> 1996. </year> <month> 50 </month>
Reference-contexts: Klien et al. have presented a way to achieve global error bounds using Hausdroff distance between the original and the simplified mesh as a meaningful geometric error value <ref> [33] </ref>. A surface simplification method that uses quadratic error metric is described by Garland and Heckbert [23]. Cohen et al. have proposed the idea of simplification envelopes for generating a hierarchy of level-of-detail approximations for a given polygonal model [8].
Reference: [34] <author> Philippe Lacroute and Mark Levoy. </author> <title> Fast volume rendering using a shear-warp factorization of the viewing transformation. </title> <booktitle> In Proceedings of SIGGRAPH'94, </booktitle> <volume> volume 28, </volume> <pages> pages 451-458, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The shear-warp factorization was first used to simplify data communication patterns in volume rendering algorithms for SIMD machines [2, 50]. Lacroute and Levoy <ref> [34] </ref> have used the shear-warp factorization in their volume rendering algorithms which are very fast. They are able to achieve almost interactive speeds.
Reference: [35] <author> Philippe G. Lacroute. </author> <title> Fast Volume Rendering Using a Shear-Warp Factorization of the Viewing Transformation. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> September </month> <year> 1995. </year>
Reference: [36] <author> D. Laur and P. Hanrahan. </author> <title> Hierarchical splatting: A progressive refinement algorithm for volume rendering. </title> <booktitle> In Proceedings of SIGGRAPH'91, </booktitle> <volume> volume 25, </volume> <pages> pages 163-170, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Furthermore, if the grid is not regular then these optimizations fail. 1.1.2 Object Order Methods Object order methods process volume elements (voxels) one by one in their memory order by projecting them onto the 2D image using splatting <ref> [59, 60, 36, 54] </ref> or cell projection [61] methods. This solves the memory access problem of the image order methods. Splatting algorithms compute the contribution of the voxel to the image by convolving the voxel with a filter that distributes the voxel's value to a neighborhood of pixels. <p> Also, such methods are not adaptive because they do not allow maintaining variable density of data over different regions of the volume and suffer from severe aliasing problems. A pyramidal representation of volume datasets defined on regular grids has been proposed by Laur and Hanrahan <ref> [36] </ref>. A number of optimizations have been proposed for regular volume datasets. In order to speed up rendering, these methods try to avoid analyzing parts of data. An example is the use of octree representation to speed up isosurface generation [62].
Reference: [37] <author> Mark Levoy. </author> <title> Efficient ray tracing of volume data. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 9(3) </volume> <pages> 245-261, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: These methods are known to have a poor performance due to large number of cache misses. Although several optimizations have been suggested <ref> [9, 37] </ref>, this class of methods is still very slow.
Reference: [38] <author> Peter Lindstrom, David Koller, William Ribarsky, Larry F. Hodges, Nick Faust, and Gregory A. Turner. </author> <title> Real-time, continuous level of detail rendering of height fields. </title> <booktitle> In Proceedings of SIGGRAPH'96, </booktitle> <pages> pages 109-119, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: They reported their importance measure to be unsuccessful for curves because it generated poor approximations, however they did not test the curvature measure for surfaces. Douglas [11] and Southard [52] have also proposed methods that use feature identification and preservation. Lindstrom et al. <ref> [38] </ref> describe a real-time continuous level of detail algorithm for rendering height fields. Their algorithm uses a compact regular grid representation, and employs a variable screen space threshold to bound the maximum error of the projected image. <p> The limitation, of course, is that the method works only for regular grids. Zhou et al. [68] use a multiresolution tetrahedral framework for visualizing regular grids. Their method is similar to Lindstrom's method of simplifying height fields <ref> [38] </ref>. The method starts with a regular grid. After an initial tetrahedralization of the grid, it builds a level-of-detail approximation which consists of uniformly represented tetrahedra.
Reference: [39] <author> W. E. Lorensen and H. E. Cline. </author> <title> Marching cubes: A high resolution 3d surface construction algorithm. </title> <journal> In Computer Graphics, </journal> <volume> volume 21, </volume> <pages> pages 163-169, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Huge surface meshes are produced in many application areas. In the field of volume visualization, an isosurface extraction algorithm called Marching Cubes <ref> [39] </ref> is known to produce 500k to 2000k triangles from a moderately sized volume dataset [51].
Reference: [40] <author> David Luebke and Carl Erikson. </author> <title> View-dependent simplification of arbitrary polygonal environments. </title> <booktitle> In Proceedings of SIGGRAPH'97, </booktitle> <pages> pages 199-208, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The system maintains a Delaunay triangulation at all times and compresses the texture using a progressive wavelet scheme. Luebke et al. <ref> [40] </ref> have proposed a method called Hierarchical Dynamical Simplification (HDS), which is well suited for large and complex CAD models consisting of thousands of parts of hundreds of thousands of polygons.
Reference: [41] <author> Jr. Michael J. DeHaemer and Michael J. Zyda. </author> <title> Simplification of objects rendered by polygonal approximations. </title> <journal> Computers & Graphics, </journal> <volume> 15(2) </volume> <pages> 175-184, </pages> <year> 1991. </year>
Reference-contexts: A 3D scanner outputs a large number of points which must be connected to form surfaces. Due to the nature of the scanner, the samples are taken at regular space intervals. DeHaemer and Zyda present several surface construction methods which attempt to minimize the number of polygons created <ref> [41] </ref>. The methods proposed are grouped into adaptive subdivision methods and polygon growth methods. The adaptive subdivision methods refine a coarse grid incrementally and the polygon growth methods start with the finest data and iteratively group small polygons into larger ones.
Reference: [42] <author> Jovan Popovic and Hugues Hoppe. </author> <title> Progressive simplical complexes. </title> <booktitle> In Proceedings of SIGGRAPH'97, </booktitle> <pages> pages 217-224, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: They have suggested that open surface meshes can be closed by dummy patches prior to using their method. El-Sana and Varshney [17] have proposed a scheme to compute approximations of polygonal CAD models by simplifying the genus of the models during simplification. Popovic and Hoppe <ref> [42] </ref> have extended the progressive meshes method to allow the genus to change during simplification of arbitrary polygonal models. A very different approach to providing many levels of detail from a complex model is described by DeRose et al. [10]. They provide a theoretically sound framework for multi-resolution models.
Reference: [43] <author> Enrico Puppo. </author> <title> Variable resolution terrain surfaces. </title> <booktitle> In Proceedings of Eighth Canadian Conference on Computational Geometry, </booktitle> <pages> pages 202-210, </pages> <year> 1996. </year>
Reference-contexts: The parameterization is such that all faces fit together seamlessly. Finally, the triangle faces are recursively split to the desired level of detail. Certain et al. extended multi-resolution analysis to represent both color and geometry in multi-resolution meshes which provide interactive speeds [3]. Puppo <ref> [43] </ref> has introduced Multi-Triangulation (MT), a triangle based multiresolution decomposition to represent terrain surfaces at variable resolution. Floriani et al. [20] have extended the MT model to represent generic meshes of triangles in 3-D.
Reference: [44] <author> Boris Rabinovich and Craig Gotsman. </author> <title> Visualization of large terrains in resource-limited computing environments. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 95-102, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: The triangles are always right-isosceles and hence the problem of slivers is reduced. The constraint is that the mesh should be representable or approximated as a mesh of regular triangles. ROAM guarantees global error bounds, and uses view dependent error metrics. Rabinovich and Gotsman <ref> [44] </ref> have developed a system for visualization of large terrains in a resource limited environment. Their system can handle large terrain databases several orders of magnitude larger than the computer's memory.
Reference: [45] <author> Shmuel Rippa. </author> <title> Adaptive approximation by piecewise linear polynomials on triangulations of subsets of scattered data. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 13(5) </volume> <pages> 1123-1141, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: This helps to minimize the occurrence of very thin sliver triangles. Data-dependent triangulation uses the height of the points in addition to their x and y coordinates. It can achieve lower error approximations than Delaunay triangulation, but the resulting triangulation has more sliver triangles <ref> [14, 45] </ref>. Some common approaches for approximating height fields are decimation, refinement, and multireso-lution. The decimation methods begin with a triangulation of all the input points and simplify the approximation by iteratively deleting vertices from the triangulation.
Reference: [46] <author> Remi Ronfard and Jarek Rossignac. </author> <title> Full-range approximation of triangulated polyhedra. </title> <booktitle> In Computer Graphics Forum, Proceedings of EUROGRAPHICS'96, </booktitle> <pages> pages 67-76, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Finally the normals must be recalculated. Compared to other methods, it can achieve high compression ratios at low computational cost. The disadvantage is that it offers minimum control over the quality of the approximation. Ronfard and Rossignac present a solution which offers more control over the quality of simplification <ref> [46] </ref>. In a pre-processing step, a cost is associated with each edge in the mesh and the edges are inserted in a priority queue. Edges are then collapsed one at a time by moving one vertex to the location of the other.
Reference: [47] <author> J. Rossignac and P. Borrel. </author> <title> Geometric Modeling in Computer Graphics, chapter Multi-resolution 3D Approximations for Rendering Complex Scenes, </title> <address> pages 455-465. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Evans et al. have reported a method to construct triangle strips from partially triangulated models [18]. Rossignac and Borrel present a vertex clustering method which is independent of the original topology of the mesh <ref> [47] </ref>. The model is represented by a vertex table and a polygon table which contains indices into the vertex table. The simplification process requires the following steps: grading the vertices, clustering the vertices, choosing representative vertices, eliminating triangles, and adjusting normals. All vertices are graded based on their importance.
Reference: [48] <author> P. Sabella. </author> <title> A rendering algorithm for visualization of 3d scalar fields. </title> <booktitle> In Proceedings of SIGGRAPH'88, </booktitle> <volume> volume 22, </volume> <pages> pages 51-58, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Samples are taken along each ray to determine the color and opacity which are then integrated to compute the ray's contribution to the pixel. Each ray can be assumed to be a light ray that accumulates color and opacity as it passes through the volume's medium <ref> [48, 56, 12] </ref>. Image order methods are generally very expensive because they do not access the volume data in the storage order and spend most of their time in calculating the location of the sample points.
Reference: [49] <author> Lori Scarlatos and Theo Pavlidis. </author> <title> Hierarchical triangulation using terrain features. </title> <booktitle> In Proceedings of Visualization'90, </booktitle> <pages> pages 168-175, </pages> <year> 1990. </year>
Reference-contexts: Refinement methods are the opposite 4 of decimation methods and start from a simple model, iteratively adding more points to the triangulation. Hierarchical methods are based on quadtree, k-d tree [53], or hierarchical triangulations <ref> [49] </ref>. Turk presents a method called re-tiling [55]. Re-tiling creates a triangulation of a surface with a user specified number of vertices. The first step is to choose a set of points which will become the vertices of a new mesh. <p> A method that uses features in the data to do constrained identification is proposed by Chen and Francis [4]. They describe methods to detect the features, as those mentioned above, both manually and automatically. Scarlatos and Pavlidis describe a method to construct hierarchical triangulations using terrain features <ref> [49] </ref>. Floriani has defined a hierarchical model of a 2 1 2 dimensional surface based on a Delaunay triangulation [19]. Her model, called a Delaunay Pyramid, consists of a sequence of Delaunay triangulations of a given set of points linked together, to form a pyramid.
Reference: [50] <author> Peter Schroder and Gordon Stoll. </author> <title> Data parallel volume rendering as line drawing. </title> <booktitle> In Proceedings of the 1992 Workshop on Volume Visualization, </booktitle> <pages> pages 25-32, </pages> <address> Boston, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The shear-warp factorization was first used to simplify data communication patterns in volume rendering algorithms for SIMD machines <ref> [2, 50] </ref>. Lacroute and Levoy [34] have used the shear-warp factorization in their volume rendering algorithms which are very fast. They are able to achieve almost interactive speeds.
Reference: [51] <author> William J. Schroder, Jonathan A. Zarge, and William E. Lorensen. </author> <title> Decimation of triangle meshes. </title> <booktitle> In Proceedings of SIGGRAPH'92, </booktitle> <volume> volume 26, </volume> <pages> pages 65-60, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Huge surface meshes are produced in many application areas. In the field of volume visualization, an isosurface extraction algorithm called Marching Cubes [39] is known to produce 500k to 2000k triangles from a moderately sized volume dataset <ref> [51] </ref>. <p> This approach organizes the volume datasets into a hierarchical representation, and then fits isosurfaces onto the simplified datasets. Any reduced mesh must meet two criteria <ref> [51] </ref>. First, as said earlier, the reduced mesh must have the same topology as the original. Second, the reduced mesh must be a good geometric approximation to the original surface mesh. <p> Several criteria to get a measure of the deviation of the approximated mesh from the original have been proposed by researchers and we will discuss some of them in the rest of this section. Schroder et al. describe a decimation method for triangle meshes <ref> [51] </ref>. Their method uses local operations on geometry and topology to reduce the number of triangles in a triangle mesh. As a first step they evaluate the local geometry and topology around a vertex. <p> Algorri and Schmitt have proposed a similar method where vertices are grouped into clusters, which are well characterized regions that can successfully accept simplification operations [1]. Ciampalini et al. [5] have proposed a decimation method based on Schroder's work <ref> [51] </ref>. Following Schroder, they also classify vertices based on local geometry and topology, but the decimation criterion for choosing which vertices to delete is based on a global approximation of the error.
Reference: [52] <author> D. A. Southard. </author> <title> Piecewise planar surface models from sampled data. </title> <journal> Scientific Visualization of Physical Phenomenon, </journal> <pages> pages 667-680, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Garland and Heckbert [22] investigated an importance measure based on a curvature measure. They reported their importance measure to be unsuccessful for curves because it generated poor approximations, however they did not test the curvature measure for surfaces. Douglas [11] and Southard <ref> [52] </ref> have also proposed methods that use feature identification and preservation. Lindstrom et al. [38] describe a real-time continuous level of detail algorithm for rendering height fields.
Reference: [53] <author> K. R. Subramanian and Donald S. Fussell. </author> <title> Applying space subdivision techniques to volume rendering. </title> <booktitle> In Proceedings of Visualization'90, </booktitle> <pages> pages 150-159, </pages> <month> October </month> <year> 1990. </year> <month> 51 </month>
Reference-contexts: Refinement methods are the opposite 4 of decimation methods and start from a simple model, iteratively adding more points to the triangulation. Hierarchical methods are based on quadtree, k-d tree <ref> [53] </ref>, or hierarchical triangulations [49]. Turk presents a method called re-tiling [55]. Re-tiling creates a triangulation of a surface with a user specified number of vertices. The first step is to choose a set of points which will become the vertices of a new mesh.
Reference: [54] <author> D. Tost, A. Puig, and I. Navazo. </author> <title> A volume visualization algorithm using a coherent extended weight matrix. </title> <journal> Computers & Graphics, </journal> <volume> 19(1) </volume> <pages> 37-45, </pages> <year> 1995. </year>
Reference-contexts: Furthermore, if the grid is not regular then these optimizations fail. 1.1.2 Object Order Methods Object order methods process volume elements (voxels) one by one in their memory order by projecting them onto the 2D image using splatting <ref> [59, 60, 36, 54] </ref> or cell projection [61] methods. This solves the memory access problem of the image order methods. Splatting algorithms compute the contribution of the voxel to the image by convolving the voxel with a filter that distributes the voxel's value to a neighborhood of pixels.
Reference: [55] <author> Greg Turk. </author> <title> Re-tiling polygonal surfaces. </title> <booktitle> In Proceedings of Visualization'92, </booktitle> <pages> pages 55-64, </pages> <year> 1992. </year>
Reference-contexts: Refinement methods are the opposite 4 of decimation methods and start from a simple model, iteratively adding more points to the triangulation. Hierarchical methods are based on quadtree, k-d tree [53], or hierarchical triangulations [49]. Turk presents a method called re-tiling <ref> [55] </ref>. Re-tiling creates a triangulation of a surface with a user specified number of vertices. The first step is to choose a set of points which will become the vertices of a new mesh. These points are chosen within the plane of each polygon in the surface.
Reference: [56] <author> Craig Upson and Michael Keeler. V-buffer: </author> <title> Visible volume rendering. </title> <booktitle> In Proceedings of SIGGRAPH'88, </booktitle> <volume> volume 22, </volume> <pages> pages 59-64, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Samples are taken along each ray to determine the color and opacity which are then integrated to compute the ray's contribution to the pixel. Each ray can be assumed to be a light ray that accumulates color and opacity as it passes through the volume's medium <ref> [48, 56, 12] </ref>. Image order methods are generally very expensive because they do not access the volume data in the storage order and spend most of their time in calculating the location of the sample points.
Reference: [57] <author> Pamela P. Waltka, Jean Clucas, R Kevin McCabe, Todd Plessel, and Rick Potter. </author> <title> Fast user guide. </title> <institution> NASA Ames Research Center: NAS Division, RND Branch, </institution> <month> june </month> <year> 1993. </year> <note> version 1.1. </note>
Reference-contexts: The data is presented to us as an unstructured grid file (NASA unstruct format <ref> [57] </ref>) and is in the form of an array of tetrahedra. For each vertex we need the following information: * a list of tetrahedra incident on each vertex * a list of neighboring vertices for each vertex We build two tables which we call triangle table and tetrahedron table.
Reference: [58] <author> Ruediger Westermann. </author> <title> A multiresolution framework for volume rendering. </title> <booktitle> In Proceedings of 1996 Symposium on Volume Visualization, </booktitle> <pages> pages 51-58, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Each node of the tree stores a model of the data in terms of a fixed number of basis functions, a measure of the error in that model, and an importance measure. These are used to selectively traverse the tree during rendering. Westermann <ref> [58] </ref> has proposed a multiresolution framework for volume rendering of regular grids. His method first projects the volume rendering integral into a wavelet basis and performs the final integration process directly on the resulting wavelet coefficients.
Reference: [59] <author> L. Westover. </author> <title> Interactive volume rendering. </title> <booktitle> In Chapel Hill Workshop on Volume Visualization, </booktitle> <pages> pages 9-16. </pages> <address> Chapel Hill, NC, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Furthermore, if the grid is not regular then these optimizations fail. 1.1.2 Object Order Methods Object order methods process volume elements (voxels) one by one in their memory order by projecting them onto the 2D image using splatting <ref> [59, 60, 36, 54] </ref> or cell projection [61] methods. This solves the memory access problem of the image order methods. Splatting algorithms compute the contribution of the voxel to the image by convolving the voxel with a filter that distributes the voxel's value to a neighborhood of pixels.
Reference: [60] <author> Lee Westover. </author> <title> Footprint evaluation for volume rendering. </title> <booktitle> In Proceedings of SIGGRAPH'90, </booktitle> <volume> volume 24, </volume> <pages> pages 267-276, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Furthermore, if the grid is not regular then these optimizations fail. 1.1.2 Object Order Methods Object order methods process volume elements (voxels) one by one in their memory order by projecting them onto the 2D image using splatting <ref> [59, 60, 36, 54] </ref> or cell projection [61] methods. This solves the memory access problem of the image order methods. Splatting algorithms compute the contribution of the voxel to the image by convolving the voxel with a filter that distributes the voxel's value to a neighborhood of pixels.
Reference: [61] <author> Jane Wilhelms and Allen Van Gelder. </author> <title> A coherent projection approach for direct volume rendering. </title> <booktitle> In Proceedings of SIGGRAPH'91, </booktitle> <volume> volume 35, </volume> <pages> pages 275-284, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Several images can be generated to explore the volume dataset by changing the position of the projection plane. One such model is discussed in Wilhelms and Van Gelder's work on coherent projection methods for volume rendering <ref> [61] </ref>. Volume rendering methods can be broadly classified into image-order and object-order methods based on the way the points in the volume dataset are traversed. 1.1.1 Image Order Methods Image order methods are also called ray-casting methods. <p> Furthermore, if the grid is not regular then these optimizations fail. 1.1.2 Object Order Methods Object order methods process volume elements (voxels) one by one in their memory order by projecting them onto the 2D image using splatting [59, 60, 36, 54] or cell projection <ref> [61] </ref> methods. This solves the memory access problem of the image order methods. Splatting algorithms compute the contribution of the voxel to the image by convolving the voxel with a filter that distributes the voxel's value to a neighborhood of pixels.
Reference: [62] <author> Jane Wilhelms and Allen Van Gelder. </author> <title> Octrees for faster isosurface generation. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 11(3) </volume> <pages> 201-227, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: A number of optimizations have been proposed for regular volume datasets. In order to speed up rendering, these methods try to avoid analyzing parts of data. An example is the use of octree representation to speed up isosurface generation <ref> [62] </ref>. An approach to the representation of the regular volume datasets based on hierarchical recursive partitioning has been proposed by Wilhelms and Van Gelder [63]. They partition the volume using a k-d tree.
Reference: [63] <author> Jane Wilhelms and Allen Van Gelder. </author> <title> Multi-dimensional trees for controlled volume rendering and compression. </title> <booktitle> In Proceedings of 1994 Symposium on Volume Visualization, </booktitle> <pages> pages 27-43, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Gieng et al. [24] have presented a method to compute multiresolution representation of surfaces meshes using an error metric based on approximation of the local curvature at the centroid of each triangle in the mesh. There are also methods that specialize in simplifying surfaces generated in volume rendering applications <ref> [63] </ref>. 2.3 Volume Simplification There has been little work done in the area of simplifying volume datasets. As compared to the number of methods available to simplify surface meshes, volume simplification is in its infancy. Volume datasets suffer from the same problem as the surface mesh data. <p> An example is the use of octree representation to speed up isosurface generation [62]. An approach to the representation of the regular volume datasets based on hierarchical recursive partitioning has been proposed by Wilhelms and Van Gelder <ref> [63] </ref>. They partition the volume using a k-d tree. Each node of the tree stores a model of the data in terms of a fixed number of basis functions, a measure of the error in that model, and an importance measure.
Reference: [64] <author> Jane Wilhelms, Allen Van Gelder, Paul Tarantino, and Jonathan Gibbs. </author> <title> Hierarchical and parallelizable direct volume rendering for irregular and multiple grids. </title> <booktitle> In Proceedings of Visualization'96, </booktitle> <pages> pages 57-64, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: We use a direct volume rendering system based on a generalized software scan conversion of polygons by Wilhelms et al. <ref> [64] </ref>, rather than the more conventional ray-casting, projection, or splatting based methods. Our renderer is a direct volume rendering system based on treating the faces of cells as independent polygons.
Reference: [65] <author> Julie C. Xia and Amitabh Varshney. </author> <title> Dynamic view-dependent simplification for polygonal models. </title> <booktitle> In Proceedings of Visualization '96, </booktitle> <pages> pages 327-334, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: He has also introduced a framework for selectively refining an arbitrary Progressive Mesh according to changing viewing parameters in real-time [30]. This latter work is based on Xia and Varshney's work on view-dependent simplification of polygonal models <ref> [65] </ref>. Xia and Varshney have introduced the notion of image-space-guided simplification.
Reference: [66] <author> R. Yagel, D. M. Reed, A. Law, Shi P. W., and N. Shareef. </author> <title> Hardware assisted volume rendering of unstructured grids by incremental slicing. </title> <booktitle> In Proceedings of 1996 Symposium on Volume Visualization, </booktitle> <pages> pages 55-62, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The vertices are inserted iteratively using Delaunay tetrahedralization. The problem with this method is that the error heuristic does not measure the error accurately. Yagel et al. have used hardware to speed up volume rendering of tetrahedral grids <ref> [66] </ref>. They have described a method to render tetrahedral grids based on incremental slicing and hardware polygon rendering.
Reference: [67] <author> J. Yla-Jaaski, F. Klein, and O. Kubler. </author> <title> Fast direct display of volume data for medical diagnosis. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 53(1) </booktitle> <pages> 7-18, </pages> <year> 1991. </year>
Reference-contexts: Software implementation of the scan conversion is expensive, but the polygon scan conversion can be accelerated using graphics hardware available on current workstations. 1.2 Shear Warp Volume rendering algorithms based on shear-warp factorization combine the advantages of image-order and object-order algorithms <ref> [32, 67] </ref>. The general method in this class of algorithms factorizes the viewing matrix into a 3D shear parallel to the slices of the volume data. The sheared volume is then projected on a plane parallel to the plane of the voxel slices to form an intermediate distorted image.
Reference: [68] <author> Yong Zhou, Baoquan Chen, and Arie Kaufman. </author> <title> Multiresolution tetrahedral framework for visualizing regular volume data. </title> <booktitle> In Proceedings of Visualization'97, </booktitle> <pages> pages 135-142, </pages> <month> October </month> <year> 1997. </year> <month> 52 </month>
Reference-contexts: Depending on the basis function chosen, a large number of coefficients can be neglected while still achieving accurate results. The limitation, of course, is that the method works only for regular grids. Zhou et al. <ref> [68] </ref> use a multiresolution tetrahedral framework for visualizing regular grids. Their method is similar to Lindstrom's method of simplifying height fields [38]. The method starts with a regular grid. After an initial tetrahedralization of the grid, it builds a level-of-detail approximation which consists of uniformly represented tetrahedra.
References-found: 68

