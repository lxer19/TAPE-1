URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.CIFEr.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: flawrence,gilesg@research.nj.nec.com, a.c.tsoi@uow.edu.au  
Phone: 1  2  
Title: Rule Inference for Financial Prediction using Recurrent Neural Networks, Rule Inference for Financial Prediction using
Author: C. Lee Giles fl Steve Lawrence y Ah Chung Tsoi 
Address: 4 Independence Way, Princeton, NJ 08540  Australia  
Affiliation: NEC Research Institute,  Department of Informatics, University of Wollongong,  
Note: Proceedings of IEEE/IAFE Conference on Computational Intelligence for Financial Engineering (CIFEr), IEEE, Piscataway, NJ, 1997, pp. 253259. Copyright IEEE.  
Abstract: This paper considers the prediction of noisy time series data, specifically, the prediction of foreign exchange rate data. A novel hybrid neural network algorithm for noisy time series prediction is presented which exhibits excellent performance on the problem. The method is motivated by consideration of how neural networks work, and by fundamental difficulties with random correlations when dealing with small sample sizes and high noise data. The method permits the inference and extraction of rules. One of the greatest complaints against neural networks is that it is hard to figure out exactly what they are doing this work provides one answer for the internal workings of the network. Furthermore, these rules can be used to gain insight into both the real world system and the predictor. This paper focuses on noisy time series prediction and rule inference use of the system in trading would typically involve the utilization of other financial indicators and domain knowledge.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Darken and J.E. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Neural Information Processing Systems 6 (a) (b) 4, </booktitle> <pages> pages 10091016. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d 1 )) (4) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes. <p> All nodes included a bias input which was part of the optimization process. Weights were initialized as shown in Haykin [4]. Target outputs were -0.8 and 0.8 using the tanh output activation function and we used the quadratic cost function. A search then converge learning rate schedule was used <ref> [1] </ref> with an initial learning rate was 0.5. 2.5 Non-stationarity The approach we use to handle non-stationarity is to build models based on short time periods only. There is a noise vs. non-stationarity tradeoff as the size of the training set is varied.
Reference: [2] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <address> 7(2/3):195226, </address> <year> 1991. </year>
Reference-contexts: A short temporal pattern can be difficult to distinguish from patterns due to random correlations although these random patterns may not occur in temporal order. Recurrent neural network (RNN) models employ feedback connections and have the potential to represent certain computational structures in a more parsimonious fashion <ref> [2] </ref>. Such models are suited to the detection of temporal patterns. However, for noisy time series prediction, training can be difficult, and random correlations with the most recent data can make predictability based on earlier data difficult. <p> Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d 1 )) (4) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes.
Reference: [3] <author> D. Farmer and J. Sidorowich. </author> <title> Predicting chaotic time series. </title> <journal> Physical Review Letters, </journal> <volume> 59:845848, </volume> <year> 1987. </year>
Reference-contexts: 1 Introduction 1.1 Prediction System Neural networks are considered by many to provide state-of-the-art solutions to noisy time series prediction problems such as financial prediction [10]. When using neural networks to predict noisy time series data, a delay embedding <ref> [3] </ref> of previous temporal inputs is typically mapped into a prediction. Neural network models can be grouped into two classes based on whether or not they address the temporal relationship of the inputs by maintaining an internal state. <p> Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d 1 )) (4) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes.
Reference: [4] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: This is problematic due to the increased dimensionality of the resulting network (cf. the curse of dimensionality). Delayed inputs are used for the other two input neurons, aiding in the training process. The networks were trained with backpropagation through time <ref> [4] </ref> for a total of 500 updates. Weights in the network were updated after all patterns were presented (batch update), as opposed to stochastic update where weights are updated after every pattern presentation (stochastic update typically performs poorly when the data is very noisy). <p> All inputs were normalized to zero mean and unit variance. All nodes included a bias input which was part of the optimization process. Weights were initialized as shown in Haykin <ref> [4] </ref>. Target outputs were -0.8 and 0.8 using the tanh output activation function and we used the quadratic cost function. <p> A confidence measure was calculated for every prediction based on the magnitude of the winning output and the difference between the winning output and the other output (for outputs transformed using the softmax transformation <ref> [4] </ref>). We assigned a confidence threshold which was used to reject predictions with confidence values below the threshold. We found that the classification error could be reduced to around 40% by rejecting all but the 10-20% of examples with the highest confidence.
Reference: [5] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1995. </year>
Reference-contexts: In other words, for similarity as measured using a metric in the input space S, the SOM attempts to preserve the similarity in the display space D. For more information on the SOM, see <ref> [5] </ref>. 2.4 Grammatical Inference Several recurrent neural network architectures have been used for grammatical inference [6]. It has been shown that a particular class of recurrent networks are at least Turing equivalent [12]. The recurrent neural network we have used for grammatical inference here is the Elman neural network.
Reference: [6] <author> Stefan C. Kremer. </author> <title> A Theory of Grammatical Induction in the Connectionist Paradigm. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Alberta, Edmonton, Alberta, </institution> <year> 1996. </year>
Reference-contexts: In other words, for similarity as measured using a metric in the input space S, the SOM attempts to preserve the similarity in the display space D. For more information on the SOM, see [5]. 2.4 Grammatical Inference Several recurrent neural network architectures have been used for grammatical inference <ref> [6] </ref>. It has been shown that a particular class of recurrent networks are at least Turing equivalent [12]. The recurrent neural network we have used for grammatical inference here is the Elman neural network. The Elman network has fully connected feedback among the hidden layer neurons.
Reference: [7] <author> A. Lapedes and R. Farber. </author> <title> Nonlinear signal processing using neural networks: Prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1987. </year>
Reference-contexts: As is usual, a delay embedding of this series is then considered <ref> [7] </ref>: X (k; d 1 ) = (x (k); x (k 1); x (k 2); : : : ;x (k d 1 + 1)) (3) where d 1 is the delay embedding dimension and is 1 or 2 for the experiments reported here.
Reference: [8] <author> B.G. Malkiel. </author> <title> Efficient Market Hypothesis. </title> <publisher> Macmillan, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: September 3, 1973 to May 18, 1987 (there are 3645 data points), and is available from http://www.cs.colorado.edu/~andreas/Time-Series/Data/Exchange.Rates.Daily. 1.3 Efficient Market Hypothesis The evolution of exchange rates has traditionally been thought to agree with the efficient market hypothesis (EMH), a theory which has found broad acceptance in the academic financial community <ref> [8] </ref>. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable.
Reference: [9] <author> C.W. Omlin and C.L. Giles. </author> <title> Extraction of rules from discrete-time recurrent neural networks. Neural Networks, </title> <address> 9(1):4152, </address> <year> 1996. </year>
Reference-contexts: The ordered triple of a discrete Markov process (fstate; input ! next stateg) can be extracted from an RNN and used to form an equivalent deterministic finite state automata (DFA). This can be done by clustering the activation values of the recurrent state neurons <ref> [9] </ref>. A simple algorithm can then be used to assign states to the clusters and insert transitions between the clusters on the relevant input symbols. Figure 3 shows sample automata extracted from the trained networks.
Reference: [10] <author> A. Refenes, </author> <title> editor. Neural Networks in the Capital Markets. </title> <publisher> John Wiley and Sons, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction 1.1 Prediction System Neural networks are considered by many to provide state-of-the-art solutions to noisy time series prediction problems such as financial prediction <ref> [10] </ref>. When using neural networks to predict noisy time series data, a delay embedding [3] of previous temporal inputs is typically mapped into a prediction.
Reference: [11] <author> A. Refenes and A. </author> <title> Zaidi. Managing exchange-rate prediction strategies with neural networks. </title> <editor> In A. Refenes, editor, </editor> <title> Neural Networks in the Capital Markets. </title> <publisher> John Wiley and Sons, </publisher> <year> 1995. </year>
Reference-contexts: Here, we consider the prediction of the direction of change in the exchange rates for the next business day. A number of people have applied neural network technology to exchange rate prediction and trading, e.g. <ref> [13, 11] </ref>. This work focuses on our approach to noisy time series prediction and rule inference (we do not consider, for example, the use of external variables such as interest rates, or the use of a trading policy).
Reference: [12] <author> H.T. Siegelmann. </author> <title> Computation beyond the Turing limit. </title> <booktitle> Science, </booktitle> <address> 268:545548, </address> <year> 1995. </year>
Reference-contexts: For more information on the SOM, see [5]. 2.4 Grammatical Inference Several recurrent neural network architectures have been used for grammatical inference [6]. It has been shown that a particular class of recurrent networks are at least Turing equivalent <ref> [12] </ref>. The recurrent neural network we have used for grammatical inference here is the Elman neural network. The Elman network has fully connected feedback among the hidden layer neurons.
Reference: [13] <author> A.S. Weigend, B.A. Huberman, and D.E. Rumelhart. </author> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, SFI Studies in the Sciences of Complexity, Proceedings Vol. XII, </booktitle> <pages> pages 395432. </pages> <publisher> Addison-Wesley, </publisher> <year> 1992. </year> <month> 7 </month>
Reference-contexts: Here, we consider the prediction of the direction of change in the exchange rates for the next business day. A number of people have applied neural network technology to exchange rate prediction and trading, e.g. <ref> [13, 11] </ref>. This work focuses on our approach to noisy time series prediction and rule inference (we do not consider, for example, the use of external variables such as interest rates, or the use of a trading policy). <p> The data used here is the same as the data used by Weigend et al. <ref> [13] </ref>, and is from the Monetary Yearbook of the Chicago Mercantile Exchange and consists of daily closing bids for five currencies (German Mark (DM), Japanese Yen, Swiss Franc, British Pound, and Canadian Dollar) with respect to the US Dollar.
References-found: 13

