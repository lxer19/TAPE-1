URL: http://www.cs.cmu.edu/~baird/papers/spurious/spurious.ps
Refering-URL: http://www.cs.cmu.edu/~baird/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: harmonme@aa.wpafb.mil  baird@cs.usafa.af.mil  
Phone: 2241  
Title: Spurious Solutions to the Bellman Equation  
Author: Mance E. Harmon Leemon C. Baird III 
Address: WL/AACF  WPAFB, OH 45433-7318  2354 Fairchild Dr. Suite 6K41 USAFA, CO 80840-6234  
Affiliation: Wright Laboratory  Avionics Circle  U.S.A.F. Academy  
Abstract: Reinforcement learning algorithms often work by finding functions that satisfy the Bellman equation. This yields an optimal solution for prediction with Markov chains and for controlling a Markov decision process (MDP) with a finite number of states and actions. This approach is also frequently applied to Markov chains and MDPs with infinite states. We show that, in this case, the Bellman equation may have multiple solutions, many of which lead to erroneous predictions and policies (Baird, 1996). Algorithms and conditions are presented that guarantee a single, optimal solution to the Bellman equation.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L. C. </author> <year> (1996). </year> <title> Spurious Bellman-equation solutions. </title> <institution> (Internal Tech Report) Department of Computer Science, USAF Academy. </institution>
Reference-contexts: However, in many cases there might exist more than a single, unique solution to the Bellman equation <ref> (Baird, 1996) </ref>. If there is a finite number of states then there does exist a unique solution to equation (4).
Reference: <author> Baird, L. C. </author> <year> (1993). </year> <note> Advantage updating (DTIC Report AD WL-TR-93-1146, available from the Defense Technical Information Center, </note> <institution> Cameron Station, Alexandria, VA 22304-6145). Wright-Patterson Air Force Base, OH. </institution>
Reference-contexts: This relationship is given in equation (2), and is referred to as the Bellman equation for this problem. V * (x ) = R + g V * (x ) (2) Bellman equations can be derived similarly for other algorithms such as Q-learning (Watkins, 1989) or advantage learning <ref> (Baird, 1993, Harmon and Baird, 1996) </ref>. 1.2 UNIQUE SOLUTIONS A learning system will maintain an approximation V to the true answer V*, and the difference between the two can be called the error e, defined in equation (3). Equation (4) shows why dynamic programming works.
Reference: <author> Baird, L. C. </author> <year> (1995). </year> <title> Residual Algorithms: Reinforcement Learning with Function Approximation . In Armand Prieditis & Stuart Russell, </title> <editor> eds. </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> 9-12 July, </address> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: For this example we used residual gradient value iteration <ref> (Baird, 1995) </ref>. The value function is represented with a single sigmoidal node. The output of the node is bounded from [-1,1]. The input to the node is the weighted sum of the state and a bias. The value of a state is the weighted output of the hidden node.
Reference: <author> Crites, R. H., and Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> To appear in Advances in Neural Information Processing Systems 8 , D. </booktitle> <editor> S. Touretzky, M. C. Mozer, M. E. Hasselmo, eds., </editor> <publisher> MIT Press. </publisher>
Reference: <author> Harmon, M. E., and Baird, L. C. </author> <year> (1996). </year> <title> Multi-agent residual advantage learning with general function approximation . (Internal Tech Report). </title> <institution> Wright Laboratory, Wright-Patterson Air Force Base, OH. </institution>
Reference: <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <booktitle> Neural Computation 6 :215-219. </booktitle>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards . Doctoral thesis, </title> <address> Cambridge University, Cambridge, England. </address>
Reference-contexts: This relationship is given in equation (2), and is referred to as the Bellman equation for this problem. V * (x ) = R + g V * (x ) (2) Bellman equations can be derived similarly for other algorithms such as Q-learning <ref> (Watkins, 1989) </ref> or advantage learning (Baird, 1993, Harmon and Baird, 1996). 1.2 UNIQUE SOLUTIONS A learning system will maintain an approximation V to the true answer V*, and the difference between the two can be called the error e, defined in equation (3). Equation (4) shows why dynamic programming works.
References-found: 7

