URL: http://www.cs.brown.edu/people/arc/papers/cs-95-11.ps.gz
Refering-URL: http://www.cs.brown.edu/people/arc/papers/papers.html
Root-URL: 
Email: mlittmancs.brown.edu  arccs.brown.edu  lpkcs.brown.edu  
Title: Learning policies for partially observable environments: Scaling up  
Author: Michael L. Littman Anthony R. Cassandra Leslie Pack Kaelbling 
Date: July 28, 1995  
Address: Providence, RI 02912-1910  
Affiliation: Department of Computer Science Brown University  
Abstract: Partially observable Markov decision processes (pomdp's) model decision problems in which an agent tries to maximize its reward in the face of limited and/or noisy sensor feedback. While the study of pomdp's is motivated by a need to address realistic problems, existing techniques for finding optimal behavior do not appear to scale well and have been unable to find satisfactory policies for problems with more than a dozen states. After a brief review of pomdp's, this paper discusses several simple solution methods and shows that all are capable of finding near-optimal policies for a selection of extremely small pomdp's taken from the learning literature. In contrast, we show that none are able to solve a slightly larger and noisier problem based on robot navigation. We find that a combination of two novel approaches performs well on these problems and suggest methods for scaling to even larger and more complicated domains. 
Abstract-found: 1
Intro-found: 1
Reference: [Astrom, 1965] <author> Astrom, K. J. </author> <year> (1965). </year> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 10 </volume> <pages> 174-205. </pages>
Reference-contexts: In spite of improvements in technology, a robot's information about its surroundings is necessarily incomplete: sensors are imperfect, objects occlude one another from view, the robot might not know its initial status or precisely where it is. The theory of partially observable Markov decision processes (pomdp's) <ref> [Astrom, 1965, Smallwood and Sondik, 1973, Cassandra et al., 1994] </ref> models this situation and provides a basis for computing optimal behavior.
Reference: [Bertsekas, 1987] <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: We will only consider the case in which these sets are finite. The functions T and R define a Markov decision process (mdp) <ref> [Bertsekas, 1987] </ref> with which the agent interacts without direct information as to the current state. <p> and seeing walls in all directions except behind, the agent is sure of where it is: b ( North in Room a ) = 1: 4 Since the agent's belief state is an accurate summary of all the rele-vant past information, it is a sufficient statistic for choosing optimal actions <ref> [Bertsekas, 1987] </ref>. That is, an agent that can choose the optimal action for any given belief state is acting optimally in the environment.
Reference: [Boutilier et al., 1995] <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference: [Cassandra, 1994] <author> Cassandra, A. </author> <year> (1994). </year> <title> Optimal policies for partially observable Markov decision processes. </title> <type> Technical Report CS-94-14, </type> <institution> Brown University, Department of Computer Science, Providence RI. </institution>
Reference-contexts: A variety of algorithms exist for solving pomdp's [Lovejoy, 1991], but because the problem is so computationally challenging [Papadimitriou and Tsitsiklis, 1987], most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states [Cheng, 1988]). Recently, the Witness algorithm <ref> [Cassandra, 1994, Littman, 1994] </ref> has been used to solve pomdp's with up to 16 states. While this problem size is considerably larger than prior state of the art, the algorithm is not efficient enough to be used for larger pomdp's.
Reference: [Cassandra et al., 1994] <author> Cassandra, A. R., Kaelbling, L. P., and Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address>
Reference-contexts: In spite of improvements in technology, a robot's information about its surroundings is necessarily incomplete: sensors are imperfect, objects occlude one another from view, the robot might not know its initial status or precisely where it is. The theory of partially observable Markov decision processes (pomdp's) <ref> [Astrom, 1965, Smallwood and Sondik, 1973, Cassandra et al., 1994] </ref> models this situation and provides a basis for computing optimal behavior. <p> From a known starting belief state, it is easy to use the transition and observation probabilities to incorporate new information into the belief state <ref> [Cassandra et al., 1994] </ref>. As an example, consider an agent that is started in any of the 12 non-goal states of the tiny navigation environment with equal probability: b (s) = 1=12 for all non-goal states. <p> None of these methods are entirely original, but none have been used to find fast approximations to optimal policies for pomdp's given the pomdp models. 3.1 Truncated Exact Value Iteration The Witness algorithm <ref> [Cassandra et al., 1994, Littman, 1994] </ref> finds exact solutions to discounted finite-horizon pomdp's using value iteration. After its k-th iteration, the algorithm returns the exact k-step Q functions as collections of vectors, L a , for each action, a. <p> of q a need to have widely different values to solve the problem at hand. 8 Name jSj jAj jj Noise Shuttle [Chrisman, 1992] 8 3 5 T /O Cheese Maze [McCallum, 1992] 11 4 7 - Part Painting [Kushmerick et al., 1993] 4 4 2 T /O 4x4 Grid <ref> [Cassandra et al., 1994] </ref> 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid [Parr and Russell, 1995] 11 4 6 T Table 1: A suite of extremely small pomdp's. <p> to solve the problem at hand. 8 Name jSj jAj jj Noise Shuttle [Chrisman, 1992] 8 3 5 T /O Cheese Maze [McCallum, 1992] 11 4 7 - Part Painting [Kushmerick et al., 1993] 4 4 2 T /O 4x4 Grid <ref> [Cassandra et al., 1994] </ref> 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid [Parr and Russell, 1995] 11 4 6 T Table 1: A suite of extremely small pomdp's. Like replicated Q-learning, linear Q-learning has the limitation that only linear approximations to the optimal Q functions are considered. <p> The probability of each resulting belief state is computed with the same machinery used to update the belief state. See <ref> [Cassandra et al., 1994] </ref> for details on updating belief states. Although the full backup idea seems well motivated, there are some strong reasons why they were not utilized. In practice, using full backups greatly increases the running time.
Reference: [Cheng, 1988] <author> Cheng, H.-T. </author> <year> (1988). </year> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada. </institution>
Reference-contexts: A variety of algorithms exist for solving pomdp's [Lovejoy, 1991], but because the problem is so computationally challenging [Papadimitriou and Tsitsiklis, 1987], most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states <ref> [Cheng, 1988] </ref>). Recently, the Witness algorithm [Cassandra, 1994, Littman, 1994] has been used to solve pomdp's with up to 16 states. While this problem size is considerably larger than prior state of the art, the algorithm is not efficient enough to be used for larger pomdp's.
Reference: [Chrisman, 1992] <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proc. Tenth National Conference on AI (AAAI). </booktitle>
Reference-contexts: We should expect the rules to behave differently when the components of q a need to have widely different values to solve the problem at hand. 8 Name jSj jAj jj Noise Shuttle <ref> [Chrisman, 1992] </ref> 8 3 5 T /O Cheese Maze [McCallum, 1992] 11 4 7 - Part Painting [Kushmerick et al., 1993] 4 4 2 T /O 4x4 Grid [Cassandra et al., 1994] 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid [Parr and Russell, 1995] <p> The last reward specification form is R: action : start-state %f %f ... %f ... which lets you specify an entire reward matrix for a particular action and start-state combination. A.2 Shuttle This problem is nearly identical to the one presented in <ref> [Chrisman, 1992] </ref>, since it was obtained directly from a section of Lonnie Chrisman's code, whom was very gracious in sending it to us. It models a simple space shuttle docking problem, where we must dock by backing up into one of two space stations.
Reference: [Connell and Mahadevan, 1993] <author> Connell, J. and Mahadevan, S. </author> <year> (1993). </year> <title> Rapid task learning for real robots. In Robot Learning. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: [Jaakkola et al., 1994] <author> Jaakkola, T., Jordan, M. I., and Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation, </booktitle> <pages> 6(6). </pages>
Reference-contexts: observations of the pomdp are sufficient to ensure that the agent is always certain of its state (i.e., b (s) = 1 for some s at all times), this rule reduces exactly to standard Q-learning and can be shown to converge to the optimal Q function under the proper conditions <ref> [Jaakkola et al., 1994, Tsitsikilis, 1994] </ref>.
Reference: [Kushmerick et al., 1993] <author> Kushmerick, N., Hanks, S., and Weld, D. </author> <year> (1993). </year> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> University of Washington Department of Computer Science and Engineering. </institution> <note> To appear in Artificial Intelligence. 57 </note>
Reference-contexts: We should expect the rules to behave differently when the components of q a need to have widely different values to solve the problem at hand. 8 Name jSj jAj jj Noise Shuttle [Chrisman, 1992] 8 3 5 T /O Cheese Maze [McCallum, 1992] 11 4 7 - Part Painting <ref> [Kushmerick et al., 1993] </ref> 4 4 2 T /O 4x4 Grid [Cassandra et al., 1994] 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid [Parr and Russell, 1995] 11 4 6 T Table 1: A suite of extremely small pomdp's. <p> 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 R: * : * : 10 : * 1.0 A.4 Part Painting This problem is loosely based upon a problem formulated in <ref> [Kushmerick et al., 1993] </ref>. There is a part that needs to be painted and shipped if it is not flawed, or simply rejected if it is flawed. A flawed part starts off blemished, though painting a part can hide the blemish.
Reference: [Littman, 1994] <author> Littman, M. L. </author> <year> (1994). </year> <title> The Witness algorithm: Solving partially observable Markov decision processes. </title> <type> Technical Report CS-94-40, </type> <institution> Brown University, Department of Computer Science, Providence, RI. </institution>
Reference-contexts: A variety of algorithms exist for solving pomdp's [Lovejoy, 1991], but because the problem is so computationally challenging [Papadimitriou and Tsitsiklis, 1987], most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states [Cheng, 1988]). Recently, the Witness algorithm <ref> [Cassandra, 1994, Littman, 1994] </ref> has been used to solve pomdp's with up to 16 states. While this problem size is considerably larger than prior state of the art, the algorithm is not efficient enough to be used for larger pomdp's. <p> state, since this class is relatively simple and we are assured that it includes an optimal policy [Ross, 1983]. 2.3 Piecewise-Linear Convex Functions A particularly powerful result of Sondik's is that the optimal value function for any pomdp can be approximated arbitrarily well by a piecewise-linear and convex (pwlc) function <ref> [Smallwood and Sondik, 1973, Littman, 1994] </ref>. Further, there is a class of pomdp's that have value functions that are exactly pwlc [Sondik, 1978]. <p> None of these methods are entirely original, but none have been used to find fast approximations to optimal policies for pomdp's given the pomdp models. 3.1 Truncated Exact Value Iteration The Witness algorithm <ref> [Cassandra et al., 1994, Littman, 1994] </ref> finds exact solutions to discounted finite-horizon pomdp's using value iteration. After its k-th iteration, the algorithm returns the exact k-step Q functions as collections of vectors, L a , for each action, a.
Reference: [Lovejoy, 1991] <author> Lovejoy, W. S. </author> <year> (1991). </year> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66. </pages>
Reference-contexts: The theory of partially observable Markov decision processes (pomdp's) [Astrom, 1965, Smallwood and Sondik, 1973, Cassandra et al., 1994] models this situation and provides a basis for computing optimal behavior. A variety of algorithms exist for solving pomdp's <ref> [Lovejoy, 1991] </ref>, but because the problem is so computationally challenging [Papadimitriou and Tsitsiklis, 1987], most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states [Cheng, 1988]).
Reference: [McCallum, 1992] <author> McCallum, R. A. </author> <year> (1992). </year> <title> First results with utile distinction memory for reinforcement learning. </title> <type> Technical Report 446, </type> <institution> Dept. Comp. Sci., Univ. Rochester. </institution> <note> See also Proceedings of Machine Learning Conference 1993. </note>
Reference-contexts: We should expect the rules to behave differently when the components of q a need to have widely different values to solve the problem at hand. 8 Name jSj jAj jj Noise Shuttle [Chrisman, 1992] 8 3 5 T /O Cheese Maze <ref> [McCallum, 1992] </ref> 11 4 7 - Part Painting [Kushmerick et al., 1993] 4 4 2 T /O 4x4 Grid [Cassandra et al., 1994] 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid [Parr and Russell, 1995] 11 4 6 T Table 1: A suite of
Reference: [Moore, 1994] <author> Moore, A. W. </author> <year> (1994). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An important consequence is that the belief states, in combination with the updating rule, form a completely observable Markov decision process (mdp) with a continuous state space, similar to problems addressed in the reinforcement-learning literature <ref> [Moore, 1994] </ref>. Our goal will be to find an approximation of the Q function over the continuous space of belief states and to use this as a basis for action in the environment.
Reference: [Nicholson and Kaelbling, 1994] <author> Nicholson, A. and Kaelbling, L. P. </author> <year> (1994). </year> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <address> Stanford, California. </address>
Reference: [Papadimitriou and Tsitsiklis, 1987] <author> Papadimitriou, C. H. and Tsitsiklis, J. N. </author> <year> (1987). </year> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450. </pages>
Reference-contexts: The theory of partially observable Markov decision processes (pomdp's) [Astrom, 1965, Smallwood and Sondik, 1973, Cassandra et al., 1994] models this situation and provides a basis for computing optimal behavior. A variety of algorithms exist for solving pomdp's [Lovejoy, 1991], but because the problem is so computationally challenging <ref> [Papadimitriou and Tsitsiklis, 1987] </ref>, most techniques are too inefficient to be used on all but the smallest problems (2 to 5 states [Cheng, 1988]). Recently, the Witness algorithm [Cassandra, 1994, Littman, 1994] has been used to solve pomdp's with up to 16 states.
Reference: [Parr and Russell, 1995] <author> Parr, R. and Russell, S. </author> <year> (1995). </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: jj Noise Shuttle [Chrisman, 1992] 8 3 5 T /O Cheese Maze [McCallum, 1992] 11 4 7 - Part Painting [Kushmerick et al., 1993] 4 4 2 T /O 4x4 Grid [Cassandra et al., 1994] 16 4 2 - Tiger [Cassandra et al., 1994] 2 3 2 O 4x3 Grid <ref> [Parr and Russell, 1995] </ref> 11 4 6 T Table 1: A suite of extremely small pomdp's. Like replicated Q-learning, linear Q-learning has the limitation that only linear approximations to the optimal Q functions are considered. <p> : * : * -1 R:open-left : tiger-left : * : * -100 R:open-left : tiger-right : * : * 10 R:open-right : tiger-left : * : * 10 R:open-right : tiger-right : * : * -100 48 A.7 4x3 Grid This problem is nearly identical to that used in <ref> [Parr and Russell, 1995] </ref> and is shown in Figure 19. The only change was to remove the zero cost absorbing state and replace the transitions to it with equally likely transitions to all non-reward and non-penalty states.
Reference: [Puterman, 1994] <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes| Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: That is, we can temporarily ignore the observation model and find the Q MDP (s; a) values for the mdp consisting of the transitions and rewards only. These values can be computed extremely efficiently for problems with dozens to thousands of states and a variety of approaches are available <ref> [Puterman, 1994] </ref>. With the Q MDP values in hand, we can treat all the Q MDP values for each action as a single linear function and estimate the Q value for a belief state 6 b as Q a (b) = s b (s) Q MDP (s; a).
Reference: [Ross, 1983] <author> Ross, S. M. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: We restrict our attention to stationary, deterministic policies on the belief state, since this class is relatively simple and we are assured that it includes an optimal policy <ref> [Ross, 1983] </ref>. 2.3 Piecewise-Linear Convex Functions A particularly powerful result of Sondik's is that the optimal value function for any pomdp can be approximated arbitrarily well by a piecewise-linear and convex (pwlc) function [Smallwood and Sondik, 1973, Littman, 1994].
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error backpropagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed 58 Processing: Explorations in the microstructures of cognition. Volume 1: Foundations, chapter 8. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This is accomplished by applying the delta rule for neural networks <ref> [Rumelhart et al., 1986] </ref>, which, adapted to the belief mdp framework, becomes: q a (s) = ff b (s)(r + fl max Like the replicated Q-learning rule, this rule reduces to ordinary Q-learning when the belief state is deterministic.
Reference: [Smallwood and Sondik, 1973] <author> Smallwood, R. D. and Sondik, E. J. </author> <year> (1973). </year> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088. </pages>
Reference-contexts: In spite of improvements in technology, a robot's information about its surroundings is necessarily incomplete: sensors are imperfect, objects occlude one another from view, the robot might not know its initial status or precisely where it is. The theory of partially observable Markov decision processes (pomdp's) <ref> [Astrom, 1965, Smallwood and Sondik, 1973, Cassandra et al., 1994] </ref> models this situation and provides a basis for computing optimal behavior. <p> state, since this class is relatively simple and we are assured that it includes an optimal policy [Ross, 1983]. 2.3 Piecewise-Linear Convex Functions A particularly powerful result of Sondik's is that the optimal value function for any pomdp can be approximated arbitrarily well by a piecewise-linear and convex (pwlc) function <ref> [Smallwood and Sondik, 1973, Littman, 1994] </ref>. Further, there is a class of pomdp's that have value functions that are exactly pwlc [Sondik, 1978].
Reference: [Sondik, 1978] <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2). </volume>
Reference-contexts: Further, there is a class of pomdp's that have value functions that are exactly pwlc <ref> [Sondik, 1978] </ref>. These results apply to the optimal Q functions as well: the Q function for action a, Q a (b) is the expected reward for a policy that starts in belief state b, takes action a, and then behaves optimally.
Reference: [Tsitsikilis, 1994] <author> Tsitsikilis, J. N. </author> <year> (1994). </year> <title> Asynchronous stohcastic aproxi-mation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3). </volume>
Reference-contexts: observations of the pomdp are sufficient to ensure that the agent is always certain of its state (i.e., b (s) = 1 for some s at all times), this rule reduces exactly to standard Q-learning and can be shown to converge to the optimal Q function under the proper conditions <ref> [Jaakkola et al., 1994, Tsitsikilis, 1994] </ref>.
Reference: [Watkins, 1989] <author> Watkins, C. J. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference-contexts: At the same time that their algorithms attempt to learn the transition and observation probabilities, they used an extension of Q-learning <ref> [Watkins, 1989] </ref> to learn approximate Q functions for the learned pomdp model. Although it was not the emphasis of their work, their "replicated Q-learning" rule is of independent interest.
Reference: [Williams and Baird, 1993] <author> Williams, R. J. and Baird, L. C. I. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address> <month> 59 </month>
Reference-contexts: After its k-th iteration, the algorithm returns the exact k-step Q functions as collections of vectors, L a , for each action, a. The algorithm can be used to find arbitrarily accurate approximations to the optimal infinite-horizon Q functions and therefore policies that are arbitrarily close to optimal <ref> [Williams and Baird, 1993] </ref>. Unfortunately, the algorithm can take many, many iterations to find an approximately optimal value function, and for problems with a large number of observations, the size of the L a sets can grow explosively from iteration to iteration.
References-found: 25

