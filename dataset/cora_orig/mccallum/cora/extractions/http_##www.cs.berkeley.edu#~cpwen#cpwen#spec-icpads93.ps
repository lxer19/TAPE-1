URL: http://www.cs.berkeley.edu/~cpwen/cpwen/spec-icpads93.ps
Refering-URL: http://www.cs.berkeley.edu/~cpwen/
Root-URL: 
Title: Compiling Sequential Programs for Speculative Parallelism  
Author: Chih-Po Wen Katherine A. Yelick 
Keyword: parallel compilers, runtime systems, speculative parallelism, optimistic concurrency, optimizing compilers.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California,  
Abstract: We present a runtime system and a parallelizing compiler for exploiting speculative parallelism in sequential programs. In speculative executions, the computation consists of tasks which may start before their data or control dependencies are resolved; dependency violation is detected and corrected at runtime. Our runtime system provides a shared memory abstraction and ensures that shared accesses appear to execute in the proper order. Our compiler transforms sequential programs into parallel tasks and manages control dependencies. It also optimizes the parallel program using data flow analysis to reduce the cost of speculative execution. We demonstrate our approach through the parallelization of an example application, and report on its performance on a distributed memory multiprocessor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: We refer to the set of live variables upon entry and exit of the statement S as livein (S) and liveout (S), respectively. The readers are refered to the standard compiler texts <ref> [1] </ref> for details. Consider the parallelization of a loop, with loop body L. Here, and in the rest of the paper, we treat the loop initialization statements as separate from the loop body.
Reference: [2] <author> D.F. Bacon and R.E. Strom. </author> <title> Optimistic paral-lelization of communicating sequential processes. </title> <booktitle> In Proc. Third Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: This abstraction is used for discrete event simulation using the Timewarp system proposed by Jefferson [6]. Message passing programming is reasonable when communication between processes is restricted to a small set of message types, but in general it can be cumbersome. Bacon and Strom <ref> [2] </ref> describes optimistic paral-lelization in the context of CSP (Communicating Sequential Processes). Starting from a parallel language, their approach is more general than ours since they can, for example, express nondeterministic programs. They also require that the user tag processes with a set of guesses for it's incoming messages.
Reference: [3] <author> C.W. Fraser and D.R. Hanson. </author> <title> A code generation interface for ansi c. </title> <journal> Software Practice and Experience, </journal> <volume> 21(9), </volume> <month> September </month> <year> 1991. </year>
Reference: [4] <author> Kaushik Ghosh and Richard M. Fujimoto. </author> <title> Parallel discrete event simulation using space-time memory. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The design does not rely on CM5-specific hardware, although a low overhead message layer is important to performance. Our compiler is based on the retargetable ANSI C compiler lcc <ref> [4] </ref>. We augmented lcc to perform data flow analysis and to generate the parallel code. We will use the program in Figure 1 as an example to illustrate our compilation techniques. The program shown is a simulated annealing solver for the traveling salesman problem (TSP).
Reference: [5] <author> D.R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Dependencies among processes are arbitrated by their timestamps, which are managed explicitly by the programmer. Partitioning and scheduling are also left to the programmer. Work at this level includes our runtime system and the space-time memory proposed by Ghosh and Fujimoto <ref> [5] </ref>. The simplest programming model is to allow the programmer to write a sequential program (with a single flat address space), and have the compiler and run-time system do all the consistency management and optimization. This is the goal of our project.
Reference: [6] <author> H.T. Kung and J.T. Robinson. </author> <title> On optimistic methods for concurrency control. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 6(2), </volume> <month> June </month> <year> 1981. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. of order. Optimistic concurrency has been applied to transaction management [7] and discrete event simulation <ref> [6] </ref>. In previous work, we showed that optimistic concurrency in timing simulation leads to speedups over 50 on a 64 processor CM5, and that the observed speedups exceed even the theoretical speedups of the conservative approach on ideal hardware (assuming free communication and unlimited processors) [11]. <p> The goal of this work is to provide the benefit of speculative parallelism with minimal programming effort. We have built a prototype runtime system that manages scheduling and side-effects of speculative parallelism. It is similar to the timewarp system of Jeffer-son <ref> [6] </ref>, but whereas timewarp uses a process and message view of the computation, we use a shared memory view. We have also built a parallelizing compiler that transforms sequential programs into speculative shared memory parallel programs. Our compiler differs from most parallelizing compilers. <p> Each curve corresponds to a different task granularity. as a collection of logical processes exchanging point-to-point messages. This abstraction is used for discrete event simulation using the Timewarp system proposed by Jefferson <ref> [6] </ref>. Message passing programming is reasonable when communication between processes is restricted to a small set of message types, but in general it can be cumbersome. Bacon and Strom [2] describes optimistic paral-lelization in the context of CSP (Communicating Sequential Processes).
Reference: [7] <author> Pete Tinker and Morry Katz. </author> <title> Parallel execution of sequential scheme with paratran. </title> <booktitle> In Proc. ACM Conference on Lisp and Functional Programming, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. of order. Optimistic concurrency has been applied to transaction management <ref> [7] </ref> and discrete event simulation [6].
Reference: [8] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference: [9] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. </note>
Reference-contexts: This is the goal of our project. Tinker and Katz also describe a runtime approach in the context of Scheme programs <ref> [9] </ref>. Their work is similar to our runtime system, but without the extensive compiler optimizations. 7 Summary Speculative parallelism is known to be a useful technique for achieving high performance for certain applications such as discrete event simulations.
References-found: 9

