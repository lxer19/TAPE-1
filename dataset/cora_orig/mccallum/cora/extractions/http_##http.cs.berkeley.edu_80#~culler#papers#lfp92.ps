URL: http://http.cs.berkeley.edu:80/~culler/papers/lfp92.ps
Refering-URL: http://http.cs.berkeley.edu:80/~culler/papers/
Root-URL: http://www.cs.berkeley.edu
Title: Global Analysis for Partitioning Non-Strict Programs into Sequential Threads  
Author: Kenneth R. Traub David E. Culler Klaus E. Schauser 
Address: 1  2  
Affiliation: Motorola Cambridge Research Center  UC Berkeley Computer Science Division  UC Berkeley Computer Science Division 2  
Abstract-found: 0
Intro-found: 1
Reference: [AA89] <author> Z. Ariola and Arvind. P-TAC: </author> <title> A parallel intermediate language. </title> <booktitle> In FPCA '89, </booktitle> <pages> pages 230-242. </pages> <publisher> ACM, </publisher> <month> Sep </month> <year> 1989. </year>
Reference-contexts: In general, the compiler does not know which I-stores go with which I-fetches. The opcode labels shown in Figure 1 3 The idea of a conditional as a generalization of procedure call is due to <ref> [AA89] </ref>. Treating conditionals in this way has the beneficial effect of breaking all cycles in the dataflow graph for a legal program. 4 There may be edges that carry no data but serve only to insure correct sequencing of operators that cause side-effects.
Reference: [AN87] <author> Arvind and R. S. Nikhil. </author> <title> Executing a program on the Massachusetts Institute of Technology tagged-token dataflow architecture. </title> <booktitle> In PARLE: Parallel Architectures and Languages Europe Volume II, volume 259 of LNCS, </booktitle> <pages> pages 1-29. </pages> <publisher> Springer-Verlag, </publisher> <month> Jun </month> <year> 1987. </year>
Reference-contexts: Indirect certain dependences commonly arise in two situations. First, there is an indirect certain dependence between a vertex that issues a split-phase transaction <ref> [AN87] </ref> and the vertex that receives the response (the dependence is completed through an implicit vertex that services the request). The fetch and I-fetch operators in global analysis can discover indirect certain dependences between pairs of send and receive vertices that interface to other basic blocks.
Reference: [ANP86] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-structures: </author> <title> Data structures for parallel computing. In Graph Reduction, </title> <booktitle> volume 279 of LNCS, </booktitle> <pages> pages 336-369. </pages> <publisher> Springer-Verlag, </publisher> <month> Oct </month> <year> 1986. </year>
Reference-contexts: 1 Introduction In this paper we present a new solution to the problem of compiling an eager, non-strict language into multiple sequential threads. The solution is described using an intermediate program form developed for the programming language Id [Nik90], a functional language extended with I-structures <ref> [ANP86] </ref> and M-structures [BNA91]. A similar intermediate form has also been suggested for imperative languages [BP89], as a way of exposing parallelism in those languages. With suitable restrictions, we believe our method is also appropriate for lazy, purely functional languages such as Haskell [HWe90]. <p> Ordinary fetch and store perform no run-time synchronization|the basic block representation is assumed to include enough edges to insure deterministic sequencing of their side-effects [BP89]. I-fetch and I-store <ref> [ANP86] </ref> are examples of synchronizing memory operations, where the response to an I-fetch is not received until an I-store to the same location takes place. In general, the compiler does not know which I-stores go with which I-fetches.
Reference: [BHA85] <author> G. L. Burn, C. L. Hankin, and S. Abramsky. </author> <title> The theory of strictness analysis for higher order functions. In Programs as Data Objects, </title> <booktitle> volume 217 of LNCS, </booktitle> <pages> pages 42-62. </pages> <publisher> Springer-Verlag, </publisher> <month> Oct </month> <year> 1985. </year>
Reference-contexts: This process may be iterated to arrive at mutually improved versions of both f and g. None of these interpro-cedural analyses have been considered in the prior work. It is also interesting to note that our interprocedural analysis is in some ways more powerful than conventional strictness analysis <ref> [BHA85, Myc80] </ref>. For example, we can conclude that the expressions computing the actual parameters to a two-argument function may be placed in the same thread, even though the function is not strict in either argument. Conditionals.
Reference: [BHY88] <author> A. Bloss, P. Hudak, and J. Young. </author> <title> Code optimizations for lazy evaluation. </title> <journal> Lisp and Symb. Comp., </journal> <volume> 1(2) </volume> <pages> 147-164, </pages> <month> Sep </month> <year> 1988. </year>
Reference-contexts: In many cases, the need to steer multiple threads through the conditional is eliminated. Partitioning as described here has similar aims to the use of strictness analysis to avoid building closures in implementations of lazy functional languages. <ref> [BHY88, Joh86] </ref> In fact, one could apply the partitioning approach directly to lazy languages [Tra89], the key difference being in what partitioning algorithms are permitted. Lazy evaluation im poses the additional restriction that no subexpression may be computed until it is known to contribute to the final answer.
Reference: [BNA91] <author> P. S. Barth, R. S. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In FPCA '91, volume 523 of LNCS, </booktitle> <pages> pages 538-568. </pages> <publisher> Springer-Verlag, </publisher> <month> Aug </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In this paper we present a new solution to the problem of compiling an eager, non-strict language into multiple sequential threads. The solution is described using an intermediate program form developed for the programming language Id [Nik90], a functional language extended with I-structures [ANP86] and M-structures <ref> [BNA91] </ref>. A similar intermediate form has also been suggested for imperative languages [BP89], as a way of exposing parallelism in those languages. With suitable restrictions, we believe our method is also appropriate for lazy, purely functional languages such as Haskell [HWe90].
Reference: [BP89] <author> M. Beck and K. Pingali. </author> <title> From control flow to dataflow. </title> <type> Technical Report TR 89-1050, </type> <institution> Cornell U. Dept. of Comp. Sci., </institution> <address> Ithaca NY, </address> <month> Oct </month> <year> 1989. </year>
Reference-contexts: The solution is described using an intermediate program form developed for the programming language Id [Nik90], a functional language extended with I-structures [ANP86] and M-structures [BNA91]. A similar intermediate form has also been suggested for imperative languages <ref> [BP89] </ref>, as a way of exposing parallelism in those languages. With suitable restrictions, we believe our method is also appropriate for lazy, purely functional languages such as Haskell [HWe90]. Throughout this paper, a thread will mean a subset of the instructions comprising a procedure body, such that: 1. <p> Ordinary fetch and store perform no run-time synchronization|the basic block representation is assumed to include enough edges to insure deterministic sequencing of their side-effects <ref> [BP89] </ref>. I-fetch and I-store [ANP86] are examples of synchronizing memory operations, where the response to an I-fetch is not received until an I-store to the same location takes place. In general, the compiler does not know which I-stores go with which I-fetches.
Reference: [CSS + 91] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain par-allellism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In 4th ASPLOS, </booktitle> <pages> pages 164-175. </pages> <publisher> ACM, </publisher> <month> Apr </month> <year> 1991. </year>
Reference-contexts: without pause, interruption, or execution of instructions from other threads. [Tra91b] Threads of this form are required to implement a language on parallel multithreaded hardware (e.g., Monsoon [PT91, PC90] and *T [NPA92]), and they yield efficient implementations on conventional architectures when combined with a suitable abstract machine such as TAM <ref> [CSS + 91] </ref>. Our algorithms are greedy, and the following comments from [SCvE91] apply: Partitioning decisions imply trade-offs between parallelism, synchronization cost, and sequential efficiency.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM TOPLAS, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> Jul </month> <year> 1987. </year>
Reference-contexts: The term basic block is used here in the dataflow sense as defined in [Tra86]; roughly, it corresponds to a group of operators with the same control dependence <ref> [FOW87] </ref>. For example, all operators comprising the "then" arm of a conditional, excluding those in nested conditionals, are a basic block. 2.1 Basic Blocks Basic blocks are represented as acyclic directed graphs, with some additional annotations.
Reference: [HDGS91] <author> J. E. Hoch, D. M. Davenport, V. G. Grafe, and K. M. Steele. </author> <title> Compile-time partitioning of a non-strict language into sequential threads. </title> <booktitle> In Proc. 3rd Symp. on Par. and Dist. Processing. IEEE, </booktitle> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Fundamentally, therefore, a compiler must sort out certain dependence (i.e., known at compile time), and potential dependence [Tra91a]. This paper extends or improves upon prior work <ref> [Tra91a, Ian90, SCvE91, HDGS91] </ref> in the following ways: More abstract framework. <p> This is in contrast to previous approaches which require special and ad hoc treatment of several kinds of nodes in the graph. Congruence. While the concept of inlet and outlet is found elsewhere ([Ian90], [SCvE91], and <ref> [HDGS91] </ref>), our annotations allow for the expression of congruence, where, for example, two inlets depend on the same (but still unknown at compile time) set of outlets. Even more complicated patterns of overlap and partial overlap are also expressible. Congruence is exploited extensively by our interprocedural analysis. Interprocedural analysis. <p> We will generally refer to such a subset of vertices simply as a thread. A basic block algorithm similar to that presented here is independently developed by Hoch et al. <ref> [HDGS91] </ref>, and our algorithm is also equivalent in power to the algorithm of Schauser et al. [SCvE91]. A significant difference is that prior algorithms had ad hoc treatment of split-phase transactions, while for us this falls out of the general method for handling indirect certain dependences (squiggly edges).
Reference: [HWe90] <editor> P. Hudak and P. Wadler (editors). </editor> <title> Report on the programming language Haskell, a non-strict purely functional language (Version 1.0). </title> <type> Technical Report YALEU/DCS/RR777, </type> <institution> Yale University Department of Computer Science, </institution> <address> New Haven CT, </address> <month> Apr </month> <year> 1990. </year>
Reference-contexts: A similar intermediate form has also been suggested for imperative languages [BP89], as a way of exposing parallelism in those languages. With suitable restrictions, we believe our method is also appropriate for lazy, purely functional languages such as Haskell <ref> [HWe90] </ref>. Throughout this paper, a thread will mean a subset of the instructions comprising a procedure body, such that: 1. A compile-time instruction ordering can be determined for the thread which is valid for all contexts in which the containing procedure can be invoked. 2.
Reference: [Ian90] <author> R. A. </author> <title> Iannucci. Parallel Machines: Parallel Machine Languages. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Fundamentally, therefore, a compiler must sort out certain dependence (i.e., known at compile time), and potential dependence [Tra91a]. This paper extends or improves upon prior work <ref> [Tra91a, Ian90, SCvE91, HDGS91] </ref> in the following ways: More abstract framework. <p> Another generalization, though perhaps less useful in practice, is the specialization of f when it is known that its only callers are g 1 ; g 2 , etc. Prior work has ad hoc treatment of conditionals, in some cases violating the semantics of the language (as in <ref> [Ian90] </ref>), and in other cases inherently limited in effectiveness (e.g., [SCvE91]). Preservation of control structure. The membership of operators in different control regions is preserved; e.g., instructions belonging to the "then" arm of a conditional are kept separate from those in the "else" arm and from the containing expression. <p> Dependence Set partitioning forms partitions by grouping together all nodes that depend on the same set of inlets, and was originally discovered by Iannucci <ref> [Ian90] </ref>.
Reference: [Joh86] <author> T. Johnsson. </author> <title> Target code generation from G-machine code. In Graph Reduction, </title> <booktitle> volume 279 of LNCS, </booktitle> <pages> pages 119-159. </pages> <publisher> Springer-Verlag, </publisher> <month> Oct </month> <year> 1986. </year>
Reference-contexts: In many cases, the need to steer multiple threads through the conditional is eliminated. Partitioning as described here has similar aims to the use of strictness analysis to avoid building closures in implementations of lazy functional languages. <ref> [BHY88, Joh86] </ref> In fact, one could apply the partitioning approach directly to lazy languages [Tra89], the key difference being in what partitioning algorithms are permitted. Lazy evaluation im poses the additional restriction that no subexpression may be computed until it is known to contribute to the final answer.
Reference: [Myc80] <author> A. Mycroft. </author> <title> The theory and practice of transforming call-by-need into call-by-value. </title> <booktitle> In International Symposium on Programming, volume 83 of LNCS, </booktitle> <pages> pages 269-281. </pages> <publisher> Springer-Verlag, </publisher> <month> Apr </month> <year> 1980. </year>
Reference-contexts: This process may be iterated to arrive at mutually improved versions of both f and g. None of these interpro-cedural analyses have been considered in the prior work. It is also interesting to note that our interprocedural analysis is in some ways more powerful than conventional strictness analysis <ref> [BHA85, Myc80] </ref>. For example, we can conclude that the expressions computing the actual parameters to a two-argument function may be placed in the same thread, even though the function is not strict in either argument. Conditionals.
Reference: [Nik90] <author> R. S. Nikhil. </author> <title> Id version 90.0 reference manual. </title> <type> CSG Memo 284-1, </type> <institution> MIT Lab. for Comp. Sci., </institution> <address> Cambridge MA, </address> <month> Sep </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In this paper we present a new solution to the problem of compiling an eager, non-strict language into multiple sequential threads. The solution is described using an intermediate program form developed for the programming language Id <ref> [Nik90] </ref>, a functional language extended with I-structures [ANP86] and M-structures [BNA91]. A similar intermediate form has also been suggested for imperative languages [BP89], as a way of exposing parallelism in those languages.
Reference: [NPA92] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proc. 19th Ann. Int. Symp. on Comp. Arch. IEEE, </booktitle> <month> May </month> <year> 1992. </year> <note> (To appear). </note>
Reference-contexts: thread is executed, it is always possible to execute each of the remaining in structions, in the compile-time ordering, without pause, interruption, or execution of instructions from other threads. [Tra91b] Threads of this form are required to implement a language on parallel multithreaded hardware (e.g., Monsoon [PT91, PC90] and *T <ref> [NPA92] </ref>), and they yield efficient implementations on conventional architectures when combined with a suitable abstract machine such as TAM [CSS + 91]. Our algorithms are greedy, and the following comments from [SCvE91] apply: Partitioning decisions imply trade-offs between parallelism, synchronization cost, and sequential efficiency.
Reference: [PC90] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an explicit token store architecture. </title> <booktitle> In Proc. 17th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 82-91. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: first instruction in a thread is executed, it is always possible to execute each of the remaining in structions, in the compile-time ordering, without pause, interruption, or execution of instructions from other threads. [Tra91b] Threads of this form are required to implement a language on parallel multithreaded hardware (e.g., Monsoon <ref> [PT91, PC90] </ref> and *T [NPA92]), and they yield efficient implementations on conventional architectures when combined with a suitable abstract machine such as TAM [CSS + 91]. Our algorithms are greedy, and the following comments from [SCvE91] apply: Partitioning decisions imply trade-offs between parallelism, synchronization cost, and sequential efficiency.
Reference: [PT91] <author> G. M. Papadopoulos and K. R. Traub. Mul-tithreading: </author> <title> A revisionist view of dataflow architectures. </title> <booktitle> In Proc. 18th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 342-351. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: first instruction in a thread is executed, it is always possible to execute each of the remaining in structions, in the compile-time ordering, without pause, interruption, or execution of instructions from other threads. [Tra91b] Threads of this form are required to implement a language on parallel multithreaded hardware (e.g., Monsoon <ref> [PT91, PC90] </ref> and *T [NPA92]), and they yield efficient implementations on conventional architectures when combined with a suitable abstract machine such as TAM [CSS + 91]. Our algorithms are greedy, and the following comments from [SCvE91] apply: Partitioning decisions imply trade-offs between parallelism, synchronization cost, and sequential efficiency.
Reference: [SCvE91] <author> K. E. Schauser, D. E. Culler, and T. von Eicken. </author> <title> Compiler-controlled multithreading for lenient parallel languages. </title> <booktitle> In FPCA '91, volume 523 of LNCS, </booktitle> <pages> pages 50-72. </pages> <publisher> Springer-Verlag, </publisher> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Our algorithms are greedy, and the following comments from <ref> [SCvE91] </ref> apply: Partitioning decisions imply trade-offs between parallelism, synchronization cost, and sequential efficiency. <p> Fundamentally, therefore, a compiler must sort out certain dependence (i.e., known at compile time), and potential dependence [Tra91a]. This paper extends or improves upon prior work <ref> [Tra91a, Ian90, SCvE91, HDGS91] </ref> in the following ways: More abstract framework. <p> This is in contrast to previous approaches which require special and ad hoc treatment of several kinds of nodes in the graph. Congruence. While the concept of inlet and outlet is found elsewhere ([Ian90], <ref> [SCvE91] </ref>, and [HDGS91]), our annotations allow for the expression of congruence, where, for example, two inlets depend on the same (but still unknown at compile time) set of outlets. Even more complicated patterns of overlap and partial overlap are also expressible. Congruence is exploited extensively by our interprocedural analysis. <p> Prior work has ad hoc treatment of conditionals, in some cases violating the semantics of the language (as in [Ian90]), and in other cases inherently limited in effectiveness (e.g., <ref> [SCvE91] </ref>). Preservation of control structure. The membership of operators in different control regions is preserved; e.g., instructions belonging to the "then" arm of a conditional are kept separate from those in the "else" arm and from the containing expression. <p> We will generally refer to such a subset of vertices simply as a thread. A basic block algorithm similar to that presented here is independently developed by Hoch et al. [HDGS91], and our algorithm is also equivalent in power to the algorithm of Schauser et al. <ref> [SCvE91] </ref>. A significant difference is that prior algorithms had ad hoc treatment of split-phase transactions, while for us this falls out of the general method for handling indirect certain dependences (squiggly edges). <p> That is, if an outlet depends on one node in a group, it depends on all the other nodes in the group as well. This algorithm is called Demand Set partitioning, and was first reported in <ref> [SCvE91] </ref>. 5 Definition 2 The demand set of a node is the set of outlets which depend on it: Dem (v) = u2Succ fl (v) Outlet (u) where Outlet (u) is the set of outlet names that annotate node u, and Succ fl (v) is the set of nodes to which <p> The number is simply the maximum distance in squiggly arcs from the roots of the thread. Algorithm 2 (Subpartitioning (forward)) Given a thread: 5 In <ref> [SCvE91] </ref>, the algorithm was called "dominance set partitioning." fetch fetch response add to pointer fetch fetch response add to pointer ? 1. <p> It also identifies a larger number of redundant synchronizations, thereby reducing the cost of dynamic scheduling. We do not yet have empirical measurements of the quality of partitioning using global analysis relative to the simpler algorithm in <ref> [SCvE91] </ref>, but the improvement appears to be substantial. The most significant shortcoming of the earlier work was the weak handling of conditionals, which resulted in a large number of very small threads.
Reference: [Tra86] <author> K. R. Traub. </author> <title> A compiler for the MIT tagged-token dataflow architecture. </title> <type> Technical Report TR-370, </type> <institution> MIT Lab. for Comp. Sci., </institution> <address> Cambridge MA, </address> <month> Aug </month> <year> 1986. </year>
Reference-contexts: A structured dataflow graph consists of a collection of acyclic graphs describing basic blocks, and interfaces which describe how the blocks relate to one another. The term basic block is used here in the dataflow sense as defined in <ref> [Tra86] </ref>; roughly, it corresponds to a group of operators with the same control dependence [FOW87]. For example, all operators comprising the "then" arm of a conditional, excluding those in nested conditionals, are a basic block. 2.1 Basic Blocks Basic blocks are represented as acyclic directed graphs, with some additional annotations. <p> The vertices are primitive operators, and straight edges connecting them indicate the flow of operands. 4 Figure 1 shows the repertoire of operators commonly used to compile Id <ref> [Tra86] </ref>.
Reference: [Tra89] <author> K. R. Traub. </author> <title> Compilation as partitioning: A new approach to compiling non-strict functional languages. </title> <booktitle> In FPCA '89, </booktitle> <pages> pages 75-88. </pages> <publisher> ACM, </publisher> <month> Sep </month> <year> 1989. </year>
Reference-contexts: Partitioning as described here has similar aims to the use of strictness analysis to avoid building closures in implementations of lazy functional languages. [BHY88, Joh86] In fact, one could apply the partitioning approach directly to lazy languages <ref> [Tra89] </ref>, the key difference being in what partitioning algorithms are permitted. Lazy evaluation im poses the additional restriction that no subexpression may be computed until it is known to contribute to the final answer.
Reference: [Tra91a] <author> K. R. Traub. </author> <title> Implementation of Non-Strict Functional Programming Languages. </title> <publisher> Pitman Publishing, </publisher> <address> London, 1991. </address> <publisher> Also published by MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: Fundamentally, therefore, a compiler must sort out certain dependence (i.e., known at compile time), and potential dependence <ref> [Tra91a] </ref>. This paper extends or improves upon prior work [Tra91a, Ian90, SCvE91, HDGS91] in the following ways: More abstract framework. <p> Fundamentally, therefore, a compiler must sort out certain dependence (i.e., known at compile time), and potential dependence [Tra91a]. This paper extends or improves upon prior work <ref> [Tra91a, Ian90, SCvE91, HDGS91] </ref> in the following ways: More abstract framework. <p> In this direction, squiggly arcs result when the caller feeds a result returned by the callee back into an argument of the callee. The possibility of such feedback is the chief reason why non-strict languages are more expressive than their strict counterparts. <ref> [Tra91a] </ref> In this section, all interfaces are assumed to be single-call-single-def, i.e., they relate a single call site to a single def site.
Reference: [Tra91b] <author> K. R. Traub. </author> <title> Multi-thread code generation for dataflow architectures from non-strict programs. </title> <booktitle> In FPCA '91, volume 523 of LNCS, </booktitle> <pages> pages 73-101. </pages> <publisher> Springer-Verlag, </publisher> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Once the first instruction in a thread is executed, it is always possible to execute each of the remaining in structions, in the compile-time ordering, without pause, interruption, or execution of instructions from other threads. <ref> [Tra91b] </ref> Threads of this form are required to implement a language on parallel multithreaded hardware (e.g., Monsoon [PT91, PC90] and *T [NPA92]), and they yield efficient implementations on conventional architectures when combined with a suitable abstract machine such as TAM [CSS + 91].
References-found: 23

