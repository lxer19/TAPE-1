URL: http://www.cs.dartmouth.edu/~nils/papers/perf.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~nils/papers/gfsperf.html
Root-URL: http://www.cs.dartmouth.edu
Email: fnils,dfkg@cs.dartmouth.edu  
Title: Performance of the Galley Parallel File System  
Author: Nils Nieuwejaar David Kotz 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Abstract: As the I/O needs of parallel scientific applications increase, file systems for multiprocessors are being designed to provide applications with parallel access to multiple disks. Many parallel file systems present applications with a conventional Unix-like interface that allows the application to access multiple disks transparently. This interface conceals the parallelism within the file system, which increases the ease of programmability, but makes it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs to exploit that parallelism. Furthermore, most current parallel file systems are optimized for a different workload than they are being asked to support. We introduce Galley, a new parallel file system that is intended to efficiently support realistic parallel workloads. Initial experiments, reported in this paper, indicate that Galley is capable of providing high-performance I/O to applications that access data in patterns that have been observed to be common. 
Abstract-found: 1
Intro-found: 1
Reference: [BBH95] <author> Sandra Johnson Baylor, Caroline B. Ben-veniste, and Yarson Hsu. </author> <title> Performance evaluation of a parallel I/O architecture. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 404-413, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, recent workload characterizations show that parallel file systems are frequently called upon to service many small requests.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Suss-man, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year> <month> ftp://hpsl.cs.umd.edu/pub/papers/splc94.ps.Z. </month>
Reference-contexts: There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices [TBC + 94, SW94]. The Jovian project explores the issues relating to the storage of irregular structures <ref> [BBS + 94] </ref>. Finally, there are also plans to extend the MPI standard to include parallel I/O operations [MPI94, CFF + 95]. These systems and their interfaces could all be considered candidates for implementation on top of Galley.
Reference: [BGMZ92] <author> Peter Brezany, Michael Gernt, Piyush Mehotra, and Hans Zima. </author> <title> Concurrent file operations in a High Performance FORTRAN. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237, </pages> <year> 1992. </year>
Reference-contexts: In addition to full file systems, there are numerous interfaces that are designed to allow programmers to describe their I/O needs at a higher semantic level. These interfaces are sometimes tightly integrated into a particular language such as HPF <ref> [BGMZ92, HPF93] </ref> or CMF [Thi94]. There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices [TBC + 94, SW94]. The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94].
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stan-fill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Section 5 we discuss some related work, and finally, in Section 6, we conclude and describe our future plans. 2 Background 2.1 Parallel File Systems Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93, WMR + 94] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> One enhancement to the conventional interface, which is offered by several multiprocessor file systems, is a file pointer that is shared among the processes in an application and provides a mechanism for regulating access to a shared file by those processes <ref> [Pie89, BGST93] </ref>. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Most parallel file systems provide this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer). <p> Other examples of this type of parallel file system are SUNMOS (and its successor, PUMA) [WMR + 94], sfs [LIN + 93], and CMMD <ref> [BGST93] </ref>. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [CFF + 95] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Parkson Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94]. Finally, there are also plans to extend the MPI standard to include parallel I/O operations <ref> [MPI94, CFF + 95] </ref>. These systems and their interfaces could all be considered candidates for implementation on top of Galley.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra John-son Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Mor-gan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <pages> pages 222-248, </pages> <year> 1995. </year>
Reference-contexts: These interfaces allow library designers to implement complex functionality (e.g., transparent replication of data, application-specific caching algorithms) in their files, but to hide that complexity from end users. The Vesta file system, and its commercial version, PI-OFS, address some of the same issues as Galley <ref> [CFP + 95] </ref>. Most importantly, both recognize that data structures stored in a single file on disk are likely to be partitioned across multiple processes in a parallel application, and that new the strided interface. interfaces are required to express this partitioning. <p> Second, Vesta's partitioning schemes do not allow for irregular partitioning. Even if your data can be fit into a rectangular model, Vesta only allows the data to be partitioned into regularly-distributed, rectangular sub-blocks of a single size. Examples in <ref> [CFP + 95] </ref> illustrate both the flexibilty and limitations of Vesta's approach to partitioning. Finally, Vesta does not provide an easy way for two processes to access overlapping regions of a file; each process's partition is strictly disjoint from every other process's partition.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: Rather, one could distribute the data across the disks using the keys and knowledge about how the dataset will be used to determine a partitioning scheme that results in highly parallel access. Finally, the parallel-I/O algorithms community has frequently argued for this kind of increased control over declustering <ref> [CK93, WGRW93] </ref>. To address these problems, Galley allows applications to fully control the way in which data is declustered across the IOPs, as well as which IOP they wish to access in each request.
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Bridge, one of the earliest parallel file systems, has disks on every node | their model does not distinguish between CPs and IOPs. ridge provides both a traditional Unix-like interface, and a more complex interface that allows applications to explicitly access the local file systems on each node <ref> [Dib90] </ref>. Intel's Concurrent File System (CFS) [Pie89, Nit92], frequently cited as the canonical first-generation parallel file system, and its successor, PFS, are examples of file systems that provide a linear file model to the applications, and offer a Unix-like interface to the data.
Reference: [GP91] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year>
Reference-contexts: PPFS includes a number of predefined data distributions, which map the logical, linear stream of records to an strided interface. Note the different scales on the y-axis. underlying (disk, record) pair, and also allows an application to provide its own mapping function. The ELFS system <ref> [GP91] </ref> and the Hurricane File System [Kri94] provide object-oriented interfaces. These interfaces allow library designers to implement complex functionality (e.g., transparent replication of data, application-specific caching algorithms) in their files, but to hide that complexity from end users.
Reference: [HP91] <institution> Hewlett Packard. </institution> <note> HP97556/58/60 5.25-inch SCSI Disk Drives Technical Reference Manual, second edition, </note> <month> June </month> <year> 1991. </year> <title> HP Part number 5960-0115. </title>
Reference-contexts: Accordingly, the performance results presented here were obtained through the use of a simulation of an HP 97560 SCSI hard disk, which has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s <ref> [HP91] </ref>. Each IOP provides access to one simulated disk. Our implementation of the disk model was based on earlier implementations [RW94, KTR94]. Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year> <note> http://www.erc.msstate.edu/hpff/report.html. </note>
Reference-contexts: This pattern is likely to arise if, for example, a two-dimensional matrix is stored on disk in row-major order, and an application distributes the columns of the matrix across its processes in a CYCLIC fashion (using High Performance Fortran terminology <ref> [HPF93] </ref>). In addition to assuming that parallel scientific applications would access files consecutively, most parallel file system implementations assume that these files would be accessed in large chunks | hundreds of kilobytes or megabytes at a time. <p> pattern could represent either a one-dimensional partitioning of data or the series of accesses we would expect to see if a two-dimensional matrix were stored on disk in row-major order, and the application distributed the rows of the matrix across the compute nodes in a BLOCK fashion (using HPF terminology <ref> [HPF93] </ref>). A partitioned access pattern at the file level can map onto two different access patterns at the IOP level. <p> In addition to full file systems, there are numerous interfaces that are designed to allow programmers to describe their I/O needs at a higher semantic level. These interfaces are sometimes tightly integrated into a particular language such as HPF <ref> [BGMZ92, HPF93] </ref> or CMF [Thi94]. There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices [TBC + 94, SW94]. The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94].
Reference: [IBM94] <author> IBM. </author> <title> AIX Version 3.2 General Programming Concepts, </title> <booktitle> twelfth edition, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: To conserve space, we do not present the details here (see [NK96, NK95]). 3.3.4 List Requests Finally, in addition to these structured operations, Galley provides a more general file interface, called the list interface, which is similar to the POSIX lio listio () interface <ref> [IBM94] </ref>. This interface simply takes an array of (file offset, memory offset, size) triples from the application. This interface is useful for applications with access patterns that do not have any inherently regular structure.
Reference: [KFG94] <author> John F. Karpovich, James C. French, and An-drew S. Grimshaw. </author> <title> High performance access to radio astronomy data: A case study. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 240-249, </pages> <month> September </month> <year> 1994. </year> <note> Also available as UVA TR CS-94-25. </note>
Reference-contexts: An example of using forks for both data and metadata may be found in data files like those described in <ref> [KFG94] </ref>. The style of FITS file described in this study contained records with 6 keys, describing the frequency domain, the antenna, and the time the data was collected.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Most modern parallel file systems were designed around several key assumptions about how scientific applications would use such systems. Several recent analyses of file-system workloads on production multiprocessors running primarily scientific applications show that many of these assumptions are incorrect <ref> [KN94, PEK + 95, NKP + 95] </ref>. Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications [Pie89, PFDJ89, LIN + 93]. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. <p> Abstracting with credit is permitted. two parallel file-system workloads, supporting many users and running a variety of applications in a variety of scientific domains, under both data-parallel and control-parallel programming models, show that many parallel, scientific applications make many small, non-consecutive requests to the file system <ref> [KN94, PEK + 95, NKP + 95, NK95] </ref>. These studies suggest that most parallel file systems have been optimized for a workload that is different than that which actually exists. <p> Until recently, however, there had been no investigation into whether this file model and interface were well suited to massively parallel scientific applications. To determine whether this model was appropriate, we examined the file-system workloads on two different massively parallel processors, running two different application work-loads <ref> [KN94, PEK + 95] </ref>. These studies show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment than in uniprocessor or supercomputer environments. <p> While the standard Unix-like interface has worked well in the past, it seems clear that it is not well suited to parallel applications, which have more complicated access patterns than uniprocessor and supercomputer applications. Furthermore, the tracing study described in <ref> [KN94] </ref> found that shared file pointers were rarely used in practice and suggests that poor performance and a failure to match the needs of applications are the likely causes.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Second, recent work has shown that providing a file system with more information about an application's access patterns can lead to tremendous performance improvements by introducing opportunities for intelligent scheduling of I/O and communication <ref> [Kot94] </ref>. The higher-level interfaces offered by Galley are summarized below.
Reference: [KR94] <author> Thomas T. Kwan and Daniel A. Reed. </author> <title> Performance of the CM-5 scalable file system. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 156-165, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, recent workload characterizations show that parallel file systems are frequently called upon to service many small requests.
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Note the different scales on the y-axis. underlying (disk, record) pair, and also allows an application to provide its own mapping function. The ELFS system [GP91] and the Hurricane File System <ref> [Kri94] </ref> provide object-oriented interfaces. These interfaces allow library designers to implement complex functionality (e.g., transparent replication of data, application-specific caching algorithms) in their files, but to hide that complexity from end users.
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Rad-hakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dart-mouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Each IOP provides access to one simulated disk. Our implementation of the disk model was based on earlier implementations <ref> [RW94, KTR94] </ref>. Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications <ref> [Pie89, PFDJ89, LIN + 93] </ref>. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. Studies of Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> Section 5 we discuss some related work, and finally, in Section 6, we conclude and describe our future plans. 2 Background 2.1 Parallel File Systems Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93, WMR + 94] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> Other examples of this type of parallel file system are SUNMOS (and its successor, PUMA) [WMR + 94], sfs <ref> [LIN + 93] </ref>, and CMMD [BGST93]. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [LMKQ89] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The simplest shared file pointer is one which supports an atomic-append mode (as in <ref> [LMKQ89] </ref>, page 174). Most parallel file systems provide this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer).
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications [Pie89, PFDJ89, LIN + 93]. These applications tend to access large files in large, consecutive chunks <ref> [MK91, PP93] </ref>. Studies of Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially <ref> [MK91] </ref>. Until recently, however, there had been no investigation into whether this file model and interface were well suited to massively parallel scientific applications.
Reference: [MPI94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <address> 1.0 edition, </address> <month> May 5 </month> <year> 1994. </year> <note> http://www.mcs.anl.gov/Projects/mpi/standard.html. </note>
Reference-contexts: The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94]. Finally, there are also plans to extend the MPI standard to include parallel I/O operations <ref> [MPI94, CFF + 95] </ref>. These systems and their interfaces could all be considered candidates for implementation on top of Galley.
Reference: [NAS94] <institution> NASA/Science Office of Standards and Tech--nology, NASA Goddard Space Flight Center, </institution> <month> Greensbelt, </month> <title> MD 020771. A User's Guide for the Flexible Image Transport System (FITS), </title> <address> 3.1 edition, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: One such example may be seen in the Flexible Image Transport System (FITS) data format, which is used for astronomical data <ref> [NAS94] </ref>. A FITS file is organized as a series of records, each of which contains a key with multiple fields and one or more data elements. It is not clear that blindly striping these records across multiple disks is the optimal approach in a parallel file system.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, recent workload characterizations show that parallel file systems are frequently called upon to service many small requests. <p> For this study, the size of each cache was 34 megabytes, large enough to hold 1100 blocks. Galley does not attempt to prefetch data for two reasons. First, indiscriminate prefetching can cause the cache to thrash <ref> [Nit92] </ref>. Second, prefetching is based on the assumption that the system can intelligently guess what an application is going to request next. <p> When a CP finished reading all the data from its first cluster, it began reading data from the next cluster. Nitzberg experimented with a similar strategy on CFS to reduce contention for cache space on the IOP <ref> [Nit92] </ref>. Figure 5 shows the results of our clustering experiment. Clearly, reducing the number of active sockets reduced the congestion at each CP, and improved overall performance. <p> Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, frequently cited as the canonical first-generation parallel file system, and its successor, PFS, are examples of file systems that provide a linear file model to the applications, and offer a Unix-like interface to the data.
Reference: [NK95] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In IPPS '95 Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Abstracting with credit is permitted. two parallel file-system workloads, supporting many users and running a variety of applications in a variety of scientific domains, under both data-parallel and control-parallel programming models, show that many parallel, scientific applications make many small, non-consecutive requests to the file system <ref> [KN94, PEK + 95, NKP + 95, NK95] </ref>. These studies suggest that most parallel file systems have been optimized for a workload that is different than that which actually exists. <p> These studies show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment than in uniprocessor or supercomputer environments. In <ref> [NK95, NKP + 95] </ref>, we looked more closely at the specific patterns in which applications accessed the files in a parallel file system. We found that these applications frequently accessed files in regular, repeating patterns. <p> These primitives are limited to read ()ing and write ()ing consecutive regions of a file. As discussed above, recent studies show that these primitives do not match the needs of many parallel applications <ref> [NK95, NKP + 95] </ref>. Specifically, parallel scientific applications frequently make many small requests to a file, with strided access patterns. We define two types of strided patterns. <p> A nested-strided access pattern is similar to a simple-strided pattern, but rather than repeating a single request at regular intervals, the application repeats a strided segment at regular intervals. Studies show that both simple-and nested-strided patterns are common in parallel, scientific applications <ref> [NK95, NKP + 95] </ref>. Indeed, in one study, over 90% of the requests in the entire workload were part of one of these two patterns. <p> The higher-level interfaces offered by Galley are summarized below. These interfaces are described in greater detail, and examples are provided, in <ref> [NK96, NK95] </ref>. 3.3.1 Simple-strided Requests gfs_read_strided (int fid, void *buf, ulong offset, ulong rec_size, int f_stride, int m_stride, int quant) Beginning at offset, the file system will read quant records of rec size bytes, where the offset of each record is f stride bytes greater than that of the previous record. <p> This interface is nested in that any of the requests in the arbitrary series may themselves be a batched request. To conserve space, we do not present the details here (see <ref> [NK96, NK95] </ref>). 3.3.4 List Requests Finally, in addition to these structured operations, Galley provides a more general file interface, called the list interface, which is similar to the POSIX lio listio () interface [IBM94]. This interface simply takes an array of (file offset, memory offset, size) triples from the application.
Reference: [NK96] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In addition to providing high performance, Galley was designed to be `library friendly', giving programmers the capability to easily layer abstractions above the file system. We summarize the model here; full details of this structure may be found in <ref> [NK96] </ref>. File System. <p> Unlike segmenting a traditional file into three regions, the fork-based structure allows each fork to grow as more records are added. A further discussion of the applications and benefits of this structure can be found in <ref> [NK96] </ref>. 3.3 Data Access Interface The standard Unix interface provides only simple primitives for accessing the data in files. These primitives are limited to read ()ing and write ()ing consecutive regions of a file. <p> The higher-level interfaces offered by Galley are summarized below. These interfaces are described in greater detail, and examples are provided, in <ref> [NK96, NK95] </ref>. 3.3.1 Simple-strided Requests gfs_read_strided (int fid, void *buf, ulong offset, ulong rec_size, int f_stride, int m_stride, int quant) Beginning at offset, the file system will read quant records of rec size bytes, where the offset of each record is f stride bytes greater than that of the previous record. <p> This interface is nested in that any of the requests in the arbitrary series may themselves be a batched request. To conserve space, we do not present the details here (see <ref> [NK96, NK95] </ref>). 3.3.4 List Requests Finally, in addition to these structured operations, Galley provides a more general file interface, called the list interface, which is similar to the POSIX lio listio () interface [IBM94]. This interface simply takes an array of (file offset, memory offset, size) triples from the application. <p> This best-case performance seems respectable, but our performance with small record sizes was certainly less than satisfactory. The goal of our new interfaces is to provide high performance for the whole range of record sizes, with particular emphasis on providing high throughput for small records. As described in <ref> [NK96] </ref>, our higher-level interfaces are essentially different faces on the same underlying mechanism, and the performance of one is indicative of the performance of the others. The tests in this section were again performed by issuing asynchronous requests to each fork.
Reference: [NKP + 95] <author> Nils Nieuwejaar, David Kotz, Apratim Pu-rakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> Submitted to IEEE TPDS. </note>
Reference-contexts: Most modern parallel file systems were designed around several key assumptions about how scientific applications would use such systems. Several recent analyses of file-system workloads on production multiprocessors running primarily scientific applications show that many of these assumptions are incorrect <ref> [KN94, PEK + 95, NKP + 95] </ref>. Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications [Pie89, PFDJ89, LIN + 93]. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. <p> Abstracting with credit is permitted. two parallel file-system workloads, supporting many users and running a variety of applications in a variety of scientific domains, under both data-parallel and control-parallel programming models, show that many parallel, scientific applications make many small, non-consecutive requests to the file system <ref> [KN94, PEK + 95, NKP + 95, NK95] </ref>. These studies suggest that most parallel file systems have been optimized for a workload that is different than that which actually exists. <p> These studies show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment than in uniprocessor or supercomputer environments. In <ref> [NK95, NKP + 95] </ref>, we looked more closely at the specific patterns in which applications accessed the files in a parallel file system. We found that these applications frequently accessed files in regular, repeating patterns. <p> These primitives are limited to read ()ing and write ()ing consecutive regions of a file. As discussed above, recent studies show that these primitives do not match the needs of many parallel applications <ref> [NK95, NKP + 95] </ref>. Specifically, parallel scientific applications frequently make many small requests to a file, with strided access patterns. We define two types of strided patterns. <p> A nested-strided access pattern is similar to a simple-strided pattern, but rather than repeating a single request at regular intervals, the application repeats a strided segment at regular intervals. Studies show that both simple-and nested-strided patterns are common in parallel, scientific applications <ref> [NK95, NKP + 95] </ref>. Indeed, in one study, over 90% of the requests in the entire workload were part of one of these two patterns. <p> Since many models of physical events require logically adjacent nodes to share boundary information, this could be an important restriction. Indeed, we have observed that such overlapping file access is likely to occur in practice. Results in <ref> [NKP + 95] </ref>, show that most read-only files had at least some bytes that were accessed by multiple processors. We should note that the same results show that in many cases, the strictly disjoint partitioning offered by Vesta may match the applications' needs for write-only files.
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Har-rison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: We compare Galley to other, more sophisticated, parallel file systems in Section 5. 2.2 Workload Characterization Experience has shown that the simple, Unix-like model of a file is well suited to uniprocessor applications that tend to access files in a simple, sequential fashion <ref> [OCH + 85] </ref>. It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Until recently, however, there had been no investigation into whether this file model and interface were well suited to massively parallel scientific applications.
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Most modern parallel file systems were designed around several key assumptions about how scientific applications would use such systems. Several recent analyses of file-system workloads on production multiprocessors running primarily scientific applications show that many of these assumptions are incorrect <ref> [KN94, PEK + 95, NKP + 95] </ref>. Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications [Pie89, PFDJ89, LIN + 93]. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. <p> Abstracting with credit is permitted. two parallel file-system workloads, supporting many users and running a variety of applications in a variety of scientific domains, under both data-parallel and control-parallel programming models, show that many parallel, scientific applications make many small, non-consecutive requests to the file system <ref> [KN94, PEK + 95, NKP + 95, NK95] </ref>. These studies suggest that most parallel file systems have been optimized for a workload that is different than that which actually exists. <p> Until recently, however, there had been no investigation into whether this file model and interface were well suited to massively parallel scientific applications. To determine whether this model was appropriate, we examined the file-system workloads on two different massively parallel processors, running two different application work-loads <ref> [KN94, PEK + 95] </ref>. These studies show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment than in uniprocessor or supercomputer environments.
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications <ref> [Pie89, PFDJ89, LIN + 93] </ref>. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. Studies of Copyright c fl1996 by the Association for Computing Machinery, Inc.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications <ref> [Pie89, PFDJ89, LIN + 93] </ref>. These applications tend to access large files in large, consecutive chunks [MK91, PP93]. Studies of Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> Section 5 we discuss some related work, and finally, in Section 6, we conclude and describe our future plans. 2 Background 2.1 Parallel File Systems Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93, WMR + 94] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> One enhancement to the conventional interface, which is offered by several multiprocessor file systems, is a file pointer that is shared among the processes in an application and provides a mechanism for regulating access to a shared file by those processes <ref> [Pie89, BGST93] </ref>. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Most parallel file systems provide this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer). <p> The declustering-unit size is frequently measured in kilobytes (e.g., 4KB in Intel's CFS <ref> [Pie89] </ref>), however, while our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes. This disparity means that most of the individual requests generated by parallel applications are not being executed in parallel. <p> Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, frequently cited as the canonical first-generation parallel file system, and its successor, PFS, are examples of file systems that provide a linear file model to the applications, and offer a Unix-like interface to the data.
Reference: [PP93] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year>
Reference-contexts: Specifically, it was commonly believed that parallel, scientific applications would have behavior similar to that of existing sequential and vector scientific applications [Pie89, PFDJ89, LIN + 93]. These applications tend to access large files in large, consecutive chunks <ref> [MK91, PP93] </ref>. Studies of Copyright c fl1996 by the Association for Computing Machinery, Inc.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Each IOP provides access to one simulated disk. Our implementation of the disk model was based on earlier implementations <ref> [RW94, KTR94] </ref>. Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache. <p> To validate our model, we used a trace-driven simulation, using data provided by Hewlett-Packard and used by Ruemmler and Wilkes in their study. 1 Comparing the results of this trace-driven simulation with the measured results from the actual disk, we obtained a demerit figure (see <ref> [RW94] </ref> for a discussion of this measure) of 5.0%, indicating that our model was extremely accurate. The simulated disk is integrated into Galley by creating a new thread on each IOP to execute the simulation.
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF [BGMZ92, HPF93] or CMF [Thi94]. There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices <ref> [TBC + 94, SW94] </ref>. The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94]. Finally, there are also plans to extend the MPI standard to include parallel I/O operations [MPI94, CFF + 95].
Reference: [SW95] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: This structuring may include storing distinct types of data in separate forks (e.g., a list of pressures in one fork and a list of temperatures in another), or it may involve storing metadata in one fork and `real' data in another (e.g., a compression library similar to that described in <ref> [SW95] </ref> could store compressed data chunks in one fork and directory information in another). An example of using forks for both data and metadata may be found in data files like those described in [KFG94].
Reference: [TBC + 94] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: These interfaces are sometimes tightly integrated into a particular language such as HPF [BGMZ92, HPF93] or CMF [Thi94]. There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices <ref> [TBC + 94, SW94] </ref>. The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94]. Finally, there are also plans to extend the MPI standard to include parallel I/O operations [MPI94, CFF + 95].
Reference: [Thi94] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, Mass. </address> <note> CM Fortran User's Guide, 2.1 edition, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: In addition to full file systems, there are numerous interfaces that are designed to allow programmers to describe their I/O needs at a higher semantic level. These interfaces are sometimes tightly integrated into a particular language such as HPF [BGMZ92, HPF93] or CMF <ref> [Thi94] </ref>. There are also many language-independent libraries to support parallel I/O, usually to support distributed matrices [TBC + 94, SW94]. The Jovian project explores the issues relating to the storage of irregular structures [BBS + 94].
Reference: [WGRW93] <author> David Womble, David Greenberg, Rolf Riesen, and Stephen Wheat. </author> <title> Out of core, out of mind: Practical parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-16, </pages> <institution> Mississippi State University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Rather, one could distribute the data across the disks using the keys and knowledge about how the dataset will be used to determine a partitioning scheme that results in highly parallel access. Finally, the parallel-I/O algorithms community has frequently argued for this kind of increased control over declustering <ref> [CK93, WGRW93] </ref>. To address these problems, Galley allows applications to fully control the way in which data is declustered across the IOPs, as well as which IOP they wish to access in each request.
Reference: [WMR + 94] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year>
Reference-contexts: Section 5 we discuss some related work, and finally, in Section 6, we conclude and describe our future plans. 2 Background 2.1 Parallel File Systems Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93, WMR + 94] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> Other examples of this type of parallel file system are SUNMOS (and its successor, PUMA) <ref> [WMR + 94] </ref>, sfs [LIN + 93], and CMMD [BGST93]. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
References-found: 39

