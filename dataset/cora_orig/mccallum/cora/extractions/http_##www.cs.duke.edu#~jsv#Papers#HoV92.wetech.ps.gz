URL: http://www.cs.duke.edu/~jsv/Papers/HoV92.wetech.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node28.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Analysis of Arithmetic Coding for Data Compression  
Author: Paul G. Howard and Jeffrey Scott Vitter 
Note: Appears in Information Processing and Management, Volume 28, Number 6, November 1992, pages 749-763. A shortened version appears in the proceedings of the IEEE Computer Society/NASA/CESDIS Data Compression Conference, Snowbird, Utah, April 8-11, 1991, pages 3-12.  
Abstract: Brown University Department of Computer Science Technical Report No. CS-92-17 Revised version, April 1992 (Formerly Technical Report No. CS-91-03) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. B. Arps, G. G. Langdon & J. J. Rissanen, </author> <title> "Method for Adaptively Initializing a Source Model for Symbol Encoding," </title> <journal> IBM Technical Disclosure Bulletin 26 (May 1984), </journal> <pages> 6292-6294. </pages>
Reference-contexts: Doing so would give too much weight to symbols that never occur.) See [2] or [3] for a detailed description of the PPM method. Witten, Cleary, Moffat, and Bell have proposed at least five methods for estimating the probability of the escape symbol [6,19,33], and Arps et al. <ref> [1] </ref> give two more. All of the methods give approximately the same compression; PPMB [6] is the most readily analyzed.
Reference: [2] <author> T. C. Bell, J. G. Cleary & I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Non-adaptive models are not very interesting, since their effectiveness depends only on how well their probabilities happen to match the statistics of the file being encoded; Bell, Cleary, and Witten show that the match can be arbitrarily bad <ref> [2] </ref>. Static and decrementing semi-adaptive codes. Semi-adaptive codes are conceptually simple, and useful when real-time operation is not required; their main drawback is that they require knowledge of the file statistics prior to encoding. <p> If we assume that all symbol distributions are equally likely for a given file length t, the cost of transmitting the exact model statistics is L M = lg (number of possible distributions) = lg t + n 1 ! ~ n lg (et=n): A similar result appears in <ref> [2] </ref> and [7]. For a typical file L M is only about 2560 bits, or 320 bytes. <p> In other words, L A = L SD + L M . This result is found in Rissanen [23,24]. Cleary and Witten [7] and Bell, Cleary, and Witten <ref> [2] </ref> present a similar result in a more general setting, showing approximate equality between enumerative codes (which are similar to arithmetic codes) and adaptive codes. <p> Doing so would give too much weight to symbols that never occur.) See <ref> [2] </ref> or [3] for a detailed description of the PPM method. Witten, Cleary, Moffat, and Bell have proposed at least five methods for estimating the probability of the escape symbol [6,19,33], and Arps et al. [1] give two more. <p> Empirical evidence that the coding effects are negligible appears in <ref> [2, 34] </ref>. 3.1 Rounding counts to integers In Section 2 we analyzed the modeling effect of periodic scaling; here we analyze the coding effect.
Reference: [3] <author> T. C. Bell, I. H. Witten & J. G. Cleary, </author> <title> "Modeling for Text Compression," </title> <journal> Comput. </journal> <note> Surveys 21 (Dec. </note> <year> 1989), </year> <pages> 557-591. </pages>
Reference-contexts: Doing so would give too much weight to symbols that never occur.) See [2] or <ref> [3] </ref> for a detailed description of the PPM method. Witten, Cleary, Moffat, and Bell have proposed at least five methods for estimating the probability of the escape symbol [6,19,33], and Arps et al. [1] give two more.
Reference: [4] <author> J. L. Bentley, D. D. Sleator, R. E. Tarjan & V. K. Wei, </author> <title> "A Locally Adaptive Data Compression Scheme," </title> <journal> Comm. ACM 29 (Apr. </journal> <year> 1986), </year> <pages> 320-330. </pages>
Reference: [5] <author> D. Chevion, E. D. Karnin & E. Walach, </author> <title> "High Efficiency, Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 43-52. </pages>
Reference: [6] <author> J. G. Cleary & I. H. Witten, </author> <title> "Data Compression Using Adaptive Coding and Partial String Matching," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-32 (Apr. </month> <year> 1984), </year> <pages> 396-402. </pages>
Reference-contexts: We conclude that the benefits of scaling usually outweigh the minor inefficiencies it sometimes introduces. 2.2 Application to Higher Order Models 13 2.2 Application to Higher Order Models We now extend our results to higher order models. Cleary and Witten <ref> [6] </ref> present a practical adaptive method called prediction by partial matching (or PPM ) in which they maintain models of various orders. <p> Witten, Cleary, Moffat, and Bell have proposed at least five methods for estimating the probability of the escape symbol [6,19,33], and Arps et al. [1] give two more. All of the methods give approximately the same compression; PPMB <ref> [6] </ref> is the most readily analyzed. In PPMB, in each context the escape event is treated as a separate symbol with its own weight and probability; the first occurrence of an ordinary symbol is not counted and the first two occurrences are coded as escapes.
Reference: [7] <author> J. G. Cleary & I. H. Witten, </author> <title> "A Comparison of Enumerative and Adaptive Codes," </title> <journal> IEEE Trans. Inform. Theory IT-30 (Mar. </journal> <year> 1984), </year> <pages> 306-315. </pages>
Reference-contexts: The decrementing count idea appears in the analysis of enumerative codes by Cleary and Witten <ref> [7] </ref>. <p> we assume that all symbol distributions are equally likely for a given file length t, the cost of transmitting the exact model statistics is L M = lg (number of possible distributions) = lg t + n 1 ! ~ n lg (et=n): A similar result appears in [2] and <ref> [7] </ref>. For a typical file L M is only about 2560 bits, or 320 bytes. <p> In other words, L A = L SD + L M . This result is found in Rissanen [23,24]. Cleary and Witten <ref> [7] </ref> and Bell, Cleary, and Witten [2] present a similar result in a more general setting, showing approximate equality between enumerative codes (which are similar to arithmetic codes) and adaptive codes.
Reference: [8] <author> G. V. Cormack & R. N. Horspool, </author> <title> "Data Compression Using Dynamic Markov Modelling," </title> <note> Computer Journal 30 (Dec. </note> <year> 1987), </year> <pages> 541-550. </pages>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models <ref> [8] </ref>. * Using a sliding window on the text [15]. This requires excessive computational resources. * Recency rank coding [4,10,29]. This is computationally simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [9,20].
Reference: [9] <author> G. V. Cormack & R. N. Horspool, </author> <title> "Algorithms for Adaptive Huffman Codes," </title> <journal> Inform. Process. Lett. </journal> <month> 18 (Mar. </month> <year> 1984), </year> <pages> 159-165. </pages>
Reference: [10] <author> P. Elias, </author> <title> "Interval and Recency Rank Source Coding: Two On-line Adaptive Variable Length Schemes," </title> <journal> IEEE Trans. Inform. Theory IT-33 (Jan. </journal> <year> 1987), </year> <pages> 3-10. </pages>
Reference: [11] <author> P. Elias, </author> <title> "Universal Codeword Sets and Representations of Integers," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 194-203. </pages>
Reference: [12] <author> A. S. Fraenkel & S. T. Klein, </author> <title> "Robust Universal Complete Codes as Alternatives to Huffman Codes," </title> <institution> Dept. of Applied Mathematics, The Weizmann Institute of Science, </institution> <type> Technical Report, </type> <institution> Rehovot, Israel, </institution> <year> 1985. </year>
Reference-contexts: The assumption of equally-likely distributions is not very good for text files; in practice we can reduce the cost of encoding the model by 50 percent or more by encoding each of the counts using a suitable encoding of the integers, such as Fibonacci coding <ref> [12] </ref>. Strictly speaking we must also encode the file length t before encoding the model; the cost is insignificant, between lg t and 2 lg t bits using an appropriate encoding of integers [11,31,32]. Adaptive codes.
Reference: [13] <author> P. G. Howard & J. S. Vitter, </author> <title> "Practical Implementations of Arithmetic Coding," in Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> 85-112. </pages>
Reference-contexts: Work by Rissanen, Langdon, Mohiuddin, and others at IBM [5,16,18,22,27] eliminates the division altogether and focuses on approximating the multiplication by combinations of additions and shifts. In <ref> [13] </ref> we present an alternative approach in which we approximate an arithmetic coder by a finite state automaton with a small number of states. Since the arithmetic computations are effectively stored in the state tables, coding can proceed quickly using only table lookups. Acknowledgement. We wish to thank Prof.
Reference: [14] <author> D. A. Huffman, </author> <title> "A Method for the Construction of Minimum Redundancy Codes," </title> <booktitle> Proceedings of the Institute of Radio Engineers 40 (1952), </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: 1 Introduction We analyze the amount of compression possible when arithmetic coding is used for text compression in conjunction with various input models. Arithmetic coding is a technique for statistical lossless encoding. It can be thought of as a generalization of Huffman coding <ref> [14] </ref> in which probabilities are not constrained to be integral powers of 2, and code lengths need not be integers. The basic algorithm for encoding using arithmetic coding works as follows: 1. We begin with a "current interval" initialized to [0::1]. 2.
Reference: [15] <author> D. E. Knuth, </author> <title> "Dynamic Huffman Coding," </title> <journal> J. </journal> <note> Algorithms 6 (June 1985), 163-180. 19 </note>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models [8]. * Using a sliding window on the text <ref> [15] </ref>. This requires excessive computational resources. * Recency rank coding [4,10,29]. This is computationally simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [9,20].
Reference: [16] <author> G. G. Langdon, </author> <title> "Probabilistic and Q-Coder Algorithms for Binary Source Adaptation," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 13-22. </pages>
Reference: [17] <author> G. G. Langdon, </author> <title> "An Introduction to Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 28 (Mar. </month> <year> 1984), </year> <pages> 135-149. </pages>
Reference-contexts: The final step uses almost exactly lg p bits to distinguish the file from all other possible files. For detailed descriptions of arithmetic coding, see <ref> [17] </ref> and especially [34].
Reference: [18] <author> G. G. Langdon & J. Rissanen, </author> <title> "Compression of Black-White Images with Arithmetic Coding," </title> <journal> IEEE Trans. Comm. COM-29 (1981), </journal> <pages> 858-867. </pages>
Reference: [19] <author> A. M. Moffat, </author> <title> "Implementing the PPM Data Compression Scheme," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-38 (Nov. </month> <year> 1990), </year> <pages> 1917-1921. </pages>
Reference: [20] <author> K. Mohiuddin, J. J. Rissanen & M. Wax, </author> <title> "Adaptive Model for Nonstationary Sources," </title> <journal> IBM Technical Disclosure Bulletin 28 (Apr. </journal> <year> 1986), </year> <pages> 4798-4800. </pages>
Reference: [21] <author> R. </author> <title> Pasco, "Source Coding Algorithms for Fast Data Compression," </title> <institution> Stanford Univ., </institution> <type> Ph.D. Thesis, </type> <year> 1976. </year>
Reference-contexts: In Section 2.2 we extend this analysis to higher-order models based on the partial string matching algorithm of Cleary and Witten. Through the years, practical adjustments have been made to arithmetic coding <ref> [21, 26,28,34] </ref> to allow the use of integer rather than rational or floating point arithmetic and to transmit output bits almost in real time instead of all at the end.
Reference: [22] <author> W. B. Pennebaker, J. L. Mitchell, G. G. Langdon & R. B. </author> <title> Arps, "An Overview of the Basic Principles of the Q-Coder Adaptive Binary Arithmetic Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 717-726. </pages>
Reference: [23] <author> J. Rissanen, </author> <title> "Stochastic Complexity and Modeling," </title> <journal> Ann. Statist. </journal> <volume> 14 (1986), </volume> <pages> 1080-1100. </pages>
Reference: [24] <author> J. Rissanen, </author> <title> "Stochastic Complexity," </title> <journal> J. Roy. Statist. Soc. Ser. </journal> <volume> B 49 (1987), </volume> <pages> 223-239, 253-265. </pages>
Reference: [25] <author> J. Rissanen, </author> <title> "Universal Coding, Information, Prediction, and Estimation," </title> <journal> IEEE Trans. Inform. Theory IT-30 (July 1984), </journal> <pages> 629-636. </pages>
Reference-contexts: Intuitively, the reason for the equality is that in the adaptive code, the cost of "learning" the model is not avoided, but merely spread over the entire file <ref> [25] </ref>. Organization of this article. Section 2 contains our main result, which precisely and provably characterizes the code length of a file dynamically coded with periodic count-scaling.
Reference: [26] <author> J. J. Rissanen, </author> <title> "Generalized Kraft Inequality and Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 20 (May </month> <year> 1976), </year> <pages> 198-203. </pages>
Reference: [27] <author> J. J. Rissanen & K. M. Mohiuddin, </author> <title> "A Multiplication-Free Multialphabet Arithmetic Code," </title> <journal> IEEE Trans. Comm. </journal> <month> 37 (Feb. </month> <year> 1989), </year> <pages> 93-98. </pages>
Reference: [28] <author> F. Rubin, </author> <title> "Arithmetic Stream Coding Using Fixed Precision Registers," </title> <journal> IEEE Trans. Inform. Theory IT-25 (Nov. </journal> <year> 1979), </year> <pages> 672-675. </pages>
Reference: [29] <author> B. Y. Ryabko, </author> <title> "Data Compression by Means of a Book Stack," </title> <note> Problemy Peredachi Informatsii 16 (1980). </note>
Reference: [30] <author> C. E. Shannon, </author> <title> "A Mathematical Theory of Communication," </title> <institution> Bell Syst. Tech. J. </institution> <month> 27 (July </month> <year> 1948), </year> <pages> 398-403. </pages>
Reference-contexts: This does not contradict Shannon's theorem <ref> [30] </ref>; he discusses only the best static code. Static semi-adaptive codes have been widely used in conjunction with Huffman coding, where they are appropriate since changing weights often requires changing the structure of the coding tree. Encoding the model.
Reference: [31] <author> R. G. Stone, </author> <title> "On Encoding of Commas Between Strings," </title> <journal> Comm. </journal> <note> ACM 22 (May 1979), 310-311. </note>
Reference: [32] <author> M. Wang, </author> <title> "Almost Asymptotically Optimal Flag Encoding of the Integers," </title> <journal> IEEE Trans. Inform. Theory IT-34 (Mar. </journal> <year> 1988), </year> <pages> 324-326. </pages> <address> 20 4 CONCLUSION </address>
Reference: [33] <author> I. H. Witten & T. C. Bell, </author> <title> "The Zero Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression," </title> <journal> IEEE Trans. Inform. Theory IT-37 (July 1991), </journal> <pages> 1085-1094. </pages>
Reference: [34] <author> I. H. Witten, R. M. Neal & J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Comm. </journal> <note> ACM 30 (June 1987), 520-540. </note>
Reference-contexts: The final step uses almost exactly lg p bits to distinguish the file from all other possible files. For detailed descriptions of arithmetic coding, see [17] and especially <ref> [34] </ref>. <p> This is the zero-frequency problem, discussed at length in [1,2,33]. For large files with small alphabets and simple models, all solutions to this problem give roughly the same compression. In this section we adopt the solution used in <ref> [34] </ref>, simply assigning an initial weight of 1 to all alphabet symbols. <p> This requires excessive computational resources. * Recency rank coding [4,10,29]. This is computationally simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [9,20]. This is moderately difficult to implement because of the changing weight increments. * Periodic scaling <ref> [34] </ref>. This is simple to implement, fast and effective in operation, and amenable to analysis. It also has the computationally desirable property of keeping the symbol weights small. In effect, scaling is a practical version of exponential aging. This is the method that we analyze. <p> Empirical evidence that the coding effects are negligible appears in <ref> [2, 34] </ref>. 3.1 Rounding counts to integers In Section 2 we analyzed the modeling effect of periodic scaling; here we analyze the coding effect.
References-found: 34

