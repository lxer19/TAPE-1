URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93322.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: ON THE CONVERGENCE OF PATTERN SEARCH ALGORITHMS  
Author: VIRGINIA TORCZON 
Note: AMS(MOS) subject classifications. 49D30, 65K05  
Abstract: This paper gives a unifying, abstract generalization of pattern search methods for solving nonlinear optimization problems. Pattern search methods are a class of direct search methods| methods that neither require nor explicitly approximate derivatives. We use the abstract description of pattern search methods to establish a global first-order stationary point convergence theory that neither requires the directional derivative nor enforces a notion of sufficient decrease. We also discuss the relationship between the convergence analysis for pattern search methods and the analysis for both line search and model trust region globalization strategies; in particular, the fact that we can relax the requirements on the acceptance of the step, at the expense of stronger conditions on the form of the step, and still guarantee global convergence. Key words. unconstrained optimization, convergence analysis, direct search methods, model trust region methods, line search methods, globalization strategies, alternating directions, alternating variable search, axial relaxation, local variation, coordinate search, response surface methodology, evolutionary operation, pattern search, multidirectional search, downhill simplex search 1. Introduction. In this paper, we study methods that require neither the directional derivative nor an approximation to the directional derivative to solve the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Avriel, </author> <title> Nonlinear Programming: Analysis and Methods, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: We set 1 = 0 so that k1 is defined when k = 0. A useful example for working through the logic of the algorithm can be found in <ref> [1] </ref>, though the presentation and notation differ somewhat from that given here. Algorithm 6. Exploratory Moves Algorithm for Hooke and Jeeves. Given x k , k , f (x k ), B, and k1 , set k = k1 and min = f (x k ).
Reference: [2] <author> G. E. P. </author> <title> Box, The exploration and exploitation of response surfaces: Some general considerations and examples, </title> <journal> Biometrics, </journal> <volume> 10 (1954), </volume> <pages> pp. 16-60. </pages>
Reference-contexts: been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box <ref> [2, 3] </ref>, and the original pattern search algorithm of Hooke and Jeeves [10]. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques.
Reference: [3] <author> G. E. P. </author> <title> Box, Evolutionary operation: A method for increasing industrial productivity, </title> <journal> Appl. Statist., </journal> <volume> 6 (1957), </volume> <pages> pp. 81-101. </pages>
Reference-contexts: been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box <ref> [2, 3] </ref>, and the original pattern search algorithm of Hooke and Jeeves [10]. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques. <p> In its earliest forms, response surface methodology was based on two-level factorial designs: evaluate the function at the vertices of a hypercube centered about the current iterate. (In fact, Box refers to this as one of a variety of "pattern of variants" <ref> [3] </ref>.) If simple decrease in the value of the objective function is observed at one of the vertices, it becomes the new iterate. Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated.
Reference: [4] <author> G. E. P. Box and K. B. Wilson, </author> <title> On the experimental attainment of optimum conditions, </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> XIII (1951), </volume> <pages> pp. 1-45. </pages>
Reference-contexts: For some time we have been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson <ref> [4] </ref> and later popularized by Box [2, 3], and the original pattern search algorithm of Hooke and Jeeves [10]. <p> We can guarantee first-order stationary point convergence, but to do so requires placing stronger conditions on the specifications for generalized pattern search methods. These stronger conditions are immediately satisfied by only one of the pattern search methods we will present (that due to G. E. P. Box and Wilson <ref> [4] </ref>). We could certainly impose these stronger conditions on the remaining pattern search methods presented in x5| none of them are unreasonable to suggest or to enforce|but we would do so at the expense of attractive algorithmic features found in the original methods. 3.1. The Algebraic Structure of the Iterates. <p> The Particular Pattern Search Methods. In x2 we stated the conditions an algorithm must satisfy to be a pattern search method. We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see <ref> [4] </ref> and [5]), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). <p> Theorem 3.5 thus holds. The exploratory moves algorithm for coordinate search would need to be modified to satisfy the Strong Hypotheses on Exploratory Moves for the conditions of Theorem 3.7 to be met, though this is a straightforward modification. 5.2. Response Surface Methodology. In 1951, Box and Wilson <ref> [4] </ref> introduced the notion of "response surface methodology" as a way to investigate an objective function by performing function evaluations at the vertices of some geometric configuration in the space of independent variables.
Reference: [5] <author> M. J. Box, D. Davies, and W. H. Swann, </author> <title> Non-Linear Optimization Techniques, </title> <journal> ICI Monograph No. </journal> <volume> 5, </volume> <publisher> Oliver & Boyd, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: The Particular Pattern Search Methods. In x2 we stated the conditions an algorithm must satisfy to be a pattern search method. We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and <ref> [5] </ref>), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). <p> Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated. Further discussions of the basic approach can be found in <ref> [5] </ref> and [17]. 5.2.1. The Matrices. As with coordinate search, the usual choice for the basis matrix is B = I, though, as with coordinate search, other choices may be made to reflect information known about the problem to be solved.
Reference: [6] <author> J. </author> <title> C ea, Optimisation : theorie et algorithmes, </title> <address> Dunod, Paris, </address> <year> 1971. </year>
Reference: [7] <author> W. C. Davidon, </author> <title> Variable metric method for minimization, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 1-17. </pages> <note> The belated preface was received by the editors June 4, 1990; accepted for publication August 10, 1990. The rest of the article was originally published as Argonne National Laboratory Research and Development Report 5990, </note> <month> May </month> <year> 1959 </year> <month> (revised November </month> <year> 1959). </year>
Reference-contexts: Coordinate Search with Fixed Step Lengths. The method of coordinate search is the simplest and most obvious of all the pattern search methods. Davidon describes it concisely in the opening of his belated preface to Argonne National Laboratory Research and Development Report 5990 <ref> [7] </ref>: Enrico Fermi and Nicholas Metropolis used one of the first digital computers, the Los Alamos Maniac, to determine which values of certain theoretical parameters (phase shifts) best fit experimental data (scattering cross sections).
Reference: [8] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: In x7.1 we show that pattern search methods are descent methods (as defined in either <ref> [8] </ref> or [14]) by showing that if x k is not a stationary point of the function then the generalized pattern search method guarantees decrease in the value of the objective function in a finite number of iterations. <p> The net effect of the algebraic structure of the pattern search methods, when combined with the Hypothesis on the Exploratory Moves and the algorithm for updating k , is to ensure that the pathologies which might otherwise occur were the Armijo-Goldstein-Wolfe conditions to be ignored (see <ref> [8] </ref>) cannot happen. These simple, if non-standard, mechanisms prevent the well-known pathologies that can arise if the length of the steps is not monitored. Parallels with the global convergence theory for model trust region methods are perhaps less obvious.
Reference: [9] <author> J. E. Dennis, Jr. and V. Torczon, </author> <title> Direct search methods on parallel machines, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 448-474. </pages>
Reference-contexts: under State of Texas Contract #1059. y Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892. 2 virginia torczon The important ideas for the convergence analysis for pattern search methods come from the convergence analysis developed by Torczon [20] for the multidirectional search algorithm of Dennis and Torczon <ref> [9, 19] </ref>. The main contribution of this paper is a concise abstraction of the key ingredients necessary for a more general convergence theory. <p> The multidirectional search algorithm is described in detail in both <ref> [9] </ref> and [20]. The formulation given here is different and, in fact, introduces some redundancy that can be eliminated when actually implementing the algorithm.
Reference: [10] <author> R. Hooke and T. A. Jeeves, </author> <title> "Direct search" solution of numerical and statistical problems, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8 (1961), </volume> <pages> pp. 212-229. </pages>
Reference-contexts: convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box [2, 3], and the original pattern search algorithm of Hooke and Jeeves <ref> [10] </ref>. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques. <p> We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and [5]), * the original pattern search method of Hooke and Jeeves <ref> [10] </ref>, and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). We will show that these algorithms satisfy the conditions that define pattern search methods and thus are special cases of the generalized pattern search method presented as Algorithm 1. <p> The algorithm, as stated above, also satisfies the conditions of Theorem 3.7. 5.3. Hooke and Jeeves' Pattern Search Algorithm. In addition to introducing the general notion of a "direct search" method, Hooke and Jeeves introduced the pattern search method|a specific kind of search strategy|in their 1961 paper <ref> [10] </ref>. The pattern search of Hooke and Jeeves is essentially a variant of coordinate search that incorporates a pattern step in an attempt to accelerate the progress of the algorithm by exploiting information gained from the search during previous successful iterations. The Hooke and Jeeves pattern search algorithm is opportunistic.
Reference: [11] <author> J. J. Mor e, </author> <title> Recent developments in algorithms and software for trust region methods, in Mathematical Programming, The State of the Art, </title> <editor> A. Bachem, M. Grotschel, and G. Korte, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1983, </year> <pages> pp. 256-287. </pages>
Reference-contexts: The proof of Theorem 3.7 is almost identical to that of an equivalent result for trust region methods that was first given by Thomas [18] and which is included, in a more general form, in the survey by More <ref> [11] </ref>. One final note: the hypotheses of Theorem 3.7 suggest that in the absence of any explicit higher-order information about the function to be minimized, it makes sense to terminate a generalized pattern search algorithm when k is less than some reasonably small tolerance.
Reference: [12] <author> J. A. Nelder and R. Mead, </author> <title> A simplex method for function minimization, </title> <journal> Comput. J., </journal> <volume> 7 (1965), </volume> <pages> pp. 308-313. </pages>
Reference-contexts: of points can be expressed as a simplex (i.e., n + 1 points, or vertices) based at the current iterate; as such, multidirectional search owes much in its conception to its predecessors, the simplex design algorithm of Spendley, Hext, and Himsworth [16] and the simplex algorithm of Nelder and Mead <ref> [12] </ref>. However, multidirectional search is a different algorithm|particularly from a theoretical standpoint. Convergence for the Spendley, Hext and Himsworth algorithm can be shown only with some modification of the original algorithm, and then only under the additional assumption that the function f is convex.
Reference: [13] <author> J. Nocedal, </author> <title> Theory of algorithms for unconstrained optimization, </title> <journal> Acta Numerica, </journal> <volume> 1 (1992), </volume> <pages> pp. 199-242. </pages>
Reference-contexts: Parallels with the line search theory are perhaps most obvious and are discussed in [20]. The outline for the convergence theory follows the outline for global convergence theorems as detailed by Ortega and Rheinboldt [14] and reviewed in the survey by Nocedal <ref> [13] </ref>: we consider iterations of the form x k+1 = x k + s k where s k 2 k P k ; the columns of P k determine the search directions and k serves as a step length parameter.
Reference: [14] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several on the convergence of pattern search algorithms 31 Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Parallels with the line search theory are perhaps most obvious and are discussed in [20]. The outline for the convergence theory follows the outline for global convergence theorems as detailed by Ortega and Rheinboldt <ref> [14] </ref> and reviewed in the survey by Nocedal [13]: we consider iterations of the form x k+1 = x k + s k where s k 2 k P k ; the columns of P k determine the search directions and k serves as a step length parameter. <p> In x7.1 we show that pattern search methods are descent methods (as defined in either [8] or <ref> [14] </ref>) by showing that if x k is not a stationary point of the function then the generalized pattern search method guarantees decrease in the value of the objective function in a finite number of iterations. <p> We then provide, in x7.2, a measure of the goodness of the search direction by showing that pattern search methods are gradient-related methods (as defined in <ref> [14] </ref>). Finally, in Theorem 3.3 and Proposition 3.4, we consider the length of the step. Given these major components for the convergence analysis, the one that is most unusual is that involving the step length control. <p> We will show, in fact, that this bound is uniform across all iterations of the pattern search algorithm. To do so, we use the notion of uniform linear independence <ref> [14] </ref>. Lemma 7.2. For a pattern search algorithm, there exists a constant ~ &gt; 0 such that for all k 0 and x 6= 0, max jx T (x i kxkkx i k x k k ) Proof. <p> Nevertheless, even though pattern search methods neither require nor explicitly approximate the gradient of the function, the uniform linear independence condition demonstrates that the pattern search methods are, in fact, gradient-related methods, as defined by Ortega and Rheinboldt <ref> [14] </ref>, which is one reason why we can establish global first-order stationary point convergence. 7.3. The Descent Condition. Having introduced the notion of uniform linear independence with the bound ~, we are now ready to show that pattern search methods reduce k only when necessary to find descent.
Reference: [15] <author> M. J. D. Powell, </author> <title> Convergence properties of a class of minimization algorithms, in Nonlinear Programming 2, </title> <editor> O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., </editor> <publisher> Academic Press, </publisher> <year> 1975, </year> <pages> pp. 1-27. </pages>
Reference-contexts: If the sampling does not produce simple decrease, we reduce the radius of the trust region. It is worth comparing the global first-order stationary point convergence results for trust region methods and generalized pattern search methods. Roughly speaking, the result due to Powell <ref> [15] </ref> says that lim inf krf (x k )k = 0 assuming that rf is continuous and the Hessian (or its approximation) remains uni formly bounded.
Reference: [16] <author> W. Spendley, G. R. Hext, and F. R. Himsworth, </author> <title> Sequential application of simplex designs in optimisation and evolutionary operation, </title> <journal> Technometrics, </journal> <volume> 4 (1962), </volume> <pages> pp. 441-461. </pages>
Reference-contexts: However, at best the pattern search methods are relying on implicit information about the gradient; they do not construct local quadratic models of the function. The only natural notion of a full step is that given by k . As has long been recognized <ref> [16] </ref>, this means that pattern search methods do not enjoy fast local convergence properties. When using these methods one trades speed of convergence for robustness and wider applicability. One other point worth making is that pattern search methods are well-defined even when the function to be minimized is not differentiable. <p> The pattern of points can be expressed as a simplex (i.e., n + 1 points, or vertices) based at the current iterate; as such, multidirectional search owes much in its conception to its predecessors, the simplex design algorithm of Spendley, Hext, and Himsworth <ref> [16] </ref> and the simplex algorithm of Nelder and Mead [12]. However, multidirectional search is a different algorithm|particularly from a theoretical standpoint.
Reference: [17] <author> W. H. Swann, </author> <title> Direct search methods, in Numerical Methods for Unconstrained Optimizations, </title> <editor> W. Murray, ed., </editor> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1972, </year> <pages> pp. 13-28. </pages>
Reference-contexts: Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated. Further discussions of the basic approach can be found in [5] and <ref> [17] </ref>. 5.2.1. The Matrices. As with coordinate search, the usual choice for the basis matrix is B = I, though, as with coordinate search, other choices may be made to reflect information known about the problem to be solved.
Reference: [18] <author> S. W. Thomas, </author> <title> Sequential estimation techniques for quasi-Newton algorithms, </title> <type> Ph.D. thesis, </type> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1975. </year>
Reference-contexts: The proof of Theorem 3.7 is almost identical to that of an equivalent result for trust region methods that was first given by Thomas <ref> [18] </ref> and which is included, in a more general form, in the survey by More [11]. <p> Roughly speaking, the result due to Powell [15] says that lim inf krf (x k )k = 0 assuming that rf is continuous and the Hessian (or its approximation) remains uni formly bounded. Thomas <ref> [18] </ref> assumes, in addition, that rf is uniformly continuous to prove that lim krf (x k )k = 0 Neither result requires L (x 0 ) to be compact; instead, f must be bounded below.
Reference: [19] <author> V. Torczon, </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines, </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1989; also available as Tech. Report 90-7, </note> <institution> Department of Mathematical Sciences, Rice University, Houston, </institution> <month> TX 77251-1892. </month> <title> [20] , On the convergence of the multidirectional search algorithm, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. </pages> <month> 123-145. </month> <title> [21] , PDS: Direct search methods for unconstrained optimization on either sequential or parallel machines, </title> <type> Tech. Report 92-9, </type> <institution> Department of Mathematical Sciences, Rice University, </institution> <address> Houston, TX 77251-1892, </address> <year> 1992. </year>
Reference-contexts: under State of Texas Contract #1059. y Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892. 2 virginia torczon The important ideas for the convergence analysis for pattern search methods come from the convergence analysis developed by Torczon [20] for the multidirectional search algorithm of Dennis and Torczon <ref> [9, 19] </ref>. The main contribution of this paper is a concise abstraction of the key ingredients necessary for a more general convergence theory. <p> illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and [5]), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and <ref> [19] </ref>). We will show that these algorithms satisfy the conditions that define pattern search methods and thus are special cases of the generalized pattern search method presented as Algorithm 1. <p> Multidirectional Search. The multidirectional search algorithm was introduced by Dennis and Torczon in 1989 <ref> [19] </ref> as a first step towards a general purpose optimization algorithm with promising properties for parallel computation. <p> There are numerical examples to demonstrate that the Nelder-Mead simplex algorithm may fail to converge to a stationary point of the function because the uniform linear independence property (discussed in x7.2), which plays a key role in the convergence analysis, cannot be guaranteed to hold <ref> [19] </ref>. The multidirectional search algorithm is described in detail in both [9] and [20]. The formulation given here is different and, in fact, introduces some redundancy that can be eliminated when actually implementing the algorithm.

Reference: [1] <author> M. Avriel, </author> <title> Nonlinear Programming: Analysis and Methods, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: We set 1 = 0 so that k1 is defined when k = 0. A useful example for working through the logic of the algorithm can be found in <ref> [1] </ref>, though the presentation and notation differ somewhat from that given here. Algorithm 6. Exploratory Moves Algorithm for Hooke and Jeeves. Given x k , k , f (x k ), B, and k1 , set k = k1 and min = f (x k ).
Reference: [2] <author> G. E. P. </author> <title> Box, The exploration and exploitation of response surfaces: Some general considerations and examples, </title> <journal> Biometrics, </journal> <volume> 10 (1954), </volume> <pages> pp. 16-60. </pages>
Reference-contexts: been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box <ref> [2, 3] </ref>, and the original pattern search algorithm of Hooke and Jeeves [10]. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques.
Reference: [3] <author> G. E. P. </author> <title> Box, Evolutionary operation: A method for increasing industrial productivity, </title> <journal> Appl. Statist., </journal> <volume> 6 (1957), </volume> <pages> pp. 81-101. </pages>
Reference-contexts: been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box <ref> [2, 3] </ref>, and the original pattern search algorithm of Hooke and Jeeves [10]. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques. <p> In its earliest forms, response surface methodology was based on two-level factorial designs: evaluate the function at the vertices of a hypercube centered about the current iterate. (In fact, Box refers to this as one of a variety of "pattern of variants" <ref> [3] </ref>.) If simple decrease in the value of the objective function is observed at one of the vertices, it becomes the new iterate. Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated.
Reference: [4] <author> G. E. P. Box and K. B. Wilson, </author> <title> On the experimental attainment of optimum conditions, </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> XIII (1951), </volume> <pages> pp. 1-45. </pages>
Reference-contexts: For some time we have been aware that the same style of argument used to prove global convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson <ref> [4] </ref> and later popularized by Box [2, 3], and the original pattern search algorithm of Hooke and Jeeves [10]. <p> We can guarantee first-order stationary point convergence, but to do so requires placing stronger conditions on the specifications for generalized pattern search methods. These stronger conditions are immediately satisfied by only one of the pattern search methods we will present (that due to G. E. P. Box and Wilson <ref> [4] </ref>). We could certainly impose these stronger conditions on the remaining pattern search methods presented in x5| none of them are unreasonable to suggest or to enforce|but we would do so at the expense of attractive algorithmic features found in the original methods. 3.1. The Algebraic Structure of the Iterates. <p> The Particular Pattern Search Methods. In x2 we stated the conditions an algorithm must satisfy to be a pattern search method. We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see <ref> [4] </ref> and [5]), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). <p> Theorem 3.5 thus holds. The exploratory moves algorithm for coordinate search would need to be modified to satisfy the Strong Hypotheses on Exploratory Moves for the conditions of Theorem 3.7 to be met, though this is a straightforward modification. 5.2. Response Surface Methodology. In 1951, Box and Wilson <ref> [4] </ref> introduced the notion of "response surface methodology" as a way to investigate an objective function by performing function evaluations at the vertices of some geometric configuration in the space of independent variables.
Reference: [5] <author> M. J. Box, D. Davies, and W. H. Swann, </author> <title> Non-Linear Optimization Techniques, </title> <journal> ICI Monograph No. </journal> <volume> 5, </volume> <publisher> Oliver & Boyd, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: The Particular Pattern Search Methods. In x2 we stated the conditions an algorithm must satisfy to be a pattern search method. We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and <ref> [5] </ref>), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). <p> Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated. Further discussions of the basic approach can be found in <ref> [5] </ref> and [17]. 5.2.1. The Matrices. As with coordinate search, the usual choice for the basis matrix is B = I, though, as with coordinate search, other choices may be made to reflect information known about the problem to be solved.
Reference: [6] <author> J. </author> <title> C ea, Optimisation : theorie et algorithmes, </title> <address> Dunod, Paris, </address> <year> 1971. </year>
Reference: [7] <author> W. C. Davidon, </author> <title> Variable metric method for minimization, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 1-17. </pages> <note> The belated preface was received by the editors June 4, 1990; accepted for publication August 10, 1990. The rest of the article was originally published as Argonne National Laboratory Research and Development Report 5990, </note> <month> May </month> <year> 1959 </year> <month> (revised November </month> <year> 1959). </year>
Reference-contexts: Coordinate Search with Fixed Step Lengths. The method of coordinate search is the simplest and most obvious of all the pattern search methods. Davidon describes it concisely in the opening of his belated preface to Argonne National Laboratory Research and Development Report 5990 <ref> [7] </ref>: Enrico Fermi and Nicholas Metropolis used one of the first digital computers, the Los Alamos Maniac, to determine which values of certain theoretical parameters (phase shifts) best fit experimental data (scattering cross sections).
Reference: [8] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: In x7.1 we show that pattern search methods are descent methods (as defined in either <ref> [8] </ref> or [14]) by showing that if x k is not a stationary point of the function then the generalized pattern search method guarantees decrease in the value of the objective function in a finite number of iterations. <p> The net effect of the algebraic structure of the pattern search methods, when combined with the Hypothesis on the Exploratory Moves and the algorithm for updating k , is to ensure that the pathologies which might otherwise occur were the Armijo-Goldstein-Wolfe conditions to be ignored (see <ref> [8] </ref>) cannot happen. These simple, if non-standard, mechanisms prevent the well-known pathologies that can arise if the length of the steps is not monitored. Parallels with the global convergence theory for model trust region methods are perhaps less obvious.
Reference: [9] <author> J. E. Dennis, Jr. and V. Torczon, </author> <title> Direct search methods on parallel machines, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 448-474. </pages>
Reference-contexts: under State of Texas Contract #1059. y Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892. 2 virginia torczon The important ideas for the convergence analysis for pattern search methods come from the convergence analysis developed by Torczon [20] for the multidirectional search algorithm of Dennis and Torczon <ref> [9, 19] </ref>. The main contribution of this paper is a concise abstraction of the key ingredients necessary for a more general convergence theory. <p> The multidirectional search algorithm is described in detail in both <ref> [9] </ref> and [20]. The formulation given here is different and, in fact, introduces some redundancy that can be eliminated when actually implementing the algorithm.
Reference: [10] <author> R. Hooke and T. A. Jeeves, </author> <title> "Direct search" solution of numerical and statistical problems, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8 (1961), </volume> <pages> pp. 212-229. </pages>
Reference-contexts: convergence for the multidirectional search algorithm could be applied, individually, to such classical algorithms as coordinate search, with fixed step sizes, variants of response surface methodology, first developed by Box and Wilson [4] and later popularized by Box [2, 3], and the original pattern search algorithm of Hooke and Jeeves <ref> [10] </ref>. The challenge was to develop an abstraction that both allowed for a general convergence theory and explained why such algorithms, often viewed as disparate direct search methods, could be analyzed using the same techniques. <p> We now illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and [5]), * the original pattern search method of Hooke and Jeeves <ref> [10] </ref>, and * the multidirectional search algorithm of Dennis and Torczon ([9] and [19]). We will show that these algorithms satisfy the conditions that define pattern search methods and thus are special cases of the generalized pattern search method presented as Algorithm 1. <p> The algorithm, as stated above, also satisfies the conditions of Theorem 3.7. 5.3. Hooke and Jeeves' Pattern Search Algorithm. In addition to introducing the general notion of a "direct search" method, Hooke and Jeeves introduced the pattern search method|a specific kind of search strategy|in their 1961 paper <ref> [10] </ref>. The pattern search of Hooke and Jeeves is essentially a variant of coordinate search that incorporates a pattern step in an attempt to accelerate the progress of the algorithm by exploiting information gained from the search during previous successful iterations. The Hooke and Jeeves pattern search algorithm is opportunistic.
Reference: [11] <author> J. J. Mor e, </author> <title> Recent developments in algorithms and software for trust region methods, in Mathematical Programming, The State of the Art, </title> <editor> A. Bachem, M. Grotschel, and G. Korte, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1983, </year> <pages> pp. 256-287. </pages>
Reference-contexts: The proof of Theorem 3.7 is almost identical to that of an equivalent result for trust region methods that was first given by Thomas [18] and which is included, in a more general form, in the survey by More <ref> [11] </ref>. One final note: the hypotheses of Theorem 3.7 suggest that in the absence of any explicit higher-order information about the function to be minimized, it makes sense to terminate a generalized pattern search algorithm when k is less than some reasonably small tolerance.
Reference: [12] <author> J. A. Nelder and R. Mead, </author> <title> A simplex method for function minimization, </title> <journal> Comput. J., </journal> <volume> 7 (1965), </volume> <pages> pp. 308-313. </pages>
Reference-contexts: of points can be expressed as a simplex (i.e., n + 1 points, or vertices) based at the current iterate; as such, multidirectional search owes much in its conception to its predecessors, the simplex design algorithm of Spendley, Hext, and Himsworth [16] and the simplex algorithm of Nelder and Mead <ref> [12] </ref>. However, multidirectional search is a different algorithm|particularly from a theoretical standpoint. Convergence for the Spendley, Hext and Himsworth algorithm can be shown only with some modification of the original algorithm, and then only under the additional assumption that the function f is convex.
Reference: [13] <author> J. Nocedal, </author> <title> Theory of algorithms for unconstrained optimization, </title> <journal> Acta Numerica, </journal> <volume> 1 (1992), </volume> <pages> pp. 199-242. </pages>
Reference-contexts: Parallels with the line search theory are perhaps most obvious and are discussed in [20]. The outline for the convergence theory follows the outline for global convergence theorems as detailed by Ortega and Rheinboldt [14] and reviewed in the survey by Nocedal <ref> [13] </ref>: we consider iterations of the form x k+1 = x k + s k where s k 2 k P k ; the columns of P k determine the search directions and k serves as a step length parameter.
Reference: [14] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several on the convergence of pattern search algorithms 31 Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Parallels with the line search theory are perhaps most obvious and are discussed in [20]. The outline for the convergence theory follows the outline for global convergence theorems as detailed by Ortega and Rheinboldt <ref> [14] </ref> and reviewed in the survey by Nocedal [13]: we consider iterations of the form x k+1 = x k + s k where s k 2 k P k ; the columns of P k determine the search directions and k serves as a step length parameter. <p> In x7.1 we show that pattern search methods are descent methods (as defined in either [8] or <ref> [14] </ref>) by showing that if x k is not a stationary point of the function then the generalized pattern search method guarantees decrease in the value of the objective function in a finite number of iterations. <p> We then provide, in x7.2, a measure of the goodness of the search direction by showing that pattern search methods are gradient-related methods (as defined in <ref> [14] </ref>). Finally, in Theorem 3.3 and Proposition 3.4, we consider the length of the step. Given these major components for the convergence analysis, the one that is most unusual is that involving the step length control. <p> We will show, in fact, that this bound is uniform across all iterations of the pattern search algorithm. To do so, we use the notion of uniform linear independence <ref> [14] </ref>. Lemma 7.2. For a pattern search algorithm, there exists a constant ~ &gt; 0 such that for all k 0 and x 6= 0, max jx T (x i kxkkx i k x k k ) Proof. <p> Nevertheless, even though pattern search methods neither require nor explicitly approximate the gradient of the function, the uniform linear independence condition demonstrates that the pattern search methods are, in fact, gradient-related methods, as defined by Ortega and Rheinboldt <ref> [14] </ref>, which is one reason why we can establish global first-order stationary point convergence. 7.3. The Descent Condition. Having introduced the notion of uniform linear independence with the bound ~, we are now ready to show that pattern search methods reduce k only when necessary to find descent.
Reference: [15] <author> M. J. D. Powell, </author> <title> Convergence properties of a class of minimization algorithms, in Nonlinear Programming 2, </title> <editor> O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., </editor> <publisher> Academic Press, </publisher> <year> 1975, </year> <pages> pp. 1-27. </pages>
Reference-contexts: If the sampling does not produce simple decrease, we reduce the radius of the trust region. It is worth comparing the global first-order stationary point convergence results for trust region methods and generalized pattern search methods. Roughly speaking, the result due to Powell <ref> [15] </ref> says that lim inf krf (x k )k = 0 assuming that rf is continuous and the Hessian (or its approximation) remains uni formly bounded.
Reference: [16] <author> W. Spendley, G. R. Hext, and F. R. Himsworth, </author> <title> Sequential application of simplex designs in optimisation and evolutionary operation, </title> <journal> Technometrics, </journal> <volume> 4 (1962), </volume> <pages> pp. 441-461. </pages>
Reference-contexts: However, at best the pattern search methods are relying on implicit information about the gradient; they do not construct local quadratic models of the function. The only natural notion of a full step is that given by k . As has long been recognized <ref> [16] </ref>, this means that pattern search methods do not enjoy fast local convergence properties. When using these methods one trades speed of convergence for robustness and wider applicability. One other point worth making is that pattern search methods are well-defined even when the function to be minimized is not differentiable. <p> The pattern of points can be expressed as a simplex (i.e., n + 1 points, or vertices) based at the current iterate; as such, multidirectional search owes much in its conception to its predecessors, the simplex design algorithm of Spendley, Hext, and Himsworth <ref> [16] </ref> and the simplex algorithm of Nelder and Mead [12]. However, multidirectional search is a different algorithm|particularly from a theoretical standpoint.
Reference: [17] <author> W. H. Swann, </author> <title> Direct search methods, in Numerical Methods for Unconstrained Optimizations, </title> <editor> W. Murray, ed., </editor> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1972, </year> <pages> pp. 13-28. </pages>
Reference-contexts: Otherwise, the lengths of the edges in the hypercube are halved and the process is repeated. Further discussions of the basic approach can be found in [5] and <ref> [17] </ref>. 5.2.1. The Matrices. As with coordinate search, the usual choice for the basis matrix is B = I, though, as with coordinate search, other choices may be made to reflect information known about the problem to be solved.
Reference: [18] <author> S. W. Thomas, </author> <title> Sequential estimation techniques for quasi-Newton algorithms, </title> <type> Ph.D. thesis, </type> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1975. </year>
Reference-contexts: The proof of Theorem 3.7 is almost identical to that of an equivalent result for trust region methods that was first given by Thomas <ref> [18] </ref> and which is included, in a more general form, in the survey by More [11]. <p> Roughly speaking, the result due to Powell [15] says that lim inf krf (x k )k = 0 assuming that rf is continuous and the Hessian (or its approximation) remains uni formly bounded. Thomas <ref> [18] </ref> assumes, in addition, that rf is uniformly continuous to prove that lim krf (x k )k = 0 Neither result requires L (x 0 ) to be compact; instead, f must be bounded below.
Reference: [19] <author> V. Torczon, </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines, </title> <type> Ph.D. thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1989; also available as Tech. Report 90-7, </note> <institution> Department of Mathematical Sciences, Rice University, Houston, </institution> <month> TX 77251-1892. </month> <title> [20] , On the convergence of the multidirectional search algorithm, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. </pages> <month> 123-145. </month> <title> [21] , PDS: Direct search methods for unconstrained optimization on either sequential or parallel machines, </title> <type> Tech. Report 92-9, </type> <institution> Department of Mathematical Sciences, Rice University, </institution> <address> Houston, TX 77251-1892, </address> <year> 1992. </year>
Reference-contexts: under State of Texas Contract #1059. y Department of Computational and Applied Mathematics, Rice University, Houston, Texas 77251-1892. 2 virginia torczon The important ideas for the convergence analysis for pattern search methods come from the convergence analysis developed by Torczon [20] for the multidirectional search algorithm of Dennis and Torczon <ref> [9, 19] </ref>. The main contribution of this paper is a concise abstraction of the key ingredients necessary for a more general convergence theory. <p> illustrate these conditions by considering the following specific algorithms: * coordinate search with fixed step lengths, * searches based on two-level factorial designs (see [4] and [5]), * the original pattern search method of Hooke and Jeeves [10], and * the multidirectional search algorithm of Dennis and Torczon ([9] and <ref> [19] </ref>). We will show that these algorithms satisfy the conditions that define pattern search methods and thus are special cases of the generalized pattern search method presented as Algorithm 1. <p> Multidirectional Search. The multidirectional search algorithm was introduced by Dennis and Torczon in 1989 <ref> [19] </ref> as a first step towards a general purpose optimization algorithm with promising properties for parallel computation. <p> There are numerical examples to demonstrate that the Nelder-Mead simplex algorithm may fail to converge to a stationary point of the function because the uniform linear independence property (discussed in x7.2), which plays a key role in the convergence analysis, cannot be guaranteed to hold <ref> [19] </ref>. The multidirectional search algorithm is described in detail in both [9] and [20]. The formulation given here is different and, in fact, introduces some redundancy that can be eliminated when actually implementing the algorithm.
Reference: [22] <author> Y. Wen-ci, </author> <title> Positive basis and a class of direct search techniques, </title> <journal> Scientia Sinica, Special Issue of Mathematics, </journal> <volume> 1 (1979), </volume> <pages> pp. 53-67. </pages>
Reference-contexts: The combination of these two mechanisms: * Hypotheses on the Exploratory Moves * Algorithm 2 for updating k introduces backtracking into the generalized pattern search methods, which prevents steps that are too short. This is the import of Lemma 3.1. 1 The analysis in <ref> [22] </ref> enforces a notion of sufficient decrease by imposing on the methods an "error-controlling sequence" that does not exist in the original algorithms.
References-found: 39

