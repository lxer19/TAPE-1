URL: http://bugle.cs.uiuc.edu/Papers/PCS.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Projects/HPF/publications.html
Root-URL: http://www.cs.uiuc.edu
Title: Integrating Compilation and Performance Analysis for Data-Parallel Programs  
Author: Vikram S. Adve John Mellor-Crummey Mark Anderson Ken Kennedy Jhy-Chun Wang Daniel A. Reed 
Address: Houston, Texas 77251-1892  Urbana, Illinois 61801  
Affiliation: Center for Research on Parallel Computation Rice University  Department of Computer Science University of Illinois  
Abstract: The advantages of abstract, high-level programming languages like High Performance Fortran will be fully realized only if programmers are able to understand and tune the performance of their applications without having to understand the details of the compiler-synthesized, explicitly parallel code. Providing such support will require extensive integration of compilation and performance analysis to overcome the deep semantic gap between source and compiled code. We describe the integration of the University of Illinois' Pablo performance environment and Rice University D compiler to form a prototype programming environment that supports performance analysis of regular, data parallel programs written in Fortran D. By this integration, the programming environment can help the programmer to understand the qualitative impact of code design choices on parallelism and communication, and understand dynamic performance characteristics in terms of the original source code.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adve, V. S., Wang, J.-C., Mellor-Crummey, J. M., Reed, D. A., Anderson, M., and Kennedy, K. </author> <title> An integrated Compilation and Performance Analysis Environment for Data-Parallel Programs. </title> <type> Tech. rep., </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The description here focuses on the intended functionality of each of these components; more detailed descriptions of these components and their implementations to date are 7 available elsewhere (in <ref> [6, 8, 1] </ref> respectively). 3.1 System Organization the Illinois' Pablo performance environment. <p> The above static information provides a strong foundation for the integration of compilation and performance analysis. The details of how the above information is organized in the static SDDF file and how it is retrieved and used to correlate dynamic performance information with the data-parallel source are described in <ref> [1] </ref>. Of the major components of the static information described above, all but two have been implemented, i.e., are now generated by the Fortran D compiler.
Reference: [2] <institution> Applied Parallel Research. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 ed. Plac-erville, CA, </note> <year> 1992. </year>
Reference-contexts: A few performance tools for high-level languages include Prism [20] and NV [7] for CM-Fortran, the MPP-Apprentice performance tool for C, Fortran and Fortran 90 on the Cray T3D [11], and Forge90 <ref> [2] </ref> for Fortran 90 and HPF. These tools share the goal of presenting source-level performance information. Prism provides statement-level performance annotations of CM-Fortran. The lack of aggressive optimization in the CM-Fortran compiler, however, makes it relatively easy to map communication costs back to the statement responsible.
Reference: [3] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: where necessary, and replaces the corresponding array references with buffer references to avoid unbuffering costs when possible. 2.2 Pablo Performance Environment The Pablo environment consists of three primary components: an extensible data capture library [16], a data meta-format for describing the structure of performance data records without constraining their contents <ref> [3] </ref>, and a graphical data analysis toolkit that allows users to quickly construct new performance data analyses [14, 19]. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing calls.
Reference: [4] <author> C.-T.King, W.-H., and Ni, L. M. </author> <title> Pipelined data-parallel algorithms: Part ii-design. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 4 (Oct. </month> <year> 1990). </year>
Reference-contexts: Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>.
Reference: [5] <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93 (Portland, </booktitle> <address> OR, </address> <month> Nov. </month> <year> 1993). </year>
Reference-contexts: Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>.
Reference: [6] <author> Hiranandani, S., Kennedy, K., Tseng, C.-W., and Warren, S. </author> <title> The D Editor: A new interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '94 (Washington, </booktitle> <address> DC, </address> <month> Nov. </month> <year> 1994). </year>
Reference-contexts: The description here focuses on the intended functionality of each of these components; more detailed descriptions of these components and their implementations to date are 7 available elsewhere (in <ref> [6, 8, 1] </ref> respectively). 3.1 System Organization the Illinois' Pablo performance environment.
Reference: [7] <author> Irvin, R. B., and Miller, B. P. </author> <title> A Performance Tool for High-Level Parallel Languages. </title> <type> Tech. Rep. 1204, </type> <institution> University of Wisconsin-Madison, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: A few performance tools for high-level languages include Prism [20] and NV <ref> [7] </ref> for CM-Fortran, the MPP-Apprentice performance tool for C, Fortran and Fortran 90 on the Cray T3D [11], and Forge90 [2] for Fortran 90 and HPF. These tools share the goal of presenting source-level performance information. Prism provides statement-level performance annotations of CM-Fortran.
Reference: [8] <author> Mellor-Crummey, J. M., Adve, V. S., and Koelbel, C. </author> <title> The Compiler's Role in Analysis and Tuning of Data-Parallel Programs. </title> <booktitle> In Proceedings of The Second Workshop on Environments and Tools for Parallel Scientific Computing (Townsend, </booktitle> <address> TN, </address> <month> May </month> <year> 1994), </year> <pages> pp. 211-220. </pages> <note> Also available via anonymous ftp from soft-lib.cs.rice.edu in pub/CRPC-TRs/reports/CRPC-TR94405.ps. 17 </note>
Reference-contexts: The description here focuses on the intended functionality of each of these components; more detailed descriptions of these components and their implementations to date are 7 available elsewhere (in <ref> [6, 8, 1] </ref> respectively). 3.1 System Organization the Illinois' Pablo performance environment. <p> The total pipeline execution time can be computed as the length of any critical path through the pipeline; one possible critical path is shown in Figure 7. Under these assumptions, it is easy to show that the optimal block size is given by <ref> [8] </ref>: B opt = u t N 2 (C f;stage + C f;rovhd + C f;sovhd ) (P 1)( C l;stage N 1 P + C l;comm ) C l;rovhd C l;sovhd (1) Note that B opt increases roughly as 1= p C l;comm , meaning that as bandwidth increases, larger
Reference: [9] <author> Miller, B. P., Clark, M., Hollingsworth, J., Kierstead, S., Lim, S.-S., and Torzewski, T. IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System. </title> <journal> IEEE Transactions on Computers 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 206-217. </pages>
Reference-contexts: Second, performance information obtained by measuring the executing program most directly relates to the characteristics of the explicitly parallel code, and is extremely difficult to interpret in relation to the source code written by the programmer. For example, for the code fragment above, current performance instrumentation and analysis tools <ref> [15, 14, 9, 17] </ref> for distributed memory parallel systems would capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [10] <author> Palermo, D. J., Su, E., Chandy, J. A., and Bannerjee, P. </author> <title> Communication optmizations used in the paradigm compiler for distributed-memory machines. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing (St. </booktitle> <address> Charles, IL, Aug. 1994), </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. </pages> <month> II:1-8. </month>
Reference-contexts: Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>. <p> Finally, as described in Section 3.4, a number of previous papers have described performance models for optimizing pipeline granularity. The only compiler implementation of such a model we are aware of is in the Paradigm compiler, which uses static estimates of the pipeline model parameters to apply the model <ref> [10] </ref>.
Reference: [11] <author> Pase, D. </author> <title> MPP Apprentice: A Non-Event Trace Performance Tool for the CRAY T3D. Presentation at the Workshop on Debugging and Performance Tuning for Parallel Computing Systems, </title> <address> Oct. </address> <year> 1994. </year>
Reference-contexts: A few performance tools for high-level languages include Prism [20] and NV [7] for CM-Fortran, the MPP-Apprentice performance tool for C, Fortran and Fortran 90 on the Cray T3D <ref> [11] </ref>, and Forge90 [2] for Fortran 90 and HPF. These tools share the goal of presenting source-level performance information. Prism provides statement-level performance annotations of CM-Fortran. The lack of aggressive optimization in the CM-Fortran compiler, however, makes it relatively easy to map communication costs back to the statement responsible.
Reference: [12] <author> Ramanujam, J. </author> <title> Compile-time Techniques for Parallel Execution of Loops on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, Ohio State University, Columbus, OH, </institution> <year> 1990. </year>
Reference-contexts: Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>.
Reference: [13] <author> Ramanujam, J., and Sadayappan, P. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proceedings of Supercomputing '91 (Albuquerque, </booktitle> <address> NM, </address> <month> Nov. </month> <year> 1991). </year>
Reference-contexts: Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>.
Reference: [14] <author> Reed, D. A. </author> <title> Performance Instrumentation Techniques for Parallel Systems. In Models and Techniques for Performance Evaluation of Computer and Communications Systems, </title> <editor> L. Donatiello and R. Nelson, Eds. </editor> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <year> 1993. </year>
Reference-contexts: 2.2 Pablo Performance Environment The Pablo environment consists of three primary components: an extensible data capture library [16], a data meta-format for describing the structure of performance data records without constraining their contents [3], and a graphical data analysis toolkit that allows users to quickly construct new performance data analyses <ref> [14, 19] </ref>. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing calls. A group of library extension interfaces allow instrumentation software developers to incrementally add new functionality to the library. <p> Second, performance information obtained by measuring the executing program most directly relates to the characteristics of the explicitly parallel code, and is extremely difficult to interpret in relation to the source code written by the programmer. For example, for the code fragment above, current performance instrumentation and analysis tools <ref> [15, 14, 9, 17] </ref> for distributed memory parallel systems would capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [15] <author> Reed, D. A. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation (May 1994). </booktitle>
Reference-contexts: Second, performance information obtained by measuring the executing program most directly relates to the characteristics of the explicitly parallel code, and is extremely difficult to interpret in relation to the source code written by the programmer. For example, for the code fragment above, current performance instrumentation and analysis tools <ref> [15, 14, 9, 17] </ref> for distributed memory parallel systems would capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [16] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Finally, it introduces explicit buffering for messages where necessary, and replaces the corresponding array references with buffer references to avoid unbuffering costs when possible. 2.2 Pablo Performance Environment The Pablo environment consists of three primary components: an extensible data capture library <ref> [16] </ref>, a data meta-format for describing the structure of performance data records without constraining their contents [3], and a graphical data analysis toolkit that allows users to quickly construct new performance data analyses [14, 19]. <p> A common interface provides uniform access to this information regardless of where individual symbolic records are stored. Dynamic records and symbolic records that must be obtained at runtime are recorded by inserting calls to the Pablo data capture library <ref> [16] </ref> . The library has been augmented, via its extension interfaces, to support integration with the Fortran D compiler.
Reference: [17] <author> Ries, B., Anderson, R., Auld, W., Breazeal, D., Callaghan, K., Richards, E., and Smith, W. </author> <title> The Paragon Performance Monitoring Environment. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. 1993), Association for Computing Machinery, </booktitle> <pages> pp. 850-859. </pages>
Reference-contexts: Second, performance information obtained by measuring the executing program most directly relates to the characteristics of the explicitly parallel code, and is extremely difficult to interpret in relation to the source code written by the programmer. For example, for the code fragment above, current performance instrumentation and analysis tools <ref> [15, 14, 9, 17] </ref> for distributed memory parallel systems would capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [18] <author> Rogers, A., and Pingali, K. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation (Portland, </booktitle> <address> OR, </address> <month> June </month> <year> 1989). </year>
Reference-contexts: The compiler first partitions the computation using the owner-computes rule <ref> [18] </ref>, and then implements the necessary communication and synchronization by inserting calls to a message-passing library for the target machine. 1 In order to generate and optimize the explicitly parallel program, the compiler transforms the code extensively. <p> Many researchers have developed simple as well as sophisticated models to compute the optimal block-size from parameters describing the computation and communication 13 cost as functions of block size in each pipeline stage <ref> [4, 5, 12, 13, 18, 10] </ref>.
Reference: [19] <author> Shields, K. A., Tavera, L. F., Scullin, W. H., Elford, C. L., and Reed, D. A. </author> <title> Virtual Reality for Parallel Computer Systems Performance Analysis. </title> <booktitle> In SIGGRAPH '94 Visual Proceedings (July 1994), Association for Computing Machinery. </booktitle>
Reference-contexts: 2.2 Pablo Performance Environment The Pablo environment consists of three primary components: an extensible data capture library [16], a data meta-format for describing the structure of performance data records without constraining their contents [3], and a graphical data analysis toolkit that allows users to quickly construct new performance data analyses <ref> [14, 19] </ref>. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing calls. A group of library extension interfaces allow instrumentation software developers to incrementally add new functionality to the library.
Reference: [20] <author> TMC. </author> <title> Prism User's Guide, </title> <institution> V1.2. Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <month> Mar. </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: A few performance tools for high-level languages include Prism <ref> [20] </ref> and NV [7] for CM-Fortran, the MPP-Apprentice performance tool for C, Fortran and Fortran 90 on the Cray T3D [11], and Forge90 [2] for Fortran 90 and HPF. These tools share the goal of presenting source-level performance information. Prism provides statement-level performance annotations of CM-Fortran.
References-found: 20

