URL: http://www.cs.uoregon.edu/~jens/tr.ps
Refering-URL: http://www.cs.uoregon.edu/~jens/research.html
Root-URL: http://www.cs.uoregon.edu
Email: fjens, log@cs.uoregon.edu  
Title: Dispersal Metrics for Non-Contiguous Processor Allocation  
Author: Jens Mache and Virginia Lo 
Keyword: massively parallel processing (MPP), resource management, processor allocation contention, dispersal, performance evaluation, performance metrics  
Address: OR 97403  
Affiliation: Department of Computer and Information Science University of Oregon, Eugene,  
Abstract: Resource management is a key area in the drive to fully realize the performance potential of parallel and distributed systems. For the task of assigning a set of processors of a massively parallel processing (MPP) system to a given job, various processor allocation strategies have been proposed in the research community and are in use at supercomputing sites. With the advent of the class of non-contiguous allocation strategies, the allocation performance bottleneck shifted from fragmentation to message-passing contention. This paper presents a method to estimate and minimize contention incurred by noncontiguous allocation strategies. Our approach is to analyze the spatial layout of dispersed nodes. Our contribution is a set of dispersal metrics that predict contention well and that are efficient to implement for a variety of interconnection topologies. We put the dispersal metrics to the test by comparing their contention estimates with measurements taken from a message-passing simulator. Our analysis and experiments consider different topologies of machines, a wide range of communication patterns and different workloads. Because our results show very high correlations between dispersal metrics and contention, we conclude that dispersal metrics have the potential to help evaluate and improve processor allocation strategies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Al-Dhelaan and B. Bose. </author> <title> A new strategy for processor allocation in an nCUBE multiprocessor. </title> <booktitle> In Proceedings of the International Phoenix Conference on Computers and Communication, </booktitle> <pages> pages 114-118, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Examples of contiguous allocation strategies for mesh topologies are 2D Buddy [9], Frame Sliding [5] and First Fit & Best Fit [18]. For hypercube topologies, examples are Gray Code [4], Partners <ref> [1] </ref> and Cyclic Buddy [11]. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. Six nodes are unallocated at this time, but there is no contiguous block of four nodes available. <p> For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner <ref> [1] </ref>. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies). Concerning architecture, we have two uni-directional links between adjacent nodes and either a 16 x 32 mesh topology or a 8-ary 3-cube topology.
Reference: [2] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS parallel benchmark results 3-94. </title> <type> Technical Report RNR-94-006, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA 94035-1000, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Jobsizes have an exponential distribution with mean of 16. For the purpose of non-empty waiting queues, we choose short interarrival times (Poisson distribution). Five communication patterns are modeled: all-to-all broadcast, one-to-all broadcast, fast fourier transform (FFT) as well as multigrid benchmark and kernel CG benchmark from NAS parallel benchmarks <ref> [2] </ref>. These cover many typical communication patterns and the complexity ranges from O (j) to O (j 2 ), j being the jobsize.
Reference: [3] <author> B. Bose, B. Broeg, Y. Kwon, and Y. Ashir. </author> <title> Lee distance and topological properties of k-ary n-cubes. </title> <type> Technical Report 93-60-11, </type> <institution> Oregon State University, </institution> <year> 1993. </year>
Reference-contexts: For our distance and diameter metrics, we computed the distance in each dimension by subtracting the coordinates. For k-ary n-cube topologies we have to take both directions into account. This is accomplished by using the Lee distance <ref> [3] </ref>.
Reference: [4] <author> M. Chen and K. G. Shin. </author> <title> Processor allocation in an nCUBE multiprocessor using Gray codes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1396-1407, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Examples of contiguous allocation strategies for mesh topologies are 2D Buddy [9], Frame Sliding [5] and First Fit & Best Fit [18]. For hypercube topologies, examples are Gray Code <ref> [4] </ref>, Partners [1] and Cyclic Buddy [11]. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. Six nodes are unallocated at this time, but there is no contiguous block of four nodes available. <p> For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code <ref> [4] </ref> and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies). Concerning architecture, we have two uni-directional links between adjacent nodes and either a 16 x 32 mesh topology or a 8-ary 3-cube topology.
Reference: [5] <author> P. Chuang and N. Tzeng. </author> <title> An efficient submesh allocation strategy for mesh computer systems. </title> <booktitle> In Proceedings of the International Conference on Distributed Computer Systems, </booktitle> <pages> pages 256-263, </pages> <year> 1991. </year>
Reference-contexts: This has the following advantages: Each job (or application) can think of owning the machine exclusively, and several runs of the same job result in approximately equal execution times. Examples of contiguous allocation strategies for mesh topologies are 2D Buddy [9], Frame Sliding <ref> [5] </ref> and First Fit & Best Fit [18]. For hypercube topologies, examples are Gray Code [4], Partners [1] and Cyclic Buddy [11]. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. <p> For mesh topologies we use the non-contiguous strategies Random [10], MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit [18], Best Fit [18] and Frame Sliding <ref> [5] </ref>. For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1].
Reference: [6] <author> D. G. Feitelson. </author> <title> A survey of scheduling in multiprogrammed parallel systems. </title> <type> Technical Report RC 19790 (87657), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The goal is to fully utilize the system and to maximize the throughput over a workload/ stream of jobs. In addition to space sharing, some systems use time sharing of nodes and a gang-scheduling scheme <ref> [6] </ref>. However, this paper will focus on the issues related to space sharing, namely processor allocation. Typical assumptions are that each job gets as many nodes as it requested and that jobs are not reallocated once they started running.
Reference: [7] <author> H.-U. Heiss. </author> <title> Processor management in two-dimensional grid-architectures. </title> <type> Technical Report 20/92, </type> <institution> Universitat Karlsruhe, </institution> <year> 1992. </year>
Reference-contexts: This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed <ref> [7, 10, 15, 14] </ref>. They are also known as scattered or fragmentation-free allocation strategies. Nodes allocated to the same job are allowed to be dispersed throughout the interconnection network. Fig. 3 shows examples.
Reference: [8] <author> P. Krueger, T. Lai, and V. A. Dixit-Radiya. </author> <title> Job scheduling is more important than processor allocation for hypercube computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(5) </volume> <pages> 488-497, </pages> <month> May </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The class of contiguous allocation strategies restricts the nodes allocated to a given job to form a "convex" shape. Fig. 1 shows an example. Performance suffers significantly due to processors being wasted because of internal and external fragmentation. Utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref>. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% [10, 15, 14]. <p> Both types of fragmentation result in unallocated nodes and thus low utilization of the 3 machine. Using First-Come, First-Served scheduling, utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref> due to serious fragmentation problems. This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed [7, 10, 15, 14].
Reference: [9] <author> K. Li and K.-H. Cheng. </author> <title> A two-dimensional buddy system for dynamic resource allocation in a partitionable mesh connected system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 79-83, </pages> <year> 1991. </year>
Reference-contexts: The class of contiguous allocation strategies restricts the nodes allocated to a given job to form a "convex" shape. Fig. 1 shows an example. Performance suffers significantly due to processors being wasted because of internal and external fragmentation. Utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref>. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% [10, 15, 14]. <p> This has the following advantages: Each job (or application) can think of owning the machine exclusively, and several runs of the same job result in approximately equal execution times. Examples of contiguous allocation strategies for mesh topologies are 2D Buddy <ref> [9] </ref>, Frame Sliding [5] and First Fit & Best Fit [18]. For hypercube topologies, examples are Gray Code [4], Partners [1] and Cyclic Buddy [11]. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. <p> Both types of fragmentation result in unallocated nodes and thus low utilization of the 3 machine. Using First-Come, First-Served scheduling, utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref> due to serious fragmentation problems. This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed [7, 10, 15, 14]. <p> For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy <ref> [9] </ref>, Gray Code [4] and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies).
Reference: [10] <author> W. Liu, V. Lo, K. Windisch, and B. Nitzberg. </author> <title> Non-contiguous processor allocation algorithms for distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 227-236, </pages> <year> 1994. </year> <title> Best student paper award. </title>
Reference-contexts: The class of contiguous allocation strategies restricts the nodes allocated to a given job to form a "convex" shape. Fig. 1 shows an example. Performance suffers significantly due to processors being wasted because of internal and external fragmentation. Utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref>. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% [10, 15, 14]. <p> Utilizations of only 34% to 66% are reported [9, 18, 8, 10]. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% <ref> [10, 15, 14] </ref>. To further improve the performance of noncontiguous strategies, it is necessary to select allocations that cause minimal message-passing contention. External contention is due to job interference: communication between dispersed nodes of a given job may require a communication link that is currently used by another job's messages. <p> Both types of fragmentation result in unallocated nodes and thus low utilization of the 3 machine. Using First-Come, First-Served scheduling, utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref> due to serious fragmentation problems. This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed [7, 10, 15, 14]. <p> This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed <ref> [7, 10, 15, 14] </ref>. They are also known as scattered or fragmentation-free allocation strategies. Nodes allocated to the same job are allowed to be dispersed throughout the interconnection network. Fig. 3 shows examples. <p> This in itself does not cause a problem, because using wormhole switching the network latency is almost independent of the path length. However, as discussed below, bigger physical distance increases the potential for message-passing contention. Examples of non-contiguous strategies for mesh topologies are defined in <ref> [10] </ref> and are shown in Fig. 3: "Random" selects unallocated nodes randomly, "Paging" scans the topology in row-major order for unallocated nodes, and "Multiple Buddy Strategy (MBS)" allocates a job to (possibly several) contiguous blocks. [15] describes similar algorithms for k-ary n-cube topologies. <p> Although contention has shown to be negligible in case of small messages <ref> [10, 12] </ref> or high software latency [12], it in general is the deciding performance factor within the class of non-contiguous processor allocation strategies [10, 12]. 5 3 Motivation for dispersal metrics As seen in Section 2, non-contiguous processor allocation strategies assign nodes that are possibly dispersed, thus achieving the best performance <p> Although contention has shown to be negligible in case of small messages <ref> [10, 12] </ref> or high software latency [12], it in general is the deciding performance factor within the class of non-contiguous processor allocation strategies [10, 12]. 5 3 Motivation for dispersal metrics As seen in Section 2, non-contiguous processor allocation strategies assign nodes that are possibly dispersed, thus achieving the best performance so far. For further improvement, it is important to study message-passing contention, the new performance bottleneck. <p> For further improvement, it is important to study message-passing contention, the new performance bottleneck. To our knowledge, previous research on contention in non-contiguous allocation strategies focused on the following: <ref> [10] </ref> studied whether the advantages of non-contiguity overcome the disadvantage of contention, [12] studied under what conditions contention matters and [14] implemented a non-contiguous strategy taking contention into account. In this paper, we present a method to estimate and minimize contention under non-contiguous allocation strategies. <p> Our approach is to analyze the spatial layout of dispersed nodes. This is motivated by the following: First, the differences in dispersal are visually striking. Fig. 3 shows snapshots of different non-contiguous allocation strategies <ref> [10] </ref> servicing a given jobstream. All snapshots show the same jobs 11, 12 and 15. For the MBS strategy, the nodes allocated to jobs 11 and 12 are contiguous and the nodes allocated to job 15 form two convex rectangles. <p> Contention is measured by recording the amount of time the header flit of each message is blocked in the network waiting for a channel to become free. To get a variety of spatial layouts we employ different allocation strategies. For mesh topologies we use the non-contiguous strategies Random <ref> [10] </ref>, MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit [18], Best Fit [18] and Frame Sliding [5]. <p> To get a variety of spatial layouts we employ different allocation strategies. For mesh topologies we use the non-contiguous strategies Random <ref> [10] </ref>, MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit [18], Best Fit [18] and Frame Sliding [5]. <p> To get a variety of spatial layouts we employ different allocation strategies. For mesh topologies we use the non-contiguous strategies Random <ref> [10] </ref>, MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit [18], Best Fit [18] and Frame Sliding [5]. <p> For correlation computations, we use the Pearson correlation coefficient. Results reported represent the statistical mean after 20 simulation runs with identical parameters, and given 95% confidence level, mean results have less than 3% error. Additional details about the simulator are described in <ref> [10] </ref>. Table 1 shows the correlation results for a 16 x 32 mesh topology, MBS allocation strategy and two different communication patterns. Fig. 9 shows the scatter plot for all-to-all communication and the dispersal metric nodes affected.
Reference: [11] <author> M. Livingston and Q. F. Stout. </author> <title> Parallel allocation algorithms for hypercubes and meshes. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 59-66, </pages> <year> 1989. </year>
Reference-contexts: Examples of contiguous allocation strategies for mesh topologies are 2D Buddy [9], Frame Sliding [5] and First Fit & Best Fit [18]. For hypercube topologies, examples are Gray Code [4], Partners [1] and Cyclic Buddy <ref> [11] </ref>. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. Six nodes are unallocated at this time, but there is no contiguous block of four nodes available.
Reference: [12] <author> S. Q. Moore and L. M. Ni. </author> <title> The effects of network contention on processor allocation strategies. </title> <type> Technical Report MSU-CPS-ACS-106, </type> <institution> Michigan State University, </institution> <year> 1995. </year>
Reference-contexts: Examples of existing machines with mesh or k-ary n-cube topologies are the Intel TFLOP, Intel Paragon, iPSC/860, Cray T3E and Cray T3D. We do not consider indirect networks (also known as multistage interconnection networks (MIN) or switch-based networks). Regarding contention, the experiments in <ref> [12] </ref> showed similar behavior for direct and indirect networks. Section 2 covers processor allocation strategies and performance issues. In Section 3 through 5, we motivate and develop several dispersal metrics. Our experiments are described in Section 6. <p> Although contention has shown to be negligible in case of small messages <ref> [10, 12] </ref> or high software latency [12], it in general is the deciding performance factor within the class of non-contiguous processor allocation strategies [10, 12]. 5 3 Motivation for dispersal metrics As seen in Section 2, non-contiguous processor allocation strategies assign nodes that are possibly dispersed, thus achieving the best performance <p> Although contention has shown to be negligible in case of small messages [10, 12] or high software latency <ref> [12] </ref>, it in general is the deciding performance factor within the class of non-contiguous processor allocation strategies [10, 12]. 5 3 Motivation for dispersal metrics As seen in Section 2, non-contiguous processor allocation strategies assign nodes that are possibly dispersed, thus achieving the best performance so far. <p> Although contention has shown to be negligible in case of small messages <ref> [10, 12] </ref> or high software latency [12], it in general is the deciding performance factor within the class of non-contiguous processor allocation strategies [10, 12]. 5 3 Motivation for dispersal metrics As seen in Section 2, non-contiguous processor allocation strategies assign nodes that are possibly dispersed, thus achieving the best performance so far. For further improvement, it is important to study message-passing contention, the new performance bottleneck. <p> For further improvement, it is important to study message-passing contention, the new performance bottleneck. To our knowledge, previous research on contention in non-contiguous allocation strategies focused on the following: [10] studied whether the advantages of non-contiguity overcome the disadvantage of contention, <ref> [12] </ref> studied under what conditions contention matters and [14] implemented a non-contiguous strategy taking contention into account. In this paper, we present a method to estimate and minimize contention under non-contiguous allocation strategies.
Reference: [13] <author> L. M. Ni and P. K. McKinley. </author> <title> A survey of wormhole routing techniques in direct networks. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 62-76, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Our results show that there is a very high correlation between dispersal metrics and contention. We conclude that dispersal metrics can be used to predict contention and thus have the potential to help evaluate and improve processor allocation strategies. We focus on dimension-ordered routing and wormhole switching <ref> [13] </ref> (both are adopted in many existing machines) and on massively parallel processing (MPP) systems with direct interconnection networks having mesh or k-ary n-cube topologies. Intuitively, a k-ary n-cube is an n-dimensional mesh with wrap-around edges in each dimension. Hypercubes, tori and rings are special cases of k-ary n-cubes.
Reference: [14] <author> M. Wan, R. Moore, G. Kremenek, and K. Steube. </author> <title> A batch scheduler for the Intel Paragon MPP system with a non-contiguous node allocation algorithm. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: Utilizations of only 34% to 66% are reported [9, 18, 8, 10]. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% <ref> [10, 15, 14] </ref>. To further improve the performance of noncontiguous strategies, it is necessary to select allocations that cause minimal message-passing contention. External contention is due to job interference: communication between dispersed nodes of a given job may require a communication link that is currently used by another job's messages. <p> This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed <ref> [7, 10, 15, 14] </ref>. They are also known as scattered or fragmentation-free allocation strategies. Nodes allocated to the same job are allowed to be dispersed throughout the interconnection network. Fig. 3 shows examples. <p> For further improvement, it is important to study message-passing contention, the new performance bottleneck. To our knowledge, previous research on contention in non-contiguous allocation strategies focused on the following: [10] studied whether the advantages of non-contiguity overcome the disadvantage of contention, [12] studied under what conditions contention matters and <ref> [14] </ref> implemented a non-contiguous strategy taking contention into account. In this paper, we present a method to estimate and minimize contention under non-contiguous allocation strategies.
Reference: [15] <author> K. Windisch, V. Lo, and B. Bose. </author> <title> Contiguous and non-contiguous processor allocation algorithms for k-ary n-cubes. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Utilizations of only 34% to 66% are reported [9, 18, 8, 10]. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% <ref> [10, 15, 14] </ref>. To further improve the performance of noncontiguous strategies, it is necessary to select allocations that cause minimal message-passing contention. External contention is due to job interference: communication between dispersed nodes of a given job may require a communication link that is currently used by another job's messages. <p> This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed <ref> [7, 10, 15, 14] </ref>. They are also known as scattered or fragmentation-free allocation strategies. Nodes allocated to the same job are allowed to be dispersed throughout the interconnection network. Fig. 3 shows examples. <p> Examples of non-contiguous strategies for mesh topologies are defined in [10] and are shown in Fig. 3: "Random" selects unallocated nodes randomly, "Paging" scans the topology in row-major order for unallocated nodes, and "Multiple Buddy Strategy (MBS)" allocates a job to (possibly several) contiguous blocks. <ref> [15] </ref> describes similar algorithms for k-ary n-cube topologies. Message-passing contention occurs when several messages want to use the same communication link at the same time. <p> For mesh topologies we use the non-contiguous strategies Random [10], MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit [18], Best Fit [18] and Frame Sliding [5]. For k-ary n-cube topologies we use the non-contiguous strategies Random <ref> [15] </ref>, MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies). <p> For k-ary n-cube topologies we use the non-contiguous strategies Random <ref> [15] </ref>, MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies). <p> For k-ary n-cube topologies we use the non-contiguous strategies Random <ref> [15] </ref>, MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies). <p> For k-ary n-cube topologies we use the non-contiguous strategies Random <ref> [15] </ref>, MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1]. Our simulation model uses wormhole switching and minimal dimension-ordered routing (XY routing for mesh and Lee routing for k-ary n-cube topologies).
Reference: [16] <author> K. Windisch, V. Lo, D. Feitelson, B. Nitzberg, and R. Moore. </author> <title> A comparison of workload traces from two production parallel machines. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Parallel jobs submitted to a massively parallel processing (MPP) system with n processors often need less than n processors <ref> [16] </ref>. This is exploited by modern systems that employ space sharing and run multiple jobs on different processors at the same time. Fig. 1 shows a snapshot of a 25-node machine and how the four jobs that are currently running share the processor space. <p> Both contention and the spatial layout of allocated nodes depends on workload characteristics. We set the workload parameters in our simulation according to workload parameters observed in traces of production machines (e.g. an Intel Paragon at the San Diego Supercomputing Center <ref> [16] </ref>). Our jobstreams consist of 1000 jobs. Jobsizes have an exponential distribution with mean of 16. For the purpose of non-empty waiting queues, we choose short interarrival times (Poisson distribution).
Reference: [17] <author> K. Windisch, J. V. Miller, and V. Lo. ProcSimity: </author> <title> an experimental tool for processor allocation and scheduling in highly parallel systems. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 414-421, </pages> <year> 1995. </year>
Reference-contexts: We compare contention estimates based on dispersal metrics to contention measurements produced by a message-passing simulator. Our experiments consider both mesh and k-ary n-cube topologies of interconnection networks, a wide range of communication patterns and different workloads. 6.1 Simulation environment To obtain contention measurements, we use ProcSimity <ref> [17] </ref>, a discrete-event simulator. It models the arrival, service and departure of a stream of jobs in a multicomputer. It supports selected allocation and scheduling strategies on architectures with a range of network topologies and several routing and flow control mechanisms.
Reference: [18] <author> Y. Zhu. </author> <title> Efficient processor allocation strategies for mesh-connected parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 328-337, </pages> <year> 1992. </year> <month> 19 </month>
Reference-contexts: The class of contiguous allocation strategies restricts the nodes allocated to a given job to form a "convex" shape. Fig. 1 shows an example. Performance suffers significantly due to processors being wasted because of internal and external fragmentation. Utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref>. In contrast, the class of noncontiguous allocation strategies allocates nodes that are dispersed throughout the system. Fig. 3 shows examples. They experience no fragmentation and thus outperform contiguous strategies reaching utilizations of up to 78% [10, 15, 14]. <p> Examples of contiguous allocation strategies for mesh topologies are 2D Buddy [9], Frame Sliding [5] and First Fit & Best Fit <ref> [18] </ref>. For hypercube topologies, examples are Gray Code [4], Partners [1] and Cyclic Buddy [11]. However, contiguous processor allocation strategies suffer from fragmentation. Consider an additional job in Fig. 1 that requests four nodes. <p> Both types of fragmentation result in unallocated nodes and thus low utilization of the 3 machine. Using First-Come, First-Served scheduling, utilizations of only 34% to 66% are reported <ref> [9, 18, 8, 10] </ref> due to serious fragmentation problems. This is unfavorable and contradicts the goal of high throughput over a stream of jobs. 2.2 Non-contiguous allocation strategies and contention To utilize unallocated nodes that are not necessarily contiguous, non-contiguous processor allocation strategies have been proposed [7, 10, 15, 14]. <p> To get a variety of spatial layouts we employ different allocation strategies. For mesh topologies we use the non-contiguous strategies Random [10], MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit <ref> [18] </ref>, Best Fit [18] and Frame Sliding [5]. For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1]. <p> To get a variety of spatial layouts we employ different allocation strategies. For mesh topologies we use the non-contiguous strategies Random [10], MBS [10] and Paging [10] (with different page sizes and different indexing schemes) as well as the contiguous strategies First Fit <ref> [18] </ref>, Best Fit [18] and Frame Sliding [5]. For k-ary n-cube topologies we use the non-contiguous strategies Random [15], MBS [15], Paging [15] and Multipartner [15] as well as the contiguous strategies Buddy [9], Gray Code [4] and Partner [1].
References-found: 18

