URL: http://www.iscs.nus.sg/~liuh/ijcai95.ps
Refering-URL: 
Root-URL: 
Email: frudys,liuhg@iscs.nus.sg  
Title: Understanding Neural Networks via Rule Extraction  
Author: Rudy Setiono and Huan Liu 
Address: Ridge, Singapore 0511  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Although backpropagation neural networks generally predict better than decision trees do for pattern classification problems, they are often regarded as black boxes, i.e., their predictions are not as interpretable as those of decision trees. This paper argues that this is because there has been no proper technique that enables us to do so. With an algorithm that can extract rules 1 , by drawing parallels with those of decision trees, we show that the predictions of a network can be explained via rules extracted from it, thereby, the network can be understood. Experiments demonstrate that rules extracted from neural networks are comparable with those of decision trees in terms of predictive accuracy, number of rules and average number of conditions for a rule; they preserve high predictive accuracy of original networks.
Abstract-found: 1
Intro-found: 1
Reference: [ Cheng et al., 1988 ] <author> J. Cheng, </author> <title> U.M. Fayyad, K.B. Irani, and Z Qian. Improved decision trees: A generalized version of id3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 100-106. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference-contexts: If necessary, the intermediate process can also be explicitly explained. C4.5 and C4.5rules [ Quinlan, 1993 ] were run on the above three datasets to generate DT rules. Briefly, C4.5 generates a decision tree which C4.5rules generalizes to rules. Since researchers <ref> [ Cheng et al., 1988; Shavlik et al., 1991 ] </ref> observed that mapping many-valued variables to two-valued variables results in decision trees with higher classification accuracy 4 , the same binary coded data for neural networks were used for C4.5 and C4.5rules. Being explicable is only one aspect of understandability.
Reference: [ Dietterich et al., 1990 ] <author> T.G. Dietterich, H. Hild, and G. Bakiri. </author> <title> A comparative study of id3 and backpropagation for english text-to-speech mapping. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference. </booktitle> <institution> University of Texas, Austin, Texas, </institution> <year> 1990. </year>
Reference: [ Fisher, 1936 ] <author> R.A. Fisher. </author> <title> The use of multiple measurements in taxonomic problems. </title> <journal> Ann. Eugenics, </journal> <volume> 7(2) </volume> <pages> 179-188, </pages> <year> 1936. </year>
Reference: [ Fu, 1994 ] <author> L. Fu. </author> <booktitle> Neural Networks in Computer Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1994. </year>
Reference-contexts: With 1 input for bias, there are total 39 inputs and three outputs. Examples in odd positions in the original dataset form the training set and the rest are for testing as was done in <ref> [ Fu, 1994 ] </ref> . * Breast Cancer the dataset consists of 699 examples, of which 458 examples are classified as benign, and 241 are malignant. 50% examples of each class were randomly selected (i.e., 229 benign and 121 malignant examples) for training, the rest for testing in the experiments. <p> NN rules cover all the possible combinations of the connections with various input values and discrete activation values of hidden units. This is a significant improvement over search-based methods <ref> [ Tow-ell and Shavlik, 1993; Fu, 1994 ] </ref> where all possible input combinations are searched for subsets that will exceed the bias on a unit. To reduce the cost of searching, they normally limit the number of antecedents in extracted rules.
Reference: [ Kerber, 1992 ] <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 123-128. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference: [ Liu, 1995 ] <author> H. Liu. </author> <title> Generating perfect rules. </title> <type> Technical report, </type> <institution> Department of Info Sys and Comp Sci, National University of Singapore, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Otherwise, * should be reduced to a smaller value. After network pruning and activation value discretiza-tion, rules can be extracted by examining the possible combinations in the network outputs (explained in detail in Section 3.2). The actual rule extraction is done by an algorithm that generates 100% accurate rules <ref> [ Liu, 1995 ] </ref> . However, when there are still too many connections (e.g., more than 7) between a hidden unit and input units, the extracted rules may not be easy to understand. Another three layer feedforward subnetwork may be employed to simplify rule extraction for the hidden unit.
Reference: [ Noordewieer et al., 1991 ] <author> M.O. Noordewieer, G.G. Towell, and J.W. Shavlik. </author> <title> Training knowledge-based neural networks to recognize genes in dna sequences. </title> <booktitle> In Advances in neural information processing systems, </booktitle> <volume> volume 3, </volume> <pages> pages 530-536. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Understanding a neural network is achieved by being able to explain, based on the rules, how each prediction is made in parallel with understanding a decision tree by having rules generated from it <ref> [ Quinlan, 1993 ] </ref> . 3.1 Datasets and Representations Three datasets used are: 1. Iris a classic dataset introduced by R.A. Fisher [ 1936 ] ; 2. Breast Cancer a widely tested real-world dataset for the Wisconsin Breast Cancer diagnosis; and 3. <p> When a rule is fired, a prediction is given that the example under consideration belongs to class C k . By examining the fired rule, it can be explained how the prediction is attained. If necessary, the intermediate process can also be explicitly explained. C4.5 and C4.5rules <ref> [ Quinlan, 1993 ] </ref> were run on the above three datasets to generate DT rules. Briefly, C4.5 generates a decision tree which C4.5rules generalizes to rules.
Reference: [ Quinlan, 1994 ] <author> J.R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In S.J. Hanson, G.A. Drastall, and R.L. Rivest, editors, </editor> <booktitle> Computational Learning Therory and Natural Learning Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 445-456. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A general picture of these comparisons is that: (1) Backpropagation (an NN learning method) usually requires a great deal more computation; (2) the predictive accuracy of both approaches is roughly the same, with backpropagation often slightly more accurate <ref> [ Quinlan, 1994 ] </ref> ; and (3) symbolic learning (decision trees induction) can produce interpretable rules while networks of weights are harder to interpret [ Shavlik et al., 1991 ] . <p> This is also true for NN rules and DT rules extraction. Due to the existence of sequential and parallel data types, and decision trees and neural networks are best suited to one type only <ref> [ Quinlan, 1994 ] </ref> , the two approaches are expected to coexist. When time is really scarce, the decision tree approach should be taken.
Reference: [ Saito and Nakano, 1988 ] <author> K. Saito and R Nakano. </author> <title> Medical diagnostic expert system based on pdp model. </title> <booktitle> In Proceedings of IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 255-262. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference: [ Setiono, 1995 ] <author> R. Setiono. </author> <title> A penalty function approach for pruning feedforward neural networks. </title> <type> Technical Report, DISCS, </type> <institution> National University of Singapore, </institution> <year> 1995. </year>
Reference-contexts: Details of the comparison and the choice of the parameter values (* 1 , * 2 , and fi) are given in <ref> [ Setiono, 1995 ] </ref> . 2.2 Network Pruning A network pruning algorithm is briefly described below. Neural network pruning algorithm 1. Let 1 and 2 be positive scalars such that 1 + 2 &lt; 0:5. 2. Pick a fully connected network.
Reference: [ Shavlik et al., 1991 ] <author> J.W. Shavlik, R.J. Mooney, and G.G. Towell. </author> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: learning method) usually requires a great deal more computation; (2) the predictive accuracy of both approaches is roughly the same, with backpropagation often slightly more accurate [ Quinlan, 1994 ] ; and (3) symbolic learning (decision trees induction) can produce interpretable rules while networks of weights are harder to interpret <ref> [ Shavlik et al., 1991 ] </ref> . In effect, a neural network is widely regarded as a black box due to the fact that little is known about how its prediction is made. <p> If necessary, the intermediate process can also be explicitly explained. C4.5 and C4.5rules [ Quinlan, 1993 ] were run on the above three datasets to generate DT rules. Briefly, C4.5 generates a decision tree which C4.5rules generalizes to rules. Since researchers <ref> [ Cheng et al., 1988; Shavlik et al., 1991 ] </ref> observed that mapping many-valued variables to two-valued variables results in decision trees with higher classification accuracy 4 , the same binary coded data for neural networks were used for C4.5 and C4.5rules. Being explicable is only one aspect of understandability.
Reference: [ Smith, 1993 ] <author> M. Smith. </author> <title> Neural networks for Statistical Modeling. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1993. </year>
Reference-contexts: Since each attribute takes a continuous value, the ChiMerge algorithm proposed by Kerber [ 1992 ] was reimplemented to discretize attribute values. The thermometer code <ref> [ Smith, 1993 ] </ref> is used to binarize the discretized values; 16, 9, 7, and 6 inputs (discrete values) for A 1 ,A 2 , A 3 , and A 4 , respectively. With 1 input for bias, there are total 39 inputs and three outputs.
Reference: [ Towell and Shavlik, 1993 ] <author> G.G. Towell and J.W. Shav-lik. </author> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101, </pages> <year> 1993. </year>
Reference-contexts: When time is really scarce, the decision tree approach should be taken. Otherwise, it is worthwhile trying both because of backpropagation's other advantages (generalizing better on a smaller dataset, predicting better in general, etc. <ref> [ Towell and Shavlik, 1993 ] </ref> ). * Average performance of NN rules. Because of neural networks' nondeterministic nature, it is not uncommon that many runs of networks are needed with different initial weights. <p> Unlike M-of-N rules <ref> [ Towell and Shavlik, 1993 ] </ref> , NN rules here reflect precisely how the network works. NN rules given here are actually the merge of the two sets: 1. from the input layer to the hidden layer; and 2. from the hidden layer to the output layer.
References-found: 14

