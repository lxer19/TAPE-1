URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-37.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: e-mail: fbodhi, kaushik, schwang@cc.gatech.edu  
Title: Experimentation with Configurable, Lightweight Threads on a KSR Multiprocessor  
Author: Kaushik Ghosh Bodhisattwa Mukherjee Karsten Schwan 
Address: Atlanta, Georgia 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Date: June 23, 1993  6 June 1993  
Pubnum: GIT-CC-93/37  
Abstract: The implementation of operating system functions can significantly affect the performance of parallel programs. Our research concerns the customization of operating system functionality for different target hardware to improve the performance of application programs. In this paper, we describe our experience with a reconfigurable, multiprocessor Mach cthreads package on a 32-node KSR-1 supercomputer. Sample static 
Abstract-found: 1
Intro-found: 1
Reference: [ALL89] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternativesfor shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Lightweight threads packages have been built for a variety of parallel and sequential machines. Typical arguments for their use include complete user-level control over threads scheduling [BLL88] and possibilities regarding the customization of threads synchronization <ref> [ALL89] </ref> or communication constructs [BALL90]. Our research requires user-level lightweight threads for two reasons: (1) to permit the customization of threads package functions to specific target applications, and (2) to perform such customization on-line in conjunction with the application's execution.
Reference: [BALL90] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year> <booktitle> Also appeared in Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Lightweight threads packages have been built for a variety of parallel and sequential machines. Typical arguments for their use include complete user-level control over threads scheduling [BLL88] and possibilities regarding the customization of threads synchronization [ALL89] or communication constructs <ref> [BALL90] </ref>. Our research requires user-level lightweight threads for two reasons: (1) to permit the customization of threads package functions to specific target applications, and (2) to perform such customization on-line in conjunction with the application's execution.
Reference: [Bla90] <author> D. Black. </author> <title> Scheduling support for concurrency and parallelism in the mach operating systems. </title> <journal> IEEE Computer Magazine, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Such differences result in major changes in the implementation of thread context switching (saving or not saving signal masks, for instance); they occur for real-time vs. non-real-time schedulers. * Application hints: Schedulers must be able to offer attributes that may be specified by appli cation programs, such as handoff hints <ref> [Bla90] </ref> or real-time parameters like deadlines. 8 We next describe the class definition of a scheduler offering attributes for its static configu-ration as a preemptive (using signalling) vs. non-preemptive scheduler.
Reference: [BLL88] <author> B. Bershad, E. Lazowska, and H. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Lightweight threads packages have been built for a variety of parallel and sequential machines. Typical arguments for their use include complete user-level control over threads scheduling <ref> [BLL88] </ref> and possibilities regarding the customization of threads synchronization [ALL89] or communication constructs [BALL90]. Our research requires user-level lightweight threads for two reasons: (1) to permit the customization of threads package functions to specific target applications, and (2) to perform such customization on-line in conjunction with the application's execution.
Reference: [CC89] <author> Houssine Chetto and Maryline Chetto. </author> <title> Some results of the earliest deadline scheduling algorithm. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 1261-1269, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: A real-time application, e.g., may not want to use the generic function in the cthreads library, whereby a FCFS policy is used to run threads until they explicitly yield. Real-time systems often use variations of priority-driven scheduling (for example Earliest Deadline First or multiprocessor variants thereof <ref> [CC89, SZG91, GFS93a] </ref>) in the absence of sporadic task arrivals, and perform schedulability analyses whenever a sporadic arrives (and interrupts the processor). From the aforementioned text, it should be clear that such application-specific schedulers are easy to install in the cthreads library.
Reference: [CD88] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C threads. </title> <type> Technical report, </type> <institution> Computer Science, Carnegie-Mellon University, CMU-CS-88-154, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Essentially, each pthread simply runs any thread it finds in its local ready queue of user-level cthreads. The basic functions offered by the user-level threads functions are those of Mach CThreads <ref> [CD88] </ref>, including thread management, synchronization, and memory management. Thread initialization differs from Mach cthreads in that users can control the total amounts of shared memory allocated for threads and thread stacks, the number of virtual processors (pthreads) used for thread execution, etc.
Reference: [CMS93] <author> Christian Clemencon, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Distributed shared abstractions (dsa) on large-scale multiprocessors. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, GIT-CC-93-25, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Similarly, multiple thread sched-ulers may be employed concurrently for performing the scheduling of application threads vs. threads performing communication processing <ref> [CMS93, LKAS93] </ref>. * Performance improvements can be attained by adapting the scheduling components of a lock construct to the application's dynamically changing locking pattern (e.g., access frequencies, lengths, and contention on critical sections), ranging from improvements of 3-5% for programs not exhibiting significant lock contention to 17% for applications making frequent <p> The current implementation of the Time Warp kernel performed better with pthreads than with cthreads chiefly because locking is cheaper for cthreads. 4.2 Travelling Sales Person with Cthreads on KSR We have implemented the TSP algorithm as a collection of asynchronous cooperating threads <ref> [CMS93] </ref>. The threads cooperate through two shared abstractions: (1) a shared work queue of subproblems which stores the search-space tree, and (2) a shared value representing the current best tour found so far. <p> Especially on the KSR machine, global queue organizations are not suitable for scalable performance in thread scheduling and thread management, so that queues must be internally fragmented across different processors <ref> [CMS93] </ref>.
Reference: [DeG91] <author> Doug DeGroot. </author> <title> Throttling speculative computation: Issues and problems. </title> <booktitle> Parallel Computing 1991, </booktitle> <pages> pages 19-37, </pages> <year> 1991. </year>
Reference-contexts: While Time Warp optimistic simulation can be seen as increasing dynamic parallelism through speculation, one often needs to get `runaway parallelism' under control through throttling <ref> [DeG91] </ref>. Dynamic parallelism, by its very nature, cannot be predicted at compile time. The amount of speculation should be large when there is a high amount of dynamic parallelism, and speculation should be throttled when such parallelism reduces.
Reference: [Fuj89] <author> R. M. Fujimoto. </author> <title> Time Warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Cthreads on KSR Time Warp is a discrete-event simulation technique that uses optimistic (rather than conservative) parallelization techniques [Jef85]. Salient features of the Time Warp testbed that we use for our research have been treated extensively elsewhere <ref> [Fuj89] </ref>.
Reference: [Fuj90] <author> R. M. Fujimoto. </author> <title> Performance of Time Warp under synthetic workloads. </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 22(1) </volume> <pages> 23-28, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Further, the list of free events has to be locked when garbage-collected events are returned to the free list. In this paper, we report the performance of the kernel in response to a Parallel Hold workload model, the generality of which has been argued elsewhere <ref> [Fuj90] </ref>. We use a single cthread on each processor; all the logical processes (LPs) mapped on that processor are multiplexed by this cthread. The following graphs show the performance of the kernel using the pthreads library, and our cthreads library.
Reference: [GFS93a] <author> Kaushik Ghosh, Richard M. Fujimoto, and Karsten Schwan. </author> <title> A testbed for optimistic execution of real-time simulations. </title> <booktitle> IEEE Workshop on Parallel and Distributed Real-Time Systems, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Using artificial workloads [MS93a] and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor [Muk91], we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications <ref> [SZG91, GFS93a] </ref>. <p> A real-time application, e.g., may not want to use the generic function in the cthreads library, whereby a FCFS policy is used to run threads until they explicitly yield. Real-time systems often use variations of priority-driven scheduling (for example Earliest Deadline First or multiprocessor variants thereof <ref> [CC89, SZG91, GFS93a] </ref>) in the absence of sporadic task arrivals, and perform schedulability analyses whenever a sporadic arrives (and interrupts the processor). From the aforementioned text, it should be clear that such application-specific schedulers are easy to install in the cthreads library.
Reference: [GFS93b] <author> Kaushik Ghosh, Richard M. Fujimoto, and Karsten Schwan. </author> <title> Time warp simulation in time constrained systems. </title> <booktitle> Proceedings of the 7th Workshop on Parallel and Distributed Simulation (PADS), </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The behavior on the other processors is expected to be similar, the application being symmetric. 12 7 Runtime Configuration Scheduling for Real-time Time Warp We are now investigating the runtime configuration of a single thread scheduler termed scheduler configuration performing real-time scheduling for Time-Warp discrete event simulations <ref> [GFS93b] </ref>. While Time Warp optimistic simulation can be seen as increasing dynamic parallelism through speculation, one often needs to get `runaway parallelism' under control through throttling [DeG91]. Dynamic parallelism, by its very nature, cannot be predicted at compile time.
Reference: [Jef85] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Cthreads on KSR Time Warp is a discrete-event simulation technique that uses optimistic (rather than conservative) parallelization techniques <ref> [Jef85] </ref>. Salient features of the Time Warp testbed that we use for our research have been treated extensively elsewhere [Fuj89].
Reference: [LKAS93] <author> Bert Lindgren, Bobby Krupczak, Mostafa Ammar, and Karsten Schwan. </author> <title> Parallel and configurable protocols: Experiences with a prototype and an architectural framework. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> GIT-CC-93/22, Atlanta, GA, </address> <month> March </month> <year> 1993. </year> <note> To appear in 1993 International Conference on Network Protocols. </note>
Reference-contexts: Similarly, multiple thread sched-ulers may be employed concurrently for performing the scheduling of application threads vs. threads performing communication processing <ref> [CMS93, LKAS93] </ref>. * Performance improvements can be attained by adapting the scheduling components of a lock construct to the application's dynamically changing locking pattern (e.g., access frequencies, lengths, and contention on critical sections), ranging from improvements of 3-5% for programs not exhibiting significant lock contention to 17% for applications making frequent
Reference: [MS93a] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Experiments with a configurable lock for multiprocessors. </title> <booktitle> In To appear in the proceedings of International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year> <note> Also available as TR# GIT-CC-93/05. </note>
Reference-contexts: describe a design of a configurable threads scheduler able to improve program performance by changing selected scheduler attributes during program execution. 2 Previous Results We have already investigated threads-level configuration for a specific construct reconfigurable locks 2 used for the synchronization of multiple processes on the BBN Butterfly NUMA multiprocessor <ref> [MS93a, MS93b] </ref>. Using artificial workloads [MS93a] and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor [Muk91], we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications [SZG91, GFS93a]. <p> Using artificial workloads <ref> [MS93a] </ref> and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor [Muk91], we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications [SZG91, GFS93a].
Reference: [MS93b] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Improving performance by use of adaptive objects: Experimentation with a configurable multiprocessor thread package. </title> <booktitle> In To appear in the proceedings of high performance distributed computing, </booktitle> <month> July </month> <year> 1993. </year> <note> Also available as TR# GIT-CC-93/17. </note>
Reference-contexts: describe a design of a configurable threads scheduler able to improve program performance by changing selected scheduler attributes during program execution. 2 Previous Results We have already investigated threads-level configuration for a specific construct reconfigurable locks 2 used for the synchronization of multiple processes on the BBN Butterfly NUMA multiprocessor <ref> [MS93a, MS93b] </ref>. Using artificial workloads [MS93a] and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor [Muk91], we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications [SZG91, GFS93a].
Reference: [Muk91] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year> <note> TR# GIT-ICS-91/02. </note>
Reference-contexts: Using artificial workloads [MS93a] and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor <ref> [Muk91] </ref>, we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications [SZG91, GFS93a].
Reference: [OSS90] <author> David M. Ogle, Karsten Schwan, and Richard Snodgrass. </author> <title> The dynamic monitoring of real-time distributed and parallel systems. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> ICS-GIT-90/23, Atlanta, GA 30332, </address> <month> May </month> <year> 1990. </year> <note> To appear in IEEE TSE. </note>
Reference-contexts: Since runtime configuration requires state monitoring, the cthreads package provides a set of functions and facilities for on-line monitoring not described in detail here (see <ref> [OSS90] </ref> for a discussion of an earlier implementation of such primitives performed by our group). Such information (about threads package state) may be used at the application level by "adaptation policies" applied to multiple threads package components (configurable objects), or it may be used to build single adaptive components.
Reference: [SZG91] <author> Karsten Schwan, Hongyi Zhou, and Ahmed Gheith. </author> <title> Multiprocessor real-time threads. </title> <journal> Operating Systems Review, </journal> <volume> 25(4) </volume> <pages> 35-46, </pages> <month> Oct. </month> <year> 1991. </year> <note> Also appears in the Jan. 1992 issue of Operating Systems Review. 15 16 17 18 </note>
Reference-contexts: Using artificial workloads [MS93a] and a representative multiprocessor application a parallel branch-and-bound program on a 32 node BBN Butterfly multiprocessor [Muk91], we have demonstrated that: * Off-line configuration is important for use of alternative thread schedulers, such as for real-time vs. non-real-time applications <ref> [SZG91, GFS93a] </ref>. <p> granularity of schedul ing defines the size of such a unit, which may consist of a single or a group of threads. * Storage/retrieval policies: The scheduler's repository for schedulable (ready to run) threads will differ among scheduling policies, ranging from simple FCFS queues to priority- or even deadline-ordered queues <ref> [SZG91] </ref>. * Locality: Scheduler performance is also affected by the distribution of the above-mentioned thread repository. Especially on the KSR machine, global queue organizations are not suitable for scalable performance in thread scheduling and thread management, so that queues must be internally fragmented across different processors [CMS93]. <p> A real-time application, e.g., may not want to use the generic function in the cthreads library, whereby a FCFS policy is used to run threads until they explicitly yield. Real-time systems often use variations of priority-driven scheduling (for example Earliest Deadline First or multiprocessor variants thereof <ref> [CC89, SZG91, GFS93a] </ref>) in the absence of sporadic task arrivals, and perform schedulability analyses whenever a sporadic arrives (and interrupts the processor). From the aforementioned text, it should be clear that such application-specific schedulers are easy to install in the cthreads library.
References-found: 19

