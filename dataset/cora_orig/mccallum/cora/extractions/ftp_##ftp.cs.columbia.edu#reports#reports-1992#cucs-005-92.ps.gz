URL: ftp://ftp.cs.columbia.edu/reports/reports-1992/cucs-005-92.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1992.html
Root-URL: http://www.cs.columbia.edu
Title: Extraction and Use of Contextual Attributes for Theory Completion: An Integration of Explanation-Based and Similarity-Based Learning  
Author: Andrea Pohoreckyj Danyluk CUCS-- 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School of Arts and Sciences.  
Date: 1992  
Affiliation: Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [Burstein 86] <author> Burstein, M. H. </author> <title> Concept Formation by Incremental Analogical Reasoning and Debugging. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: Thus the matching techniques of SBL are often applied here. Furthermore, since similarities might be found at a different level from the purely superficial one of entities' observable characteristics, the entities are often analyzed using deductive, or explanation-based, mechanisms as well. Examples of work in analogy include <ref> [Burstein 86; Carbonell 83a; Carbonell 83b; Kedar-Cabelli 85; Russell 86] </ref>. Exemplar-based learning is another mechanism that combines inductive and deductive techniques. In exemplar-based learning a concept is represented by a stereotypical example.
Reference: [Carbonell 83a] <author> Carbonell, J. G. </author> <title> Derivational Analogy in Problem Solving and Knowledge Acquisition. </title> <booktitle> In Proceedings of the Second International Machine Learning Workshop, </booktitle> <pages> pages 12 - 18. </pages> <address> Champaign-Urbana, Illinois, </address> <year> 1983. </year> <month> 193 </month>
Reference-contexts: Thus the matching techniques of SBL are often applied here. Furthermore, since similarities might be found at a different level from the purely superficial one of entities' observable characteristics, the entities are often analyzed using deductive, or explanation-based, mechanisms as well. Examples of work in analogy include <ref> [Burstein 86; Carbonell 83a; Carbonell 83b; Kedar-Cabelli 85; Russell 86] </ref>. Exemplar-based learning is another mechanism that combines inductive and deductive techniques. In exemplar-based learning a concept is represented by a stereotypical example.
Reference: [Carbonell 83b] <author> Carbonell, J. C. </author> <title> Learning by Analogy: Formulating and Generalizing Plans from Past Experience. Machine Learning: An Artificial Intelligence Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1983. </year>
Reference-contexts: Thus the matching techniques of SBL are often applied here. Furthermore, since similarities might be found at a different level from the purely superficial one of entities' observable characteristics, the entities are often analyzed using deductive, or explanation-based, mechanisms as well. Examples of work in analogy include <ref> [Burstein 86; Carbonell 83a; Carbonell 83b; Kedar-Cabelli 85; Russell 86] </ref>. Exemplar-based learning is another mechanism that combines inductive and deductive techniques. In exemplar-based learning a concept is represented by a stereotypical example.
Reference: [Clancey 84] <author> Clancey, W. J. NEOMYCIN: </author> <title> reconfiguring a rule-based system with application to teaching. </title> <booktitle> Readings in Medical Artificial Intelligence. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1984. </year>
Reference-contexts: ODYSSEUS is designed to improve a knowledge base of the type used by the HERACLES expert system shell. HERACLES is essentially the NEOMYCIN expert system with the medical domain knowledge removed <ref> [Clancey 84] </ref>. Experiments run on ODYSSEUS were performed using the NEOMYCIN medical knowledge base for diagnosis of neurological problems. HERACLES uses heuristic classification to solve problems. This is a process in which a solution is chosen from a pre-specified set using heuristics.
Reference: [Cohen 91] <author> Cohen, W. W. </author> <title> The Generality of Overgenerality. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pages 490 - 494. </pages> <institution> Northwestern University, </institution> <year> 1991. </year>
Reference-contexts: Solutions using induction alone often concentrate on learning a new definition for the goal concept that includes the training example in its class membership. Thus the domain theory retains classification accuracy for goal concepts, but loses some of the underlying structure of subgoals (e.g., <ref> [Cohen 91; Ourston and Mooney 90] </ref>). Our work concentrates on learning subgoals, or intermediate concepts, that may be useful in a variety of contexts. This is especially important for knowledge bases that are used to generate explanations for a user.
Reference: [Danyluk 87] <author> Danyluk, A. P. </author> <title> The Use of Explanations for Similarity-Based Learning. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 274 - 276. </pages> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: The specific time of occurrence, while related to the fact that the target was crowded, would not be considered to be important. SBL would organize these events closely together in memory as their descriptions are similar. The use of explanations for similarity-based learning in this domain is described in <ref> [Danyluk 87] </ref>. 3.3. The Learning Model Gemini integrates EBL and SBL as follows: EBL analyzes an input example by explaining it and by providing SBL with additional information about the features describing the input.
Reference: [Davis 76] <author> Davis, R. </author> <title> Applications of Meta-level Knowledge to the Construction, Maintenance, and Use of Large Knowledge Bases. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, </institution> <note> 1976. </note> <author> [De Raedt and Bruynooghe 89] De Raedt, L. and Bruynooghe, M. </author> <title> Towards Friendly Concept-Learners. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 849 - 854. </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: In order to facilitate this process a number of tools were developed: (1) knowledge base editors and interfaces, (2) explanation 1 facilities, and (3) knowledge base revision facilities. Knowledge acquisition tools provided sophisticated mechanisms for the building and revision of expert system knowledge bases. TEIRESIAS <ref> [Davis 76] </ref>, for example, allowed the expert to specify a new rule, or rule revision, in natural language format. It interacted with the user to check for consistency between the concepts expressed in the rule and those already represented in the knowledge base.
Reference: [DeJong 81] <author> DeJong, G. F. </author> <title> Generalizations Based on Explanations. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 67 - 69. </pages> <address> Vancouver, </address> <publisher> B. </publisher> <address> C., Canada, </address> <year> 1981. </year>
Reference: [DeJong 83] <author> DeJong, G. F. </author> <title> Acquiring Schemata through Understanding and Generalizing Plans. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence. </booktitle> <address> Karlsruhe, West Germany, </address> <note> 1983. </note> <author> [DeJong and Mooney 86] DeJong, G. and Mooney, R. </author> <title> Explanation-Based Learning: An Alternative View. </title> <booktitle> Machine Learning 1(2):145 - 176, </booktitle> <year> 1986. </year> <note> 194 [Dietterich and Flann 88] Dietterich, </note> <author> T. G. and Flann, N. S. </author> <title> An Inductive Approach to Solving the Imperfect Theory Problem. </title> <booktitle> In Proceedings of the AAAI Symposium on Explanation-Based Learning, </booktitle> <pages> pages 42 - 46. </pages> <institution> Stanford University, </institution> <year> 1988. </year> <note> Unpublished Proceedings. </note>
Reference: [Ellman 87] <author> Ellman, T. </author> <title> Explanation-Based Methods for Simplifying Intractable Theories. </title> <type> Technical Report CUCS-265-87, </type> <institution> Columbia University Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Viewing explanation generation as search, it is characterized by large values for D. The large search space problem is addressed by [Lebowitz 86a; Lebowitz 90] as well as by Ellman <ref> [Ellman 87; Ellman 88a; Ellman 88b; Ellman 89b] </ref>. Lebowitz uses SBL generalizations to focus on features that would appear specifically as causes or effects in an explanation. This constrains the rules that must be considered. Ellman introduces the notion of approximating the domain theory to create simpler explanations.
Reference: [Ellman 88a] <author> Ellman, T. </author> <title> Explanation-Directed Search for Simplifying Assumptions. </title> <booktitle> In Proceedings of the AAAI Symposium on Explanation-Based Learning, </booktitle> <pages> pages 95 - 99. </pages> <institution> Stanford University, </institution> <year> 1988. </year> <note> Unpublished Proceedings. </note>
Reference-contexts: Viewing explanation generation as search, it is characterized by large values for D. The large search space problem is addressed by [Lebowitz 86a; Lebowitz 90] as well as by Ellman <ref> [Ellman 87; Ellman 88a; Ellman 88b; Ellman 89b] </ref>. Lebowitz uses SBL generalizations to focus on features that would appear specifically as causes or effects in an explanation. This constrains the rules that must be considered. Ellman introduces the notion of approximating the domain theory to create simpler explanations.
Reference: [Ellman 88b] <author> Ellman, T. </author> <title> Approximate Theory Formation: An Explanation-Based Approach. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 570 - 574. </pages> <address> St. Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: Viewing explanation generation as search, it is characterized by large values for D. The large search space problem is addressed by [Lebowitz 86a; Lebowitz 90] as well as by Ellman <ref> [Ellman 87; Ellman 88a; Ellman 88b; Ellman 89b] </ref>. Lebowitz uses SBL generalizations to focus on features that would appear specifically as causes or effects in an explanation. This constrains the rules that must be considered. Ellman introduces the notion of approximating the domain theory to create simpler explanations.
Reference: [Ellman 89a] <author> Ellman, T. </author> <title> Explanation-Based Learning: A Survey of Programs and Perspectives. </title> <journal> Computing Surveys 21(2):163 - 221, </journal> <year> 1989. </year>
Reference-contexts: Cup The discussion above is intended to give an intuitive view of explanation-based learning. A more formal discussion is beyond the scope of this chapter. (But see [DeJong and Mooney 86; Mitchell et al. 86]; a survey of EBL systems is presented in <ref> [Ellman 89a] </ref>.) The key ideas of EBL are: Explanation-based learning is a method in which a single example is analyzed and then generalized by a system. <p> A single definitive view of EBL does not exist. (See Chapter 2.2.1. [Mitchell et al. 86] and [DeJong and Mooney 86] present two views of the method. <ref> [Ellman 89a] </ref> discusses many apparently diverse systems that may be characterized as performing EBL.) A number of key ideas, however, are shared in all views of the method, and are shown in Figure 35, which expands the hierarchy of Figure 34 for information relevant to EBL.
Reference: [Ellman 89b] <author> Ellman, T. </author> <title> Integrated Analytic and Empirical Learning of Approximations for Intractable Theories. </title> <type> PhD thesis, </type> <institution> Columbia University Department of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Viewing explanation generation as search, it is characterized by large values for D. The large search space problem is addressed by [Lebowitz 86a; Lebowitz 90] as well as by Ellman <ref> [Ellman 87; Ellman 88a; Ellman 88b; Ellman 89b] </ref>. Lebowitz uses SBL generalizations to focus on features that would appear specifically as causes or effects in an explanation. This constrains the rules that must be considered. Ellman introduces the notion of approximating the domain theory to create simpler explanations.
Reference: [Eshelman 88] <author> Eshelman, L. </author> <type> 1988 Personal Communication. </type>
Reference-contexts: with a particular destination, but the same user is able to reach other nodes on the network, then the problem lies with the destination of the unestablished connection.'' The rule base has been encoded from a prototype knowledge base that was extracted from experts maintaining the CMU campus computer network <ref> [Eshelman 88] </ref>. 6 Examples will be taken from the network fault diagnosis domain to illustrate points in the following section. Examples from our other three domains will be used in later chapters, where the domains will be described in greater detail. 1.3.2. <p> Nil would indicate that a slot value was unknown. The input frame in Figure 19 is shown with the actual slot names and values used by Gemini. Slots for this domain were extracted from a prototype expert system for network fault diagnosis developed at CMU <ref> [Eshelman 88] </ref>. They correspond to questions asked of the user directly, rather than inferences made by the system. Fault Diagnosis 57 A second example is taken from the domain of terrorist event news stories. Here, actual news stories are transcribed into hierarchical frames. <p> These numbers will be used later in the section when abbreviations of the attributes are necessary. 126 The rule base for this domain contains 56 rules. Rules were translated into Gemini's representation from a prototype knowledge base that was extracted from experts maintaining the CMU campus computer network <ref> [Eshelman 88] </ref>. We deleted the three rules in Figure 53 to perform test runs characterizing the behavior of learning 14 heuristics in this domain. The following sections describe the runs we performed for each of these three rules.
Reference: [Fawcett 89] <author> Fawcett, T. </author> <title> Learning from Plausible Explanations. </title> <booktitle> In Proceedings of the Sixth International Machine Learning Workshop, </booktitle> <pages> pages 37 - 39. </pages> <institution> Cornell University, </institution> <year> 1989. </year>
Reference-contexts: Any of these is potentially responsible for the failure of the explanation. One promising approach to the problem of abduction - or selection of a ``best'' (partial) explanation - is described by <ref> [Fawcett 89] </ref> and [Ng and Mooney 90], among others. It involves ranking all possible (partial) explanations according to heuristic criteria such as: the conciseness of the explanation (``shorter is better''); the number of features referenced in the explanation (``greater coverage is better''). <p> Except for the initial input to Gemini, outside guidance is virtually unnecessary. One exception is that the selection of good partial explanations is currently done by hand. This could be automated by employing a selection algorithm such as Fawcett's <ref> [Fawcett 89] </ref>. 85 Chapter 4 Contextual Heuristics for Rule Induction 4.1. Introduction A major focus of this research is the problem of incompleteness in explanation-based learning. Our solution introduces heuristics that guide induction of missing domain theory rules by extracting information from partial as well as previous complete explanations.
Reference: [Fisher 87] <author> Fisher, D. H. </author> <title> Conceptual Clustering, Learning From Examples, and Inference. </title> <booktitle> In Proceedings of the Fourth International Machine Learning Workshop, </booktitle> <pages> pages 38 - 49. </pages> <address> Irvine, California, </address> <year> 1987. </year> <note> [Flann and Dietterich 86] Flann, </note> <author> N. S. and Dietterich, T. G. </author> <title> Selecting Appropriate Representations for Learning from Examples. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 460 - 466. </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1986. </year> <month> 195 </month>
Reference-contexts: The system looks for subsets of the input examples such that the elements of each subset share a number of descriptive features not shared by the members of any other set. Work in this area includes that of <ref> [Fisher 87; Lebowitz 87; Michalski and Stepp 83] </ref>. As with the overview of explanation-based learning above, this chapter does not present a formal treatment of similarity-based learning.
Reference: [Forbus 84] <author> Forbus, K. </author> <title> Qualitative Process Theory. </title> <booktitle> Artificial Intelligence 24:85 - 168, </booktitle> <year> 1984. </year>
Reference-contexts: Michalski and Ko [Michalski and Ko 88] have also discussed the use of experimentation in addressing this problem. The discussion here focuses on Rajamoney's work. ADEPT's domain theory is represented using Forbus's Qualitative Process Theory <ref> [Forbus 84] </ref>. According to this theory, changes in the world such as boiling or evaporation are due to processes, which in reasoning may be used analogously to rules.
Reference: [Genest et al. 90] <author> Genest, J., Matwin, S., Plante, B. </author> <title> Explanation-Based Learning with Incomplete Theories: A Three-step Approach. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 286 - 294. </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: Hall also relies on a user to input an analogue to a failure situation from which the system can learn. His system is left to analyze the analogue as well as to generate a new rule from it. Although not discussed here, [Laird et al. 90] and <ref> [Genest et al. 90] </ref> similarly rely on user guidance to provide the information to be included in a missing rule. Our work is based on the observation that such detailed guidance is not always available to a system.
Reference: [Ginsberg 88] <author> Ginsberg, A. </author> <title> Theory Revision via Prior Operationalization. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 590 - 595. </pages> <address> St. Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: The first manifestation of incorrectness is addressed using SBL techniques by 46 [Araya 84], [Silver 88], and [Bergadano, et al. 87]. [Smith et al. 85] also address the problem, but with the aid of a detailed theory underlying the abstract one that is actually used. <ref> [Ginsberg 88] </ref> solves the problem by doing a massive initial check for redundancy and inconsistency. The second manifestation of incorrectness is addressed by [Dietterich and Flann 88]. 2.5.1.2.
Reference: [Hall 86] <author> Hall, R. J. </author> <title> Learning by Failing to Explain. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 568 - 572. </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1986. </year>
Reference-contexts: Though reasonable in many cases, this assumption may be too optimistic in others. Finally, ADEPT is limited to applications where experimental validation can potentially be performed. 2.3.2. Hall Hall describes a method for learning new rules called Learning by Failing to Explain <ref> [Hall 86] </ref>. In this method, a system that fails to find an explanation for an input example is given a new, analogous example by a teacher. The system learns by analyzing the analogue and comparing its explanation using similarity-based methods to the incomplete explanation. <p> Two major criticisms were raised in Chapter 2 with respect to previous solutions to the problem of theory incompleteness in EBL: (1) the requirement that auxiliary theories be complete (e.g., [Rajamoney 89; Pazzani 88; Wilkins 88]); (2) the need for active presence of an expert (e.g., <ref> [Hall 86; Kodratoff 87; Wilkins 88] </ref>). Similarity-based learning plays a role, but this role is limited to verification or generalization of knowledge proposed by another source. Our solution to the problem of incompleteness places greater emphasis on SBL as the primary mechanism for acquiring missing knowledge.
Reference: [Hammond 88] <author> Hammond, K. </author> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pages 17 - 20. </pages> <address> Clearwater Beach, Florida, </address> <year> 1988. </year>
Reference-contexts: In CBR cases of previously seen problems and their solutions are stored in memory. When a new problem arises an old case is retrieved. Mechanisms similar to those described above for analogy may be used to retrieve appropriate cases for modification. Work in CBR includes <ref> [Hammond 88; Kolodner 87] </ref>. 2.6. Summary This chapter has surveyed work in machine learning related to our research.
Reference: [Hirsh 89] <author> Hirsh, H. </author> <title> Incremental Version-Space Merging: A General Framework for Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, </institution> <year> 1989. </year> <note> [Kedar-Cabelli 85] Kedar-Cabelli, </note> <author> S. </author> <title> Purpose-Directed Analogy. </title> <booktitle> In Proceedings of the Cognitive Science Society Conference. </booktitle> <address> Irvine, California, </address> <year> 1985. </year> <note> [Kedar-Cabelli 88] Kedar-Cabelli, </note> <author> S. </author> <title> Formulating Concepts and Analogies According to Purpose. </title> <type> PhD thesis, </type> <institution> Rutgers University Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: This set of sufficient conditions provides a basis for refining the S set of the version space. Negative instances are still used to refine the G set. This has recently been implemented by Hirsh <ref> [Hirsh 89] </ref>. A number of researchers have more recently investigated the idea of using EBL to provide extra information to SBL in order that it might intelligently prune its search space. The following sections present the approaches of Salzberg, Swaminathan, and Van Lehn. 42 2.4.1.
Reference: [Keller 87] <author> Keller, R. M. </author> <title> The Role of Explicit Contextual Knowledge in Learning Concepts to Improve Performance. </title> <type> PhD thesis, </type> <institution> Rutgers University Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: If learning takes place within a particular performance system, external knowledge includes information about that as well for instance, the performance system's efficiency constraints. The general notion of making implicit system-external knowledge explicit for learning has been explored, most notably by Keller <ref> [Keller 87] </ref> and Kedar [Kedar-Cabelli 88]. Both Keller's and Kedar's work focused on the learning task of performance improvement, rather than on the acquisition of new information. As discussed in Chapter 2, EBL is a learning method for the task of performance improvement.
Reference: [Kodratoff 87] <editor> Kodratoff, Y. and Tecuci, G. DISCIPLE-1: </editor> <title> Interactive Apprentice System in Weak Theory Fields. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 271 - 273. </pages> <address> Milan, Italy, </address> <year> 1987. </year> <note> 196 [Kodratoff and Tecuci 87] Kodratoff, </note> <editor> Y. and Tecuci, G. </editor> <booktitle> What is an Explanation in DISCIPLE? In Proceedings of the Fourth International Machine Learning Workshop, </booktitle> <pages> pages 160 - 166. </pages> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: A second set of work addressing the problem of incompleteness has focused on developing interactive knowledge refinement systems that require the presence of an expert at all phases of learning (e.g., <ref> [Kodratoff 87] </ref> and [Wilkins 88]). While this is a valid approach, we have been motivated by the need for systems that can operate 15 autonomously. Contextual heuristics make use of previously seen examples and explanations to automatically guide the creation of a new rule. <p> OCCAM proposes a tentative rule based on a single example and then waits for later examples to confirm or deny the rule. Unlike Gemini, however, it is not able to extract other potentially relevant information from its previous experience. 2.3.4. Kodratoff and Tecuci DISCIPLE <ref> [Kodratoff and Tecuci 87; Kodratoff 87] </ref> uses an incomplete domain theory to pose questions to a user who then teaches the system missing rules. DISCIPLE has been applied to the domain of designing technologies for the manufacture of loudspeakers. <p> Next DISCIPLE enters an SBL phase in order to generalize the rule. It searches the domain theory for information that matches the graph edges of the explanation selected. For example, the partial explanation in Figure 14b was extracted 8 From <ref> [Kodratoff 87] </ref>, page 271. Mowicoll is roughly translated as Romanian glue. 37 from DISCIPLE's domain theory as being similar to the first partial explanation. Before the matching and generalization of SBL can proceed, the user must validate all examples as being similar and relevant in the current context. <p> Two major criticisms were raised in Chapter 2 with respect to previous solutions to the problem of theory incompleteness in EBL: (1) the requirement that auxiliary theories be complete (e.g., [Rajamoney 89; Pazzani 88; Wilkins 88]); (2) the need for active presence of an expert (e.g., <ref> [Hall 86; Kodratoff 87; Wilkins 88] </ref>). Similarity-based learning plays a role, but this role is limited to verification or generalization of knowledge proposed by another source. Our solution to the problem of incompleteness places greater emphasis on SBL as the primary mechanism for acquiring missing knowledge.
Reference: [Kolodner 83] <author> Kolodner, J. L. </author> <title> Maintaining Organization in a Dynamic Long-Term Memory. </title> <booktitle> Cognitive Science 7(4):243 - 280, </booktitle> <year> 1983. </year>
Reference-contexts: The concept of building a memory of input examples for later use is not new. Memories of this type were constructed previously in systems such as UNIMEM [Lebowitz 87], IPP [Lebowitz 80], and CYRUS <ref> [Kolodner 83] </ref>, among others. There are a number of differences between our implementation and the others.
Reference: [Kolodner 87] <author> Kolodner, J. L. </author> <title> Extending Problem Solver Capabilities Through Case-Based Inference. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 167 - 178. </pages> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: In CBR cases of previously seen problems and their solutions are stored in memory. When a new problem arises an old case is retrieved. Mechanisms similar to those described above for analogy may be used to retrieve appropriate cases for modification. Work in CBR includes <ref> [Hammond 88; Kolodner 87] </ref>. 2.6. Summary This chapter has surveyed work in machine learning related to our research. <p> Gemini's memory building is also similar in spirit to work in the area of Case-Based Reasoning (CBR), most notably Kolodner's <ref> [Kolodner 87] </ref>. The underlying principle of case-based reasoning is the idea that in problem solving it is often most effective to recall a solution to an earlier problem and to modify it, rather than to derive a new solution entirely.
Reference: [Laird et al. 90] <author> Laird, J., Hucka, M., Yager, E., and Tuck, C. </author> <title> Correcting and Extending Domain Knowledge using Outside Guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 235 - 243. </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: Hall also relies on a user to input an analogue to a failure situation from which the system can learn. His system is left to analyze the analogue as well as to generate a new rule from it. Although not discussed here, <ref> [Laird et al. 90] </ref> and [Genest et al. 90] similarly rely on user guidance to provide the information to be included in a missing rule. Our work is based on the observation that such detailed guidance is not always available to a system.
Reference: [Lebowitz 80] <author> Lebowitz, M. </author> <title> Generalization and Memory in an Integrated Understanding System. </title> <type> PhD thesis, </type> <institution> Yale University Department of Computer Science, </institution> <year> 1980. </year>
Reference-contexts: Michel. The blast occurred at 7:40 P.M. with scores of customers in the store. Gemini is given as input a conceptual description of these events (though it would be reasonable to parse the natural language input <ref> [Lebowitz 80] </ref>). The conceptual description is a hierarchical frame, as will be discussed in Section 3.5.2. <p> Clearly, people find a qualitative difference between the levels of similarity. Explanations generated by G-EBL can help focus on specific sets of relevant features. 3.6.3. Output Relationships among input examples are recorded in a Generalization-Based Memory (GBM) <ref> [Lebowitz 80; Lebowitz 86b] </ref> that is the output of G-CC. In general, a generalization-based memory is a directed acyclic graph in which terminal nodes are specific instances; internal nodes represent generalizations that classify the input instances that are their children. <p> That information can later be used in selecting examples from which to perform induction as described in Section 3.7. The concept of building a memory of input examples for later use is not new. Memories of this type were constructed previously in systems such as UNIMEM [Lebowitz 87], IPP <ref> [Lebowitz 80] </ref>, and CYRUS [Kolodner 83], among others. There are a number of differences between our implementation and the others.
Reference: [Lebowitz 86a] <author> Lebowitz, M. </author> <title> Integrated Learning: Controlling Explanation. </title> <booktitle> Cognitive Science 10:219 - 240, </booktitle> <year> 1986. </year>
Reference-contexts: This problem is partially independent of the large search space problem and may occur even when no search is 47 involved. Viewing explanation generation as search, it is characterized by large values for D. The large search space problem is addressed by <ref> [Lebowitz 86a; Lebowitz 90] </ref> as well as by Ellman [Ellman 87; Ellman 88a; Ellman 88b; Ellman 89b]. Lebowitz uses SBL generalizations to focus on features that would appear specifically as causes or effects in an explanation. This constrains the rules that must be considered.
Reference: [Lebowitz 86b] <author> Lebowitz, M. </author> <title> Concept Learning in a Rich Input Domain: </title> <booktitle> Generalization-Based Memory. Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: Clearly, people find a qualitative difference between the levels of similarity. Explanations generated by G-EBL can help focus on specific sets of relevant features. 3.6.3. Output Relationships among input examples are recorded in a Generalization-Based Memory (GBM) <ref> [Lebowitz 80; Lebowitz 86b] </ref> that is the output of G-CC. In general, a generalization-based memory is a directed acyclic graph in which terminal nodes are specific instances; internal nodes represent generalizations that classify the input instances that are their children.
Reference: [Lebowitz 87] <author> Lebowitz, M. </author> <title> Experiments with Incremental Concept Formation: </title> <booktitle> UNIMEM. Machine Learning 2(2):103 - 138, </booktitle> <year> 1987. </year>
Reference-contexts: The system looks for subsets of the input examples such that the elements of each subset share a number of descriptive features not shared by the members of any other set. Work in this area includes that of <ref> [Fisher 87; Lebowitz 87; Michalski and Stepp 83] </ref>. As with the overview of explanation-based learning above, this chapter does not present a formal treatment of similarity-based learning. <p> That information can later be used in selecting examples from which to perform induction as described in Section 3.7. The concept of building a memory of input examples for later use is not new. Memories of this type were constructed previously in systems such as UNIMEM <ref> [Lebowitz 87] </ref>, IPP [Lebowitz 80], and CYRUS [Kolodner 83], among others. There are a number of differences between our implementation and the others.

Reference: [Minton 88] <author> Minton, S. </author> <title> Learning Effective Search Control Knowledge: An Explanation-Based Approach. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: It can be used by the robot to 4 Modified from [Mitchell et al. 86], page 59. 23 efficiently recognize cups, because it can now simply apply a single rule rather than having to construct a proof. (It has more recently been shown by <ref> [Minton 88] </ref> that there are cases in which the application of a single rule is less efficient than proof construction due to the complexity of determining that the rule applies.) In domains such as that of solving algebraic equations, where proofs can resemble traces of problem solving steps, the generalized proofs
Reference: [Mitchell 78] <author> Mitchell, T. M. </author> <title> Version Spaces: An Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, </institution> <year> 1978. </year>
Reference-contexts: Those methods that receive input classified by an external source are termed learning from examples. Work in this area includes that of <ref> [Mitchell 78; Quinlan 86; Winston 72] </ref> among others. These systems in turn may be divided on the basis of the types of examples input. Some are given both positive and negative examples of a concept, while others receive only positive examples. <p> Mitchell's proposed solution is an extension of the version space algorithm <ref> [Mitchell 78] </ref> for SBL. A version space is a lattice of representable concepts. Representations of specific instances are at one end of the lattice, while the most general concept definition, of which all entities are members, is at the other end. The version space algorithm maintains two sets of concepts.
Reference: [Mitchell 80] <author> Mitchell, T. M. </author> <title> The Need for Biases in Learning Generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Rutgers University Department of Computer Science, </institution> <year> 1980. </year>
Reference-contexts: Many SBL systems deal with more complex representations or disjunctive definitions, for example. Though they do not possess the explicit domain theories of EBL systems, SBL programs do have an implicit bias built into them that allows them to arrive at particular general concept definitions <ref> [Mitchell 80; Utgoff 86] </ref>. For instance, in generating the CUP definition of Figure 10, the algorithm did not recognize the similarity in the names of the CUP owners. The capacity to look for such similarities was simply not encoded in the learning algorithm. <p> Many domain and implementation-specific decisions will be reflected in the system as well. Although the effects of explicit domain theories in guiding learning were long accepted, Mitchell <ref> [Mitchell 80] </ref> was the first to draw attention to the various types of information that influence learning in systems traditionally viewed as knowledge-poor. Thus even in SBL systems, there exists contextual information that influences learning.
Reference: [Mitchell 84] <author> Mitchell, T. M. </author> <title> Toward Combining Empirical and Analytical Methods for Inferring Heuristics. </title> <booktitle> Human and Artificial Intelligence. </booktitle> <publisher> North Holland Publishing Company, </publisher> <address> Amsterdam, </address> <note> 1984. </note> <author> [Mitchell et al. 86] Mitchell, T. M., Keller, R. M., and Kedar-Cabelli, S. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning 1(1):47 - 80, </journal> <note> 1986. </note> <author> [Ng and Mooney 90] Ng, H. T. and Mooney, R. J. </author> <title> The Role of Coherence in Constructing and Evaluating Abductive Explanations. </title> <booktitle> In Proceedings of the AAAI Symposium on Automated Abduction. </booktitle> <institution> Stanford University, </institution> <year> 1990. </year> <note> Unpublished Proceedings. </note>
Reference-contexts: METHOD: compare examples to find similarities among positive examples and differences of positive examples from negative. 2.2.3. A Comparison of Explanation-Based and Similarity-Based Learning Explanation-based and similarity-based learning methods are approaches that can be placed on opposite ends of a spectrum describing purely deductive to purely inductive techniques <ref> [Mitchell 84] </ref>. In explanation-based learning, analysis of a single example guides the generalization of knowledge possessed by a system in order to make itself more efficient. Its power derives from having extensive domain knowledge. The dependence upon the domain theory is also a weakness of the method. <p> The hypothesis space is potentially huge. A problem to be addressed is how search may be directed to prune large parts of the hypothesis space. <ref> [Mitchell 84] </ref> was among the first to propose that explanation-based learning be used to focus the attention of SBL, enabling it to make larger leaps through its hypothesis space of concept definitions. Mitchell's proposed solution is an extension of the version space algorithm [Mitchell 78] for SBL.
Reference: [O'Rorke 90] <author> O'Rorke, P. </author> <title> Working Notes of the AAAI Symposium on Automated Abduction. </title> <type> Technical Report TR 90-32, </type> <institution> University of California, Irvine, </institution> <year> 1990. </year> <note> 198 [Ourston and Mooney 90] Ourston, </note> <author> D. and Mooney, R. </author> <title> Changing the Rules: A Comprehensive Approach to Theory Refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815 - 820. </pages> <address> Boston, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Their systems are well-suited for this purpose, but cannot function on their own. One respect in which Gemini currently requires human intervention is in the selection of ``best'' partial explanations. Although this is a hard problem, there are already paths of research showing sufficiently promising results <ref> [O'Rorke 90] </ref> and thus we assume that it will become less relevant a concern in the near future. 181 6.4.5.
Reference: [Pazzani 87] <author> Pazzani, M. J. </author> <title> Inducing Causal and Social Theories: A Prerequisite for Explanation-Based Learning. </title> <booktitle> In Proceedings of the Fourth International Machine Learning Workshop, </booktitle> <pages> pages 230 - 241. </pages> <address> Irvine, California, </address> <year> 1987. </year>
Reference: [Pazzani 88] <author> Pazzani, M. J. </author> <title> Learning Causal Relationships: An Integration of Empirical and Explanation-Based Learning Methods. </title> <type> PhD thesis, </type> <institution> UCLA Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: Previous approaches, however, make strong assumptions about information that will be made available to the learning system when new rules for a domain theory are required. Some earlier solutions to the problem of incompleteness have required encoding auxiliary domain theories (e.g., <ref> [Pazzani 88; Rajamoney 89] </ref>). Our contention is that such auxiliary theories are problematic for two reasons: 1. <p> Two major criticisms were raised in Chapter 2 with respect to previous solutions to the problem of theory incompleteness in EBL: (1) the requirement that auxiliary theories be complete (e.g., <ref> [Rajamoney 89; Pazzani 88; Wilkins 88] </ref>); (2) the need for active presence of an expert (e.g., [Hall 86; Kodratoff 87; Wilkins 88]). Similarity-based learning plays a role, but this role is limited to verification or generalization of knowledge proposed by another source.
Reference: [Pazzani et al. 86] <author> Pazzani, M., Dyer, M., and Flowers, M. </author> <title> The Role of Prior Causal Theories in Generalization. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 545 - 550. </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1986. </year>
Reference: [Pazzani et al. 87] <author> Pazzani, M., Dyer, M., and Flowers, M. </author> <title> Using Prior Learning to Facilitate the Learning of New Causal Theories. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 277 - 279. </pages> <address> Milan, Italy, </address> <year> 1987. </year>
Reference: [Quinlan 86] <author> Quinlan, J. R. </author> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning 1(1):81 - 106, </booktitle> <year> 1986. </year> <title> [Radio Manual 86] Department of the Army. Technical Manual - Unit Maintenance. </title> <type> Technical Report TM 11-5820-890-20-1, </type> <institution> Department of the Army, </institution> <year> 1986. </year>
Reference-contexts: Those methods that receive input classified by an external source are termed learning from examples. Work in this area includes that of <ref> [Mitchell 78; Quinlan 86; Winston 72] </ref> among others. These systems in turn may be divided on the basis of the types of examples input. Some are given both positive and negative examples of a concept, while others receive only positive examples.
Reference: [Rajamoney 88] <author> Rajamoney, S. A. </author> <title> Experimentation-Based Theory Revision. </title> <booktitle> In Proceedings of the AAAI Symposium on Explanation-Based Learning, </booktitle> <pages> pages 7 - 11. </pages> <institution> Stanford University, </institution> <year> 1988. </year> <note> Unpublished Proceedings. </note>
Reference-contexts: Here Rajamoney and DeJong's second description of incompleteness is treated as incorrectness of the domain theory. This section discusses five approaches to the problem of missing rules. 7 In defining incompleteness types, incorrectness and intractability are ignored. 30 2.3.1. Rajamoney Rajamoney <ref> [Rajamoney et al. 85; Rajamoney 88; Rajamoney 89] </ref> proposes a method called experimentation-based theory revision, implemented in the ADEPT system, as a solution to the problem of incompleteness in a domain theory. Michalski and Ko [Michalski and Ko 88] have also discussed the use of experimentation in addressing this problem.

Reference: [Russell 86] <author> Russell, S. J. </author> <title> Analogical and Inductive Reasoning. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, </institution> <year> 1986. </year>
Reference-contexts: Thus the matching techniques of SBL are often applied here. Furthermore, since similarities might be found at a different level from the purely superficial one of entities' observable characteristics, the entities are often analyzed using deductive, or explanation-based, mechanisms as well. Examples of work in analogy include <ref> [Burstein 86; Carbonell 83a; Carbonell 83b; Kedar-Cabelli 85; Russell 86] </ref>. Exemplar-based learning is another mechanism that combines inductive and deductive techniques. In exemplar-based learning a concept is represented by a stereotypical example.
Reference: [Salzberg 83] <author> Salzberg, S. </author> <title> Generating Hypotheses to Explain Prediction Failures. </title> <booktitle> In Proceedings of the Third National Conference on Artificial Intelligence, </booktitle> <pages> pages 352 - 355. </pages> <address> Washington, DC, </address> <year> 1983. </year>
Reference-contexts: A number of researchers have more recently investigated the idea of using EBL to provide extra information to SBL in order that it might intelligently prune its search space. The following sections present the approaches of Salzberg, Swaminathan, and Van Lehn. 42 2.4.1. Salzberg Salzberg, in HANDICAPPER <ref> [Salzberg 83; Salzberg 85] </ref>, derives explanations to identify potentially important features for revising the definition of a concept. HANDICAPPER's domain of application is horse racing, where its performance task is to predict the horse that will win a race.
Reference: [Salzberg 85] <author> Salzberg, S. </author> <title> Heuristics for Inductive Learning. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 603 - 609. </pages> <address> Los Angeles, California, </address> <year> 1985. </year>
Reference-contexts: A number of researchers have more recently investigated the idea of using EBL to provide extra information to SBL in order that it might intelligently prune its search space. The following sections present the approaches of Salzberg, Swaminathan, and Van Lehn. 42 2.4.1. Salzberg Salzberg, in HANDICAPPER <ref> [Salzberg 83; Salzberg 85] </ref>, derives explanations to identify potentially important features for revising the definition of a concept. HANDICAPPER's domain of application is horse racing, where its performance task is to predict the horse that will win a race. <p> Discussion in Chapter 2 on providing focus for similarity-based learning 51 concentrated on two basic approaches: (1) felicity conditions between a teacher and the learning system [Van Lehn 87] and (2) the role of explanations in determining relevance of features <ref> [Salzberg 85; Swaminathan 88] </ref>. Since a goal of our work is to minimize interaction with an expert or teacher, we concentrate on the latter approach. In addition to focusing on specific features, however, our heuristics make use of explanation information to guide SBL's selection of examples for induction.
Reference: [Silver 86] <author> Silver, B. </author> <title> Precondition Analysis: Learning Control Information. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference: [Silver 88] <editor> Silver, </editor> <publisher> B. </publisher>
Reference-contexts: A second type of incorrectness, probably the more basic one, is that in which invalid knowledge exists in the theory and must be retracted. The first manifestation of incorrectness is addressed using SBL techniques by 46 [Araya 84], <ref> [Silver 88] </ref>, and [Bergadano, et al. 87]. [Smith et al. 85] also address the problem, but with the aid of a detailed theory underlying the abstract one that is actually used. [Ginsberg 88] solves the problem by doing a massive initial check for redundancy and inconsistency.
References-found: 48

