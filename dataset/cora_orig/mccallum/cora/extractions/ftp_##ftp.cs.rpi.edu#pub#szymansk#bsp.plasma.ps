URL: ftp://ftp.cs.rpi.edu/pub/szymansk/bsp.plasma.ps
Refering-URL: http://www.cs.rpi.edu/~szymansk/papers.html
Root-URL: http://www.cs.rpi.edu
Email: fnibhanum, nortonc, szymanskg@cs.rpi.edu  
Title: Plasma Simulation on Networks of Workstations using the Bulk-Synchronous Parallel Model  
Author: Mohan V. Nibhanupudi Charles D. Norton Boleslaw K. Szymanski 
Keyword: BSP Model, Networks of Workstations, Parallel Computing, Plasma  
Address: Troy, NY, USA 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract: Computationally intensive applications with frequent communication and synchronization require careful design for efficient execution on networks of workstations. We describe a Bulk-Synchronous Processing (BSP) model implementation of a plasma simulation and use of BSP analysis techniques for tuning the program for arbitrary architectures. In addition, we compare the performance of the BSP implementation with a version using MPI. Our results indicate that the BSP model, serving as a basis for an efficient implementation, compares favorably with MPI. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Birdsall and A. B. Langdon. </author> <title> Plasma Physics via Computer Simulation. </title> <booktitle> The Adam Hilger Series on Plasma Physics. </booktitle> <address> Adam Hilger, New York, </address> <year> 1991. </year>
Reference-contexts: Familiar examples of plasma include the Aurora Borealis, neon signs, the ionosphere, and solar winds. Fusion energy is also an important application area of plasma physics research. The plasma Particle In Cell simulation model <ref> [1] </ref> integrates in time the trajectories of millions of charged particles in their self-consistent electromagnetic fields. Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. Following [6], our simulation models only the electrostatic (coulomb) interactions.
Reference: [2] <author> P. C. Liewer and V. K. Decyk. </author> <title> A General Concurrent Algorithm for Plasma Particle-in-Cell Simulation Codes. </title> <journal> J. of Computational Physics, </journal> <volume> 85 </volume> <pages> 302-322, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Complex scientific applications such as plasma simulation rely on parallel processing for efficiency. The General Concurrent Particle In Cell (GCPIC) method <ref> [2] </ref> partitions the particle and field data among the processors to distribute the computational load. However, this method leads to frequent communication and synchronization among the component processes because particle and field data must be exchanged between partitions. <p> Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. Following [6], our simulation models only the electrostatic (coulomb) interactions. The General Concurrent Particle in Cell (GCPIC) Algorithm <ref> [2] </ref> partitions the particles and grid points among the processors of the MIMD (multiple-instruction, multiple-data) distributed-memory machine. The particles are evenly distributed among processors in the primary decomposition, which makes advancing particle positions in space and computing their velocities efficient.
Reference: [3] <author> P. C. Liewer, V. K. Decyk, J. D. Dawson, and G. C. Fox. </author> <title> Plasma Particle Simulations on the Mark III Hypercube. </title> <journal> Mathematical and Computer Modelling, </journal> <volume> 11 </volume> <pages> 53-54, </pages> <year> 1988. </year>
Reference-contexts: All the processors send charge depositions to their local grid, to one processor. This processor combines the individual charge depositions and broadcasts the final grid to all the processors. 5 Plasma Simulation using a replicated grid P. C. Liewer et. al. <ref> [3] </ref> implemented the plasma simulation on a 32 node JPL Mark III hypercube. The particle computations were done in parallel; each processor had a copy of the field data. Thus their implementation used a replicated grid, as a first step towards parallelizing the particle simulation codes.
Reference: [4] <author> Richard Miller. </author> <title> A Library for Bulk-synchronous Parallel Programming. </title> <booktitle> Proceedings of the British Computer Society Parallel Processing Specialist Group workshop on General Purpose Parallel Computing, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Conversely, linear gather restricts computations to be delayed until all of the data arrives at the processor. This feature may make logarithmic gather more attractive than linear gather under some circumstances. 3.2 The Oxford BSP Library The Oxford BSP Library <ref> [4, 5] </ref>, developed by Richard Miller, is used to implement the plasma simulation on NOWs. It is based on a slightly simplified version of the model presented in [8]. The programming model uses a SPMD program style, and static allocation of processors.
Reference: [5] <author> Richard Miller and Joy Reed. </author> <title> The Oxford BSP Library Users' Guide Version 1.0. </title> <type> Technical report, </type> <institution> Programming Research Group, University of Oxford, </institution> <year> 1993. </year>
Reference-contexts: Conversely, linear gather restricts computations to be delayed until all of the data arrives at the processor. This feature may make logarithmic gather more attractive than linear gather under some circumstances. 3.2 The Oxford BSP Library The Oxford BSP Library <ref> [4, 5] </ref>, developed by Richard Miller, is used to implement the plasma simulation on NOWs. It is based on a slightly simplified version of the model presented in [8]. The programming model uses a SPMD program style, and static allocation of processors.
Reference: [6] <author> C. D. Norton, B. K. Szymanski, and V. K. Decyk. </author> <title> Object Oriented Parallel Computation for Plasma PIC Simulation. </title> <journal> Communications of the ACM, </journal> <note> 38(10): to appear, </note> <month> October </month> <year> 1995. </year>
Reference-contexts: The plasma Particle In Cell simulation model [1] integrates in time the trajectories of millions of charged particles in their self-consistent electromagnetic fields. Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. Following <ref> [6] </ref>, our simulation models only the electrostatic (coulomb) interactions. The General Concurrent Particle in Cell (GCPIC) Algorithm [2] partitions the particles and grid points among the processors of the MIMD (multiple-instruction, multiple-data) distributed-memory machine. <p> As particles move among partitioned regions, they are passed to the processor responsible for the new region. For computational efficiency, field/grid data on the border of partitions are replicated on the neighboring processor to avoid frequent off-processor references. As in <ref> [6] </ref>, we perform a Beam-Plasma instability experiment in which a weak low density electron beam is injected into a stationary (yet mobile) background plasma of high density, driving plasma waves to instability. <p> Using FFT Exchange Guard Cells Gather Step Scatter Step Scatter Step - Electron & Ion Background Loop SectionInitialization Section <ref> [6] </ref> under c flCopyright 1995 by ACM Inc. processors (p), processor speed (s), synchronization periodicity (L) and a parameter to indicate the global computation to communication balance (g). <p> The library is small, simple to use, yet robust. Figure 2 lists the library functions for C programs. 4 Plasma Simulation on NOWs using BSP Model Norton et. al. <ref> [6] </ref> describe an object oriented implementation of the plasma simulation for distributed memory machines based on the GCPIC method. Their implementation partitions both the particles and the field data among the processors. As particles advance in space, they need to be redistributed to appropriate partitions. <p> It also has one disadvantage. The field computation is not parallelized. Since it constitutes 10% of overall computation, replicating the grid limits the potential speedup to about 10. There are other sources of execution inefficiency on a network of workstations. Parallel computer implementation <ref> [6] </ref> redistributes particles in a series of synchronous steps in which each processor exchanges particles with its neighbors. Such synchronization steps are very expensive on NOWs. In our current approach, the processors maintain a set of buffers, one for each processor.
Reference: [7] <author> W. Gropp E. Karrels E. Lusk P. Bridges, N. Doss and A. Skjellum. </author> <title> User's Guide to mpich, a Portable Implementation of MPI. </title> <year> 1995. </year>
Reference-contexts: We have implemented the replicated grid version of plasma simulation using Version 1.0.8 of the MPI message passing library, jointly developed by Argonne National Laboratory and Mississippi State University <ref> [7] </ref>. While the BSP implementation uses a Bulk Synchronous approach, the MPI version uses asynchronous communication and introduces no other synchronization than inherently present in the application. Both versions use a replicated grid with particle redistribution. Table 3 presents a comparison between the two implementations.
Reference: [8] <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The content does not necessarily reflect the position or policy of the U.S. Government. overall execution time (wall clock time), rather than the system or user time. Second, we want to investigate suitability of the Bulk-Synchronous Processing Model <ref> [8] </ref> for implementing such applications. <p> Experiments such as this can be used to verify plasma theories and to study the time evolution of macroscopic quantities such as potential and velocity distributions. Figure 1 shows an overview of the organization of the simulation program. 3 Bulk-Synchronous Processing Model and NOWs The Bulk-Synchronous Processing model <ref> [8] </ref>, introduced by Leslie Valiant, consists of components (processors) which perform processing or memory functions, a router that provides point to point communication between pairs of components, and a synchronization mechanism to synchronize all or a subset of the components at regular intervals. Computation consists of a sequence of supersteps. <p> It is based on a slightly simplified version of the model presented in <ref> [8] </ref>. The programming model uses a SPMD program style, and static allocation of processors. The most significant feature of the library is its use of remote assignment semantics for non-local data access, which simplifies debugging. The library is small, simple to use, yet robust.
References-found: 8

