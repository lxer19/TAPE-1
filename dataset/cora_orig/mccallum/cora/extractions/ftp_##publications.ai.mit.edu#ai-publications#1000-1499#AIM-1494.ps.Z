URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1494.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/web-pis/poggio/memos.html
Root-URL: 
Title: Towards an Example-Based Image Compression Architecture for Video-Conferencing  
Author: Sebastian Toelg and Tomaso Poggio 
Address: Mary-land, College Park, MD 20742, USA  
Note: Copyright c Massachusetts Institute of Technology,  Current address:  
Date: 1494 June, 1994  100  1994  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  Computer Vision Laboratory, Center for Automation Research, Bd. 115, University of  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Very-low bandwidth video-conferencing, which is the simultaneous transmission of speech and pictures (face-to-face communication) of the communicating parties, is a challenging application requiring an integrated effort of computer vision and computer graphics. This paper consists of two major parts. First, we present the outline of a simple approach to video-conferencing relying on an example-based hierarchical image compression scheme. In particular, we discuss the use of example images as a model, the number of required examples, faces as a class of semi-rigid objects, a hierarchical model based on decomposition into different time-scales, and the decomposition of face images into patches of interest. In the second part, we present several algorithms for image processing and animation as well as their experimental evaluation. Among the original contributions of this paper is an automatic algorithm for pose estimation and normalization. Experiments suggest interesting estimates of necessary spatial resolution and frequency bands. We also review and compare different algorithms for finding the nearest neighbors in a database for a new input as well as a generalized algorithm for blending patches of interest in order to synthesize new images. Extensions for image sequences are proposed together with possible extensions based on the interpolation techniques of Beymer, Shashua and Poggio (1993) between example images. Finally, we outline the possible integration of several algorithms to illustrate a simple model-based video-conference system. This report describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences and at the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology. This research is sponsored by grants from the Office of Naval Research under contracts N00014-92-J-1879 and N00014-93-1-0385; and by a grant from the National Science Foundation under contract ASC-9217041 (this award includes funds from ARPA provided under the HPCC program). Additional support is provided by the North Atlantic Treaty Organization, ATR Audio and Visual Perception Research Laboratories, Mitsubishi Electric Corporation, Sumitomo Metal Industries, and Siemens AG. Support for the A.I. Laboratory's artificial intelligence research is provided by ARPA contract N00014-91-J-4038. S. Toelg was supported by a postdoctoral fellowship from the Deutsche Forschungsgemeinschaft while he was at MIT. Parts of this paper were written and edited while S. Toelg was at the Institut fur Neuroinformatik, Ruhr-University Bochum, Germany, and later at the Computer Vision Lab., Center for Automation Research, University of Maryland, College Park. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Aizawa, H. Harashima, and T. Saito. </author> <title> Model-based analysis synthesis image coding (MBASIC) system for a person's face. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 1(2) </volume> <pages> 139-152, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: With this kind of coding, very low bit rates can by realized since basically only the analyzed model parameters are transmitted. The general approach of such model-based coding techniques | being still an active research topic (cf. <ref> [1, 20, 44, 24, 25] </ref>, to give some examples) | for faces is to use a volumetric 3-D facial model (such as polygonal wire frame model). Full face images under standard view can be projected onto the wire frame model for reconstruction by using texture mapping techniques. <p> Most of them rely on an explicit metric 3-D model (wire frame model) of the specific face <ref> [1, 44] </ref>. These volumetric models are obtained from image data under different viewpoints or from laser range-scanners. Shape-from-motion algorithms are known to be not very stable and quite noise sensitive. Recently, several structure-from-motion algorithms have been demonstrated to yield good results from real image sequences. <p> However, they crucially depend on stable features that can be accurately localized and tracked in the images. In face images, features of this kind are not present in sufficient number or quality. In some work auxiliary marks (like white points in <ref> [1] </ref>) were attached to the person's skin. While such aids may be useful for research purposes they are certainly not acceptable for a commercial video-conference system. On the other hand, laser range-scanners are very expensive, comparatively slow, and difficult to handle (due to subtle scanning mechanics).
Reference: [2] <author> S. Akamatsu, T. Sasaki, H. Fukamachi, N. Masui, and Y. Suenaga. </author> <title> An accurate and robust face identification scheme. </title> <booktitle> In Proc. 11 IARP Intl. Conf. on Pat. Recog., </booktitle> <pages> pages 217-220, </pages> <address> The Hague, the Nether-lands, Aug. 1992. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> More robustness could be achieved by a preceding segmentation step. Suitable techniques for segmentation of the face from the static background are readily available, such as color segmentation <ref> [2, 4] </ref> and integral projection of directional information [15].
Reference: [3] <author> S. Akamatsu, T. Sasaki, H. Fukamachi, and Y. Sue-naga. </author> <title> A robust face identification scheme - KL expansion of an invariant feature space. In Intelligent Robots and Computer Vision X: </title> <booktitle> Algorithms and Techniques, </booktitle> <volume> volume 1607, </volume> <pages> pages 71-84, </pages> <address> Boston, MA, </address> <month> Nov. </month> <year> 1991. </year> <booktitle> SPIE|The International Society for Optical Engineering. </booktitle>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [4] <author> S. Akamatsu, T. Sasaki, H. Fukamachi, and Y. Sue-naga. </author> <title> Automatic extraction of target images for face identification using the sub-space classification method. </title> <journal> IEICE Transactions on Information and Systems, </journal> <note> 1993. Appears in special section on machine vision & applications. </note>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio [13, 15]. Several other approaches to face recognition have also been presented (for example <ref> [6, 61, 36, 4] </ref>). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. First the eyes are located by computing the normalized cross-correlation coefficient with a single eye template at different resolution scales. <p> More robustness could be achieved by a preceding segmentation step. Suitable techniques for segmentation of the face from the static background are readily available, such as color segmentation <ref> [2, 4] </ref> and integral projection of directional information [15].
Reference: [5] <author> D. H. Ballard and C. M. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: To obtain a pose compensated face, the new image I is warped according to the pose parameters. However, here we use bicubic interpolation (e.g., Lagrangian interpolation <ref> [11, 5] </ref> or bicubic splines) for the warping, since we do not want the high frequency components of the original image to be sup pressed and to reduce aliasing effects. 4.1.5 Experimental results Using real image data, we will now present some typical experimental results to demonstrate the robustness and versatility
Reference: [6] <author> R. J. Baron. </author> <title> Mechanisms of human facial recognition. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 15 </volume> <pages> 137-178, </pages> <year> 1981. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio [13, 15]. Several other approaches to face recognition have also been presented (for example <ref> [6, 61, 36, 4] </ref>). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. First the eyes are located by computing the normalized cross-correlation coefficient with a single eye template at different resolution scales. <p> A total of 35 geometrical features are extracted. Recognition is then performed with a Bayes classifier applied to the vector of geometric features. The second approach uses template matching by correlation and can be regarded as an extension of the pioneering work of Baron <ref> [6] </ref>. Images of frontal views of faces are normalized as described above. Each person is represented by four rectangular masks centered around the eyes, nose, and mouth, respectively. The relative position of these masks is the same for all persons. <p> If E and I are identical we have complete positive correlation C N = 1:0. If C N 0:0, then the images are uncorrelated. The use of this standard technique is suggested by the good results reported for face recognition by other researchers (cf. <ref> [6, 13, 28] </ref>). Our implementation performs normalized linear cross-correlation within a multiresolution hierarchy. Correlation is computed between corresponding levels of Lapla-cian pyramids for the new images and the examples for the subregion. Computation starts at a high pyramid level at low resolution.
Reference: [7] <author> J. L. Barron, D. J. Fleet, and S. S. Beauchemin. </author> <title> Performance of optical flow techniques. </title> <type> Technical Report RPL-TR 9107, </type> <institution> Queen's University, Kingston, Ontario, Robotics and Perception Laboratory, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation [26, 31, 43, 32, 34, 29, 55]. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin <ref> [7] </ref>. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques. <p> A matrix is ill-conditioned if its condition number is too large, and it is singular if is infinite. 8 i.e., the spatial Gaussian curvature in the intensity image, turned out to be a better confidence measure. This finding is in accordance with the more recent results discussed in <ref> [7] </ref>. Here, we may use the condition number of the matrix D: (D) = max = min as a measurement for reliability. 4. Here, we advocate the magnitude of the smallest eigenvalue min = min ( 1 ; 2 ) as an appropriate confidence measure. <p> Here, we advocate the magnitude of the smallest eigenvalue min = min ( 1 ; 2 ) as an appropriate confidence measure. This is along the lines of the practical results reported in <ref> [7] </ref>. A brief justification for this choice will be given. We are only interested in the first case for solving (2) where the full 2-D displacement vector can be recovered reliably.
Reference: [8] <author> J. R. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, </institution> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: While this coupling between the coefficients for geometric and texture interpolation makes sense for certain applications of 1D interpolation, e.g., for frame rate conversion (see <ref> [8] </ref>), it is not necessarily the best approach for more general cases. We suggest exploiting the freedom of adjusting the coefficients for geometric and texture interpolation independently.
Reference: [9] <author> D. Beymer. </author> <title> Face recognition under varying pose. </title> <note> Technical Report AI Memo 1461 and CBIP Paper 89, </note> <institution> Artificial Intelligence Laboratory, MIT and Center for Biological Information Processing, Whitaker College, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [10] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <note> Technical Report AI Memo 1431 and CBIP Paper 80, </note> <institution> Artificial Intelligence Laboratory, MIT and Center for Biological Information Processing, Whitaker College, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: There may also be higher dimensional cases where better interpolation can be achieved if examples other than the nearest neighbors are used <ref> [10] </ref>. For the purpose of video-conferencing this second approach seems more natural. It is in fact not immediately obvious how details of facial expressions should be parameterized in a video-conference system. Moreover, the first method requires explicit estimation of these parameters in addition to pose. <p> Thus, it is potentially more flexible at the expense of a data-dependent higher dimensional parameter space. To evaluate the feasibility of this concept in the preliminary implementation of this paper we will consider only the nearest neighbor in the database. Beymer, Shashua & Poggio <ref> [10] </ref> have provided a preliminary evaluation of the two approaches for video-conferencing. 3.2 Number of examples The major objection to the example-based approach for a video-conference system might be an excessive requirement of memory to store the example database common to the sender (encoder) and the receiver (decoder) side. <p> reconstruct the face image on the receiver side by blending the regions of the subimages together 5. transform the composed face image into the pose of the original image on the sender side A desirable extension to this simple scheme is to "interpolate" each subimage between several suitable examples (see <ref> [10] </ref>). Also, adequate ways to update the example database automatically have to be devised. 4.1 Automatic and robust pose estimation In what follows we describe a novel algorithm for automatic pose estimation and normalization of new face images relative to a given example, i.e., a reference image. <p> Instead of using the nearest neighbors only, it is natural to "interpolate" novel views between examples from the database as already mentioned in Section 3.1. Extending previous results [45, 47, 48, 46], recent work of Beymer, Shashua & Poggio <ref> [10] </ref> presents the mathematical formulation and experimental demonstrations of several versions of such an approach. The feasibility of interpolation between images has been successfully demonstrated for the multidimensional interpolation of novel human face images. So far, the examples are selected manually for training. <p> The argument for the strategy of taking the nearest neighbor as a interpolation basis is not obvious and requires a better practical understanding of the interpolation algorithms (see <ref> [10] </ref>). The algorithm consists of two major steps. First, correspondence vector fields are estimated, which capture the geometrical relations between the novel image and the examples in the best possible way. <p> So far, the same coefficients are used for interpolating an approximated geometric relation (correspondence vector fields) between the examples and the novel image as well as for the pixel-wise interpolation of the intensity (texture) information (see Sections 4.1. and 4.2 in <ref> [10] </ref>). These coefficients are estimated to yield the "best" possible approximation, e.g., in the least square sense, for the correspondence vector field of the novel image with respect to the example (s).
Reference: [11] <author> I. N. Bronstein and K. A. Semendjajew. </author> <title> Taschen-buch der Mathematik. </title> <publisher> Verlag Harri Deutsch, </publisher> <year> 1980. </year>
Reference-contexts: To obtain a pose compensated face, the new image I is warped according to the pose parameters. However, here we use bicubic interpolation (e.g., Lagrangian interpolation <ref> [11, 5] </ref> or bicubic splines) for the warping, since we do not want the high frequency components of the original image to be sup pressed and to reduce aliasing effects. 4.1.5 Experimental results Using real image data, we will now present some typical experimental results to demonstrate the robustness and versatility
Reference: [12] <author> V. Bruce and M. Burton. </author> <title> Processing Images of Faces. </title> <publisher> ABLEX Publishing-Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1992. </year>
Reference-contexts: Also, 3-D motion parameters have to be computed precisely from image data. 2.2 Work on face images and recognition A good survey of the state of the art on image processing of faces is given in <ref> [12] </ref>. However, most of the work with faces in computer vision was done on processing for recognition [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] and some work on different kinds of classification tasks [21, 22, 14].
Reference: [13] <author> R. Brunelli and T. Poggio. </author> <title> Face recognition: Features versus templates. </title> <type> Technical Report TR 9110-04, </type> <institution> Istituto per la Ricerca Scientifica e Tecnologica, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio <ref> [13, 15] </ref>. Several other approaches to face recognition have also been presented (for example [6, 61, 36, 4]). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. <p> This indicates that quite small templates can be used, thus making correlation feasible at very low computational cost. Gilbert & Yang presented a real time face recognition system using custom VLSI hardware [28]. Their system is based on the template-matching recognition scheme outlined by Brunelli & Poggio <ref> [13, 15] </ref>. In most of the work with faces the images are normalized so that the faces have the same position, size and orientation after manually locating the eyes and mouth. <p> If E and I are identical we have complete positive correlation C N = 1:0. If C N 0:0, then the images are uncorrelated. The use of this standard technique is suggested by the good results reported for face recognition by other researchers (cf. <ref> [6, 13, 28] </ref>). Our implementation performs normalized linear cross-correlation within a multiresolution hierarchy. Correlation is computed between corresponding levels of Lapla-cian pyramids for the new images and the examples for the subregion. Computation starts at a high pyramid level at low resolution. <p> Instead of using different frequency bands within a Laplacian pyramid we also tried correlation using gradient-magnitude images within a Gaussian pyramid of the images. This kind of preprocessing before correlation has been reported to yield superior recognition results <ref> [13] </ref>. There was no difference in the chosen nearest neighbors.
Reference: [14] <author> R. Brunelli and T. Poggio. </author> <title> Caricatural effects in automated face perception. </title> <journal> Kybernetik / Biol. Cybernetics, </journal> <volume> 69 </volume> <pages> 235-241, </pages> <year> 1993. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] and some work on different kinds of classification tasks <ref> [21, 22, 14] </ref>. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [15] <author> R. Brunelli and T. Poggio. </author> <title> Face recognition: Features versus templates. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> 1993. accepted for publication. </note>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio <ref> [13, 15] </ref>. Several other approaches to face recognition have also been presented (for example [6, 61, 36, 4]). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. <p> This indicates that quite small templates can be used, thus making correlation feasible at very low computational cost. Gilbert & Yang presented a real time face recognition system using custom VLSI hardware [28]. Their system is based on the template-matching recognition scheme outlined by Brunelli & Poggio <ref> [13, 15] </ref>. In most of the work with faces the images are normalized so that the faces have the same position, size and orientation after manually locating the eyes and mouth. <p> More robustness could be achieved by a preceding segmentation step. Suitable techniques for segmentation of the face from the static background are readily available, such as color segmentation [2, 4] and integral projection of directional information <ref> [15] </ref>. These techniques perform grey-level based static segmentation of single images, whereas motion segmentation in front of the static background exploits relative motion due to the unavoidable small jitter or movements of a person's head (a robust motion segmentation algorithm is described in [60, 58], for instance).
Reference: [16] <author> P. J. Burt. </author> <title> Fast filter transforms for image processing. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 16 </volume> <pages> 20-51, </pages> <year> 1981. </year>
Reference-contexts: For each image we compute a multiresolution pyra-mid, where I (x; y; t) denotes the discrete grey-level image. The correspondence algorithm can be applied using either Gaussian or Laplacian pyramids. We compute these pyramids, adopting the algorithms proposed by Burt <ref> [16] </ref> and Burt & Adelson [17]. A multiresolution pyramid is a stack of N levels of progressively smaller versions of the original image. Let l denote the level within the pyramid and let G l be the reduced image at the l-th level.
Reference: [17] <author> P. J. Burt and E. H. Adelson. </author> <title> The Laplacian pyramid as a compact image code. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 31(4) </volume> <pages> 532-540, </pages> <month> Apr. </month> <year> 1983. </year>
Reference-contexts: For each image we compute a multiresolution pyra-mid, where I (x; y; t) denotes the discrete grey-level image. The correspondence algorithm can be applied using either Gaussian or Laplacian pyramids. We compute these pyramids, adopting the algorithms proposed by Burt [16] and Burt & Adelson <ref> [17] </ref>. A multiresolution pyramid is a stack of N levels of progressively smaller versions of the original image. Let l denote the level within the pyramid and let G l be the reduced image at the l-th level.
Reference: [18] <author> P. J. Burt and E. H. Adelson. </author> <title> A multiresolution spline with application to image mosaics. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 2(4) </volume> <pages> 217-236, </pages> <month> Oct. </month> <year> 1983. </year>
Reference-contexts: These conflicting requirements cannot be fulfilled simultaneously in general, i.e., for images covering a wide range of spatial frequencies. A suitable transition width can be found only if the spatial frequency band of the images is relatively narrow. To overcome this problem Burt & Adelson <ref> [18, 19] </ref> proposed a multiresolution approach for merging images 5 . First, each source image is decomposed into a set of bandpass filtered component images. In the next step, the component images are merged separately for each band to form mosaic images by weighted averaging within a transition zone. <p> Finally, these bandpass mosaic images are simply summed to obtain the desired composite image. Thus, the transition zone always matches the size of the image features. This technique has been formulated for pairs of static source images and demonstrated to yield superior results over simpler techniques in several applications <ref> [18, 19] </ref>. We adopt this idea and give a more general formulation that applies to any finite number of source images and to time sequences of images. The subregions of face images will be called patches of interest (POI).
Reference: [19] <author> P. J. Burt and E. H. Adelson. </author> <title> Merging images through pattern decomposition. </title> <booktitle> In Applications of Digital Image Processing VIII, </booktitle> <volume> volume 575, </volume> <pages> pages 173-181. </pages> <booktitle> SPIE|The International Society for Optical Engineering, </booktitle> <year> 1985. </year>
Reference-contexts: These conflicting requirements cannot be fulfilled simultaneously in general, i.e., for images covering a wide range of spatial frequencies. A suitable transition width can be found only if the spatial frequency band of the images is relatively narrow. To overcome this problem Burt & Adelson <ref> [18, 19] </ref> proposed a multiresolution approach for merging images 5 . First, each source image is decomposed into a set of bandpass filtered component images. In the next step, the component images are merged separately for each band to form mosaic images by weighted averaging within a transition zone. <p> Finally, these bandpass mosaic images are simply summed to obtain the desired composite image. Thus, the transition zone always matches the size of the image features. This technique has been formulated for pairs of static source images and demonstrated to yield superior results over simpler techniques in several applications <ref> [18, 19] </ref>. We adopt this idea and give a more general formulation that applies to any finite number of source images and to time sequences of images. The subregions of face images will be called patches of interest (POI).
Reference: [20] <author> C. S. Choi, H. Harashima, and T. Takebe. </author> <title> Analysis and synthesis of facial expressions in knowledge-based coding of facial image sequences. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 2737-2740, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: With this kind of coding, very low bit rates can by realized since basically only the analyzed model parameters are transmitted. The general approach of such model-based coding techniques | being still an active research topic (cf. <ref> [1, 20, 44, 24, 25] </ref>, to give some examples) | for faces is to use a volumetric 3-D facial model (such as polygonal wire frame model). Full face images under standard view can be projected onto the wire frame model for reconstruction by using texture mapping techniques.
Reference: [21] <author> G. Cottrell and M. Fleming. </author> <title> Face recognition using unsupervised feature extraction. </title> <booktitle> In Proc. Intl. Neural Network Conf. (INNC), </booktitle> <address> Paris, </address> <year> 1990. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] and some work on different kinds of classification tasks <ref> [21, 22, 14] </ref>. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [22] <author> G. W. Cottrell and J. Metcalfe. EMPATH: </author> <title> Face, emotion, and gender recognition using holons. </title> <booktitle> In Proc. NIPS 3, </booktitle> <year> 1991. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] and some work on different kinds of classification tasks <ref> [21, 22, 14] </ref>. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [23] <author> J. L. Crowley and R. M. Stern. </author> <title> Fast computation of the difference of low-pass transform. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6(2) </volume> <pages> 212-222, </pages> <month> Mar. </month> <year> 1984. </year> <month> 17 </month>
Reference-contexts: An efficient algorithm to compute a pyramid of bandpass filtered images having a ratio of p 2:0 is the DOLP transform (difference of lowpass transform) proposed by Crowley & Stern <ref> [23] </ref>. However, here we construct a Laplacian pyramid from the difference of images at adjacent levels of the Gaussian pyramid as proposed by Burt & Adelson. Therefore, the ratio of the standard deviations is 2.0. This results in a broader filter bandwidth.
Reference: [24] <author> N. D. Duffy. </author> <title> Animation using image samples. </title> <editor> In V. Bruce and M. Burton, editors, </editor> <booktitle> Processing Images of Faces, </booktitle> <pages> pages 179-201. </pages> <publisher> ABLEX Publishing-Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1992. </year>
Reference-contexts: With this kind of coding, very low bit rates can by realized since basically only the analyzed model parameters are transmitted. The general approach of such model-based coding techniques | being still an active research topic (cf. <ref> [1, 20, 44, 24, 25] </ref>, to give some examples) | for faces is to use a volumetric 3-D facial model (such as polygonal wire frame model). Full face images under standard view can be projected onto the wire frame model for reconstruction by using texture mapping techniques. <p> Interestingly, the number of subimages (pasted into a base image of a face) needed to achieve realistic animation is often surprisingly small. This fact has also been pointed out in the literature (see <ref> [24] </ref> for example). A realistic animation of an eye blink can be achieved by using only four distinct subimages if the eyeball is fixed. Duffy noted that only five different images are required for satisfactory simulation of natural eye movements. <p> Quite realistic animation of facial details such as eye movements or speech can be achieved by blending sequences of eye and mouth subimages into a base image at appropriate positions prior to mapping (see for instance <ref> [24] </ref>). Both base and subimage are static frontal views of the face. As observed by Duffy [24], simply pasting a subim-age, e.g., a rectangular region of the mouth, yields fairly unsatisfactory results due to visible discontinuities at the edges of the pasted area. <p> animation of facial details such as eye movements or speech can be achieved by blending sequences of eye and mouth subimages into a base image at appropriate positions prior to mapping (see for instance <ref> [24] </ref>). Both base and subimage are static frontal views of the face. As observed by Duffy [24], simply pasting a subim-age, e.g., a rectangular region of the mouth, yields fairly unsatisfactory results due to visible discontinuities at the edges of the pasted area.
Reference: [25] <author> I. A. Essa. </author> <title> Visual interpretation of facial expressions using dynamic modeling. </title> <type> Technical Report TR 235, </type> <institution> Perceptual Computing Group, Media Laboratory, MIT, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: With this kind of coding, very low bit rates can by realized since basically only the analyzed model parameters are transmitted. The general approach of such model-based coding techniques | being still an active research topic (cf. <ref> [1, 20, 44, 24, 25] </ref>, to give some examples) | for faces is to use a volumetric 3-D facial model (such as polygonal wire frame model). Full face images under standard view can be projected onto the wire frame model for reconstruction by using texture mapping techniques. <p> Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. Only recently the atten-tion of researchers has shifted to the temporal aspect of facial expressions by using optical flow in sequences of face images <ref> [41, 42, 25, 65] </ref>. We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work.
Reference: [26] <author> C. L. Fennema and W. B. Thompson. </author> <title> Velocity determination in scenes containing several moving objects. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 9 </volume> <pages> 301-315, </pages> <year> 1979. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques.
Reference: [27] <author> W. T. Freeman and E. H. Adelson. </author> <title> The design and use of steerable filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(9) </volume> <pages> 891-906, </pages> <year> 1991. </year>
Reference-contexts: It is desirable, however, to locate boundaries within regions of low surface texture and not at conspicuous facial features | loosely speaking, we apply a reversed edge detector. For this purpose orientation selective filters (like Gabor filters, wavelets, or steerable filters <ref> [27] </ref>) may be the way to go. They make it possible to seek for appropriate locations depending on the orientations of boundary lines that are approximately given. An important step is the automatic acquisition of the example database. Here at least two distinct tasks have to be distinguished.
Reference: [28] <author> J. M. Gilbert and W. Yang. </author> <title> A real-time face recognition system using custom VLSI hardware. </title> <booktitle> In Proc. of Computer Architectures for Machine Perception Workshop, </booktitle> <month> Dec. </month> <year> 1993. </year> <note> accepted for publication. </note>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> This indicates that quite small templates can be used, thus making correlation feasible at very low computational cost. Gilbert & Yang presented a real time face recognition system using custom VLSI hardware <ref> [28] </ref>. Their system is based on the template-matching recognition scheme outlined by Brunelli & Poggio [13, 15]. In most of the work with faces the images are normalized so that the faces have the same position, size and orientation after manually locating the eyes and mouth. <p> Moreover, during the recognition phase, it is often possible to repeatedly take snapshot images until one comes close enough to the standard conditions (as mentioned in <ref> [28] </ref> for instance). A different paradigm applies for applications where recognition is used for validation or verification only. For instance, in an access control system, there may be prior information about the person's identity available, e.g., by means of a personalized key or code-card. <p> If E and I are identical we have complete positive correlation C N = 1:0. If C N 0:0, then the images are uncorrelated. The use of this standard technique is suggested by the good results reported for face recognition by other researchers (cf. <ref> [6, 13, 28] </ref>). Our implementation performs normalized linear cross-correlation within a multiresolution hierarchy. Correlation is computed between corresponding levels of Lapla-cian pyramids for the new images and the examples for the subregion. Computation starts at a high pyramid level at low resolution.
Reference: [29] <author> F. Girosi, A. Verri, and V.Torre. </author> <title> Constraints for the computation of optical flow. </title> <booktitle> In Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 116-124, </pages> <address> Irvine, CA, </address> <month> Mar. </month> <year> 1989. </year> <booktitle> IEEE Computer Society Order Number 1903. </booktitle>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques. <p> The matrix H is the Hessian of image intensity I (x; t). It arises from an optical flow tech nique using second order constraints to recover the 2-D-velocity locally (see also <ref> [29] </ref>). Based on this approach Toelg developed a refined and robust algorithm that is used in an active vi sion system [60, 59].
Reference: [30] <author> R. L. </author> <title> Height. Lip reader trainer: Computer program for the hearing impaired. </title> <booktitle> In Proc. Johns Hop-kins first national search for applications of personal computing to aid the handicapped, </booktitle> <pages> pages 4-5, </pages> <address> Los Alamitos, CA, 1981. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Moreover, in experiments on lip-reading it has been demonstrated that the essence of a conversation can be picked up from a sequence of images alone. Remarkable, however, is that a set of only 19 visually distinguishable images (showing particular arrangements of lip, tongue, and teeth) is sufficient <ref> [30] </ref>. Although for realistic video-conferencing a larger number of images may be required, this result is very encouraging. The advantage of having distinct example patches for different regions of a face is manifold.
Reference: [31] <author> B. Horn and B. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques.
Reference: [32] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1986. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques.
Reference: [33] <author> T. Kanade. </author> <title> Picture Processing System by Computer Complex and Recognition of Human Faces. </title> <type> Unpublished Ph.D. thesis, </type> <institution> Dept. of Information Science, Kyoto Univ., </institution> <year> 1973. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio [13, 15]. Several other approaches to face recognition have also been presented (for example [6, 61, 36, 4]). The first approach is influenced by the work of Kanade <ref> [33] </ref> and uses a vector of geometrical features for recognition. First the eyes are located by computing the normalized cross-correlation coefficient with a single eye template at different resolution scales. The image is then normalized in scale and translation.
Reference: [34] <author> J. K. Kearney, W. B. Thompson, and D. L. Bo-ley. </author> <title> Optical flow estimation: An error analysis of gradient-based methods with local optimization. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(2) </volume> <pages> 229-244, </pages> <month> Mar. </month> <year> 1987. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques.
Reference: [35] <author> G. A. Korn and T. M. Korn. </author> <title> Mathematical Handbook for Scientists and Engineers. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1968. </year>
Reference-contexts: Secondly, D is positive semi-definite (the quadratic form xDx 0 8 x 2 lR 2 and x 6= 0) as can be verified by Sylvester's criterion <ref> [35] </ref>. Consequently the eigenvalues are nonnegative ( 1 ; 2 0). The eigenvalues are computed as the roots of the characteristic quadratic polynomial in our implementation.
Reference: [36] <author> M. Lades, J. C. Vorbruggen, J. Buhmann, J. Lange, C. v. d. Malsburg, R. P. Wurtz, and W. Konen. </author> <title> Distortion invariant object recognition in the dynamic link architecture. </title> <journal> IEEE Transactions on Computers, </journal> <note> 1992. accepted for publication. </note>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio [13, 15]. Several other approaches to face recognition have also been presented (for example <ref> [6, 61, 36, 4] </ref>). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. First the eyes are located by computing the normalized cross-correlation coefficient with a single eye template at different resolution scales.
Reference: [37] <author> B. D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 121-130, </pages> <year> 1981. </year>
Reference-contexts: Even though the discussion here uses the terminology tailored to our video-conference system, the results and algorithms can be generalized to other problems. In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo <ref> [37, 38] </ref>. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation [26, 31, 43, 32, 34, 29, 55]. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7].
Reference: [38] <author> B. D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proc. International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 674-679, </pages> <address> Vancouver, </address> <year> 1981. </year>
Reference-contexts: Even though the discussion here uses the terminology tailored to our video-conference system, the results and algorithms can be generalized to other problems. In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo <ref> [37, 38] </ref>. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation [26, 31, 43, 32, 34, 29, 55]. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7].
Reference: [39] <author> B. S. Manjunath, R. Chellappa, and C. v. d. Mals-burg. </author> <title> A feature based approach to face recognition. </title> <institution> Technical Report CAR-TR-604 and CS-TR-2834, Computer Vision Laboratory, Center for Automation Research , Univ. of Maryland, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images.
Reference: [40] <author> D. Marr and E. Hildreth. </author> <title> Theory of edge detection. </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 207 </volume> <pages> 187-217, </pages> <year> 1980. </year>
Reference-contexts: A Laplacian pyramid may be regarded as a stack of bandpass filtered image "incarnations". The name arises from the fact that the Laplacian edge detector 2 commonly used in image enhancement can be approximated by the difference of Gaussians <ref> [40] </ref>. The optimal ratio | the one that leads to the best approximation | of standard deviations for inhibiting and excitatory Gaussian about is 1.6.
Reference: [41] <author> K. Mase. </author> <title> Recognition of facial expression from optical flow. </title> <journal> IEICE Transactions, </journal> <volume> E 74(10) </volume> <pages> 3474-3483, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. Only recently the atten-tion of researchers has shifted to the temporal aspect of facial expressions by using optical flow in sequences of face images <ref> [41, 42, 25, 65] </ref>. We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work.
Reference: [42] <author> K. Mase and A. Pentland. </author> <title> Automatic lipreading by optical flow analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6) </volume> <pages> 67-76, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. Only recently the atten-tion of researchers has shifted to the temporal aspect of facial expressions by using optical flow in sequences of face images <ref> [41, 42, 25, 65] </ref>. We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work.
Reference: [43] <author> H.-H. Nagel. </author> <title> Displacement vectors derived from second-order intensity variations in image sequences. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 21 </volume> <pages> 85-117, </pages> <year> 1983. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques.
Reference: [44] <author> Y. Nakaya, Y. C. Chuah, and H. Harashima. </author> <title> Model-based/waveform hybrid coding for videotelephone images. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 2741-2744, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: With this kind of coding, very low bit rates can by realized since basically only the analyzed model parameters are transmitted. The general approach of such model-based coding techniques | being still an active research topic (cf. <ref> [1, 20, 44, 24, 25] </ref>, to give some examples) | for faces is to use a volumetric 3-D facial model (such as polygonal wire frame model). Full face images under standard view can be projected onto the wire frame model for reconstruction by using texture mapping techniques. <p> Most of them rely on an explicit metric 3-D model (wire frame model) of the specific face <ref> [1, 44] </ref>. These volumetric models are obtained from image data under different viewpoints or from laser range-scanners. Shape-from-motion algorithms are known to be not very stable and quite noise sensitive. Recently, several structure-from-motion algorithms have been demonstrated to yield good results from real image sequences.
Reference: [45] <author> T. Poggio. </author> <title> A theory of how the brain might work. </title> <booktitle> In Cold Spring Harbor Symposia on Quantitative Biology, volume LV, </booktitle> <pages> pages 899-910. </pages> <note> Cold Spring Harbor Laboratory Press, </note> <year> 1990. </year>
Reference-contexts: As suggested for instance by Poggio & Brunelli ([46]) object images can be generated by interpolating between a small set of example images. They described this interpolation in terms of learning from examples <ref> [45, 47, 48] </ref>. To accomplish the interpolation for image reconstruction, two different approaches are conceivable. The first is related to a new approach to computer graphics [46]. This method has been proposed to synthesize new images from examples attributed with specific and given parameters. <p> Instead of using the nearest neighbors only, it is natural to "interpolate" novel views between examples from the database as already mentioned in Section 3.1. Extending previous results <ref> [45, 47, 48, 46] </ref>, recent work of Beymer, Shashua & Poggio [10] presents the mathematical formulation and experimental demonstrations of several versions of such an approach. The feasibility of interpolation between images has been successfully demonstrated for the multidimensional interpolation of novel human face images.
Reference: [46] <author> T. Poggio and R. Brunelli. </author> <title> A novel approach to graphics. </title> <note> Technical Report AI Memo 1354 and CBIP Paper 71, </note> <institution> Artificial Intelligence Laboratory, MIT and Center for Biological Information Processing, Whitaker College, </institution> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: They described this interpolation in terms of learning from examples [45, 47, 48]. To accomplish the interpolation for image reconstruction, two different approaches are conceivable. The first is related to a new approach to computer graphics <ref> [46] </ref>. This method has been proposed to synthesize new images from examples attributed with specific and given parameters. For instance, the image sequence of a walking person can be interpolated over time from a few images showing distinct postures; here the parameter is simply time (cf. [46]). <p> approach to computer graphics <ref> [46] </ref>. This method has been proposed to synthesize new images from examples attributed with specific and given parameters. For instance, the image sequence of a walking person can be interpolated over time from a few images showing distinct postures; here the parameter is simply time (cf. [46]). In computer graphics we can interactively choose the right examples and tailor the parameterization to synthesize images that are close enough to what we want by interpolation in a relatively low-dimensional parameter space. Some applications for spe 3 cial animation effects in movies can also be found in [64]. <p> Instead of using the nearest neighbors only, it is natural to "interpolate" novel views between examples from the database as already mentioned in Section 3.1. Extending previous results <ref> [45, 47, 48, 46] </ref>, recent work of Beymer, Shashua & Poggio [10] presents the mathematical formulation and experimental demonstrations of several versions of such an approach. The feasibility of interpolation between images has been successfully demonstrated for the multidimensional interpolation of novel human face images.
Reference: [47] <author> T. Poggio and S. Edelman. </author> <title> A network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343 </volume> <pages> 263-266, </pages> <year> 1990. </year>
Reference-contexts: As suggested for instance by Poggio & Brunelli ([46]) object images can be generated by interpolating between a small set of example images. They described this interpolation in terms of learning from examples <ref> [45, 47, 48] </ref>. To accomplish the interpolation for image reconstruction, two different approaches are conceivable. The first is related to a new approach to computer graphics [46]. This method has been proposed to synthesize new images from examples attributed with specific and given parameters. <p> Instead of using the nearest neighbors only, it is natural to "interpolate" novel views between examples from the database as already mentioned in Section 3.1. Extending previous results <ref> [45, 47, 48, 46] </ref>, recent work of Beymer, Shashua & Poggio [10] presents the mathematical formulation and experimental demonstrations of several versions of such an approach. The feasibility of interpolation between images has been successfully demonstrated for the multidimensional interpolation of novel human face images.
Reference: [48] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: As suggested for instance by Poggio & Brunelli ([46]) object images can be generated by interpolating between a small set of example images. They described this interpolation in terms of learning from examples <ref> [45, 47, 48] </ref>. To accomplish the interpolation for image reconstruction, two different approaches are conceivable. The first is related to a new approach to computer graphics [46]. This method has been proposed to synthesize new images from examples attributed with specific and given parameters. <p> Instead of using the nearest neighbors only, it is natural to "interpolate" novel views between examples from the database as already mentioned in Section 3.1. Extending previous results <ref> [45, 47, 48, 46] </ref>, recent work of Beymer, Shashua & Poggio [10] presents the mathematical formulation and experimental demonstrations of several versions of such an approach. The feasibility of interpolation between images has been successfully demonstrated for the multidimensional interpolation of novel human face images.
Reference: [49] <author> W. H. Press, S. A. Teulolsky, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge Univ. Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: However, in extensive experiments the magnitude of the determinant det (H), 3 The condition number is defined as the ratio between the largest and the smallest absolute eigenvalue of a matrix (cf. <ref> [49] </ref>). A matrix is ill-conditioned if its condition number is too large, and it is singular if is infinite. 8 i.e., the spatial Gaussian curvature in the intensity image, turned out to be a better confidence measure. This finding is in accordance with the more recent results discussed in [7].
Reference: [50] <editor> A. Rosenfeld, editor. </editor> <title> Multiresolution Image Processing and Analysis, </title> <booktitle> volume 12 of Information Sciences. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: A comprehensive overview of multiresolution image processing and pyramid structures is given by Rosenfeld <ref> [50] </ref>. For each image we compute a multiresolution pyra-mid, where I (x; y; t) denotes the discrete grey-level image. The correspondence algorithm can be applied using either Gaussian or Laplacian pyramids. We compute these pyramids, adopting the algorithms proposed by Burt [16] and Burt & Adelson [17].
Reference: [51] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <type> Technical Report AI Memo 1452, </type> <institution> Artificial Intelligence Laboratory, MIT, </institution> <month> Jan. </month> <year> 1994. </year> <note> Submitted to PAMI, </note> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The second achievement is a generalization of the "linear combination of views" result obtained by Ullman & Basri [62] that relates three orthographic views of a 3-D object (ignoring self-occlusion). Recently, Shashua <ref> [52, 51] </ref> proved that the image coordinates of corresponding points over any three perspective views (uncalibrated pinhole camera) of a 3-D object are related by a pair of trilinear equations. The 17 independent coefficients of this trilinear form can be recovered linearly from 9 corresponding points over all three views. <p> The direct approach of using the trilinear result to generate new views has several theoretical advantages over classical structure from motion methods as well as over methods to recover non-metric structure (see <ref> [51] </ref> for a detailed discussion). Moreover, processes that are known to be unstable in the presence of noise, such as recovering the epipolar geometry, are avoided. The tri-linear algorithm proved to be significantly more stable in the presence of errors in the image measurements. <p> So far, the trilinear algorithm has been evaluated only in computer simulations and using re-projection of isolated points in real imagery, though an implementation to transform dense images is planned for the near future 7 . Although <ref> [52, 51] </ref> emphasis is given to the task of recognition of 3-D objects, the trilinear method may have interesting applications in an example-based video-conference system.
Reference: [52] <author> A. Shashua. </author> <title> Trilinearity in visual recognition by alignment. </title> <booktitle> In Proc. 3rd European Conf. on Computer Vision, </booktitle> <pages> pages 479-484, </pages> <address> Stockholm, Sweden, May 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The second achievement is a generalization of the "linear combination of views" result obtained by Ullman & Basri [62] that relates three orthographic views of a 3-D object (ignoring self-occlusion). Recently, Shashua <ref> [52, 51] </ref> proved that the image coordinates of corresponding points over any three perspective views (uncalibrated pinhole camera) of a 3-D object are related by a pair of trilinear equations. The 17 independent coefficients of this trilinear form can be recovered linearly from 9 corresponding points over all three views. <p> So far, the trilinear algorithm has been evaluated only in computer simulations and using re-projection of isolated points in real imagery, though an implementation to transform dense images is planned for the near future 7 . Although <ref> [52, 51] </ref> emphasis is given to the task of recognition of 3-D objects, the trilinear method may have interesting applications in an example-based video-conference system.
Reference: [53] <author> A. Shashua and S. Toelg. </author> <title> The quadric reference surface: Applications in registering views of complex 18 3D objects. </title> <booktitle> In Proc. 3rd European Conf. on Com--puter Vision, </booktitle> <pages> pages 407-416, </pages> <address> Stockholm, Sweden, </address> <month> May </month> <year> 1994. </year> <note> Springer-Verlag. Also CAR-TR-702 and CS-TR-3220,Computer Vision Laboratory, </note> <institution> Center for Automation Research , Univ. of Maryland, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: This suggests the use of a generic object model such as an average or prototypical face. Some appropriate model assumptions at this level are: a rough model of the 3-D shape of a face/head (e.g., a quadric surface, see <ref> [53, 54] </ref> for details); human heads exhibit approximately bilateral symmetry; the set of constituents of a face (two eyes, nose, mouth, two ears, etc.); stable features of constituents (the pupil is round and darker than the white eye bulbus, teeth are white, the holes in the nose are dark, the relative <p> Recently, an algorithm has been presented that applies the model of a general quadric surface to map two images of faces taken under perspective projection onto each other <ref> [53, 54] </ref>. Using a projective framework, a constructive proof is given that in general nine corresponding reference points and the epipoles in two views of an approximately quadric surface determine the correspondences for all other image points of that surface. Encouraging results have been achieved for real face images. <p> However, for small variations the linear compensation will give reasonable results at a very low computational cost. Two recent achievements should be mentioned in the context of generating new views from a small number of model views or example images. Shashua & Toelg <ref> [53] </ref> showed that a nominal quadric transformation for all image points, i.e., a transformation assuming that an object surface can be approximated by a quadric, can be successfully applied to register two face images. <p> All parameters of the transformation (the quadric and the relative camera geometry) can be recovered from only nine corresponding points over two views <ref> [53, 54] </ref>. Alternatively, the transformation can be recovered using only four corresponding points and a given conic in one view (encompassing the face in our application) [54]. This algorithm is relevant here for two purposes. <p> The nominal quadric transformation is significantly more general than simpler transformations commonly used (e.g., affine, or transformation due to a plane) and superior registration results have been obtained with face images (see <ref> [53, 54] </ref> for examples). The second achievement is a generalization of the "linear combination of views" result obtained by Ullman & Basri [62] that relates three orthographic views of a 3-D object (ignoring self-occlusion).
Reference: [54] <author> A. Shashua and S. Toelg. </author> <title> The quadric reference surface: </title> <journal> Theory and applications. </journal> <note> Technical Report AI Memo 1448 and CBCL Paper 85, </note> <institution> Artificial Intelligence Laboratory, MIT and Center for Biological and Computational Learning, Whitaker College, </institution> <month> June </month> <year> 1994. </year> <note> Also submitted for publication to International Journal of Computer Vision. </note>
Reference-contexts: This suggests the use of a generic object model such as an average or prototypical face. Some appropriate model assumptions at this level are: a rough model of the 3-D shape of a face/head (e.g., a quadric surface, see <ref> [53, 54] </ref> for details); human heads exhibit approximately bilateral symmetry; the set of constituents of a face (two eyes, nose, mouth, two ears, etc.); stable features of constituents (the pupil is round and darker than the white eye bulbus, teeth are white, the holes in the nose are dark, the relative <p> Recently, an algorithm has been presented that applies the model of a general quadric surface to map two images of faces taken under perspective projection onto each other <ref> [53, 54] </ref>. Using a projective framework, a constructive proof is given that in general nine corresponding reference points and the epipoles in two views of an approximately quadric surface determine the correspondences for all other image points of that surface. Encouraging results have been achieved for real face images. <p> All parameters of the transformation (the quadric and the relative camera geometry) can be recovered from only nine corresponding points over two views <ref> [53, 54] </ref>. Alternatively, the transformation can be recovered using only four corresponding points and a given conic in one view (encompassing the face in our application) [54]. This algorithm is relevant here for two purposes. <p> All parameters of the transformation (the quadric and the relative camera geometry) can be recovered from only nine corresponding points over two views [53, 54]. Alternatively, the transformation can be recovered using only four corresponding points and a given conic in one view (encompassing the face in our application) <ref> [54] </ref>. This algorithm is relevant here for two purposes. Firstly, it can be used as a preprocessing step to facilitate pixelwise correspondence, i.e., bringing two views into closer alignment. <p> The nominal quadric transformation is significantly more general than simpler transformations commonly used (e.g., affine, or transformation due to a plane) and superior registration results have been obtained with face images (see <ref> [53, 54] </ref> for examples). The second achievement is a generalization of the "linear combination of views" result obtained by Ullman & Basri [62] that relates three orthographic views of a 3-D object (ignoring self-occlusion).
Reference: [55] <author> E. P. Simoncelli, E. H. Adelson, and D. J. Heeger. </author> <title> Probability distributions of optical flow. </title> <booktitle> In IEEE Proc. of CVPR, </booktitle> <pages> pages 310-315, </pages> <address> Maui, Hawaii, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In our derivation we follow the lines of Lucas & Kanade who first proposed a differential algorithm for image registration and stereo [37, 38]. However, more recently related techniques have been presented in various flavors in the context of motion estimation or optical flow computation <ref> [26, 31, 43, 32, 34, 29, 55] </ref>. A comprehensive survey and comparison of differential and other optical flow techniques is given by Barron, Fleet & Beau-chemin [7]. Unfortunately, they do not consider coarse-to-fine methods that are essential to extend the velocity range of differential techniques. <p> The confidence measure k (x) associated with the displacement vector d (x) can be derived from the entries in the matrix D (x). Several ways to do this have been proposed in the literature: 1. Simoncelli, Adelson & Heeger presented a Bayesian framework for optical flow computation <ref> [55] </ref>. They emphasized the relevance of the trace of the spatial derivative matrix for the probability distributions of velocity vectors. Here, we have for the trace of D: = W 2 (x)I 2 X y (x) 2.
Reference: [56] <author> S. A. Sirohey. </author> <title> Human face segmentation and identification. </title> <institution> Technical Report CAR-TR-695 and CS-TR-3176, Computer Vision Laboratory, Center for Automation Research , Univ. of Maryland, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The most important components are briefly discussed in the sequel. The separation of the face and the uncovered background can be achieved by the methods sketched at the end of Section 4.2. Recently an algorithm for human face segmentation by fitting an ellipse to the head has been described <ref> [56] </ref>. This algorithm is robust enough to deal with images having moderately cluttered backgrounds. Another, more critical problem is the automatic selection and positioning of the POIs in the face image. This task is significantly simplified by the robust pose normalization presented in this paper.
Reference: [57] <author> S. Tanimoto and T. Pavlidis. </author> <title> A hierarchical data structure for picture processing. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 4(2) </volume> <pages> 104-119, </pages> <year> 1975. </year>
Reference-contexts: and affine motion transformation at higher resolutions can used to assess the similarity between both images (see Section 4.3.2). 4.1.1 Analysis in spatial frequency bands All computation is performed in a hierarchical data structure of the kind originally proposed by Tanimoto & Pavlidis to speed up various image processing operations <ref> [57] </ref>. A comprehensive overview of multiresolution image processing and pyramid structures is given by Rosenfeld [50]. For each image we compute a multiresolution pyra-mid, where I (x; y; t) denotes the discrete grey-level image. The correspondence algorithm can be applied using either Gaussian or Laplacian pyramids.
Reference: [58] <author> S. Toelg. </author> <title> Video conferencing: An application of learning. </title> <booktitle> In Proceedings of the CBCL Learning Day, </booktitle> <publisher> Endicott House, MIT, </publisher> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Moreover, the algorithm works well with strong expressions and deformations in both images. More examples with a variety of different face images and also different backgrounds are presented in <ref> [58] </ref>. 4.2 Discussion of pose estimation The pose estimation and normalization algorithm described in this section can be located at an intermediate level of complexity among models describing global motions of a face in images. <p> These techniques perform grey-level based static segmentation of single images, whereas motion segmentation in front of the static background exploits relative motion due to the unavoidable small jitter or movements of a person's head (a robust motion segmentation algorithm is described in <ref> [60, 58] </ref>, for instance). Combinations of these techniques should be considered in order to achieve greater generality. 4.3 Finding the nearest neighbors In order to find the nearest neighbor (s) in an example database for the subimages extracted from incoming face images, several approaches are conceivable.
Reference: [59] <author> S. Tolg. </author> <title> Gaze control for an active camera system by modeling human pursuit eye movements. </title> <editor> In D. P. Casasent, editor, </editor> <booktitle> Intelligent Robots and Computer Vision XI: Algorithms, Techniques, and Active Vision, </booktitle> <volume> volume 1825, </volume> <pages> pages 585-598, </pages> <address> Boston, MA, </address> <month> Nov. </month> <year> 1992. </year> <booktitle> SPIE|The International Society for Optical Engineering. </booktitle>
Reference-contexts: It arises from an optical flow tech nique using second order constraints to recover the 2-D-velocity locally (see also [29]). Based on this approach Toelg developed a refined and robust algorithm that is used in an active vi sion system <ref> [60, 59] </ref>. However, in extensive experiments the magnitude of the determinant det (H), 3 The condition number is defined as the ratio between the largest and the smallest absolute eigenvalue of a matrix (cf. [49]).
Reference: [60] <editor> S. Tolg. Strukturuntersuchungen zur Informa-tionsverar-beitung in neuronaler Architektur am Beispiel der Modellierung von Augenbewegungen fur aktives Se-hen, </editor> <booktitle> volume 197 of Fortschritt-Berichte VDI, </booktitle> <address> Reihe 10: Informatik/Kommunikationstechnik. VDI Ver-lag, Dusseldorf, </address> <year> 1992. </year> <note> ISBN 3-18-149710-X. </note>
Reference-contexts: It arises from an optical flow tech nique using second order constraints to recover the 2-D-velocity locally (see also [29]). Based on this approach Toelg developed a refined and robust algorithm that is used in an active vi sion system <ref> [60, 59] </ref>. However, in extensive experiments the magnitude of the determinant det (H), 3 The condition number is defined as the ratio between the largest and the smallest absolute eigenvalue of a matrix (cf. [49]). <p> These techniques perform grey-level based static segmentation of single images, whereas motion segmentation in front of the static background exploits relative motion due to the unavoidable small jitter or movements of a person's head (a robust motion segmentation algorithm is described in <ref> [60, 58] </ref>, for instance). Combinations of these techniques should be considered in order to achieve greater generality. 4.3 Finding the nearest neighbors In order to find the nearest neighbor (s) in an example database for the subimages extracted from incoming face images, several approaches are conceivable. <p> An adequate way to accomplish this is to apply a low-pass filter to the binary mask image M i before generating the Gaussian pyramid GM i . Weighting the past few mask images with an exponentially decaying weighting function can be implemented very efficiently in a recursive way (see <ref> [60] </ref> for algorithm). However, application of such a filter may require an additional normalization step of the pixel values in the mosaic images. This is because in general it cannot be guaranteed that the sum of all weights for each image location is equal to unity.
Reference: [61] <author> M. Turk and A. Pentland. </author> <title> Face recognition using eigenfaces. </title> <booktitle> In IEEE Proc. of CVPR, </booktitle> <pages> pages 586-591, </pages> <address> Maui, Hawaii, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: However, most of the work with faces in computer vision was done on processing for recognition <ref> [33, 6, 61, 36, 39, 3, 2, 4, 15, 28, 9] </ref> and some work on different kinds of classification tasks [21, 22, 14]. Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. <p> Rather, we will summarize some ideas that are relevant to our work. Recently, a systematic comparison of typical approaches (feature-based versus template-based techniques) to face recognition was carried out by Brunelli & Poggio [13, 15]. Several other approaches to face recognition have also been presented (for example <ref> [6, 61, 36, 4] </ref>). The first approach is influenced by the work of Kanade [33] and uses a vector of geometrical features for recognition. First the eyes are located by computing the normalized cross-correlation coefficient with a single eye template at different resolution scales.
Reference: [62] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year> <note> Also MIT AI Memo 1051, </note> <year> 1989. </year>
Reference-contexts: The second achievement is a generalization of the "linear combination of views" result obtained by Ullman & Basri <ref> [62] </ref> that relates three orthographic views of a 3-D object (ignoring self-occlusion). Recently, Shashua [52, 51] proved that the image coordinates of corresponding points over any three perspective views (uncalibrated pinhole camera) of a 3-D object are related by a pair of trilinear equations.
Reference: [63] <author> S. Uras, F. Girosi, A. Verri, and V. Torre. </author> <title> A computational approach to motion perception. </title> <journal> Kybernetik / Biol. Cybernetics, </journal> <volume> 60 </volume> <pages> 79-87, </pages> <year> 1988. </year>
Reference-contexts: Here, we have for the trace of D: = W 2 (x)I 2 X y (x) 2. It is obvious from the previous discussion that the larger det (D) is, the more stable is the solution of the linear system (2). 3. Uras et al. <ref> [63] </ref> proposed the smallest condition number 3 (H) of the matrix H as an accuracy criterion. The matrix H is the Hessian of image intensity I (x; t). It arises from an optical flow tech nique using second order constraints to recover the 2-D-velocity locally (see also [29]).
Reference: [64] <author> G. Wolberg. </author> <title> Digital Image Warping. </title> <booktitle> IEEE Computer Society Order Number 1944, </booktitle> <year> 1990. </year>
Reference-contexts: In computer graphics we can interactively choose the right examples and tailor the parameterization to synthesize images that are close enough to what we want by interpolation in a relatively low-dimensional parameter space. Some applications for spe 3 cial animation effects in movies can also be found in <ref> [64] </ref>. The second approach has the same memory-based flavor and is in fact provably almost equivalent. The first step is to find the nearest neighbors, i.e., the most similar examples according to some appropriate distance measure, to the novel view within the database. <p> Inverting the mapping of the image warping (that is computing the mapping from image 2 to image 1 if the mapping from 1 to 2 is given) in not trivial in general <ref> [64] </ref>. However, due to the parametric model applied here, the parameters for warping the normalized pose face image to the original pose can be computed easily from the transmitted pose parameters describing the alignment with the reference image.
Reference: [65] <author> Y. Yacoob and L. S. Davis. </author> <title> Computing spatio-temporal representations of human faces. </title> <booktitle> In IEEE Proc. of CVPR, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: Almost all of this work treats face recognition as a static problem approached by pattern recognition techniques 1 applied to single static images. Only recently the atten-tion of researchers has shifted to the temporal aspect of facial expressions by using optical flow in sequences of face images <ref> [41, 42, 25, 65] </ref>. We do not intend to give a comprehensive overview of face recognition here. Rather, we will summarize some ideas that are relevant to our work.
References-found: 65

