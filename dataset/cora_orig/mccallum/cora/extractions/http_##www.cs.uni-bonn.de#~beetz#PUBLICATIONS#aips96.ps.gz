URL: http://www.cs.uni-bonn.de/~beetz/PUBLICATIONS/aips96.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/~rhino/publications.html
Root-URL: http://cs.uni-bonn.de
Email: beetz@cs.yale.edu, mcdermott@cs.yale.edu  
Phone: Tel.: (203) 432-1229, Fax: (203) 432-0593  
Title: Local Planning of Ongoing Activities  
Author: Michael Beetz and Drew McDermott 
Address: P.O. Box 208285, Yale Station New Haven, CT 06520-8285  
Affiliation: Yale University, Department of Computer Science  
Abstract: An agent that learns about the world while performing its jobs has to plan in a flexible and focused manner: it has to reflect on how to perform its jobs while accomplishing them, focus on critical aspects of important subtasks, and ignore irrelevant aspects of their context. It also has to postpone planning when lacking information, reconsider its course of action when noticing opportunities, risks, or execution failures, and integrate plan revisions smoothly into its ongoing activities. In this paper, we add constructs to rpl (Reactive Plan Language) that allow for local planning of ongoing activities. The additional constructs constitute an interface between rpl and planning processes that is identical to the interface between rpl and continuous control processes like moving or grasping. The uniformity of the two interfaces and the control structures provided by rpl enable a programmer to concisely specify a wide spectrum of interactions between planning and execution. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ambros-Ingerson, J., and Steel, S. </author> <year> 1988. </year> <title> Integrating planning, execution, and monitoring. </title> <booktitle> In Proc. of AAAI-88, </booktitle> <pages> 735-740. </pages>
Reference: <author> Beetz, M., and McDermott, D. </author> <year> 1994. </year> <title> Improving robot plans during their execution. </title> <editor> In Hammond, K., ed., </editor> <booktitle> Proc. of AIPS-94, </booktitle> <pages> 7-12. </pages>
Reference-contexts: Each time the algorithm has found a better plan p 0 it provides p 0 , the best plan it has found so far, as its result. In our experiments we have used planning algorithms that schedule errands (McDermott 1992) and forestall behavior flaws <ref> (Beetz & McDermott 1994) </ref>. Several other planning algorithms have the required functionality (McDermott 1992; Drummond & Bresina 1990; Dean et al. 1993; Lyons & Hendriks 1992). For our implementation of local planning, we have used a simplified version of the xfrm planning algorithm (McDermott 1992; Beetz & McDermott 1994).
Reference: <author> Beetz, M. </author> <year> 1996. </year> <title> Anticipating and Forestalling Execution Failures in Structured Reactive Plans. </title> <type> Technical report, </type> <institution> Yale University. forthcoming. </institution>
Reference-contexts: Using this controller, the robot performs its jobs almost as efficiently as it would using efficient default plans, but much more reliably (see <ref> (Beetz 1996) </ref>). Related Work. Any robot that executes its plans has to coordinate its planning and execution actions. Most systems implement a particular type of coordination between planning and execution. Others leave the coordination flexible.
Reference: <author> Bonasso, P.; Antonisse, H.; and Slack, M. </author> <year> 1992. </year> <title> A reactive robot system for find and fetch tasks in an outdoor environment. </title> <booktitle> In Proc. of AAAI-92. </booktitle>
Reference: <author> Dean, T.; Kaelbling, L.; Kirman, J.; and Nicholson, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proc. of AAAI-93, </booktitle> <pages> 574-579. </pages>
Reference: <author> Drummond, M., and Bresina, J. </author> <year> 1990. </year> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proc. of AAAI-90, </booktitle> <pages> 138-144. </pages>
Reference: <author> Fikes, R.; Hart, P.; and Nilsson, N. </author> <year> 1972. </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3(4) </booktitle> <pages> 189-208. </pages>
Reference-contexts: Most systems implement a particular type of coordination between planning and execution. Others leave the coordination flexible. Our approach proposes to specify the interaction between planning and execution as part of the robot controllers and provides the necessary control statements. PLANEX <ref> (Fikes, Hart, & Nilsson 1972) </ref> and SIPE (Wilkins 1988) reason to recover from execution failures. NASL (McDermott 1978) and IPEM (Ambros-Ingerson & Steel 1988) interleave planning and execution. NASL uses planning to execute complex actions and IPEM integrates partial-order planning and plan execution.
Reference: <author> Firby, J. </author> <year> 1989. </year> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> Technical report 672, </type> <institution> Yale University, Department of Computer Science. </institution>
Reference-contexts: Hybrid-layered control systems (for example, (Gat 1992; Bonasso, Antonisse, & Slack 1992)) plan and execute in parallel by decoupling planning and execution with a sequencing layer. The most common approach for the implementation of the sequencing layer is the rap system <ref> (Firby 1989) </ref> that can create, manipulate, and manage networks of sketchy plan steps with different priorities. In rpl, a successor of rap, we can accomplish the same kinds of behavior by implementing vital behavior as global policies that are always active.
Reference: <author> Gat, E. </author> <year> 1992. </year> <title> Integrating planning and reacting in a heterogeneous asynchronous architecture for controlling real-world mobile robots. </title> <booktitle> In Proc. of AAAI-92. </booktitle>
Reference: <author> Georgeff, M., and Ingrand, F. </author> <year> 1989. </year> <title> Decision making in an embedded reasoning system. </title> <booktitle> In Proc. of the 11 th IJCAI, </booktitle> <pages> 972-978. </pages>
Reference-contexts: The functionality of the sequencing layer is provided by the operating system of rpl that manages and updates the task queue according to the semantics of rpl control structures. PRS <ref> (Georgeff & Ingrand 1989) </ref> and blackboard architectures (Hayes-Roth 1989) are data-driven control systems that use "meta-level" reasoning to choose between different applicable courses of action.
Reference: <author> Hayes-Roth, B. </author> <year> 1989. </year> <title> Intelligent monitoring and control. </title> <booktitle> In Proc. of the 11 th IJCAI, </booktitle> <pages> 243-249. </pages>
Reference: <author> Lyons, D., and Hendriks, A. </author> <year> 1992. </year> <title> A practical approach to integrating reaction and deliberation. </title> <booktitle> In Proc. of AIPS-92, </booktitle> <pages> 153-162. </pages>
Reference-contexts: BETTER-PLAN ... BEST-PLAN) (WAIT-FOR BETTER-PLAN?) (WAIT-FOR (TASK-END AN-ITER))) (IF BETTER-PLAN? (SWAP-PLAN BEST-PLAN NEXT-ITER))))))) Variants of this kind of planned iterative behavior have been implemented for the kitting robot <ref> (Lyons & Hen-driks 1992) </ref> and for controlling the walking behavior of Ambler (Simmons 1990). Recovering from execution failures. Sometimes execution failures cannot be forestalled and therefore local planning processes have to plan to recover from execution failures after they have occurred. <p> PLANEX (Fikes, Hart, & Nilsson 1972) and SIPE (Wilkins 1988) reason to recover from execution failures. NASL (McDermott 1978) and IPEM (Ambros-Ingerson & Steel 1988) interleave planning and execution. NASL uses planning to execute complex actions and IPEM integrates partial-order planning and plan execution. RS <ref> (Lyons & Hendriks 1992) </ref> integrates planning and execution for highly repetitive tasks. The interactions between planning and action in these systems can be expressed concisely in extended rpl.
Reference: <author> McDermott, D. </author> <year> 1978. </year> <title> Planning and acting. </title> <booktitle> Cognitive Science 2(2) </booktitle> <pages> 71-109. </pages>
Reference-contexts: Others leave the coordination flexible. Our approach proposes to specify the interaction between planning and execution as part of the robot controllers and provides the necessary control statements. PLANEX (Fikes, Hart, & Nilsson 1972) and SIPE (Wilkins 1988) reason to recover from execution failures. NASL <ref> (McDermott 1978) </ref> and IPEM (Ambros-Ingerson & Steel 1988) interleave planning and execution. NASL uses planning to execute complex actions and IPEM integrates partial-order planning and plan execution. RS (Lyons & Hendriks 1992) integrates planning and execution for highly repetitive tasks.
Reference: <author> McDermott, D. </author> <year> 1991. </year> <title> A reactive plan language. </title> <institution> Research Report YALEU/DCS/RR-864, Yale University. </institution>
Reference-contexts: In the remainder of the paper we describe how local planning capabilities can be integrated into rpl and how important patterns of interaction between planning and execution can be realized in extended rpl. The Reactive Plan Language (RPL) We implement plans in rpl (Reactive Plan Language, <ref> (McDermott 1991) </ref>), a Lisp-like robot control language with conditionals, loops, program variables, processes, and subroutines. rpl provides high-level constructs (interrupts, monitors) to synchronize parallel physical actions and make plans reactive and robust by incorporating sensing and monitoring actions, and reactions triggered by observed events. rpl also provides concepts that are crucial
Reference: <author> McDermott, D. </author> <year> 1992. </year> <title> Transformational planning of reactive behavior. </title> <institution> Research Report YALEU/DCS/RR-941, Yale University. </institution>
Reference-contexts: Each time the algorithm has found a better plan p 0 it provides p 0 , the best plan it has found so far, as its result. In our experiments we have used planning algorithms that schedule errands <ref> (McDermott 1992) </ref> and forestall behavior flaws (Beetz & McDermott 1994). Several other planning algorithms have the required functionality (McDermott 1992; Drummond & Bresina 1990; Dean et al. 1993; Lyons & Hendriks 1992).
Reference: <author> Nilsson, N. </author> <year> 1984. </year> <title> Shakey the robot. </title> <type> Technical Note 323, </type> <institution> SRI AI Center. </institution>
Reference-contexts: Beside common patterns of interaction between planning and execution we can also implement agent architectures in extended rpl. The implementation of the Sense/Plan/Act architecture <ref> (Nilsson 1984) </ref> is straightforward.
Reference: <author> Simmons, R. </author> <year> 1990. </year> <title> An architecture for coordinating planning, sensing, and action. </title> <editor> In Sycara, K., ed., </editor> <title> Innovative Approaches to Planning, </title> <journal> Scheduling and Control, </journal> <pages> 292-297. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: BETTER-PLAN ... BEST-PLAN) (WAIT-FOR BETTER-PLAN?) (WAIT-FOR (TASK-END AN-ITER))) (IF BETTER-PLAN? (SWAP-PLAN BEST-PLAN NEXT-ITER))))))) Variants of this kind of planned iterative behavior have been implemented for the kitting robot (Lyons & Hen-driks 1992) and for controlling the walking behavior of Ambler <ref> (Simmons 1990) </ref>. Recovering from execution failures. Sometimes execution failures cannot be forestalled and therefore local planning processes have to plan to recover from execution failures after they have occurred. <p> The rpl extensions for local planning of ongoing behavior are tools that facilitate the implementation of such "meta-level" reasoning modules and their synchronization with reactive execution. Our work is closely related to Simmons' TCA architecture <ref> (Simmons 1990) </ref>, which provides control concepts to synchronize different threads of control, interleave planning and execution, recover from execution failures, and monitor the environment continuously. rpl provides control structures for the same purposes at a higher level of abstraction.
Reference: <author> Wilkins, D. </author> <year> 1988. </year> <title> Practical Planning: Extending the AI Planning Paradigm. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Most systems implement a particular type of coordination between planning and execution. Others leave the coordination flexible. Our approach proposes to specify the interaction between planning and execution as part of the robot controllers and provides the necessary control statements. PLANEX (Fikes, Hart, & Nilsson 1972) and SIPE <ref> (Wilkins 1988) </ref> reason to recover from execution failures. NASL (McDermott 1978) and IPEM (Ambros-Ingerson & Steel 1988) interleave planning and execution. NASL uses planning to execute complex actions and IPEM integrates partial-order planning and plan execution. RS (Lyons & Hendriks 1992) integrates planning and execution for highly repetitive tasks.
References-found: 18

