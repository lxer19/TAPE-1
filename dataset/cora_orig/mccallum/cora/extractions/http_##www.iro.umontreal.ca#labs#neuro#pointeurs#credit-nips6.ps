URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/credit-nips6.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: Credit Assignment through Time: Alternatives to Backpropagation  
Author: Yoshua Bengio Paolo Frasconi 
Address: Montreal, Qc H3C-3J7  50139 Firenze (Italy)  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de Montreal  Dip. di Sistemi e Informatica Universita di Firenze  
Abstract: Learning to recognize or predict sequences using long-term context has many applications. However, practical and theoretical problems are found in training recurrent neural networks to perform tasks in which input/output dependencies span long intervals. Starting from a mathematical analysis of the problem, we consider and compare alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled. Results on the new algorithms show performance qualitatively superior to that obtained with backpropagation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> S. Becker and Y. Le Cun. </author> <title> (1988) Improving the convergence of back-propagation learning with second order methods, </title> <booktitle> Proc. of the 1988 Connectionist Models Summer School, </booktitle> <editor> (eds. Touretzky, Hinton and Sejnowski), </editor> <publisher> Morgan Kaufman, </publisher> <pages> pp. 29-37. </pages>
Reference: <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> (1994) Learning long-term dependencies with gradient descent is difficult, </title> <journal> IEEE Trans. Neural Networks, </journal> <note> (in press). </note>
Reference-contexts: To "erase" that bit of information, the inputs may push the system activity a t out of this basin of attraction and possibly into another one. In <ref> (Bengio, Simard, & Frasconi, 1994) </ref> we show that only two conditions can arise when using hyperbolic attractors to latch bits of information in such a system. <p> See proofs in <ref> (Bengio, Simard, & Frasconi, 1994) </ref>. A consequence of these results is that it is generally very difficult to train a parametric dynamical system (such as a recurrent neural network) to learn long-term dependencies using gradient descent. <p> Details of the algorithm will appear in <ref> (Bengio, Simard, & Frasconi, 1994) </ref>.
Reference: <author> A. Corana, M. Marchesi, C. Martini, and S. Ridella. </author> <title> (1987) Minimizing multimodal functions of continuous variables with the simulated annealing algorithm, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 13, no. 13, </volume> <pages> pp. 262-280. </pages>
Reference-contexts: Based on the understanding brought by this analysis, we have explored and compared several alternative algorithms and architectures. 3 Global Search Methods Global search methods such as simulated annealing can be applied to this problem, but they are generally very slow. We implemented the simulated annealing algorithm presented in <ref> (Corana, Marchesi, Martini, & Ridella, 1987) </ref> for optimizing functions of continuous variables. This is a "batch learning" algorithm (updating parameters after all examples of the training set have been seen). It performs a cycle of random moves, each along one coordinate (parameter) direction.
Reference: <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> (1977) Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. of Royal Stat. Soc., </journal> <volume> vol. </volume> <pages> B39, pp. 1-38. </pages>
Reference-contexts: Missing variables, over which an expectation is taken, are the paths in state-space. The EM (Expectation/Maximization) or GEM (Generalized EM) algorithms <ref> (Dempster, Laird, & Rubin, 1977) </ref> can be used to help decoupling the influence of different hypothetical paths in state-space. The estimation step of EM requires propagating backward a discrete distribution of targets.
Reference: <author> M.I. Jordan and R.A. Jacobs. </author> <title> (1994) Hierarchical mixtures of experts and the EM algorithm, Neural Computation, </title> <publisher> (in press). </publisher>
Reference-contexts: The estimation step of EM requires propagating backward a discrete distribution of targets. In contrast to HMMs, where parameters are adjusted in an unsupervised learning framework, we use EM in a supervised fashion. This new perspective has been successful in training static models <ref> (Jordan & Jacobs, 1994) </ref>. Transition probabilities, conditional on the current input, can be computed by a parametric function such as a layer of a neural network with softmax units. We propose a modular architecture with one subnetwork N j for each state (see Figure 1).

References-found: 5

