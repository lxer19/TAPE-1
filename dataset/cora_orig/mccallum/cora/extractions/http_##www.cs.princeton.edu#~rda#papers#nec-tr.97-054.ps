URL: http://www.cs.princeton.edu/~rda/papers/nec-tr.97-054.ps
Refering-URL: http://www.cs.princeton.edu/~rda/
Root-URL: http://www.cs.princeton.edu
Email: falpert|philbing@research.nj.nec.com  
Title: cBSP: Zero-Cost Synchronization in a Modified BSP Model  
Author: Richard D Alpert James F Philbin 
Keyword: Parallel (cBSP) runtime library incorporating these ideas.  
Address: 4 Independence Way Princeton NJ 08540  
Affiliation: NEC Research Institute  
Abstract: The Bulk Synchronous Parallel (BSP) model of computing proposed by Valiant [Val90] axiomatically assumes inexpensive global synchronization. Typically, however, global synchronization is expensive, detracting from the appeal of BSP on machines without special synchronization hardware. Decreasing the cost of global synchronization is critical to improving the performance of any BSP implementation. We address this problem by exploiting local knowledge of the state of remote processes that can be inferred from the number of messages received from the remote process. We found that frequently, synchronization may be effected with no network traffic overhead. This paper describes the design, implementation, and performance of a counting Bulk Synchronous 
Abstract-found: 1
Intro-found: 1
Reference: [ADFL96] <author> Richard D Alpert, Cezay Dubnicki, Edward Felten, and Kai Li. </author> <title> The Design and Implemenation of NX Message Passing Using SHRIMP Virtual Memory Mapped Communication. </title> <booktitle> In Proceedings of 25th International Conference on Parallel Processing, </booktitle> <address> pages I-111 - I-119, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Because message buffers are completely cleared at the end of each superstep, there is no need for a buffer to have a strict, regular format. This contrasts with the VMMC implementation of NX/2 <ref> [ADFL96, Pie94] </ref>. Because NX messages can be consumed out-of-order, and because there is no time at which it is known that a message buffer will be completely empty, NX messages are sent to clearly defined "slots" in the message buffer.
Reference: [BCF + 95] <author> N J Boden, D Cohen, R E Felderman, A E Kulawik, C L Seitz, J N Seizovic, and W Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The VMMC mechanism provides good support for protected, user-level message passing, user-level buffer management, and zero-copy protocols [DIFL96, FAB + 96]. Our test platform consists of 16 commodity dual 200 MHz P6, PCI-bus-based PCs, running a slightly modified Linux v2.0.24, connected with M2M-Dual-SW8 Myrinet interconnects <ref> [BCF + 95, Myr96] </ref>. Under VMMC, receivers grant permission for remote writes into their virtual address spaces by exporting regions of their virtual memory. Senders then import memory regions to which they wish to write. Protection is verified at the time of import.
Reference: [BDG + 91] <author> A Beguelin, J Dongarra, A Geist, R Manchek, and V Sunderam. </author> <title> A Users' Guide to PVM Parallel Virtual Machine. </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Real parallel programming always has been based on specific types of application programming interfaces (APIs). These APIs have been designed around experience and need, integrating techniques of concurrent programming (e.g. Mutual Exclusion, Monitors, Threads) and interprocessor communications (e.g. PVM, RPC, NX, MPI) <ref> [BDG + 91, BN84, Pie94, SOHL + 95] </ref>. Because these APIs are not models of computation, it is difficult to use them to study or to predict the behavior of parallel algorithms running on parallel machines.
Reference: [BH86] <author> J. E. Barnes and P. Hut. </author> <title> A hierarchical O(N logN ) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: On 8 nodes, solving a 1024-equation system, gauss used 1025 supersteps in about 11 seconds. Counting synchronization showed a speedup of 1.116 over global synchronization. 6.2.2 N-Body Barnes Hut N-Body <ref> [BH86] </ref> is a hierarchical O (N log N ) force calculation algorithm. Bodies, each having different mass, acceleration, and initial velocity, are distributed in space. Space is then recursively partitioned, forming an octree. Bodies are at the leaves; regions of space are represented by interior nodes.
Reference: [Bis] <author> Bisseling. </author> <title> BSP Dense LU Decomposition. </title> <address> http://www.math.ruu.nl/people/bisseling.html. </address>
Reference-contexts: The original idea for counting synchronization grew out of discussions with Kai Li. The LU decomposition code was provided by Bob Bisseling, via his web page <ref> [Bis] </ref>. The flow control buffer management scheme was developed during a conversation with Stefanos Damianakis. Richard Alpert has enjoyed the partial support of an ARPA Fellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland.
Reference: [BM93] <author> R H Bisseling and W F McColl. </author> <title> Scientific Computing on Bulk Synchronous Parallel Architectures. </title> <type> Technical Report 836, </type> <institution> Department of Mathematics, University of Utrecht, </institution> <month> Dec 93. </month>
Reference-contexts: In summary, however, BSP divides a computation into supersteps, delineated by global synchronization. Communication initiated during one superstep is not visible at a receiver until the start of the next superstep. fl This work was completed in part while this author was at Princeton University. Researchers <ref> [BM93, Har, Oxf] </ref> are striving to implement run-time libraries and exploit the strengths of the the BSP model. Through these efforts, programmers can develop portable and scalable parallel programs, where the same code executes with predictable performance on shared or distributed memory systems, on clustered workstations or PCs.
Reference: [BN84] <author> A D Birrell and B J Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2 </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Real parallel programming always has been based on specific types of application programming interfaces (APIs). These APIs have been designed around experience and need, integrating techniques of concurrent programming (e.g. Mutual Exclusion, Monitors, Threads) and interprocessor communications (e.g. PVM, RPC, NX, MPI) <ref> [BDG + 91, BN84, Pie94, SOHL + 95] </ref>. Because these APIs are not models of computation, it is difficult to use them to study or to predict the behavior of parallel algorithms running on parallel machines.
Reference: [CKP + 92] <author> D Culler, R Karp, D Patterson, A Sahay, K Schauser, E Santos, R Subramonian, and T von Eicken. </author> <title> LogP: towards a realistic model of parallel computation. </title> <institution> Computer science division report, University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: PVM, RPC, NX, MPI) [BDG + 91, BN84, Pie94, SOHL + 95]. Because these APIs are not models of computation, it is difficult to use them to study or to predict the behavior of parallel algorithms running on parallel machines. Recent efforts such as the LogP model <ref> [CKP + 92] </ref> can be used to study the behavior of message passing programs quite realistically, but they are not designed for programming parallel algorithms. A model of parallel computing analogous to the von Neumann model of sequential computing could wed the tools of theoretical analysis to real parallel programming.
Reference: [DBLP97] <author> Cezary Dubnicki, Angelos Bilas, Kai Li, and Jim Philbin. </author> <title> Design and Implementation of Virtual Memory-Mapped Communication on Myrinet. </title> <booktitle> In 11th International Parallel Processing Symposium. IEEE Computer Society Technical Committee on Parallel Processing, </booktitle> <year> 1997. </year> <note> To Appear. </note>
Reference-contexts: Buffer management is performed by senders. Data are written directly into the address space of the receiver; there is no explicit receive instruction. Details of the VMMC implementation can be found in <ref> [DBLP97] </ref>. An important feature of virtual memory-mapped communication is that it not only supports, but requires, user-level buffer management. Libraries and user programs customize their own buffer management to implement zero-copy protocols, to achieve very low-latency message passing, and to exploit the raw bandwidth available in hardware.
Reference: [DIFL96] <author> Cezary Dubnicki, Liviu Iftode, Edward Felten, and Kai Li. </author> <title> Software Support for Virtual Memory-Mapped Communication. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Writes then propagate with little or no sending processor involvement, and no receiver involvement whatsoever. The VMMC mechanism provides good support for protected, user-level message passing, user-level buffer management, and zero-copy protocols <ref> [DIFL96, FAB + 96] </ref>. Our test platform consists of 16 commodity dual 200 MHz P6, PCI-bus-based PCs, running a slightly modified Linux v2.0.24, connected with M2M-Dual-SW8 Myrinet interconnects [BCF + 95, Myr96].
Reference: [FAB + 96] <author> E Felten, R Alpert, A Bilas, M Blumrich, D W Clark, S Damianakis, C Dubnicki, L Iftode, and K Li. </author> <title> Early Experience with Message-Passing on the Shrimp Multicomputer. </title> <booktitle> In International Symposium on Computer Architecture XXIII, </booktitle> <year> 1996. </year>
Reference-contexts: Writes then propagate with little or no sending processor involvement, and no receiver involvement whatsoever. The VMMC mechanism provides good support for protected, user-level message passing, user-level buffer management, and zero-copy protocols <ref> [DIFL96, FAB + 96] </ref>. Our test platform consists of 16 commodity dual 200 MHz P6, PCI-bus-based PCs, running a slightly modified Linux v2.0.24, connected with M2M-Dual-SW8 Myrinet interconnects [BCF + 95, Myr96].
Reference: [FW78] <author> Steven Fortune and James Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Conference Record of the Tenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118. </pages> <institution> Association for Computing Machinery, </institution> <year> 1978. </year>
Reference-contexts: 1 Introduction In parallel computing, the gap separating theory and practice is wider than in sequential computing. Most theoretical models of parallel computing were designed to study algorithmic complexity <ref> [FW78, Gib89, KLadH92] </ref> and, like the physics students' frictionless plane, do not reflect the state of real-world technology. While useful for theoretical exploration, these models of parallel computing do not incorporate the limitations and features of existing parallel machines.
Reference: [GHL + ] <author> Mark W Goudreau, Jonathan M D Hill, Kevin Lang, Bill McColl, Satish B Rao, Dan C Stefanescu, Torsten Suel, and Thanasis Tsantilas. </author> <title> A Proposal for the BSP Worldwide Standard Library. </title> <address> http://www.bsp-worldwide.org/standard/stand2.htm. </address>
Reference-contexts: There is a significant effort underway to create a world-wide standard BSP API <ref> [GHL + ] </ref>. We do not suggest that our API can or should compete with this effort. We chose to keep our API as simple as possible.
Reference: [Gib89] <author> P B Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168. </pages> <institution> Association for Computing Machinery, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction In parallel computing, the gap separating theory and practice is wider than in sequential computing. Most theoretical models of parallel computing were designed to study algorithmic complexity <ref> [FW78, Gib89, KLadH92] </ref> and, like the physics students' frictionless plane, do not reflect the state of real-world technology. While useful for theoretical exploration, these models of parallel computing do not incorporate the limitations and features of existing parallel machines.
Reference: [GRT94] <author> Mark W Goudreau, Satish B Rao, and Thanasis Tsantilas. </author> <title> A Study of the BSP Model: Algorithms and Implementation. </title> <type> Technical Report UCFCS:CS-TR-94-04, </type> <institution> Department of Computer Science, University of Central Florida, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The model makes no assumptions about the topology of the interconnect of a parallel architecture. BSP is useful not only to algorithm designers, but also to programmers and architects. Adhering to the BSP model permits advances in hardware and software to occur fairly independently of each other <ref> [GRT94] </ref>. Valiant asserts that the BSP model achieves both portability and efficiency for a large class of problems. The cost of portability is that efficient operation typically requires a larger input size for BSP code than for machine-specific code.
Reference: [GV92] <author> Alexandros V. Gerbessiotis and Leslie G. Valiant. </author> <title> Direct Bulk-Synchronous Parallel Algorithms. </title> <type> Technical Report TR-10-92, </type> <institution> Harvard University, Computer Science Department, </institution> <year> 1992. </year>
Reference-contexts: Valiant asserts that the BSP model achieves both portability and efficiency for a large class of problems. The cost of portability is that efficient operation typically requires a larger input size for BSP code than for machine-specific code. A formal analysis of some BSP algorithms can be found in <ref> [GV92] </ref>.
Reference: [Har] <institution> Harvard University. The Harvard BSP Project. </institution> <note> http://www.das.harvard.edu/cs/research/bsp.html. </note>
Reference-contexts: In summary, however, BSP divides a computation into supersteps, delineated by global synchronization. Communication initiated during one superstep is not visible at a receiver until the start of the next superstep. fl This work was completed in part while this author was at Princeton University. Researchers <ref> [BM93, Har, Oxf] </ref> are striving to implement run-time libraries and exploit the strengths of the the BSP model. Through these efforts, programmers can develop portable and scalable parallel programs, where the same code executes with predictable performance on shared or distributed memory systems, on clustered workstations or PCs.
Reference: [HCB96] <author> Jonathan M D Hill, Paul I Crumpton, and David A Burgess. </author> <title> The theory, practice, and a tool for BSP performance prediction applied to a CFD application. </title> <type> Technical Report TR-4-96, </type> <institution> Programming Research Group, Oxford University Computing Laboratory, </institution> <address> Wolfson Building, Parks Road, Oxford, England. OX1 3QD, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: BSP offers both a a common abstraction toward which computer architects and compiler writers can design, and a concise model of parallel program execution enabling accurate performance prediction for proactive application design <ref> [HCB96, Kne94, RPL95] </ref>. The BSP model is described in more detail in section 2. In summary, however, BSP divides a computation into supersteps, delineated by global synchronization.
Reference: [KLadH92] <editor> R M Karp, M Luby, and M Meyer auf der Heide. </editor> <title> Efficient PRAM Simulation on a Distributed Memory Machine. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual Symposium of the Theory of Computing, </booktitle> <pages> pages 318-326. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In parallel computing, the gap separating theory and practice is wider than in sequential computing. Most theoretical models of parallel computing were designed to study algorithmic complexity <ref> [FW78, Gib89, KLadH92] </ref> and, like the physics students' frictionless plane, do not reflect the state of real-world technology. While useful for theoretical exploration, these models of parallel computing do not incorporate the limitations and features of existing parallel machines.
Reference: [Kne94] <author> Simon Knee. </author> <title> Program development and performance prediction on BSP machines using opal. </title> <type> Technical Report PRG-TR-18-1994, </type> <institution> Oxford University Computing Laboratory, </institution> <address> Wolfson Building, Parks Road, Oxford OX1 3QD, </address> <year> 1994. </year>
Reference-contexts: BSP offers both a a common abstraction toward which computer architects and compiler writers can design, and a concise model of parallel program execution enabling accurate performance prediction for proactive application design <ref> [HCB96, Kne94, RPL95] </ref>. The BSP model is described in more detail in section 2. In summary, however, BSP divides a computation into supersteps, delineated by global synchronization.
Reference: [MR94] <author> Richard Miller and Joy Reed. </author> <title> The Oxford BSP Library Users' Guide. </title> <institution> Oxford Parallel, Oxford University Computing Laboratory, </institution> <address> Wolfson Building, Parks Road, Oxford, OX1 3QD, 1.0 edition, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A formal analysis of some BSP algorithms can be found in [GV92]. A rather nice example of how a programmer can use the parameters p, L, g, and s, to optimize code is "parameterizing for performance," given in <ref> [MR94] </ref>: Depending on the values of the performance parameters, it may be less costly to broadcast data to processors linearly than to broadcast logarithmically.
Reference: [Myr96] <institution> The Myrinet on-line documentation. </institution> <note> http://www.myri.com:80/scs/documentation, 1996. </note>
Reference-contexts: The VMMC mechanism provides good support for protected, user-level message passing, user-level buffer management, and zero-copy protocols [DIFL96, FAB + 96]. Our test platform consists of 16 commodity dual 200 MHz P6, PCI-bus-based PCs, running a slightly modified Linux v2.0.24, connected with M2M-Dual-SW8 Myrinet interconnects <ref> [BCF + 95, Myr96] </ref>. Under VMMC, receivers grant permission for remote writes into their virtual address spaces by exporting regions of their virtual memory. Senders then import memory regions to which they wish to write. Protection is verified at the time of import.
Reference: [Oxf] <institution> Oxford University. The Oxford BSP Group. </institution> <note> http://www.comlab.ox.ac.uk/oucl/groups/bsp/research.html. </note>
Reference-contexts: In summary, however, BSP divides a computation into supersteps, delineated by global synchronization. Communication initiated during one superstep is not visible at a receiver until the start of the next superstep. fl This work was completed in part while this author was at Princeton University. Researchers <ref> [BM93, Har, Oxf] </ref> are striving to implement run-time libraries and exploit the strengths of the the BSP model. Through these efforts, programmers can develop portable and scalable parallel programs, where the same code executes with predictable performance on shared or distributed memory systems, on clustered workstations or PCs.
Reference: [Pie94] <author> Paul Pierce. </author> <title> The NX Message Passing Interface, </title> <journal> Parallel Computing Vol. </journal> <volume> 20 no. 4, </volume> <month> April. </month> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Real parallel programming always has been based on specific types of application programming interfaces (APIs). These APIs have been designed around experience and need, integrating techniques of concurrent programming (e.g. Mutual Exclusion, Monitors, Threads) and interprocessor communications (e.g. PVM, RPC, NX, MPI) <ref> [BDG + 91, BN84, Pie94, SOHL + 95] </ref>. Because these APIs are not models of computation, it is difficult to use them to study or to predict the behavior of parallel algorithms running on parallel machines. <p> Because message buffers are completely cleared at the end of each superstep, there is no need for a buffer to have a strict, regular format. This contrasts with the VMMC implementation of NX/2 <ref> [ADFL96, Pie94] </ref>. Because NX messages can be consumed out-of-order, and because there is no time at which it is known that a message buffer will be completely empty, NX messages are sent to clearly defined "slots" in the message buffer.
Reference: [RPL95] <author> Joy Reed, Kevin Parrott, and Tim Lanfear. </author> <title> Portability, predictability and performance for parallel computing: </title> <booktitle> BSP in practice. </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: BSP offers both a a common abstraction toward which computer architects and compiler writers can design, and a concise model of parallel program execution enabling accurate performance prediction for proactive application design <ref> [HCB96, Kne94, RPL95] </ref>. The BSP model is described in more detail in section 2. In summary, however, BSP divides a computation into supersteps, delineated by global synchronization.
Reference: [SOHL + 95] <author> Marc Snir, Steve W Otto, Steven Huss-Lederman, David W Walker, and Jack Dongarra. </author> <title> MPI The Complete Reference. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1995. </year> <note> ISBN 0-262-69184-1. </note>
Reference-contexts: Real parallel programming always has been based on specific types of application programming interfaces (APIs). These APIs have been designed around experience and need, integrating techniques of concurrent programming (e.g. Mutual Exclusion, Monitors, Threads) and interprocessor communications (e.g. PVM, RPC, NX, MPI) <ref> [BDG + 91, BN84, Pie94, SOHL + 95] </ref>. Because these APIs are not models of computation, it is difficult to use them to study or to predict the behavior of parallel algorithms running on parallel machines.
Reference: [SWG] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44. </pages>
Reference-contexts: Using counting synchronization yielded a speedup of 1.052over global synchronization. 6.2.4 Ocean Ocean models the role of eddy and boundary currents in influencing large-scale ocean movements. This implementation, modified from the SPLASH suite <ref> [SWG] </ref>, uses dynamically allocated four-dimensional arrays for grid data storage. The behavior of ocean meets intuitive expectations: counting synchronization is faster than global synchronization, though with results less dramatic (7% speedup) than some of the more regular applications. 7 Conclusions Some applications can benefit from counting synchronization.
Reference: [Val90] <author> Leslie G Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8):103, </volume> <month> August </month> <year> 1990. </year>
Reference-contexts: A model of parallel computing analogous to the von Neumann model of sequential computing could wed the tools of theoretical analysis to real parallel programming. The BSP model was introduced by Les Valiant <ref> [Val90] </ref> as just such a bridging model, linking architecture and software, theory and practice. BSP offers both a a common abstraction toward which computer architects and compiler writers can design, and a concise model of parallel program execution enabling accurate performance prediction for proactive application design [HCB96, Kne94, RPL95]. <p> we describe the performance of our library, and in section 7, summarize our findings and discuss some related open research questions. 2 BSP 2.1 The BSP Model Superstep COMPUTE COMPUTE COMPUTE BLOCK SYNC SYNC SYNC message message message message COMPUTE BLOCK BLOCK Superstep In 1990, Valiant introduced the BSP model <ref> [Val90] </ref> to bridge the gap between the theory and the practice of parallel computing. The BSP model provides an abstraction toward which both compiler writers and computer architects can aim.
References-found: 28

