URL: http://www.cs.cornell.edu/Info/People/prakas/papers/sc96.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/prakas/
Root-URL: http://www.cs.cornell.edu
Email: fprakas,pingalig@cs.cornell.edu  
Title: Transformations for Imperfectly Nested Loops  
Author: Induprakas Kodukula Keshav Pingali 
Date: October 20, 1996  
Address: Ithaca, NY 14853.  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: Loop transformations are critical for compiling high-performance code for modern computers. Existing work has focused on transformations for perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop of a loop nest). In practice, most loop nests, such as those in matrix factorization codes, are imperfectly nested. In some programs, imperfectly nested loops can be converted into perfectly nested loops by loop distribution, but this is not always legal. In this paper, we present an approach to transforming imperfectly nested loops directly. Our approach is an extension of the linear loop transformation framework for perfectly nested loops, and it models permutation, reversal, skewing, scaling, alignment, distribution and jamming.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Principle and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another. <p> Suppose that S1 writes to some array location in iteration I w of the outer loop, 7 and that this location is read by statement S2 in iteration (I r ; J r ). The instance vector for the statement execution performing the write is <ref> [I w ; 0; 1; I w ] </ref> 0 . Similarly, the instance vector for the statement execution performing the read is [I r ; 1; 0; J r ] 0 . <p> The instance vector for the statement execution performing the write is [I w ; 0; 1; I w ] 0 . Similarly, the instance vector for the statement execution performing the read is <ref> [I r ; 1; 0; J r ] </ref> 0 . The difference between these two instance vectors is [I r I w ; 1; 1; J r I w ] 0 . <p> In our example, it is easy to see that 1 = 0 and 2 = +. Therefore, the flow dependence in the above example will be represented in our framework as <ref> [0; 1; 1; +] </ref> 0 Using a similar procedure, we can determine the other dependences in this code. These dependences can be collected into a dependence matrix; for our example, this matrix is the following: 2 6 4 1 1 0 + 0 1 7 7 . <p> We show how to add the additional loops, using the following program as an example. do I = 1..N do J = I..N enddo enddo Executions of S1 and S2 are represented by initial instance vectors <ref> [I; 0; 1; I] </ref> 0 and [I; 1; 0; J] 0 respectively. Dependence analysis on this code produces the matrix D = 2 6 4 0 1 1 1 7 7 . <p> We show how to add the additional loops, using the following program as an example. do I = 1..N do J = I..N enddo enddo Executions of S1 and S2 are represented by initial instance vectors [I; 0; 1; I] 0 and <ref> [I; 1; 0; J] </ref> 0 respectively. Dependence analysis on this code produces the matrix D = 2 6 4 0 1 1 1 7 7 . Sup pose that the transformation matrix M = 2 6 4 0 0 1 0 0 0 0 1 7 7 . <p> In our example, the per-statement transformations M S1 and M S2 are respectively h i " 0 1 . In general, these matrices can be computed by adding appropriate columns of M and projecting onto loop positions. For example, for S1, we note that the general instance vector is <ref> [I; 0; 1; I] </ref> 0 . Therefore, we add the first and fourth columns of M and project onto loop I; this gives us the matrix [0] as desired. For lack of space, we omit the details of the general algorithm. <p> Proof: Obvious from the fact that T S is of rank k and from the construction of N S . 2 In our skewing example, matrix N S1 is <ref> [1] </ref>, while N S2 is " 0 1 . Definition 9 The loops surrounding S in the transformed AST (after augmentation) corresponding to the rows of T S that are retained in N S are called the non-singular loops of S.
Reference: [2] <author> E. Ayguade and Jordi Torres. </author> <title> Partitioning the statement per iteration space using non-singular matrices. </title> <booktitle> In 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 407-415, </pages> <address> Tokyo, </address> <month> jul </month> <year> 1993. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another.
Reference: [3] <author> Uptal Banerjee. </author> <title> A theory of loop permutations. </title> <booktitle> In Languages and compilers for parallel computing, </booktitle> <pages> pages 54-74, </pages> <year> 1989. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another. <p> Second, the frameworks themselves provide little help in producing desirable transformations. This should be contrasted with the case of perfectly nested loops. In that case, determining a parallel outermost loop merely requires finding a vector in the null space of the columns of the dependence matrix <ref> [3] </ref>. Similarly, general `completion procedures' have been developed for producing complete transformations from partial ones [10]. In this paper, we present an approach to transforming imperfectly nested loops that trades off generality for simplicity.
Reference: [4] <author> Uptal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Languages and compilers for parallel computing, </booktitle> <pages> pages 192-219, </pages> <year> 1990. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another.
Reference: [5] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Petersen, W. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <type> Technical Report 1375, </type> <institution> Center for Supercomputing Research and Development (CSRD), University of Illinois Urbana-Champaign. </institution>
Reference-contexts: Currently, these two transformation can be expressed in our 20 framework, but we do not make use of these two transformations in our completion procedure. We are also implementing our loop transformations in the Polaris compiler test-bed <ref> [5] </ref>.
Reference: [6] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem part i: one dimensional time. </title> <journal> International Journal of Parallel Programming, </journal> <month> October </month> <year> 1992. </year>
Reference-contexts: These techniques were extended by Feautrier in his theory of schedules in multi-dimensional time <ref> [6, 7] </ref>; a related approach is Kelly and Pugh's mappings [8]. The second problem | finding legal and desirable transformations | is solved by searching the space of transformations. However, this process is usually very expensive for two reasons.
Reference: [7] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem part ii: multidimensional time. </title> <journal> International Journal of Parallel Programming, </journal> <month> December </month> <year> 1992. </year>
Reference-contexts: These techniques were extended by Feautrier in his theory of schedules in multi-dimensional time <ref> [6, 7] </ref>; a related approach is Kelly and Pugh's mappings [8]. The second problem | finding legal and desirable transformations | is solved by searching the space of transformations. However, this process is usually very expensive for two reasons.
Reference: [8] <author> Wayne Kelly, William Pugh, and Evan Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 332-341, </pages> <address> McLean, Virginia, </address> <month> feb </month> <year> 1995. </year>
Reference-contexts: These techniques were extended by Feautrier in his theory of schedules in multi-dimensional time [6, 7]; a related approach is Kelly and Pugh's mappings <ref> [8] </ref>. The second problem | finding legal and desirable transformations | is solved by searching the space of transformations. However, this process is usually very expensive for two reasons.
Reference: [9] <author> S.Y. Kung. </author> <title> VLSI Array Processors. </title> <publisher> Prentice-Hall Inc, </publisher> <year> 1988. </year>
Reference-contexts: These schedules specify mappings from statement instances to processor/time axes; these mappings are usually restricted to be affine functions of loop variables <ref> [9] </ref>. It is straight-forward to interpret these schedules or mappings as loop transformations in which each assignment statement in a loop nest is transformed by a possibly different linear (or pseudo-linear) function to a target iteration space.
Reference: [10] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation based on non-singular matrices. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(2), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another. <p> In that case, determining a parallel outermost loop merely requires finding a vector in the null space of the columns of the dependence matrix [3]. Similarly, general `completion procedures' have been developed for producing complete transformations from partial ones <ref> [10] </ref>. In this paper, we present an approach to transforming imperfectly nested loops that trades off generality for simplicity. <p> The algorithm for adding these loops is identical to the completion procedure given by Li and Pingali in the context of perfectly nested loops <ref> [10] </ref>, and is shown in Figure 7. Theorem 3 Let M be a legal transformation. For a statement S, let D s = d 1 ; d 2 ; : : : ; d k be the set of self dependences of S unsatisfied by M . <p> Proof: As we have already mentioned, N S is an integer non-singular matrix that represents the transformation from the loops surrounding S initially to the non-singular loops surrounding S after transformation. We can use an approach identical to the one in <ref> [10] </ref> used for perfectly nested loops for this purpose. 2 17 Thus, for any statement, we can generate the loop bounds and steps for all non-singular loops surrounding it after transformation. The only remaining issue is regarding the singular loops surrounding S. We deal with it as follows. <p> For example, in earlier work, Li and Pingali <ref> [10] </ref> have described a completion procedure which, given a dependence matrix and the first few rows of a desired transformation, automatically appends additional rows to the matrix to produce a complete transformation matrix that satisfies all 18 dependences [10]. We have developed a similar procedure for imperfectly nested loops. <p> For example, in earlier work, Li and Pingali <ref> [10] </ref> have described a completion procedure which, given a dependence matrix and the first few rows of a desired transformation, automatically appends additional rows to the matrix to produce a complete transformation matrix that satisfies all 18 dependences [10]. We have developed a similar procedure for imperfectly nested loops.
Reference: [11] <author> William Pugh. </author> <title> The omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Communications of the ACM, </booktitle> <pages> pages 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: as follows: 1 = I r I w ; 2 = J r I w (3) We treat Equations 2 and 3 as a single system of equations, and project the solution of this system onto 1 and 2, using any integer linear programming tool, such as the Omega tool-kit <ref> [11] </ref>. In our example, it is easy to see that 1 = 0 and 2 = +. Therefore, the flow dependence in the above example will be represented in our framework as [0; 1; 1; +] 0 Using a similar procedure, we can determine the other dependences in this code.
Reference: [12] <author> J. Ramanujam. </author> <title> Optimal code parallelization using unimodular transformations. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another.
Reference: [13] <author> M. E. Wolf and M. S. Lam. </author> <title> An algorithmic approach to compound loop transformations. </title> <booktitle> In Languages and compilers for parallel computing, </booktitle> <pages> pages 243-273, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: For perfectly nested loops (that is, loops in which all assignment statements are contained within the innermost loop), polyhedral methods can be used to synthesize sequences of linear loop transformations (permutation, skewing, reversal and scaling) for enhancing parallelism and locality <ref> [1, 3, 4, 13, 10, 12, 2] </ref>. The key idea is to model the iterations of the loop nest as points in an integer lattice, and to model linear loop transformations as nonsingular matrices mapping one lattice to another.
Reference: [14] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1995. </year> <month> 22 </month>
Reference-contexts: 1 Introduction Modern compilers perform a variety of loop transformations, like permutation, skewing, reversal, scaling, distribution and jamming, to generate high quality code for high-performance computers <ref> [14] </ref>. Determining an optimal sequence of loop transformations for enhancing parallelism or locality of reference is a very difficult problem.
References-found: 14

