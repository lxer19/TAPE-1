URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps
Refering-URL: http://www.cs.wisc.edu/~paulb/papers.html
Root-URL: 
Email: email: paulb@cs.wisc.edu, olvi@cs.wisc.edu.  email: nstreet@a.cs.okstate.edu.  
Title: Feature Selection via Mathematical Programming  
Author: P. S. Bradley O. L. Mangasarian and W. N. Street 
Date: (Received March 1996; revised April 1997)  
Address: Wisconsin, 1210 West Dayton Street, Madison, WI 53706,  Stillwater, OK 74078,  
Affiliation: Computer Sciences Department, University of  Computer Science Department, 207 Mathematical Sciences, Oklahoma State University,  
Abstract: The problem of discriminating between two finite point sets in n-dimensional feature space by a separating plane that utilizes as few of the features as possible, is formulated as a mathematical program with a parametric objective function and linear constraints. The step function that appears in the objective function can be approximated by a sigmoid or by a concave exponential on the nonnegative real line, or it can be treated exactly by considering the equivalent linear program with equilibrium constraints (LPEC). Computational tests of these three approaches on publicly available real-world databases have been carried out and compared with an adaptation of the optimal brain damage (OBD) method for reducing neural network complexity. One feature selection algorithm via concave minimization (FSV) reduced cross-validation error on a cancer prognosis database by 35.4% while reducing problem features from 32 to 4. Feature selection is an important problem in machine learning [18, 15, 16, 17, 33]. In its basic form the problem consists of eliminating as many of the features in a given problem as possible, while still carrying out a preassigned task with acceptable accuracy. Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. In the present work, our task is to discriminate between two given sets in an n-dimensional feature space by using as few of the given features as possible. We shall formulate this problem as a mathematical program with a parametric objective function that will attempt to achieve this task by generating a separating plane in a feature space of as small a dimension as possible while minimizing the average distance of misclassified points to the plane. One of the computational experiments that we carried out on our feature selection procedure showed its effectiveness, not only in minimizing the number of features selected, but also in quickly recognizing and removing spurious random features that were introduced. Thus, on the Wisconsin Prognosis Breast Cancer WPBC database [36] with a feature space of 32 dimensions and 6 random features added, one of our algorithms FSV (11) immediately removed the 6 random features as well as 28 of the original features resulting in a separating plane in a 4-dimensional reduced feature space. By using tenfold cross-validation [35], separation error in the 4-dimensional space was reduced 35.4% from the corresponding error in the original problem space. (See Section 3 for details.) We note that mathematical programming approaches to the feature selection problem have been recently proposed in [4, 22]. Even though the approach of [4] is based on an LPEC formulation, both the LPEC and its method of solution are different from the ones used here. The polyhedral concave minimization approach of [22] is principally involved with theoretical considerations of one specific algorithm and no cross-validatory results are given. Other effective computational applications of mathematical programming to neural networks are given in [30, 26]. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> K. P. BENNETT and O. L. MANGASARIAN, </author> <year> 1992. </year> <title> Neural Network Training via Linear Programming, </title> <booktitle> in Advances in Optimization and Parallel Computing, </booktitle> <editor> P. M. Pardalos (ed.), </editor> <publisher> North Holland, Amsterdam, </publisher> <pages> 56-67. </pages>
Reference-contexts: Formulation (4) is equivalent to the following robust linear programming formulation proposed in <ref> [1] </ref> and utilized effectively to solve real-world problems [23]: min ( m e T z fi fi fi ) In order to suppress as many of the components of w as possible we introduce an extra term with parameter 2 [0; 1) into the objective of (5) while weighting the original <p> Several of the features are correlated and hence provide redundant information. Each of the six random features for the WPBC dataset were generated first by selecting a random number from a uniform distribution on <ref> [0; 1] </ref>, multiplying it by 3500 (to get the correct scaling), rounding it down to the closest integer, then adding 1. Thus the resulting random feature was a random integer in the range [1; 3500]. <p> Thus the resulting random feature was a random integer in the range <ref> [1; 3500] </ref>. The 34-feature Ionosphere problem has 225 points of radar returns from the ionosphere in one category and 126 points in the other. The 34 original problem features were augmented with 6 random features sampled from a uniform distribution on [1; 1]. <p> The 34-feature Ionosphere problem has 225 points of radar returns from the ionosphere in one category and 126 points in the other. The 34 original problem features were augmented with 6 random features sampled from a uniform distribution on <ref> [1; 1] </ref>. Each problem was solved by the four feature selection algorithms: FSS (10), FSV (11), FSB (14) and OBD 4.1, using the GAMS general algebraic and modeling system [5]. <p> Because the underlying feature selection problem is nonconvex, there is no easy way of obtaining a global solution to it. Our approach has been to start with an initial vector with elements sampled from a uniform distribution on <ref> [1; 1] </ref> and then apply the various proposed iterative algorithms. In some instances, starting with different random initial points and taking the best of the final solutions may be as good or a better strategy.
Reference: 2. <author> K. P. BENNETT and O. L. MANGASARIAN, </author> <year> 1992. </year> <title> Robust Linear Programming Discrimination of Two Linearly Inseparable Sets, </title> <booktitle> Optimization Methods and Software 1, </booktitle> <pages> 23-34. </pages>
Reference-contexts: Two principal reasons for using the 1-norm in (4) are: (i) Problem (4) is reducible to a linear program (5) with many important theoretical properties that make it a very effective computational tool <ref> [2] </ref>. (ii) The 1-norm leads to insensitivity to outliers such as those resulting from distributions with pronounced tails and hence has a similar effect to that of robust regression [13],[11, pp 82-87].
Reference: 3. <author> K. P. BENNETT and O. L. MANGASARIAN, </author> <year> 1993. </year> <title> Bilinear Separation of Two Sets in n-space, </title> <booktitle> Computational Optimization & Applications 2, </booktitle> <pages> 207-227. </pages>
Reference-contexts: To get around this difficulty we reformulate (13) as a parametric bilinear program that can be easily processed by solving a finite succession of linear programs that terminate at a stationary point <ref> [3, Algorithm 2.1] </ref>. <p> Because of the bilinear nature of its objective function, the simple, but fast, bilinear algorithm of <ref> [3, Algorithm 2.1] </ref> can be applied to (14) as follows. 2.3 Bilinear Algorithm (BA) for FSB (14). Choose 2 [0; 1); 2 (0; 1). <p> The parameter was chosen as the smallest in the set f0.05, 0.15, 0.25, : : :, 0.95g such that the following complementarity condition holds: r i+1 T (u i+1 v i+1 ) + u i+1 T 6 . The following finite termination theorem follows from <ref> [3, Theorem 2.1] </ref>. 2.4 Finite Termination of Bilinear Algorithm for FSB (14).
Reference: 4. <author> E. J. BREDENSTEINER and K. P. BENNETT, </author> <year> 1995. </year> <title> Feature Minimization within Decision Trees, </title> <note> Department of Mathematical Sciences Math Report No. 218, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180. </address>
Reference: 5. <author> A. BROOKE, D. KENDRICK, and A. MEERAUS, </author> <year> 1988. </year> <title> GAMS: A User's Guide, </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco. </address>
Reference-contexts: The 34 original problem features were augmented with 6 random features sampled from a uniform distribution on [1; 1]. Each problem was solved by the four feature selection algorithms: FSS (10), FSV (11), FSB (14) and OBD 4.1, using the GAMS general algebraic and modeling system <ref> [5] </ref>. GAMS was interfaced with the OSL simplex solver [14] on a Sun SparcStation 20, in the last three algorithms and with MINOS [28] in the first algorithm.
Reference: 6. <author> CHUNHUI CHEN and O. L. MANGASARIAN, </author> <year> 1995. </year> <title> Hybrid Misclassification Minimization, </title> <type> Technical Report 95-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin. </institution> <note> Advances in Computational Mathematics, to appear. Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: However, this is a much harder NP-complete <ref> [6] </ref> formulation of the problem that we shall not employ here.
Reference: 7. <author> S.-J. CHUNG, </author> <year> 1989. </year> <title> NP-Completeness of the Linear Complementarity Problem, </title> <journal> Journal of Optimization Theory and Applications 60, </journal> <pages> 393-399. </pages>
Reference-contexts: However FSL (13) is difficult 4 to solve computationally, and in fact LPECs are NP-hard in general, because they subsume the general linear complementarity problem which is NP-complete <ref> [7] </ref>. To get around this difficulty we reformulate (13) as a parametric bilinear program that can be easily processed by solving a finite succession of linear programs that terminate at a stationary point [3, Algorithm 2.1].
Reference: 8. <author> R. W. COTTLE, J.-S. PANG, and R. E. STONE, </author> <year> 1992. </year> <title> The Linear Complementarity Problem, </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: 0; 0 u ? r + e 0g (12) With this lemma the FS problem can be written in the completely equivalent form of a linear program with equilibrium constraints (LPEC) as follows. (The "equilibrium" terminology comes from the term involving the ?-condition in the constraints which characterizes complementarity problems <ref> [8, 24] </ref>.) (FSL) min 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : e T y + k fi fi fi fi fi fi fi Bw efl + e z; v w v; 0 u ? r + e 0 &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; 2 [0;
Reference: 9. <author> U. M. GARCIA PALOMARES and O. L. MANGASARIAN, </author> <year> 1976. </year> <title> Superlinearly Convergent Quasi-Newton Algorithms for Nonlinearly Constrained Optimization Problems, </title> <booktitle> Mathematical Programming 11, </booktitle> <pages> 1-13. </pages>
Reference-contexts: In fact, all our computational implementations terminated finitely without this transformation. We first consider the FSS problem (10). Because its objective has no convexity or concavity properties, we prescribe the locally fast iterative quadratic programming (IQP) algorithm <ref> [9, 10] </ref> which is the equivalent of a Newton or quasi-Newton method, and consists of taking a quadratic approximation of the objective function of (15) and solving the resulting quadratic program for a potential next iterate, which becomes the next iterate if it is the best point along the line joining
Reference: 10. <author> S.-P. HAN, </author> <year> 1976. </year> <title> Superlinearly Convergent Variable Metric Algorithms for General Nonlinear Programming Problems, </title> <booktitle> Mathematical Programming 11, </booktitle> <pages> 263-282. </pages>
Reference-contexts: In fact, all our computational implementations terminated finitely without this transformation. We first consider the FSS problem (10). Because its objective has no convexity or concavity properties, we prescribe the locally fast iterative quadratic programming (IQP) algorithm <ref> [9, 10] </ref> which is the equivalent of a Newton or quasi-Newton method, and consists of taking a quadratic approximation of the objective function of (15) and solving the resulting quadratic program for a potential next iterate, which becomes the next iterate if it is the best point along the line joining
Reference: 11. <author> M. H. HASSOUN, </author> <year> 1995. </year> <title> Fundamentals of Artificial Neural Networks, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: 12. <author> J. HERTZ, A. KROGH, and R. G. PALMER, </author> <year> 1991. </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference-contexts: Because of the discontinuity in the step function term e T v fl , the FS problem will be approximated by smoothing the step function v fl by using either the standard sigmoid function of neural networks <ref> [31, 12] </ref> or by using a concave exponential approximation on the nonnegative real line [22].
Reference: 13. <author> P. J. HUBER, </author> <year> 1981. </year> <title> Robust Statistics, </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference: 14. <institution> IBM OPTIMIZATION SUBROUTINE LIBRARY, </institution> <year> 1994. </year> <title> GAMS The Solver Manuals: OSL, GAMS Development Corporation, </title> <address> Washington, D.C. </address>
Reference-contexts: Each problem was solved by the four feature selection algorithms: FSS (10), FSV (11), FSB (14) and OBD 4.1, using the GAMS general algebraic and modeling system [5]. GAMS was interfaced with the OSL simplex solver <ref> [14] </ref> on a Sun SparcStation 20, in the last three algorithms and with MINOS [28] in the first algorithm. Each of Figures 1 to 8 plots the average correctness (percentage of correctly classified points) of ten cross-validation runs versus the number of features retained by each feature selection algorithm.
Reference: 15. <author> G. H. JOHN, R. KOHAVI, and K. PFLEGER, </author> <year> 1994. </year> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> in Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 15 </month>
Reference: 16. <author> K. KIRA and L. RENDELL, </author> <year> 1992. </year> <title> The Feature Selection Problem: Traditional Methods and a New Algorithm, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 129-134. </pages>
Reference: 17. <author> J. KITTLER, </author> <year> 1986. </year> <title> Feature Selection and Extraction, in Handbook of Pattern Recognition and Image Processing, </title> <editor> Young & Fu (eds.), </editor> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: 18. <author> Y. LE CUN, J. S. DENKER, and S. A. SOLLA, </author> <year> 1990. </year> <title> Optimal Brain Damage, </title> <booktitle> in Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <editor> D. S. Touretzky (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 598-605. </pages>
Reference-contexts: next section, we will briefly describe effective computational algorithms for solving each of the three feature selection problem reformulations: FSS (10), FSV (11) and FSB (14), and will test these algorithms in the subsequent section on some public real-world databases and compare them with an adaptation of the OBD method <ref> [18] </ref>. 2 Algorithms for the Feature Selection Problem By replacing the variables (w; fl) by the nonnegative variables (w 1 ; fl 1 ; 1 ) using the standard transformation w = w 1 e 1 ; fl = fl 1 1 ; the problems FSS (10), FSV (11) and FSB <p> If we attempt to model exactly the discontinuous squashing term e T jwj fl we arrive at the FSL model (13) with equilibrium constraints and the bilinear model (14) of the FSB Algorithm. Another algorithm, the OBD Algorithm 3.1, patterned after the Optimal Brain Damage method <ref> [18] </ref>, looks at directional derivatives of the nondifferentiable objective function of the linear separation problem (4) at a solution point, and squashes features with directional derivatives less than a certain tolerance. 3 Computational Test Results We report in this section on numerical tests carried out using the three proposed formulations of <p> directional derivatives less than a certain tolerance. 3 Computational Test Results We report in this section on numerical tests carried out using the three proposed formulations of the feature selection problem: FSS (10), FSV (11), and FSB (14), as well as an adaptation of the optimal brain damage (OBD) method <ref> [18] </ref> to our nondifferentiable problem (4) that we outline in the next paragraph. For each of the FSS, FSV and FSB algorithms, once the features have been selected by the algorithm, the linear program (4) is re-solved with all the non-selected features set to zero. <p> We summarize our version of the OBD algorithm <ref> [18] </ref> for problem (4) as follows. 3.1 Optimal Brain Damage (OBD) Algorithm for (4). <p> FSS FSV FSB OBD Prognosis 3.578 3.298 25.965 2.423 Ionosphere 38.571 11.895 23.630 15.946 Table 3: Solution times in seconds for the four feature selection algorithms for the Prognosis and Ionosphere Problems 13 Acknowledgement We are indebted to Jude Shavlik for suggesting comparisons with the Optimal Brain Damage method <ref> [18] </ref>. This material is based on research supported by Air Force Office of Scientific Research Grants F49620-94-1-0036, F49620-97-1-0326, National Science Foundation Grant CCR-9322479 and National Institutes of Health INRSA Fellowship 1 F32 CA 68690-01. 14
Reference: 19. <author> Z.-Q. LUO, J.-S. PANG, D. RALPH, and S.-Q. WU, </author> <year> 1996. </year> <title> Mathematical Programs with Equilibrium Constraints, </title> <booktitle> Mathematical Programming 75, </booktitle> <pages> 19-76. </pages>
Reference-contexts: Aw + efl + e y; y 0; z 0; 9 &gt; &gt; &gt; ; We now make use of the following lemma [22, Lemma 2.3] in order to get an exact reformulation of (7), without the step function v fl , as a linear program with equilibrium constraints (LPEC) <ref> [21, 19] </ref>. Lemma 1.1 Let a 2 R m .
Reference: 20. <author> O. L. MANGASARIAN, </author> <year> 1993. </year> <title> Mathematical Programming in Neural Networks, </title> <journal> ORSA Journal on Computing 5, </journal> <pages> 349-360. </pages>
Reference: 21. <author> O. L. MANGASARIAN, </author> <year> 1994. </year> <title> Misclassification Minimization, </title> <journal> Journal of Global Optimization 5, </journal> <pages> 309-323. </pages>
Reference-contexts: We thus attempt to satisfy (3) in some approximate sense, for example, by minimizing some norm of the average violations of (3) such as min f (w; fl) := min 1 k (Aw + efl + e) + k 1 + k Other <ref> [21] </ref> approximate ways of satisfying (3) consist of minimizing the number misclassified points by the plane P . However, this is a much harder NP-complete [6] formulation of the problem that we shall not employ here. <p> Aw + efl + e y; y 0; z 0; 9 &gt; &gt; &gt; ; We now make use of the following lemma [22, Lemma 2.3] in order to get an exact reformulation of (7), without the step function v fl , as a linear program with equilibrium constraints (LPEC) <ref> [21, 19] </ref>. Lemma 1.1 Let a 2 R m .
Reference: 22. <author> O. L. MANGASARIAN, </author> <year> 1996. </year> <title> Machine Learning via Polyhedral Concave Minimization, in Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </title> <editor> H. Fischer, B. Riedmueller, and S. Schae*er (eds.), </editor> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Heidelberg. </address> <note> Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: of the discontinuity in the step function term e T v fl , the FS problem will be approximated by smoothing the step function v fl by using either the standard sigmoid function of neural networks [31, 12] or by using a concave exponential approximation on the nonnegative real line <ref> [22] </ref>. <p> 2 [0; 1) (10) w;fl;y;z;v &gt; &gt; &gt; &lt; (1 )( m e T z ) + (n e T " ffv ) fi fi fi fi Aw + efl + e y; y 0; z 0; 9 &gt; &gt; &gt; ; We now make use of the following lemma <ref> [22, Lemma 2.3] </ref> in order to get an exact reformulation of (7), without the step function v fl , as a linear program with equilibrium constraints (LPEC) [21, 19]. Lemma 1.1 Let a 2 R m . <p> e z; v w v; 0 u ? r + e 0 &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; 2 [0; 1) (13) This LPEC reformulation of the FS problem (7) has a number of important consequences, such as existence of a solution to the FS problem (7) <ref> [22, Proposition 2.5] </ref>. However FSL (13) is difficult 4 to solve computationally, and in fact LPECs are NP-hard in general, because they subsume the general linear complementarity problem which is NP-complete [7]. <p> We now consider the FSV problem (11). Because its objective function is a differentiable concave function, and has a vertex solution in the formulation (15), we use a stepless successive linear approximation algorithm. The algorithm solves a finite sequence of linear programs and terminates at a stationary point <ref> [22] </ref>. We outline the algorithm now. 2.1 Successive Linearization Algorithm (SLA) for FSV (11). Choose 2 [0; 1). Start with a random (w 0 ; fl 0 ; y 0 ; z 0 ; v 0 ). <p> y; y 0; z 0; 9 &gt; &gt; &gt; ; Stop if (w i ; fl i ; y i ; z i ; v i ) is feasible and (1 )( m e T (z i+1 z i ) ) + ff (" ffv i It can be shown <ref> [22, Theorem 4.2] </ref> that this algorithm terminates in a finite number of steps at a stationary point, which may be a global solution as well. We state this result as follows. 2.2 SLA Finite Termination for FSV (11).
Reference: 23. <author> O. L. MANGASARIAN, W. N. STREET, and W. H. WOLBERG, </author> <year> 1995. </year> <title> Breast Cancer Diagnosis and Prognosis via Linear Programming, </title> <journal> Operations Research 43, </journal> <pages> 570-577. </pages>
Reference-contexts: Formulation (4) is equivalent to the following robust linear programming formulation proposed in [1] and utilized effectively to solve real-world problems <ref> [23] </ref>: min ( m e T z fi fi fi ) In order to suppress as many of the components of w as possible we introduce an extra term with parameter 2 [0; 1) into the objective of (5) while weighting the original function by (1 ) as follows: min 8
Reference: 24. <author> L. MATHIESEN, </author> <year> 1987. </year> <title> An Algorithm Based on a Sequence of Linear Complementarity Problems Applied to a Walrasian Equilibrium Model: An Example, </title> <booktitle> Mathematical Programming 37, </booktitle> <pages> 1-18. </pages>
Reference-contexts: 0; 0 u ? r + e 0g (12) With this lemma the FS problem can be written in the completely equivalent form of a linear program with equilibrium constraints (LPEC) as follows. (The "equilibrium" terminology comes from the term involving the ?-condition in the constraints which characterizes complementarity problems <ref> [8, 24] </ref>.) (FSL) min 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : e T y + k fi fi fi fi fi fi fi Bw efl + e z; v w v; 0 u ? r + e 0 &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; 2 [0;
Reference: 25. <author> MATHWORKS, INC., </author> <year> 1991. </year> <title> PRO-MATLAB for UNIX Computers, The Mathworks, </title> <publisher> Inc., </publisher> <address> South Natick, MA 01760. </address>
Reference-contexts: = t (v; ff) := e " ffv ; ff &gt; 0 (9) Here e is a vector of ones, " is the base of natural logarithms, ff is a positive parameter, and the application of either function to a vector is interpreted componentwise as in the standard MATLAB notation <ref> [25] </ref>. Advantages of the exponential (9) over the standard sigmoid (8) are its simplicity and concavity. These properties lead to a finitely terminating algorithm and a more accurate representation of the step function v fl at 0, because t (0; ff) = 0, whereas s (0; ff) = 1 e.
Reference: 26. <author> S. MUKHOPADHYAY, A. ROY, and S. GOVIL, </author> <year> 1993. </year> <title> A Polynomial Time Algorithm for Generating Neural Networks for Pattern Classification: Its Stability Properties and Some Test Results, </title> <booktitle> Neural Computation 5, </booktitle> <pages> 317-330. </pages>
Reference: 27. <author> P. M. MURPHY and D. W. AHA, </author> <year> 1992. </year> <title> UCI Repository of Machine Learning Databases, </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, www.ics.uci.edu/AI/ML/MLDBRepository.html. </address>
Reference-contexts: re-solve (4) or (5) where: I := fi fi fi fi fi where f (w; fl) is the objective function of (4). 7 Figures 1 to 8 and Tables 1 to 3 summarize our results for two problems: the Wisconsin Prognostic Breast Cancer (WPBC) problem [36] and the Ionosphere problem <ref> [34, 27] </ref>. The 32-feature WPBC problem has 28 points in category one of breast cancer patients for which the cancer recurred within 24 months and category two of 119 patients for which the cancer did not recur within 24 months.
Reference: 28. <author> B. A. MURTAGH and M. A. SAUNDERS, </author> <year> 1983, 1992. </year> <title> MINOS 5.0 User's Guide, </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University. </institution> <note> MINOS 5.4 Release Notes. </note>
Reference-contexts: This algorithm has been implemented professionally using powerful linear algebra packages in the MINOS software package <ref> [28] </ref>. Thus, MINOS was used to solve the FSS problem (10). We now consider the FSV problem (11). Because its objective function is a differentiable concave function, and has a vertex solution in the formulation (15), we use a stepless successive linear approximation algorithm. <p> GAMS was interfaced with the OSL simplex solver [14] on a Sun SparcStation 20, in the last three algorithms and with MINOS <ref> [28] </ref> in the first algorithm. Each of Figures 1 to 8 plots the average correctness (percentage of correctly classified points) of ten cross-validation runs versus the number of features retained by each feature selection algorithm.
Reference: 29. <author> B. T. POLYAK, </author> <year> 1987. </year> <title> Introduction to Optimization, Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York. </address>
Reference-contexts: Because the objective function of our separating plane problem (4) is piecewise-linear, the second derivatives needed by the OBD method do not exist. However, at a solution point ( w; fl) of (4), obtained by solving the linear program (5), even though a subgradient is zero <ref> [29, Theorem 1, p. 133] </ref>, the directional derivatives of f (w; fl) of (4) in the directions of the components w i of w are generally not zero.
Reference: 30. <author> A. ROY, L. S. KIM, and S. MUKHOPADHYAY, </author> <year> 1993. </year> <title> A Polynomial Time Algorithm for the Construction and Training of a Class of Multilayer Perceptrons, </title> <booktitle> Neural Networks 6, </booktitle> <pages> 535-545. 16 </pages>
Reference: 31. <author> D. E. RUMELHART, G. E. HINTON, and R. J. WILLIAMS, </author> <year> 1986. </year> <title> Learning Internal Represen--tations by Error Propagation, in Parallel Distributed Processing, </title> <editor> D. E. Rumelhard and J. L. McClelland (eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Because of the discontinuity in the step function term e T v fl , the FS problem will be approximated by smoothing the step function v fl by using either the standard sigmoid function of neural networks <ref> [31, 12] </ref> or by using a concave exponential approximation on the nonnegative real line [22].
Reference: 32. <author> D. E. RUMELHART and J. L. MCCLELLAND, </author> <year> 1986. </year> <title> Parallel Distributed Processing, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: 33. <author> W. SIEDLECKI and J. SKLANSKY, </author> <year> 1988. </year> <title> On Automatic Feature Selection, </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 2, </journal> <pages> 197-220. </pages>
Reference: 34. <author> V. G. SIGILLITO, S. P. WING, L. V. HUTTON, and K. B. BAKER, </author> <year> 1989. </year> <title> Classification of Radar Returns from the Ionosphere Using Neural Networks. </title> <type> APL Technical Digest 10, </type> <institution> Johns Hopkins University, Baltimore, MD. </institution>
Reference-contexts: re-solve (4) or (5) where: I := fi fi fi fi fi where f (w; fl) is the objective function of (4). 7 Figures 1 to 8 and Tables 1 to 3 summarize our results for two problems: the Wisconsin Prognostic Breast Cancer (WPBC) problem [36] and the Ionosphere problem <ref> [34, 27] </ref>. The 32-feature WPBC problem has 28 points in category one of breast cancer patients for which the cancer recurred within 24 months and category two of 119 patients for which the cancer did not recur within 24 months.
Reference: 35. <author> M. STONE, </author> <year> 1974. </year> <title> Cross-validatory Choice and Assessment of Statistical Predictions, </title> <journal> Journal of the Royal Statistical Society 36, </journal> <pages> 111-147. </pages>
Reference: 36. <author> W. H. WOLBERG, W. N. STREET, and O. L. MANGASARIAN, </author> <year> 1995. </year> <institution> WPBC: Wisconsin Prognostic Breast Cancer Database, Computer Sciences Department, University of Wisconsin, Madison. </institution> <address> ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/WPBC/ 17 </address>
Reference-contexts: for i 2 I and re-solve (4) or (5) where: I := fi fi fi fi fi where f (w; fl) is the objective function of (4). 7 Figures 1 to 8 and Tables 1 to 3 summarize our results for two problems: the Wisconsin Prognostic Breast Cancer (WPBC) problem <ref> [36] </ref> and the Ionosphere problem [34, 27]. The 32-feature WPBC problem has 28 points in category one of breast cancer patients for which the cancer recurred within 24 months and category two of 119 patients for which the cancer did not recur within 24 months.
References-found: 36

