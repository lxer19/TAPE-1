URL: http://www.cs.huji.ac.il/~feit/parsched/p-96-6.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched96.html
Root-URL: http://www.cs.huji.ac.il
Email: feit@cs.huji.ac.il  
Title: Packing Schemes for Gang Scheduling  
Author: Dror G. Feitelson 
Web: http://www.cs.huji.ac.il/~feit  
Address: 91904 Jerusalem, Israel  
Affiliation: Institute of Computer Science The Hebrew University,  
Abstract: Jobs that do not require all processors in the system can be packed together for gang scheduling. We examine accounting traces from several parallel computers to show that indeed many jobs have small sizes and can be packed together. We then formulate a number of such packing algorithms, and evaluate their effectiveness using simulations based on our workload study. The results are that two algorithms are the best: either perform the mapping based on a buddy system of processors, or use migration to re-map the jobs more tightly whenever a job arrives or terminates. Other approaches, such as mapping to the least loaded PEs, proved to be counterproductive. The buddy system approach depends on the capability to gang-schedule jobs in multiple slots, if there is space. The migration algorithm is more robust, but is expected to suffer greatly due to the overhead of the migration itself. In either case fragmentation is not an issue, and utilization may top 90% with sufficiently high loads.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Barak and A. Shiloh, </author> <title> "A distributed load-balancing policy for a multicom-puter". </title> <journal> Software | Pract. & Exp. </journal> <volume> 15(9), </volume> <pages> pp. 901-913, </pages> <month> Sep </month> <year> 1985. </year>
Reference-contexts: It is debatable whether this algorithm is realistic, because of the expected overhead, especially on distributed memory machines. It is true that systems that support migration have been implemented successfully <ref> [1, 6] </ref>, but these systems do not attempt to perform migration at such a high rate. However, this algorithm is useful as a bound on the performance that is obtainable. 3 Workload Modeling The most straightforward way to evaluate scheduling algorithms without a full scale implementation is through simulations.
Reference: 2. <author> J. M. Barton and N. Bitar, </author> <title> "A scalable multi-discipline, multiple-processor scheduling framework for IRIX ". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 45-69, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Gang scheduling is a prominent feature of the Connection Machine CM-5 system [28], and is available on the Intel Paragon [17], the Meiko CS-2, and multiprocessor SGI workstations <ref> [2] </ref>. It has also been used extensively in a home-grown system on a BBN Butterfly at Lawrence Livermore Labs [13], which has recently been ported to their new Cray T3D system. The main drawback of using gang scheduling is the problem of fragmentation.
Reference: 3. <author> S-H. Chiang, R. K. Mansharamani, and M. K. Vernon, </author> <title> "Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Naturally, the quality of the results depends on the quality of the inputs to the simulation. An important issue is the workload model. It has often been stated that there is no reliable information about workloads on parallel machine <ref> [23, 20, 19, 3] </ref>. However, this is in fact not true. Like uniprocessor systems, most parallel systems maintain administrative traces of all jobs run on the system. Analyzing these traces reveals a wealth of information about the workload.
Reference: 4. <author> E. G. Coffman, Jr., M. R. Garey, and D. S. Johnson, </author> <title> "Approximation algorithms for bin-packing | an updated survey". In Algorithm Design for Computer Systems Design, </title> <editor> G. Ausiello, M. Lucertini, and P. </editor> <booktitle> Serafini (eds.), </booktitle> <pages> pp. 49-106, </pages> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Specifically, our algorithm re-maps all jobs upon every job arrival and termination, using a first-fit-decreasing allocation to slots <ref> [4] </ref> (this algorithm is optimal if all job sizes divide each other, e.g. if they are powers of two [5]). It is debatable whether this algorithm is realistic, because of the expected overhead, especially on distributed memory machines.
Reference: 5. <author> E. G. Coffman, Jr., M. R. Garey, and D. S. Johnson, </author> <title> "Bin packing with divisible item sizes". </title> <journal> J. Complex. </journal> <volume> 3(4), </volume> <pages> pp. 406-428, </pages> <month> Dec </month> <year> 1987. </year>
Reference-contexts: Specifically, our algorithm re-maps all jobs upon every job arrival and termination, using a first-fit-decreasing allocation to slots [4] (this algorithm is optimal if all job sizes divide each other, e.g. if they are powers of two <ref> [5] </ref>). It is debatable whether this algorithm is realistic, because of the expected overhead, especially on distributed memory machines. It is true that systems that support migration have been implemented successfully [1, 6], but these systems do not attempt to perform migration at such a high rate.
Reference: 6. <author> F. Douglis and J. Ousterhout, </author> <title> "Process migration in the Sprite operating system". </title> <booktitle> In 7th Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 18-25, </pages> <month> Sep </month> <year> 1987. </year>
Reference-contexts: It is debatable whether this algorithm is realistic, because of the expected overhead, especially on distributed memory machines. It is true that systems that support migration have been implemented successfully <ref> [1, 6] </ref>, but these systems do not attempt to perform migration at such a high rate. However, this algorithm is useful as a bound on the performance that is obtainable. 3 Workload Modeling The most straightforward way to evaluate scheduling algorithms without a full scale implementation is through simulations.
Reference: 7. <author> D. G. Feitelson, </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Research Report RC 19790 (87657), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Oct </month> <year> 1994. </year>
Reference-contexts: As such they must provide convenient scheduling facilities that will handle the allocation of resources to different user jobs. A large number of scheduling schemes have been proposed for parallel machines <ref> [7, 11] </ref>. One of these is gang scheduling, where all the threads of a parallel job are scheduled for simultaneous execution on distinct PEs [24]. If the total number of threads in all the jobs exceeds the number of PEs in the system, time slicing is used.
Reference: 8. <author> D. G. Feitelson and B. Nitzberg, </author> <title> "Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 337-360, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The traced systems are summarized in Table 1. system trace comments 128-node iPSC/860 42050 jobs, 4Q93 Intel scheduler [16] NASA Ames 10821 parallel user jobs analysis described in <ref> [8] </ref> 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no <p> First, batch jobs tend to run longer than interactive ones. Second, batch queues are often enabled for execution only during the night, thus creating a daily cycle of completely different workloads at prime time and non-prime time. This effect is very pronounced in the NASA Ames trace <ref> [8] </ref>, and can also be seen in the SDSC trace. The reason to delay batch jobs to non-prime time is that in systems that use space slicing without preemption, the decision to run a batch job might block future requests to run interactive jobs. <p> In our simulations of the buddy and migration schemes, the system only saturated when the utilization was higher than 95%, which is significantly higher than the 50-80% range reported for production systems using static partitioning <ref> [8, 29, 15] </ref>. This means that fragmentation is less of a concern than is sometimes thought. The high utilization can be attributed to two factors: first, when using time slicing, bad scheduling decisions are less harmful than when using static partitioning, because they only affect one scheduling slot.
Reference: 9. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Distributed hierarchical control for parallel processing". </title> <booktitle> Computer 23(5), </booktitle> <pages> pp. 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These groups are organized as a buddy system, based on concepts that were originally developed for memory allocation [18, 25], and following the PE allocation mechanism in the Distributed Hierarchical Control scheme (DHC) <ref> [9, 10] </ref>. Specifically, the PEs are partitioned recursively into groups that are powers of two. Logically, each group has a controller, thus creating a hierarchy of such controllers. <p> These are selected in groups that are powers of two, based on the loads on the controller's descendents. This scheme is equivalent to the "minimal fragmentation" scheme that was shown to be advantageous for DHC <ref> [9, 10] </ref>. The remaining PEs are not reserved or allocated in any sense, and can later be assigned to other (smaller) jobs.
Reference: 10. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Evaluation of design choices for gang scheduling using distributed hierarchical control". </title> <journal> J. Parallel & Distributed Comput., </journal> <note> 1996. to appear. </note>
Reference-contexts: These groups are organized as a buddy system, based on concepts that were originally developed for memory allocation [18, 25], and following the PE allocation mechanism in the Distributed Hierarchical Control scheme (DHC) <ref> [9, 10] </ref>. Specifically, the PEs are partitioned recursively into groups that are powers of two. Logically, each group has a controller, thus creating a hierarchy of such controllers. <p> These are selected in groups that are powers of two, based on the loads on the controller's descendents. This scheme is equivalent to the "minimal fragmentation" scheme that was shown to be advantageous for DHC <ref> [9, 10] </ref>. The remaining PEs are not reserved or allocated in any sense, and can later be assigned to other (smaller) jobs.
Reference: 11. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Parallel job scheduling: issues and approaches". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 1-18, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Com--puter Science Vol. 949. </note>
Reference-contexts: As such they must provide convenient scheduling facilities that will handle the allocation of resources to different user jobs. A large number of scheduling schemes have been proposed for parallel machines <ref> [7, 11] </ref>. One of these is gang scheduling, where all the threads of a parallel job are scheduled for simultaneous execution on distinct PEs [24]. If the total number of threads in all the jobs exceeds the number of PEs in the system, time slicing is used.
Reference: 12. <author> D. G. Feitelson and L. Rudolph, </author> <booktitle> "Wasted resources in gang scheduling ". In 5th Jerusalem Conf. Information Technology, </booktitle> <pages> pp. 127-136, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Specifically, it may happen that a number of jobs are scheduled to run, and a few PEs are left over, but they are insufficient for any of the other queued jobs. The severity of this problem depends to a large degree on the distribution of job sizes <ref> [12] </ref>. One solution, used in the CM-5, is to use all the PEs for each job, rather than allowing subsets to be used. In this paper, we investigate alternative solutions based on different schemes for packing the jobs together for scheduling.
Reference: 13. <author> B. Gorda and R. Wolski, </author> <title> "Time sharing massively parallel machines". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <month> Aug </month> <year> 1995. </year>
Reference-contexts: Gang scheduling is a prominent feature of the Connection Machine CM-5 system [28], and is available on the Intel Paragon [17], the Meiko CS-2, and multiprocessor SGI workstations [2]. It has also been used extensively in a home-grown system on a BBN Butterfly at Lawrence Livermore Labs <ref> [13] </ref>, which has recently been ported to their new Cray T3D system. The main drawback of using gang scheduling is the problem of fragmentation. <p> jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace <ref> [13] </ref> 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace [15] 96-node Paragon 1723 jobs Intel scheduler ETH Zurich no direct access to trace [27] Table 1.
Reference: 14. <author> B. C. Gorda and E. D. Brooks III, </author> <title> Gang Scheduling a Parallel Machine. </title> <type> Technical Report UCRL-JC-107020, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: user jobs analysis described in [8] 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler <ref> [14] </ref> LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace [15] 96-node Paragon 1723 jobs Intel scheduler ETH Zurich no direct access to trace [27] Table 1.
Reference: 15. <author> S. Hotovy, </author> <title> "Workload evolution on the Cornell Theory Center IBM SP2". In Job Scheduling Strategies for Parallel Processing II, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science. </note>
Reference-contexts: SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace <ref> [15] </ref> 96-node Paragon 1723 jobs Intel scheduler ETH Zurich no direct access to trace [27] Table 1. <p> Fig. 3. Histograms of job sizes on the ANL SP1 and SDSC Paragon. 16MB, so for many jobs the effective maximum is 256. Also, 32 nodes were typically reserved for interactive use, so using all 400 nodes required turning off interactive use. The Cornell SP2 is also heterogeneous <ref> [15] </ref>, and was never configured for using all 512 nodes. <p> Using linear regression, the harmonic order ( in the equation for the probability distribution) is around 2:2 for both cases, after deleting outliers that appear only a small number of times. Similar results were obtained for the Cornell trace <ref> [15] </ref>. 3.4 Job Classes In many cases jobs in a system can be classified into a number of classes, and such classification is often an explicit goal of workload analysis. <p> In our simulations of the buddy and migration schemes, the system only saturated when the utilization was higher than 95%, which is significantly higher than the 50-80% range reported for production systems using static partitioning <ref> [8, 29, 15] </ref>. This means that fragmentation is less of a concern than is sometimes thought. The high utilization can be attributed to two factors: first, when using time slicing, bad scheduling decisions are less harmful than when using static partitioning, because they only affect one scheduling slot.
Reference: 16. <author> Intel Corp., </author> <title> iPSC/860 Multi-User Accounting, Control, and Scheduling Utilities Manual. Order number 312261-002, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: For this study we used information derived from traces gathered on 6 different systems, all of which supported a real production workload. The traced systems are summarized in Table 1. system trace comments 128-node iPSC/860 42050 jobs, 4Q93 Intel scheduler <ref> [16] </ref> NASA Ames 10821 parallel user jobs analysis described in [8] 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992
Reference: 17. <author> Intel Supercomputer Systems Division, </author> <title> Paragon User's Guide. Order number 312489-003, </title> <month> Jun </month> <year> 1994. </year>
Reference-contexts: However, the context switching is coordinated across the PEs, such that all the threads in a job are scheduled and de-scheduled at the same time. Gang scheduling is a prominent feature of the Connection Machine CM-5 system [28], and is available on the Intel Paragon <ref> [17] </ref>, the Meiko CS-2, and multiprocessor SGI workstations [2]. It has also been used extensively in a home-grown system on a BBN Butterfly at Lawrence Livermore Labs [13], which has recently been ported to their new Cray T3D system. <p> 1. system trace comments 128-node iPSC/860 42050 jobs, 4Q93 Intel scheduler [16] NASA Ames 10821 parallel user jobs analysis described in [8] 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler <ref> [29, 17] </ref> San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace [15] 96-node Paragon
Reference: 18. <author> K. C. Knowlton, </author> <title> "A fast storage allocator". </title> <journal> Comm. ACM 8(10), </journal> <pages> pp. 623-625, </pages> <month> Oct </month> <year> 1965. </year>
Reference-contexts: These groups are organized as a buddy system, based on concepts that were originally developed for memory allocation <ref> [18, 25] </ref>, and following the PE allocation mechanism in the Distributed Hierarchical Control scheme (DHC) [9, 10]. Specifically, the PEs are partitioned recursively into groups that are powers of two. Logically, each group has a controller, thus creating a hierarchy of such controllers.
Reference: 19. <author> P. Krueger, T-H. Lai, and V. A. Radiya, </author> <title> "Processor allocation vs. job scheduling on hypercube computers". </title> <booktitle> In 11th Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 394-401, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Naturally, the quality of the results depends on the quality of the inputs to the simulation. An important issue is the workload model. It has often been stated that there is no reliable information about workloads on parallel machine <ref> [23, 20, 19, 3] </ref>. However, this is in fact not true. Like uniprocessor systems, most parallel systems maintain administrative traces of all jobs run on the system. Analyzing these traces reveals a wealth of information about the workload.
Reference: 20. <author> S. T. Leutenegger and M. K. Vernon, </author> <title> "The performance of multiprogrammed multiprocessor scheduling policies". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Naturally, the quality of the results depends on the quality of the inputs to the simulation. An important issue is the workload model. It has often been stated that there is no reliable information about workloads on parallel machine <ref> [23, 20, 19, 3] </ref>. However, this is in fact not true. Like uniprocessor systems, most parallel systems maintain administrative traces of all jobs run on the system. Analyzing these traces reveals a wealth of information about the workload.
Reference: 21. <author> D. Lifka, </author> <title> "The ANL/IBM SP scheduling system". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 295-303, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The traced systems are summarized in Table 1. system trace comments 128-node iPSC/860 42050 jobs, 4Q93 Intel scheduler [16] NASA Ames 10821 parallel user jobs analysis described in [8] 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler <ref> [21] </ref> Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler [29, 17] San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs,
Reference: 22. <author> M. H. MacDougall, </author> <title> Simulating Computer Systems: Techniques and Tools. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Each data point represents the average of 30 experiments, each including 1000 job terminations. An additional initial experiment was discarded in order to account for simulation warmup. 95% confidence intervals were computed using the batch means approach <ref> [22] </ref>. The simulation itself is event-driven, where events are job arrival and termination. The average interarrival time is changed to simulate different load conditions. Between consecutive events, jobs are assigned constant run fractions according to the number of slots in which they can run.
Reference: 23. <author> S. Majumdar, D. L. Eager, and R. B. Bunt, </author> <title> "Scheduling in multiprogrammed parallel systems". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Naturally, the quality of the results depends on the quality of the inputs to the simulation. An important issue is the workload model. It has often been stated that there is no reliable information about workloads on parallel machine <ref> [23, 20, 19, 3] </ref>. However, this is in fact not true. Like uniprocessor systems, most parallel systems maintain administrative traces of all jobs run on the system. Analyzing these traces reveals a wealth of information about the workload.
Reference: 24. <author> J. K. Ousterhout, </author> <title> "Scheduling techniques for concurrent systems". </title> <booktitle> In 3rd Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 22-30, </pages> <month> Oct </month> <year> 1982. </year>
Reference-contexts: A large number of scheduling schemes have been proposed for parallel machines [7, 11]. One of these is gang scheduling, where all the threads of a parallel job are scheduled for simultaneous execution on distinct PEs <ref> [24] </ref>. If the total number of threads in all the jobs exceeds the number of PEs in the system, time slicing is used. However, the context switching is coordinated across the PEs, such that all the threads in a job are scheduled and de-scheduled at the same time. <p> Section 4 then describes the experimental results obtained when using the different packing schemes in conjunction with the workload model. The conclusions are presented in Section 5. 2 Packing Schemes Our work is done within the framework of a gang scheduling system based on the matrix algorithm by Ousterhout <ref> [24] </ref>. This algorithm views scheduling space as a matrix, where rows represent time slots and columns represent PEs. Each job is allocated to a single row. If space permits, a number of jobs may be allocated to the same row.
Reference: 25. <author> J. L. Peterson and T. A. Norman, </author> <title> "Buddy systems". </title> <journal> Comm. ACM 20(6), </journal> <pages> pp. 421-431, </pages> <month> Jun </month> <year> 1977. </year>
Reference-contexts: These groups are organized as a buddy system, based on concepts that were originally developed for memory allocation <ref> [18, 25] </ref>, and following the PE allocation mechanism in the Distributed Hierarchical Control scheme (DHC) [9, 10]. Specifically, the PEs are partitioned recursively into groups that are powers of two. Logically, each group has a controller, thus creating a hierarchy of such controllers.
Reference: 26. <author> D. L. Russell, </author> <title> "Internal fragmentation in a class of buddy systems". </title> <journal> SIAM J. Comput. </journal> <volume> 6(4), </volume> <pages> pp. 607-621, </pages> <month> Dec </month> <year> 1977. </year>
Reference-contexts: It is seen that some sequences are extremely long (the maximum observed is 402 runs on the ANL SP1). The fact that the slope is a straight line in these log-log plots indicates a generalized Zipf distribution (i.e. p (n) / 1=n ) <ref> [30, 26] </ref>. Using linear regression, the harmonic order ( in the equation for the probability distribution) is around 2:2 for both cases, after deleting outliers that appear only a small number of times.
Reference: 27. <author> T. Suzuoka, J. Subhlok, and T. Gross, </author> <title> Evaluating Job Scheduling Techniques for Highly Parallel Computers. </title> <type> Technical Report CMU-CS-95-149, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace [15] 96-node Paragon 1723 jobs Intel scheduler ETH Zurich no direct access to trace <ref> [27] </ref> Table 1. Summary of systems and traces used in workload analysis. 3.1 Distribution of Job Sizes An important feature of the workload model is the distribution of job sizes, in terms of the number of nodes used by each job 3 .
Reference: 28. <institution> Thinking Machines Corp., </institution> <type> Connection Machine CM-5 Technical Summary. </type> <month> Nov </month> <year> 1992. </year>
Reference-contexts: However, the context switching is coordinated across the PEs, such that all the threads in a job are scheduled and de-scheduled at the same time. Gang scheduling is a prominent feature of the Connection Machine CM-5 system <ref> [28] </ref>, and is available on the Intel Paragon [17], the Meiko CS-2, and multiprocessor SGI workstations [2]. It has also been used extensively in a home-grown system on a BBN Butterfly at Lawrence Livermore Labs [13], which has recently been ported to their new Cray T3D system.
Reference: 29. <author> M. Wan, R. Moore, G. Kremenek, and K. Steube, </author> <title> "A batch scheduler for the Intel Paragon MPP system with a non-contiguous node allocation algorithm". In Job Scheduling Strategies for Parallel Processing II, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science. </note>
Reference-contexts: 1. system trace comments 128-node iPSC/860 42050 jobs, 4Q93 Intel scheduler [16] NASA Ames 10821 parallel user jobs analysis described in [8] 128-node IBM SP1 19980 jobs, 12/94-6/95 home grown scheduler [21] Argonne Natl Lab 15654 were parallel submit trace, not run trace 400-node Paragon 32500 jobs, 12/94-4/95 SDSC/Intel scheduler <ref> [29, 17] </ref> San-Diego SC 25867 were parallel 126-node Butterfly 35848 jobs, 1991-1992 home grown gang scheduler [14] LLNL &gt;30000 were parallel no direct access to trace [13] 512-node IBM SP2 17947 jobs, 9/95-11/95 Scheduling by IBM LoadLeveler Cornell Theory Ctr 8598 were parallel no direct access to trace [15] 96-node Paragon <p> In our simulations of the buddy and migration schemes, the system only saturated when the utilization was higher than 95%, which is significantly higher than the 50-80% range reported for production systems using static partitioning <ref> [8, 29, 15] </ref>. This means that fragmentation is less of a concern than is sometimes thought. The high utilization can be attributed to two factors: first, when using time slicing, bad scheduling decisions are less harmful than when using static partitioning, because they only affect one scheduling slot.
Reference: 30. <author> G. K. Zipf, </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley, </publisher> <year> 1949. </year>
Reference-contexts: It is seen that some sequences are extremely long (the maximum observed is 402 runs on the ANL SP1). The fact that the slope is a straight line in these log-log plots indicates a generalized Zipf distribution (i.e. p (n) / 1=n ) <ref> [30, 26] </ref>. Using linear regression, the harmonic order ( in the equation for the probability distribution) is around 2:2 for both cases, after deleting outliers that appear only a small number of times.
References-found: 30

