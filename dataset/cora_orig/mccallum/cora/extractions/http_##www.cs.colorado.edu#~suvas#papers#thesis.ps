URL: http://www.cs.colorado.edu/~suvas/papers/thesis.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Title: RUNTIME LOOP OPTIMIZATIONS FOR LOCALITY AND PARALLELISM  
Author: by Suvas Vajracharya B. S., M. S., 
Degree: 1994 A thesis submitted to the Faculty of the Graduate School of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: 1997  
Address: 1986  
Affiliation: University of Wisconsin-Madison,  University of Colorado,  University of Colorado  Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> L.M. Adams. </author> <title> Iterative Methods for Solving Partial Differential Equations of Elliptic Type. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cam-bridge, Mass., </address> <year> 1950. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 62] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance, and therefore does well under static scheduling schemes. Red/Black SOR is representative of a class of PDE solvers where each element depends only on its neighboring elements.
Reference: [2] <author> L.M. Adams. </author> <title> Iterative Algorithms for Large Sparse Linear Systems on Parallel Computers. </title> <type> PhD thesis, </type> <institution> Univ. of Virginia, Charlottesville, </institution> <year> 1982. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 62] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance, and therefore does well under static scheduling schemes. Red/Black SOR is representative of a class of PDE solvers where each element depends only on its neighboring elements.
Reference: [3] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6) </volume> <pages> 233-246, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: The set of distance vectors makes up the data dependencies that determine the allowable re-ordering transformation. Based on these dependencies, optimizing compilers may make the following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [82, 3] </ref> swaps an inner loop with an outer loop. Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. <p> This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by loop interchange <ref> [3, 82] </ref>. Other compiler transformations attempt to organize loops such that the loops with maximum parallelism are the outer loops. Compiler transformations such as loop normalization and loop fusion can also be used to collapse a multi-dimensional iteration space to a single dimensional iteration space.
Reference: [4] <author> Jennifer M. Anderson, Lance M. Berc, Jeffrey Dean, Sanjay Ghemawat, Monika Henzinger, Shun-Tak Leung, Richard L. Sites, Mark Vandevoorde, Carl Waldspurger, and William Weihl. </author> <title> Continuous profiling: Where have all the cycles gone? In SOSP (To appear in), </title> <month> October </month> <year> 1996. </year>
Reference-contexts: As shown in the table, the time spent for computation is only 4%, whereas the time taken by data cache misses is 76%. To determine where the cycles (cache miss, branch mis-predict, useful computation, etc.) were spent, we used the Digital Continuous Profiling Infrastructure (DCPI) <ref> [4] </ref> available on the Alpha platforms. <p> However, overlapping executions of different phases is only legal if the computation respects the data dependencies between the two phases. CHAPTER 4 EXPERIMENTAL RESULTS 4.1 Experimental Infrastructure and Design 4.1.1 Tools To analyze the performance of various applications and various methods, we used two tools, DCPI <ref> [4] </ref> and ATOM [73], both of which are available on the DEC Alphas. Digital Continuous Profiling Infrastructure (DCPI) runs on the background with low overhead (slowdown of 1-3%) and unobtrusively generates profile data for the applications running on the machines by sampling hardware performance counters available on the Alphas.
Reference: [5] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 10(1) </volume> <pages> 52-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: On multi-programmed multiprocessors, a participating processor maybe swapped out to run a process from a different job, making the problem worse. See figure 3.13. Most of the work on multiprocessors on multiprogrammed environment has been on the operating system or combination of user space/operating 76 77 system. Several researchers <ref> [5, 24, 52, 53] </ref> have focused on ways to establish some co-operation between the application and the operating system kernel. By extending the kernel to communicate with the application, the kernel can avoid pre-empting a task that is in a critical section.
Reference: [6] <author> Robert Babb. </author> <title> Parallel processing with large-grain data flow techniques. </title> <journal> IEEE Computer, </journal> <volume> 17(7) </volume> <pages> 55-61, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: This is possible because the 42 Dude system has been built with the intention of optimizing loop constructs to support data-parallel languages. multiplication 2.5.3 Large-Grain Data flow Babb and DiNucci <ref> [6, 31] </ref> describe a large-grain data flow, or LGDF, where the nodes in the dataflow graph consist of program blocks corresponding to between 5 and 50 (or even more) statements in a higher level programming language. <p> In <ref> [6] </ref>, Babb describes the steps involved in modeling and implementing a Fortran program using LGDF techniques as follows: (a) Draw the data flow graph on paper, perhaps in a hierarchical fashion, that is, each node may itself be exploded to reveal a more detailed data flow graph. (b) Create a wirelist <p> A disadvantage of this approach is the amount of effort required by the programmer. For example, consider the graph shown in Figure 2.6 for a simple matrix/vector multiplication, an example taken from <ref> [6] </ref>.
Reference: [7] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(4) </volume> <pages> 345-420, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: While the generated code is more complex than the original, the new code has better locality and parallelism. A survey of compiler transformations for high performance computing can be found in the works by Bacon et. al. and Wolfe <ref> [7, 82] </ref>. Another important loop transformation is loop fusion.
Reference: [8] <author> C.F. Baillie, Dirk Grunwald, and Suvas Vajracharya. </author> <title> Application of an object-oriented runtime system to a grand challenge 3d multi-grid code. </title> <booktitle> In Proc. Twenty-Ninth Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 256-260, </pages> <year> 1996. </year>
Reference-contexts: Planetary-scale fluid motions in the Earth's atmosphere are important to the study of Earth's climate. A more complete description of QG equations can be found in the work by Baillie et al <ref> [8, 22] </ref> [86]. Here, we concentrate on only the computational aspect of the problem as it relates to the Dude system runtime system.
Reference: [9] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In our work, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 20, 10] </ref> could be used. Here, the runtime system generates descriptors for iteration spaces that are manipulated during runtime by representing them using recursive 23 structures called quadtrees. This allows the runtime system to execute independent portions of multiple loops in parallel.
Reference: [10] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing 115 data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of teh ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 41-53, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In our work, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 20, 10] </ref> could be used. Here, the runtime system generates descriptors for iteration spaces that are manipulated during runtime by representing them using recursive 23 structures called quadtrees. This allows the runtime system to execute independent portions of multiple loops in parallel.
Reference: [11] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: In other cases, when the compiler cannot derive the distance vector, the compiler may be able to derive a direction vector, which describes the direction of the dependence arc from one iteration to another. Techniques for extracting distance and direction vectors from loop nests are well known <ref> [45, 11, 12, 82, 85] </ref>. In our own work, dependencies between iterations are not represented in the form of distance/direction vectors, but in the form of symbolic rules or functions. This implies that the system does not need to solve the data dependence system.
Reference: [12] <author> U. Banerjee. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> J. Supercomp., </journal> <volume> 2(2) </volume> <pages> 133-149, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In other cases, when the compiler cannot derive the distance vector, the compiler may be able to derive a direction vector, which describes the direction of the dependence arc from one iteration to another. Techniques for extracting distance and direction vectors from loop nests are well known <ref> [45, 11, 12, 82, 85] </ref>. In our own work, dependencies between iterations are not represented in the form of distance/direction vectors, but in the form of symbolic rules or functions. This implies that the system does not need to solve the data dependence system.
Reference: [13] <author> Forest Baskett. </author> <title> Keynote address. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: INTRODUCTION 1.1 The Problem Modern high-performance computers promise to deliver unprecedented computing power to scientific and engineering applications. To fully realize the power of modern computers, three primary obstacles must be overcome: * How to reduce/tolerate the ever increasing cost of accessing main memory. An estimate <ref> [13] </ref> shows that the performance of single-chip microprocessors is improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount of time [13] [42]. <p> An estimate <ref> [13] </ref> shows that the performance of single-chip microprocessors is improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount of time [13] [42]. The growing inability of the memory systems to keep up with the processors increases the importance of cache data re-use to reduce traffic to main memory, and pre-fetching mechanisms to hide memory access latencies.
Reference: [14] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 17th International Conference on Computer Architecture, </booktitle> <pages> pages 168-176, </pages> <year> 1990. </year>
Reference-contexts: Secondly, the Dude system uses a producer/consumer model, which can aid type- specific memory coherence. In Munin <ref> [14] </ref>, Bennett et al. proposed using a type-specific memory coherence. These types included write-once objects, private objects, write-many objects, result objects, synchronization objects, migratory objects and producer-consumer objects.
Reference: [15] <author> M.J. Berger and P. Colella. </author> <title> Local adaptive mesh refinement for shock hydrodynamics. </title> <journal> Journal of Computational Physics, </journal> <volume> 82 </volume> <pages> 64-84, </pages> <year> 1989. </year>
Reference-contexts: Adaptive Mesh Refinement (AMR) techniques allow computational resources such as CPUs and memory, to be used only when needed (i.e., for those areas of the problem grid where the error is unacceptable). Examples include the shock waves in computational fluid dynamics <ref> [15] </ref> where some regions containing shock waves need more computational effort than others. 18 Since loops are first-class objects in the Dude system, loops can be described and instantiated at run-time as needed during execution. 1.4 Architectural Assumptions We assume that the underlying machine architecture consists of one or more processors <p> Adaptive Mesh Refinement (AMR) techniques allow computational resources such as CPUs and memory, to be used only when needed (i.e., for those areas of problem grid where the error is unacceptable). Examples include the shock waves in computation fluid dynamics <ref> [15] </ref> where regions containing shock waves need more computational efforts that other areas of the computational space. These applications require the ability to instantiate a loop during runtime to compute a given region.
Reference: [16] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Of the runtime systems which we will consider, the Chores runtime system is the most important since it has many of the same design philosophies as the Dude system. 2.4.1 The Chores Runtime Systems The Chores system [24], implemented on top of Presto <ref> [16] </ref>, is most similar to our design. A per-processor worker (a user-level thread) grab chunks of work from a central queue using the guided self-scheduling method. As in the proposed work, the Chores system supports loop iterations with dependencies by using data-level synchronization.
Reference: [17] <author> Brian N. Bershad, Mathew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The midway distribured shared memory system. </title> <booktitle> In Proceedings 38th IEEE Computer Society Society International Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This is called lazy release consistency [47]. In both release and lazy release consistencies, the system makes the entire memory consistent at each synchronization point. A weaker model would be to make only the necessary subset of memory consistent. There are two approaches to this basic idea. Midway <ref> [17] </ref>, introduces an entry consistency. In this model, the user associates a user-defined data object with a synchronization object. This association allows the system to update only the data object associated with the synchronization object, upon its acquisition. <p> This is possible because the runtime system knows which processors are producers and which processors are consumers of the data described by descriptors. In Red/Black SOR, for example, the coherence operations can be limited to only the neighboring processors. Several works, e.g, Bershad et. al. <ref> [17] </ref>, have suggested the use of multiple policies to support a given program. However, little work has been 113 done to automate when and which of these policies are appropriately used. The runtime system can direct the use of a given policy.
Reference: [18] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Programmers need to lock the user-defined data structures to atomically decrement the counters which keep track of the number of unsatisfied precedence arcs. The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk <ref> [18] </ref>, Mentat [36, 35] and Tam [23], but at the expense of requiring users to use a low-level abstractions. Users are responsible for decomposition and scheduling using the given abstractions.
Reference: [19] <author> A. Brandt. </author> <title> Eliptic problem solvers. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1981. </year>
Reference-contexts: At each odd phase, each odd point compares and swaps with the neighboring even point if necessary. After N phases, the algorithm sorts the array. Two dimensional Multigrid Solver: The Multigrid technique has received much attention due to its importance and the challenge of par-allelizing the algorithm <ref> [19, 72, 30, 27] </ref>. Multigrid algorithms are used to accelerate the convergence of relaxation methods such as Gauss-Seidel for the numerical solution of partial differential equations.
Reference: [20] <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In our work, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 20, 10] </ref> could be used. Here, the runtime system generates descriptors for iteration spaces that are manipulated during runtime by representing them using recursive 23 structures called quadtrees. This allows the runtime system to execute independent portions of multiple loops in parallel.
Reference: [21] <author> David Cann. </author> <title> Retire fortran?: a debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year> <month> 116 </month>
Reference-contexts: Sisal has been ported to various machines including the Cray C90, Y-MP, or X-MP, Cray 2 running UNICOS, and SGI multi-processor running IRIX. Cann <ref> [21] </ref> reports favorable performance comparisons of Fortran and Sisal on the Cray Y-MP/864 computer. The benchmarks included the Liv-ermore Loops (24 scientific kernels), the SIMPLE hydrodynamics program, and a weather prediction program.
Reference: [22] <author> Jeffrey B. Weiss Clive F. Baillie, James C. McWilliams and Irad Yavneh. </author> <title> Parallel superconvergent multigrid. </title> <note> In submitted to Supercomputing '95., </note> <year> 1995. </year>
Reference-contexts: Planetary-scale fluid motions in the Earth's atmosphere are important to the study of Earth's climate. A more complete description of QG equations can be found in the work by Baillie et al <ref> [8, 22] </ref> [86]. Here, we concentrate on only the computational aspect of the problem as it relates to the Dude system runtime system.
Reference: [23] <author> D.E. Culler, A. Sah, K.E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [18], Mentat [36, 35] and Tam <ref> [23] </ref>, but at the expense of requiring users to use a low-level abstractions. Users are responsible for decomposition and scheduling using the given abstractions. <p> Although intended for imperative languages, the methods introduced in this thesis can be applied to a Sisal-like language to improve the performance of loops. This improved performance would come from using data level synchronization at the granularity of chunks of iterations instead of entire loops. 41 2.5.2 Tam Tam <ref> [23] </ref> is a compiler-controlled threaded abstract machine intended to support functional or dataflow languages. It evolved from graph-based execution for dataflow languages and provides a bridge between dataflow models and the control flow models typically employed by standard multiprocessors.
Reference: [24] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Of the runtime systems which we will consider, the Chores runtime system is the most important since it has many of the same design philosophies as the Dude system. 2.4.1 The Chores Runtime Systems The Chores system <ref> [24] </ref>, implemented on top of Presto [16], is most similar to our design. A per-processor worker (a user-level thread) grab chunks of work from a central queue using the guided self-scheduling method. As in the proposed work, the Chores system supports loop iterations with dependencies by using data-level synchronization. <p> By putting together and specializing these objects, the user specializes the system to create a "software systolic array" for the application at hand. This object-oriented model is based on AWESIME [37] and the Chores <ref> [24] </ref> runtime systems. The following is a list of 49 objects in the Dude system: * Data Descriptor: Data Descriptors describe a sub-region of the data space. For example, the system can divide a matrix into sub-matrices with each sub-matrix being defined by a data descriptor. <p> On multi-programmed multiprocessors, a participating processor maybe swapped out to run a process from a different job, making the problem worse. See figure 3.13. Most of the work on multiprocessors on multiprogrammed environment has been on the operating system or combination of user space/operating 76 77 system. Several researchers <ref> [5, 24, 52, 53] </ref> have focused on ways to establish some co-operation between the application and the operating system kernel. By extending the kernel to communicate with the application, the kernel can avoid pre-empting a task that is in a critical section.
Reference: [25] <author> Zhixi Fang, Peiyi Tang, Pen chung Yew, and Chuan qi Zhu. </author> <title> Dynamic processor self-scheduling for general parallel nested loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 919-929, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The runtime system manipulates this structure to coalesce or divide the iteration space during scheduling. As we will describe, this allows our runtime system to schedule multi-dimensional loops with dependencies across iterations and across dimensions. Runtime Methods: Fang et. al. <ref> [25] </ref> described a two-level scheduling scheme for general, arbitrarily nested parallel loops. Each loop doser, doall or doacross is termed a task. An extremely large-grain data-flow graph, where each node is an entire loop, is created. <p> Data-level synchronization occurs at a very high granularity since an entire loop is a node in the dependence graph. As in the work by Fang et. al <ref> [25] </ref>, the loop scheduler consists of two levels: one level that schedules from the for pool (pool of for loops) which consists of loop iteration descriptors, and second level that schedules from the Ready Pool containing large grain threads.
Reference: [26] <author> John T. Feo and David C. Cann. </author> <title> A report on the sisal language project. </title> <journal> Jounal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 81-89, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The Sisal compiler improves on other dataflow languages by avoiding the problem with large arrays in static single-assignment 40 languages by using an optimization called copy elimination. Although Sisal is based on a dataflow model, the runtime system, as described in <ref> [26] </ref>, does not aggressively take advantage of the parallelism in the loop iterations. Data-level synchronization occurs at a very high granularity since an entire loop is a node in the dependence graph.
Reference: [27] <author> P. Frederickson and O. McBryan. </author> <title> Parallel superconvergent multigrid. </title> <booktitle> In Proceedings of the Third Copper Muntain Conference on Multigird Methods. </booktitle> <publisher> Marcel Dekker, </publisher> <year> 1989. </year>
Reference-contexts: At each odd phase, each odd point compares and swaps with the neighboring even point if necessary. After N phases, the algorithm sorts the array. Two dimensional Multigrid Solver: The Multigrid technique has received much attention due to its importance and the challenge of par-allelizing the algorithm <ref> [19, 72, 30, 27] </ref>. Multigrid algorithms are used to accelerate the convergence of relaxation methods such as Gauss-Seidel for the numerical solution of partial differential equations.
Reference: [28] <author> D.D Gajski, D.A. Padua, D. J. Kuck, and R.H. Kuhn. </author> <title> A second opinion on data flow machines and languages. </title> <journal> Computer, </journal> <volume> 15(6) </volume> <pages> 58-69, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Since the dataflow model requires a single static assignment policy, data cannot be overwritten. Writing to a single element in a large array is difficult to make efficient since simply copying the entire array would be prohibitively expensive. A comprehensive critique of the dataflow approach can be found in <ref> [28] </ref>. Of the two principles of data flow model, only asynchrony is relevant to our own work.
Reference: [29] <author> D. Gannon and J.K. Lee. </author> <title> Object oriented parallelism. </title> <booktitle> In Proceedings of 1991 Japan Society for Parallel Processing, </booktitle> <pages> pages 13-23, </pages> <year> 1991. </year>
Reference-contexts: The user chooses the decomposition method such as by BLOCK or CYCLIC similar to decomposition and distribution utilities available in HPF [43] and pC++ <ref> [29] </ref>. One important difference, however, is that in the process of data decomposition, The Dude system takes flat data and creates objects or Iterates, which are tuples consisting of both data and operation.
Reference: [30] <author> D. Gannon and J. van Rosendale. . J. </author> <booktitle> Parallel Distributed Computing, </booktitle> <volume> 3 </volume> <pages> 106-135, </pages> <year> 1986. </year>
Reference-contexts: At each odd phase, each odd point compares and swaps with the neighboring even point if necessary. After N phases, the algorithm sorts the array. Two dimensional Multigrid Solver: The Multigrid technique has received much attention due to its importance and the challenge of par-allelizing the algorithm <ref> [19, 72, 30, 27] </ref>. Multigrid algorithms are used to accelerate the convergence of relaxation methods such as Gauss-Seidel for the numerical solution of partial differential equations.
Reference: [31] <editor> Dennis B. Gannon and editors Robert J. Douglass. </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 335-349. </pages> <publisher> Scientific Computation Series. MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: This is possible because the 42 Dude system has been built with the intention of optimizing loop constructs to support data-parallel languages. multiplication 2.5.3 Large-Grain Data flow Babb and DiNucci <ref> [6, 31] </ref> describe a large-grain data flow, or LGDF, where the nodes in the dataflow graph consist of program blocks corresponding to between 5 and 50 (or even more) statements in a higher level programming language.
Reference: [32] <author> Guang R. Gao, Russ Olsen, Vivek Sarkar, and Radhika Thekkath. </author> <title> Collective loop fusion for array contraction. </title> <booktitle> In 1992 Workshop on Languages and Compilers for Parallel Computing, number 757 in Lecture Notes in Computer Science, </booktitle> <pages> pages 281-295, </pages> <address> New Haven, Conn., </address> <month> 117 August </month> <year> 1992. </year> <title> Berlin: </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: In our work, we use both intra-loop dependencies and inter-loop dependencies (or cross-loop dependencies [82]) which can be computed in the same way as canonical dependencies. Compilers also use cross-loop dependencies to determine the legality of loop fusions <ref> [32, 49, 82] </ref>. 2.1.2 Detection of Independent Sub-loops Graham et al. [34] noticed that in many applications, two separate, but consecutive loops 22 might be partially independent. Normally, when the compiler encounters consecutive loops, the compiler would parallelize the first loop and then the second. <p> A survey of compiler transformations for high performance computing can be found in the works by Bacon et. al. and Wolfe [7, 82]. Another important loop transformation is loop fusion. Loop fusion <ref> [32, 49, 84] </ref> transforms multiple distinct loops into a single loop to improve 29 temporal locality and to increase the grain size of parallelism. doall I=1,N A [I] = 0.0 doall I = 1, N endo =&gt; A [I] = 0.0 doall J=1,N B [I] = A [I] A [J] =
Reference: [33] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluations of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: This can be weakened (and thereby consistency 111 traffic can be reduced) by taking advantage of the fact that the semantics of parallel programs only expect consistencies at the synchronization points. This is the idea behind release consistency <ref> [33] </ref>, which uses a delayed update queue to buffer the writes to a memory location until the program reaches a synchronization points. More specifically, the system invalidates or updates remote memory at the release of a synchronization object.
Reference: [34] <author> Susan Graham, Steven Lucco, and Oliver Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> April </month> <year> 1993. </year> <note> ACM, ACM. </note>
Reference-contexts: In our work, we use both intra-loop dependencies and inter-loop dependencies (or cross-loop dependencies [82]) which can be computed in the same way as canonical dependencies. Compilers also use cross-loop dependencies to determine the legality of loop fusions [32, 49, 82]. 2.1.2 Detection of Independent Sub-loops Graham et al. <ref> [34] </ref> noticed that in many applications, two separate, but consecutive loops 22 might be partially independent. Normally, when the compiler encounters consecutive loops, the compiler would parallelize the first loop and then the second. <p> The size of the current batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [76], tapering <ref> [34, 63] </ref>, and safe self-scheduling [61]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling is affinity loop scheduling [64], which takes affinity of the data to processors into account. <p> Some examples of how such opportunities to combine task/data parallelism arises in scientific and engineering computations include the following: * In languages that support parbegin/parend constructs, the loops which occur in different branches of the constructs are independent, and therefore can be run in parallel. * Graham et al. <ref> [34] </ref> noticed that in many applications two separate but consecutive loops may be partially independent. Normally, when a compiler encounters consecutive loops, the compiler would parallelize the first loop and then the second.
Reference: [35] <author> A. S. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [18], Mentat <ref> [36, 35] </ref> and Tam [23], but at the expense of requiring users to use a low-level abstractions. Users are responsible for decomposition and scheduling using the given abstractions.
Reference: [36] <author> A. S. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <booktitle> IEEE Parallel and Distributed Technology: Systems and Applications, </booktitle> <pages> pages 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [18], Mentat <ref> [36, 35] </ref> and Tam [23], but at the expense of requiring users to use a low-level abstractions. Users are responsible for decomposition and scheduling using the given abstractions.
Reference: [37] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: By putting together and specializing these objects, the user specializes the system to create a "software systolic array" for the application at hand. This object-oriented model is based on AWESIME <ref> [37] </ref> and the Chores [24] runtime systems. The following is a list of 49 objects in the Dude system: * Data Descriptor: Data Descriptors describe a sub-region of the data space. For example, the system can divide a matrix into sub-matrices with each sub-matrix being defined by a data descriptor.
Reference: [38] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> Efficient barriers for distributed shared memory computers. </title> <booktitle> In Intl. Parallel Processing Symposium. IEEE, IEEE Computer Society, </booktitle> <month> April </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: The performance graph shows the speedup on the KSR1. The slightly better performance of the Awesime thread package over the native Pthreads is due to the more efficient f -way tournament barrier <ref> [38] </ref> available in Awesime. 4.4.2 The Dude system In our approach, we begin by loading the initially unconstrained Iterates from both loops onto the system queue. Each level in the Multigrid solver consists of four collective operations or IterateCollections: smooth even elements, smooth odd elements, approximate, and prolong/restrict.
Reference: [39] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The manchester prototype dataflow computer. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 34-52, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Both specialized hardware and the languages using this model has been built. Dataflow architectures include the classical Manchester Dataflow Machine <ref> [39, 40] </ref>. Dataflow languages include Sisal, VAL, Id,and LAU. 2.5.1 Sisal Sisal, a descendant of VAL, is an applicative language which has been specially designed for efficient execution of scientific data-parallel computations.
Reference: [40] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The Manchester Dataflow Computing System, </title> <publisher> pages 516,517,519,520,529. North-Holland, </publisher> <year> 1987. </year>
Reference-contexts: Both specialized hardware and the languages using this model has been built. Dataflow architectures include the classical Manchester Dataflow Machine <ref> [39, 40] </ref>. Dataflow languages include Sisal, VAL, Id,and LAU. 2.5.1 Sisal Sisal, a descendant of VAL, is an applicative language which has been specially designed for efficient execution of scientific data-parallel computations.
Reference: [41] <author> A. Gursoy and L. V. Kale. Dagger: </author> <title> Cominging the benefits of synchronous and asynchronous communication styles. </title> <type> Technical Report 9303, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, </institution> <year> 1993. </year>
Reference-contexts: Charm++ supports abstractions for sharing information as opposed to simply messages. Specific abstract shared objects such as "read-only objects", "write-once objects", "accumulator objects", and 45 "monotonic (or idempotent) objects" are also provided. To simplify the complexity of sending asynchronous messages while preserving their order, a coordination language called Dagger <ref> [41] </ref> is used to specify synchronization constraints. Howerver, even with the use of the Dagger language, the message model seems unnatural for shared memory MIMD machines since users need to send messages explicitly.
Reference: [42] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture: a Quantitative Approach. </title> <address> Morgan-Kaufman, </address> <year> 1990. </year> <month> 118 </month>
Reference-contexts: An estimate [13] shows that the performance of single-chip microprocessors is improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% in that same amount of time [13] <ref> [42] </ref>. The growing inability of the memory systems to keep up with the processors increases the importance of cache data re-use to reduce traffic to main memory, and pre-fetching mechanisms to hide memory access latencies.
Reference: [43] <author> High Performance Fortran Forum HPFF. </author> <title> Draft high performance fortran specificition, </title> <note> version 0.4. In Proceedings of 1991 Japan Society for Parallel Processing, page Available from anonymous ftp site titan.cs.rice.edu, </note> <year> 1992. </year>
Reference-contexts: One of the parameters used in instantiating a StaticCollection or an AffinityCollection is the decomposition method, which determines how the system divides the data between the different Iterates. The user chooses the decomposition method such as by BLOCK or CYCLIC similar to decomposition and distribution utilities available in HPF <ref> [43] </ref> and pC++ [29]. One important difference, however, is that in the process of data decomposition, The Dude system takes flat data and creates objects or Iterates, which are tuples consisting of both data and operation.
Reference: [44] <author> S. F. Hummel, Edith Schonberg, and L. E. Flynn. </author> <title> Factoring, a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In guided-self scheduling [66], the number of iterations assigned to a processor is a function of the number of iterations left to be scheduled. A variation on this method is factoring <ref> [44] </ref> where, at each scheduling operation, the scheduler computes the size of a batch of chunks with the motivation to reduce the number of scheduling operations. The size of the current batch is exactly half of the previous batch.
Reference: [45] <author> F. Irigoin and R. Triolet. </author> <title> Computing dependence direction vectors and depedence cones. </title> <type> Technical Report E94, </type> <institution> Centre D'Automatique et Informatique, </institution> <year> 1988. </year>
Reference-contexts: In other cases, when the compiler cannot derive the distance vector, the compiler may be able to derive a direction vector, which describes the direction of the dependence arc from one iteration to another. Techniques for extracting distance and direction vectors from loop nests are well known <ref> [45, 11, 12, 82, 85] </ref>. In our own work, dependencies between iterations are not represented in the form of distance/direction vectors, but in the form of symbolic rules or functions. This implies that the system does not need to solve the data dependence system.
Reference: [46] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> Crl: High-performance all-software distributed shared memory. </title> <booktitle> In SIGOPS, </booktitle> <pages> pages 213-228, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This association allows the system to update only the data object associated with the synchronization object, upon its acquisition. A problem with this approach is that providing the synchronization objects explicitly for every data object and using these synchronization objects, can be burdensome to the programmer. In CRL <ref> [46] </ref>, the synchronization objects are associated with regions of memory and these synchronization objects are implicit in the rgn start op and rgn end op, which bracket the memory operations. A dependence-driven execution can improve the performance of DSM's in the several ways.
Reference: [47] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Conference on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <year> 1992. </year>
Reference-contexts: More specifically, the system invalidates or updates remote memory at the release of a synchronization object. A variation of this is to pull the data instead of pushing by making local memory consistent upon acquiring a synchronization object. This is called lazy release consistency <ref> [47] </ref>. In both release and lazy release consistencies, the system makes the entire memory consistent at each synchronization point. A weaker model would be to make only the necessary subset of memory consistent. There are two approaches to this basic idea. Midway [17], introduces an entry consistency.
Reference: [48] <institution> Kendall Square Research, </institution> <address> Boston, MA. </address> <booktitle> The KSR-1 System Architecture Manual, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: We also used atom to count the number of instructions used by various loop-optimizing methods. 4.1.2 Machines To study the scalability of the Dude system, we used a 64 processor KSR-1 <ref> [48] </ref>, with a shared memory COMA. KSR-1 has a hierarchical memory model where the cost of communication between processors is not constant between all processors. A hierarchy of rings connects processors with each ring supporting up to 32 processors. In the 64-processor machine that we used, there are two rings.
Reference: [49] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In 1993 Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pages 301-320, </pages> <address> Portland, Ore., </address> <month> August </month> <year> 1993. </year> <title> Berlin: </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: In our work, we use both intra-loop dependencies and inter-loop dependencies (or cross-loop dependencies [82]) which can be computed in the same way as canonical dependencies. Compilers also use cross-loop dependencies to determine the legality of loop fusions <ref> [32, 49, 82] </ref>. 2.1.2 Detection of Independent Sub-loops Graham et al. [34] noticed that in many applications, two separate, but consecutive loops 22 might be partially independent. Normally, when the compiler encounters consecutive loops, the compiler would parallelize the first loop and then the second. <p> A survey of compiler transformations for high performance computing can be found in the works by Bacon et. al. and Wolfe [7, 82]. Another important loop transformation is loop fusion. Loop fusion <ref> [32, 49, 84] </ref> transforms multiple distinct loops into a single loop to improve 29 temporal locality and to increase the grain size of parallelism. doall I=1,N A [I] = 0.0 doall I = 1, N endo =&gt; A [I] = 0.0 doall J=1,N B [I] = A [I] A [J] =
Reference: [50] <author> Scott R. Kohn. </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations. </title> <type> PhD thesis, </type> <institution> UCSD CSE Dept., </institution> <month> June </month> <year> 1995. </year> <note> Tech. Report CS95-429. </note>
Reference-contexts: Finally, the Chores system does not concern itself with improving memory locality, only increasing parallelism. 2.4.2 LPARX LPARX <ref> [50, 51] </ref> is a runtime library designed specifically for dynamic irregular scientific calculations. It consists of three parts: numerical operations, grid management facilities, and display routines. The numerical operations define the elliptic partial differential equations to be solved. The display library provides data visualization utilities.
Reference: [51] <author> S.R. Kohn and S.B. Baden. </author> <title> A parallel software infrastructure for structured adaptive mesh methods. </title> <booktitle> In Supercomputing '95, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Finally, the Chores system does not concern itself with improving memory locality, only increasing parallelism. 2.4.2 LPARX LPARX <ref> [50, 51] </ref> is a runtime library designed specifically for dynamic irregular scientific calculations. It consists of three parts: numerical operations, grid management facilities, and display routines. The numerical operations define the elliptic partial differential equations to be solved. The display library provides data visualization utilities.
Reference: [52] <author> L. Kontothanassis, R. Wisniewski, </author> <title> and M.L. Scott. Using scheduler information to achieve optimal barrier synchronization performance. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming., </booktitle> <pages> pages 13-23, </pages> <month> May </month> <year> 1993. </year> <month> 119 </month>
Reference-contexts: On multi-programmed multiprocessors, a participating processor maybe swapped out to run a process from a different job, making the problem worse. See figure 3.13. Most of the work on multiprocessors on multiprogrammed environment has been on the operating system or combination of user space/operating 76 77 system. Several researchers <ref> [5, 24, 52, 53] </ref> have focused on ways to establish some co-operation between the application and the operating system kernel. By extending the kernel to communicate with the application, the kernel can avoid pre-empting a task that is in a critical section.
Reference: [53] <author> L. Kontothanassis, R. Wisniewski, </author> <title> and M.L. Scott. Scheduler-conscious synchronization. </title> <journal> ACM TOCS, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: On multi-programmed multiprocessors, a participating processor maybe swapped out to run a process from a different job, making the problem worse. See figure 3.13. Most of the work on multiprocessors on multiprogrammed environment has been on the operating system or combination of user space/operating 76 77 system. Several researchers <ref> [5, 24, 52, 53] </ref> have focused on ways to establish some co-operation between the application and the operating system kernel. By extending the kernel to communicate with the application, the kernel can avoid pre-empting a task that is in a critical section.
Reference: [54] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-10016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The simplest dynamic method is self-scheduling [74] where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking <ref> [54] </ref> was proposed, where each process grabs chunks of K iterations instead of one iteration. If the processors do not start simultaneously or if one processor gets 31 an expensive chunk, the potential for load imbalance still exists.
Reference: [55] <author> H.T. Kung and C.E. Leiserson. </author> <title> Systolic arrays (for vlsi). In Sparse Matrix Proc.(Society for Industrial and Applied Mathematics, </title> <booktitle> 1979), </booktitle> <pages> pages 256-282, </pages> <year> 1978. </year>
Reference-contexts: For example, consider the graph shown in Figure 2.6 for a simple matrix/vector multiplication, an example taken from [6]. The central node, P01, has to be further expanded (not shown) to specify an equally complex graph computing the inner product. 2.6 Systolic Arrays Systolic arrays <ref> [55, 57] </ref> are examples of SIMD (Single Instruction Multiple Data) machines consisting of a collection of identical processing elements called cells which are interconnected to exchange data. The interconnections are local in that each cell can communicate via direct physical interconnections with neighboring cells only.
Reference: [56] <author> S.Y Kung. </author> <title> Wavefront array processors. </title> <booktitle> In Systolic Signal Processing Systems, </booktitle> <pages> pages 97-160, </pages> <year> 1987. </year>
Reference-contexts: Furthermore, unlike computations in systolic arrays, computations in the Dude system are not synchronized by a global clock (and in that sense, our model is closer to wavefront arrays <ref> [56] </ref>). 2.7 Charm++ Charm++ is a machine independent parallel programming system running on MIMD machines. Charm++ supports abstractions for sharing information as opposed to simply messages. Specific abstract shared objects such as "read-only objects", "write-once objects", "accumulator objects", and 45 "monotonic (or idempotent) objects" are also provided.
Reference: [57] <author> S.Y Kung. </author> <title> VLSI Array Processors. Systems Sciences. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: For example, consider the graph shown in Figure 2.6 for a simple matrix/vector multiplication, an example taken from [6]. The central node, P01, has to be further expanded (not shown) to specify an equally complex graph computing the inner product. 2.6 Systolic Arrays Systolic arrays <ref> [55, 57] </ref> are examples of SIMD (Single Instruction Multiple Data) machines consisting of a collection of identical processing elements called cells which are interconnected to exchange data. The interconnections are local in that each cell can communicate via direct physical interconnections with neighboring cells only.
Reference: [58] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimization of blocked algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, the transformation of the following FORTRAN loop, 28 do J=2, M do I=1, N enddo enddo enddo enddo improves memory locality because the inner loop of the transformed loop accesses consecutive memory addresses. * Blocking (or Tiling): Blocking <ref> [83, 58, 81] </ref> takes advantage of applications with spatial locality by traversing one rectangle of the iteration space at a time.
Reference: [59] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Research efforts have succeeded to some extent by relaxing the requirement that memory be sequentially consistent. According to Lamport <ref> [59] </ref>, this means that the "result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each processor appear in this sequence in the order specified by its program".
Reference: [60] <author> S. Levialdi. </author> <title> On shrinking binary picture patterns. </title> <journal> Communications of the ACM, </journal> <volume> 15(1) </volume> <pages> 7-10, </pages> <month> January </month> <year> 1972. </year>
Reference-contexts: We chose this algorithm because this is a classic example of a compiler transformation called skewing with which we wish to compare the performance. Component Labeling: Levialdi's algorithm <ref> [60] </ref> for component labeling is used in image processing to detect connected components of a picture. It involves a series of phases, each phase consisting of changing a 1-pixel to a 0-pixel if its upper, left, and upper-left neighbors are 0-pixels. <p> A closer look at the assembly instructions verified this assumption. 4.2.3 Component Labeling Levialdi's algorithm <ref> [60] </ref> for component labeling is used in image processing to detect connected components of a picture. We used a matrix size of 2048x2048 with a block size of 32x32 for the Dude system, 512x512 for tiling by block and 256x2048 for tiling by row.
Reference: [61] <author> Liu and et al. </author> <title> Scheduling parallel loops with variable length iteration execution times on parallel computers. </title> <booktitle> In Proc. 5th Intl. Conf. Parallel and Distributed Computing and System, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The size of the current batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [76], tapering [34, 63], and safe self-scheduling <ref> [61] </ref>. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling is affinity loop scheduling [64], which takes affinity of the data to processors into account.
Reference: [62] <author> L.M. Adams and H.F. Jordan. </author> <title> Is SOR Color Blind? SIAM Sci. </title> <journal> Stat. Computation, </journal> <volume> 7(2) </volume> <pages> 490-506, </pages> <year> 1986. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 62] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance, and therefore does well under static scheduling schemes. Red/Black SOR is representative of a class of PDE solvers where each element depends only on its neighboring elements.
Reference: [63] <author> Steven Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In Proceedings of ACM SIGPLAN '92 Conference on Porgramming Language Design and Implementation, </booktitle> <pages> pages 200-211. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The size of the current batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [76], tapering <ref> [34, 63] </ref>, and safe self-scheduling [61]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling is affinity loop scheduling [64], which takes affinity of the data to processors into account.
Reference: [64] <author> E.P Markatos and T. J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared Memory Multiprocessors. </title> <booktitle> In Intl. Conference on Parallel Processing, </booktitle> <pages> pages 258-257, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year> <month> 120 </month>
Reference-contexts: Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [76], tapering [34, 63], and safe self-scheduling [61]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling is affinity loop scheduling <ref> [64] </ref>, which takes affinity of the data to processors into account. This scheme is most like chunking except that chunks have an affinity to a particular processor. Another difference is that chunking is a centralized algorithm while affinity scheduling is a distributed algorithm based on work stealing. <p> The advantage of affinity scheduling is that the scheduler can adapt to the dynamic fluctuation of the workload by distributing Iterates to idle processors while trying to maintain data locality <ref> [64] </ref>. One of the parameters used in instantiating a StaticCollection or an AffinityCollection is the decomposition method, which determines how the system divides the data between the different Iterates.
Reference: [65] <author> John Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Others have focused on various multiprogramming policies for scheduling kernel processes such as unsynchronized time-sharing (time-slicing), synchronized time-sharing (co-scheduling) <ref> [65] </ref>, and space-sharing (hardware partitions) [75]. It is extremely difficult to find a single policy that can maximize utilization, ensure fairness, and, at the same time, keep the overheads low. We take an approach that is not incompatible with the above methods.
Reference: [66] <author> C. D. Polochronopoulous and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If the processors do not start simultaneously or if one processor gets 31 an expensive chunk, the potential for load imbalance still exists. In guided-self scheduling <ref> [66] </ref>, the number of iterations assigned to a processor is a function of the number of iterations left to be scheduled.
Reference: [67] <author> C. D. Polychronopoulos. </author> <title> Loop coalescing: A compiler transformation for parallel machines. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: Static Methods: A special case of multiply nested loops are the one-way (or perfectly) nested doall loops in which there exists exactly one loop at each nest level. Given a one-way nested doall loop, a compile-time transformation, loop coalescing, <ref> [67] </ref> can be used to coalesce m doall loops into a single doall with N = Q m i=1 (N i ) iterations. This transformation then allows the application of the methods described in the previous section.
Reference: [68] <author> J. Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and runtime compilation. </title> <booktitle> In Proceedings of Inernational Workshop on Compilers for Parallel Computers. </booktitle> <address> Paris, </address> <year> 1990. </year>
Reference-contexts: Optimizing at runtime may indeed add more instructions. However, if the optimization can reduce these hazards, the overall execution time of the program can be decreased. 5 For example, Saltz et. al. <ref> [68, 69] </ref> presented a runtime parallelization method, involving an inspector and an executor, which detected and orchestrated parallelism in loops dynamically that compile-time analysis could not detect because of the nonlinear expressions that characterize the dependencies of loops found in some applications. <p> Sparse triangular solve is an example of an application that is not amenable to compile-time dependency analysis. Saltz et. al. <ref> [68, 69] </ref> proposed a compile-time/runtime compilation technique to parallelize such loops. At compile-time, the system sets up the framework for analyzing dependencies. At runtime, the inspector identifies and executes sets of concurrently executable loop iterations called wavefronts. Given a loop L, the system generates two new loops, L1 and L2.
Reference: [69] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Optimizing at runtime may indeed add more instructions. However, if the optimization can reduce these hazards, the overall execution time of the program can be decreased. 5 For example, Saltz et. al. <ref> [68, 69] </ref> presented a runtime parallelization method, involving an inspector and an executor, which detected and orchestrated parallelism in loops dynamically that compile-time analysis could not detect because of the nonlinear expressions that characterize the dependencies of loops found in some applications. <p> Sparse triangular solve is an example of an application that is not amenable to compile-time dependency analysis. Saltz et. al. <ref> [68, 69] </ref> proposed a compile-time/runtime compilation technique to parallelize such loops. At compile-time, the system sets up the framework for analyzing dependencies. At runtime, the inspector identifies and executes sets of concurrently executable loop iterations called wavefronts. Given a loop L, the system generates two new loops, L1 and L2. <p> This creates a new set of iterations for wavefront k + 1 that does not have any incoming arcs. The overhead of running the inspector can be quite large because the algorithm for the topological sort is O (n) where n is the number of iterations. Saltz et al <ref> [69] </ref> discusses a method to use a doacross loop to parallelize the inspector but performance may still be poor. For example, Saltz at al [69] showed performance on a 16 processor Encore Multimax in which the inspector took 100ms while the executor required only 23ms achieving a speedup of about two <p> Saltz et al <ref> [69] </ref> discusses a method to use a doacross loop to parallelize the inspector but performance may still be poor. For example, Saltz at al [69] showed performance on a 16 processor Encore Multimax in which the inspector took 100ms while the executor required only 23ms achieving a speedup of about two with respect to sequential speed (240ms.) Leung and Zahorjan [71] discuss methods to improve the performance of the inspector/executor model by sectioning, a technique
Reference: [70] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In the quadtree decomposition, each subdivision is a block divided into four equal parts. Alternatively, at each subdivision, the block could be divided into two parts as in a bintree. Samet <ref> [70] </ref> gives a comprehensive description of these data-structures. These structures solve the problem of inefficient indexing into the pools of Iterates because the chunk sizes are uniform at any given level.
Reference: [71] <author> Shun tak Leung and John Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In Conf. Record ACM Symp. Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, Saltz at al [69] showed performance on a 16 processor Encore Multimax in which the inspector took 100ms while the executor required only 23ms achieving a speedup of about two with respect to sequential speed (240ms.) Leung and Zahorjan <ref> [71] </ref> discuss methods to improve the performance of the inspector/executor model by sectioning, a technique for parallelizing the inspector, and bootstrapping a technique to reduce the number of wavefronts.
Reference: [72] <author> M. Zubair S.N. Gupta and C.E. </author> <title> Grosch. </title> . <journal> J. of Scientific Commput., </journal> <volume> 7 </volume> <pages> 263-279, </pages> <year> 1992. </year>
Reference-contexts: At each odd phase, each odd point compares and swaps with the neighboring even point if necessary. After N phases, the algorithm sorts the array. Two dimensional Multigrid Solver: The Multigrid technique has received much attention due to its importance and the challenge of par-allelizing the algorithm <ref> [19, 72, 30, 27] </ref>. Multigrid algorithms are used to accelerate the convergence of relaxation methods such as Gauss-Seidel for the numerical solution of partial differential equations.
Reference: [73] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Finally, figure 1.4 shows the number of instructions used in each method. To calculate the number of instructions used by the different methods we used the ATOM <ref> [73] </ref> instrumentation tool available on the DEC Alphas. ATOM takes instrumentation code written by the user and links it with the executable. <p> However, overlapping executions of different phases is only legal if the computation respects the data dependencies between the two phases. CHAPTER 4 EXPERIMENTAL RESULTS 4.1 Experimental Infrastructure and Design 4.1.1 Tools To analyze the performance of various applications and various methods, we used two tools, DCPI [4] and ATOM <ref> [73] </ref>, both of which are available on the DEC Alphas. Digital Continuous Profiling Infrastructure (DCPI) runs on the background with low overhead (slowdown of 1-3%) and unobtrusively generates profile data for the applications running on the machines by sampling hardware performance counters available on the Alphas.
Reference: [74] <author> P. Tang and P.C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages 528-535. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: This scheme is simple and there is no runtime scheduling overhead. The disadvantage, however, is that the method cannot adapt to dynamic fluctuation of the load in the application. Dynamic Scheduling: Dynamic assignment of iterations to processors can achieve better load balance. The simplest dynamic method is self-scheduling <ref> [74] </ref> where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking [54] was proposed, where each process grabs chunks of K iterations instead of one iteration.
Reference: [75] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> Dec </month> <year> 1989. </year> <month> 121 </month>
Reference-contexts: Others have focused on various multiprogramming policies for scheduling kernel processes such as unsynchronized time-sharing (time-slicing), synchronized time-sharing (co-scheduling) [65], and space-sharing (hardware partitions) <ref> [75] </ref>. It is extremely difficult to find a single policy that can maximize utilization, ensure fairness, and, at the same time, keep the overheads low. We take an approach that is not incompatible with the above methods.
Reference: [76] <author> T.H. Tzen and L.M. Ni. </author> <title> Trapezoid self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions Parrallel Distributed Systems, </journal> <volume> 4 </volume> <pages> 87-98, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The size of the current batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling <ref> [76] </ref>, tapering [34, 63], and safe self-scheduling [61]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling is affinity loop scheduling [64], which takes affinity of the data to processors into account.
Reference: [77] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Dependence-driven run-time system. </title> <booktitle> In Proceedings of Language and Compilers for Parallel Computing, </booktitle> <pages> pages 168-176, </pages> <year> 1996. </year>
Reference-contexts: A description of these schedulers in the Dude system can also be found in our paper <ref> [77] </ref>. Depending on the application, or based on runtime conditions, the user can choose one of these methods by instantiating the appropriate IterateCollection template which determines the scheduling behavior.
Reference: [78] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Exploiting temporal locality using a dependence-driven execution. </title> <booktitle> In International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Each collective operation must re-load the data because the previous operations would have spilt the data from the cache. By using a dependence-driven execution, we can apply multiple collection operations to 62 the data before it leaves the cache as described in our papers <ref> [79, 78] </ref>.
Reference: [79] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Loop re-ordering and pre-fetching at runtime. </title> <booktitle> In High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: Each collective operation must re-load the data because the previous operations would have spilt the data from the cache. By using a dependence-driven execution, we can apply multiple collection operations to 62 the data before it leaves the cache as described in our papers <ref> [79, 78] </ref>.
Reference: [80] <author> Chien-Min Wang and Sheng-De Wang. </author> <title> A hybrid scheme for efficiently executing loops on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 625-637, </pages> <year> 1992. </year>
Reference-contexts: Hybrid Methods: An optimized compile-time/runtime version of loop interchange and loop distribution called IGSS and MGSS was proposed by Wang et. al. <ref> [80] </ref>. These schemes also allowed the scheduling of hybrid loops consisting of doser and doalls. This method is useful in cases where the number of iterations of a doall loop is much larger than the number of processors involved.
Reference: [81] <author> Michael Edward Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: For example, the transformation of the following FORTRAN loop, 28 do J=2, M do I=1, N enddo enddo enddo enddo improves memory locality because the inner loop of the transformed loop accesses consecutive memory addresses. * Blocking (or Tiling): Blocking <ref> [83, 58, 81] </ref> takes advantage of applications with spatial locality by traversing one rectangle of the iteration space at a time. <p> First, compile-time loop transformation only applies to loops which are either perfectly nested or loops which can be put in the form of perfectly nested loops. With the exception of loop fusion and loop distribution, compile-time transformations only apply to a single collective operation. However, as Wolfe <ref> [81] </ref> pointed out in the future work of his dissertation, there is much that we can gain from exploiting temporal locality across consecutive but different loop nests.
Reference: [82] <author> M.J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana, </institution> <month> April </month> <year> 1987. </year> <type> Rep. 329. </type>
Reference-contexts: In other cases, when the compiler cannot derive the distance vector, the compiler may be able to derive a direction vector, which describes the direction of the dependence arc from one iteration to another. Techniques for extracting distance and direction vectors from loop nests are well known <ref> [45, 11, 12, 82, 85] </ref>. In our own work, dependencies between iterations are not represented in the form of distance/direction vectors, but in the form of symbolic rules or functions. This implies that the system does not need to solve the data dependence system. <p> Canonical data dependence analysis does not compute dependencies between two adjacent loop nests. In our work, we use both intra-loop dependencies and inter-loop dependencies (or cross-loop dependencies <ref> [82] </ref>) which can be computed in the same way as canonical dependencies. <p> In our work, we use both intra-loop dependencies and inter-loop dependencies (or cross-loop dependencies [82]) which can be computed in the same way as canonical dependencies. Compilers also use cross-loop dependencies to determine the legality of loop fusions <ref> [32, 49, 82] </ref>. 2.1.2 Detection of Independent Sub-loops Graham et al. [34] noticed that in many applications, two separate, but consecutive loops 22 might be partially independent. Normally, when the compiler encounters consecutive loops, the compiler would parallelize the first loop and then the second. <p> The set of distance vectors makes up the data dependencies that determine the allowable re-ordering transformation. Based on these dependencies, optimizing compilers may make the following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [82, 3] </ref> swaps an inner loop with an outer loop. Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. <p> In some cases, skewing <ref> [82] </ref> can be applied to enable blocking transformation. Skewing tra verses the iteration space diagonally in waves. From the transformed iteration space, the compiler generates code in the form of new loops. <p> While the generated code is more complex than the original, the new code has better locality and parallelism. A survey of compiler transformations for high performance computing can be found in the works by Bacon et. al. and Wolfe <ref> [7, 82] </ref>. Another important loop transformation is loop fusion. <p> This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by loop interchange <ref> [3, 82] </ref>. Other compiler transformations attempt to organize loops such that the loops with maximum parallelism are the outer loops. Compiler transformations such as loop normalization and loop fusion can also be used to collapse a multi-dimensional iteration space to a single dimensional iteration space.
Reference: [83] <author> M.J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. of Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: For example, the transformation of the following FORTRAN loop, 28 do J=2, M do I=1, N enddo enddo enddo enddo improves memory locality because the inner loop of the transformed loop accesses consecutive memory addresses. * Blocking (or Tiling): Blocking <ref> [83, 58, 81] </ref> takes advantage of applications with spatial locality by traversing one rectangle of the iteration space at a time.
Reference: [84] <author> M.J. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: The optimization methods presented in this thesis require that the subscript expressions of arrays are linear functions of the induction variables of the loop. The reason for this restriction is discussed in chapter 3. Fortunately, linear subscript expressions are also the most common forms of subscripts in loops <ref> [84] </ref>. Most existing compile-time loop transformation and existing loop scheduling methods also have similar restrictions. However, unlike compiler transformations and loop scheduling methods, the linear functions may contain expressions which can only be determined during run-time. Furthermore, our work addresses imperfectly nested loops. <p> A survey of compiler transformations for high performance computing can be found in the works by Bacon et. al. and Wolfe [7, 82]. Another important loop transformation is loop fusion. Loop fusion <ref> [32, 49, 84] </ref> transforms multiple distinct loops into a single loop to improve 29 temporal locality and to increase the grain size of parallelism. doall I=1,N A [I] = 0.0 doall I = 1, N endo =&gt; A [I] = 0.0 doall J=1,N B [I] = A [I] A [J] = <p> Fortunately, affine subscript expressions are also the most common form of subscripts in loops <ref> [84] </ref>. Table 3.1 shows some linear and non-linear subscript functions.
Reference: [85] <author> M.J. Wolfe and C.W. Tseng. </author> <title> The power test for data dependence. </title> <type> Technical Report CS/E 90-015, </type> <institution> Oregon Graduate Institute, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: In other cases, when the compiler cannot derive the distance vector, the compiler may be able to derive a direction vector, which describes the direction of the dependence arc from one iteration to another. Techniques for extracting distance and direction vectors from loop nests are well known <ref> [45, 11, 12, 82, 85] </ref>. In our own work, dependencies between iterations are not represented in the form of distance/direction vectors, but in the form of symbolic rules or functions. This implies that the system does not need to solve the data dependence system.
Reference: [86] <author> Irad Yavneh and James C. McWilliams. </author> <title> Multigrid solution of stably stratified flows: the quasi-geostrophic equations. </title> <note> In submitted to J. Sci. Comp., </note> <year> 1995. </year>
Reference-contexts: Planetary-scale fluid motions in the Earth's atmosphere are important to the study of Earth's climate. A more complete description of QG equations can be found in the work by Baillie et al [8, 22] <ref> [86] </ref>. Here, we concentrate on only the computational aspect of the problem as it relates to the Dude system runtime system.
References-found: 86

