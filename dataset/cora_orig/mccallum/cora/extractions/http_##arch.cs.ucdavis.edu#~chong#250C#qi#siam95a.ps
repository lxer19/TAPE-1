URL: http://arch.cs.ucdavis.edu/~chong/250C/qi/siam95a.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/qi/
Root-URL: http://www.cs.ucdavis.edu
Title: Chapter 1 An Overview of the SUIF Compiler for Scalable Parallel Machines  
Author: Saman P. Amarasinghe Jennifer M. Anderson Monica S. Lam Chau-Wen Tseng 
Abstract: We are building a compiler that automatically translates sequential scientific programs into parallel code for scalable parallel machines. Many of the compiler techniques needed to generate correct and efficient code are common across all scalable machines, regardless of whether its address space is shared or distributed. This paper describes the structure of the compiler, emphasizing the common analyses and optimizations. We focus on the three major phases of the compiler: parallelism and locality analysis, communication and synchronization analysis, and code generation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe and M. S. Lam, </author> <title> Communication optimization and code generation for distributed memory machines, </title> <booktitle> in Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This paper discusses the fundamental issues in optimization and code generation needed for all scalable parallel machines. We give an overview of the algorithms used in the SUIF compiler to address these issues. The details of the individual algorithms can be found elsewhere <ref> [1, 3, 6, 8, 10] </ref>. We have currently implemented a complete compiler that generates code for SAS machines, and our DAS code generator is in progress. <p> By composing data and computation mappings and array references as a system of linear inequalities, non-local accesses and the identity of the sending and receiving processors may be calculated by applying Fourier-Motzkin elimination <ref> [1] </ref>. The SUIF compiler also uses array data-flow information to optimize the communication whenever possible. Array data-flow information identifies the processor that produces the values desired, thus data can be sent from the producer to the consumer as soon as the data are produced. <p> The compiler must also manage buffers for non-local data. Finally, the code generator generates necessary send and receive operations. Again, linear inequalities are used to represent the data accesses. Projections of these inequalities are used to generate the send and receive code for each processor <ref> [1] </ref>. The compiler-generated DAS code for the example is shown in Figure 4. 5 Conclusions This paper shows how the compiler techniques used to generate correct and efficient code are common to all scalable machines, independent of whether the address space is shared or distributed.
Reference: [2] <author> C. Ancourt and F. Irigoin, </author> <title> Scanning polyhera with do loops, </title> <booktitle> in Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The compiler generates the bounds of each loop in the parallel SPMD code by projecting the polyhedron represented by this system of inequalities onto a lower-dimensional space <ref> [2] </ref>. The generated code is parameterized by the number of processors; each processor gets the number of processors and its processor id from calls to the run-time library. Next, the code generator changes the original data declarations according to the data mapping derived by the parallelism and locality optimizer.
Reference: [3] <author> J. M. Anderson and M. S. Lam, </author> <title> Global optimizations for parallelism and locality on scalable parallel machines, </title> <booktitle> in Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This paper discusses the fundamental issues in optimization and code generation needed for all scalable parallel machines. We give an overview of the algorithms used in the SUIF compiler to address these issues. The details of the individual algorithms can be found elsewhere <ref> [1, 3, 6, 8, 10] </ref>. We have currently implemented a complete compiler that generates code for SAS machines, and our DAS code generator is in progress. <p> It reorders the computation to discover the largest granularity of parallelism using unimodular code transformations (e.g. loop interchange, skewing and reversal). A global analysis phase then examines all loop nests together to determine the best overall mapping of data and computation across the processors of the machine <ref> [3] </ref>. The global analysis phase represents the mappings of computation and data onto the processors as the composition of two functions: an affine function that maps onto a virtual processor space, and a folding function that maps the virtual space onto the physical processors of the target machine.
Reference: [4] <author> M. W. Hall, B. R. Murphy, and S. P. Amarasinghe, </author> <title> Interprocedural parallelization analysis: A case study, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: We have also implemented a pass that recognizes reductions to both scalar and array variables. Our intraprocedural version of these optimizations are fully functional, and we are experimenting with our interprocedural version <ref> [4] </ref>. Parallelism and Locality Analysis. The parallelism and locality analysis phase first identifies and optimizes the loop-level parallelism in the program. The compiler then maps the computation to the processors and determines a data layout for the array variables such that parallelism is maximized while minimizing communication.
Reference: [5] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran language specification, version 1.0, </title> <type> Tech. Rep. CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The need to optimize the data layout for DAS machines is well understood. For example, the HPF language <ref> [5] </ref> is designed so that the user can explicitly guide this optimization process.
Reference: [6] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> Compiling Fortran D for MIMD distributed-memory machines, </title> <journal> Communications of the ACM, </journal> <volume> 35 (1992), </volume> <pages> pp. 66-80. </pages>
Reference-contexts: This paper discusses the fundamental issues in optimization and code generation needed for all scalable parallel machines. We give an overview of the algorithms used in the SUIF compiler to address these issues. The details of the individual algorithms can be found elsewhere <ref> [1, 3, 6, 8, 10] </ref>. We have currently implemented a complete compiler that generates code for SAS machines, and our DAS code generator is in progress.
Reference: [7] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy, </author> <title> The DASH prototype: Implementation and performance, </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We are targeting distributed address space (DAS) machines, such as the Intel Paragon and the IBM SP2, as well as shared address space (SAS) machines, such as the Stanford DASH multiprocessor <ref> [7] </ref> and the Kendall Square Research KSR-1. On SAS machines, the shared address-space is maintained in hardware using coherent caches. At first glance, it seems much easier to compile to SAS machines as compared to DAS machines. On SAS machines, the programmer need not explicitly manage communication for non-local data.
Reference: [8] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam, </author> <title> Array data-flow analysis and its use in array privatization, </title> <booktitle> in Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Charleston, SC, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This paper discusses the fundamental issues in optimization and code generation needed for all scalable parallel machines. We give an overview of the algorithms used in the SUIF compiler to address these issues. The details of the individual algorithms can be found elsewhere <ref> [1, 3, 6, 8, 10] </ref>. We have currently implemented a complete compiler that generates code for SAS machines, and our DAS code generator is in progress.
Reference: [9] <author> Stanford SUIF Compiler Group, </author> <title> SUIF: A parallelizing & optimizing research compiler, </title> <type> Tech. Rep. </type> <institution> CSL-TR-94-620, Computer Systems Lab, Stanford University, </institution> <month> May </month> <year> 1994. </year>
Reference: [10] <author> M. E. Wolf, </author> <title> Improving Locality and Parallelism in Nested Loops, </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: This paper discusses the fundamental issues in optimization and code generation needed for all scalable parallel machines. We give an overview of the algorithms used in the SUIF compiler to address these issues. The details of the individual algorithms can be found elsewhere <ref> [1, 3, 6, 8, 10] </ref>. We have currently implemented a complete compiler that generates code for SAS machines, and our DAS code generator is in progress. <p> Both SAS and DAS machines thus benefit from optimizing the data layout for locality. Our compiler algorithm to maximize interprocessor parallelism and locality is divided into two phases. First a local analysis phase optimizes parallelism at the loop level <ref> [10] </ref>. It reorders the computation to discover the largest granularity of parallelism using unimodular code transformations (e.g. loop interchange, skewing and reversal). A global analysis phase then examines all loop nests together to determine the best overall mapping of data and computation across the processors of the machine [3].
References-found: 10

