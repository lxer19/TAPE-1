URL: http://www.tc.cornell.edu/~coleman/PAPERS/liu2.ps
Refering-URL: http://www.tc.cornell.edu/~coleman/papers.html
Root-URL: http://www.tc.cornell.edu
Title: A QUASI-NEWTON QUADRATIC PENALTY METHOD FOR MINIMIZATION SUBJECT TO NONLINEAR EQUALITY CONSTRAINTS  
Author: THOMAS F. COLEMANy JIANGUO LIUz AND WEI YUANx 
Keyword: Key words. nonlinearly constrained optimization, equality constraints, quasi-Newton methods, BFGS, quadratic penalty function, reduced Hessian approximation  
Note: AMS(MOS) subject classifications. 65K05, 65K10, 65H10, 90C30, 90C05, 68L10  
Abstract: We present a modified quadratic penalty function method for equality constrained optimization problems. The pivotal feature of our algorithm is that at every iterate we invoke a special change of variables to improve the ability of the algorithm to follow the constraint level sets. This change of variables gives rise to a suitable block diagonal approximation to the Hessian which is then used to construct a quasi-Newton method. We show that the complete algorithm is globally convergent. Preliminary computational results are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.C. Bartholomew-Biggs, </author> <title> "Constrained minimization using recursive quadratic programming", In F.A. Lootsma, </title> <editor> ed., </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pp. 411-428, </pages> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1972. </year>
Reference-contexts: An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in [15]. Methods based on the union of quasi-Newton method and quadratic penalty function have been seen in <ref> [1, 17] </ref>. We can broadly classify this body of work into two categories: local projected quasi-Newton updating strategies for the nonlinearly constrained problem (1.1) and the use of the quadratic penalty function to force convergence from remote points. We briefly discuss each category in turn.
Reference: [2] <author> P.T. Boggs, J.W. Tolls and P. Wang, </author> <title> "On the local convergence of quasi-Newton methods for constrained optimization", </title> <note> SIAM Journal of Control and Optimization 20 (1982) 161-171. </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S.
Reference: [3] <author> I. Bongartz, A.R. Conn, N.I.M. Could and Ph.L. Toint, "CUTE: </author> <title> constrained and unconstrained testing environment", </title> <type> Research Report, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <year> (1993). </year>
Reference-contexts: Numerical Results. In this section we present results of numerical experiments illustrating the performance of Algorithm 2.1. The problem set consists of a number of nonlinear equality constrained problems selected from the CUTE collection <ref> [3] </ref> and two problems generated by the authors. <p> Most problems in Table 1 (all except TEST1 and TEST2) are from the CUTE collection <ref> [3] </ref>.
Reference: [4] <author> R.H. Byrd and J. Nocedal, </author> <title> "An analysis of reduced Hessian methods for constrained optimization", </title> <note> Mathematical Programming 49(1991) 285-323. </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S. <p> But using Theorem 3.1 it follows from (3.41) that lim k!1 kh k k = 0 and lim k!1 kv k k = 0. Hence, lim k!1 kx k+1 x k k = 0. 15 3.1. Remarks. In <ref> [4] </ref> Byrd and Nocedal propose algorithms based on reduced Hessian methods. Byrd and Nocedal prove that, for their algorithms, lim [kZ (x k ) T rf (x k )k + kc (x k )k] = 0 (3.45) under an assumption stronger than condition (3.34). <p> In particular, Byrd and Nocedal assume that there exists a fl &gt; 0 such that eig min (Z T k r 2 L (x; k )Z k ) fl; 8x in the line search segment: (3.46) Moreover, algorithms in <ref> [4] </ref> cannot preserve the positive definiteness of B k without assumption (3.46). However, assumption (3.46) is rarely satisfied when x k is far away from the solution. Therefore, in contrast to Algorithm 2.1, algorithms in [4] may fail when applied to general nonlinear functions. 4. Numerical Results. <p> k )Z k ) fl; 8x in the line search segment: (3.46) Moreover, algorithms in <ref> [4] </ref> cannot preserve the positive definiteness of B k without assumption (3.46). However, assumption (3.46) is rarely satisfied when x k is far away from the solution. Therefore, in contrast to Algorithm 2.1, algorithms in [4] may fail when applied to general nonlinear functions. 4. Numerical Results. In this section we present results of numerical experiments illustrating the performance of Algorithm 2.1.
Reference: [5] <author> R.H. Byrd and R.B. Schnabel, </author> <title> "Continuity of the null space basis and constrained optimization", </title> <note> Mathematical Programming 35(1986) 32-41. </note>
Reference-contexts: Coleman and Conn [7] establish a 2-step superlinear convergence result for this algorithm; subsequently, this result was strengthened <ref> [5, 6] </ref> to 1-step superlinear convergence of the intermediate sequence fx (k) + Z (k) h (k) g. A number of variations of this basic scheme have now been proposed e.g., [18], using different definitions of y (k) and slightly different corrections for x (k) .
Reference: [6] <author> T.F. Coleman, </author> <title> "On characterizations of superlinear convergence for constrained optimization", </title> <note> Lectures in Applied Mathematics 26(1990) 113-133. </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S. <p> Coleman and Conn [7] establish a 2-step superlinear convergence result for this algorithm; subsequently, this result was strengthened <ref> [5, 6] </ref> to 1-step superlinear convergence of the intermediate sequence fx (k) + Z (k) h (k) g. A number of variations of this basic scheme have now been proposed e.g., [18], using different definitions of y (k) and slightly different corrections for x (k) .
Reference: [7] <author> T.F. Coleman and A.R. Conn, </author> <title> "On the local convergence of a quasi-Newton method for the nonlinear programming problem", </title> <note> SIAM Journal on Numerical Analysis 21(1984) 755-769. </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S. <p> In particular, second-order optimality conditions strongly suggest that approximating the reduced Hessian of the Lagrangian function is the right thing to do. Indeed, the effective asymptotic methods do just this <ref> [7, 12, 18] </ref>. However, local minimizers of (1.1) are not necessarily local minimizers of the Lagrangian function; therefore, it is not possible to design consistent line search rules based on the Lagrangian function. <p> Local projected quasi-Newton methods approximate H (x (k) ) with a positive definite matrix B (k) . In most projected Hessian methods B (k) is updated using Broyden's class of formulas, e.g., BFGS. For example, Coleman and Conn <ref> [7] </ref> propose the following local quasi-Newton method: solve B (k) h (k) = (Z (k) ) T rf (k) (1.2) v (k) A (k) ((A (k) ) T A (k) ) 1 c (x (k) + Z (k) h (k) ) (1.3) Matrix B (k) is updated using the BFGS formula <p> An important point is that in a neighborhood of a strong minimizer to (1.1) the inner product (y (k) ) T h (k) is positive and therefore the reduced BFGS update is well-defined; positive definiteness is preserved. Coleman and Conn <ref> [7] </ref> establish a 2-step superlinear convergence result for this algorithm; subsequently, this result was strengthened [5, 6] to 1-step superlinear convergence of the intermediate sequence fx (k) + Z (k) h (k) g. <p> ff l h &gt; 0 such that ffd (k) h satisfies (2.14, 2.15) if ff 2 (ff l h ; ff u The importance of this result is the implication that sufficient decrease along the curved path u (d (k) h (ff)) is compatible with the projected BFGS update e.g., <ref> [7] </ref>. This follows because (2.15) and r h p (u (0)) T d (k) h &lt; 0 imply (y (k) ) T s (k) &gt; 0 where y (k) = rp (h (k) ) rp (x (k) ). 2.2. The Normal Step.
Reference: [8] <author> T.F. Coleman and C. Hempel, </author> <title> "Computing a trust region step for a penalty function", </title> <note> SIAM Journal on Scientific Computing 11(1990) 180-201. </note>
Reference-contexts: A curvilinear search solves this zigzagging problem and allows for large steps. Another traditional criticism of the quadratic penalty function concerns the asymptotic ill-conditioning of the Hessian matrix. However, several studies e.g., <ref> [8, 13] </ref> have shown how to circumvent possible negative effects of this ill-conditioning by either using "extended" systems, or orthogonal transformations to isolate the ill-conditioning. 2 Our proposed approach is in line with the latter view. <p> Related and Supporting Work. There is considerable literature on the quadratic penalty function. The fundamental reference is Fiacco and McCormick's [10] influential book. Numerically sound approaches for dealing with ill-conditioning of the Hessian are given in <ref> [8, 13] </ref>. An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in [15]. Methods based on the union of quasi-Newton method and quadratic penalty function have been seen in [1, 17].
Reference: [9] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> "Numerical methods for unconstrained optimization and nonlinear equations", </title> <publisher> (Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983). </year>
Reference-contexts: We state this result formally: the proof is a straightforward adaptation of Thm 6.3.2 in Dennis and Schnabel <ref> [9] </ref>. Lemma 2.2. Suppose the functions f; c i : &lt; n ! &lt;, i = 1; : : : ; m are continuously differentiable on &lt; n . <p> T c (k) &lt; Therefore, rp (x (k) ) T d (k) v + v kc (k) k 2 &lt; 0: (2.18) Assuming p is bounded below in the direction Y (k) d (k) v , a sufficient decrease step size condition follows (see Thm 6.3.2 in Dennis and Schnabel <ref> [9] </ref>): Lemma 2.4. Suppose the functions f; c i : &lt; n ! &lt;, i = 1; : : : ; m are continuously differentiable on &lt; n . Assume fp (x (k) + fiY (k) d (k) v ) : fi &gt; 0g is bounded below. <p> Most problems in Table 1 (all except TEST1 and TEST2) are from the CUTE collection [3]. Problem TEST1 is minimization of a Rosenbrock function <ref> [9] </ref> with quadratic equality constraints, i.e., minimize P n1 i ) 2 ] subject to a T i x + :5x T M i x = 0; i = 1; : : : ; m; where a i 2 &lt; n , i = 1; 2; : : : ; m,
Reference: [10] <author> A.V. </author> <title> Fiacco and G.P. McCormick "Nonlinear Programming: Sequential Unconstrained Minimization Techniques", </title> <publisher> John Wiley and Sons, </publisher> <year> 1968. </year>
Reference-contexts: Related and Supporting Work. There is considerable literature on the quadratic penalty function. The fundamental reference is Fiacco and McCormick's <ref> [10] </ref> influential book. Numerically sound approaches for dealing with ill-conditioning of the Hessian are given in [8, 13]. An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in [15].
Reference: [11] <author> R.Fontecilla, </author> <title> "Local convergence of secant methods for nonlinear constrained optimization", </title> <note> SIAM Journal on Numerical Analysis 25(1988) 692-712. </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S.
Reference: [12] <author> D. Gabay, </author> <title> "Reduced quasi-Newton methods with feasibility improvement for nonlinearly constrained optimization", Mathematical Programming Study 16(1982) 18-44. </title>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S. <p> In particular, second-order optimality conditions strongly suggest that approximating the reduced Hessian of the Lagrangian function is the right thing to do. Indeed, the effective asymptotic methods do just this <ref> [7, 12, 18] </ref>. However, local minimizers of (1.1) are not necessarily local minimizers of the Lagrangian function; therefore, it is not possible to design consistent line search rules based on the Lagrangian function.
Reference: [13] <author> N.I.M. Gould, </author> <title> "On the accurate determination of search directions for simple differentiable penalty functions", </title> <note> I.M.A. Journal on Numerical Analysis 6(1986) 357-372. 18 </note>
Reference-contexts: A curvilinear search solves this zigzagging problem and allows for large steps. Another traditional criticism of the quadratic penalty function concerns the asymptotic ill-conditioning of the Hessian matrix. However, several studies e.g., <ref> [8, 13] </ref> have shown how to circumvent possible negative effects of this ill-conditioning by either using "extended" systems, or orthogonal transformations to isolate the ill-conditioning. 2 Our proposed approach is in line with the latter view. <p> Related and Supporting Work. There is considerable literature on the quadratic penalty function. The fundamental reference is Fiacco and McCormick's [10] influential book. Numerically sound approaches for dealing with ill-conditioning of the Hessian are given in <ref> [8, 13] </ref>. An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in [15]. Methods based on the union of quasi-Newton method and quadratic penalty function have been seen in [1, 17].
Reference: [14] <author> N.I.M. Gould, </author> <title> "On the convergence of a sequential penalty function method for constrained minimization", </title> <note> SIAM J. on Numerical Analysis 26(1989) 107-128. </note>
Reference-contexts: That is, first order necessary conditions for problem (1.1) are satisfied inexactly: ( b) kc (x (k) )k fl (k) : Upon satisfaction of (2.21), is reduced, yielding + satisfying: 6=5 + ; (2.22) where &lt; 1. Gould's analysis <ref> [14] </ref> underpins conditions (2.22). 8 2.3. The Algorithm. Next we present the algorithm which mixes (tangential) path searches with normal steps.
Reference: [15] <author> W.W. Hager, </author> <title> "Analysis and implementation of a dual algorithm for constrained optimization", </title> <journal> Journal of Optimization Theory and Applications, </journal> <month> 79 </month> <year> (1993) </year> <month> 427-462. </month>
Reference-contexts: The fundamental reference is Fiacco and McCormick's [10] influential book. Numerically sound approaches for dealing with ill-conditioning of the Hessian are given in [8, 13]. An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in <ref> [15] </ref>. Methods based on the union of quasi-Newton method and quadratic penalty function have been seen in [1, 17].
Reference: [16] <author> J.J. </author> <title> More and D.C. Sorensen, "Computing a trust region step", </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <pages> 4(1983) 553-572. </pages>
Reference-contexts: Suppose fx k g does not converge. Since x fl is an isolated accumulation point of fx k g, there exists a subsequence fx k j g of fx k g and an * &gt; 0 such that kx k j +1 x k j k * (Lemma 4.10, <ref> [16] </ref>). But using Theorem 3.1 it follows from (3.41) that lim k!1 kh k k = 0 and lim k!1 kv k k = 0. Hence, lim k!1 kx k+1 x k k = 0. 15 3.1. Remarks. In [4] Byrd and Nocedal propose algorithms based on reduced Hessian methods.
Reference: [17] <author> W. Murray, </author> <title> "An algorithm for constrained minimization", </title> <editor> In R. Fletcher, ed., </editor> <booktitle> Optimization, </booktitle> <pages> pp. 189-196, </pages> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1969. </year>
Reference-contexts: An algorithm somewhat similar to ours for constrained optimization using penalty function and constrained step is proposed in [15]. Methods based on the union of quasi-Newton method and quadratic penalty function have been seen in <ref> [1, 17] </ref>. We can broadly classify this body of work into two categories: local projected quasi-Newton updating strategies for the nonlinearly constrained problem (1.1) and the use of the quadratic penalty function to force convergence from remote points. We briefly discuss each category in turn.
Reference: [18] <author> J. Nocedal and M. Overton, </author> <title> "Projected Hessian updating algorithms for nonlinearly constrained optimization", </title> <note> SIAM Journal on Numerical Analysis 22(1985) 821-850. 19 </note>
Reference-contexts: The problem is not in the asymptotics where there are now many effective choices, especially with respect to reduced Hessian approximations, e.g., <ref> [2, 4, 6, 7, 11, 12, 18] </ref>. The main problem lies in smoothly connecting global fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research the U.S. <p> In particular, second-order optimality conditions strongly suggest that approximating the reduced Hessian of the Lagrangian function is the right thing to do. Indeed, the effective asymptotic methods do just this <ref> [7, 12, 18] </ref>. However, local minimizers of (1.1) are not necessarily local minimizers of the Lagrangian function; therefore, it is not possible to design consistent line search rules based on the Lagrangian function. <p> Coleman and Conn [7] establish a 2-step superlinear convergence result for this algorithm; subsequently, this result was strengthened [5, 6] to 1-step superlinear convergence of the intermediate sequence fx (k) + Z (k) h (k) g. A number of variations of this basic scheme have now been proposed e.g., <ref> [18] </ref>, using different definitions of y (k) and slightly different corrections for x (k) .
References-found: 18

