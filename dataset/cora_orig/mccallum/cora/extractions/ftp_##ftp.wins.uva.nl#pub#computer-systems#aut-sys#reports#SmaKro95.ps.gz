URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/SmaKro95.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: smagt@fwi.uva.nl  
Phone: Phone: +31 20 5257463, Fax: +31 20 5257490  
Title: Using Many-Particle Decomposition to get a Parallel Self-Organising Map  
Author: Patrick van der Smagt and Ben Krose 
Date: September 8, 1995  
Address: Kruislaan 403, NL-1098 SJ Amsterdam  
Affiliation: Department of Computer Systems University of Amsterdam  
Abstract: We propose a method for decreasing the computational complexity of self-organising maps. The method uses a partitioning of the neurons into disjoint clusters. Teaching of the neurons occurs on a cluster-basis instead of on a neuron-basis. For teaching an N-neuron network with N 0 samples, the computational complexity decreases from O(N 0 N) to O(N 0 log N). Furthermore, we introduce a measure for the amount of order in a self-organising map, and show that the introduced algorithm behaves as well as the original algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. W. Appel. </author> <title> An efficient program for many-body simulation. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6, </volume> <year> 1985. </year>
Reference-contexts: The neurons in those clusters are not (yet) updated, only the cluster weight mean is updated. The algorithm is derived from hierarchical methods for the simulation of many-particle systems <ref> [1] </ref>. In these methods, the interaction of N bodies that exert forces on each other (e.g., molecules in a gas) is simulated. <p> Apart from measuring the computation time, we have to determine if the resulting map has the same amount of order as the original map. We tested a normal Kohonen network against the above implementation for several network configurations and cluster sizes. Samples are uniformly drawn from <ref> [0; 1] </ref> d . For each network the average error E is computed after 100,000 learning trials.
Reference: [2] <author> H.-U. Bauer and K. R. Pawelzik. </author> <title> Quantifying the neighborhood preservation of self-organizing feature maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(4) </volume> <pages> 570-579, </pages> <year> 1992. </year>
Reference-contexts: Several approaches have been presented in literature to quantify this order <ref> [2, 11, 10] </ref>. We will discuss two of them. 3.1.1 The method by Bauer and Pawelzik Bauer and Pawelzik [2] introduce the following quantification. Let V be the m-dimensional input space of the SOM and A be the d-dimensional discretised network space. <p> Several approaches have been presented in literature to quantify this order [2, 11, 10]. We will discuss two of them. 3.1.1 The method by Bauer and Pawelzik Bauer and Pawelzik <ref> [2] </ref> introduce the following quantification. Let V be the m-dimensional input space of the SOM and A be the d-dimensional discretised network space.
Reference: [3] <author> L. Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Since the computational cost O (N 2 ) is too high, a hierarchical decomposition of the particle space into clusters, in which the average forces that the clusters exert on each other is computed. Furthermore, Greengard <ref> [3] </ref> gives a detailed analysis of the errors that result from the clustering, which can be used in assessing the errors introduced by our clustering/learning method. 3 Experiments 3.1 Quantifying the amount of order in a SOM In order to honestly compare the original and clustered SOM methods, we need to
Reference: [4] <author> G. E. Hinton. </author> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 185-234, </pages> <year> 1989. </year>
Reference-contexts: determined by forming connections as is done in the neural gas algorithm (see below): present the learning samples to the network after learning, and connect the first and second winning neurons. 3.1.3 Continuous measurement for the order In order to evaluate the resulting mapping, we use an error measure (cf. <ref> [4] </ref>) E = x 2 2 where k x indicates the winning neuron when the sample x is presented. This error measure can be shown to be minimised with the learning rule eq. (1). Consider a neural network with one neuron in a one-dimensional input space.
Reference: [5] <author> T. Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biolog--ical Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: Furthermore, neurons `in the neighbourhood' (i.e., neighbours along the grid) of k are similarly adapted, but with a learning rate ff 0 &lt; ff. This algorithm has been introduced by Teuvo Kohonen in the early eighties <ref> [5] </ref>. Later algorithmic improvements by Ritter and Schulten [8], in which the learning parameter ff 0 is a Gaussian-shaped curve depending on the distance between the winning and updated neuron, lead to proofs of stability [9] and convergence [6] of the algorithm (albeit for a one-dimensional grid).
Reference: [6] <author> Z.-P. Lo, Y. Lu, </author> <title> and KB. Bavarian. Analysis of the convergence properties using neural networks: Controllability and stabilization. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 207-220, </pages> <year> 1993. </year>
Reference-contexts: Later algorithmic improvements by Ritter and Schulten [8], in which the learning parameter ff 0 is a Gaussian-shaped curve depending on the distance between the winning and updated neuron, lead to proofs of stability [9] and convergence <ref> [6] </ref> of the algorithm (albeit for a one-dimensional grid).
Reference: [7] <author> Th. Martinetz. </author> <title> Competitive hebbian learning rule forms perfectly topology preserving maps. </title> <editor> In S. Gielen and B. Kappen, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <pages> pages 427-434. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: In the right figure, however, the topology of M is preserved. The M A is a monotonically decreasing function, and we obtain M A (8k) = 0 if and only if the SOM is a topologically correct map. As mentioned in <ref> [7] </ref>, the adjacency of two receptive fields R i and R j can be determined by forming connections as is done in the neural gas algorithm (see below): present the learning samples to the network after learning, and connect the first and second winning neurons. 3.1.3 Continuous measurement for the order
Reference: [8] <author> H. Ritter and K. Schulten. </author> <title> Topology conserving mappings for learning motor tasks. </title> <editor> In J. S. Denker, editor, </editor> <booktitle> Neural Networks of Computing, </booktitle> <pages> pages 376-380. </pages> <booktitle> AIP Conference Proceedings 151, </booktitle> <address> Snowbird, Utah, </address> <year> 1986. </year>
Reference-contexts: Furthermore, neurons `in the neighbourhood' (i.e., neighbours along the grid) of k are similarly adapted, but with a learning rate ff 0 &lt; ff. This algorithm has been introduced by Teuvo Kohonen in the early eighties [5]. Later algorithmic improvements by Ritter and Schulten <ref> [8] </ref>, in which the learning parameter ff 0 is a Gaussian-shaped curve depending on the distance between the winning and updated neuron, lead to proofs of stability [9] and convergence [6] of the algorithm (albeit for a one-dimensional grid). <p> We end up with a network which is a discrete representation of input space, where each neuron takes care of that part of the input space that coincides with its own `region of influence.' The resulting structure can be used for, e.g., robot navigation <ref> [8] </ref>. As mentioned, convergence properties of eq. (1) are noticeably inferior to those of eq. (2), and for that reason the latter is almost exclusively used.
Reference: [9] <author> H. Ritter and K. Schulten. </author> <title> Convergence properties of Kohonen's topology conserving maps: Fluctuations, stability, and dimension selection. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 59-71, </pages> <year> 1988. </year>
Reference-contexts: This algorithm has been introduced by Teuvo Kohonen in the early eighties [5]. Later algorithmic improvements by Ritter and Schulten [8], in which the learning parameter ff 0 is a Gaussian-shaped curve depending on the distance between the winning and updated neuron, lead to proofs of stability <ref> [9] </ref> and convergence [6] of the algorithm (albeit for a one-dimensional grid).
Reference: [10] <author> Th. Villmann, R. Der, and Th. Martinetz. </author> <title> A novel approach to measure the topology preservation of feature maps. </title> <editor> In M. Marinaro and P. G. Morasso, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <pages> pages 298-301. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Several approaches have been presented in literature to quantify this order <ref> [2, 11, 10] </ref>. We will discuss two of them. 3.1.1 The method by Bauer and Pawelzik Bauer and Pawelzik [2] introduce the following quantification. Let V be the m-dimensional input space of the SOM and A be the d-dimensional discretised network space. <p> The deviation of P 1 and P 2 from 1 can be used to determine the `topology preservingness' of the SOM. 3.1.2 The method by Villmann et al. As Villmann et al. <ref> [10] </ref> point out, the above method only works for linear manifolds M . In the case that the SOM obtains samples from a submanifold M V , the above measures do not work correctly if the mapping of M on V is nonlinear.
Reference: [11] <author> S. Zrehen. </author> <title> Analyzing Kohonen maps with geometry. </title> <editor> In S. Gielen and B. Kappen, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <pages> pages 609-612. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Several approaches have been presented in literature to quantify this order <ref> [2, 11, 10] </ref>. We will discuss two of them. 3.1.1 The method by Bauer and Pawelzik Bauer and Pawelzik [2] introduce the following quantification. Let V be the m-dimensional input space of the SOM and A be the d-dimensional discretised network space.
References-found: 11

