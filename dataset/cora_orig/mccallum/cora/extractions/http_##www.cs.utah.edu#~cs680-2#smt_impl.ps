URL: http://www.cs.utah.edu/~cs680-2/smt_impl.ps
Refering-URL: http://www.cs.utah.edu/~cs680-2/index.html
Root-URL: 
Title: Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor  
Author: Dean M. Tullsen Susan J. Eggers Joel S. Emer Henry M. Levy Jack L. Lo and Rebecca L. Stamm 
Address: HLO2-3/J3 Box 352350 77 Reed Road Seattle, WA 98195-2350 Hudson, MA 01749  
Affiliation: Dept of Computer Science and Engineering Digital Equipment Corporation University of Washington  
Abstract: Simultaneous multithreading is a technique that permits multiple independent threads to issue multiple instructions each cycle. In previous work we demonstrated the performance potential of simultaneous multithreading, based on a somewhat idealized model. In this paper we show that the throughput gains from simultaneous multithreading can be achieved without extensive changes to a conventional wide-issue superscalar, either in hardware structures or sizes. We present an architecture for simultaneous multithreading that achieves three goals: (1) it minimizes the architectural impact on the conventional superscalar design, (2) it has minimal performance impact on a single thread executing alone, and (3) it achieves significant throughput gains when running multiple threads. Our simultaneous multithreading architecture achieves a throughput of 5.4 instructions per cycle, a 2.5-fold improvement over an unmodified superscalar with similar hardware resources. This speedup is enhanced by an advantage of multithreading previously unexploited in other architectures: the ability to favor for fetch and issue those threads most efficiently using the processor each cycle, thereby providing the best instructions to the processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> a processor architecture for multiprocessing. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Simultaneous multithreading (SMT) is a technique that permits multiple independent threads to issue multiple instructions each cycle to a superscalar processor's functional units. SMT combines the multiple-instruction-issue features of modern superscalars with the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref> [1, 2, 15, 23] </ref>, which depend on fast context switching to share processor execution resources, all hardware contexts in an SMT processor are active simultaneously, competing each cycle for all available resources.
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Simultaneous multithreading (SMT) is a technique that permits multiple independent threads to issue multiple instructions each cycle to a superscalar processor's functional units. SMT combines the multiple-instruction-issue features of modern superscalars with the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref> [1, 2, 15, 23] </ref>, which depend on fast context switching to share processor execution resources, all hardware contexts in an SMT processor are active simultaneously, competing each cycle for all available resources. <p> Gulati and Bagherzadeh model an instruction window composed of four-instruction blocks, each block holding instructions from a single thread. The M-Machine [9] and the Multiscalar project [25] combine multiple-issue with multithreading, but assign work onto processors at a coarser level than individual instructions. Tera <ref> [2] </ref> combines LIW with fine-grain multithreading. 9 Summary This paper presents a simultaneous multithreading architecture that: * borrows heavily from conventional superscalar design, requir ing little additional hardware support, * minimizes the impact on single-thread performance, running only 2% slower in that scenario, and * achieves significant throughput improvements over the
Reference: [3] <author> C.J. Beckmann and C.D. Polychronopoulos. </author> <title> Microarchitec-ture support for dynamic scheduling of acyclic task graphs. </title> <booktitle> In 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 140-148, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Daddis and Torng [6] plot increases in instruction throughput as a function of the fetch bandwidth and the size of the dispatch stack, a structure similar to our instruction queue. Their system has two threads, unlimited functional units, and unlimited issue bandwidth. In addition to these, Beckmann and Polychronopoulus <ref> [3] </ref>, Gun-ther [12], Li and Chu [16], and Govindarajan, et al., [10] all discuss architectures that feature simultaneous multithreading, none of which can issue more than one instruction per cycle per thread.
Reference: [4] <author> B. Calder and D. Grunwald. </author> <title> Fast and accurate instruction fetch and branch prediction. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-11, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme <ref> [4] </ref>. We use a 256-entry BTB, organized as four-way set associative. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register [18, 30]. Return destinations are predicted with a 12-entry return stack (per context).
Reference: [5] <author> T.M. Conte, K.N. Menezes, P.M. Mills, and B.A. Patel. </author> <title> Opti--mization of instruction fetch mechanisms for high issue rates. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 333-344, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: 7% 10% 9% fp IQ-full (% of cycles) 14% 9% 3% avg (combined) queue population 25 25 27 wrong-path instructions fetched 24% 7% 7% wrong-path instructions issued 9% 4% 3% Table 3: The result of increased multithreading on some low-level metrics for the base architecture. even for smaller block sizes <ref> [5, 24] </ref>. In this processor, we can spread the burden of filling the fetch bandwidth among multiple threads. For example, the probability of finding four instructions from each of two threads should be greater than that of finding eight from one thread.
Reference: [6] <author> G.E. Daddis, Jr. and H.C. Torng. </author> <title> The concurrent execution of multiple instruction streams on superscalar processors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:76-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Gulati and Bagherzadeh [11] model a 4-issue machine with four hardware contexts and a single compiler-partitioned register file. Keckler and Dally [14] and Prasadh and Wu [19] describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units. Daddis and Torng <ref> [6] </ref> plot increases in instruction throughput as a function of the fetch bandwidth and the size of the dispatch stack, a structure similar to our instruction queue. Their system has two threads, unlimited functional units, and unlimited issue bandwidth.
Reference: [7] <author> K.M. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In COMPCON, Spring 1992, </booktitle> <pages> pages 305-310, </pages> <year> 1992. </year>
Reference-contexts: We eventually squash all wrong-path instructions a cycle after a branch misprediction is discovered in the exec stage. Our throughput results only count useful instructions. Our workload comes primarily from the SPEC92 benchmark suite <ref> [7] </ref>. We use five floating point programs (alvinn, doduc, fpppp, ora, and tomcatv) and two integer programs (espresso and xlisp) from that suite, and the document typesetting program TeX.
Reference: [8] <author> J. Edmondson and P. Rubinfield. </author> <title> An overview of the 21164 AXP microprocessor. </title> <booktitle> In Hot Chips VI, </booktitle> <pages> pages 1-8, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: We assume that all functional units are completely pipelined. Table 1 shows the instruction latencies, which are derived from the Alpha 21164 <ref> [8] </ref>. We assume a 32-entry integer instruction queue (which handles integer instructions and all load/store operations) and a 32-entry floating point queue, not significantly larger than the HP PA-8000 [21], which has two 28-entry queues.
Reference: [9] <author> M. Fillo, S.W. Keckler, W.J. Dally, N.P. Carter, A. Chang, Y. Gurevich, and W.S. Lee. </author> <title> The M-Machine multicomputer. </title> <booktitle> In 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Gulati and Bagherzadeh model an instruction window composed of four-instruction blocks, each block holding instructions from a single thread. The M-Machine <ref> [9] </ref> and the Multiscalar project [25] combine multiple-issue with multithreading, but assign work onto processors at a coarser level than individual instructions.
Reference: [10] <author> R. Govindarajan, S.S. Nemawarkar, and P. LeNir. </author> <title> Design and peformance evaluation of a multithreaded architecture. </title> <booktitle> In First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 298-307, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Their system has two threads, unlimited functional units, and unlimited issue bandwidth. In addition to these, Beckmann and Polychronopoulus [3], Gun-ther [12], Li and Chu [16], and Govindarajan, et al., <ref> [10] </ref> all discuss architectures that feature simultaneous multithreading, none of which can issue more than one instruction per cycle per thread. Our work is distinguished from most of these studies in our dual goals of maintaining high single-thread performance and minimizing the architectural impact on a conventional processor.
Reference: [11] <author> M. Gulati and N. Bagherzadeh. </author> <title> Performance study of a mul-tithreaded superscalar microprocessor. </title> <booktitle> In Second International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 291-301, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). Yamamoto and Nemirovsky [28] simulate an SMT architecture with separate instruction queues and up to four threads. Gulati and Bagherzadeh <ref> [11] </ref> model a 4-issue machine with four hardware contexts and a single compiler-partitioned register file. Keckler and Dally [14] and Prasadh and Wu [19] describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units.
Reference: [12] <author> B.K. Gunther. </author> <title> Superscalar performance in a multithreaded microprocessor. </title> <type> PhD thesis, </type> <institution> University of Tasmania, </institution> <month> Decem-ber </month> <year> 1993. </year>
Reference-contexts: Their system has two threads, unlimited functional units, and unlimited issue bandwidth. In addition to these, Beckmann and Polychronopoulus [3], Gun-ther <ref> [12] </ref>, Li and Chu [16], and Govindarajan, et al., [10] all discuss architectures that feature simultaneous multithreading, none of which can issue more than one instruction per cycle per thread.
Reference: [13] <author> H. Hirata, K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and T. Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This paper presents an architecture that realizes much of the potential demonstrated by that work, simulating it in detail. Hirata, et al., <ref> [13] </ref> present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel ray-tracing application. They do not simulate caches or TLBs and their architecture has no branch prediction mechanism. Yamamoto, et al., [29] present an analytical model of multithreaded superscalar performance, backed up by simulation.
Reference: [14] <author> S.W. Keckler and W.J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Yamamoto and Nemirovsky [28] simulate an SMT architecture with separate instruction queues and up to four threads. Gulati and Bagherzadeh [11] model a 4-issue machine with four hardware contexts and a single compiler-partitioned register file. Keckler and Dally <ref> [14] </ref> and Prasadh and Wu [19] describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units. Daddis and Torng [6] plot increases in instruction throughput as a function of the fetch bandwidth and the size of the dispatch stack, a structure similar to our instruction queue.
Reference: [15] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A multithreading technique targeting multiprocessors and workstations. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 308-318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Simultaneous multithreading (SMT) is a technique that permits multiple independent threads to issue multiple instructions each cycle to a superscalar processor's functional units. SMT combines the multiple-instruction-issue features of modern superscalars with the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref> [1, 2, 15, 23] </ref>, which depend on fast context switching to share processor execution resources, all hardware contexts in an SMT processor are active simultaneously, competing each cycle for all available resources.
Reference: [16] <author> Y. Li and W. Chu. </author> <title> The effects of STEF in finely parallel multithreaded processors. </title> <booktitle> In First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 318-325, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Their system has two threads, unlimited functional units, and unlimited issue bandwidth. In addition to these, Beckmann and Polychronopoulus [3], Gun-ther [12], Li and Chu <ref> [16] </ref>, and Govindarajan, et al., [10] all discuss architectures that feature simultaneous multithreading, none of which can issue more than one instruction per cycle per thread.
Reference: [17] <author> P.G. Lowney, S.M. Freudenberger, T.J. Karzes, W.D. Licht-enstein, R.P. Nix, J.S. ODonnell, and J.C. Ruttenberg. </author> <title> The multiflow trace scheduling compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1-2):51-142, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Each of the 8 runs uses a different combination of the benchmarks. We compile each program with the Multiflow trace scheduling compiler <ref> [17] </ref>, modified to produce Alpha code. In contrast to [27], we turn off trace scheduling in the compiler for this study, for two reasons.
Reference: [18] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> DEC-WRL, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: We use a 256-entry BTB, organized as four-way set associative. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [18, 30] </ref>. Return destinations are predicted with a 12-entry return stack (per context). We assume an efficient, but not perfect, implementation of dynamic memory disambiguation.
Reference: [19] <author> R.G. Prasadh and C.-L. Wu. </author> <title> A benchmark evaluation of a multi-threaded RISC processor architecture. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:84-91, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Yamamoto and Nemirovsky [28] simulate an SMT architecture with separate instruction queues and up to four threads. Gulati and Bagherzadeh [11] model a 4-issue machine with four hardware contexts and a single compiler-partitioned register file. Keckler and Dally [14] and Prasadh and Wu <ref> [19] </ref> describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units. Daddis and Torng [6] plot increases in instruction throughput as a function of the fetch bandwidth and the size of the dispatch stack, a structure similar to our instruction queue.
Reference: [20] <institution> Microprocessor Report, </institution> <month> October 24 </month> <year> 1994. </year>
Reference-contexts: Fetched instructions are then decoded and passed to the register renaming logic, which maps logical registers onto a pool of physical registers, removing false dependences. Instructions are then placed in one of two instruction queues. Those instruction queues are similar to the ones used by the MIPS R10000 <ref> [20] </ref> and the HP PA-8000 [21], in this case holding instructions until they are issued. Instructions are issued to the functional units out-of-order when their operands are available. After completing execution, instructions are retired in-order, freeing physical registers that are no longer needed.
Reference: [21] <institution> Microprocessor Report, </institution> <month> November 14 </month> <year> 1994. </year>
Reference-contexts: Instructions are then placed in one of two instruction queues. Those instruction queues are similar to the ones used by the MIPS R10000 [20] and the HP PA-8000 <ref> [21] </ref>, in this case holding instructions until they are issued. Instructions are issued to the functional units out-of-order when their operands are available. After completing execution, instructions are retired in-order, freeing physical registers that are no longer needed. Our SMT architecture is a straightforward extension to this conventional superscalar design. <p> Table 1 shows the instruction latencies, which are derived from the Alpha 21164 [8]. We assume a 32-entry integer instruction queue (which handles integer instructions and all load/store operations) and a 32-entry floating point queue, not significantly larger than the HP PA-8000 <ref> [21] </ref>, which has two 28-entry queues. The caches (Table 2) are multi-ported by interleaving them into banks, similar to the design of Sohi and Franklin [26]. We model lockup-free caches and TLBs. TLB misses require two full memory accesses and no execution resources.
Reference: [22] <author> E.G. Sirer. </author> <title> Measuring limits of fine-grained parallelism. Senior Independent Work, </title> <institution> Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This is emulated by using only part of the address (10 bits) to disambiguate memory references, so that it is occasionally over-conservative. 3 Methodology The methodology in this paper closely follows the simulation and measurement methodology of [27]. Our simulator uses emulation-based, instruction-level simulation, and borrows significantly from MIPSI <ref> [22] </ref>, a MIPS-based simulator. The simulator executes unmodified Alpha object code and models the execution pipelines, memory hierarchy, TLBs, and the branch prediction logic of the processor described in Section 2. In an SMT processor a branch misprediction introduces wrong-path instructions that interact with instructions from other threads.
Reference: [23] <author> B.J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE Real Time Signal Processing IV, </booktitle> <pages> pages 241-248, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Simultaneous multithreading (SMT) is a technique that permits multiple independent threads to issue multiple instructions each cycle to a superscalar processor's functional units. SMT combines the multiple-instruction-issue features of modern superscalars with the latency-hiding ability of multithreaded architectures. Unlike conventional multithreaded architectures <ref> [1, 2, 15, 23] </ref>, which depend on fast context switching to share processor execution resources, all hardware contexts in an SMT processor are active simultaneously, competing each cycle for all available resources.
Reference: [24] <author> M.D. Smith, M. Johnson, and M.A. Horowitz. </author> <title> Limits on multiple instruction issue. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 290-302, </pages> <year> 1989. </year>
Reference-contexts: 7% 10% 9% fp IQ-full (% of cycles) 14% 9% 3% avg (combined) queue population 25 25 27 wrong-path instructions fetched 24% 7% 7% wrong-path instructions issued 9% 4% 3% Table 3: The result of increased multithreading on some low-level metrics for the base architecture. even for smaller block sizes <ref> [5, 24] </ref>. In this processor, we can spread the burden of filling the fetch bandwidth among multiple threads. For example, the probability of finding four instructions from each of two threads should be greater than that of finding eight from one thread.
Reference: [25] <author> G.S. Sohi, </author> <title> S.E. Breach, </title> <booktitle> and T.N. Vijaykumar. Multiscalar processors. In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Gulati and Bagherzadeh model an instruction window composed of four-instruction blocks, each block holding instructions from a single thread. The M-Machine [9] and the Multiscalar project <ref> [25] </ref> combine multiple-issue with multithreading, but assign work onto processors at a coarser level than individual instructions.
Reference: [26] <author> G.S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The caches (Table 2) are multi-ported by interleaving them into banks, similar to the design of Sohi and Franklin <ref> [26] </ref>. We model lockup-free caches and TLBs. TLB misses require two full memory accesses and no execution resources.
Reference: [27] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This dynamic sharing of the functional units allows simultaneous multithread-ing to substantially increase throughput, attacking the two major impediments to processor utilization long latencies and limited per-thread parallelism. Tullsen, et al., <ref> [27] </ref> showed the potential of Proceedings of the 23rd Annual International Symposium on Computer Architecture, Philadelphia, PA, May, 1996 an SMT processor to achieve significantly higher throughput than either a wide superscalar or a multithreaded processor. <p> We assume an efficient, but not perfect, implementation of dynamic memory disambiguation. This is emulated by using only part of the address (10 bits) to disambiguate memory references, so that it is occasionally over-conservative. 3 Methodology The methodology in this paper closely follows the simulation and measurement methodology of <ref> [27] </ref>. Our simulator uses emulation-based, instruction-level simulation, and borrows significantly from MIPSI [22], a MIPS-based simulator. The simulator executes unmodified Alpha object code and models the execution pipelines, memory hierarchy, TLBs, and the branch prediction logic of the processor described in Section 2. <p> Each of the 8 runs uses a different combination of the benchmarks. We compile each program with the Multiflow trace scheduling compiler [17], modified to produce Alpha code. In contrast to <ref> [27] </ref>, we turn off trace scheduling in the compiler for this study, for two reasons. In our measurements, we want to differentiate between useful and useless speculative instructions, which is easy with hardware speculation, but not possible for software speculation with our system. <p> We also note, however, that the throughput peaks before 8 threads, and the processor utilization, at less than 50% of the 8-issue processor, is well short of the potential shown in <ref> [27] </ref>. We make several conclusions about the potential bottlenecks of this system as we approach 8 threads, aided by Figure 3 and Table 3. <p> This was achieved by a combination of (1) partitioning the fetch bandwidth over multiple threads, and (2) making that partition flexible. This is the same approach (although in a more limited fashion here) that simultaneous multithreading uses to improve the throughput of the functional units <ref> [27] </ref>. 5.2 Exploiting Thread Choice in the Fetch Unit The efficiency of the entire processor is affected by the quality of instructions fetched. A multithreaded processor has a unique ability to control that factor. <p> Also, register file access time will likely be a limiting factor in the number of threads an architecture can support. 8 Related Work A number of other architectures have been proposed that exhibit simultaneous multithreading in some form. Tullsen, et al., <ref> [27] </ref> demonstrated the potential for simultaneous multithreading, but did not simulate a complete architecture, nor did that paper present a specific solution to register file access or instruction scheduling. This paper presents an architecture that realizes much of the potential demonstrated by that work, simulating it in detail.
Reference: [28] <author> W. Yamamoto and M. Nemirovsky. </author> <title> Increasing superscalar performance through multistreaming. </title> <booktitle> In Conference on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 49-58, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Yamamoto, et al., [29] present an analytical model of multithreaded superscalar performance, backed up by simulation. Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). Yamamoto and Nemirovsky <ref> [28] </ref> simulate an SMT architecture with separate instruction queues and up to four threads. Gulati and Bagherzadeh [11] model a 4-issue machine with four hardware contexts and a single compiler-partitioned register file.
Reference: [29] <author> W. Yamamoto, M.J. Serrano, A.R. Talcott, R.C. Wood, and M. Nemirosky. </author> <title> Performance estimation of multistreamed, superscalar processors. </title> <booktitle> In Twenty-Seventh Hawaii International Conference on System Sciences, </booktitle> <pages> pages I:195-204, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Hirata, et al., [13] present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel ray-tracing application. They do not simulate caches or TLBs and their architecture has no branch prediction mechanism. Yamamoto, et al., <ref> [29] </ref> present an analytical model of multithreaded superscalar performance, backed up by simulation. Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). Yamamoto and Nemirovsky [28] simulate an SMT architecture with separate instruction queues and up to four threads.
Reference: [30] <author> T.-Y. Yeh and Y. Patt. </author> <title> Alternative implementations of two-level adaptive branch prediction. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124-134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We use a 256-entry BTB, organized as four-way set associative. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [18, 30] </ref>. Return destinations are predicted with a 12-entry return stack (per context). We assume an efficient, but not perfect, implementation of dynamic memory disambiguation.
References-found: 30

