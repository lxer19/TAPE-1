URL: http://L2R.cs.uiuc.edu/~danr/Papers/nonmon.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: danr@das.harvard.edu  
Title: Learning to Reason: The Non-Monotonic Case  
Author: Dan Roth 
Address: Cambridge, MA 02138.  
Affiliation: Division of Applied Sciences, Harvard University,  
Date: 1995  
Note: To appear in IJCAI  
Abstract: We suggest a new approach for the study of the non-monotonicity of human commonsense reasoning. The two main premises that underlie this work are that commonsense reasoning is an inductive phenomenon, and that missing information in the interaction of the agent with the environment may be as informative for future interactions as observed information. This intuition is formalized and the problem of reasoning from incomplete information is presented as a problem of learning attribute functions over a generalized domain. We consider examples that illustrate various aspects of the non-monotonic reasoning phenomena, which have been used over the years as bench-marks for various formalisms, and translate them into Learning to Reason problems. We demonstrate that these have concise representations over the generalized domain and prove that these representations can be learned efficiently. The framework developed suggests an operational approach to studying reasoning that is nevertheless rigorous and amenable to analysis. We show that this approach efficiently supports reasoning with incomplete information and at the same time matches our expectations of plausible patterns of reasoning in cases where other theories do not. This work continues previous works in the Learning to Reason framework, and supports the thesis that in order to develop a computational account for commonsense reasoning one should study the phenomena of learning and reasoning together.
Abstract-found: 1
Intro-found: 1
Reference: [ AI, 1980 ] <author> AI. </author> <note> Special issue on non-monotonic logic. Artificial Intelligence, 13(1,2), </note> <year> 1980. </year>
Reference: [ Angluin and Laird, 1988 ] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: This is usually done by augmenting the true knowledge the agent is given about the world with a set of default assumptions that capture what is typically the case. When presented with a query, the inference produced should agree with the true 2 Classification noise <ref> [ Angluin and Laird, 1988 ] </ref> occurs when there is some probability (the error rate) that the label of an example is flipped (from 0 to 1 or vice versa).
Reference: [ Angluin, 1988 ] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: A richer class of functions can be learned when given access to membership queries, in addition to examples <ref> [ Angluin, 1988; Blum et al., 1994; Bshouty, 1993 ] </ref> . Many of these algorithms can be extended to work over f0; 1; flg n .
Reference: [ Bacchus et al., 1993 ] <author> F. Bacchus, A. Grove, J. Y. Halpern, and D. Koller. </author> <title> Statistical foundations for default reasoning. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 563-569, </pages> <year> 1993. </year>
Reference-contexts: No general method exists according to which one can rank defaults [ Geffner, 1990 ] . The only way to figure out why and when certain defaults are preferred to others is to understand what the defaults say about the world. While probabilistic and statistical approaches <ref> [ Geffner, 1990; Bacchus et al., 1993 ] </ref> present an important step in this direction, they still suffer from some of the same problems [ Geffner, 1994 ] , and are infeasible computationally. The approach developed here does not use defaults. Rather, it is a theory of inference.
Reference: [ Blum et al., 1994 ] <author> A. Blum, R. Khardon, A. Kushilevitz, L. Pitt, and D. Roth. </author> <booktitle> On learning read-k satisfy-j DNF. In Proceedings of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 110-117, </pages> <year> 1994. </year> <note> (Submitted for publication). </note>
Reference-contexts: If we have efficient learning (to classify) algorithms for F that can tolerate classification noise, we can Learn to Reason with F . It turns out that many of the existing learning algorithms for Boolean functions studied in computational learning theory (see a survey in <ref> [ Blum et al., 1994 ] </ref> ) can be extended to learning algorithms over f0; 1; flg n . Since in all the examples considered in Section 3 we used the oracle EX (D) only we start by considering learning from examples only. <p> A richer class of functions can be learned when given access to membership queries, in addition to examples <ref> [ Angluin, 1988; Blum et al., 1994; Bshouty, 1993 ] </ref> . Many of these algorithms can be extended to work over f0; 1; flg n .
Reference: [ Bshouty, 1993 ] <author> N. H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the IEEE Symp. on Foundation of Computer Science, </booktitle> <pages> pages 302-311, </pages> <address> Palo Alto, CA., </address> <year> 1993. </year>
Reference-contexts: A richer class of functions can be learned when given access to membership queries, in addition to examples <ref> [ Angluin, 1988; Blum et al., 1994; Bshouty, 1993 ] </ref> . Many of these algorithms can be extended to work over f0; 1; flg n . <p> Many of these algorithms can be extended to work over f0; 1; flg n . In particular, using the algorithms studied in <ref> [ Bshouty, 1993 ] </ref> we have: Theorem 2 There exists an efficient PAC-L2R algorithm that uses RQ D (f j ) and M Q (f j ), for the reasoning problem RQ (F ) where (i) F is the class of Decision Trees over f0; 1; flg n . (ii) F
Reference: [ Etherington, 1988 ] <author> D. W. Etherington. </author> <title> Reasoning With Incomplete Information. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [ Geffner, 1990 ] <author> H. Geffner. </author> <title> Default Reasoning: Casual and Conditional Theories. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Attempts to represent and reason with defaults have encountered many problems (e.g., <ref> [ Neufeld, 1989; Poole, 1989; Geffner, 1990 ] </ref> ). In many cases, reasoning with acceptable defaults lead to unacceptable conclusions. Problems occur whenever defaults interact, and can be characterized frequently as problems of distinguishing good defaults from bad ones. <p> Problems occur whenever defaults interact, and can be characterized frequently as problems of distinguishing good defaults from bad ones. But, reasons for deciding between good and bad defaults vary, and in most cases depend on the situation. No general method exists according to which one can rank defaults <ref> [ Geffner, 1990 ] </ref> . The only way to figure out why and when certain defaults are preferred to others is to understand what the defaults say about the world. <p> No general method exists according to which one can rank defaults [ Geffner, 1990 ] . The only way to figure out why and when certain defaults are preferred to others is to understand what the defaults say about the world. While probabilistic and statistical approaches <ref> [ Geffner, 1990; Bacchus et al., 1993 ] </ref> present an important step in this direction, they still suffer from some of the same problems [ Geffner, 1994 ] , and are infeasible computationally. The approach developed here does not use defaults. Rather, it is a theory of inference.
Reference: [ Geffner, 1994 ] <author> H. Geffner. </author> <title> Causal default reasoning: </title> <booktitle> Principles and algorithms. In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 245-250, </pages> <year> 1994. </year>
Reference-contexts: While probabilistic and statistical approaches [ Geffner, 1990; Bacchus et al., 1993 ] present an important step in this direction, they still suffer from some of the same problems <ref> [ Geffner, 1994 ] </ref> , and are infeasible computationally. The approach developed here does not use defaults. Rather, it is a theory of inference. It reasons from a knowledge representation into which the incompleteness is compiled via a learning process.
Reference: [ Goldszmidt and Pearl, 1991 ] <author> M. Goldszmidt and J. Pearl. </author> <title> System Z+: A formalism for reasoning with variable strength defaults. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 399-404, </pages> <year> 1991. </year>
Reference: [ Hanks and McDermott, 1986 ] <author> S. Hanks and D. McDermott. </author> <title> Default reasoning, nonmonotonic logics, and the frame problem. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 328-333, </pages> <year> 1986. </year>
Reference-contexts: While the standard non-monotonic reasoning formalisms do not capture the desirable behavior, that things stay as they are <ref> [ Hanks and McDermott, 1986 ] </ref> , our representation of incomplete information does so [ Roth, 1995 ] .
Reference: [ Kearns, 1993 ] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: Most learning algorithms known can tolerate classification noise with error rate &lt; 1=2 <ref> [ Kearns, 1993 ] </ref> . world knowledge and some subset of the default assumptions, and at the same time support our intuition about a plausible conclusion. Attempts to represent and reason with defaults have encountered many problems (e.g., [ Neufeld, 1989; Poole, 1989; Geffner, 1990 ] ).
Reference: [ Khardon and Roth, 1994a ] <author> R. Khardon and D. Roth. </author> <title> Exploiting relevance through model-based reasoning. </title> <booktitle> In AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 109-114, </pages> <year> 1994. </year>
Reference-contexts: An Example Oracle with respect to the probability distribution D on f0; 1; flg n , denoted EX (D), is an oracle that when accessed, returns v 2 f0; 1; flg n , where v is drawn at random according to D. As discussed in <ref> [ Khardon and Roth, 1994a ] </ref> , in situations constrained to satisfy some context condition (e.g., Q = fx 1 = we are in Bostong or Q = fx 1 ^ x 2 ! x 3 g), the occurrences of observations is not governed by D, but by the distribution D
Reference: [ Khardon and Roth, 1994b ] <author> R. Khardon and D. Roth. </author> <title> Learning to reason. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 682-687, </pages> <year> 1994. </year> <note> Full version: </note> <institution> TR-02-94, Aiken Computation Lab., Harvard University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Most formalisms, in attempting to capture some aspects of default reasoning give up on others. Multiple levels of specificity of information, irrelevant information and conflicting defaults are among the aspects that the various formalisms have found difficult to reconcile. In <ref> [ Khardon and Roth, 1994b ] </ref> a new framework for the study of reasoning is introduced. The framework incorporates a role for inductive learning within efficient reasoning and exhibits the importance of studying the learning and reasoning phenomena together. <p> In particular, reasoning problems that are provably intractable in the traditional approach are given efficient Learning to Reason algorithms. Previous works in the Learning to Reason framework <ref> [ Khardon and Roth, 1994b; 1995b ] </ref> have considered reasoning tasks whose functionality is well defined. This paper, on the other hand, considers tasks in which, in many cases, there is no agreement on what constitutes a plausible outcome. The disagreement, we believe, is justified. <p> collection of oracles that represent a reasonable interaction of the agent with the environment and might depend on the arbitrary and unknown distribution D over f0; 1; flg n or some restriction of it, D Q . (We exclude RQ from I for notational convenience.) Other oracles considered include (see <ref> [ Khardon and Roth, 1994b; 1995b ] </ref> ) a Membership Query Oracle for the attribute function f j (which, on input v 2 f0; 1; flg n1 and j, returns f j (v)), an Equivalence Query Oracle for f j , (which, on input g : f0; 1; flg n1 ! <p> Consequently, different learning questions may arise, the reasoning algorithms might be more complicated, and one can also pose more general queries 5 . In particular, it can be shown that the algorithms used in <ref> [ Khardon and Roth, 1994b ] </ref> to learn model-based representation can be extended to work over f0; 1; flg n .
Reference: [ Khardon and Roth, 1995a ] <author> R. Khardon and D. Roth. </author> <title> Default-reasoning with models. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: In [ Khardon and Roth, 1995b ] a Learning to Reason approach that can deal with partial information is developed and shown to support efficient deduction. The interpretation taken there, however, is not expressive enough to support non-monotonic reasoning. In <ref> [ Khardon and Roth, 1995a ] </ref> a solution to some restricted cases of the traditional default reasoning problem is suggested, using learnable model-based representations. The approach presented in [ Schuurmans and Greiner, 1994 ] is closest to ours in that they study the problem of learning default rules.
Reference: [ Khardon and Roth, 1995b ] <author> R. Khardon and D. Roth. </author> <title> Learning to reason with a restricted view. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Later in the paper we discuss the relation of this work to the default reasoning literature. Now we briefly mention some works that are related to the approach presented here. In <ref> [ Khardon and Roth, 1995b ] </ref> a Learning to Reason approach that can deal with partial information is developed and shown to support efficient deduction. The interpretation taken there, however, is not expressive enough to support non-monotonic reasoning.
Reference: [ Kraus et al., 1990 ] <author> S. Kraus, D. Lehmann, and M. Magidor. </author> <title> Preferential models and cumulative logics. </title> <journal> Artificial Intelligence, </journal> <volume> 44 </volume> <pages> 167-207, </pages> <year> 1990. </year>
Reference-contexts: Clearly, the observations lead to f has beak (bird = 1), and evaluating it yields has beak = 1. We note that while the conclusion above is very intuitive, it is not supported by many treatments of default reasoning (e.g., <ref> [ Kraus et al., 1990 ] </ref> ), which encounter difficulties in trying to support both specificity and irrelevance.
Reference: [ Kushilevitz and Roth, 1995 ] <author> E. Kushilevitz and D. Roth. </author> <title> On learning visual concepts and DNF formulae. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1995. </year> <note> Earlier version appeared in Proceedings of the ACM Workshop on Computational Learning Theory '93. </note>
Reference-contexts: It can be shown that this procedure provides a mistake bound, and therefore a pac algorithm for Boolean conjunctions over f0; 1; flg n . Using the techniques introduced in <ref> [ Kushilevitz and Roth, 1995 ] </ref> we can show how to learn kDNF and kCNF formulae over f0; 1; flg n , for any fixed k. Moreover, these algorithm are shown to tolerate noise, and thus can be used to construct L2R algorithms.
Reference: [ Littlestone, 1989 ] <author> N. Littlestone. </author> <title> Mistake bounds and logarithmic linear-threshold learning algorithms. </title> <type> PhD thesis, </type> <address> U. C. Santa Cruz, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: The learning scenario most appropriate in our case is an on-line scenario (or, continuous learning) <ref> [ Littlestone, 1989; Valiant, 1994a ] </ref> . Every example received by the algorithm can be used to update many attribute functions in parallel. <p> As performance criteria we will use the criteria accepted in computational learning theory (which we do not define here), namely, either the pac criterion [ Valiant, 1984 ] or the mistake-bound criterion <ref> [ Littlestone, 1989 ] </ref> .
Reference: [ McCarthy and Hayes, 1969 ] <author> J. McCarthy and P. Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 4. </booktitle> <publisher> Edinburgh University Press, </publisher> <year> 1969. </year>
Reference: [ McCarthy, 1980 ] <author> J. McCarthy. </author> <title> Circumscription a form of non-monotonic reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13(1,2), </volume> <year> 1980. </year>
Reference-contexts: As an example, consider the case of preferred interpretations <ref> [ McCarthy, 1980; Selman and Kautz, 1990; Papadimitriou, 1991 ] </ref> . There, a theory F and a set D of defaults are given. The theory defines a set of possible models, and the default rules define a preference relation (a partial order) on those.
Reference: [ Minsky, 1975 ] <author> M. Minsky. </author> <title> A framework for representing knowledge. </title> <editor> In P. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1975. </year> <note> Also in R. </note> <editor> Brachman and H. Levesque, </editor> <booktitle> Readings in Knowledge Representation, </booktitle> <year> 1985. </year>
Reference-contexts: Earlier formalisms in this framework have abstracted the reasoning task as a deduction task, of determining whether a sentence, assumed to capture the situation at hand, is implied from the knowledge base, capturing our theory of the world. This abstraction has been criticized by many (e.g., <ref> [ Minsky, 1975 ] </ref> ) on the ground that it cannot support non-monotonic reasoning. It is widely acknowledged today that a large part of our everyday reasoning involves arriving at conclusions that are not logically entailed by our theory of the world.
Reference: [ Neufeld, 1989 ] <author> E. Neufeld. </author> <title> Default and probabilities; extensions and coherence. </title> <booktitle> In Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 312-323, </pages> <year> 1989. </year>
Reference-contexts: Attempts to represent and reason with defaults have encountered many problems (e.g., <ref> [ Neufeld, 1989; Poole, 1989; Geffner, 1990 ] </ref> ). In many cases, reasoning with acceptable defaults lead to unacceptable conclusions. Problems occur whenever defaults interact, and can be characterized frequently as problems of distinguishing good defaults from bad ones.
Reference: [ Papadimitriou, 1991 ] <author> C. H. Papadimitriou. </author> <title> On selecting a satisfying truth assignment. </title> <booktitle> In Proc. 32nd Ann. IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 163-169, </pages> <year> 1991. </year>
Reference-contexts: This is true not only for the task of deduction, but also for many other forms of reasoning which have been developed [ Selman, 1990; Roth, 1993 ] . Of particular interest in this context are the hardness results on default reasoning tasks <ref> [ Selman, 1990; Papadimitriou, 1991 ] </ref> , where the increase in complexity (relative to corresponding deduction tasks) is clearly at odds with the intuition that reasoning with defaults should somehow re duce the complexity of reasoning. <p> As an example, consider the case of preferred interpretations <ref> [ McCarthy, 1980; Selman and Kautz, 1990; Papadimitriou, 1991 ] </ref> . There, a theory F and a set D of defaults are given. The theory defines a set of possible models, and the default rules define a preference relation (a partial order) on those.
Reference: [ Pearl, 1988 ] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference: [ Poole, 1989 ] <author> D. Poole. </author> <title> What the lottery paradox tells us about default reasoning. </title> <booktitle> In Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 333-340, </pages> <year> 1989. </year>
Reference-contexts: Attempts to represent and reason with defaults have encountered many problems (e.g., <ref> [ Neufeld, 1989; Poole, 1989; Geffner, 1990 ] </ref> ). In many cases, reasoning with acceptable defaults lead to unacceptable conclusions. Problems occur whenever defaults interact, and can be characterized frequently as problems of distinguishing good defaults from bad ones.
Reference: [ Reiter and G., 1981 ] <author> R. Reiter and Criscuolo G. </author> <title> On interacting defaults. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 270-276, </pages> <year> 1981. </year>
Reference: [ Reiter, 1980 ] <author> R. Reiter. </author> <title> A logic for default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13(1,2), </volume> <year> 1980. </year>
Reference: [ Reiter, 1987 ] <author> R. Reiter. </author> <title> Nonmonotonic reasoning. </title> <booktitle> In Annual Reviews of Computer Science, </booktitle> <pages> pages 147-188. </pages> <year> 1987. </year>
Reference: [ Roth, 1993 ] <author> D. Roth. </author> <title> On the hardness of approximate reasoning. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 613-618, </pages> <month> August </month> <year> 1993. </year> <note> To Appear in Artificial Intelligence Journal, </note> <year> 1995. </year>
Reference-contexts: Computational considerations, however, render all the formalisms suggested within the knowledge-based systems approach apparently inadequate for commonsense reasoning. This is true not only for the task of deduction, but also for many other forms of reasoning which have been developed <ref> [ Selman, 1990; Roth, 1993 ] </ref> .
Reference: [ Roth, 1995 ] <author> D. Roth. </author> <title> Learning to reason: the non-monotonic case. </title> <note> 1995. Full Version. In Preparation. </note>
Reference-contexts: While the standard non-monotonic reasoning formalisms do not capture the desirable behavior, that things stay as they are [ Hanks and McDermott, 1986 ] , our representation of incomplete information does so <ref> [ Roth, 1995 ] </ref> . <p> Moreover, these algorithm are shown to tolerate noise, and thus can be used to construct L2R algorithms. To summarize (see <ref> [ Roth, 1995 ] </ref> ): Theorem 1 Let F be the class of conjunctions, disjunctions, kCNF and kDNF formulae over f0; 1; flg n . <p> We have discussed a knowledge representation that consists of a collection of attribute functions. Using our interpretation of incomplete information it can be shown <ref> [ Roth, 1995 ] </ref> that other representations can support the reasoning behavior demonstrated in this paper. Consequently, different learning questions may arise, the reasoning algorithms might be more complicated, and one can also pose more general queries 5 .
Reference: [ Schuurmans and Greiner, 1994 ] <author> D. Schuurmans and R. Greiner. </author> <title> Learning default concepts. </title> <booktitle> In Proceedings of the Tenth Cana-dian Conference on Artificial Intelligence (CSCSI-94), </booktitle> <year> 1994. </year>
Reference-contexts: The interpretation taken there, however, is not expressive enough to support non-monotonic reasoning. In [ Khardon and Roth, 1995a ] a solution to some restricted cases of the traditional default reasoning problem is suggested, using learnable model-based representations. The approach presented in <ref> [ Schuurmans and Greiner, 1994 ] </ref> is closest to ours in that they study the problem of learning default rules. The reasoning stage, however, is not considered, and presumably is performed by a traditional reasoner, and is thus intractable.
Reference: [ Selman and Kautz, 1990 ] <author> B. Selman and H. Kautz. </author> <title> Model-preference default theories. </title> <journal> Artificial Intelligence, </journal> <volume> 45 </volume> <pages> 287-322, </pages> <year> 1990. </year>
Reference-contexts: As an example, consider the case of preferred interpretations <ref> [ McCarthy, 1980; Selman and Kautz, 1990; Papadimitriou, 1991 ] </ref> . There, a theory F and a set D of defaults are given. The theory defines a set of possible models, and the default rules define a preference relation (a partial order) on those.
Reference: [ Selman, 1990 ] <author> B. Selman. </author> <title> Tractable Default Reasoning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1990. </year>
Reference-contexts: Computational considerations, however, render all the formalisms suggested within the knowledge-based systems approach apparently inadequate for commonsense reasoning. This is true not only for the task of deduction, but also for many other forms of reasoning which have been developed <ref> [ Selman, 1990; Roth, 1993 ] </ref> . <p> This is true not only for the task of deduction, but also for many other forms of reasoning which have been developed [ Selman, 1990; Roth, 1993 ] . Of particular interest in this context are the hardness results on default reasoning tasks <ref> [ Selman, 1990; Papadimitriou, 1991 ] </ref> , where the increase in complexity (relative to corresponding deduction tasks) is clearly at odds with the intuition that reasoning with defaults should somehow re duce the complexity of reasoning.
Reference: [ Touretzky et al., 1987 ] <author> D. Touretzky, J. Horty, and R. Thomason. </author> <title> A clash of intuitions: The current state of nonmonotonic multiple inheritance systems. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1987. </year>
Reference: [ Touretzky, 1986 ] <author> D. Touretzky. </author> <title> The Mathematics of Inheritance Systems. </title> <publisher> Morgan Kaufman, </publisher> <year> 1986. </year>
Reference: [ Valiant, 1984 ] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In this paper we extend the Learning to Reason framework to deal explicitly with reasoning in the presence of incomplete information. Inspired by the pac-learning approach <ref> [ Valiant, 1984 ] </ref> we present the view that the world is very complicated and there is no hope of acquiring an exact representation of it; our aim should be to acquire enough information with which to cope effectively in the world. <p> Notice that when using attribute-function representations there is no need to make assumptions about the world and, in particular, to assume it is consistent. We use oracles to model the type of interaction the agent has with the world, in the spirit of the formal study of learning <ref> [ Valiant, 1984 ] </ref> and the Learning to Reason framework. The oracles differ according to the amount and type of information they supply the agent about the world. <p> As performance criteria we will use the criteria accepted in computational learning theory (which we do not define here), namely, either the pac criterion <ref> [ Valiant, 1984 ] </ref> or the mistake-bound criterion [ Littlestone, 1989 ] . <p> Since in all the examples considered in Section 3 we used the oracle EX (D) only we start by considering learning from examples only. We extend the standard elimination algorithm for learning conjunctions <ref> [ Valiant, 1984 ] </ref> to work over f0; 1; flg n . In this case the values assigned to the variables are non-empty subsets of f0; 1; flg rather than of f0; 1g, as is usually the case.
Reference: [ Valiant, 1994a ] <author> L. G. Valiant. </author> <title> Circuits of the Mind. </title> <publisher> Oxford University Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: The learning scenario most appropriate in our case is an on-line scenario (or, continuous learning) <ref> [ Littlestone, 1989; Valiant, 1994a ] </ref> . Every example received by the algorithm can be used to update many attribute functions in parallel. <p> We do not know of any traditional formalism that can handle in a satisfying way (efficiently, or even qualitatively) all the aspects presented by those examples. We note, though, that our first example is a variant of an example considered in <ref> [ Valiant, 1994a ] </ref> , and that all the examples we consider here could be considered also in the Rationality framework and be implemented, in principle, on the Neuroidal Model [ Valiant, 1994b; 1994a ] .
Reference: [ Valiant, 1994b ] <author> L. G. Valiant. </author> <title> Rationality. </title> <type> Technical Report TR-32-94, </type> <institution> Aiken Computation Lab., Harvard University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: In this paper we formalize this intuition and use it to develop a theory that supports efficient reasoning with incomplete information. Our treatment of incomplete information follows a suggestion made in <ref> [ Valiant, 1994b ] </ref> . <p> Q = fx 1 ^ x 2 ! x 3 g), the occurrences of observations is not governed by D, but by the distribution D Q which is the distribution we see by filtering out all those observations that do not satisfy Q. (We follow here the formulation suggested in <ref> [ Valiant, 1994b ] </ref> ). We denote this oracle by EX (D Q ). <p> We note, though, that our first example is a variant of an example considered in [ Valiant, 1994a ] , and that all the examples we consider here could be considered also in the Rationality framework and be implemented, in principle, on the Neuroidal Model <ref> [ Valiant, 1994b; 1994a ] </ref> . A (partial) list of papers that have discussed (a subset of) these examples includes [ Bacchus et al., 1993; Ethering-ton, 1988; Geffner, 1990; Reiter, 1980; Reiter and G., 1981; Selman, 1990; Touretzky et al., 1987 ] .
References-found: 39

