URL: http://www.adi.uam.es/~ajustel/Outgs.ps
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: BAYESIAN UNMASKING IN LINEAR MODELS  
Author: Ana Justel and Daniel Pe~na 
Keyword: Key words: Gibbs sampler. Linear regression. Multiple outliers. Sequential learning.  
Address: Madrid  
Affiliation: Department of Mathematics, Universidad Autonoma de Madrid Department of Statistics and Econometrics, Universidad Carlos III de  
Abstract: We propose a Bayesian procedure for multiple outlier detection in linear models avoiding the masking problem. The posterior probabilities of each data point being an outlier are estimated by using a new adaptive Gibbs sampling method, which modifies the initial conditions of the Gibbs sampler by using the eigen-structure of the covariance matrix of the indicator variables. This procedure also overcomes the false convergence of the Gibbs sampling in problems with strong masking. Our proposal is illustrated with several examples in which our procedure seems to work very well. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkinson, A.C. </author> <year> (1994). </year> <title> Fast very robust methods for the detection of multiple outliers. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89, </volume> <pages> 1329-1339. </pages>
Reference: <author> Casella, G. & George, E.I. </author> <year> (1992). </year> <title> Explaining the Gibbs sampler. </title> <journal> American Statistician, </journal> <volume> 46, </volume> <pages> 167-174. </pages>
Reference-contexts: Also we assume, for simplicity, that k is known. The application of the Gibbs sampling <ref> (see Casella and George, 1992) </ref> is carried out by augmenting the parameter vector with a set of classification variables ffi = (ffi 1 ; : : : ; ffi n ) 0 , defined as ffi i = 1 when y i is generated by the alternative distribution N (x 0
Reference: <author> Cook, R.D. & Weisberg, S. </author> <year> (1982). </year> <title> Residuals and Influence in Regression. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference: <author> Gelman, A. & Rubin, D.B. </author> <year> (1992). </year> <title> Inference from iterative simulation using multiple sequences (with discussion). </title> <journal> Statistical Science, </journal> <volume> 7, </volume> <pages> 457-511. </pages>
Reference: <author> Hadi, A.S. & Simonoff, J.S. </author> <year> (1993). </year> <title> Procedures for the identification of multiple outliers in linear models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88, </volume> <pages> 1264-1272. </pages>
Reference: <author> Hawkins, D.M., Bradu, D. & Kass, G.V. </author> <year> (1984). </year> <title> Location of several outliers in multiple regression data using elemental sets. </title> <journal> Technometrics, </journal> <volume> 26, </volume> <pages> 197-208. </pages> <note> 19 Jeffreys, </note> <author> H. </author> <year> (1961). </year> <title> Theory of Probability, third edition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: Therefore a clear objective is to find a set S 0 that is outlier free. This idea is similar to the one used in robust estimation procedures based on resampling <ref> (Rousseeuw, 1984, and Hawkins, Bradu and Kass, 1984) </ref>. 2.2 Finding an outlier free subset Two outliers are masked when they need to be identified as such jointly. Suppose that the sample includes two or more masked outliers. <p> In any case, we need to take at least an elemental set <ref> (Hawkins, Bradu and Kass, 1984) </ref>, that is a set of size p. To compute n 0 we must consider that ff is the prior probability of each observation being an outlier, then n (1 ff) observations in the sample are expected to be good and nff to be outliers.
Reference: <author> Jorgensen, B. </author> <year> (1992). </year> <title> Finding rank leverage subsets in regression. </title> <journal> Scandinavian Journal of Statistics, </journal> <volume> 19, </volume> <pages> 139-156. </pages>
Reference: <author> Justel, A. & Pe ~ na, D. </author> <year> (1996). </year> <title> Gibbs Sampling will fail in outlier problems with strong masking. </title> <journal> Journal of Computational and Graphical Statistics, </journal> <volume> 5, </volume> <pages> 176-189. </pages>
Reference: <author> Pe ~ na, D. & Guttman, I. </author> <year> (1993). </year> <title> Comparing probabilistic methods for outlier detection in linear models. </title> <journal> Biometrika, </journal> <volume> 80, 603- 610. </volume>
Reference: <author> Pe ~ na, D. & Yohai, V.J. </author> <year> (1995). </year> <title> The detection of influential subsets in linear regression by using an influence matrix. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 57, </volume> <pages> 145-156. </pages>
Reference: <author> Pettit, L.I. & Smith, A.F.M. </author> <year> (1985). </year> <title> Outliers and influential observations in linear models. In Bayesian Statistics 2, </title> <editor> Ed. J.M. Bernardo, M.H. DeGroot, </editor> <publisher> D.V. </publisher>
Reference: <author> Lindley and A.F.M. </author> <booktitle> Smith, </booktitle> <pages> pp. 473-494. </pages> <address> Amsterdam: </address> <publisher> Elsevier. </publisher>
Reference: <author> Robert, C.P. </author> <year> (1995). </year> <title> Convergence control methods for Markov Chain Monte Carlo algorithms. </title> <journal> Statistical Science, </journal> <volume> 10, </volume> <pages> 231-253. </pages>
Reference: <author> Rousseeuw, P.J. </author> <year> (1984). </year> <title> Least median of squares regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 79, </volume> <pages> 871-880. </pages>
Reference-contexts: Therefore a clear objective is to find a set S 0 that is outlier free. This idea is similar to the one used in robust estimation procedures based on resampling <ref> (Rousseeuw, 1984, and Hawkins, Bradu and Kass, 1984) </ref>. 2.2 Finding an outlier free subset Two outliers are masked when they need to be identified as such jointly. Suppose that the sample includes two or more masked outliers.
Reference: <author> Rousseeuw, P.J. & van Zomeren, </author> <title> B.C. (1990) Unmaskig multivariate outliers and leverage points. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 85, </volume> <pages> 633-639. </pages>
Reference: <author> Verdinelli, I. & Wasserman, L. </author> <year> (1991). </year> <title> Bayesian analysis of outlier problems using the Gibbs sampler. </title> <journal> Statistics and Computing, </journal> <volume> 1, </volume> <pages> 105-117. 20 </pages>
Reference-contexts: Rousseeuw and Zomeren (1990) and Atkinson (1994) have proposed the use of robust estimation to identify multiple outliers. The masking problem has received very few attention in the Bayesian literature. In principle, Gibbs sampling <ref> (see Verdinelli and Wasserman, 1991) </ref> could be used to carry out the heavy computations involved in obtaining the 2 n posterior probabilities which correspond to all the possible configurations for the generation of the data.
References-found: 16

