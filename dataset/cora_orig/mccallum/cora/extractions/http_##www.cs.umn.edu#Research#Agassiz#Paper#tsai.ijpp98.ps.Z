URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.ijpp98.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: j-tsai1@uiuc.edu fzjiang,yewg@cs.umn.edu  
Title: Compiler Techniques for the Superthreaded Architectures  
Author: Jenn-Yuan Tsai Zhenzhen Jiang and Pen-Chung Yew 
Address: Urbana, IL 61801 Minneapolis, MN 55455  
Affiliation: Department of Computer Science Department of Computer Science University of Illinois University of Minnesota  
Abstract: Several useful compiler and program transformation techniques for the superthreaded architectures [8] are presented in this paper. The superthreaded architecture adopts a thread pipelining execution model to facilitate runtime data dependence checking between threads, and to maximize thread overlap to enhance concurrency. In this paper, we present some important program transformation techniques to facilitate concurrent execution among threads, and to manage critical system resources such as the memory buffers effectively. We evaluate the effectiveness of those program transformation techniques by applying them manually on several benchmark programs, and using a trace-driven, cycle-by-cycle superthreaded processor simulator. The simulation results show that a superthreaded processor can achieve promising speedup for most of the benchmark programs. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> Statement reordering for doacross loops. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <volume> volume Vol. II, </volume> <pages> pages 24-28, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In the thread pipelining, the compiler will try to increase the execution overlap of concurrent threads by minimizing the stall caused by data dependences between threads. To do this, we can perform statement reordering <ref> [1] </ref> and schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.4 Advanced Compiler Techniques for Superthreading Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar [4,
Reference: 2. <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors. <p> MIP 9610379, CDA 9502979; by the U.S. Army Intelligence Center and Fort Huachuca under Contract DABT63-95-C-0127 and ARPA order No. D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation <ref> [5, 7, 2, 8] </ref> and run-time data dependence checking between threads [7, 2, 8]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation [5, 7, 2, 8] and run-time data dependence checking between threads <ref> [7, 2, 8] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing parallelizing compiler techniques, which are originally developed for multiprocessors, to extract loop-level parallelism to generate multiple-threaded code. <p> stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.4 Advanced Compiler Techniques for Superthreading Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar [4, 7] and SPSM <ref> [2] </ref>, provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 3. <author> Marco Fillo, Stephen W. Keckler, William J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors.
Reference: 4. <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: [1] and schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.4 Advanced Compiler Techniques for Superthreading Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [4, 7] </ref> and SPSM [2], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 5. <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors. <p> MIP 9610379, CDA 9502979; by the U.S. Army Intelligence Center and Fort Huachuca under Contract DABT63-95-C-0127 and ARPA order No. D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation <ref> [5, 7, 2, 8] </ref> and run-time data dependence checking between threads [7, 2, 8]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs.
Reference: 6. <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, </institution> <address> California 94305, </address> <month> November </month> <year> 1991. </year> <note> Technical Report CSL-TR-91-497. </note>
Reference-contexts: In the transformed programs, special superthreading instructions, such as fork and store ts, are represented as function calls to specific subroutines, so that they could be recognized by the simulator. The transformed programs are compiled by the SGI C compiler. The programs are then instrumented by pixie <ref> [6] </ref> to generate instruction and memory reference traces. The simulator executes the instrumented program on the host SGI machine and collects the traces generated by the program. During the trace collection phase, the function calls which represent the superthreading instructions will be converted to the actual superthreading instructions for simulation.
Reference: 7. <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors. <p> MIP 9610379, CDA 9502979; by the U.S. Army Intelligence Center and Fort Huachuca under Contract DABT63-95-C-0127 and ARPA order No. D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation <ref> [5, 7, 2, 8] </ref> and run-time data dependence checking between threads [7, 2, 8]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation [5, 7, 2, 8] and run-time data dependence checking between threads <ref> [7, 2, 8] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing parallelizing compiler techniques, which are originally developed for multiprocessors, to extract loop-level parallelism to generate multiple-threaded code. <p> [1] and schedule target stores as early as possible, or schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.4 Advanced Compiler Techniques for Superthreading Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [4, 7] </ref> and SPSM [2], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 8. <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <month> October 20-23, </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors. <p> MIP 9610379, CDA 9502979; by the U.S. Army Intelligence Center and Fort Huachuca under Contract DABT63-95-C-0127 and ARPA order No. D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation <ref> [5, 7, 2, 8] </ref> and run-time data dependence checking between threads [7, 2, 8]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> D 346; and by a gift from Intel Corporation. cannot be analyzed at compile time. Most of the proposed multithreaded ar-chitectural models provide some hardware support for thread-level speculation [5, 7, 2, 8] and run-time data dependence checking between threads <ref> [7, 2, 8] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing parallelizing compiler techniques, which are originally developed for multiprocessors, to extract loop-level parallelism to generate multiple-threaded code. <p> New compiler techniques are thus needed to take advantages of such hardware support. Those compiler techniques are more aggressive than those in conventional parallelizing compilers. In this paper, we study a concurrent multithreaded architecture, called the Superthreaded architecture <ref> [8] </ref>. It uses a thread pipelining execution model to enhance overlapping between threads. It also provides hardware and software mechanism to facilitate run-time data dependence checking and its enforcement between threads, and to support thread-level control speculation. <p> Therefore, the compiler must generate threads at the appropriate level where the maximum combined performance gains can be achieved from both superthread-ing and superscalar. In addition, the size of a thread cannot be too large in order to avoid overflowing the memory buffer <ref> [8] </ref>. 7 To generate threads at the right level, the compiler examines data depen-dences between contiguous execution portions of a program and estimates the amount of thread-level parallelism at each level with respect to the data dependences.
Reference: 9. <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithread-ing: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year> <title> This article was processed using the L A T E X macro package with LLNCS style 17 </title>
Reference-contexts: 1 Introduction Recently, a number of concurrent multithreaded execution models <ref> [5, 7, 9, 3, 2, 8] </ref> are proposed as an alternative to the current single-threaded superscalar execution model for future generations of microprocessors.
References-found: 9

