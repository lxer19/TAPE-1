URL: http://www.cs.jhu.edu/~sheppard/mem.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Email: sheppard@arinc.com  salzberg@cs.jhu.edu  
Title: Memory Based Learning of Pursuit Games  
Author: John W. Sheppard Steven L. Salzberg 
Address: 2551 Riva Road Annapolis, MD 21401  Baltimore, Maryland 21218  
Affiliation: ARINC, Incorporated  Department of Computer Science The Johns Hopkins University  
Abstract-found: 0
Intro-found: 1
Reference: [AK89] <author> D. Aha and D. Kibler. </author> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <pages> pages 794-799, </pages> <address> Dtroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This demonstrates that the two pursuer problem is significantly more difficult for memory-based reasoning. One possible reason for the poor performance of k-NN on the two pursuer task is presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [AK89, Sal91] </ref>. <p> Another possible reason for MBR's problems is the presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [AK89, Sal91] </ref>. But the most likely reason for MBR's troubles, we concluded, was that we were generating bad examples in the early phases of the game. As stated above, MBR needs to have the "correct" action, or something close to it, stored with every state in memory.
Reference: [AS93] <author> D. Aha and S. Salzberg. </author> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on AI and Statistics, </booktitle> <year> 1993. </year> <month> 26 </month>
Reference-contexts: Atkeson [Atk89] employed a memory-based technique to train a robot arm to follow a prespecified trajectory. More recently, Moore and Atkeson [MA93] developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating. In another study, Aha and Salzberg <ref> [AS93] </ref> used nearest-neighbor techniques to train a simulated robot to catch a ball. In their study, they provided an agent that knew the correct behavior for the robot, and therefore provided corrected actions when the robot made a mistake.
Reference: [Atk89] <author> C. Atkeson. </author> <title> Using local models to control movement. </title> <booktitle> In Neural Information Systems Conference, </booktitle> <year> 1989. </year>
Reference-contexts: The idea of using memory-based methods for delayed reinforcement tasks has only very recently been considered by a small number of researchers. Atkeson <ref> [Atk89] </ref> employed a memory-based technique to train a robot arm to follow a prespecified trajectory. More recently, Moore and Atkeson [MA93] developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating.
Reference: [BO82] <author> T. Basar and G. J. Olsder. </author> <title> Dynamic Noncooperative Game Theory. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1982. </year>
Reference-contexts: Generally, there are additional assumptions that both the car and the pedestrian are traveling at a fixed speed, the car has a fixed minimum radius of curvature, and the pedestrian is able to make arbitrarily sharp turns <ref> [BO82] </ref>. In the following, we present a summary of an analysis of this game by Isaacs [Isa63] and Basar and Olsder [BO82]. <p> car and the pedestrian are traveling at a fixed speed, the car has a fixed minimum radius of curvature, and the pedestrian is able to make arbitrarily sharp turns <ref> [BO82] </ref>. In the following, we present a summary of an analysis of this game by Isaacs [Isa63] and Basar and Olsder [BO82]. <p> What this means is if the ratio of the lethal range to the radius of curvature exceeds the maneuverability of the pedestrian at the designated speeds, then the pedestrian will be hit no matter what. From this analysis, Basar and Olsder <ref> [BO82] </ref> determine optimal strategies for the players to ensure the state trajectories follow the BUP.
Reference: [BSA83] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: Temporal difference learning methods apply delayed reinforcement to a sequence of actions to predict future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE <ref> [BSA83, BSW90] </ref> and Q-learning [Wat89]. The original work by Barto et al. [BSA83] demonstrated that the cart and pole problem could be solved using this method. <p> Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE [BSA83, BSW90] and Q-learning [Wat89]. The original work by Barto et al. <ref> [BSA83] </ref> demonstrated that the cart and pole problem could be solved using this method. Since then, Clouse and Utgoff [CU92] applied ACE/ASE with a separate teacher to the cart and pole problem and applied Q-learning to navigating a race track. <p> Very little other research has addressed teaching strategies for reinforcement learning problems. Barto's ACE/ASE <ref> [BSA83] </ref> differs from the bootstrapping approach in that no feedback is provided to the GA to modify its learning algorithm. Further, ACE/ASE are both connectionist architectures whose weights are modified based on reinforcement received from experience. In our model, only the GA learns from reinforcement.
Reference: [BSW90] <author> A. Barto, R. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In Gabriel and Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <address> Cambridge, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Temporal difference learning methods apply delayed reinforcement to a sequence of actions to predict future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE <ref> [BSA83, BSW90] </ref> and Q-learning [Wat89]. The original work by Barto et al. [BSA83] demonstrated that the cart and pole problem could be solved using this method.
Reference: [Cha87] <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 333-377, </pages> <year> 1987. </year>
Reference-contexts: Then we made the task substantially harder to study the limitations of memory-based reasoning (MBR) methods on this class of problems. The more complicated task, which is described further in section 3.3, also resembles complicated planning tasks in which an agent has to satisfy several goals at once <ref> [Cha87] </ref>. As our experiments will show, we were successful at developing a method to solve our difficult reinforcement learning task. The key idea behind our success was the combined use of both a MBR and a GA .
Reference: [CU92] <author> J. Clouse and P. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Two popular temporal difference algorithms are ACE/ASE [BSA83, BSW90] and Q-learning [Wat89]. The original work by Barto et al. [BSA83] demonstrated that the cart and pole problem could be solved using this method. Since then, Clouse and Utgoff <ref> [CU92] </ref> applied ACE/ASE with a separate teacher to the cart and pole problem and applied Q-learning to navigating a race track. Lin [Lin91] applied Q-learning to teach a robot to navigate the halls of a classroom building and plug itself into a wall socket to recharge its batteries. <p> Further, ACE/ASE are both connectionist architectures whose weights are modified based on reinforcement received from experience. In our model, only the GA learns from reinforcement. Another related teaching method is that of Clouse and Utgoff <ref> [CU92] </ref>, who used ACE/ASE with a separate teacher. Their teacher monitored the overall progress of the learning agent, and "reset" the eligibility traces of the two learning elements when the performance failed to improve. <p> Such a helpful teacher is similar to the oracle used by Clouse and Utgoff <ref> [CU92] </ref> except that it provides the theoretical minimum number of examples from which to learn the task. A reasonable question to ask is why one would use the GA to develop a set of rules to perform a task and then furnish examples to k-NN.
Reference: [Fri71] <author> A. Friedman. </author> <title> Differential Games. </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: We can also interpret differential games to be an extension of optimal control theory in which players' positions develop continuously in time, and where the goal is to optimize competing control laws for the players <ref> [Fri71] </ref>. 3.1 Differential games Differential game theory originated in the early 1960s [Isa63] in response to the need for a more formal analysis of war games.
Reference: [Gol89] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: The plan with the highest fitness is used to control E. The heart of the learning algorithm lies in the application of two genetic operators: mutation and crossover. Rules within a plan are selected for mutation using fitness proportional selection <ref> [Gol89] </ref>. After selection, each clause on the left hand side of a rule is mutated according to a fixed mutation probability.
Reference: [Gre88] <author> J. Grefenstette. </author> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 225-245, </pages> <year> 1988. </year>
Reference-contexts: Following a game the strengths of the rules that fired are updated based on the payoff received from the game (the same payoff used in Q-learning). Given the payoff function, rule strengths are updated using the profit sharing plan described by Grefenstette <ref> [Gre88] </ref> as follows: (t) = (1 c)(t 1) + c payoff (0) = payoff (t) = (1 c)(t 1) + c ((t) payoff) (0) = 0 strength (t) = (t) (t) 17 where c is the profit sharing rate (c = 0:01 for our experiments), is an estimate of the mean
Reference: [Gre91] <editor> J. Grefenstette. </editor> <booktitle> Lamarckian learning in multi-agent environments. In Proceedings of the Fourth International Conference of Genetic Algorithms, </booktitle> <pages> pages 303-310. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Unfortunately, little has been done to apply these algorithms to differential games. The most notable exception to this is the work by Grefenstette on genetic algorithms. In addition to studying the evasive maneuvers task, Grefenstette <ref> [Gre91] </ref> applied his genetic algorithm system SAMUEL to aerial dogfighting and target tracking (in a 2 game of cat and mouse). The idea of using memory-based methods for delayed reinforcement tasks has only very recently been considered by a small number of researchers. <p> The knowledge for the evasive maneuvers problem requires rules in which the terms have numeric values; therefore, the standard GA representation and operators have been modified for this problem. In particular, we use the formulation described in <ref> [Gre91] </ref>. We call a set of rules a plan.
Reference: [GRS90] <author> J. Grefenstette, C. Ramsey, and A. Schultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: For this study, we began by considering a differential game that involved one agent trying to pursue and capture another (i.e., a pursuit game). Earlier research showed that at least one implementation of this task, known as evasive maneuvers <ref> [GRS90] </ref>, can be solved by a genetic algorithm (GA). We first developed a memory-based approach using k-nearest neighbor (k-NN) for the same task. Then we made the task substantially harder to study the limitations of memory-based reasoning (MBR) methods on this class of problems. <p> Thus we should reasonably expect the surface for extensions to the problem (such as those discussed in this paper) to be more difficult to characterize. Grefenstette et al. <ref> [GRS90] </ref> examined the evasive maneuvers task to demonstrate the ability of genetic algorithms to solve complex sequential decision making tasks. In their 2-D simulation, a single aircraft attempts to evade a single missile.
Reference: [II93] <author> F. Imado and T. Ishihara. </author> <title> Pursuit-evasion geometry analysis between two missiles and an aircraft. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26(3) </volume> <pages> 125-139, </pages> <year> 1993. </year>
Reference-contexts: For our extension, we added a second pursuer. Unlike the single-pursuer problems, the two-pursuer problem (Figure 2)) has no known optimal strategy <ref> [II93] </ref>. Second, we gave the evader additional capabilities: in the one-pursuer game, E only controls its turn angle at each time step. Thus E basically zigzags back and forth or makes a series of hard turns into the path of P to escape.
Reference: [Isa63] <author> R. Isaacs. </author> <title> Differential games: A mathematical theory with applications to warfare and other topics. </title> <type> Technical Report Research Contribution No. 1, </type> <institution> Center for Naval Analysis, </institution> <address> Washington, DC, </address> <year> 1963. </year>
Reference-contexts: The class of RL problems studied here has also been studied in the field of differential game theory. Differential game theory is an extension of traditional game theory in which a game follows a sequence of actions through a continuous state space to achieve some payoff <ref> [Isa63] </ref>. This sequence can be modeled with a set of differential equations which are analyzed to determine optimal play by the players. <p> We can also interpret differential games to be an extension of optimal control theory in which players' positions develop continuously in time, and where the goal is to optimize competing control laws for the players [Fri71]. 3.1 Differential games Differential game theory originated in the early 1960s <ref> [Isa63] </ref> in response to the need for a more formal analysis of war games. <p> In the following, we present a summary of an analysis of this game by Isaacs <ref> [Isa63] </ref> and Basar and Olsder [BO82].
Reference: [Lin91] <author> L. Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <year> 1991. </year>
Reference-contexts: The original work by Barto et al. [BSA83] demonstrated that the cart and pole problem could be solved using this method. Since then, Clouse and Utgoff [CU92] applied ACE/ASE with a separate teacher to the cart and pole problem and applied Q-learning to navigating a race track. Lin <ref> [Lin91] </ref> applied Q-learning to teach a robot to navigate the halls of a classroom building and plug itself into a wall socket to recharge its batteries.
Reference: [Lit94] <author> M. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Finally, Tesauro used temporal difference learning [Tes92] and neural networks [TS89] to "self-teach" connectionist systems to play backgammon. In many ways, backgammon extends beyond "strategic games" due to the stochastic nature of play, and his 3 TD-based system is currently playing at a master level. Very recently, Michael Littman <ref> [Lit94] </ref> observed that reinforcement learning canbe applied to multi-agent game learning in the context of Markov games. Littman expanded Watkins' Q-learning algorithm to cover two players in a simplified game of soccer.
Reference: [MA93] <author> A. Moore and C. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: The idea of using memory-based methods for delayed reinforcement tasks has only very recently been considered by a small number of researchers. Atkeson [Atk89] employed a memory-based technique to train a robot arm to follow a prespecified trajectory. More recently, Moore and Atkeson <ref> [MA93] </ref> developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating. In another study, Aha and Salzberg [AS93] used nearest-neighbor techniques to train a simulated robot to catch a ball.
Reference: [Min89] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: We call the resulting system GANNE (GA plus nearest neighbor plus editing). Early work by Wilson [Wil72] showed that examples could be removed from a set used for classification, and that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees <ref> [Min89] </ref>). Wilson's algorithm was to classify each example in a data set with its own k nearest neighbors. Those points that are incorrectly classified are deleted from the example set, the idea being that such points probably represent noise.
Reference: [MT92] <author> J. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 363-395, </pages> <year> 1992. </year>
Reference-contexts: A similar algorithm was applied to learning strategies for backing a truck into a loading dock [NW89]. Both of these methods incorporated knowledge of the correct behavior during training. In addition, Millan and Torras <ref> [MT92] </ref> demonstrated a technique for training a neural network using reinforcement learning in which the control variables are permitted to vary continuously. They addressed the problem of teaching a robot to navigate around obstacles. Considerable research has been performed using a form of reinforcement learning called temporal difference learning [Sut88].
Reference: [NW89] <author> D. Nguyen and B. Widrow. </author> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 357-363, </pages> <year> 1989. </year>
Reference-contexts: For example, back-propagation has been used to solve the cart and pole problem [Wid87], in which a pole must be balanced vertically on a wheeled cart. A similar algorithm was applied to learning strategies for backing a truck into a loading dock <ref> [NW89] </ref>. Both of these methods incorporated knowledge of the correct behavior during training. In addition, Millan and Torras [MT92] demonstrated a technique for training a neural network using reinforcement learning in which the control variables are permitted to vary continuously.
Reference: [Pel93] <author> B. D. Pell. </author> <title> Strategy Generation and Evaluation for Meta-Game Playing. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> Cambridge, England, </address> <year> 1993. </year>
Reference-contexts: Some recent work in learning strategies for game playing have begun to deal with issues of co-learning at a superficial level with strategic games such as chess and othello. Pell developed an environment for deriving strategies in what he calls "symmetric chess-like games" <ref> [Pel93] </ref>. His METAGAMER focused on translating the rules and constraints of a game into a strategy using a declarative formulation of the game's characteristics.
Reference: [RWLI75] <author> G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour. </author> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21(6) </volume> <pages> 665-669, </pages> <year> 1975. </year>
Reference-contexts: Tomek [Tom76] modified this approach by taking a sample (&gt; 1) of the data and classifying the sample with the remaining examples. Editing then proceeds as in Wilson editing. Ritter et al. <ref> [RWLI75] </ref> developed another editing method, which differs from Wilson in that points that are correctly classified are discarded. The Ritter method basically keeps only points near the boundaries between classes, and eliminates examples that are in the midst of a homogenous region.
Reference: [Sal91] <author> S. Salzberg. </author> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, ISMIS '91, </booktitle> <pages> pages 399-408, </pages> <year> 1991. </year>
Reference-contexts: This demonstrates that the two pursuer problem is significantly more difficult for memory-based reasoning. One possible reason for the poor performance of k-NN on the two pursuer task is presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [AK89, Sal91] </ref>. <p> Another possible reason for MBR's problems is the presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [AK89, Sal91] </ref>. But the most likely reason for MBR's troubles, we concluded, was that we were generating bad examples in the early phases of the game. As stated above, MBR needs to have the "correct" action, or something close to it, stored with every state in memory.
Reference: [SDHK91] <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In addition, we found that editing the example set produced a relatively small set of examples that still play the game extremely well. It might be possible with careful editing to reduce the size of memory even further. This question is related to theoretical work by Salzberg et al. <ref> [SDHK91] </ref> that studies the question of how to find a minimal-size training set through the use of a "helpful teacher", which explicitly provides very good examples.
Reference: [SG93] <author> R. E. Smith and B. Gray. </author> <title> Co-adaptive genetic algorithms: An example in othello strategy. </title> <type> Technical Report TCGA Report No. 94002, </type> <institution> University of Alabama, Tuscaloosa, Alabama, </institution> <year> 1993. </year>
Reference-contexts: METAGAMER has been applied to chess, checkers, noughts and crosses (i.e., Tic-Tac-Toe), and GO and has yielded performance at an intermediate level for each of the games. Smith and Gray <ref> [SG93] </ref> applied what they call a co-adaptive genetic algorithm to learn to play othello. A co-adaptive GA is a genetic algorithm in which fitness values of members of the population are dependent on the fitness of other members in the population.
Reference: [Sut88] <author> R. Sutton. </author> <title> Learning to predict by methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: They addressed the problem of teaching a robot to navigate around obstacles. Considerable research has been performed using a form of reinforcement learning called temporal difference learning <ref> [Sut88] </ref>. Temporal difference learning methods apply delayed reinforcement to a sequence of actions to predict future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. <p> The correct reward is usually unknown until after a sequence of actions is complete; therefore, temporal credit assignment (i.e., giving the reward to the "key" actions) is a central issue in TD methods. TD methods usually assume that both the input space and the output space are discrete <ref> [Sut88, Tes92] </ref>. Q-learning generally represents a problem using a lookup table that contains all the states, and naturally it encounters problems with large, continuous state spaces such as those encountered in differential games.
Reference: [Tes92] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: They found they were able to control the development of niches in the population to handle several different types of opponents. Finally, Tesauro used temporal difference learning <ref> [Tes92] </ref> and neural networks [TS89] to "self-teach" connectionist systems to play backgammon. In many ways, backgammon extends beyond "strategic games" due to the stochastic nature of play, and his 3 TD-based system is currently playing at a master level. <p> The correct reward is usually unknown until after a sequence of actions is complete; therefore, temporal credit assignment (i.e., giving the reward to the "key" actions) is a central issue in TD methods. TD methods usually assume that both the input space and the output space are discrete <ref> [Sut88, Tes92] </ref>. Q-learning generally represents a problem using a lookup table that contains all the states, and naturally it encounters problems with large, continuous state spaces such as those encountered in differential games.
Reference: [Tom76] <author> I. Tomek. </author> <title> An experiment with the edited nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6(6):448-452, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Wilson's algorithm was to classify each example in a data set with its own k nearest neighbors. Those points that are incorrectly classified are deleted from the example set, the idea being that such points probably represent noise. Tomek <ref> [Tom76] </ref> modified this approach by taking a sample (&gt; 1) of the data and classifying the sample with the remaining examples. Editing then proceeds as in Wilson editing. Ritter et al. [RWLI75] developed another editing method, which differs from Wilson in that points that are correctly classified are discarded.
Reference: [TS89] <author> G. Tesauro and T. J. Sejnowski. </author> <title> A parallel network that learns to play backgammon. </title> <journal> Artificial Intelligence, </journal> <volume> 39 </volume> <pages> 357-390, </pages> <year> 1989. </year>
Reference-contexts: They found they were able to control the development of niches in the population to handle several different types of opponents. Finally, Tesauro used temporal difference learning [Tes92] and neural networks <ref> [TS89] </ref> to "self-teach" connectionist systems to play backgammon. In many ways, backgammon extends beyond "strategic games" due to the stochastic nature of play, and his 3 TD-based system is currently playing at a master level.
Reference: [Wat89] <author> C. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE [BSA83, BSW90] and Q-learning <ref> [Wat89] </ref>. The original work by Barto et al. [BSA83] demonstrated that the cart and pole problem could be solved using this method. Since then, Clouse and Utgoff [CU92] applied ACE/ASE with a separate teacher to the cart and pole problem and applied Q-learning to navigating a race track. <p> MBR approach had no way (at first) to throw away examples, if it collects many bad examples it could be forever stuck at a low level of performance. 5 Q-Learning with MBR for Evasive Maneuvers Q-learning is a particular reinforcement learning algorithm that uses a temporal difference (TD) learning rule <ref> [Wat89] </ref>. TD learning develops a model that predicts action and rewards for a set of states. The action model is used to decide what action to take from a given state, and the reward model is used during learning to revise the state-action prediction model. <p> Such an architecture could lead to much faster training times. In addition, there are several reinforcement learning algorithms that appear to be closely related to memory-based methods such as k-NN. For example, Watkins' Q-learning <ref> [Wat89] </ref> constructs a lookup table of state-action pairs with corresponding expected discounted rewards. Q-learning performs asynchronous dynamic programming to determine these expected rewards and is known for being able to adapt to changing conditions.
Reference: [Wid87] <author> B. Widrow. </author> <title> The original adaptive neural net broom-balancer. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <pages> pages 351-357, </pages> <year> 1987. </year> <month> 28 </month>
Reference-contexts: This approach is typical in nearest-neighbor applications that rely on determining "good" actions before storing examples. One of the most popular approaches to reinforcement learning has been using neural network learning algorithms. For example, back-propagation has been used to solve the cart and pole problem <ref> [Wid87] </ref>, in which a pole must be balanced vertically on a wheeled cart. A similar algorithm was applied to learning strategies for backing a truck into a loading dock [NW89]. Both of these methods incorporated knowledge of the correct behavior during training.
Reference: [Wil72] <author> D. Wilson. </author> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 2(3) </volume> <pages> 408-421, </pages> <month> July </month> <year> 1972. </year> <month> 29 </month>
Reference-contexts: We therefore modified a known editing algorithm for our problem. We call the resulting system GANNE (GA plus nearest neighbor plus editing). Early work by Wilson <ref> [Wil72] </ref> showed that examples could be removed from a set used for classification, and that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees [Min89]). Wilson's algorithm was to classify each example in a data set with its own k nearest neighbors.
References-found: 33

