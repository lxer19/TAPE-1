URL: http://www.mli.gmu.edu/~bloedorn/papers/ismis96.final.ps.gz
Refering-URL: http://www.mli.gmu.edu/~bloedorn/pubs.html
Root-URL: 
Title: Machine Learning and Inference  
Note: 1 Also with GMU  
Address: Fairfax VA  Warsaw, Poland  
Affiliation: George Mason University,  Departments of Computer Science and Systems Engineering, and the Institute of Computer Science at the Polish Academy of Sciences,  
Pubnum: Laboratory,  
Abstract: Constructive induction divides the problem of learning an inductive hypothesis into two intertwined searches: onefor the best representation space, and twofor the best hypothesis in that space. In data-driven constructive induction (DCI), a learning system searches for a better representation space by analyzing the input examples (data). The presented data-driven constructive induction method combines an AQ-type learning algorithm with two classes of representation space improvement operators: constructors, and destructors. The implemented system, AQ17-DCI, has been experimentally applied to a GNP prediction problem using a World Bank database. The results show that decision rules learned by AQ17-DCI outperformed the rules learned in the original representation space both in predictive accuracy and rule simplicity.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Baim, P.W., </author> <title> The PROMISE Method for Selecting the Most Relevant Attributes for Inductive Learning Systems, </title> <type> Rep. </type> <institution> No. UIUCDCS-F-82-898, Dept. of Computer Science, University of Illinois-Urbana Champaign, IL, </institution> <year> 1982. </year>
Reference-contexts: The latter process has been investigated in the rough set approach (e.g., [27], [41] [28], and [10]). The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE <ref> [1] </ref> or information gain ratio [30]. The rough set approach also applies an attribute-value abstraction operator, which removes values that are not needed for describing data. In contrast to this, AQ17-DCI combines less relevant values with adjacent values into larger units, that is, performs an abstraction of the attribute domain. <p> Here, we view this process more generally as a form of reduction of the representation space. SELECT uses a measure of attribute relevance, such as well-known information gain ratio [30], or PROMISE <ref> [1] </ref> for the purpose of determining which attributes should be used for defining the representation space in which search for the inductive hypothesis will occur. The attributes that score on the attribute relevance measure above a certain threshold are selected as dimensions of the transformed representation space. 5.
Reference: 2. <author> Bloedorn, E. and Michalski, </author> <title> R.S. "Data-Driven Constructive Induction in AQ17-PRE: A Method and Experiments", </title> <booktitle> Proceedings of the Third International Conference on Tools for AI, </booktitle> <month> November </month> <year> 1991a. </year>
Reference-contexts: More recent work has viewed constructive induction more generally, namely, as a doublesearch process, in which one search is for an improved representation space and the second for best hypothesis in this space <ref> [2] </ref> [3] [38]. The improvement of a representation space is done in several The AQ17-DCI System For Data-Driven Constructive Induction and Its Application to the Analysis of World Economics Eric Bloedorn and Ryszard S.
Reference: 3. <author> Bloedorn, E. and Michalski, </author> <title> R.S., Constructive Induction from Data in AQ17-DCI: Further Experiments," Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 91-12, </type> <institution> School of Information Technology and Engineering, George Mason University, Fairfax, VA, </institution> <month> December </month> <year> 1991b. </year>
Reference-contexts: More recent work has viewed constructive induction more generally, namely, as a doublesearch process, in which one search is for an improved representation space and the second for best hypothesis in this space [2] <ref> [3] </ref> [38]. The improvement of a representation space is done in several The AQ17-DCI System For Data-Driven Constructive Induction and Its Application to the Analysis of World Economics Eric Bloedorn and Ryszard S.
Reference: 4. <author> Bloedorn, E., Michalski, R., and Wnek, J., </author> <title> Multistrategy Constructive Induction, </title> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <address> Harpers Ferry, WV, </address> <month> May 26-29, </month> <year> 1993. </year>
Reference: 5. <author> Bloedorn, E., Michalski, R.S., and Wnek, J., </author> <title> Matching Methods with Problems: A Comparative Analysis of Constructive Induction Approaches, Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 94-2, </type> <institution> George Mason University, Fairfax, VA, </institution> <year> 1994. </year>
Reference-contexts: Experiments have shown that representation space expansion is very useful when the attribute construction operators are well-matched with the problem at hand. Representation space contraction, however, must be performed with great care, as it may lead to a removal of information that is crucial for learning a correct hypothesis <ref> [5] </ref>. For this reason, default thresholds on the space contraction operators are set conservatively. The following sections describe these operators in more detail. 4.2.1 Space Expansion: Attribute Construction By GENERATE The GENERATE method for constructing new attributes employs both mathematical and logical operators to construct new attributes.
Reference: 6. <author> Bloedorn, E. and Kaufman, </author> <title> K, Data-Driven Constructive Induction in INLEN, Reports of the Machine Learning and Inference Laboratory, </title> <institution> George Mason University, Fairfax, VA, </institution> <note> 1996 (to appear). </note>
Reference-contexts: An Experimental Application to World Economics This section describes an application of the AQ17-DCI system to a problem of determining the economic and demographic patterns in the countries of the world. The data were obtained from a World Bank database <ref> [6] </ref>. This database contains economic and demographic records for the countries of the world from 1965 to 1990 [12].
Reference: 7. <author> Dougherty, J., Kohavi, R., and Sahami, M., </author> <title> Supervised and Unsupervised Discretization of Continuous Features, </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 194-201, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A summary of representation space contraction operators in AQ17-DCI. 4.2.2.1 Attribute-value Abstraction Using QUANT Research on attribute-value abstraction is usually performed under the name attribute-value discretization <ref> [7] </ref>, [8]. We view this process as a form of abstraction because the result of it is a decrease of information about an object [24]. By replacing original attribute values by more abstract ones the representation space is reduced, thus this process represents a representation space contraction transformation.
Reference: 8. <author> Fulton, T., Kasif, S., and Salzberg S., </author> <title> Efficient Algorithms for Finding Multi-way Splits for Decision Trees, </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 244-251., </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A summary of representation space contraction operators in AQ17-DCI. 4.2.2.1 Attribute-value Abstraction Using QUANT Research on attribute-value abstraction is usually performed under the name attribute-value discretization [7], <ref> [8] </ref>. We view this process as a form of abstraction because the result of it is a decrease of information about an object [24]. By replacing original attribute values by more abstract ones the representation space is reduced, thus this process represents a representation space contraction transformation.
Reference: 9. <author> Greene, </author> <title> G.H., Quantitative Discovery: Using Dependencies to Discover NonLinear Terms, M.S. </title> <type> Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year>
Reference-contexts: Relation to Other Research Research on constructive induction has produced a number of working programs. The first program that was explicitly dedicated to exhibit constructive induction capabilities was INDUCE [22]. INDUCE generates new attributes or new predicates by applying various constructive generalization rules. BACON.3 [14] and ABACUS <ref> [9] </ref> search for mathematical relationships or laws that summarize numerical (or numerical and symbolic) data. Lenats AM and Eurisko programs [15] can be viewed as performing a form of knowledge-based constructive induction, as they generate new concepts according to certain heuristics.
Reference: 10. <author> Gryzmala-Busse, J.W., </author> <title> LERS - A System for Learning from Examples based on Rough Sets in Slowinski, </title> <editor> R., Ed. </editor> <title> Intelligent Decision Support Handbook of Applications and Advances of the Rough Sets Theory, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 3-18. </pages> <year> 1992. </year>
Reference-contexts: As mentioned earlier, constructive induction is a process in which the original representation space is improved during learning. This can be done by generating new attributes and/or removing less relevant or redundant ones. The latter process has been investigated in the rough set approach (e.g., [27], [41] [28], and <ref> [10] </ref>). The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE [1] or information gain ratio [30].
Reference: 11. <author> Jensen, G., "SYM-1: </author> <title> A Program that Detects Symmetry of Variable-Valued Logic Functions", </title> <type> Report UIUCDCS-R-75-729, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1975. </year>
Reference: 12. <author> Kaufman, K., </author> <title> Comparing International Development Patterns Using Multi-Operator Learning and Discovery Tools, </title> <booktitle> Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Seattle, WA, </address> <pages> pp. 431-440. </pages> <year> 1994. </year>
Reference-contexts: The data were obtained from a World Bank database [6]. This database contains economic and demographic records for the countries of the world from 1965 to 1990 <ref> [12] </ref>. The goal of the experiment presented here was to determine a set of rules characterizing the dependence of GNP (Gross National Product) in various countries during the period from 1986 to 1990 on the available economic and demographic characteristics. In the experiment we considered 41 countries.
Reference: 13. <author> Kerber, R., ChiMerge: </author> <title> Discretization of Numeric Attributes, </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 123-128, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: By replacing original attribute values by more abstract ones the representation space is reduced, thus this process represents a representation space contraction transformation. QUANT abstracts attribute values using the ChiMerge method described by Kerber <ref> [13] </ref>. This abstraction, a.k.a. scaling, is performed for continuous attributes and for discrete attributes with large domains. Because it reduces the size of the representation space, abstraction can significantly speed up the search for hypothesis.
Reference: 14. <author> Langley, P., </author> <title> Rediscovering Physics with Bacon 3, </title> <booktitle> Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 505-507, </pages> <address> Cambridge, MA: </address> , <year> 1977. </year>
Reference-contexts: Relation to Other Research Research on constructive induction has produced a number of working programs. The first program that was explicitly dedicated to exhibit constructive induction capabilities was INDUCE [22]. INDUCE generates new attributes or new predicates by applying various constructive generalization rules. BACON.3 <ref> [14] </ref> and ABACUS [9] search for mathematical relationships or laws that summarize numerical (or numerical and symbolic) data. Lenats AM and Eurisko programs [15] can be viewed as performing a form of knowledge-based constructive induction, as they generate new concepts according to certain heuristics.
Reference: 15. <author> Lenat, Douglas, </author> <title> Learning from Observation and Discovery, in Machine Learning: A n Artificial Intelligence Approach, Vol. I, R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell (Eds.), </editor> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann (reprint), </publisher> <year> 1983 </year>
Reference-contexts: INDUCE generates new attributes or new predicates by applying various constructive generalization rules. BACON.3 [14] and ABACUS [9] search for mathematical relationships or laws that summarize numerical (or numerical and symbolic) data. Lenats AM and Eurisko programs <ref> [15] </ref> can be viewed as performing a form of knowledge-based constructive induction, as they generate new concepts according to certain heuristics. Schlimmers STAGGER [33] is a constructive induction program that uses three cooperating learning modules: weight adjustment, Boolean feature construction, and attribute value aggregation.
Reference: 16. <author> Matheus, C. J. and Rendell, L.A., </author> <title> Constructive Induction on Decision Trees, </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 645-650, </pages> <year> 1989. </year>
Reference-contexts: The idea of constructive induction is not new [23]. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data [21] [36] <ref> [16] </ref> [32]. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated. <p> Muggleton's Duce [25] is an oracle-based approach (knowledge-driven). Pagallo and Haussler's FRINGE, GREEDY3 and GROVE [26] base the construction of new attributes on patterns found in learned decision trees. Another decision tree-based method is CITRE <ref> [16] </ref>, which constructs new terms by repeatedly applying Boolean operators to nodes on the positively labeled branches. A hypothesis-driven approach based on decision rules is AQ17-HCI [38]. In this system patterns prevalent in strong rules are used for constructing new attributes.
Reference: 17. <author> Michalski, </author> <title> R.S., Recognition of Total or Partial Symmetry in a Completely or Incompletely Specified Switching Function, </title> <booktitle> Proceedings of the IV Congress of the International Federation on Automatic Control (IFAC), Vol. 27 (Finite Automata and Switching Systems), </booktitle> <pages> pp. 109-129, </pages> <address> Warsaw, </address> <month> June 16-21, </month> <year> 1969. </year>
Reference: 18. <author> Michalski, </author> <title> R.S., </title> <booktitle> On the Quasi-Minimal Solution of the Covering Problem Proceedings of the V International Symposium on Information Processing (FCIP 69), Vol. A3 (Switching Circuits), Bled, Yugoslavia, </booktitle> <pages> pp. 125-128, </pages> <year> 1969. </year>
Reference-contexts: The AQ algorithm generates an optimal or near-optimal set of rules characterizing training examples, according to a given criterion of optimality (originally described in <ref> [18] </ref>, and [19]). The criterion of optimality may take into consideration such factors as the number of rules, the number of conditions, the cost of attributes in the rules, and others.
Reference: 19. <author> Michalski, R.S. and McCormick, B.H., </author> <title> Interval Generalization of Switching Theory. </title> <type> Report No. 442, </type> <institution> Dept. of Computer Science, University of Illinois, Urbana. </institution> <year> 1971. </year>
Reference-contexts: The AQ algorithm generates an optimal or near-optimal set of rules characterizing training examples, according to a given criterion of optimality (originally described in [18], and <ref> [19] </ref>). The criterion of optimality may take into consideration such factors as the number of rules, the number of conditions, the cost of attributes in the rules, and others.
Reference: 20. <author> Michalski, </author> <title> R.S., </title> <booktitle> Variable-Valued Logic: System VL 1 , Proceedings of the 1974 International Symposium on Multiple-Valued Logic, </booktitle> <pages> pp. 323-346. </pages> <institution> West Virginia University, Morgantown, </institution> <year> 1974. </year>
Reference: 21. <author> Michalski, R.S. and Larson, J.B., </author> <title> Selection of Most Representative Training Examples and Incremental Generation of VL 1 Hypotheses: the underlying methodology and the description of programs ESEL and AQ11, </title> <type> Report No. 867, </type> <institution> Dept. of Computer Science, University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: The idea of constructive induction is not new [23]. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data <ref> [21] </ref> [36] [16] [32]. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated.
Reference: 22. <author> Michalski, </author> <title> R.S., Pattern Recognition as Rule-Guided Inductive Inference, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), </journal> <volume> Vol. 2, No. 4, </volume> <pages> pp. 349-361, </pages> <year> 1980. </year>
Reference-contexts: This rule exactly represents the intended target concept, and thus has a predictive accuracy of 100%. 3. Relation to Other Research Research on constructive induction has produced a number of working programs. The first program that was explicitly dedicated to exhibit constructive induction capabilities was INDUCE <ref> [22] </ref>. INDUCE generates new attributes or new predicates by applying various constructive generalization rules. BACON.3 [14] and ABACUS [9] search for mathematical relationships or laws that summarize numerical (or numerical and symbolic) data.
Reference: 23. <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning: Developing Foundations for Multistrategy Learning, </title> <booktitle> in Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. I, </volume> <editor> R.S. Michalski, J.G. Carbonell and T.M. Mitchell (Eds.), </editor> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann (reprint), </publisher> <year> 1983. </year>
Reference-contexts: Using another terminology that is also used in machine learning, constructive induction includes the problem of automatically determining the best representation space bias as a part of the induction process. The idea of constructive induction is not new <ref> [23] </ref>. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data [21] [36] [16] [32]. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated. <p> The idea of constructive induction is not new <ref> [23] </ref>. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data [21] [36] [16] [32]. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated.
Reference: 24. <author> Michalski, </author> <title> R.S., Inferential Theory of Learning, in Machine Learning: An Multistrategy Approach, Vol. IV, R.S. </title> <editor> Michalski, and G. Tecuci (Eds.), </editor> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: We view this process as a form of abstraction because the result of it is a decrease of information about an object <ref> [24] </ref>. By replacing original attribute values by more abstract ones the representation space is reduced, thus this process represents a representation space contraction transformation. QUANT abstracts attribute values using the ChiMerge method described by Kerber [13].
Reference: 25. <author> Muggleton, S., "Duce, </author> <title> an Oracle-Based Approach to Constructive Induction", </title> <booktitle> Proceedings of IJCAI-87, </booktitle> <pages> pp. 287-292, </pages> <publisher> Morgan Kaufman, </publisher> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: Schlimmers STAGGER [33] is a constructive induction program that uses three cooperating learning modules: weight adjustment, Boolean feature construction, and attribute value aggregation. Muggleton's Duce <ref> [25] </ref> is an oracle-based approach (knowledge-driven). Pagallo and Haussler's FRINGE, GREEDY3 and GROVE [26] base the construction of new attributes on patterns found in learned decision trees. Another decision tree-based method is CITRE [16], which constructs new terms by repeatedly applying Boolean operators to nodes on the positively labeled branches.
Reference: 26. <author> Pagallo, G., and Haussler, D., </author> <title> "Boolean Feature Discovery in Empirical Learning", </title> <journal> Machine Learning, </journal> <volume> vol. 5, </volume> <pages> pp. 71-99, </pages> <year> 1990. </year>
Reference-contexts: Schlimmers STAGGER [33] is a constructive induction program that uses three cooperating learning modules: weight adjustment, Boolean feature construction, and attribute value aggregation. Muggleton's Duce [25] is an oracle-based approach (knowledge-driven). Pagallo and Haussler's FRINGE, GREEDY3 and GROVE <ref> [26] </ref> base the construction of new attributes on patterns found in learned decision trees. Another decision tree-based method is CITRE [16], which constructs new terms by repeatedly applying Boolean operators to nodes on the positively labeled branches. A hypothesis-driven approach based on decision rules is AQ17-HCI [38].
Reference: 27. <author> Pawlak, Z. </author> <title> Rough Sets and their Applications, </title> <booktitle> Workshop on Mathematics and AI, Schloss Reisburg, W. Germany. </booktitle> <volume> Vol II. </volume> <pages> pp. 543-572. </pages> <year> 1988. </year>
Reference-contexts: As mentioned earlier, constructive induction is a process in which the original representation space is improved during learning. This can be done by generating new attributes and/or removing less relevant or redundant ones. The latter process has been investigated in the rough set approach (e.g., <ref> [27] </ref>, [41] [28], and [10]). The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE [1] or information gain ratio [30].
Reference: 28. <author> Pawlak, Z. </author> <title> Rough Sets: Theoretical Aspects of Reasoning about Data, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> AA Dordrecht, The Netherlands, </address> <year> 1991. </year>
Reference-contexts: As mentioned earlier, constructive induction is a process in which the original representation space is improved during learning. This can be done by generating new attributes and/or removing less relevant or redundant ones. The latter process has been investigated in the rough set approach (e.g., [27], [41] <ref> [28] </ref>, and [10]). The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE [1] or information gain ratio [30].
Reference: 29. <author> Quinlan, J. R., </author> <title> Learning Efficient Classification Procedures, Machine Learning: A n Artificial Intelligence Approach, </title> <editor> Michalski, R.S., Carbonell, J.G, and Mitchell, T.M. (Eds.), </editor> <publisher> Morgan Kaufmann 1983, </publisher> <pages> pp. 463-482. </pages>
Reference: 30. <author> Quinlan, J.R., C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE [1] or information gain ratio <ref> [30] </ref>. The rough set approach also applies an attribute-value abstraction operator, which removes values that are not needed for describing data. In contrast to this, AQ17-DCI combines less relevant values with adjacent values into larger units, that is, performs an abstraction of the attribute domain. <p> Here, we view this process more generally as a form of reduction of the representation space. SELECT uses a measure of attribute relevance, such as well-known information gain ratio <ref> [30] </ref>, or PROMISE [1] for the purpose of determining which attributes should be used for defining the representation space in which search for the inductive hypothesis will occur.
Reference: 31. <author> Reinke, R.E., </author> <title> Knowledge Acquisition and Refinement Tools for the ADVISE Meta-expert System, </title> <type> Masters Thesis, </type> <institution> University of Illinois, </institution> <year> 1984. </year>
Reference: 32. <author> Rendell, L., and Seshu, R., </author> <title> Learning Hard Concepts Through Constructive Induction: Framework and Rationale, </title> <journal> Computer Intelligence, </journal> <volume> Vol. 6, </volume> <pages> pp. 247-270, </pages> <year> 1990. </year>
Reference-contexts: The idea of constructive induction is not new [23]. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data [21] [36] [16] <ref> [32] </ref>. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated.
Reference: 33. <author> Schlimmer, J., </author> <title> "Concept Acquisition Through Representational Adjustment," </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: BACON.3 [14] and ABACUS [9] search for mathematical relationships or laws that summarize numerical (or numerical and symbolic) data. Lenats AM and Eurisko programs [15] can be viewed as performing a form of knowledge-based constructive induction, as they generate new concepts according to certain heuristics. Schlimmers STAGGER <ref> [33] </ref> is a constructive induction program that uses three cooperating learning modules: weight adjustment, Boolean feature construction, and attribute value aggregation. Muggleton's Duce [25] is an oracle-based approach (knowledge-driven). Pagallo and Haussler's FRINGE, GREEDY3 and GROVE [26] base the construction of new attributes on patterns found in learned decision trees.
Reference: 34. <author> Thrun, S.B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzerowski, S., Fahlman, S.E., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R.S., Mitchell, T., Pachowicz, P., Vafaie, H., Van de Velde, W., Wenzel, W., Wnek, J., and Zhang, J., </author> <title> The MONKS Problems: A Performance Comparison of Different Learning Algorithms, </title> <note> (revised version), </note> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, CMU-CS-91-197, </address> <year> 1991. </year>
Reference-contexts: In conventional machine learning methods this space is identical to the space in which training examples are represented. As mentioned earlier, the choice of the representation space has a profound effect on the quality of the generated hypotheses. This effect is well illustrated by the second Monks problem <ref> [34] </ref>. The original representation space with the training examples denoted by + and - is shown in Figure 1 (a) using DIAV [39]. The shaded area represents the target concept. (a) (b) space; and (b) improved space due to the data-driven constructive induction.
Reference: 35. <author> Utgoff, P., </author> <title> "Shift of Bias for Inductive Learning,", </title> <booktitle> in Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, </volume> <editor> R. Michalski, J. Carbonell, and T. Mitchell (eds.), </editor> <publisher> Morgan Kaufman, </publisher> <address> Los Altos, CA, </address> <pages> pp. 107-148, </pages> <year> 1986. </year>
Reference-contexts: A hypothesis-driven approach based on decision rules is AQ17-HCI [38]. In this system patterns prevalent in strong rules are used for constructing new attributes. An approach which uses disjunctive or arithmetic combinations of the original attributes to extend the initial attribute set was developed by Utgoff in STABB <ref> [35] </ref>. As mentioned earlier, constructive induction is a process in which the original representation space is improved during learning. This can be done by generating new attributes and/or removing less relevant or redundant ones.
Reference: 36. <author> Watanabe, L., and Elio, R., </author> <title> Guiding Constructive Induction for Incremental Learning from Examples, </title> <booktitle> Proceedings of IJCAI-87, </booktitle> <pages> pp. 293-296, </pages> <address> Milan, Italy: </address> , <year> 1987. </year>
Reference-contexts: The idea of constructive induction is not new [23]. Initial research on this topic concentrated solely on constructing new attributes beyond those provided in the input data [21] <ref> [36] </ref> [16] [32]. Michalski [23] presented a set of constructive generalization rules that describe various ways in which new attributes can be generated.
Reference: 37. <author> Weiss, S. M., and Kulikowski, C. A., </author> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <year> 1991. </year>
Reference-contexts: Thus, each country was described by 55 attributes. AQ17-DCI was applied to determine rules that characterize GNP changes in terms of the given attributes (or their relevant subset).The quality of the generated rules was evaluated by the 10-fold cross-validation method <ref> [37] </ref>. The learning process involved the following steps: 1. Remove less relevant attributes . AQ17-DCI removed 24 attributes using the SELECT method described in Section 4.2.2.2 with a information threshold set to 0.6. 2. Abstract away unnecessary detail .
Reference: 38. <author> Wnek, J. and Michalski, R., </author> <title> Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments, </title> <journal> Machine Learning, </journal> <volume> Vol. 14, No. 2, </volume> <pages> pp. </pages> <year> 139-168.1993. </year>
Reference-contexts: More recent work has viewed constructive induction more generally, namely, as a doublesearch process, in which one search is for an improved representation space and the second for best hypothesis in this space [2] [3] <ref> [38] </ref>. The improvement of a representation space is done in several The AQ17-DCI System For Data-Driven Constructive Induction and Its Application to the Analysis of World Economics Eric Bloedorn and Ryszard S. <p> Michalski 1 waysby generating new attributes, by removing less relevant or irrelevant attributes, and/or by abstracting values of given attributes (grouping values to larger units). The search for an improved representation space can be guided by information from three sources <ref> [38] </ref>: training data (as in data-driven constructive inductionDCI), initial hypotheses learned from the data (as in hypothesis-driven constructive inductionHCI), or expert knowledge provided by the user to the system (as in knowledge-driven constructive inductionKCI). These sources can also be combined into a multistrategy constructive induction method. <p> Another decision tree-based method is CITRE [16], which constructs new terms by repeatedly applying Boolean operators to nodes on the positively labeled branches. A hypothesis-driven approach based on decision rules is AQ17-HCI <ref> [38] </ref>. In this system patterns prevalent in strong rules are used for constructing new attributes. An approach which uses disjunctive or arithmetic combinations of the original attributes to extend the initial attribute set was developed by Utgoff in STABB [35].
Reference: 39. <author> Wnek, J. </author> <title> DIAV 2.0 User Manual: Specification and Guide through the Diagrammatic Visualization System, Reports of the Machine Learning and Inference Laboratory, </title> <institution> MLI95-5, George Mason University, Fairfax, </institution> <address> VA 1995. </address>
Reference-contexts: This effect is well illustrated by the second Monks problem [34]. The original representation space with the training examples denoted by + and - is shown in Figure 1 (a) using DIAV <ref> [39] </ref>. The shaded area represents the target concept. (a) (b) space; and (b) improved space due to the data-driven constructive induction. In this original representation space, the learning problem is difficult because the target concept is highly irregular.
Reference: 40. <author> Wnek, J., Kaufman, K., Bloedorn, E., and Michalski, </author> <title> R.S., Selective Induction Learning System AQ15c: The Method and Users Guide, Reports of the Machine Learning and Inference Laboratory, </title> <address> MLI 95-4. </address>
Reference: 41. <author> Ziarko, W. </author> <title> On Reduction of Knowledge Representation, </title> <booktitle> Proceedings of the 2nd International Symposium on Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, NC. </address> <publisher> North Holland, </publisher> <pages> pp. 99-113. </pages>
Reference-contexts: As mentioned earlier, constructive induction is a process in which the original representation space is improved during learning. This can be done by generating new attributes and/or removing less relevant or redundant ones. The latter process has been investigated in the rough set approach (e.g., [27], <ref> [41] </ref> [28], and [10]). The AQ17-DCI system presented in this paper executes a complementary process of selecting the most relevant attributes. This is done by applying some measure of attribute relevance, for example, PROMISE [1] or information gain ratio [30].
References-found: 41

