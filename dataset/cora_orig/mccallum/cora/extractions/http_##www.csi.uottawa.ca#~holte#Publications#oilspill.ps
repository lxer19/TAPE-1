URL: http://www.csi.uottawa.ca/~holte/Publications/oilspill.ps
Refering-URL: http://www.csi.uottawa.ca/~holte/Publications/index.html
Root-URL: 
Email: fmkubat, holte,stang@site.uottawa.ca  
Title: Machine Learning for the Detection of Oil Spills in Satellite Radar Images  
Author: MIROSLAV KUBAT ROBERT C. HOLTE, STAN MATWIN Editors: Ron Kohavi, Foster Provost 
Keyword: Inductive learning, classification, radar images, methodology  
Address: Ottawa, 150 Louis Pasteur, Ottawa Ontario, K1N 6N5 Canada  
Affiliation: School of Information Technology and Engineering University of  
Note: Machine Learning, xx, 1-23 (xxxx) c xxxx Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: During a project examining the use of machine learning techniques for oil spill detection, we encountered several essential questions that we believe deserve the attention of the research community. We use our particular case study to illustrate such issues as problem formulation, selection of evaluation measures, and data preparation. We relate these issues to properties of the oil spill application, such as its imbalanced class distribution, that are shown to be common to many applications. Our solutions to these issues are implemented in the Canadian Environmental Hazards Detection System (CEHDS), which is about to undergo field testing. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1), </volume> <pages> 37-66. </pages>
Reference: <author> Aha, D. </author> <year> (1992). </year> <title> Generalizing from Case Studies: A Case Study. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 1-10), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brodley, C., & Smyth, P. </author> <year> (1995). </year> <title> The Process of Applying Machine Learning Algorithms. Working Notes for Applying Machine Learning in Practice: </title> <booktitle> A Workshop at the Twelfth International Conference on Machine Learning, Technical Report AIC-95-023 (pp. </booktitle> <pages> 7-13), </pages> <institution> NRL, Navy Center for Applied Research in AI, </institution> <address> Washington, DC. </address>
Reference: <author> Burl, M.C., Asker, L., Smyth, P., Fayyad, U.M., Perona, P., Crumpler, L., & Aubele, J. </author> <note> (this issue). Learning to Recognize Volcanoes on Venus. Machine Learning, ??(??), ??-??. DETECTION OF OIL SPILLS 21 Caruana, </note> <author> R. </author> <year> (1993). </year> <title> Multitask Learning: A Knowledge-based Source of Inductive Bias. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 41-48), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A Test Flight. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 596-599), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We currently have 9 carefully selected images containing a total of 41 oil slicks. While many applications work with large amounts of available data <ref> (Catlett, 1991) </ref>, our domain application is certainly not unique in its data scarcity. For example, in the drug activity application reported by Dietterich, Lathrop and Lozano-Perez (1997) the two datasets contain 47 and 39 positive examples respectively. <p> As early as the late sixties, Hart (1968) presented a mechanism that removes re DETECTION OF OIL SPILLS 13 dundant examples and, somewhat later, Tomek (1976) introduced a simple method to detect borderline and noisy examples. In machine learning the best known sampling technique is windowing <ref> (Catlett, 1991) </ref>. For more recent alternatives, see, for instance, Aha, Kibler and Albert (1991), Zhang (1992), Skalak (1994), Floyd and Warmuth (1995), and Lewis and Catlett (1994). Variations of data reduction techniques, namely those that remove only negative examples, are analyzed by Kubat and Matwin (1997).
Reference: <author> Cherkauer, K.J., & Shavlik, J.W. </author> <year> (1994). </year> <title> Selecting Salient Features for Machine Learning from Large Candidate Pools through Parallel Decision-Tree Construction. </title> <editor> In Kitano, H. & Hendler, J. (Eds.), </editor> <booktitle> Massively Parallel Artificial Intelligence (pp. </booktitle> <pages> 102-136), </pages> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Clark, P. & Matwin, S. </author> <year> (1993). </year> <title> Using Qualitative Models to Guide Inductive Learning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 49-56), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clearwater, S., & Stern, E. </author> <year> (1991). </year> <title> A rule-learning program in high energy physics event classification. </title> <journal> Comp. Physics Comm., </journal> <volume> 67, </volume> <pages> 159-182. </pages>
Reference: <author> DeRouin, E., Brown, J., Beck, H., Fausett, L., & Schneider, M. </author> <year> (1991). </year> <title> Neural Network Training on Unequally Represented Classes. </title> <editor> In Dagli, C.H., Kumara, S.R.T., & Shin, Y.C. (Eds.), </editor> <booktitle> Intelligent Engineering Systems Through Artificial Neural Networks, </booktitle> <pages> pp. 135-145, </pages> <publisher> ASME Press. </publisher>
Reference-contexts: Variations of data reduction techniques, namely those that remove only negative examples, are analyzed by Kubat and Matwin (1997). Conversely, the training set can be balanced by duplicating the training examples of the minority class or by creating new examples by corrupting existing ones with artificial noise <ref> (DeRouin et al., 1991) </ref>. Solberg and Solberg (1996) do both; positives are duplicated and negatives are randomly sampled. Honda, Mo-tizuki, Ho, and Okumura (1997) reduce the imbalance by doing classification in two stages. In the first stage, the negatives most similar to the positives are included in the positive class.
Reference: <author> Dietterich, T.G., Hild, H., & Bakiri, G. </author> <year> (1995). </year> <title> A Comparison of ID3 and Backpropagation for English Text-to-Speech Mapping. </title> <journal> Machine Learning, </journal> <volume> 18, </volume> <pages> 51-80. </pages>
Reference: <author> Dietterich, T.G., Lathrop, R.H., & Lozano-Perez, T. </author> <year> (1997). </year> <title> Solving the Multiple-Instance Problem with Axis-Parallel Rectangles. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 89(1-2), 31-71. </pages>
Reference: <author> Ezawa, K.J., Singh, M., & Norton, S.W. </author> <year> (1996). </year> <title> Learning Goal Oriented Bayesian Networks for Telecommunications Management. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 139-147), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: KUBAT, R. HOLTE, AND S. MATWIN examples; the majority class thus comprises almost 96% of the data. Highly imbalanced training sets occur in applications where the classifier is to detect a rare but important event, such as fraudulent telephone calls (Fawcett & Provost, 1997), unreliable telecommunications customers <ref> (Ezawa, Singh & Norton, 1996) </ref>, failures or delays in a manufacturing process (Riddle, Segal & Etzioni, 1994), rare diagnoses such as the thyroid diseases in the UCI repository (Murphy & Aha, 1994), or carcinogenicity of chemical compounds (Lee, Buchanan & Aronis, this issue).
Reference: <author> Fawcett, T., & Provost, F. </author> <year> (1997). </year> <title> Adaptive Fraud Detection. Data Mining and Knowledge Discovery, </title> <booktitle> 1(3), </booktitle> <pages> 291-316. </pages>
Reference-contexts: KUBAT, R. HOLTE, AND S. MATWIN examples; the majority class thus comprises almost 96% of the data. Highly imbalanced training sets occur in applications where the classifier is to detect a rare but important event, such as fraudulent telephone calls <ref> (Fawcett & Provost, 1997) </ref>, unreliable telecommunications customers (Ezawa, Singh & Norton, 1996), failures or delays in a manufacturing process (Riddle, Segal & Etzioni, 1994), rare diagnoses such as the thyroid diseases in the UCI repository (Murphy & Aha, 1994), or carcinogenicity of chemical compounds (Lee, Buchanan & Aronis, this issue).
Reference: <author> Fayyad, U.M., Weir, N., & Djorgovski, S. </author> <year> (1993). </year> <title> SKICAT: A Machine Learning System for Automated Cataloging of Large Scale Sky Surveys. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 112-119), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This problem has been mentioned by several other authors, including Burl et al. (this issue), Cherkauer and Shavlik (1994), Ezawa et al. (1996), Fawcett and Provost (1997), Kubat, Pfurtscheller and Flotzinger (1994), and Pfurtscheller, Flotzinger and Kalcher (1992). For instance, in the SKICAT system <ref> (Fayyad, Weir & Djorgovski, 1993) </ref>, the "batches" were plates, from which image regions were selected. When the system trained on images from one plate was applied to images from another plate, the classification accuracy dropped well below that of manual classification.
Reference: <author> Floyd, S., & Warmuth, M. </author> <year> (1995). </year> <title> Sample Compression, Learnability, and the Vapnik-Chervonenkis Dimension. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <pages> 269-304. </pages>
Reference: <author> Hart, P.E. </author> <year> (1968). </year> <title> The Condensed Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14, </volume> <pages> 515-516. </pages>
Reference: <author> Haverkamp, D., Tsatsoulis, C., & Gogineni, S. </author> <year> (1994). </year> <title> The Combination of Algorithmic and Heuristic Methods for the Classification of Sea Ice Imagery. </title> <journal> Remote Sensing Reviews, </journal> <volume> 9, </volume> <pages> 135-159. </pages>
Reference: <author> Heerman, P. D., & Khazenie, N. </author> <year> (1992). </year> <title> Classification of Multispectral Remote Sensing Data using a back-propagation Neural Network. </title> <journal> IEEE Trans. of Geoscience and Remote Sensing, </journal> <volume> 30, </volume> <pages> 81-88. </pages>
Reference: <author> Holte, R. C., Acker, L., & Porter, B. W. </author> <year> (1989). </year> <title> Concept Learning and the Problem of Small Disjuncts. </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 813-818), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, if the individual batches have a relatively small number of positive examples such a system will be prone to the problem of small disjuncts <ref> (Holte, Acker & Porter, 1989) </ref>. Moreover, the batches that have many examples (e.g., images 2 and 9) will dominate those that have few (e.g., image 1). Batched examples also raise an issue about experimental methodology.
Reference: <author> Honda, T., Motizuki, H., Ho, T.B., & Okumura, M. </author> <year> (1997). </year> <title> Generating Decision Trees from an Unbalanced Data Set. </title> <editor> In van Someren, M., & Widmer, G. (Eds.), </editor> <booktitle> Poster papers presented at the 9th European Conference on Machine Learning (pp. </booktitle> <pages> 68-77). </pages>
Reference: <author> Hovland, H. A., Johannessen, J. A., & Digranes, G. </author> <year> (1994). </year> <title> Slick Detection in SAT Images. </title> <booktitle> Proceedings of IGARSS'94 (pp. </booktitle> <pages> 2038-2040). </pages>
Reference-contexts: Elsewhere a group of environmental scientists and remote sensing experts have developed a preliminary model of properties of an oil spill image. The model, expressed as a decision tree <ref> (Hovland, Johannessen & Digranes, 1994) </ref>, uses attributes such as the shape and size of the dark regions in the image, the wind speed at the time when the image was taken, the incidence angle of the radar beam, proximity to land, etc.
Reference: <author> Ioerger, T.R., Rendell, L.A., & Subramaniam, S. </author> <year> (1995). </year> <title> Searching for Representations to Improve Protein Sequence Fold-Class Prediction. </title> <journal> Machine Learning, </journal> <volume> 21, </volume> <pages> 151-176. </pages>
Reference: <author> Keeney, R.L., & Raiffa, H. </author> <year> (1993). </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs, </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Kohavi, R., & John, </author> <title> G.H. (to appear). Wrappers for Feature Subset Selection. </title> <journal> Artificial Intelligence (special issue on relevance). </journal>
Reference: <author> Kohavi, R., & John, G.H. </author> <year> (1995). </year> <title> Automatic Parameter Selection by Minimizing Estimated Error. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 304-312), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Before running the learning algorithm the user selects a specific measure within this family by supplying values for the parameters (i.e., the costs). A second example is the "wrapper approach" to feature selection (Kohavi & John, to appear), parameter setting <ref> (Kohavi & John, 1995) </ref>, or inductive bias selection (Provost & Buchanan, 20 M. KUBAT, R. HOLTE, AND S. MATWIN 1995). It can be adapted easily to work with any performance measure.
Reference: <author> Kononenko, I., & Bratko, I. </author> <year> (1991). </year> <title> Information-Based Evaluation Criterion for Classifier's Performance. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80. </pages>
Reference: <author> Kubat, M., Holte, R., & Matwin, S. </author> <year> (1997). </year> <title> Learning when Negative Examples Abound. </title> <booktitle> Machine Learning: ECML-97, Lecture Notes in Artificial Intelligence 1224 (pp. </booktitle> <pages> 146-153), </pages> <address> Springer. </address> <note> 22 M. KUBAT, </note> <author> R. HOLTE, AND S. MATWIN Kubat, M., & Matwin, S. </author> <year> (1997). </year> <title> Addressing the Curse of Imbalanced Training Sets: One-Sided Sampling. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 179-186), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To try to ensure that our learning algorithm is not specific to our particular dataset we have tested it on other datasets having similar characteristics <ref> (Kubat, Holte & Matwin, 1997) </ref>. Another issue is the imbalance of the dataset's class distribution. This issue has two facets. The first, discussed above, is that when working with imbalanced datasets it is desirable to use a performance measure other than accuracy. <p> By measuring performance only of the positive predicting rules BRUTE is not influenced by the invariably high accuracy on the negative examples that are not covered by the positive predicting rules. Our SHRINK algorithm <ref> (Kubat et al., 1997) </ref> follows the same general principle| find the rule that best summarizes the positive examples|but uses a definition of "best" (g-mean) that takes into account performance of the negative predicting rules as well as the positive predicting ones.
Reference: <author> Kubat, M., Pfurtscheller, G., & Flotzinger D. </author> <year> (1994). </year> <title> AI-Based Approach to Automatic Sleep Classification. </title> <journal> Biological Cybernetics, </journal> <volume> 79, </volume> <pages> 443-448. </pages>
Reference: <editor> Kubat, M., & Widmer, G. (Eds.) </editor> <booktitle> (1996). Proceedings of the ICML'96 Pre-Conference Workshop on Learning in Context-Sensitive Domains. </booktitle>
Reference-contexts: Learning from batched examples is related to the issues of learning in the presence of context, as the batches often represent the unknown context in which the training examples were collected. Learning in context has only recently been recognized as an important problem re-occurring in applications of machine learning <ref> (Kubat & Widmer, 1996) </ref>. Various tradeoffs arose in our project which certainly warrant scientific study. In formulating a problem, one must choose the granularity of the examples (images, regions, or pixels in our application) and the number of classes. Different choices usually lead to different results.
Reference: <author> Langley, P., & Simon, H.A. </author> <year> (1995). </year> <title> Applications of Machine Learning and Rule Induction. </title> <journal> Communications of the ACM, </journal> <volume> 38(11), </volume> <pages> 55-64. </pages>
Reference-contexts: Problem Formulation Issues Machine learning research usually assumes the existence of carefully prepared data that is then subjected only to minor, if any, further processing; an attribute or two might be deleted, missing values filled in, some classes merged or dropped. In applications the situation is not that straightforward <ref> (Langley & Simon, 1995) </ref>. In DETECTION OF OIL SPILLS 7 our case we did not have a precise statement of the problem, much less a data file prepared in a standard format. This section briefly discusses issues related to problem formulation.
Reference: <author> Lee, Y., Buchanan, B.G., & Aronis, J.M. </author> <note> (this issue). Learning Rules to Predict Rodent Carcinogenicity. Machine Learning, </note> ??(??), ??-??. 
Reference: <author> Lewis, D., & Catlett, J. </author> <year> (1994). </year> <title> Heterogeneous Uncertainty Sampling for Supervised Learning. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 148-156), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To measure performance in environments with imbalanced classes, the information retrieval community works with recall (r = d c+d ) and precision (p = d b+d ) and combines them by way of a geometric mean ( p r p) or the more sophisticated F-measure <ref> (Lewis & Gale, 1994) </ref>. Other measures have been suggested (van Rijs-bergen, 1979, Chapter 7), including an information theoretic formula suggested by Kononenko and Bratko (1991).
Reference: <author> Lewis, D., & Gale, W. </author> <year> (1994). </year> <title> A Sequential Algorithm for Training Text Classifiers. </title> <booktitle> Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. </booktitle> <pages> 3-12), </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To measure performance in environments with imbalanced classes, the information retrieval community works with recall (r = d c+d ) and precision (p = d b+d ) and combines them by way of a geometric mean ( p r p) or the more sophisticated F-measure <ref> (Lewis & Gale, 1994) </ref>. Other measures have been suggested (van Rijs-bergen, 1979, Chapter 7), including an information theoretic formula suggested by Kononenko and Bratko (1991).
Reference: <author> Lubinsky, David (1994). </author> <title> Bivariate Splits and Consistent Split Criteria in Dichotomous Classification Trees. </title> <type> Ph.D. thesis, </type> <institution> Computer Science, Rutgers University. </institution>
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI Repository of Machine Learning Databases (machine-readable data repository). </title> <institution> University of California, Irvine. </institution>
Reference-contexts: the classifier is to detect a rare but important event, such as fraudulent telephone calls (Fawcett & Provost, 1997), unreliable telecommunications customers (Ezawa, Singh & Norton, 1996), failures or delays in a manufacturing process (Riddle, Segal & Etzioni, 1994), rare diagnoses such as the thyroid diseases in the UCI repository <ref> (Murphy & Aha, 1994) </ref>, or carcinogenicity of chemical compounds (Lee, Buchanan & Aronis, this issue). Extremely imbalanced classes also arise in information retrieval and filtering tasks. In the domain studies by Lewis and Catlett (1994), only 0.2% (1 in 500) examples are positive.
Reference: <author> Ossen, A., Zamzow, T., Oswald, H., & Fleck, E. </author> <year> (1994). </year> <title> Segmentation of Medical Images Using Neural-Network Classifiers. </title> <booktitle> Proceedings of the International Conference on Neural Networks and Expert Systems in Medicine and Healthcare (NNESMED'94) (pp. </booktitle> <pages> 427-432). </pages>
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. </author> <year> (1994). </year> <title> Reducing Misclassifi-cation Costs. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 217-225), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The standard decision theoretic approach to defining the "optimal" tradeoff between false and true positives is to assign relative costs to errors of omission and errors of commission, and to make the classification that minimizes expected cost <ref> (Pazzani et al., 1994) </ref>. One deterrent to using this approach is that the costs are often hard to determine and may involve multiple considerations whose units are incommensurable (e.g., monetary cost, pollution levels, international reputation). Decision analysis techniques have been developed to cope with this difficulty (von 10 M.
Reference: <author> Pfurtscheller, G., Flotzinger, D., & Kalcher, J. </author> <year> (1992). </year> <title> Brain-Computer Interface ANew Communication Device for Handicapped Persons. </title> <editor> In Zagler, W. (Ed.), </editor> <booktitle> Computer for Handicapped Persons: Proceedings of the Third International Conference (pp. </booktitle> <pages> 409-415). </pages>
Reference: <author> Provost, F.J., & Buchanan, B.G. </author> <year> (1995). </year> <title> Inductive Policy: The Pragmatics of Bias Selection. </title> <booktitle> Machine Learning, </booktitle> <month> 20(1/2), </month> <pages> 35-62. </pages>
Reference: <author> Provost, F.J., & Fawcett, T. </author> <year> (1997). </year> <title> Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions. </title> <booktitle> Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 43-48). </pages>
Reference-contexts: KUBAT, R. HOLTE, AND S. MATWIN examples; the majority class thus comprises almost 96% of the data. Highly imbalanced training sets occur in applications where the classifier is to detect a rare but important event, such as fraudulent telephone calls <ref> (Fawcett & Provost, 1997) </ref>, unreliable telecommunications customers (Ezawa, Singh & Norton, 1996), failures or delays in a manufacturing process (Riddle, Segal & Etzioni, 1994), rare diagnoses such as the thyroid diseases in the UCI repository (Murphy & Aha, 1994), or carcinogenicity of chemical compounds (Lee, Buchanan & Aronis, this issue).
Reference: <author> Quinlan, J.R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The first, discussed above, is that when working with imbalanced datasets it is desirable to use a performance measure other than accuracy. The second facet, shown by Kubat et al. (1997) and by Kubat and Matwin (1997), is that learning systems designed to optimize accuracy, such as C4.5 <ref> (Quinlan, 1993) </ref> and the 1-nearest-neighbor rule (1-NN), can behave poorly if the training set is highly imbalanced. The induced classifiers tend to be highly accurate on negative examples but usually misclassify many of the positives. This will be demonstrated in the experimental part of this paper.
Reference: <author> Riddle, P., Segal, R., & Etzioni, O. </author> <year> (1994). </year> <title> Representation design and brute-force induction in a Boeing manufacturing domain. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 8, </volume> <pages> 125-147. </pages>
Reference-contexts: Highly imbalanced training sets occur in applications where the classifier is to detect a rare but important event, such as fraudulent telephone calls (Fawcett & Provost, 1997), unreliable telecommunications customers (Ezawa, Singh & Norton, 1996), failures or delays in a manufacturing process <ref> (Riddle, Segal & Etzioni, 1994) </ref>, rare diagnoses such as the thyroid diseases in the UCI repository (Murphy & Aha, 1994), or carcinogenicity of chemical compounds (Lee, Buchanan & Aronis, this issue). Extremely imbalanced classes also arise in information retrieval and filtering tasks. <p> Extreme examples of this are algorithms that learn from positive examples only. A less extreme approach is to learn from positive and negative examples but to learn only rules that predict the positive class, as is done by BRUTE <ref> (Riddle et al., 1994) </ref>. By measuring performance only of the positive predicting rules BRUTE is not influenced by the invariably high accuracy on the negative examples that are not covered by the positive predicting rules.
Reference: <author> Rieger, A. </author> <year> (1995). </year> <title> Data Preparation for Inductive Learning in Robotics. </title> <booktitle> Proceedings of the IJCAI-95 workshop on Data Engineering for Inductive Learning (pp. </booktitle> <pages> 70-78). </pages>
Reference: <author> Saitta, L., Giordana, A., & Neri, F. </author> <year> (1995). </year> <title> What Is the "Real World"?. Working Notes for Applying Machine Learning in Practice: </title> <booktitle> A Workshop at the Twelfth International Conference on Machine Learning, Technical Report AIC-95-023 (pp. </booktitle> <pages> 34-40), </pages> <institution> NRL, Navy Center for Applied Research in AI, </institution> <address> Washington, DC. </address>
Reference: <author> Shapiro, A.D. </author> <year> (1987). </year> <title> Structured Induction in Expert Systems. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Practitioners always emphasize the importance of having good features, but there are few guidelines on how to acquire them. Constructive induction techniques can be applied when there is sufficient data that overtuning will not occur. An alternative to purely automatic techniques are elicitation techniques such as structured induction <ref> (Shapiro, 1987) </ref>. More generally, one can elicit domain knowledge, as Solberg and Volden (1997) have done, and use a learning algorithm guided by a weak domain theory as done by Clark and Matwin (1993).
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 293-301), </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Solberg, A.H.S., & Solberg, R. </author> <year> (1996). </year> <title> A Large-Scale Evaluation of Features for Automatic Detection of Oil Spills in ERS SAR Images. </title> <booktitle> IEEE Symp. Geosc. Rem. </booktitle> <pages> Sens (IGARSS) (pp. 1484-1486). </pages>
Reference: <author> Solberg, A.H.S., & Volden, E. </author> <year> (1997). </year> <title> Incorporation of Prior Knowledge in Automatic Classification of Oil Spills in ERS SAR Images. </title> <booktitle> IEEE Symp. Geosc. Rem. </booktitle> <pages> Sens (IGARSS) (pp. 157-159). </pages>
Reference-contexts: The system described by Solberg and Solberg (1996) uses learning to produce a classifier in the form of a decision tree. As this system is similar to ours in many ways, it will be reviewed in detail in the discussion of our results. TSS's most recent automatic classification system <ref> (Solberg & Volden, 1997) </ref> is more knowledge intensive. The classifier is a statistical system in which the prior probability of a region being an oil spill is computed using a domain theory that relates features of the region to the prior.
Reference: <author> Swets, J.A. </author> <year> (1988). </year> <title> Measuring the Accuracy of Diagnostic Systems. </title> <journal> Science, </journal> <volume> 240, </volume> <pages> 1285-1293. </pages>
Reference-contexts: Informally, we want to present to the user as many spills as possible provided that the total number of false alarms is not too large. Curves used to visualize the tradeoff between these two requirements are called ROC curves <ref> (Swets, 1988) </ref>. classified positive examples ( d c+d ) on the y-axis and the false positive rate ( b a+b ) on the x-axis.
Reference: <author> Tomek, I. </author> <year> (1976). </year> <title> Two Modifications of CNN. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-6, </volume> <pages> 769-772. </pages> <note> DETECTION OF OIL SPILLS 23 Turney, </note> <author> P. </author> <year> (1995). </year> <title> Data Engineering for the Analysis of Semiconductor Manufacturing Data. </title> <booktitle> Proceedings of the IJCAI-95 workshop on Data Engineering for Inductive Learning (pp. </booktitle> <pages> 50-59). </pages>
Reference: <author> Turney, P. </author> <year> (1993). </year> <title> Exploiting Context when Learning to Classify. </title> <booktitle> Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 402-407), </pages> <publisher> Springer-Verlag. </publisher> <editor> van Rijsbergen, C.J. </editor> <year> (1979). </year> <note> Information Retrieval (second edition), Butterworths. </note>
Reference: <author> Venables, W.N., & Ripley, B.D. </author> <year> (1994). </year> <title> Modern Applied Statistics with S-Plus. </title> <publisher> Springer-Verlag. </publisher> <editor> von Winterfeldt, D., & Edwards, W. </editor> <year> (1986). </year> <title> Decision Analysis and Behavioral Research. </title> <publisher> Cam-bridge University Press. </publisher>
Reference-contexts: KUBAT, R. HOLTE, AND S. MATWIN 8. Comparison with Solberg and Solberg (1996) As mentioned in the introduction, Solberg and Solberg (1996) use machine learning to classify oil spills. Their classifier is represented as a decision tree and is learned from training data using S-plus <ref> (Venables & Ripley, 1994) </ref>. To cope with the imbalanced classes Solberg and Solberg (1996) sample, with replacement, 100 examples from each class (four oil spill classes and a nonspill class).
Reference: <author> Widmer, G., & Kubat, M. </author> <year> (1996). </year> <title> Learning in the Presence of Concept Drift and Hidden Contexts. </title> <journal> Machine Learning, </journal> <volume> 23, </volume> <pages> 69-101. </pages>
Reference-contexts: Learning from batched examples is related to the issues of learning in the presence of context, as the batches often represent the unknown context in which the training examples were collected. Learning in context has only recently been recognized as an important problem re-occurring in applications of machine learning <ref> (Kubat & Widmer, 1996) </ref>. Various tradeoffs arose in our project which certainly warrant scientific study. In formulating a problem, one must choose the granularity of the examples (images, regions, or pixels in our application) and the number of classes. Different choices usually lead to different results.
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting Typical Instances in Instance-Based Learning. </title> <booktitle> Proceedings of the Ninth International Machine Learning Workshop (pp. </booktitle> <pages> 470-479), </pages> <note> Morgan Kaufmann. Received Date Accepted Date Final Manuscript Date </note>
References-found: 54

