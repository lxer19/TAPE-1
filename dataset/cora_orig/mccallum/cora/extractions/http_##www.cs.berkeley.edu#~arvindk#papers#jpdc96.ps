URL: http://www.cs.berkeley.edu/~arvindk/papers/jpdc96.ps
Refering-URL: http://www.cs.berkeley.edu/~arvindk/
Root-URL: 
Title: Analyses and Optimizations for Shared Address Space Programs  
Author: Arvind Krishnamurthy and Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: We present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, called cycle analysis, is based on work by Shasha and Snir and checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from synchronization analysis, which handles post-wait synchronization, barriers, and locks. We also make the analysis efficient by exploiting the common code image property of SPMD programs. We demonstrate the use of this analysis by optimizing remote access on distributed memory machines by automatically transforming programs written in a conventional shared memory style into a Split-C program, which has primitives for non-blocking memory operations and one-way communication. The optimizations include message pipelining, to allow multiple outstanding remote memory operations, conversion of two-way to one-way communication, and elimination of communication through data re-use. The performance improvements are as high as 20-35% for programs running on a CM-5 multiprocessor using the Split-C language as a global address layer. Even larger benefits can be expected on machines with higher communication latency relative to processor speed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak Ordering-A New Definition. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Our algorithm for analyzing post-wait synchronization is similar in spirit; however, we can also exploit mutual-exclusion information on accesses. Also related to our work is the research that proposes weaker memory models <ref> [1, 8] </ref>. Those approaches change the programmer's model by giving programming conventions under which sequential consistency is ensured. Our work shifts this burden from the programmer to the compiler.
Reference: [2] <author> B. S. Ang, Arvind, and D. Chiou. </author> <title> StarT the Next Generation: Integrating Global Caches and Dataflow Architecture. </title> <booktitle> In ISCA 1992 Dataflow Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: As shown in table 1, a remote reference on such a machine has a long latency [3][23][13]. However, most of this latency can be overlapped with local computation or with the initiation of more communication, especially on machines like the J-Machine [18] and *T <ref> [2] </ref>, with their low overheads for communication startup. CM-5 T3D DASH Remote Access 400 85 110 Local Access (Cache miss) 30 23 26 Table 1: Access latencies for local and remote memory modules expressed in terms of machine cycles.
Reference: [3] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year> <month> 17 </month>
Reference: [4] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution Time Support for Adaptive Scientific Algorithms on Distributed Memory Multiprocessors. </title> <journal> Concurrenty: Practice and Experience, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication <ref> [4] </ref>, and these optimizations have also been studied in the context of parallelizing compilers [19]. However, as discussed earlier, compiling data parallel programs is fundamentally different from compiling explicitly parallel programs.
Reference: [5] <author> D. Callahan and J. Subhlok. </author> <title> Static Analysis of Low-level Synchronization. </title> <booktitle> In ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <month> May </month> <year> 1988. </year>
Reference: [6] <author> W. W. Carlson and J. M. Draper. </author> <title> Distributed Data Access in AC. </title> <booktitle> In ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Neither of these included implementations and the algorithms as presented were not practical because synchronization behavior is ignored. Related to our work is the AC compiler <ref> [6] </ref>, which uses the non-blocking memory operations on the Cray T3D. However, since the AC compiler does not employ cycle detection, the compiled code could potentially generate executions that are not sequentially consistent.
Reference: [7] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Three important optimizations for these multiprocessors are overlapping communication, eliminating round-trip message traffic, and avoiding communication altogether. The first optimization, message pipelining, changes remote read and write operations into their split-phase analogs, get and put <ref> [7] </ref>. In a split-phase operation, the initiation of an access is separated from its completion. The operation to force completion of outstanding split-phase operations comes in many forms, the simplest of which (called sync or fence) blocks until all outstanding accesses are complete. <p> Related work is surveyed in section 9 and conclusions drawn in section 10. 2 Programming Language Our analyses are designed for explicitly parallel shared memory programs. We have implemented them in a source-to-source transformer for a subset of Split-C <ref> [7] </ref>. Split-C is an explicitly parallel SPMD language for programming distributed memory machines using a global address space abstraction. The parallel threads interact through reads and writes on a shared address space that contains distributed arrays and shared objects accessible through global pointers. <p> Hence, the program has performance characteristics that are different from those of the pointer-based versions described in <ref> [7] </ref>. 15 optimized versions scale better with processors. Health: This benchmark is from the Presto application suite. Health simulates the Colombian health service system, which has an hierarchical service-dispensing system. Exclusive access to shared data structures is guaranteed by the use of locks.
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Our algorithm for analyzing post-wait synchronization is similar in spirit; however, we can also exploit mutual-exclusion information on accesses. Also related to our work is the research that proposes weaker memory models <ref> [1, 8] </ref>. Those approaches change the programmer's model by giving programming conventions under which sequential consistency is ensured. Our work shifts this burden from the programmer to the compiler.
Reference: [9] <author> D. Grunwald and H. Srinivasan. </author> <title> Data flow equations for Explicitly Parallel Programs. </title> <booktitle> In ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> June </month> <year> 1993. </year>
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler Optimziations for Fortran D on MIMD Distributed-Memory Machines. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Our analysis could also be used for compiling weak memory programs since it can determine when code motion is legal, which is critical for generating prefetch instructions. 16 Compilers and runtime systems for data parallel languages like HPF and Fortran-D <ref> [10] </ref> implement message pipelining optimizations and data re-use. The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication [4], and these optimizations have also been studied in the context of parallelizing compilers [19].
Reference: [11] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations. </title> <booktitle> In ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Rather than adding sophisticated analysis to line up barriers <ref> [11] </ref>, we use a simple runtime solution that works well for many real programs. We add a run-time check to each barrier to determine whether these are the ones lined up during compilation. <p> This approach to analyzing barriers also allows us to overcome separate compilation issues for many real programs. If, however, adequate information is available at compile-time through analysis similar to the algorithm described in <ref> [11] </ref>, the cycle detection analysis is applied on all the code fragments that could be executing concurrently on different processors. 5.3 Lock Based Synchronization We can extend our synchronization analysis to locks, even though there are no strict precedence relations implied by the use of locks.
Reference: [12] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Intuitively, the parallel programmer relies on the notion of sequential consistency: the parallel execution must behave as if it were an interleaving of the sequences of memory operations from each of the processors <ref> [12] </ref>. If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent [16]. To guarantee sequential consistency under reordering transformations, a new type of analysis called cycle detection is required [20]. An example to illustrate sequential consistency is shown in Figure 1. <p> An execution E is sequentially consistent if there exists a total order S of the operations in E, i.e., E S, such that S is a correct sequential execution where the reads must return the value of the most recent preceding write <ref> [12] </ref>. For example, in Figure 2, if the read to Y returns a new value written by E 1 , then the read of X must also return the value written by E 1 .
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference: [14] <author> S. Luna. </author> <title> Implementing an Efficient Global Memory Portability Layer on Distributed Memory Multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The Cray T3D, the Stanford DASH, and the Wisconsin Wind Tunnel are examples of machines that have support for prefetching read and non-blocking write operations. Instead of generating code for a particular machine, the compiler produces Split-C code, and the Split-C compiler <ref> [14] </ref> is responsible for mapping the Split-C operations to the primitive operations supported by the target machine. In this section we describe the transformations introduced by our source level transformer.
Reference: [15] <author> N. K. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwell's Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Ocean studies the role of eddy and boundary currents in large-scale ocean movements. The primary data structures are grids that are updated using stencil-like computations. Distinct phases of the program synchronize using barriers. EM3D: Em3d models the propagation of electromagnetic waves through objects in three dimensions <ref> [15] </ref> 3 The computation consists of a series of "leapfrog" integration steps: on alternate half time steps, changes in the electric field are calculated as a linear function of the neighboring magnetic field values and vice versa. The alternate half time steps are separated using barrier synchronization.
Reference: [16] <author> S. Midkiff and D. Padua. </author> <title> Issues in the Optimization of Parallel Programs. </title> <booktitle> In International Conference on Parallel Processing - Vol II, </booktitle> <year> 1990. </year>
Reference-contexts: If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent <ref> [16] </ref>. To guarantee sequential consistency under reordering transformations, a new type of analysis called cycle detection is required [20]. An example to illustrate sequential consistency is shown in Figure 1. <p> As a result, they either generate incorrect code or miss opportunities for optimizing communication and synchronization, and the quality of the scalar code is limited by the inability to move code around parallelism primitives. Midkiff and Padua <ref> [16] </ref> describe eleven instances where a uniprocessor compiler would generate either incorrect or inefficient code. In this paper, we present optimizations for multiprocessors with physically distributed memory and hardware or software support for a global address space. <p> In the more general control parallel setting, Midkiff and Padua <ref> [16] </ref> describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir [20], which was later extended by Midkiff et al [17] to handle array based accesses.
Reference: [17] <author> S. P. Midkiff, D. Padua, and R. G. Cytron. </author> <title> Compiling Programs with User Parallelism. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: Cycle detection was first described by Shasha and Snir [20] and later extended by Midkiff, Padua, and Cytron to handle array indices <ref> [17] </ref>. In this paper, we show that by restricting attention to Single Program Multiple Data (SPMD) programs, one can significantly reduce the complexity of cycle detection. We also improve the accuracy of cycle detection by making use of the synchronization information in the program. <p> Analysis for these programs is based on the pioneering work by Shasha and Snir [20], which was later extended by Midkiff et al <ref> [17] </ref> to handle array based accesses. Neither of these included implementations and the algorithms as presented were not practical because synchronization behavior is ignored. Related to our work is the AC compiler [6], which uses the non-blocking memory operations on the Cray T3D.
Reference: [18] <author> M. D. Noakes, D. A. Wallach, and W. J. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In 20th International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: As shown in table 1, a remote reference on such a machine has a long latency [3][23][13]. However, most of this latency can be overlapped with local computation or with the initiation of more communication, especially on machines like the J-Machine <ref> [18] </ref> and *T [2], with their low overheads for communication startup. CM-5 T3D DASH Remote Access 400 85 110 Local Access (Cache miss) 30 23 26 Table 1: Access latencies for local and remote memory modules expressed in terms of machine cycles.
Reference: [19] <author> A. Rogers and K. Pingali. </author> <title> Compiling for distributed memory architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> March </month> <year> 1994. </year>
Reference-contexts: The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to optimize communication [4], and these optimizations have also been studied in the context of parallelizing compilers <ref> [19] </ref>. However, as discussed earlier, compiling data parallel programs is fundamentally different from compiling explicitly parallel programs.
Reference: [20] <author> D. Shasha and M. Snir. </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2), </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: If only the local dependencies within a processor are observed, the program execution might not be sequentially consistent [16]. To guarantee sequential consistency under reordering transformations, a new type of analysis called cycle detection is required <ref> [20] </ref>. An example to illustrate sequential consistency is shown in Figure 1. The program is indeterminate in that the read of Flag may return either 0 or 1, and if it is 0, then the read to Data may return either 0 or 1. <p> A final optimization, caching remote values, eliminates remote accesses by either re-using values of previous accesses or updating a remote 2 value locally multiple times before issuing a write operation on the final value. Cycle detection was first described by Shasha and Snir <ref> [20] </ref> and later extended by Midkiff, Padua, and Cytron to handle array indices [17]. In this paper, we show that by restricting attention to Single Program Multiple Data (SPMD) programs, one can significantly reduce the complexity of cycle detection. <p> In order to extend the system contract for programs with weak memory accesses, rather than relying on a particular instruction set with non-blocking memory operations and synchronizing accesses, we use a more general framework proposed by Shasha and Snir <ref> [20] </ref>. A delay set D specifies some pairs of memory accesses as being ordered, which says that the second operation must be delayed until the first one is complete. <p> Definition 3 D S&S = f [a i ; a j ] 2 P j [a i ; a j ] has a back-path in P [ Cg. Theorem 1 <ref> [20] </ref> D S&S is sufficient. The above definition of D S&S suggests an obvious algorithm for generating correct code. <p> is not as strong as one would like, because it ignores the existence of control structures and synchronization constructs that prevent certain access patterns. 4.2 Cycle Detection for MIMD Programs is NP Hard Although Shasha and Snir do not specify the details of an algorithm for cycle detection, they claim <ref> [20] </ref> there is a polynomial time algorithm for detecting cycles in a program that "consists of a fixed number of serial program segments." In practice, one does not typically compile a program for a fixed number of processors: either the language contains constructs for dynamically creating parallel threads, or there is <p> In the more general control parallel setting, Midkiff and Padua [16] describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir <ref> [20] </ref>, which was later extended by Midkiff et al [17] to handle array based accesses. Neither of these included implementations and the algorithms as presented were not practical because synchronization behavior is ignored.
Reference: [21] <author> J. P. Singh, W. D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: A brief description of the applications is given below: Ocean: This benchmark is from the Splash benchmark suite <ref> [21] </ref>. Ocean studies the role of eddy and boundary currents in large-scale ocean movements. The primary data structures are grids that are updated using stencil-like computations. Distinct phases of the program synchronize using barriers.
Reference: [22] <institution> The SPARC Architecture Manual: </institution> <note> Version 8. Sparc International, </note> <institution> Inc., </institution> <year> 1992. </year>
Reference-contexts: Most processors have write buffers, which allow read operations to overtake preceding write operations. In fact, on the SuperSparcs <ref> [22] </ref> the write-buffer itself is not guaranteed to be FIFO.
Reference: [23] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
References-found: 23

