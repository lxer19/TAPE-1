URL: http://www.cs.rutgers.edu/~ellman/ml93.ps
Refering-URL: http://www.cs.rutgers.edu/~ellman/ellman-publications.html
Root-URL: 
Email: ellman@cs.rutgers.edu  
Title: Synthesis of Abstraction Hierarchies for Constraint Satisfaction by Clustering Approximately Equivalent Objects  
Author: Thomas Ellman 
Address: New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Hill Center for Mathematical Sciences Rutgers University,  
Abstract: Abstraction techniques are important for solving constraint satisfaction problems with global constraints and low solution density. In the presence of global constraints, backtracking search is unable to prune partial solutions. It therefore operates like pure generate-and-test. Abstraction improves on generate-and-test by enabling entire subsets of the solution space to be pruned early in a backtracking search process. Unfortunately, a suitable abstraction space may not be included in the problem description provided to a problem solving system. The benefits of abstraction will not be available unless the system can automatically construct an abstraction space. This paper describes how abstraction spaces can be generated by clustering the objects appearing in a problem into classes that contain approximately equivalent objects. It presents a program synthesis algorithm for automatically building abstraction spaces and hierarchic problem solvers that exploit approximate equivalence of objects. The fully implemented synthesis algorithm operates by analyzing declarative descriptions of classes of constraint satisfaction problems. The paper presents data from experimental tests of the synthesis algorithm and the resulting problem solvers on the NP-hard Partition and Minimum Flow Cut problems. 
Abstract-found: 1
Intro-found: 1
Reference: [ Dechter and Pearl, 1987 ] <author> R. Dechter and J. Pearl. </author> <title> Network-based heuristics for constraint-satisfaction problems. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 1-38, </pages> <year> 1987. </year>
Reference-contexts: Furthermore, approximate equivalence is more general, because it depends only on the algebraic form of the problem and not on semantic notions such as "resources". Approximations and abstractions have been used to construct heuristic evaluation functions in the context of constraint satisfaction <ref> [ Dechter and Pearl, 1987 ] </ref> and state space search [ Prieditis, 1991 ] . 8 Summary The research reported in this paper contributes to the field of Machine Learning in two distinct ways.
Reference: [ Ellman, 1993 ] <author> T. </author> <title> Ellman. Abstraction via approxi mate symmetry. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Nevertheless, the abstract solution does not specify precisely which elements in ^e actually receive T rue values. This type of abstraction is discussed more fully in <ref> [ Ellman, 1993 ] </ref> . 3 Parameterized CSPs Classes of constraint satisfaction problems are represented using a notation called "parameterized constraint satisfaction problems" (PCSPs). A PCSP includes a signature S and a goal-function G.
Reference: [ Garey and Johnson, 1979 ] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: 1 Introduction Abstraction techniques are important for solving constraint satisfaction problems (CSPs) with global constraints and low solution density. Examples of such problems include the Partition problem and the Minimum Flow Cut problem, both of which are NP-hard <ref> [ Garey and Johnson, 1979 ] </ref> . (See Figure 1). In the presence of global constraints, backtracking search is unable to prune partial solutions [ Nadel, 1988 ] . It therefore operates like pure generate-and-test.
Reference: [ Giunchiglia and Walsh, 1992 ] <author> F. Giunchiglia and T. Walsh. </author> <title> A theory of abstraction. </title> <journal> Artificial Intelligence, </journal> <volume> 57 </volume> <pages> 323-389, </pages> <year> 1992. </year>
Reference-contexts: hierarchy might eliminate the need for a means of automatically choosing an abstraction space of the best possible size, since many sizes would be available at once. 7 Related Work Abstraction techniques have been studied previously in the context of planning [ Knoblock et al., 1991 ] and theorem proving <ref> [ Giunchiglia and Walsh, 1992 ] </ref> . The research presented here is similar in spirit; however, it differs by focusing on constraint satisfaction problems, rather than planning or theorem proving. A program called "HiT" for automatically constructing abstraction spaces for CSPs is presented in [ Mohan, 1991 ] .
Reference: [ Kaufman and Rousseeuw, 1990 ] <author> L. Kaufman and P. Rousseeuw. </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: In each case, the bottom-up method at 3 The details of these clustering algorithms are not central to the claims of this paper. More sophisticated clustering algorithms are presented in <ref> [ Kaufman and Rousseeuw, 1990 ] </ref> . Random Clustering: Form a list of singleton sets. Put the sets in random order. Repeatedly merge the first two sets on the list and put the merged set at the end of the list. <p> The smaller clusters of vertices preserve more of the graph structure and lead to more effective pruning of abstract states. 6 Future Work Future work is planned to determine whether more sophisticated clustering algorithms, such as those discussed in <ref> [ Kaufman and Rousseeuw, 1990 ] </ref> , might yield more compact clusters and more effective pruning at the abstract level. More sophisticated clustering algorithms may also be necessary to handle more complex constraint satisfaction problems. In the test problems studied so far, only one function is the "unknown function".
Reference: [ Knoblock et al., 1991 ] <author> C. Knoblock, J. Tennenberg, and Q. Yang. </author> <title> Characterizing abstraction hierarchies for planning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <year> 1991. </year> <title> Bottom-Up Clustering Optimal Size (Solid) v. Hierarchic Random Clustering Optimal Size (Dotted) </title>
Reference-contexts: Use of such a multi-level hierarchy might eliminate the need for a means of automatically choosing an abstraction space of the best possible size, since many sizes would be available at once. 7 Related Work Abstraction techniques have been studied previously in the context of planning <ref> [ Knoblock et al., 1991 ] </ref> and theorem proving [ Giunchiglia and Walsh, 1992 ] . The research presented here is similar in spirit; however, it differs by focusing on constraint satisfaction problems, rather than planning or theorem proving.
Reference: [ Korf, 1987 ] <author> R. E. Korf. </author> <title> Planning as search: A quan titative approach. </title> <booktitle> Artificial Intelligence, </booktitle> <address> 33(1):65 -88, </address> <year> 1987. </year>
Reference-contexts: the default size abstraction space are shown in Figure 9, for the Partition problem class, and Figure 10 for the Minimum 4 The choice of the default abstraction space size is based on a rough analogy with Korf's result on the optimal size of abstraction spaces for state space search <ref> [ Korf, 1987 ] </ref> . Hierarchic Bottom-Up Clustering Optimal Size (Solid) Flow Cut problem class.
Reference: [ Lowry and Linden, 1992 ] <author> M. Lowry and T. Linden. </author> <title> Generation and exploitation of aggregation abstractions for scheduling and resource allocation. </title> <booktitle> Working Notes of the AAAI Workshop on Approximation and Abstraction of Computational Theories, </booktitle> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: Abstractions Bottom-Up Clustering Default Size (Solid) v. Hierarchic Random Clustering Default Size (Dotted) based on quotas have been studied in the context of resource allocation problems <ref> [ Lowry and Linden, 1992 ] </ref> . Approximate equivalence provides a rational reconstruction of the quota concept. Furthermore, approximate equivalence is more general, because it depends only on the algebraic form of the problem and not on semantic notions such as "resources".
Reference: [ Mackworth et al., 1985 ] <author> A. Mackworth, J. Mulder, and W. Havens. </author> <title> Hierarchical arc consistency: Exploiting structured domains in constraint satisfaction problems. </title> <booktitle> Computational Intelligence, </booktitle> <address> 1:118 -126, </address> <year> 1985. </year>
Reference-contexts: HiT differs from the method developed here by partitioning the range, rather than the domain, in function finding problems. HiT may therefore be seen as a complementary method of building abstraction spaces. Methods of attacking hierarchical CSPs are discussed in <ref> [ Mackworth et al., 1985 ] </ref> ; however, the methods are aimed at exploiting an existing hierarchy rather than automatically constructing a new one. Abstractions Bottom-Up Clustering Default Size (Solid) v.
Reference: [ Mohan, 1991 ] <author> S. Mohan. </author> <title> Constructing hierarchical solvers for functional constraint satisfaction problems. </title> <booktitle> Working notes of the AAAI Spring Symposium on Constraint-Based Reasoning, </booktitle> <address> Stanford, CA, </address> <year> 1991. </year>
Reference-contexts: The research presented here is similar in spirit; however, it differs by focusing on constraint satisfaction problems, rather than planning or theorem proving. A program called "HiT" for automatically constructing abstraction spaces for CSPs is presented in <ref> [ Mohan, 1991 ] </ref> . HiT differs from the method developed here by partitioning the range, rather than the domain, in function finding problems. HiT may therefore be seen as a complementary method of building abstraction spaces.
Reference: [ Nadel, 1988 ] <author> B. </author> <title> Nadel. Tree search and arc con sistency in constraint satisfaction algorithms. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 287 - 342. </pages> <publisher> Springer Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: Examples of such problems include the Partition problem and the Minimum Flow Cut problem, both of which are NP-hard [ Garey and Johnson, 1979 ] . (See Figure 1). In the presence of global constraints, backtracking search is unable to prune partial solutions <ref> [ Nadel, 1988 ] </ref> . It therefore operates like pure generate-and-test. When overall solution density is low, this approach is not effective, except when applied to small problems.
Reference: [ Prieditis, 1991 ] <author> A. </author> <title> Prieditis. Machine discovery of ef fective admissable heuristics. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> Sidney, Australia, </address> <year> 1991. </year>
Reference-contexts: Approximations and abstractions have been used to construct heuristic evaluation functions in the context of constraint satisfaction [ Dechter and Pearl, 1987 ] and state space search <ref> [ Prieditis, 1991 ] </ref> . 8 Summary The research reported in this paper contributes to the field of Machine Learning in two distinct ways. To begin with, it has developed an analytic learning technique for improving the performance of problem solvers on constraint satisfaction problems.
References-found: 12

