URL: ftp://ftp.speech.sri.com/pub/people/francois/wcnn94-1.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Email: @J  
Title: Network Reciprocity: A Simple Approach to Derive Gradient Algorithms for Arbitrary Neural Network Structures. Adaptation
Author: Eric A. Wan and Fran~coise Beaufays @w ij (k) a i (k); () 
Keyword: W (k) @W (k)  
Address: P.O.box 91000, Portland, OR 97291.  Stanford, CA  
Affiliation: Dept. of Electrical Engineering and Applied Physics, Oregon Graduate Institute of Science and Technology,  Dept. of Electrical Engineering, Stanford University,  
Date: June 4-9, 1994  
Note: Proc. WCNN'94 San Diego,  Network  @J  94305-4055. This work was sponsored by EPRI under contract RP8010-13 and NSF under grant NSF IRI 91-12531.  is treated in [5,6].  
Abstract: Deriving backpropagation algorithms for time-dependent neural network structures typically requires numerous chain rule expansions, diligent bookkeeping, and careful manipulation of terms. In this paper, we show how to use the principle of Network Reciprocity to derive such algorithms via a set of block diagram manipulation rules. Examples are provided that illustrate the simplicity of the approach. Algorithms are derived for a variety of structures, including feedforward and feedback systems. In certain problems (e.g., time series prediction, system identification), a desired output is specified at each time step; in others (e.g., terminal control), the desired output is defined only at final time k = K. Therefore, we define the error vector e(k) as the difference between the desired and the actual output vectors when a desired output is available, and as zero otherwise. According to gradient descent, the contribution to the weight update at each time step is where controls the learning rate. Note we evaluate @J=@W (k) rather than the instantaneous gradient @(e T (k)e(k))=@W (k). This is essential for the desired Network Reciprocity result. At the architectural level, a variable weight z w ij may be isolated between two points in a network with corresponding signals a i (k) and a j (k) (i.e., a j (k) = w ij a i (k)). Using the chain rule, we get 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> W.T. Miller III, R.S. Sutton, P.J. Werbos, </author> <title> editors. Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: These are precisely the equations describing backpropagation-through-time, which have been derived in the past using either ordered derivatives [8] or Euler-Lagrange techniques [2]. Network Reciprocity is by far the simplest and most direct approach. Other examples Backpropagation-through-time has been modified for a variety of neural control problems <ref> [1] </ref>. Suppose the state-space model of a dynamic system is given and a neural controller is to be built to drive the plant.
Reference: 2. <author> E. Plumer. </author> <title> Optimal Terminal Control Using Feedforward Neural Networks. </title> <type> Ph.D. dissertation. </type> <institution> Stanford University, </institution> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: These are precisely the equations describing backpropagation-through-time, which have been derived in the past using either ordered derivatives [8] or Euler-Lagrange techniques <ref> [2] </ref>. Network Reciprocity is by far the simplest and most direct approach. Other examples Backpropagation-through-time has been modified for a variety of neural control problems [1]. Suppose the state-space model of a dynamic system is given and a neural controller is to be built to drive the plant.
Reference: 3. <editor> D.E. Rumelhart, J.L. McClelland, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <volume> Vol. 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The exact equations are "read-out" directly from the reciprocal network. A formal proof that this always provides the correct derivation may be found in [5,6]. Examples Backpropagation We start be rederiving standard backpropagation <ref> [3] </ref> using the principles of Network Reciprocity. For consistency with traditional notation, we have labeled the summing junction signal s l i rather than a i , and added superscripts to denote the layer. In addition, since the multilayer networks are static structures, we omit the time index k.
Reference: 4. <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing. </journal> <volume> Vol. 37(3), </volume> <month> March </month> <year> 1989, </year> <pages> pp. 328-339. </pages>
Reference-contexts: Related to cascaded networks are structures that distribute time delays through the entire network. Such architectures include FIR neural networks [6,7] (where the synaptic connections of the traditional multilayer neural network are replaced by FIR (Finite Impulse Response) filters), time-delay neural networks <ref> [4] </ref> (where time-delays are introduced between the hidden layers of a feedforward neural network), IIR (Infinite Impulse Response) structures, and lattice filters. For such networks, direct chain rule expansions or equivalent unfolded structures are extremely complicated.
Reference: 5. <author> E. Wan and F. Beaufays. </author> <title> Network Reciprocity: A Unified Approach to Derive Gradient Algorithms for Arbitrary Neural Network Structures. </title> <note> Submitted to Neural Computation. </note>
Reference: 6. <author> E. Wan. </author> <title> Finite Impulse Response Neural Networks with Applications in Time Series Prediction. </title> <type> Ph.D. dissertation. </type> <institution> Stanford University, </institution> <month> Nov. </month> <year> 1993. </year>
Reference: 7. <author> E. Wan. </author> <title> Time series prediction using a connectionist network with internal delay lines. </title> <editor> In A. Weigend and N. Gershenfeld, editors, </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference: 8. <author> P. Werbos. </author> <title> Generalization of backpropagation with application to a recurrent gas market model. </title> <booktitle> Neural Networks. </booktitle> <volume> Vol. 1, </volume> <year> 1988, </year> <pages> pp. 339-356. </pages>
Reference-contexts: These are precisely the equations describing backpropagation-through-time, which have been derived in the past using either ordered derivatives <ref> [8] </ref> or Euler-Lagrange techniques [2]. Network Reciprocity is by far the simplest and most direct approach. Other examples Backpropagation-through-time has been modified for a variety of neural control problems [1].
Reference: 9. <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation. </journal> <volume> Vol. 1(2), </volume> <year> 1989, </year> <pages> pp. 270-280. </pages>
Reference-contexts: N is a multilayer neural network. If N has only one layer of neurons, every neuron output has a feedback connection to the input of every other neuron and the structure is referred to as a fully recurrent network <ref> [9] </ref>. Typically, only a select set of the outputs have an actual desired response. The remaining outputs have no desired response (error equals zero) and are used for internal computation. Calculating the gradients for such a structure can be extremely complicated.
References-found: 9

