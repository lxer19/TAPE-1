URL: http://www.cs.berkeley.edu/~johnw/papers/jvsp93.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~johnw/publications.html
Root-URL: 
Title: Simulation of Reduced Precision Arithmetic for Digital Neural Networks Using the RAP Machine  
Author: Krste Asanovic, Nelson Morgan, and John Wawrzynek 
Note: Draft Do not distribute.  
Date: December 26, 1991  
Abstract: This paper describes some of our recent work in the development of computer architectures for efficient execution of artificial neural network algorithms. Our earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme. We have used the RAP to simulate variable precision arithmetic and guide us in the design of higher performance neurocomputers based on custom VLSI. The RAP system played a critical role in this study, enabling us to experiment with much larger networks than would otherwise be possible. Our study shows that back-propagation training algorithms only require moderate precision. Specifically, 16b weight values and 8b output values are sufficient to achieve training and classification results comparable to 32b floating point. Although these results were gathered for frame classification in continuous speech, we expect that they will extend to many other connectionist calculations. We have used these results as part of the design of a programmable single chip microprocessor, SPERT. The reduced precision arithmetic permits the use of multiple units per processor. Also, reduced precision operands make more efficient use of valuable processor-memory bandwidth. For our moderate-precision fixed-point arithmetic applications, SPERT represents more than an order of magnitude reduction in cost over systems based on DSP chips. 
Abstract-found: 1
Intro-found: 1
Reference: [ABK+91] <author> Krste Asanovic, James Beck, Brian E.D. Kingsbury, Phil Kohn, Nelson Morgan, and John Wawrzynek. SPERT: </author> <title> A VLIW/SIMD Microprocessor for Artificial Neural Network Computations. </title> <type> Technical Report TR-91-072, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year>
Reference-contexts: A consistent set of software abstractions will assist in porting code between these platforms. This section provides an overview of the SPERT project. A more complete description can be found in <ref> [ABK+91] </ref>. 5.1 SPERT Architecture Based on the reduced precision experiments, our requirements for the arithmetic units in SPERT were that they provide fast 16bfi8b fixed-point multiplication, with efficient handling of larger (24-32b) intermediary results.
Reference: [ASPF92] <author> Krste Asanovic, Klaus Erik Schauser, David A. Patterson, and Edward H. Frank. </author> <title> Evaluation of a Stall Cache: An Efficient Restricted On-Chip Instruction Cache. </title> <booktitle> In Proceedings 25th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1992. </year> <month> 30 </month>
Reference: [Ham90] <author> Dan Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference-contexts: For problems 27 requiring longer words, such as 16fi16 multiplications, the steady-state time penalty will be an extra cycle per multiplication, or a halfing of throughput. 5.4 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions <ref> [Ham90] </ref> and the MA-16 chip from Siemens [RBR + 91]. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer.
Reference: [Her90] <author> H. Hermansky. </author> <title> Perceptual Linear Predictive (PLP) Analysis of Speech. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 87(4), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Initial experiments indicate that this method compares favourably with conventional HMM speech recognition methods [MB90]. For some of our experiments, the network has 26 inputs. These are typically 12th order Perceptual Linear Prediction (PLP) coefficients <ref> [Her90] </ref> plus first order derivative terms for each speech frame. The input values are normalized to zero-mean and unity variance across the set of training data. For the simulations described here, these inputs are directly and fully connected to a layer of 256 hidden units.
Reference: [KAW + 91] <author> Brian E. D. Kingsbury, Krste Asanovic, John Wawrzynek, Bertrand Irissou, and Nelson Morgan. </author> <title> Recent Work in VLSI Elements for Digital Implementations of Artificial Neural Networks. </title> <type> Technical Report TR-91-074, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year>
Reference-contexts: The multiplier design has been simulated, completed and is out for fabrication. The custom cells we are developing are also being made available as Lager library cells. A full description of the VLSI cell library work is available in <ref> [KAW + 91] </ref>. 5.3 SPERT Performance Analysis During the architectural design process, a number of applications were considered and used to evaluate design alternatives. The primary envisaged application is backpropagation training and in this section we present detailed performance results for backpropagation training on SPERT.
Reference: [K91] <author> Phil Kohn. </author> <title> CLONES: Connectionist Layered Object-oriented NEtwork Simulator. </title> <type> Technical Report TR-91-073, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year>
Reference-contexts: These object classes divide the data and processing as evenly as possible among the available processing nodes, using the ring to redistribute data. The top level of RAP software is the Connectionist Layered Object-oriented NEtwork Simulator (CLONES) environment for constructing ANNs <ref> [K91] </ref>. The goals of the CLONES design are efficiency, flexibility and ease of use. Experimental researchers often generate either a proliferation of versions of the same basic program, or one giant program with a large number of options and many potential interactions and side-effects.
Reference: [MB90] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition using Multilayer Perceptrons with Hidden Markov models. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 413-416, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: The speech recognizer uses the ANN to generate emission probabilities for a hidden Markov model (HMM) speech recognizer. Initial experiments indicate that this method compares favourably with conventional HMM speech recognition methods <ref> [MB90] </ref>. For some of our experiments, the network has 26 inputs. These are typically 12th order Perceptual Linear Prediction (PLP) coefficients [Her90] plus first order derivative terms for each speech frame. The input values are normalized to zero-mean and unity variance across the set of training data. <p> The test set is used to check the performance of the training algorithm to avoid over-fitting of the network to the training set. There are two phases in the training scheme, which is a recent variant of the cross-validation learning approach used in <ref> [MB90] </ref>. Training starts by assigning some range of random values to all the weights, say r. An initial value for ff, the learning constant, is chosen and this value remains constant throughout the first training phase.
Reference: [MBKB92] <author> N. Morgan, J. Beck, P. Kohn, and J. Bilmes. </author> <title> Neurocomput-ing on the RAP In Digital Parallel Implementations of Neural Networks, </title> <editor> editors K. W. Przytula and V. K. Prasanna, </editor> <publisher> Prentice Hall, </publisher> <year> 1992 </year>
Reference-contexts: 1 Introduction Our research group has developed several computer architectures for efficient execution of artificial neural network algorithms. Our first system, the RAP [MBAB90] <ref> [MBKB92] </ref>, was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme. This system has been in serious use for over a year and has greatly accelerated our research into connectionist calculations, particularly speech recognition. <p> Furthermore, connectionist applications research (particularly speech recognition) in our group required improved computational resources for ANN training independent of any VLSI design effort. For these reasons, in 1989-1990 we designed and implemented a Ring Array Processor (RAP) for fast implementation of layered neural network algorithms [MBAB90], <ref> [MBKB92] </ref>. This system was implemented in roughly one year (from conception to working prototype), and has since been in serious use for our applications research|we estimate that our group has performed computations that would have taken a century on a SPARC2 workstation. <p> This and other issues of the RAP design are described in detail in <ref> [MBKB92] </ref>. The RAP runs in VME-based systems under control of a SPARC 4/300 series host. An extensive set of software tools has been implemented with an emphasis on improving the efficiency of layered artificial neural network algorithms.
Reference: [MBAB90] <author> N. Morgan, J. Beck, E. Allman, and J. Beer. </author> <title> RAP: A Ring Array Processor for Multilayer Perceptron applications. </title> <booktitle> In Proc. 31 IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 1005-1008, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Our research group has developed several computer architectures for efficient execution of artificial neural network algorithms. Our first system, the RAP <ref> [MBAB90] </ref> [MBKB92], was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme. This system has been in serious use for over a year and has greatly accelerated our research into connectionist calculations, particularly speech recognition. <p> Furthermore, connectionist applications research (particularly speech recognition) in our group required improved computational resources for ANN training independent of any VLSI design effort. For these reasons, in 1989-1990 we designed and implemented a Ring Array Processor (RAP) for fast implementation of layered neural network algorithms <ref> [MBAB90] </ref>, [MBKB92]. This system was implemented in roughly one year (from conception to working prototype), and has since been in serious use for our applications research|we estimate that our group has performed computations that would have taken a century on a SPARC2 workstation.
Reference: [MHB + 91] <author> N. Morgan, H. Hermansky, H. Bourlard, P. Kohn, and C. Woot-ers. </author> <title> Continuous speech recognition using PLP analysis with Multilayer Perceptrons. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 49-52, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference: [RBR + 91] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. </author> <title> Wesseling. Design of a 1st Generation Neurocomputer. In VLSI Design of Neural Networks. </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: longer words, such as 16fi16 multiplications, the steady-state time penalty will be an extra cycle per multiplication, or a halfing of throughput. 5.4 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions [Ham90] and the MA-16 chip from Siemens <ref> [RBR + 91] </ref>. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer. As with SPERT, processing elements are similar to general purpose DSPs with reduced precision multipliers.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning Internal Representations by Error Propagation. In Parallel Distributed Processing. </title> <journal> Exploration of the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The architecture is fully programmable, and executes a wide range of connectionist computations efficiently. Special emphasis is being placed on support for variants of the commonly used backpropagation training algorithm for multi-layer feedfor-ward networks <ref> [RHW86, Wer74] </ref>, as we have used for the speech classification task described earlier.
Reference: [Tex88] <institution> Texas Instruments, Houston, Texas, USA. </institution> <note> Third-Generation TMS320 User's Guide, </note> <year> 1988. </year>
Reference: [Waw92] <author> J. Wawrzynek. </author> <title> A 250MHz 64-bit Datapath in 1.2 m CMOS. </title> <note> Technical Report In Preparation, </note> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Our group has been experimenting with the "True Single Phase Clocking" (TSPC) proposed in [YS89] for CMOS circuits, gaining experience with the technique for larger and more complex systems <ref> [Waw92] </ref>. This methodology is being used in the implementation of SPERT. We expect that the design will have less complexity, higher density, and higher speeds than one based on conventional CMOS clocking.
Reference: [Wer74] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Harvard University, </institution> <year> 1974. </year> <month> 32 </month>
Reference-contexts: The architecture is fully programmable, and executes a wide range of connectionist computations efficiently. Special emphasis is being placed on support for variants of the commonly used backpropagation training algorithm for multi-layer feedfor-ward networks <ref> [RHW86, Wer74] </ref>, as we have used for the speech classification task described earlier.
Reference: [YS89] <author> Jiren Yuan and Christer Svensson. </author> <title> High-Speed CMOS Circuit Technique. </title> <journal> IEEE JSSC, </journal> <volume> 24(1) </volume> <pages> 62-70, </pages> <month> February </month> <year> 1989. </year> <month> 33 </month>
Reference-contexts: Our group has been experimenting with the "True Single Phase Clocking" (TSPC) proposed in <ref> [YS89] </ref> for CMOS circuits, gaining experience with the technique for larger and more complex systems [Waw92]. This methodology is being used in the implementation of SPERT. We expect that the design will have less complexity, higher density, and higher speeds than one based on conventional CMOS clocking.
References-found: 16

