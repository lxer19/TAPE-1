URL: ftp://sound.media.mit.edu/pub/Papers/kdm-TR399.ps.gz
Refering-URL: http://sound.media.mit.edu/papers.html
Root-URL: http://www.media.mit.edu
Title: Automatic Transcription of Simple Polyphonic Music: Robust Front End Processing  
Author: Keith D. Martin 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Room E15-401, The Media Laboratory Massachusetts Institute of Technology  
Abstract: M.I.T. Media Laboratory Perceptual Computing Section Technical Report No. 399 Presented at the Third Joint Meeting of the Acoustical Societies of America and Japan, December 1996 Abstract It is only very recently that systems have been developed that transcribe polyphonic music with more than two voices in even limited generality. Two of these systems [Kashino et al.1995, Martin 1996] have been built within a blackboard framework, integrating front ends based on sinusoidal analysis with musical knowledge. These and other systems to date rely on instrument models for detecting octaves. Recent results have shown that an autocorrelation-based front end may make bottom-up detection of octaves possible, thereby improving system performance as well as reducing the distance between transcription models and human audition. This report outlines the blackboard approach to automatic transcription and presents a new system based on the log-lag correlogram of [El-lis 1996]. Preliminary results are presented, outlining the bottom-up detection of octaves and transcription of simple polyphonic music.
Abstract-found: 1
Intro-found: 1
Reference: [Bregman 1990] <author> Albert S. Bregman. </author> <title> Auditory Scene Analysis. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: [Bregman 1995] <author> Albert S. Bregman. </author> <title> Psychological Data and Computational ASA. </title> <booktitle> In Proc. of the Computational Auditory Scene Analysis Workshop; 1995 International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Blackboard systems are notable also for their ability to perform both bottom-up (data-driven) and top-down (expectation-or explanation-driven) processing. [Scheirer 1996] points out that top-down, or predictive, processing is necessary to account for human music perception, and as [Slaney 1995] and <ref> [Bregman 1995] </ref> have noted, both top-down and bottom-up processing appear to be necessary to explain human auditory scene analysis, of which music transcription can be viewed as a special case (one which requires a great deal of expert musical knowledge, however!). 1.4 A pitch perception model as front end Human pitch
Reference: [Brown and Puckette 1993] <author> Judith C. Brown and Miller S. Puck-ette. </author> <title> A high resolution fundamental frequency determination based on phase changes of the Fourier transform. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 94 </volume> <pages> 662-667, </pages> <year> 1993. </year>
Reference-contexts: Monophonic transcription (equivalently dubbed pitch-tracking in this context) is a mature field, with many well-understood algorithms including time-domain techniques based on zero-crossings and autocorrelation, and frequency-domain techniques based on the discrete Fourier transform and the cepstrum (c.f., <ref> [Brown and Zhang 1991, Brown 1992, Brown and Puckette 1993] </ref>). Polyphonic transcription (analysis of signals with multiple simultaneously sounding notes) has enjoyed much less relative success. In the early 1970s, Moorer built a system for transcribing duets [Moorer 1975].
Reference: [Brown and Zhang 1991] <author> Judith C. Brown and Bin Zhang. </author> <title> Musical frequency tracking using the methods of conventional and narrowed autocorrelation. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 89(5) </volume> <pages> 2346-2354, </pages> <year> 1991. </year>
Reference-contexts: Monophonic transcription (equivalently dubbed pitch-tracking in this context) is a mature field, with many well-understood algorithms including time-domain techniques based on zero-crossings and autocorrelation, and frequency-domain techniques based on the discrete Fourier transform and the cepstrum (c.f., <ref> [Brown and Zhang 1991, Brown 1992, Brown and Puckette 1993] </ref>). Polyphonic transcription (analysis of signals with multiple simultaneously sounding notes) has enjoyed much less relative success. In the early 1970s, Moorer built a system for transcribing duets [Moorer 1975].
Reference: [Brown 1992] <author> Judith C. Brown. </author> <title> Musical fundamental frequency tracking using a pattern recognition method. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 92(3) </volume> <pages> 1394-1402, </pages> <year> 1992. </year>
Reference-contexts: Monophonic transcription (equivalently dubbed pitch-tracking in this context) is a mature field, with many well-understood algorithms including time-domain techniques based on zero-crossings and autocorrelation, and frequency-domain techniques based on the discrete Fourier transform and the cepstrum (c.f., <ref> [Brown and Zhang 1991, Brown 1992, Brown and Puckette 1993] </ref>). Polyphonic transcription (analysis of signals with multiple simultaneously sounding notes) has enjoyed much less relative success. In the early 1970s, Moorer built a system for transcribing duets [Moorer 1975].
Reference: [Dorken et al.1992] <author> Erkan Dorken, Evangelos Milios, and S. Hamid Nawab. </author> <title> Knowledge-Based Signal Processing Application. </title> <editor> In Alan V. Oppenheim and S. Hamid Nawab, editors, </editor> <booktitle> Symbolic and Knowledge-Based Signal Processing, chapter 9, </booktitle> <pages> pages 303-330. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference: [Ellis and Rosenthal 1995] <author> Daniel P. W. Ellis and David Rosen-thal. </author> <title> Mid-level representation for computational auditory scene analysis. </title> <booktitle> In Proc. of the Computational Auditory Scene Analysis Workshop; 1995 International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Some of these ideas have been implemented in [Kashino et al.1995], and it will be fruitful to apply the same approach to this system. 4.2.2 Automatic acquisition of instrument models <ref> [Ellis and Rosenthal 1995] </ref> and [Ellis 1996] describe a novel representational element for pitched signals, called the weft, which 2 It should be noted that the width of peaks in the summary autocorrelation can be reduced by introducing additional smoothing to the correlogram calculation after the multiplication, and by reducing smoothing
Reference: [Ellis 1996] <author> Daniel P. W. Ellis. </author> <title> Prediction-driven computational auditory scene analysis. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <address> Cambridge, MA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: In his dissertation, Ellis presents a signal processing algorithm that can be viewed as a variant of the Meddis and Hewitt model <ref> [Ellis 1996] </ref>. Ellis computes a log-lag correlogram, where the three axes of the the correlogram volume are: filter channel frequency, lag (or inverse pitch) on a logarithmic scale, and time (see by a simple filter structure, as shown in Figure 3. <p> The signal processing underlying the system's front end is described, followed by descriptions of the blackboard system control structure, data abstraction hierarchy and knowledge base. 2.1 The front end As described in the Introduction, the front end signal processing in the current system is modeled after the log-lag correlogram of <ref> [Ellis 1996] </ref>, which may be viewed as a variant of the correlogram of Slaney and Lyon and of the pitch perception model of Meddis and Hewitt. <p> The time axis is downsampled to 220.5 Hz before being processed by the blackboard system. The correlogram implementation is identical to that described in <ref> [Ellis 1996] </ref>, with the exception that the envelope follower lowpass filter cutoff frequency is decreased with increasing lag, such that the correlogram output is nearly critically sampled (in lag) at all lags (Ellis chose a single cutoff as a compromise between oversampling at short lags and undersampling at long lags). <p> Some of these ideas have been implemented in [Kashino et al.1995], and it will be fruitful to apply the same approach to this system. 4.2.2 Automatic acquisition of instrument models [Ellis and Rosenthal 1995] and <ref> [Ellis 1996] </ref> describe a novel representational element for pitched signals, called the weft, which 2 It should be noted that the width of peaks in the summary autocorrelation can be reduced by introducing additional smoothing to the correlogram calculation after the multiplication, and by reducing smoothing before multiplication (in the envelope
Reference: [Goldstein 1973] <author> J. L. Goldstein. </author> <title> an optimum processor theory for the central formation of the pitch of complex tones. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 54 </volume> <pages> 1496-1515, </pages> <year> 1973. </year>
Reference-contexts: The best known of these models have been based on resolving individual partials with narrow filters, on envelope modulation due to the beating musical signals, which might be employed within a blackboard transcription system. of multiple partials in a wider filter, on the alignment of subhar-monics, and on autocorrelation (c.f., <ref> [Goldstein 1973, Terhardt 1979, Patterson 1987] </ref>). The model which seems to most compactly explain the widest range of psychoacoustic phenomena is the one proposed in [Med-dis and Hewitt 1991], which is related to the correlogram described in [Slaney and Lyon 1993].
Reference: [Hawley 1993] <author> Michael Hawley. </author> <title> Structure out of Sound. </title> <type> PhD thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1993. </year>
Reference-contexts: In 1993, Hawley described a system which he purported could transcribe polyphonic piano performances <ref> [Hawley 1993] </ref>. His approach was based on a differential spectrum analysis (similar to taking the difference of two adjacent FFT frames in a short-time Fourier transform) and was reported to be fairly successful, largely because piano notes do not modulate in pitch.
Reference: [Kashino et al.1995] <author> Kunio Kashino, Kazuhiro Nakadai, To-moyoshi Kinoshita, and Hidehiko Tanaka. </author> <title> Application of bayesian probability network to music scene analysis. </title> <booktitle> In IJCAI-95 Workshop on Computational Auditory Scene Analysis, </booktitle> <address> Mon-treal, Quebec, </address> <month> August </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: This report highlights the differences between our approach and the one described in <ref> [Kashino et al.1995] </ref>. transcriptions of piano performances), they go a long way toward forming a useful symbolic representation of the music. <p> While automatic music transcription has been a research goal for over 25 years, it is only in the last few years that systems have been demonstrated that are capable of transcribing more than two simultaneous musical voices in even limited generality (c.f., <ref> [Katayose and Inokuchi 1989, Kashino et al.1995, Martin 1996] </ref>). Systems to date have relied on signal processing front ends that can be characterized as extracting individual partials of musical notes by frequency-domain analysis. The transcription problem then becomes one of explaining the set of isolated partials as components of notes. <p> Continuing the metaphor, the system includes a collection of knowledge sources corresponding to the experts. An excellent introduction to the history of blackboard systems may be found in [Nii 1986]. It is notable that the systems described in both <ref> [Kashino et al.1995] </ref> and [Martin 1996] are built within a blackboard framework. Music has a natural hierarchical structure which lends itself to the type of data abstraction hierarchy typically used in blackboard systems (a portion of one possible musical hierarchy is shown in Figure 1). <p> This knowledge may take the form of hypotheses regarding the number and type of instruments in a performance as well as melodic/harmonic motion of the piece. Some of these ideas have been implemented in <ref> [Kashino et al.1995] </ref>, and it will be fruitful to apply the same approach to this system. 4.2.2 Automatic acquisition of instrument models [Ellis and Rosenthal 1995] and [Ellis 1996] describe a novel representational element for pitched signals, called the weft, which 2 It should be noted that the width of peaks
Reference: [Katayose and Inokuchi 1989] <author> Haruhiro Katayose and Seiji Inokuchi. </author> <title> The Kansei Music System. </title> <journal> Computer Music Journal, </journal> <volume> 13(4) </volume> <pages> 72-77, </pages> <year> 1989. </year>
Reference-contexts: While automatic music transcription has been a research goal for over 25 years, it is only in the last few years that systems have been demonstrated that are capable of transcribing more than two simultaneous musical voices in even limited generality (c.f., <ref> [Katayose and Inokuchi 1989, Kashino et al.1995, Martin 1996] </ref>). Systems to date have relied on signal processing front ends that can be characterized as extracting individual partials of musical notes by frequency-domain analysis. The transcription problem then becomes one of explaining the set of isolated partials as components of notes.
Reference: [Klassner et al.1995] <author> Frank Klassner, Victor Lesser, and Hamid Nawab. </author> <title> The IPUS Blackboard Architecture as a Framework for Computational Auditory Scene Analysis. </title> <booktitle> In Proc. of the Computational Auditory Scene Analysis Workshop; 1995 International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference: [Martin 1996] <author> Keith Martin. </author> <title> A blackboard system for automatic transcription of simple polyphonic music. </title> <type> Technical Report #385, </type> <institution> MIT Media Lab, Perceptual Computing Section, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: as loudness and timbre are completely ignored (see [Scheirer 1995] for an attempt to achieve perceptual equivalence in score-guided y The author was supported by an NSF graduate fellowship during a portion of this research. fl The content of this report differs from the abstract printed in the meeting program; <ref> [Martin 1996] </ref> corresponds more closely to the printed abstract. This report highlights the differences between our approach and the one described in [Kashino et al.1995]. transcriptions of piano performances), they go a long way toward forming a useful symbolic representation of the music. <p> While automatic music transcription has been a research goal for over 25 years, it is only in the last few years that systems have been demonstrated that are capable of transcribing more than two simultaneous musical voices in even limited generality (c.f., <ref> [Katayose and Inokuchi 1989, Kashino et al.1995, Martin 1996] </ref>). Systems to date have relied on signal processing front ends that can be characterized as extracting individual partials of musical notes by frequency-domain analysis. The transcription problem then becomes one of explaining the set of isolated partials as components of notes. <p> Continuing the metaphor, the system includes a collection of knowledge sources corresponding to the experts. An excellent introduction to the history of blackboard systems may be found in [Nii 1986]. It is notable that the systems described in both [Kashino et al.1995] and <ref> [Martin 1996] </ref> are built within a blackboard framework. Music has a natural hierarchical structure which lends itself to the type of data abstraction hierarchy typically used in blackboard systems (a portion of one possible musical hierarchy is shown in Figure 1). <p> It was only recently that it was thought feasible to use a correlation-based front-end, and very little of the previous implementation <ref> [Martin 1996] </ref> was reusable. At present, only five knowledge sources are present in the system, and they act almost entirely in a bottom-up, or data-driven, fashion.
Reference: [Meddis and Hewitt 1991] <author> Ray Meddis and Michael J. Hewitt. </author> <title> Virtual pitch and phase sensitivity of a computer model of the auditory periphery. I: Pitch identification. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 89(6) </volume> <pages> 2866-2882, </pages> <year> 1991. </year>
Reference: [Moorer 1975] <author> James A. Moorer. </author> <title> On the segmentation and analysis of continuous musical sound by digital computer. </title> <type> PhD thesis, </type> <institution> Department of Music, Stanford University, Stanford, </institution> <address> CA, </address> <month> May </month> <year> 1975. </year>
Reference-contexts: Polyphonic transcription (analysis of signals with multiple simultaneously sounding notes) has enjoyed much less relative success. In the early 1970s, Moorer built a system for transcribing duets <ref> [Moorer 1975] </ref>.
Reference: [Nii 1986] <author> H. Penni Nii. </author> <title> Blackboard Systems: The Blackboard Model of Problem Solving and the Evolution of Blackboard Architectures. </title> <journal> The AI Magazine, </journal> <pages> pages 38-53, </pages> <month> Summer </month> <year> 1986. </year>
Reference-contexts: Continuing the metaphor, the system includes a collection of knowledge sources corresponding to the experts. An excellent introduction to the history of blackboard systems may be found in <ref> [Nii 1986] </ref>. It is notable that the systems described in both [Kashino et al.1995] and [Martin 1996] are built within a blackboard framework.
Reference: [Patterson and Holdsworth 1990] <author> R. D. Patterson and J. Holdsworth. </author> <title> A functional model of neural activity patterns and auditory images. </title> <editor> In W. A. Ainsworth, editor, </editor> <booktitle> Advances in speech, hearing and language processing vol. </booktitle> <volume> 3. </volume> <publisher> JAI Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: In the current implementation, the filter bank is made up of forty gammatone filters (six per octave), with center frequencies ranging from 100 Hz to 10 kHz, spaced evenly in log frequency. The standard Patterson-Holdsworth filter parameters have been used, yielding filter bandwidths based on the ERB scale <ref> [Patterson and Holdsworth 1990] </ref>. The lag axis of the correlogram volume is sampled at 48 lags/octave, from 20 Hz to approximately 1 kHz, which yields adequate resolution for most musical signals. The time axis is downsampled to 220.5 Hz before being processed by the blackboard system.
Reference: [Patterson 1987] <author> R. D. Patterson. </author> <title> a pulse ribbon model of monaural phase perception. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 82 </volume> <pages> 1560-1586, </pages> <year> 1987. </year>
Reference-contexts: The best known of these models have been based on resolving individual partials with narrow filters, on envelope modulation due to the beating musical signals, which might be employed within a blackboard transcription system. of multiple partials in a wider filter, on the alignment of subhar-monics, and on autocorrelation (c.f., <ref> [Goldstein 1973, Terhardt 1979, Patterson 1987] </ref>). The model which seems to most compactly explain the widest range of psychoacoustic phenomena is the one proposed in [Med-dis and Hewitt 1991], which is related to the correlogram described in [Slaney and Lyon 1993].
Reference: [Scheirer 1995] <author> Eric Scheirer. </author> <title> Using musical knowledge to extract expressive performance information from audio recordings. </title> <booktitle> In IJCAI-95 Workshop on Computational Auditory Scene Analysis, </booktitle> <address> Montreal, Quebec, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Although these data are not a sufficient representation for reproduction of a perceptually equivalent copy of the original performance, as loudness and timbre are completely ignored (see <ref> [Scheirer 1995] </ref> for an attempt to achieve perceptual equivalence in score-guided y The author was supported by an NSF graduate fellowship during a portion of this research. fl The content of this report differs from the abstract printed in the meeting program; [Martin 1996] corresponds more closely to the printed abstract.
Reference: [Scheirer 1996] <author> Eric Scheirer. Bregman's Chimerae: </author> <title> Music Perception as Auditory Scene Analysis. </title> <booktitle> In Proc. 1996 Intl Conf on Music Perception and Cognition, </booktitle> <year> 1996. </year>
Reference-contexts: Blackboard systems are also easy to expand adding new knowledge amounts to coding a handful of procedural routines and registering them with the control system. Blackboard systems are notable also for their ability to perform both bottom-up (data-driven) and top-down (expectation-or explanation-driven) processing. <ref> [Scheirer 1996] </ref> points out that top-down, or predictive, processing is necessary to account for human music perception, and as [Slaney 1995] and [Bregman 1995] have noted, both top-down and bottom-up processing appear to be necessary to explain human auditory scene analysis, of which music transcription can be viewed as a special
Reference: [Slaney and Lyon 1993] <author> Malcolm Slaney and Richard F. Lyon. </author> <title> On the importance of time a temporal representation of sound. </title> <editor> In Martin Cooke, Steve Beet, and Malcolm Crawford, editors, </editor> <booktitle> Visual Representations of Speech Signals, </booktitle> <pages> pages 95-116. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: The model which seems to most compactly explain the widest range of psychoacoustic phenomena is the one proposed in [Med-dis and Hewitt 1991], which is related to the correlogram described in <ref> [Slaney and Lyon 1993] </ref>. In the pitch perception model, the audio signal is first decomposed into frequency bands by a model of basilar membrane mechanics (implemented by a gam-matone filter bank). Each filter channel is further processed by a model of inner hair cell (IHC) dynamics.
Reference: [Slaney 1995] <author> M. Slaney. </author> <title> A critique of pure audition. </title> <booktitle> In Proc. of the Computational Auditory Scene Analysis Workshop; 1995 International Joint Conference on Artificial Intelligence, </booktitle> <address> Mon-treal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Blackboard systems are notable also for their ability to perform both bottom-up (data-driven) and top-down (expectation-or explanation-driven) processing. [Scheirer 1996] points out that top-down, or predictive, processing is necessary to account for human music perception, and as <ref> [Slaney 1995] </ref> and [Bregman 1995] have noted, both top-down and bottom-up processing appear to be necessary to explain human auditory scene analysis, of which music transcription can be viewed as a special case (one which requires a great deal of expert musical knowledge, however!). 1.4 A pitch perception model as front
Reference: [Terhardt 1979] <author> E. Terhardt. </author> <title> calculating virtual pitch. </title> <journal> Hearing Research, </journal> <volume> 1 </volume> <pages> 155-182, </pages> <year> 1979. </year>
Reference-contexts: The best known of these models have been based on resolving individual partials with narrow filters, on envelope modulation due to the beating musical signals, which might be employed within a blackboard transcription system. of multiple partials in a wider filter, on the alignment of subhar-monics, and on autocorrelation (c.f., <ref> [Goldstein 1973, Terhardt 1979, Patterson 1987] </ref>). The model which seems to most compactly explain the widest range of psychoacoustic phenomena is the one proposed in [Med-dis and Hewitt 1991], which is related to the correlogram described in [Slaney and Lyon 1993].
Reference: [Winograd and Nawab 1995] <author> Joseph M. Winograd and S. Hamid Nawab. </author> <title> A C++ Software Environment for the Development of Embedded Signal Processing Systems. </title> <booktitle> In Proceedings of the IEEE ICASSP-95, </booktitle> <address> Detroit, MI, </address> <month> May </month> <year> 1995. </year>
Reference: [Winograd 1994] <author> Joseph M. Winograd. </author> <title> IPUS C++ Platform Version 0.1 User's Manual. </title> <type> Technical report, </type> <institution> Dept. of Electrical, Computer, and Systems Engineering, Boston University, </institution> <year> 1994. </year> <month> 11 </month>
References-found: 26

