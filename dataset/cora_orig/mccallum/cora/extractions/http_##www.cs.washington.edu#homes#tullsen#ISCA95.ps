URL: http://www.cs.washington.edu/homes/tullsen/ISCA95.ps
Refering-URL: http://www.cs.washington.edu/homes/tullsen/research.html
Root-URL: 
Title: Simultaneous Multithreading: Maximizing On-Chip Parallelism  
Author: Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: This paper examines simultaneous multithreading, a technique permitting several independent threads to issue instructions to a superscalar's multiple functional units in a single cycle. We present several models of simultaneous multithreading and compare them with alternative organizations: a wide superscalar, a fine-grain mul-tithreaded processor, and single-chip, multiple-issue multiprocessing architectures. Our results show that both (single-threaded) superscalar and fine-grain multithreaded architectures are limited in their ability to utilize the resources of a wide-issue processor. Simultaneous multithreading has the potential to achieve 4 times the throughput of a superscalar, and double that of fine-grain multi-threading. We evaluate several cache configurations made possible by this type of organization and evaluate tradeoffs between them. We also show that simultaneous multithreading is an attractive alternative to single-chip multiprocessors; simultaneous multithreaded processors with a variety of organizations outperform corresponding conventional multiprocessors with similar execution resources. While simultaneous multithreading has excellent potential to increase processor utilization, it can add substantial complexity to the design. We examine many of these complexities and evaluate alternative organizations in the design space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste. <p> Competition for non-execution resources, then, plays nearly as significant a role in this performance region as the competition for execution resources. Others have observed that caches are more strained by a multi-threaded workload than a single-thread workload, due to a decrease in locality <ref> [21, 33, 1, 31] </ref>. Our data (not shown) pinpoints the ex models, and (d) shows the total throughput for all threads for each of the six machine models.
Reference: [2] <author> A. Agarwal, B.H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> a processor architecture for multiprocessing. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The effects of these are shown as horizontal waste and vertical waste in Figure 1. Multi-threaded architectures, on the other hand, such as HEP [28], Tera [3], MASA [15] and Alewife <ref> [2] </ref> employ multiple threads with fast context switch between threads. Traditional multithreading hides memory and functional unit latencies, attacking vertical waste. In any one cycle, though, these architectures issue instructions from only one thread. <p> The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste. <p> Previous work on coarse-grain <ref> [2, 27, 31] </ref> and fine-grain [28, 3, 15, 22, 19] multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle.
Reference: [3] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The effects of these are shown as horizontal waste and vertical waste in Figure 1. Multi-threaded architectures, on the other hand, such as HEP [28], Tera <ref> [3] </ref>, MASA [15] and Alewife [2] employ multiple threads with fast context switch between threads. Traditional multithreading hides memory and functional unit latencies, attacking vertical waste. In any one cycle, though, these architectures issue instructions from only one thread. <p> This hides all sources of vertical waste, but does not hide horizontal waste. It is the only model that does not feature simultaneous multithreading. Among existing or proposed ar chitectures, this is most similar to the Tera processor <ref> [3] </ref>, which issues one 3-operation LIW instruction per cycle. * SM:Full Simultaneous Issue. This is a completely flexible simultaneous multithreaded superscalar: all eight threads compete for each of the issue slots each cycle. <p> For example, if we wish to execute around four instructions per cycle, we can build a four-issue or full simultaneous machine with 3 to 4 hardware contexts, a dual-issue machine with 4 contexts, a limited connection machine with 5 contexts, or a single-issue machine with 6 contexts. Tera <ref> [3] </ref> is an extreme example of trading pipeline complexity for more contexts; it has no forwarding in its pipelines and no data caches, but supports 128 hardware contexts. <p> Previous work on coarse-grain [2, 27, 31] and fine-grain <ref> [28, 3, 15, 22, 19] </ref> multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle. In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions.
Reference: [4] <author> R. Bedichek. </author> <title> Some efficient architecture simulation techniques. </title> <booktitle> In Winter 1990 Usenix Conference, </booktitle> <pages> pages 53-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: that defines an implementation of a simultaneous multithreaded architecture; that architecture is a straightforward extension of next-generation wide superscalar processors, running a real multiprogrammed workload that is highly optimized for execution on our target machine. 2.1 Simulation Environment Our simulator uses emulation-based instruction-level simulation, similar to Tango [8] and g88 <ref> [4] </ref>. Like g88, it features caching of partially decoded instructions for fast emulated execution. Our simulator models the execution pipelines, the memory hierarchy (both in terms of hit rates and bandwidths), the TLBs, and the branch prediction logic of a wide superscalar processor.
Reference: [5] <author> M. Butler, T.Y. Yeh, Y. Patt, M. Alsup, H. Scales, and M. She-banow. </author> <title> Single instruction steam parallelism is greater than two. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Table 3 specifies all possible sources of wasted cycles in our model, and some of the latency-hiding or latency-reducing techniques that might apply to them. Previous work <ref> [32, 5, 18] </ref>, in contrast, quantified some of these same effects by removing barriers to parallelism and measuring the resulting increases in performance. Our results, shown in Figure 2, demonstrate that the functional units of our wide superscalar processor are highly underutilized. <p> Smith, et al., [28] focus on the effects of fetch, decoding, dependence-checking, and branch prediction limitations on ILP; Butler, et al., <ref> [5] </ref> examine these limitations plus scheduling window size, scheduling policy, and functional unit configuration; Lam and Wilson [18] focus on the interaction of branches and ILP; and Wall [32] examines scheduling window size, branch prediction, register renaming, and aliasing.
Reference: [6] <author> G.E. Daddis, Jr. and H.C. Torng. </author> <title> The concurrent execution of multiple instruction streams on superscalar processors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:76-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Real implementations may see reduced performance due to various design tradeoffs; we intend to explore these implementation issues in future work. Previous studies have examined architectures that exhibit simultaneous multithreading through various combinations of VLIW, superscalar, and multithreading features, both analytically [34] and through simulation <ref> [16, 17, 6, 23] </ref>; we discuss these in detail in Section 7. <p> The scheduling of instructions onto functional units is more complex on all types of simultaneous multithreaded processors. The Hirata, et al., design [16] is closest to the single-issue model, although they simulate a small number of configurations where the per-thread issue bandwidth is increased. Others <ref> [34, 17, 23, 6] </ref> implement models that are more similar to full simultaneous issue, but the issue width of the architectures, and thus the complexity of the schemes, vary considerably. 4.2 The Performance of Simultaneous Multithreading of the number of threads. <p> Prasadh and Wu also examine the register file bandwidth requirements for 4 threads scheduled in this manner. They use infinite caches and show a maximum speedup above 3 over single-thread execution for parallel applications. Daddis and Torng <ref> [6] </ref> plot increases in instruction throughput as a function of the fetch bandwidth and the size of the dispatch stack. The dispatch stack is the global instruction window that issues all fetched instructions. Their system has two threads, unlimited functional units, and unlimited issue bandwidth (but limited fetch bandwidth).
Reference: [7] <author> W.J. Dally, S.W. Keckler, N. Carter, A. Chang, M. Fillo, and W.S. Lee. </author> <title> M-Machine architecture v1.0. Technical Report - MIT Concurrent VLSI Architecture Memo 58, </title> <institution> Massachusetts Institute of Technology, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Superscalar execution (as opposed to single-issue execution) both introduces horizontal waste and increases the amount of vertical waste. ventional multiprocessor. As chip densities increase, single-chip multiprocessors will become a viable design option <ref> [7] </ref>. The simultaneous multithreaded processor and the single-chip multiprocessor are two close organizational alternatives for increasing on-chip execution resources. We compare these two approaches and show that simultaneous multithreading is potentially superior to multiprocessing in its ability to utilize processor resources. <p> In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions. In Section 4, we extended these results by showing how fine-grain multithreading runs on a multiple-issue processor. In the M-Machine <ref> [7] </ref> each processor cluster schedules LIW instructions onto execution units on a cycle-by-cycle basis similar to the Tera scheme. There is no simultaneous issue of instructions from multiple threads to functional units in the same cycle on individual clusters.
Reference: [8] <author> H. Davis, S.R. Goldschmidt, and J. Hennessy. </author> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II:99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: a simulation environment that defines an implementation of a simultaneous multithreaded architecture; that architecture is a straightforward extension of next-generation wide superscalar processors, running a real multiprogrammed workload that is highly optimized for execution on our target machine. 2.1 Simulation Environment Our simulator uses emulation-based instruction-level simulation, similar to Tango <ref> [8] </ref> and g88 [4]. Like g88, it features caching of partially decoded instructions for fast emulated execution. Our simulator models the execution pipelines, the memory hierarchy (both in terms of hit rates and bandwidths), the TLBs, and the branch prediction logic of a wide superscalar processor.
Reference: [9] <author> M. Denman. </author> <title> PowerPC 604. </title> <booktitle> In Hot Chips VI, </booktitle> <pages> pages 193-200, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Current microprocessors employ various techniques to increase parallelism and processor utilization; however, each technique has its limits. For example, modern superscalars, such as the DEC Alpha 21164 [11], PowerPC 604 <ref> [9] </ref>, MIPS R10000 [24], Sun Ul-traSparc [25], and HP PA-8000 [26] issue up to four instructions per cycle from a single thread. Multiple instruction issue has the potential to increase performance, but is ultimately limited by instruction dependencies (i.e., the available parallelism) and long-latency operations within the single executing thread.
Reference: [10] <author> K.M. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In COMPCON, Spring 1992, </booktitle> <pages> pages 305-310, </pages> <year> 1992. </year>
Reference-contexts: With 8 threads, where throughput is more tolerant of misprediction delays, the impact was less than .5%. 2.2 Workload Our workload is the SPEC92 benchmark suite <ref> [10] </ref>. To gauge the raw instruction throughput achievable by multithreaded superscalar processors, we chose uniprocessor applications, assigning a distinct program to each thread. This models a parallel workload achieved by multiprogramming rather than parallel processing.
Reference: [11] <author> J. Edmondson and P Rubinfield. </author> <title> An overview of the 21164 AXP microprocessor. </title> <booktitle> In Hot Chips VI, </booktitle> <pages> pages 1-8, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Current microprocessors employ various techniques to increase parallelism and processor utilization; however, each technique has its limits. For example, modern superscalars, such as the DEC Alpha 21164 <ref> [11] </ref>, PowerPC 604 [9], MIPS R10000 [24], Sun Ul-traSparc [25], and HP PA-8000 [26] issue up to four instructions per cycle from a single thread. <p> This study evaluates the potential improvement, relative to wide superscalar architectures and conventional multithreaded architectures, of various simultaneous multithreading models. To place our evaluation in the context of modern superscalar processors,we simulate a base architecture derived from the 300 MHz Alpha 21164 <ref> [11] </ref>, enhanced for wider superscalar execution; our SM architectures are extensions of that basic design. Since code scheduling is crucial on wide superscalars, we generate code using the state-of-the-art Multiflow trace scheduling compiler [20].
Reference: [12] <author> M. Franklin. </author> <title> The Multiscalar Architecture. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1993. </year>
Reference-contexts: In the M-Machine [7] each processor cluster schedules LIW instructions onto execution units on a cycle-by-cycle basis similar to the Tera scheme. There is no simultaneous issue of instructions from multiple threads to functional units in the same cycle on individual clusters. Franklin's Multiscalar architecture <ref> [13, 12] </ref> assigns fine-grain threads to processors, so competition for execution resources (processors in this case) is at the level of a task rather than an individual instruction. Hirata, et al., [16] present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel ray-tracing application.
Reference: [13] <author> M. Franklin and G.S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grain parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In the M-Machine [7] each processor cluster schedules LIW instructions onto execution units on a cycle-by-cycle basis similar to the Tera scheme. There is no simultaneous issue of instructions from multiple threads to functional units in the same cycle on individual clusters. Franklin's Multiscalar architecture <ref> [13, 12] </ref> assigns fine-grain threads to processors, so competition for execution resources (processors in this case) is at the level of a task rather than an individual instruction. Hirata, et al., [16] present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel ray-tracing application.
Reference: [14] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste.
Reference: [15] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The effects of these are shown as horizontal waste and vertical waste in Figure 1. Multi-threaded architectures, on the other hand, such as HEP [28], Tera [3], MASA <ref> [15] </ref> and Alewife [2] employ multiple threads with fast context switch between threads. Traditional multithreading hides memory and functional unit latencies, attacking vertical waste. In any one cycle, though, these architectures issue instructions from only one thread. <p> Previous work on coarse-grain [2, 27, 31] and fine-grain <ref> [28, 3, 15, 22, 19] </ref> multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle. In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions.
Reference: [16] <author> H. Hirata, K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and T. Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Real implementations may see reduced performance due to various design tradeoffs; we intend to explore these implementation issues in future work. Previous studies have examined architectures that exhibit simultaneous multithreading through various combinations of VLIW, superscalar, and multithreading features, both analytically [34] and through simulation <ref> [16, 17, 6, 23] </ref>; we discuss these in detail in Section 7. <p> The scheduling of instructions onto functional units is more complex on all types of simultaneous multithreaded processors. The Hirata, et al., design <ref> [16] </ref> is closest to the single-issue model, although they simulate a small number of configurations where the per-thread issue bandwidth is increased. <p> Franklin's Multiscalar architecture [13, 12] assigns fine-grain threads to processors, so competition for execution resources (processors in this case) is at the level of a task rather than an individual instruction. Hirata, et al., <ref> [16] </ref> present an architecture for a multithreaded superscalar processor and simulate its performance on a parallel ray-tracing application. They do not simulate caches or TLBs, and their architecture has no branch prediction mechanism. They show speedups as high as 5.8 over a single-threaded architecture when using 8 threads.
Reference: [17] <author> S.W. Keckler and W.J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Real implementations may see reduced performance due to various design tradeoffs; we intend to explore these implementation issues in future work. Previous studies have examined architectures that exhibit simultaneous multithreading through various combinations of VLIW, superscalar, and multithreading features, both analytically [34] and through simulation <ref> [16, 17, 6, 23] </ref>; we discuss these in detail in Section 7. <p> The scheduling of instructions onto functional units is more complex on all types of simultaneous multithreaded processors. The Hirata, et al., design [16] is closest to the single-issue model, although they simulate a small number of configurations where the per-thread issue bandwidth is increased. Others <ref> [34, 17, 23, 6] </ref> implement models that are more similar to full simultaneous issue, but the issue width of the architectures, and thus the complexity of the schemes, vary considerably. 4.2 The Performance of Simultaneous Multithreading of the number of threads. <p> Yamamoto, et al., [34] present an analytical model of multithreaded superscalar performance, backed up by simulation. Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). They report increases in instruction throughput of 1.3 to 3 with four threads. Keckler and Dally <ref> [17] </ref> and Prasadh and Wu [23] describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units. Keckler and Dally report speedups as high as 3.12 for some highly parallel applications. Prasadh and Wu also examine the register file bandwidth requirements for 4 threads scheduled in this manner.
Reference: [18] <author> M.S. Lam and R.P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Table 3 specifies all possible sources of wasted cycles in our model, and some of the latency-hiding or latency-reducing techniques that might apply to them. Previous work <ref> [32, 5, 18] </ref>, in contrast, quantified some of these same effects by removing barriers to parallelism and measuring the resulting increases in performance. Our results, shown in Figure 2, demonstrate that the functional units of our wide superscalar processor are highly underutilized. <p> Smith, et al., [28] focus on the effects of fetch, decoding, dependence-checking, and branch prediction limitations on ILP; Butler, et al., [5] examine these limitations plus scheduling window size, scheduling policy, and functional unit configuration; Lam and Wilson <ref> [18] </ref> focus on the interaction of branches and ILP; and Wall [32] examines scheduling window size, branch prediction, register renaming, and aliasing.
Reference: [19] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A multithreading technique targeting multiprocessors and workstations. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 308-318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste. <p> Previous work on coarse-grain [2, 27, 31] and fine-grain <ref> [28, 3, 15, 22, 19] </ref> multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle. In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions.
Reference: [20] <author> P.G. Lowney, S.M. Freudenberger, T.J. Karzes, W.D. Licht-enstein, R.P. Nix, J.S. ODonnell, and J.C. Ruttenberg. </author> <title> The multiflow trace scheduling compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1-2):51-142, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Since code scheduling is crucial on wide superscalars, we generate code using the state-of-the-art Multiflow trace scheduling compiler <ref> [20] </ref>. Our results show the limits of superscalar execution and traditional multithreading to increase instruction throughput in future processors. <p> Instructions not scheduled due to functional unit availability have priority in the next cycle. We complement this straightforward issue model with the use of state-of-the-art static scheduling, using the Multiflow trace scheduling compiler <ref> [20] </ref>. This reduces the benefits that might be gained by full dynamic execution, thus eliminating a great deal of complexity (e.g., we don't need register renaming unless we need precise exceptions, and we can use a simple 1-bit-per-register scoreboarding scheme) in the replicated register sets and fetch/decode pipes.
Reference: [21] <author> D.C. McCrackin. </author> <title> The synergistic effect of thread scheduling and caching in multithreaded computers. </title> <booktitle> In COMPCON, Spring 1993, </booktitle> <pages> pages 157-164, </pages> <year> 1993. </year>
Reference-contexts: Competition for non-execution resources, then, plays nearly as significant a role in this performance region as the competition for execution resources. Others have observed that caches are more strained by a multi-threaded workload than a single-thread workload, due to a decrease in locality <ref> [21, 33, 1, 31] </ref>. Our data (not shown) pinpoints the ex models, and (d) shows the total throughput for all threads for each of the six machine models.
Reference: [22] <author> R.S. Nikhil and Arvind. </author> <booktitle> Can dataflow subsume von Neumann computing? In 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Previous work on coarse-grain [2, 27, 31] and fine-grain <ref> [28, 3, 15, 22, 19] </ref> multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle. In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions.
Reference: [23] <author> R.G. Prasadh and C.-L. Wu. </author> <title> A benchmark evaluation of a multi-threaded RISC processor architecture. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:84-91, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Real implementations may see reduced performance due to various design tradeoffs; we intend to explore these implementation issues in future work. Previous studies have examined architectures that exhibit simultaneous multithreading through various combinations of VLIW, superscalar, and multithreading features, both analytically [34] and through simulation <ref> [16, 17, 6, 23] </ref>; we discuss these in detail in Section 7. <p> The scheduling of instructions onto functional units is more complex on all types of simultaneous multithreaded processors. The Hirata, et al., design [16] is closest to the single-issue model, although they simulate a small number of configurations where the per-thread issue bandwidth is increased. Others <ref> [34, 17, 23, 6] </ref> implement models that are more similar to full simultaneous issue, but the issue width of the architectures, and thus the complexity of the schemes, vary considerably. 4.2 The Performance of Simultaneous Multithreading of the number of threads. <p> Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). They report increases in instruction throughput of 1.3 to 3 with four threads. Keckler and Dally [17] and Prasadh and Wu <ref> [23] </ref> describe architectures that dynamically interleave operations from VLIW instructions onto individual functional units. Keckler and Dally report speedups as high as 3.12 for some highly parallel applications. Prasadh and Wu also examine the register file bandwidth requirements for 4 threads scheduled in this manner.
Reference: [24] <institution> Microprocessor Report, </institution> <month> October 24 </month> <year> 1994. </year>
Reference-contexts: Current microprocessors employ various techniques to increase parallelism and processor utilization; however, each technique has its limits. For example, modern superscalars, such as the DEC Alpha 21164 [11], PowerPC 604 [9], MIPS R10000 <ref> [24] </ref>, Sun Ul-traSparc [25], and HP PA-8000 [26] issue up to four instructions per cycle from a single thread. Multiple instruction issue has the potential to increase performance, but is ultimately limited by instruction dependencies (i.e., the available parallelism) and long-latency operations within the single executing thread.
Reference: [25] <institution> Microprocessor Report, </institution> <month> October 3 </month> <year> 1994. </year>
Reference-contexts: Current microprocessors employ various techniques to increase parallelism and processor utilization; however, each technique has its limits. For example, modern superscalars, such as the DEC Alpha 21164 [11], PowerPC 604 [9], MIPS R10000 [24], Sun Ul-traSparc <ref> [25] </ref>, and HP PA-8000 [26] issue up to four instructions per cycle from a single thread. Multiple instruction issue has the potential to increase performance, but is ultimately limited by instruction dependencies (i.e., the available parallelism) and long-latency operations within the single executing thread.
Reference: [26] <institution> Microprocessor Report, </institution> <month> November 14 </month> <year> 1994. </year>
Reference-contexts: Current microprocessors employ various techniques to increase parallelism and processor utilization; however, each technique has its limits. For example, modern superscalars, such as the DEC Alpha 21164 [11], PowerPC 604 [9], MIPS R10000 [24], Sun Ul-traSparc [25], and HP PA-8000 <ref> [26] </ref> issue up to four instructions per cycle from a single thread. Multiple instruction issue has the potential to increase performance, but is ultimately limited by instruction dependencies (i.e., the available parallelism) and long-latency operations within the single executing thread.
Reference: [27] <author> R.H. Saavedra-Barrera, D.E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> In Second Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Previous work on coarse-grain <ref> [2, 27, 31] </ref> and fine-grain [28, 3, 15, 22, 19] multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle.
Reference: [28] <author> B.J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE Real Time Signal Processing IV, </booktitle> <pages> pages 241-248, </pages> <year> 1981. </year>
Reference-contexts: The effects of these are shown as horizontal waste and vertical waste in Figure 1. Multi-threaded architectures, on the other hand, such as HEP <ref> [28] </ref>, Tera [3], MASA [15] and Alewife [2] employ multiple threads with fast context switch between threads. Traditional multithreading hides memory and functional unit latencies, attacking vertical waste. In any one cycle, though, these architectures issue instructions from only one thread. <p> The data presented in Section 3 provides a different perspective from previous studies on ILP, which remove barriers to parallelism (i.e. apply real or ideal latency-hiding techniques) and measure the resulting performance. Smith, et al., <ref> [28] </ref> focus on the effects of fetch, decoding, dependence-checking, and branch prediction limitations on ILP; Butler, et al., [5] examine these limitations plus scheduling window size, scheduling policy, and functional unit configuration; Lam and Wilson [18] focus on the interaction of branches and ILP; and Wall [32] examines scheduling window size, <p> Previous work on coarse-grain [2, 27, 31] and fine-grain <ref> [28, 3, 15, 22, 19] </ref> multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle. In fact, most of these architectures are single-issue, rather than superscalar, although Tera has LIW (3-wide) instructions.
Reference: [29] <author> J. Smith. </author> <title> A study of branch prediction strategies. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 135-148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: A 2048-entry, direct-mapped, 2-bit branch prediction history table <ref> [29] </ref> supports branch prediction; the table improves coverage of branch addresses relative to the Alpha (with an 8 KB I cache), which only stores prediction information for branches that remain in the I cache. Conflicts in the table are not resolved.
Reference: [30] <author> G.S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We also ran simulations with caches closer to current processorswe discuss these experiments as appropriate, but do not show any results. The caches (Table 2) are multi-ported by interleaving them into banks, similar to the design of Sohi and Franklin <ref> [30] </ref>. An instruction cache access occurs whenever the program counter crosses a 32-byte boundary; otherwise, the instruction is fetched from the prefetch buffer. We model lockup-free caches and TLBs. TLB misses require two full memory accesses and no execution resources.
Reference: [31] <author> R. Thekkath and S.J. Eggers. </author> <title> The effectiveness of multiple hardware contexts. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 328-337, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste. <p> Competition for non-execution resources, then, plays nearly as significant a role in this performance region as the competition for execution resources. Others have observed that caches are more strained by a multi-threaded workload than a single-thread workload, due to a decrease in locality <ref> [21, 33, 1, 31] </ref>. Our data (not shown) pinpoints the ex models, and (d) shows the total throughput for all threads for each of the six machine models. <p> Previous work on coarse-grain <ref> [2, 27, 31] </ref> and fine-grain [28, 3, 15, 22, 19] multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle.
Reference: [32] <author> D.W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languagesand Operating Systems, </booktitle> <pages> pages 176-188, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Table 3 specifies all possible sources of wasted cycles in our model, and some of the latency-hiding or latency-reducing techniques that might apply to them. Previous work <ref> [32, 5, 18] </ref>, in contrast, quantified some of these same effects by removing barriers to parallelism and measuring the resulting increases in performance. Our results, shown in Figure 2, demonstrate that the functional units of our wide superscalar processor are highly underutilized. <p> Smith, et al., [28] focus on the effects of fetch, decoding, dependence-checking, and branch prediction limitations on ILP; Butler, et al., [5] examine these limitations plus scheduling window size, scheduling policy, and functional unit configuration; Lam and Wilson [18] focus on the interaction of branches and ILP; and Wall <ref> [32] </ref> examines scheduling window size, branch prediction, register renaming, and aliasing. Previous work on coarse-grain [2, 27, 31] and fine-grain [28, 3, 15, 22, 19] multithreading provides the foundation for our work on simultaneous multithreading, but none features simultaneous issuing of instructions from different threads during the same cycle.
Reference: [33] <author> W.D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> In 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The graph shows that there is little advantage to adding more than four threads in this model. In fact, with four threads, the vertical waste has been reduced to less than 3%, which bounds any further gains beyond that point. This result is similar to previous studies <ref> [2, 1, 19, 14, 33, 31] </ref> for both coarse-grain and fine-grain multithreading on single-issue processors, which have concluded that multithreading is only beneficial for 2 to 5 threads. These limitations do not apply to simultaneous multithreading, however, because of its ability to exploit horizontal waste. <p> Competition for non-execution resources, then, plays nearly as significant a role in this performance region as the competition for execution resources. Others have observed that caches are more strained by a multi-threaded workload than a single-thread workload, due to a decrease in locality <ref> [21, 33, 1, 31] </ref>. Our data (not shown) pinpoints the ex models, and (d) shows the total throughput for all threads for each of the six machine models.
Reference: [34] <author> W. Yamamoto, M.J. Serrano, A.R. Talcott, R.C. Wood, and M. Nemirosky. </author> <title> Performance estimation of multistreamed, superscalar processors. </title> <booktitle> In Twenty-Seventh Hawaii Interna-tion Conferenceon System Sciences, </booktitle> <pages> pages I:195-204, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Real implementations may see reduced performance due to various design tradeoffs; we intend to explore these implementation issues in future work. Previous studies have examined architectures that exhibit simultaneous multithreading through various combinations of VLIW, superscalar, and multithreading features, both analytically <ref> [34] </ref> and through simulation [16, 17, 6, 23]; we discuss these in detail in Section 7. <p> The scheduling of instructions onto functional units is more complex on all types of simultaneous multithreaded processors. The Hirata, et al., design [16] is closest to the single-issue model, although they simulate a small number of configurations where the per-thread issue bandwidth is increased. Others <ref> [34, 17, 23, 6] </ref> implement models that are more similar to full simultaneous issue, but the issue width of the architectures, and thus the complexity of the schemes, vary considerably. 4.2 The Performance of Simultaneous Multithreading of the number of threads. <p> They do not simulate caches or TLBs, and their architecture has no branch prediction mechanism. They show speedups as high as 5.8 over a single-threaded architecture when using 8 threads. Yamamoto, et al., <ref> [34] </ref> present an analytical model of multithreaded superscalar performance, backed up by simulation. Their study models perfect branching, perfect caches and a homogeneous workload (all threads running the same trace). They report increases in instruction throughput of 1.3 to 3 with four threads.
References-found: 34

