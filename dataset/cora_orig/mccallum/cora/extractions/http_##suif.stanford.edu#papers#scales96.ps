URL: http://suif.stanford.edu/papers/scales96.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: Transparent Fault Tolerance for Parallel Applications on Networks of Workstations  
Author: Daniel J. Scales and Monica S. Lam 
Address: Stanford, CA 94305  Ave., Palo Alto, CA 94301.  
Affiliation: Computer Systems Laboratory Stanford University  Western Research Laboratory, 250 University  
Date: January, 1996  
Note: To appear in the Winter 1996 USENIX Technical Conference,  Author's current address: Digital Equipment Corporation  This research was supported in part by DARPA contract DABT63-91-K-0003.  
Abstract: This paper describes a new method for providing transparent fault tolerance for parallel applications on a network of workstations. We have designed our method in the context of shared object system called SAM, a portable run-time system which provides a global name space and automatic caching of shared data. SAM incorporates a novel design intended to address the problem of the high communication overheads in distributed memory environments and is implemented on a variety of distributed memory platforms. Our fundamental approach to providing fault tolerance is to ensure the replication of all data on more than one workstation using the dynamic caching already provided by SAM. The replicated data is accessible to the local processor like other cached data, making access to shared data faster and potentially offsetting some of the fault tolerance overhead. In addition, our method uses information available in SAM applications on how processes access shared data to enable several optimizations which reduce the fault-tolerance overhead. We have built an implementation of our fault-tolerance method in SAM for heterogeneous networks of workstations running PVM3. In this paper, we present our fault-tolerance method and describe its implementation in detail. We give performance results and overhead numbers for several large SAM applications running on a cluster of Alpha workstations connected by an ATM network. Our method is successful in providing transparent fault tolerance for parallel applications running on a network of workstations and is unique in requiring no global synchronizations and no disk operations to a reliable file server. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: In this section we give performance results for three long-running applications-GPS, Water, and Barnes-Hut-executing on a cluster of eight 225 MHz Alpha workstations connected by an ATM network. The workstations are connected by a 155 Mbit/sec AN2 ATM network <ref> [1, 19] </ref> developed at DEC SRC, and use a version of PVM3 that is highly tuned for the AN2 network. The maximum achievable bandwidth under PVM3 is about 14.6 Mbytes/sec and the latency for sending a message from one host to another is about 90 s.
Reference: [2] <author> J. E. Barnes and P. Hut. </author> <title> A Hierarchical O(NlogN) Force-Calculation Algorithm. </title> <journal> Nature, </journal> <volume> 324(6096) </volume> <pages> 446-449, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Again, the miss rate is actually lower with fault tolerance because the replication of objects during checkpoints eliminates some misses to these objects. Barnes-Hut <ref> [2] </ref> is an application that simulates the evolution of an n-body system using a tree data structure to compute the forces between bodies in O (nlogn) time. The SAM version of the code is based on the original serial version of the code and is written in a shared-memory style.
Reference: [3] <author> M. Berry and et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: The Water application evaluates forces and potentials in a system of water molecules in the liquid state. Water is derived from the Perfect Club benchmark MDG <ref> [3] </ref> and performs the same computation. Water is implemented using Jade [14], which is a parallel language implemented entirely in SAM.
Reference: [4] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> A Message Passing System Supporting Fault-Tolerance. </title> <booktitle> In Proceedings of the ACM SIGOPS Symposium on Operating System Principles, </booktitle> <pages> pages 90-99, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The independent checkpointing methods divide into two classes, optimistic and pessimistic methods. In the pessimistic method <ref> [4] </ref>, when a process communicates with another process, it atomically checkpoints or logs its incremental state change with the communication.
Reference: [5] <author> S. R. Cannon. </author> <title> Adding Fault-tolerant Transaction Processing to Linda. </title> <journal> Software Practice and Experience, </journal> <volume> 24(5) </volume> <pages> 449-466, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: However, their fault tolerance method depends completely on their use of reliable ordered broadcast, whose performance does not scale well for large numbers of hosts or networks with variable delays. MOM <ref> [5] </ref> is a fault-tolerant implementation of a Linda-like programming model. The programmer uses 11 the basic operations of Linda, but must sometimes spec-ify additional information that is necessary only to support fault tolerance.
Reference: [6] <author> K. M. Chandy and L. Lamport. </author> <title> Determining Global States of Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: We illustrate these methods in Figure 2, where the arrows indicate communication between processes P1, P2, and P3, and the heavy horizontal bars indicate checkpoints. Figure 2 (a) shows consistent checkpoint-ing <ref> [6, 11] </ref>, in which all processes periodically synchronize and checkpoint their state simultaneously. This method requires a global synchronization that ensures that all processes are in a consistent state (in which all messages sent by one process have been received by the destination process).
Reference: [7] <author> S. Handley. </author> <title> The Prediction of the Degree of Exposure to Solvent of Amino Acid Residues via Genetic Programming. </title> <booktitle> In Second International Conferenceon Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 156-160, </pages> <address> Stan-ford, CA, </address> <year> 1994. </year>
Reference-contexts: For each of the applications below, recovery from a failed process only takes on the order of a few seconds. GPS is an application which attempts to determine a useful formula that predicts the degree of exposure to solvent of amino acids via a technique called genetic programming <ref> [7] </ref>. Genetic programming applications attempt to evolve useful formulas by emulating evolution and typically require large amounts of computer time.
Reference: [8] <author> B. Janssens and W. K. Fuchs. </author> <title> Relaxing Consistency in Recoverable Distributed Shared Memory. </title> <booktitle> In Proceedings of the Twenty-third International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 155-163, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: They propose using two copies of each virtual memory page to allow check-pointing of pages to disk to proceed quickly. Janssens and Fuchs <ref> [8] </ref> apply this method to a DSM system with a relaxed memory consistency model. The expected checkpointing overhead is greatly reduced because a host need only checkpoint when it holds a synchronization variable that another host attempts to acquire.
Reference: [9] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Recovery in Distributed Systems Using Optimistic Message Logging and Checkpointing. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 171-181, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: The advantage of this approach is that no global synchronization is required, and only the work of the failed process is rolled back. The disadvantage is that fault-tolerance overhead depends on the pattern of communication and can be large. In the optimistic method <ref> [9, 17] </ref>, a process also checkpoints or logs information when it communicates with another process, but the checkpointing or logging is not required to occur atomically with the communication. Because of this fact, it is not guaranteed that a process can be recovered without affecting the state of other processes.
Reference: [10] <author> M. F. Kaashoek, R. Michiels, H. E. Bal, and A. S. Tanen-baum. </author> <title> Transparent Fault-Tolerance in Parallel Orca Programs. In Symposium on Experiences with Distributed and Multiprocessor Systems, </title> <booktitle> pages 297-311,Mar. </booktitle> <year> 1992. </year>
Reference-contexts: Like our work, Kaashoek et al. <ref> [10] </ref> implement fault tolerance for a shared object system called Orca. However, the methods for achieving fault tolerance are completely different. Kaashoek et al. use global checkpoints at periodic intervals to ensure that the system can be restarted from a previous state if there is a host failure.
Reference: [11] <author> K. Li, , J. F. Naughton, and J. S. Plank. </author> <title> Checkpoint-ing Multicomputer Applications. </title> <booktitle> In Proceedings of the Tenth Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 2-11, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: We illustrate these methods in Figure 2, where the arrows indicate communication between processes P1, P2, and P3, and the heavy horizontal bars indicate checkpoints. Figure 2 (a) shows consistent checkpoint-ing <ref> [6, 11] </ref>, in which all processes periodically synchronize and checkpoint their state simultaneously. This method requires a global synchronization that ensures that all processes are in a consistent state (in which all messages sent by one process have been received by the destination process).
Reference: [12] <author> N. Neves, M. Castro, and P. Guedes. </author> <title> A Checkpoint Protocol for an Entry Consistent Shared Memory System. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 121-129, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: A processor must periodically checkpoint (independently of other processors) in order to reduce the size of the log. Neves et al. <ref> [12] </ref> describe a method of providing fault-tolerance for an entry-consistent DSM system called DiSOM. Each process logs the contents of an object in its own local volatile memory, whenever it releases a write lock on the object.
Reference: [13] <author> G. G. Richard III and M. Singhal. </author> <title> Using Logging and Asynchronous Checkpointing to Implement Recoverable Distributed Shared Memory. </title> <booktitle> In Proceedings of the Twelfth Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 58-67, </pages> <year> 1993. </year>
Reference-contexts: Janssens and Fuchs [8] apply this method to a DSM system with a relaxed memory consistency model. The expected checkpointing overhead is greatly reduced because a host need only checkpoint when it holds a synchronization variable that another host attempts to acquire. Richard and Singhal <ref> [13] </ref> describe a related approach in which a processor logs to disk the pages that it has accessed via read operations whenever it sends a modified page to another processor, instead of doing a full checkpoint.
Reference: [14] <author> M. C. Rinard, D. J. Scales, and M. S. Lam. </author> <title> Heterogeneous Parallel Programming in Jade. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 245-256, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The Water application evaluates forces and potentials in a system of water molecules in the liquid state. Water is derived from the Perfect Club benchmark MDG [3] and performs the same computation. Water is implemented using Jade <ref> [14] </ref>, which is a parallel language implemented entirely in SAM. Since we have added fault tolerance to the SAM system, all Jade applications are also automatically fault-tolerant. 1728 particles, both when not using fault tolerance and when using fault tolerance, and with the same statistics as before.
Reference: [15] <author> D. J. Scales and M. S. Lam. </author> <title> An Efficient Shared Memory System for Distributed Memory Machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: SAM is a run-time system for distributed memory machines and networks of workstations that eases programming by providing a global name space and dynamic caching of shared data in software at the level of user-defined types (or objects) <ref> [15, 16] </ref>. SAM incorporates a novel design that retains many of the ef ficient properties of message passing and minimizes communication. <p> SAM is implemented as preprocessor and run-time system for C programs, but could be modified to work with FORTRAN-90 and C++ applications as well. A more detailed description is provided in <ref> [15, 16] </ref>. SAM is a software distributed shared memory system that provides a global name space for accessing shared 2 objects on distributed memory machines.
Reference: [16] <author> D. J. Scales and M. S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: SAM is a run-time system for distributed memory machines and networks of workstations that eases programming by providing a global name space and dynamic caching of shared data in software at the level of user-defined types (or objects) <ref> [15, 16] </ref>. SAM incorporates a novel design that retains many of the ef ficient properties of message passing and minimizes communication. <p> SAM is implemented as preprocessor and run-time system for C programs, but could be modified to work with FORTRAN-90 and C++ applications as well. A more detailed description is provided in <ref> [15, 16] </ref>. SAM is a software distributed shared memory system that provides a global name space for accessing shared 2 objects on distributed memory machines.
Reference: [17] <author> R. E. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 204-226, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: The advantage of this approach is that no global synchronization is required, and only the work of the failed process is rolled back. The disadvantage is that fault-tolerance overhead depends on the pattern of communication and can be large. In the optimistic method <ref> [9, 17] </ref>, a process also checkpoints or logs information when it communicates with another process, but the checkpointing or logging is not required to occur atomically with the communication. Because of this fact, it is not guaranteed that a process can be recovered without affecting the state of other processes.
Reference: [18] <author> V. Sunderam. </author> <title> PVM: a Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: To date, networks of workstations have been used for only a limited range of parallel applications, because of issues of programmability and efficiency. The basic message-passing primitives provided by systems such as PVM <ref> [18] </ref> support only a very low-level programming interface and are difficult to use for programming applications with complex communication patterns. Systems that support a global shared-memory model in software greatly ease programming of parallel applications, but can encounter efficiency problems because of the high cost of communication in workstation environments.
Reference: [19] <author> C. P. Thacker and M. D. Schroeder. </author> <title> AN2 Switch Overview. </title> <note> In preparation. </note>
Reference-contexts: In this section we give performance results for three long-running applications-GPS, Water, and Barnes-Hut-executing on a cluster of eight 225 MHz Alpha workstations connected by an ATM network. The workstations are connected by a 155 Mbit/sec AN2 ATM network <ref> [1, 19] </ref> developed at DEC SRC, and use a version of PVM3 that is highly tuned for the AN2 network. The maximum achievable bandwidth under PVM3 is about 14.6 Mbytes/sec and the latency for sending a message from one host to another is about 90 s.
Reference: [20] <author> K.-L. Wu and W. K. Fuchs. </author> <title> Recoverable Distributed Shared Virtual Memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 460-469, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: However, they do not consider the problem of restarting Linda tasks that were running on hosts that have failed. Wu and Fuchs <ref> [20] </ref> describe techniques for making a page-based shared virtual memory system fault-tolerant. Like our work, Wu and Fuchs implement asynchronous pessimistic checkpointing by forcing a checkpoint whenever a host sends a page to another host in response to a request.

References-found: 20

