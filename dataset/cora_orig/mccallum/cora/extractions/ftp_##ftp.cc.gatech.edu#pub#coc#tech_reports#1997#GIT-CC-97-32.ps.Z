URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1997/GIT-CC-97-32.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.97.html
Root-URL: 
Title: A Performance Model for Parallel Programs  
Keyword: Workstations.  
Abstract: In this paper, we describe a model for determining the optimal data and computation decomposition for a parallel program by predicting its execution time. The model takes into account various types of data and computation decompositions for each loop nest and combines these to determine a global optimum. The unique features of the model are its accuracy, platform independence, and ability to take potential dynamic decompositions and interleaving of computation and communication into account. We give performance results for the application of the model to standard benchmarks on the IBM SP/2 and a Network of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Saman P. Amarasinghe, Jennifer M. Anderson, Monica S. Lam, and Amy W. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Sixth International Workshop on Languages and Compilers for Parallel Computing, volume 768 of LNCS. </booktitle> <publisher> Springer, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Even if the performance estimators for individual loop nests are 100% accurate, determining the global optimum is NP complete. Hence, we use a greedy heuristic to search for a global optimum rather than an exhaustive search. In practice, the greedy heuristic performs well. Our toolkit utilizes SUIF <ref> [1] </ref> to statically analyze a sequential program to determine available parallelism in loop nests. Sequential execution times and frequencies of execution of loops are determined by profiling, although they could also be estimated by compile-time analysis.
Reference: [2] <author> Vasanth Balasundaram, Geoffrey Fox, Ken Kennedy, and Ulrich Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practce of Parallel Programming, </booktitle> <year> 1991. </year>
Reference-contexts: In comparison, we use a heuristic for finding the optimal dynamic decomposition. Sophisticated cost models have been developed by Chatterjee et. al [4], Sussman [12], and Balasundaram et al. <ref> [2] </ref>. Sussman's cost model works for programs with simple loop-nests. In comparison, we are able to handle codes with complex control structures. Balasundaram et al. introduced the idea of a training set, which contain a cost model for computation and communication.
Reference: [3] <author> Robert Bixby, Ken Kennedy, and Ulrich Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93349-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Knobe et al. have proposed a framework that is similar to the CAG by Li and Chen [8]. However, unlike our PAL program representation, the CAG does not capture opportunities for dynamic distribution and does not have an underlying performance model. Kremer et al. <ref> [7, 3] </ref> modeled the dynamic decomposition problem as an explicit search space that contains all the candidate decompositions for each phase in a program. The optimal dynamic decomposition is formulated as a 0-1 integer linear programming problem.
Reference: [4] <author> Siddhartha Chatterjee, John Gilbert, Robert Schreiber, and Thomas Sheffler. </author> <title> Modeling data-parallel programs with the alignment-distribution graph. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> to appear. </note>
Reference-contexts: The weakness of this approach is that for large programs, the integer programming problem can take unduly large time to find the solution. In comparison, we use a heuristic for finding the optimal dynamic decomposition. Sophisticated cost models have been developed by Chatterjee et. al <ref> [4] </ref>, Sussman [12], and Balasundaram et al. [2]. Sussman's cost model works for programs with simple loop-nests. In comparison, we are able to handle codes with complex control structures. Balasundaram et al. introduced the idea of a training set, which contain a cost model for computation and communication.
Reference: [5] <author> T.M. Eidson and G. Erlebacher. </author> <title> Implementation of a fully-balanced periodic tridiagonal solver on a parallel distributed memory architecture. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 7(4) </volume> <pages> 273-302, </pages> <year> 1995. </year>
Reference-contexts: For processors sizes 10 to 16, the predicted values differed up to 4 i 10% from the actual values. A likely explanation for this is the contention in the SP2 high--performance switch during collective communication. Erlebacher The Erlebacher benchmark program implements a tridiagonal solver for calculating variable derivatives <ref> [5] </ref>. The program consists of 600 lines of Fortran code and has four three-dimensional data arrays of size 128x128x128. This program consists of three symmetrical phases, one along each of the three dimensions.
Reference: [6] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Support for Scalable Multiprocessors, </title> <address> New York, 1991. </address> <publisher> Elsevier. </publisher>
Reference-contexts: A decomposition-flow edge represents potential dynamic decomposition for an array. The reorganization of data is done at runtime, hence this cost contributes to the total execution time of the program. Our analysis for computing reaching decompositions 3 is similar to that implemented in the Fortran D compiler <ref> [6] </ref>. with performance data obtained by running the program on a uniprocessor system. The nodes in the graph are annotated with the execution time per iteration, and the number of times the corresponding loop nest is visited (trip count) during the execution of the program.
Reference: [7] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: Knobe et al. have proposed a framework that is similar to the CAG by Li and Chen [8]. However, unlike our PAL program representation, the CAG does not capture opportunities for dynamic distribution and does not have an underlying performance model. Kremer et al. <ref> [7, 3] </ref> modeled the dynamic decomposition problem as an explicit search space that contains all the candidate decompositions for each phase in a program. The optimal dynamic decomposition is formulated as a 0-1 integer linear programming problem.
Reference: [8] <author> Jingke Li and Marina Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Proceedings of the Third Symposium on the Frontiers of Massively Parallel Computation (Frontiers'90), </booktitle> <pages> pages 424-433, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Finally, we discuss the accuracy of the model for several standard benchmarks. 1.1 Related Work Several different representations for available parallelism have been proposed and studied, such as the component-affinity graph (CAG). Li and Chen were the first to use CAG for modeling the alignment problem <ref> [8, 9] </ref>. They proposed a heuristic for finding the optimal partitioning of a CAG by selecting one index domain at a time for alignment. Knobe et al. have proposed a framework that is similar to the CAG by Li and Chen [8]. <p> They proposed a heuristic for finding the optimal partitioning of a CAG by selecting one index domain at a time for alignment. Knobe et al. have proposed a framework that is similar to the CAG by Li and Chen <ref> [8] </ref>. However, unlike our PAL program representation, the CAG does not capture opportunities for dynamic distribution and does not have an underlying performance model.
Reference: [9] <author> Jingke Li and Marina Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: Finally, we discuss the accuracy of the model for several standard benchmarks. 1.1 Related Work Several different representations for available parallelism have been proposed and studied, such as the component-affinity graph (CAG). Li and Chen were the first to use CAG for modeling the alignment problem <ref> [8, 9] </ref>. They proposed a heuristic for finding the optimal partitioning of a CAG by selecting one index domain at a time for alignment. Knobe et al. have proposed a framework that is similar to the CAG by Li and Chen [8].
Reference: [10] <author> R. Mirchandaney, J.H. Saltz, R.M. Smith, K. Crowley, </author> <title> and D.M. Nicol. </title> <booktitle> Principles of run--time support for parallel processors. In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The restriction to a message passing style communication could be removed by a more elaborate, or platform dependent, cost function for communication. The restriction to regular dense-matrix style codes could be relaxed by incorporating a run-time calculation of data and computation distributions, using an inspector-executor <ref> [10] </ref> model for iterative computations.
Reference: [11] <author> T. J. Sheffler, R. Schreiber, W. Pugh, J. R. Gilbert, and S. Chatterjee. </author> <title> Efficient distribution analysis via graph contraction. </title> <booktitle> In 8th International Workshop on Languages and Compilers for Parallel Computing, volume 1033 of LNCS, </booktitle> <pages> pages 377-391. </pages> <publisher> Springer, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: The optimal dynamic decomposition is formulated as a 0-1 integer linear programming problem. However, the complexity of their optimization algorithms is exponential in the size of the search space. However, algorithms for phase compaction <ref> [11] </ref> and pruning of search space are developed to 2 improve the performance of the optimization algorithms. The weakness of this approach is that for large programs, the integer programming problem can take unduly large time to find the solution.
Reference: [12] <author> Alan Sussman. </author> <title> Model-driven mapping onto distributed memory parallel computers. </title> <booktitle> In Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: The weakness of this approach is that for large programs, the integer programming problem can take unduly large time to find the solution. In comparison, we use a heuristic for finding the optimal dynamic decomposition. Sophisticated cost models have been developed by Chatterjee et. al [4], Sussman <ref> [12] </ref>, and Balasundaram et al. [2]. Sussman's cost model works for programs with simple loop-nests. In comparison, we are able to handle codes with complex control structures. Balasundaram et al. introduced the idea of a training set, which contain a cost model for computation and communication.
Reference: [13] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: parallel A loop is considered to be parallel if there are no loop-carried dependences in the loop. pipelined-parallel A loop is considered to be pipelined parallel, if it has one or more loop-carried dependencies in the loop and belongs to a set of nested loops that are fully per mutable <ref> [13] </ref>. serial A loop is considered to be serial if it is neither parallel nor pipelined-parallel according to the criteria stated above. Our intermediate representation of a program is called the (Parallelism And Locality) (PAL) graph.
References-found: 13

