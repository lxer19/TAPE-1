URL: http://www.cm.deakin.edu.au/~zijian/Papers/sascmb-tr-C98-13.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: Stochastic Attribute Selection Committees with Multiple Boosting: Learning More Accurate and More Stable Classifier Committees  
Author: Zijian Zheng and Geoffrey I. Webb 
Keyword: committee learning, boosting, decision tree learning, classification learning, data mining, machine learning.  
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C98/13)  
Email: fzijian,webbg@deakin.edu.au  
Date: May, 1998  
Abstract: Approaches to learning classifier committees, including Boosting, Bagging, Sasc, and SascB, have demonstrated great success in increasing the prediction accuracy of decision trees. This type of technique generates several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Boosting and Bagging create different classifiers by modifying the distribution of the training set. Sasc (Stochastic Attribute Selection Committees) adopts a different method. It generates committees by stochastic manipulation of the set of attributes considered at each node during tree induction, but keeping the distribution of the training set unchanged. SascB, a combination of Boosting and Sasc, has been shown to be able to further increase, on average, the prediction accuracy of decision trees. It has been found that the performance of SascB and Boosting is more variable than that of Sasc, although SascB is more accurate than the others on average. In this paper, we present a novel method to reduce variability of SascB and Boosting, and further increase their average accuracy. It generates multiple committees by incorporating Bagging into SascB. As well as improving stability and average accuracy, the resulting method is amenable to parallel or distributed processing, while Boosting and SascB are not. This is an important characteristic for datamining in large datasets. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K.M. </author> <year> 1996. </year> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> Ph.D. </type> <institution> diss., Dept of Info. and Computer Science, Univ. of California, Irvine. </institution>
Reference: <author> Bauer, E. and Kohavi, R. </author> <year> 1998. </year> <title> An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants. </title> <note> Submitted to Machine Learning (available at http://reality.sgi.com/ronnyk/vote.ps.gz). </note>
Reference: <author> Breiman, L. </author> <year> 1996a. </year> <title> Bagging Predictors. </title> <booktitle> Machine Learning 24: </booktitle> <pages> 123-140. </pages>
Reference-contexts: If classifiers in a committee partition the instance space differently, and most points in the instance space are correctly covered by the majority of the committee, then the committee has a lower error rate than the individual classifiers. Bagging <ref> (Breiman 1996a) </ref> and Boosting (Schapire 1990; Freund and Schapire 1996a; 1996b; Freund 1996; Schapire et al. 1997), as two representative methods of this type, can significantly decrease the error rate of decision tree learning (Quinlan 1996; Freund and Schapire 1996b; Bauer and Kohavi 1998).
Reference: <author> Breiman, L. </author> <year> 1996b. </year> <note> Arcing Classifiers. Technical Report (available at http://www.stat. Berkeley.EDU/users/breiman/). </note> <institution> Dept of Statistics, Univ of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. </author> <year> 1984. </year> <title> Classification And Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Chan, P., Stolfo, S., and Wolpert, D. </author> <year> 1996. </year> <title> Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <address> Oregon. </address>
Reference: <author> Dietterich, T.G. and Bakiri, G. </author> <year> 1995. </year> <title> Solving Multiclass Learning Problems via Error-correcting Output Codes. </title> <journal> Journal of Artificial Intelligence Research 2: </journal> <pages> 263-286. </pages>
Reference-contexts: In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Dietterich, T.G. and Kong, E.B. </author> <year> 1995. </year> <title> Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms. </title> <type> Technical Report, </type> <institution> Dept of Computer Science, Ore-gon State University, Corvallis, Oregon (available at ftp://ftp.cs.orst.edu/pub/tgd/papers/ tr-bias.ps.gz). </institution>
Reference-contexts: In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Dietterich, T.G. </author> <year> 1997. </year> <journal> Machine Learning Research. AI Magazine 18: </journal> <pages> 97-136. </pages>
Reference-contexts: Each attribute set is selected stochastically. Experiments show that Sasc, like Boosting, can also significantly reduce the error rate 1 Committees are also referred to as ensembles <ref> (Dietterich 1997) </ref>. 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms (Zheng and Webb 1998). In addition, Sasc is more stable than Boosting (Zheng and Webb 1998).
Reference: <author> Domingos, P. </author> <year> 1997. </year> <title> Why does Bagging Work? a Bayesian Account and its Implications. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 155-158. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Freund, Y. </author> <year> 1996. </year> <title> Boosting a Weak Learning Algorithm by Majority. </title> <booktitle> Information and Computation 121(2): </booktitle> <pages> 256-285. </pages>
Reference: <author> Freund, Y. and Schapire, R.E. </author> <year> 1996a. </year> <title> A Decision-theoretic Generalization of On-line Learning and an Application to Boosting. </title> <note> Unpublished manuscript (available at http://www. research.att.com/~yoav). </note>
Reference: <author> Freund, Y. and Schapire, R.E. </author> <year> 1996b. </year> <title> Experiments with a New Boosting Algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 148-156. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model Selection. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1137-1143. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This test suite covers a wide variety of different domains with respect to dataset size, the number of classes, the number of attributes, and types of attributes. In every domain, two stratified 10-fold cross-validations <ref> (Kohavi 1995) </ref> were carried out for each algorithm. The result reported for each algorithm in each domain is an average value over 20 trials. All the algorithms are run on the same training and test set 8 Table 1: Description of learning tasks Domain Size No. of No. of Att.
Reference: <author> Kwok, S.W. and Carter, C. </author> <year> 1990. </year> <title> Multiple Decision Trees. </title> <editor> In Schachter, R.D., Levitt, T.S., Kanal, L.N., and Lemmer, J.F. (eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> 327-335. </pages> <address> Elsevier Science. </address> <note> 14 Merz, C.J. and Murphy, P.M. 1997. UCI Repository of machine learning databases [http:// www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </note> <institution> CA: Univ of California, Dept of Info and Computer Science. </institution>
Reference-contexts: In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters <ref> (Kwok and Carter 1990) </ref>, error-correcting output codes (Dietterich and Bakiri 1995), and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc (Zheng and Webb 1998). Reviews of related methods are provided in Dietterich (1997) and Ali (1996).
Reference: <author> Quinlan, J.R. </author> <year> 1993. </year> <title> C4.5: Program for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 <ref> (Quinlan 1993) </ref>. Let w t (x) denote the weight of instance x in D at trial t. At the first trial, each instance has weight 1; that is, w 1 (x) = 1 for each x. <p> 3 To make the algorithm efficient, this step is limited to 10 fi T times. 4 2.2 S ASC During the growth of a decision tree, at each decision node, a decision tree learning algorithm searches for the best attribute to form a test based on some test selection functions <ref> (Quinlan 1993) </ref>. The key idea of Sasc is to vary the members of a decision tree committee by stochastic manipulation of the set of attributes available for selection at decision nodes. This creates decision trees that each partition the instance space differently. <p> In order to have a good quality tree in the sense that it can correctly cover most parts of the instance space, the tests used at decision nodes should be as good as possible with respect to the test selection function employed. We use C4.5 <ref> (Quinlan 1993) </ref> with the modifications described below as the base classifier learning algorithm in Sasc. When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test (Quinlan 1993). <p> We use C4.5 <ref> (Quinlan 1993) </ref> with the modifications described below as the base classifier learning algorithm in Sasc. When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test (Quinlan 1993). To force C4.5 to generate different trees using the same training set, we modified C4.5 by stochastically restricting the set of attributes available for selection at a decision node. <p> This is performed by tracing the example down to a leaf of the tree. The class distribution for the example is estimated using the proportion of the training examples of each class at the leaf, if the leaf is not empty. This is the same as in C4.5 <ref> (Quinlan 1993) </ref>. When the leaf contains no training examples, C4.5 produces a class distribution with the labeled class of the leaf having the probability 1, and all other classes having the probability 0. In this case, the three committee learning algorithms are different from C4.5.
Reference: <author> Quinlan, J.R. </author> <year> 1996. </year> <title> Bagging, Boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: The key idea of Boosting was presented in Section 1. Here, we describe our implementation of the Boosting algorithm with decision tree learning, called Boost. It follows the Boosted C4.5 algorithm (Ad-aBoost.M1) <ref> (Quinlan 1996) </ref> but uses a new Boosting equation as shown in Equation 1, derived from Schapire et al. (1997). Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 (Quinlan 1993).
Reference: <author> Schapire, R.E. </author> <year> 1990. </year> <title> The Strength of Weak Learnability. </title> <booktitle> Machine Learning 5: </booktitle> <pages> 197-227. </pages>
Reference: <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee, W.S. </author> <year> 1997. </year> <title> Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning, </booktitle> <pages> 322-330. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Webb, G.I. </author> <year> 1998. </year> <title> Idealized Models of Decision Committee Performance and Their Application to Reduce Committee Error. </title> <type> Tech Report (TR C98/11), </type> <institution> School of Computing and Mathematics, Deakin University, </institution> <note> Australia (available at http://www3.cm.deakin.edu.au/~webb/ papers/TRC9811.ps.Z). </note>
Reference-contexts: Although Boosting is generally more accurate than Bagging, the performance of Boosting is more variable than that of Bagging (Quinlan 1996; Bauer and Kohavi 1998). As an alternative approach to generating different classifiers to form a committee, Sasc (Stochastic Attribute Selection Committees) <ref> (Zheng and Webb 1998) </ref> builds different classifiers by modifying the set of attributes considered at each node, while the distribution of the training set is kept unchanged. Each attribute set is selected stochastically. <p> Experiments show that Sasc, like Boosting, can also significantly reduce the error rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes (Dietterich and Bakiri 1995), and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc <ref> (Zheng and Webb 1998) </ref>. Reviews of related methods are provided in Dietterich (1997) and Ali (1996). A collection of recent research in this area can be found from Chan et al. (1996). <p> Base on the observation that both Boosting and Sasc can significantly increase the prediction accuracy of decision trees but through different mechanisms, we developed another technique to further improve the accuracy of decision trees <ref> (Zheng, Webb, and Ting 1998) </ref>. The new approach is called SASCB (Stochastic Attribute Selection Committees with Boosting), a combination of the Boosting and Sasc techniques. SascB has been shown to be able to outperform, on average, either Sasc or Boosting alone in terms of lower error rate. <p> In the light of these findings, in this paper, we present a novel approach, namely SASCMB (Stochastic Attribute Selection Committees with Multiple Boosting), to improving the stability and average accuracy of SascB and Boosting. It generates multiple committees by incorporating Bagging into SascB using the multi-boosting technique <ref> (Webb 1998) </ref>. We expect that splitting one committee into multiple committees, with each committee being created from a bootstrap sample of the training set, can reduce the variability of Boosting and SascB, since the Boosting process is broken down into several small processes.
Reference: <author> Zheng, Z. and Webb, G.I. </author> <year> 1998. </year> <title> Stochastic Attribute Selection Committees. </title> <type> Technical Report (TR C98/08), </type> <institution> School of Computing and Mathematics, </institution> <note> Deakin University (available at http://www3.cm.deakin.edu.au/~zijian/Papers/sasc-tr-C98-08.ps.gz). </note>
Reference-contexts: Although Boosting is generally more accurate than Bagging, the performance of Boosting is more variable than that of Bagging (Quinlan 1996; Bauer and Kohavi 1998). As an alternative approach to generating different classifiers to form a committee, Sasc (Stochastic Attribute Selection Committees) <ref> (Zheng and Webb 1998) </ref> builds different classifiers by modifying the set of attributes considered at each node, while the distribution of the training set is kept unchanged. Each attribute set is selected stochastically. <p> Experiments show that Sasc, like Boosting, can also significantly reduce the error rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes (Dietterich and Bakiri 1995), and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc <ref> (Zheng and Webb 1998) </ref>. Reviews of related methods are provided in Dietterich (1997) and Ali (1996). A collection of recent research in this area can be found from Chan et al. (1996). <p> Base on the observation that both Boosting and Sasc can significantly increase the prediction accuracy of decision trees but through different mechanisms, we developed another technique to further improve the accuracy of decision trees <ref> (Zheng, Webb, and Ting 1998) </ref>. The new approach is called SASCB (Stochastic Attribute Selection Committees with Boosting), a combination of the Boosting and Sasc techniques. SascB has been shown to be able to outperform, on average, either Sasc or Boosting alone in terms of lower error rate.
Reference: <author> Zheng, Z., Webb, G.I., and Ting, K.M. </author> <year> 1998. </year> <title> Integrating Boosting and Stochastic Attribute Selection Committees for Further Improving the Performance of Decision Tree Learning. </title> <type> Technical Report (TR C98/12), </type> <institution> School of Computing and Mathematics, Deakin University, </institution> <note> Australia (available at http://www3.cm.deakin.edu.au/~zijian/Papers/sascb-tr-C98-12.ps.gz). </note>
Reference-contexts: Although Boosting is generally more accurate than Bagging, the performance of Boosting is more variable than that of Bagging (Quinlan 1996; Bauer and Kohavi 1998). As an alternative approach to generating different classifiers to form a committee, Sasc (Stochastic Attribute Selection Committees) <ref> (Zheng and Webb 1998) </ref> builds different classifiers by modifying the set of attributes considered at each node, while the distribution of the training set is kept unchanged. Each attribute set is selected stochastically. <p> Experiments show that Sasc, like Boosting, can also significantly reduce the error rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> rate 1 Committees are also referred to as ensembles (Dietterich 1997). 2 Breiman (1996b) refers to Boosting as the arcing (adaptively resample and combine) method. 2 of decision tree learning, although the two techniques use quite different mechanisms <ref> (Zheng and Webb 1998) </ref>. In addition, Sasc is more stable than Boosting (Zheng and Webb 1998). <p> There are some other classifier committee learning approaches such as generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes (Dietterich and Bakiri 1995), and generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996) which is similar to Sasc <ref> (Zheng and Webb 1998) </ref>. Reviews of related methods are provided in Dietterich (1997) and Ali (1996). A collection of recent research in this area can be found from Chan et al. (1996). <p> Base on the observation that both Boosting and Sasc can significantly increase the prediction accuracy of decision trees but through different mechanisms, we developed another technique to further improve the accuracy of decision trees <ref> (Zheng, Webb, and Ting 1998) </ref>. The new approach is called SASCB (Stochastic Attribute Selection Committees with Boosting), a combination of the Boosting and Sasc techniques. SascB has been shown to be able to outperform, on average, either Sasc or Boosting alone in terms of lower error rate.
References-found: 22

