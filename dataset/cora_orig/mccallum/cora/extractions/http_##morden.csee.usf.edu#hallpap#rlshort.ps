URL: http://morden.csee.usf.edu/hallpap/rlshort.ps
Refering-URL: http://daffy.csee.usf.edu/ailab/hall.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: hall@csee.usf.edu  
Title: AVERAGED REWARD REINFORCEMENT LEARNING APPLIED TO FUZZY RULE TUNING  
Author: Lawrence O. Hall and Michael A. Pokorny 
Address: Tampa, Fl. 33620  
Affiliation: Department of Computer Science and Engineering University of South Florida  
Abstract: Fuzzy rules for control can be effectively tuned via reinforcement learning. Reinforcement learning is a weak learning method, which only requires information on the success or failure of the control application. The tuning process allows people to generate fuzzy rules which are unable to accurately perform control and have them tuned to be rules which provide smooth control. This paper explores a new simplified method of using reinforcement learning for the tuning of fuzzy control rules. It is shown that the learned fuzzy rules provide smoother control in the pole balancing domain than another approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Berenji and P. Khedkar, </author> <title> "Learning and Tuning Fuzzy Controllers Through Reinforcements", </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp 724-740, </pages> <year> 1992. </year>
Reference-contexts: Previous work has shown that the process of tuning an existing set of fuzzy rules may be automated by the use of reinforcement learning <ref> [1, 4] </ref>. In this paper, we introduce a simple model of reinforcement learning which is able to effectively tune an initial set of fuzzy rules. The initial fuzzy rules can contain fuzzy sets that are far from optimum. <p> A limited set of rule firings has equal and average (negative) reward applied to it. There are no changes made to fuzzy sets until a failure occurs. Our approach does not use a critic to predict the rewards in each continuous state <ref> [1, 11] </ref>. Also, each action recommended by the fuzzy rules is taken with no modifications. The only exploration in the state space comes from different random starting configurations after failures have occurred. Even with these simplifications, we will show that the rules tuned by our approach are effective and robust. <p> AVERAGED REWARD APPROACH Only in the case of failure are the fuzzy sets making up the antecedent and consequent parts of a given rule set modified. The actual rules with their associated antecedent and consequent fuzzy sets are represented in a fuzzy neural network type <ref> [1] </ref> initially proposed by Berenji. This neural network provides a continuous action value. We will discuss modifications to the weight updating algorithm for the network. <p> We will discuss modifications to the weight updating algorithm for the network. The reinforcement learning approach described here uses gradient de-scent in conjunction with an averaged reward to train the action selection network (ASN), shown in Figure 1. A typical rule is also shown in Figure 1. Following <ref> [1] </ref> the differentiable soft min is used in place of and in the antecedent and the "Local Mean of Maxima" or LMOM is used for defuzzification. <p> As k ! inf, the soft-min becomes the usual min operation. We use k = 6. The LMOM is differentiable for use with gradient descent while de-fuzzification strategies such as center of gravity and mean of maxima are not <ref> [1, 12] </ref>. LMOM is applied locally as each rule fires, essentially combining the rule firing and defuzzification stages. The final control output is then a simple summation of the individual defuzzified outputs for each fired rule. <p> There are two major differences from the approach of Berenji and Chung <ref> [1, 4] </ref>. RLTFR does not contain a critic or action evaluation network (AEN) and there is no stochastic action modifier (SAM) to provide exploration of the space. Instead adequate exploration is achieved by using random starting positions after failures in RLTFR. <p> Instead adequate exploration is achieved by using random starting positions after failures in RLTFR. The direction of failure and a window of recent actions allow the AEN to be deleted. In <ref> [1, 4] </ref> the AEN is itself a neural network that is trained over a set of simulations or trials. The deletion of an AEN allows RLTFR to learn fast (only 1 network is trained) and our results show that it can learn accurately in complex domains. <p> The RLTFR system update equations for the consequent fuzzy sets at each step are derived in the same way as Berenji showed in <ref> [1] </ref>. The only difference is the inclusion of direction information. <p> For an input with full membership in a fuzzy set only the location (center point), not the shape, of the fuzzy set is modified. 3. POLE BALANCING The learning/tuning of the fuzzy sets used by the rule set from <ref> [1] </ref> which creates a fuzzy logic controller (FLC) for the cart-pole problem is discussed below in the context of controller stability and final oscillation. Stability of the FLC refers to whether or not it is capable of doing its assigned task. <p> Final oscillation refers to the amount of movement within the acceptable limits of the pole position and cart position. The smaller final oscillations, the better the FLC. Comparisons will be made with the results in <ref> [1] </ref>. We have done a complete set of comparative experiments [8], but show just a representative subset of them here. The cart-pole problem was simulated by the Euler method and details can be found in [1, 8]. For testing purposes the time slice, t, was set to 0.02 seconds. <p> Comparisons will be made with the results in [1]. We have done a complete set of comparative experiments [8], but show just a representative subset of them here. The cart-pole problem was simulated by the Euler method and details can be found in <ref> [1, 8] </ref>. For testing purposes the time slice, t, was set to 0.02 seconds. The output force applied to the cart as determined by the FLC is represented by f . <p> The initial descriptions of the fuzzy sets and the 13 fuzzy rules used are given in <ref> [8, 1] </ref>. The initial fuzzy sets are not sufficient to balance the pole. The simulation parameters p and c are set to zero (the frictionless case) which is implied in [1, 4]. The cart-pole simulator has two independent failure conditions. <p> The initial descriptions of the fuzzy sets and the 13 fuzzy rules used are given in [8, 1]. The initial fuzzy sets are not sufficient to balance the pole. The simulation parameters p and c are set to zero (the frictionless case) which is implied in <ref> [1, 4] </ref>. The cart-pole simulator has two independent failure conditions. The pole may fall beyond a desired angle and the cart may go off the end of the track. For comparison purposes with [1] jj &gt; 12 o is one failure and x &gt; 2:4 meters is the other failure. <p> The cart-pole simulator has two independent failure conditions. The pole may fall beyond a desired angle and the cart may go off the end of the track. For comparison purposes with <ref> [1] </ref> jj &gt; 12 o is one failure and x &gt; 2:4 meters is the other failure. RLTFR gets information about the failure direction upon failure. So if the pole falls to the right or the cart goes off the right end of the track, a "right" failure occurs. <p> The resulting rule set will be robust. The averaged a b reward approach presented here does not require that an online critic be concurrently trained as in <ref> [1, 4] </ref>. This makes the learning time faster, as the time required to learn per trial is less. There is the drawback that the system can only update itself after a failure, where a good critic would allow (possibly) correction before failure. <p> The RLTFR system has been shown able to learn in less trials in many cases than a reinforcement learner that has a critic [8]. One of the reasons for this involves the fact that each time an action is recommended RLTFR follows it, where in <ref> [1] </ref> a different action (possibly bad) might be taken early on to help explore the state space. State exploration through random restarts seems both effective and more efficient in some cases.
Reference: [2] <author> J.J. Buckley, </author> <title> "Universal Fuzzy Controllers", </title> <journal> Automatica, V. </journal> <volume> 28, No. 6, </volume> <pages> pp. 1245-1248, </pages> <year> 1992. </year>
Reference-contexts: 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications [7, 10, 9]. The use of fuzzy rules has also been shown to provide universal function approximation <ref> [2] </ref>. Efforts to learn fuzzy control rules from data [10, 5] have been spawned by the observation that they are useful and non-trivial to develop by hand.
Reference: [3] <author> P. Cichosz, </author> <title> "Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning", </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> no. 2, </volume> <pages> pp. 286-318, </pages> <year> 1995. </year>
Reference: [4] <author> H. Chung and C. Chiang, </author> <title> "A Self-Learning and Tuning Fuzzy Logic Controller Employing Reinforcement Learning". </title> <note> article submitted to MITA Press, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Previous work has shown that the process of tuning an existing set of fuzzy rules may be automated by the use of reinforcement learning <ref> [1, 4] </ref>. In this paper, we introduce a simple model of reinforcement learning which is able to effectively tune an initial set of fuzzy rules. The initial fuzzy rules can contain fuzzy sets that are far from optimum. <p> There are two major differences from the approach of Berenji and Chung <ref> [1, 4] </ref>. RLTFR does not contain a critic or action evaluation network (AEN) and there is no stochastic action modifier (SAM) to provide exploration of the space. Instead adequate exploration is achieved by using random starting positions after failures in RLTFR. <p> Instead adequate exploration is achieved by using random starting positions after failures in RLTFR. The direction of failure and a window of recent actions allow the AEN to be deleted. In <ref> [1, 4] </ref> the AEN is itself a neural network that is trained over a set of simulations or trials. The deletion of an AEN allows RLTFR to learn fast (only 1 network is trained) and our results show that it can learn accurately in complex domains. <p> The initial descriptions of the fuzzy sets and the 13 fuzzy rules used are given in [8, 1]. The initial fuzzy sets are not sufficient to balance the pole. The simulation parameters p and c are set to zero (the frictionless case) which is implied in <ref> [1, 4] </ref>. The cart-pole simulator has two independent failure conditions. The pole may fall beyond a desired angle and the cart may go off the end of the track. For comparison purposes with [1] jj &gt; 12 o is one failure and x &gt; 2:4 meters is the other failure. <p> The resulting rule set will be robust. The averaged a b reward approach presented here does not require that an online critic be concurrently trained as in <ref> [1, 4] </ref>. This makes the learning time faster, as the time required to learn per trial is less. There is the drawback that the system can only update itself after a failure, where a good critic would allow (possibly) correction before failure.
Reference: [5] <author> Jang, J.-S. R. and C.-T. Sun, "ANFIS: </author> <title> Adaptive-Network-based Fuzzy Inference Systems", </title> <journal> IEEE Transaction on Systems, Man and Cybernetics, V. </journal> <volume> 23, No. 3, </volume> <pages> pp. 665-685. </pages>
Reference-contexts: 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications [7, 10, 9]. The use of fuzzy rules has also been shown to provide universal function approximation [2]. Efforts to learn fuzzy control rules from data <ref> [10, 5] </ref> have been spawned by the observation that they are useful and non-trivial to develop by hand. It has been shown that a set of fuzzy rules may be generated by hand and then tuned in a time consuming process to make excellent fuzzy controllers [9, 7].
Reference: [6] <author> S. Mahadevan, </author> <title> "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results", </title> <journal> Machine Learning, Special Issue on Reinforcement Learning (edited by Leslie Kae-bling), </journal> <volume> vol. 22, </volume> <pages> pp. 159-196, </pages> <year> 1996. </year>
Reference-contexts: A reward value of 1 min (m;n) , where n is the total number of steps for a simulation, is given to each action in the circular buffer. This is a strictly averaged reward scheme and is a weak method of error assignment <ref> [6] </ref>. RLTFR begins with the last step of the failed trial and works towards the earliest step in the circular buffer. RLTFR calculates the amount to change each fuzzy set's location and shape and stores the values in each set's global delta (change) parameters.
Reference: [7] <author> F. McNeill, </author> <title> Fuzzy Logic A Practical Approach. Boston: AP Professional, </title> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications <ref> [7, 10, 9] </ref>. The use of fuzzy rules has also been shown to provide universal function approximation [2]. Efforts to learn fuzzy control rules from data [10, 5] have been spawned by the observation that they are useful and non-trivial to develop by hand. <p> It has been shown that a set of fuzzy rules may be generated by hand and then tuned in a time consuming process to make excellent fuzzy controllers <ref> [9, 7] </ref>. Previous work has shown that the process of tuning an existing set of fuzzy rules may be automated by the use of reinforcement learning [1, 4].
Reference: [8] <author> M.A. Pokorny, </author> <title> "A Method of Tuning Fuzzy Sets by Reinforcement Learning", </title> <type> Master's Thesis, </type> <institution> Department of Computer Science and Engineering, Univ. of South Florida, </institution> <address> Tampa, Fl., </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Final oscillation refers to the amount of movement within the acceptable limits of the pole position and cart position. The smaller final oscillations, the better the FLC. Comparisons will be made with the results in [1]. We have done a complete set of comparative experiments <ref> [8] </ref>, but show just a representative subset of them here. The cart-pole problem was simulated by the Euler method and details can be found in [1, 8]. For testing purposes the time slice, t, was set to 0.02 seconds. <p> Comparisons will be made with the results in [1]. We have done a complete set of comparative experiments [8], but show just a representative subset of them here. The cart-pole problem was simulated by the Euler method and details can be found in <ref> [1, 8] </ref>. For testing purposes the time slice, t, was set to 0.02 seconds. The output force applied to the cart as determined by the FLC is represented by f . <p> The initial descriptions of the fuzzy sets and the 13 fuzzy rules used are given in <ref> [8, 1] </ref>. The initial fuzzy sets are not sufficient to balance the pole. The simulation parameters p and c are set to zero (the frictionless case) which is implied in [1, 4]. The cart-pole simulator has two independent failure conditions. <p> The results of this experiment are shown in Figures 3 a and b, which show that the rules are quite robust, as they were able to re-balance the pole after force application. 4. SUMMARY The result shown here, and others <ref> [8] </ref> we could not show due to space constraints, from the cart-pole balancing domain show that our simplified method of reinforcement learning can be used to tune the fuzzy sets of fuzzy rules. The resulting rule set will be robust. <p> There is the drawback that the system can only update itself after a failure, where a good critic would allow (possibly) correction before failure. The RLTFR system has been shown able to learn in less trials in many cases than a reinforcement learner that has a critic <ref> [8] </ref>. One of the reasons for this involves the fact that each time an action is recommended RLTFR follows it, where in [1] a different action (possibly bad) might be taken early on to help explore the state space.
Reference: [9] <author> T. Procyk and E. Mamdani, </author> <title> "A Linguistic Self-Organizing Process Controller", </title> <journal> Automatica, </journal> <volume> vol. 15, no. 1, </volume> <pages> pp. 15-30, </pages> <year> 1979. </year>
Reference-contexts: 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications <ref> [7, 10, 9] </ref>. The use of fuzzy rules has also been shown to provide universal function approximation [2]. Efforts to learn fuzzy control rules from data [10, 5] have been spawned by the observation that they are useful and non-trivial to develop by hand. <p> It has been shown that a set of fuzzy rules may be generated by hand and then tuned in a time consuming process to make excellent fuzzy controllers <ref> [9, 7] </ref>. Previous work has shown that the process of tuning an existing set of fuzzy rules may be automated by the use of reinforcement learning [1, 4].
Reference: [10] <author> M. Sugeno, </author> <title> "A Fuzzy-Logic-Based Approach to Qualitative Modeling", </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 7-31, </pages> <year> 1993. </year>
Reference-contexts: 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications <ref> [7, 10, 9] </ref>. The use of fuzzy rules has also been shown to provide universal function approximation [2]. Efforts to learn fuzzy control rules from data [10, 5] have been spawned by the observation that they are useful and non-trivial to develop by hand. <p> 1. INTRODUCTION Fuzzy rules have been shown able to provide excellent control in a number of practical applications [7, 10, 9]. The use of fuzzy rules has also been shown to provide universal function approximation [2]. Efforts to learn fuzzy control rules from data <ref> [10, 5] </ref> have been spawned by the observation that they are useful and non-trivial to develop by hand. It has been shown that a set of fuzzy rules may be generated by hand and then tuned in a time consuming process to make excellent fuzzy controllers [9, 7].
Reference: [11] <author> R. Sutton, </author> <title> "Learning to predict by the methods of temporal differences", </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference-contexts: A limited set of rule firings has equal and average (negative) reward applied to it. There are no changes made to fuzzy sets until a failure occurs. Our approach does not use a critic to predict the rewards in each continuous state <ref> [1, 11] </ref>. Also, each action recommended by the fuzzy rules is taken with no modifications. The only exploration in the state space comes from different random starting configurations after failures have occurred. Even with these simplifications, we will show that the rules tuned by our approach are effective and robust.
Reference: [12] <author> H. Zimmermann, </author> <title> Fuzzy Set Theory and its Application. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: As k ! inf, the soft-min becomes the usual min operation. We use k = 6. The LMOM is differentiable for use with gradient descent while de-fuzzification strategies such as center of gravity and mean of maxima are not <ref> [1, 12] </ref>. LMOM is applied locally as each rule fires, essentially combining the rule firing and defuzzification stages. The final control output is then a simple summation of the individual defuzzified outputs for each fired rule.
References-found: 12

