URL: http://www.cs.washington.edu/homes/map/research/papers/ss95.ps
Refering-URL: http://www.cs.washington.edu/homes/map/research/publications.html
Root-URL: 
Email: fmap, etzionig@cs.washington.edu  
Phone: (206) 616-1845 Fax: (206) 543-2969  
Title: Category Translation: Learning to understand information on the Internet  
Author: Mike Perkowitz Oren Etzioni 
Keyword: machine learning, empirical  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington,  
Abstract: This paper investigates the problem of automatically learning declarative models of information sources available on the Internet. We report on ILA, a domain-independent program that learns the meaning of external information by explaining it in terms of internal categories. In our experiments, ILA starts with knowledge of local faculty members, and is able to learn models of the Internet service whois and of the personnel directories available at Berkeley, Brown, Caltech, Cornell, Rice, Rutgers, and UCI, averaging fewer than 40 queries per information source. ILA's hypothesis language is first-order conjunctions, and its bias is compactly encoded as a determination. We analyze ILA's sample complexity within the Valiant model, and using a probabilistic model specifically tailored to ILA. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D., and Laird, P. </author> <year> 1988. </year> <title> Learning from noisy examples. </title> <booktitle> Machine Learning 2(4) </booktitle> <pages> 343-370. </pages>
Reference-contexts: We can model the violation of these assumptions as random classification noise and use the bound due to <ref> (Angluin & Laird 1988) </ref>: n 2 2jHj ffi ), where b is an upper bound on the frequency of noisy classifications, and the learner chooses the hypothesis that is correct most often. Unfortunately, the number of queries suggested by the bound is very large.
Reference: <author> Etzioni, O., and Perkowitz, M. </author> <year> 1995. </year> <title> A probabilistic model of sample complexity. </title> <note> in preparation. </note>
Reference-contexts: The random-queries curve is derived from Equation 1; Due to lack of space, we omit the mathematics underlying the discriminating-queries curve, but see <ref> (Etzioni & Perkowitz 1995) </ref>. When A g = 0:95 and A b = 0:5, ILA requires only a single discriminating query to have 90% confidence that it has found the better hypothesis. <p> Also, under the worst-case assumption that n 1 hypotheses all have accuracy A b , a similar formula can be derived for a hypothesis space of size n, and for ILA's more complex evaluation function <ref> (Etzioni & Perkowitz 1995) </ref>. queries required to be 90% sure of choosing the better hypothesis, as a function of A b . The lower line is using discriminating queries; the upper line is without. In both graphs, A g is fixed at 0.95. formalizing its bias as a determination.
Reference: <author> Etzioni, O., and Weld, D. </author> <year> 1994. </year> <title> A softbot-based interface to the internet. </title> <journal> CACM 37(7) </journal> <pages> 72-76. </pages>
Reference-contexts: A number of more sophisticated AI systems have emerged including SIMS (Knoblock, Arens, & Hsu 1994), the Information Manifold at AT&T (Levy, Srivastava, & Kirk 1994), and the Internet softbot <ref> (Etzioni & Weld 1994) </ref>. However, each of these AI systems requires sophisticated declarative models of the different information sources it is fl We thank Dayne Freitag, Craig Knoblock, and Tom Mitchell for inspiring discussions that contributed to our problem formulation.
Reference: <author> Haussler, D. </author> <year> 1988. </year> <title> Quantifying inductive bias: AI learning algorithms and valiant's learning framework. </title> <booktitle> Artificial Intelligence 36(2) </booktitle> <pages> 177-221. </pages>
Reference-contexts: Haussler <ref> (Haussler 1988) </ref> derives a lower bound on the number of examples necessary for PAC learning. If h is any hypothesis that agrees with at least n queries from I, where n 1 jHj ffi ), then we have the following: P (E (h; I) ") 1 ffi.
Reference: <author> Knoblock, C., Arens, Y., and Hsu, C.-N. </author> <year> 1994. </year> <title> Cooperating agents for information retrieval. </title> <booktitle> In Proceedings of the Second International Conference on Cooperative Information Systems. </booktitle>
Reference-contexts: However, these tools are unable to interpret the results of their searches and unable to use multiple information sources in concert. A number of more sophisticated AI systems have emerged including SIMS <ref> (Knoblock, Arens, & Hsu 1994) </ref>, the Information Manifold at AT&T (Levy, Srivastava, & Kirk 1994), and the Internet softbot (Etzioni & Weld 1994).
Reference: <author> Levy, A. Y., Srivastava, D., and Kirk, T. </author> <year> 1994. </year> <title> The IM global information system: Data model and query evaluation. </title> <institution> AT&T Bell Laboratories Technical Report. </institution> <note> Submitted for publication. </note>
Reference-contexts: However, these tools are unable to interpret the results of their searches and unable to use multiple information sources in concert. A number of more sophisticated AI systems have emerged including SIMS (Knoblock, Arens, & Hsu 1994), the Information Manifold at AT&T <ref> (Levy, Srivastava, & Kirk 1994) </ref>, and the Internet softbot (Etzioni & Weld 1994). However, each of these AI systems requires sophisticated declarative models of the different information sources it is fl We thank Dayne Freitag, Craig Knoblock, and Tom Mitchell for inspiring discussions that contributed to our problem formulation.
Reference: <author> Rajamoney, S. </author> <year> 1993. </year> <title> The design of discrimination experiments. </title> <booktitle> Machine Learning 12(1/2/3). </booktitle>
Reference-contexts: In addition, ILA employs several heuristics to reduce the number of queries necessary to converge to a satisfactory model of the IS. Most important, ILA attempts to discriminate between two competing hypotheses by choosing an object for which the hypotheses make different predictions (cf. <ref> (Rajamoney 1993) </ref>). For example, if ILA has seen the record Oren Etzioni 685-3035 FR-35, it will consider both lastname and userid as hypotheses for the second field, because Et-zioni's userid is his last name.
Reference: <author> Richards, B. L., and Mooney, R. J. </author> <year> 1992. </year> <title> Learning relations by pathfinding. </title> <booktitle> In Proc. 10th Nat. Conf. on A.I., </booktitle> <pages> 50-55. </pages>
Reference-contexts: For example, in ILA's model, people are associated with departments and departments with mail-stops. The relation between a person and her mail-stop, then, is a composition of department and mail-stop | the mail-stop of P is mail-stop (department (P )). We employ a variant of relational pathfinding <ref> (Richards & Mooney 1992) </ref> to discover a relation between the query object and each response token. Richards and Mooney's pathfinding technique performs a bidirectional breadth-first search in which constants are nodes in the graph and attributes on constants are edges between nodes.
Reference: <author> Wiederhold, G. </author> <year> 1992. </year> <booktitle> Mediators in the architecture of future information systems. IEEE Computer 38-49. </booktitle>
Reference-contexts: We have identified several problems that ILA does not yet address. Category mismatch occurs when ILA fails to find categories corresponding to those of the external information source <ref> (Wiederhold 1992) </ref>. For example, the IS records fax numbers, of which ILA is ignorant. Token mismatch occurs when, despite having appropriate categories, ILA fails to find matching tokens due to a difference in representation.
Reference: <author> Wittgenstein, L. </author> <year> 1958. </year> <title> Philosophical Investigations. </title> <publisher> Macmillan Publishing Co., Inc. Translated by G.E.M. Anscombe. </publisher>
Reference-contexts: As a first step, this paper investigates the question of learning semantics. Our learning method is based on the following idea, due to St. Augustine <ref> (Wittgenstein 1958) </ref>. Consider how we might teach English nouns to an Israeli. If we do not know Hebrew, we are reduced to pointing at individual objects and saying "chair" or "tree." In doing so, we rely on three key assumptions.
References-found: 10

