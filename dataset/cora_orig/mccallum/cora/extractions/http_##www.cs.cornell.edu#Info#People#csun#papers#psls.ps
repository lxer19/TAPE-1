URL: http://www.cs.cornell.edu/Info/People/csun/papers/psls.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Title: PARALLEL SOLUTION OF SPARSE LINEAR LEAST SQUARES PROBLEMS ON DISTRIBUTED-MEMORY MULTIPROCESSORS 1  
Author: Chunguang Sun 
Keyword: Key words. parallel algorithms, sparse matrix, orthogonal factorization, multifrontal method, least squares problems, triangular solution, distributed-memory multiprocessors  Abbreviated title: Parallel Solution of Sparse Least Squares Problems  
Note: AMS subject classifications.  
Address: Ithaca, NY 14853-3801  Ithaca, NY,  
Affiliation: Advanced Computing Research Institute Cornell Theory Center Cornell University  Tech.  Advanced Computing Research Institute, Cornell Theory Center, Cornell University,  
Pubnum: Report CTC95TR212,  
Email: csun@cs.cornell.edu  
Web: 65F05, 65F50  
Date: May 1995.  
Abstract: This paper studies the solution of large-scale sparse linear least squares problems on distributed-memory multiprocessors. The method of corrected semi-normal equations is considered. New block-oriented parallel algorithms are developed for solving the related sparse triangular systems. The arithmetic and communication complexities of the new algorithms applied to regular grid problems are analyzed. The proposed parallel sparse triangular solution algorithms together with a block-oriented parallel sparse QR factorization algorithm result in a highly efficient block-oriented approach to the parallel solution of sparse linear least squares problems on distributed-memory multiprocessors. Performance of the block-oriented approach is demonstrated empirically through an implementation on an IBM Scalable POWERparallel system SP2. The largest problem solved has over two million rows and more than a quarter million columns. The execution speed for the numerical factorization of this problem achieves over 3.7 gigaflops per second on an IBM SP2 machine with 128 processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Bisseling and J. G. G. van de Vorst, </author> <title> Parallel triangular system solving on a mesh network of transputers, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12 (1991), </volume> <pages> pp. 787-799. </pages>
Reference-contexts: A parallel multifrontal sparse back substitution algorithm on a processor . 4. Block-oriented parallel dense triangular solution algorithms. A number of parallel algorithms have been designed for solving dense triangular systems of linear equations on distributed-memory multiprocessors <ref> [1, 4, 8, 10, 11, 17] </ref>. Most of the previous works on parallel triangular solution have assumed that a triangular matrix is distributed to processors by single columns or rows. A crucial step in the parallel implementation of the CSNE method is to solve R t Rx = c.
Reference: [2] <author> A. Bj orck, </author> <title> Stability analysis of the method of seminormal equations for linear least squares problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 88/89 (1987), </volume> <pages> pp. 31-48. </pages>
Reference-contexts: To avoid the explicit formation of A t A, the semi-normal equations R t Rx = A t b can be used to handle new right hand sides without storing Q. Bjorck <ref> [2] </ref> has shown that the method of semi-normal equations (SNE) is numerically unstable and proposed that a correction step or an iterative refinement step be added to the SNE method as follows: 8 &gt; : R t R4x = A t r; (3) y This work was supported in part by <p> funding from members of its Corporate Research Institute, the National Science Foundation (NSF), the Advanced Research Projects Agency (ARPA), the National Institutes of Health (NIH), New York State, and IBM corporation. z Advanced Computing Research Institute, Cornell Theory Center, Cornell University, Ithaca, NY 14853 3801 (e-mail:csun@cs.cornell.edu, phone:607-254-8863, fax:607-254-8888). 1 Bjorck <ref> [2] </ref> has shown that this new method, the method of corrected semi-normal equations (CSNE), is in general as accurate as the QR factorization method. However, it is not always backward stable and may not be accurate for problems with widely different row norms.
Reference: [3] <author> E. Chu and J. A. George, </author> <title> Sparse orthogonal decomposition on a hypercube multiprocessor, </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 453-465. </pages>
Reference-contexts: Typically, the first task in a direct method for solving sparse linear least squares problems is to compute a sparse QR factorization. The multifrontal method has proved to be effective for sparse QR factorization [6, 9, 12, 14]. Parallel implementations of multifrontal sparse QR factorization have been discussed in <ref> [3, 15, 19] </ref>. Sparse QR factorization involves the following steps: 1. Find a permutation matrix P such that AP has a sparse upper triangular factor R. 2. Determine the symbolic structure of R. 3. Perform numerical factorization|i.e., compute the numerical values of the nonzeros of R.
Reference: [4] <author> S. C. Eisenstat, M. T. Heath, C. S. Henkel, and C. H. Romine, </author> <title> Modified cyclic algorithms for solving triangular systems on distributed-memory multiprocessors, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 589-600. </pages>
Reference-contexts: A parallel multifrontal sparse back substitution algorithm on a processor . 4. Block-oriented parallel dense triangular solution algorithms. A number of parallel algorithms have been designed for solving dense triangular systems of linear equations on distributed-memory multiprocessors <ref> [1, 4, 8, 10, 11, 17] </ref>. Most of the previous works on parallel triangular solution have assumed that a triangular matrix is distributed to processors by single columns or rows. A crucial step in the parallel implementation of the CSNE method is to solve R t Rx = c.
Reference: [5] <author> J. A. George and J. W. H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <title> [6] , Householder reflections versus Givens rotations in sparse orthogonal decomposition, Linear Algebra and its Appl., </title> <booktitle> 88/89 (1987), </booktitle> <pages> pp. 223-238. </pages>
Reference-contexts: The reordering and symbolic factorization algorithms described in <ref> [5] </ref> can be applied to A t A to accomplish step 1 and step 2 stated above. 2 Let R j denote the structure of row j of R|i.e., the set of column indices of nonzeros in row j of R.
Reference: [7] <author> G. Golub, </author> <title> Numerical methods for solving linear least squares problems, </title> <journal> Numer. Math., </journal> <volume> 7 (1965), </volume> <pages> pp. 206-216. </pages>
Reference-contexts: Let A be an M fi N matrix with full column rank. The QR factorization method <ref> [7] </ref> for solving (1) first computes the QR factorization A = Q R ! Then the solution to (1) is obtained by solving the triangular system Rx = c, where c is the first N components of Q t b. <p> The parallel sparse QR factorization algorithm [19] and the parallel sparse back substitution algorithm described in section 3 can also be used to implement the QR factorization method <ref> [7] </ref> for solving sparse linear least squares problems as discussed in [20]. Acknowledgements. I would like to thank Professor Thomas F. Coleman for many discussions relating to this work and for his helpful comments on the manuscript.
Reference: [8] <author> M. T. Heath and C. H. Romine, </author> <title> Parallel solution of triangular systems on distributed-memory multiprocessors, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 558-588. </pages>
Reference-contexts: A parallel multifrontal sparse back substitution algorithm on a processor . 4. Block-oriented parallel dense triangular solution algorithms. A number of parallel algorithms have been designed for solving dense triangular systems of linear equations on distributed-memory multiprocessors <ref> [1, 4, 8, 10, 11, 17] </ref>. Most of the previous works on parallel triangular solution have assumed that a triangular matrix is distributed to processors by single columns or rows. A crucial step in the parallel implementation of the CSNE method is to solve R t Rx = c.
Reference: [9] <author> J. G. Lewis, D. J. Pierce, and D. K. Wah, </author> <title> Multifrontal Householder QR factorization, </title> <type> Tech. Report ECA-TR-127, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Finally, concluding remarks are contained in x7. 2. Parallel multifrontal sparse QR factorization. Typically, the first task in a direct method for solving sparse linear least squares problems is to compute a sparse QR factorization. The multifrontal method has proved to be effective for sparse QR factorization <ref> [6, 9, 12, 14] </ref>. Parallel implementations of multifrontal sparse QR factorization have been discussed in [3, 15, 19]. Sparse QR factorization involves the following steps: 1. Find a permutation matrix P such that AP has a sparse upper triangular factor R. 2. Determine the symbolic structure of R. 3.
Reference: [10] <author> G. Li and T. F. Coleman, </author> <title> A parallel triangular solver for a hypercube multiprocessor, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. </pages> <month> 485-502. </month> <title> [11] , A new method for solving triangular systems on distributed-memory message-passing multiprocessors, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 382-396. </pages>
Reference-contexts: A parallel multifrontal sparse back substitution algorithm on a processor . 4. Block-oriented parallel dense triangular solution algorithms. A number of parallel algorithms have been designed for solving dense triangular systems of linear equations on distributed-memory multiprocessors <ref> [1, 4, 8, 10, 11, 17] </ref>. Most of the previous works on parallel triangular solution have assumed that a triangular matrix is distributed to processors by single columns or rows. A crucial step in the parallel implementation of the CSNE method is to solve R t Rx = c. <p> The same pattern is observed for other values of p and n. In other words, the block partitions which produce the optimal performance for the parallel sparse QR factorization also produce the optimal or nearly optimal performance for the block-oriented dense lower triangular solver. Comparison with the cyclic algorithm <ref> [10] </ref> is shown in Table 1, where "time" is the execution time and "mflops" is the number of mega flops 10 executed per second. The above discussions on the parallel lower triangular solver with equal-row partitioning scheme also apply to the parallel lower triangular solver with equal-volume partitioning scheme. <p> Block sizes equal to 1; 2; ; 50 are examined. Discussions and conclusions on the parallel dense upper triangular solver are the same as those on the parallel dense lower triangular solver, and they are omitted. Comparison with the cyclic algorithm <ref> [10] </ref> is shown in Table 2. It can be easily shown that the arithmetic and communication complexities for the upper triangular solver are the same as those for the lower triangular solver.
Reference: [12] <author> J. W. H. Liu, </author> <title> On general row merging schemes for sparse Givens transformations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. </pages> <month> 1190-1211. </month> <title> [13] , The role of elimination trees in sparse factorization, </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 134-172. </pages>
Reference-contexts: Finally, concluding remarks are contained in x7. 2. Parallel multifrontal sparse QR factorization. Typically, the first task in a direct method for solving sparse linear least squares problems is to compute a sparse QR factorization. The multifrontal method has proved to be effective for sparse QR factorization <ref> [6, 9, 12, 14] </ref>. Parallel implementations of multifrontal sparse QR factorization have been discussed in [3, 15, 19]. Sparse QR factorization involves the following steps: 1. Find a permutation matrix P such that AP has a sparse upper triangular factor R. 2. Determine the symbolic structure of R. 3.
Reference: [14] <author> P. Matstoms, </author> <title> Sparse QR Factorization with Applications to Linear Least Squares Problems, </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1994. </year>
Reference-contexts: Finally, concluding remarks are contained in x7. 2. Parallel multifrontal sparse QR factorization. Typically, the first task in a direct method for solving sparse linear least squares problems is to compute a sparse QR factorization. The multifrontal method has proved to be effective for sparse QR factorization <ref> [6, 9, 12, 14] </ref>. Parallel implementations of multifrontal sparse QR factorization have been discussed in [3, 15, 19]. Sparse QR factorization involves the following steps: 1. Find a permutation matrix P such that AP has a sparse upper triangular factor R. 2. Determine the symbolic structure of R. 3. <p> In Table 7, ffix = x x, where x is the computed solution. The four rows of results for each problem represent the solution of the semi-normal equations and results of three iterative refinement steps. In practice, maximal accuracy is often achieved within 1-3 iterative refinement steps <ref> [14] </ref>. Table 7 Numerical accuracy problem kffixk 1 kffixk 2 =kxk 2 kffixk 1 1.6723e-09 5.2781e-16 2.9843e-13 GRID300 1.1723e-10 6.6784e-17 4.2633e-14 3.6702e-11 3.5425e-17 2.8422e-14 1.4873e-10 8.9103e-16 9.5923e-14 CUBE27 6.4135e-12 6.7688e-17 7.1054e-15 1.3531e-12 2.9442e-17 3.5527e-15 7. Concluding remarks.
Reference: [15] <author> P. E. Plassmann, </author> <title> Sparse Jacobian estimation and factorization on a multiprocessor, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990, </year> <pages> pp. 152-179. </pages>
Reference-contexts: Typically, the first task in a direct method for solving sparse linear least squares problems is to compute a sparse QR factorization. The multifrontal method has proved to be effective for sparse QR factorization [6, 9, 12, 14]. Parallel implementations of multifrontal sparse QR factorization have been discussed in <ref> [3, 15, 19] </ref>. Sparse QR factorization involves the following steps: 1. Find a permutation matrix P such that AP has a sparse upper triangular factor R. 2. Determine the symbolic structure of R. 3. Perform numerical factorization|i.e., compute the numerical values of the nonzeros of R.
Reference: [16] <author> A. Pothen and C. Sun, </author> <title> A mapping algorithm for parallel sparse Cholesky factorization, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1253-1257. </pages>
Reference-contexts: A S C C C A 0 ;(7) where Q S is an orthogonal matrix. end for Fig. 4. A serial multifrontal QR factorization algorithm In a parallel setting, the computational tasks are mapped onto node processors by the proportional mapping scheme <ref> [16] </ref>. Assume four processors f 0 ; 1 ; 2 ; 3 g are used for solving the problem discussed above. Then the task of forming F 7 is partitioned among the four processors. Computation of F 3 is partitioned among 0 and 1 .
Reference: [17] <author> C. H. Romine and J. M. Ortega, </author> <title> Parallel solution of triangular systems of equations, </title> <booktitle> Parallel Computing, 6 (1988), </booktitle> <pages> pp. 109-114. </pages>
Reference-contexts: A parallel multifrontal sparse back substitution algorithm on a processor . 4. Block-oriented parallel dense triangular solution algorithms. A number of parallel algorithms have been designed for solving dense triangular systems of linear equations on distributed-memory multiprocessors <ref> [1, 4, 8, 10, 11, 17] </ref>. Most of the previous works on parallel triangular solution have assumed that a triangular matrix is distributed to processors by single columns or rows. A crucial step in the parallel implementation of the CSNE method is to solve R t Rx = c. <p> Therefore, the algorithm requires no more than k1 X (d p k1 X d p floating-point operations. Let k = qp + r, where q and r are non-negative integers with 0 &lt; r &lt; p. It has been shown in <ref> [17] </ref> that k1 X (d p 1 (k 2 + kp 2k + pr + 2r 2p r 2 ): Since n = ks and s n=p, it can be easily derived that the number of arithmetic operations required by the algorithm is bounded above by 4n 2 =p.

References-found: 14

