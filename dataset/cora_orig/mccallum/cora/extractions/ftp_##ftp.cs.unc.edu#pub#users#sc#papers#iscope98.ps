URL: ftp://ftp.cs.unc.edu/pub/users/sc/papers/iscope98.ps
Refering-URL: http://www.cs.unc.edu/~sc/research/papers.html
Root-URL: http://www.cs.unc.edu
Title: An Evaluation of Java for Numerical Computing  
Author: Brian Blount and Siddhartha Chatterjee 
Web: http://www.cs.unc.edu/Research/HARPOON/  
Address: Chapel Hill  
Affiliation: Department of Computer Science The University of North Carolina at  
Abstract: This paper describes the design and implementation of high performance numerical software in Java. Our primary goals are to characterize the performance of object-oriented numerical software written in Java and to investigate whether Java is a suitable language for such endeavors. We have implemented JLAPACK, a subset of the LAPACK library in Java. LAPACK is a high-performance Fortran 77 library used to solve common linear algebra problems. JLAPACK is an object-oriented library, using encapsulation, inheritance, and exception handling. It performs within a factor of four of the optimized Fortran version for certain platforms and test cases. When used with the native BLAS library, JLA-PACK performs comparably with the Fortran version using the native BLAS library. We conclude that high-performance numerical software could be written in Java if a handful of concerns about language features and compilation strategies are adequately addressed.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A.-R. Adl-Tabatabai et al. </author> <title> Fast, effective code generation in a just-in-time Java compiler. </title> <booktitle> In Proc. PLDI'98, </booktitle> <pages> pages 280-290, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: We present performance results for solving the system of linear equations AX = B, using a coefficient matrix A and a right hand side matrix B whose entries are generated using a pseudorandom number generator from a uniform distribution in the range <ref> [0; 1] </ref>. The same seeds are used in both the Fortran and Java versions, to guarantee that both versions solve identical problems. The square matrix A has between 10 and 1000 columns. The matrix B has from 1 to 50 columns. <p> HPJava is somewhat similar to HPF and is designed for SPMD programming. Several projects are developing optimizers for Java. Moreira et al. [21] are developing a static compiler that optimizes array bounds checks and null pointer checks within loops. Adl-Tabatabai et al. <ref> [1] </ref> have developed a JIT compiler that performs a set of optimizations, including subexpression elimination, register allocation, and the elimination of array bounds checking.
Reference: 2. <author> O. Agesin, S. Freund, and J. Mitchell. </author> <title> Adding type parameterization to the Java language. </title> <booktitle> In Proc. OOPSLA'97, </booktitle> <pages> pages 49-65, </pages> <year> 1997. </year>
Reference-contexts: Two language issues hinder the development of JLAPACK: the absence of parametric polymorphism and the absence of operator overloading. The absence of parametric polymorphism required us to create a version of the JLAPACK library for each data type, which results in code bloat and extra programmer effort. Several projects <ref> [2, 22, 23] </ref> have examined methods for providing parametric polymorphism, either by modifying the JVM or by a adding a preprocessing phase, and it is possible that the feature will be available in future versions of Java.
Reference: 3. <author> E. Anderson et al. </author> <note> LAPACK User's Guide. SIAM, second edition, </note> <year> 1995. </year>
Reference-contexts: We use exceptions to perform error handling. For performance analysis, we executed our model using a fully compliant JVM, with bounds checking and garbage collection enabled. JLAPACK performs within a factor of four of the optimized Fortran version for certain platforms and test cases. 2 LAPACK LAPACK <ref> [3] </ref> is a library of Fortran 77 routines for common linear algebra problems, such as systems of linear equations, linear least square problems, eigenvalue problems, and singular value problems. LAPACK uses block-oriented algorithms for many operations, providing more locality of reference and allowing the use of matrix-matrix operations.
Reference: 4. <author> K. Arnold and J. Gosling. </author> <title> The Java T M Programming Language. The Java T M Series. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction The Java programming language <ref> [4] </ref> achieved rapid success due to several features key to the language. Java bytecodes are portable, which means that programs can be run on any machine that has an implementation of the Java Virtual Machine (JVM). Java provides garbage collection, freeing programmers from concerns about memory management and memory leaks.
Reference: 5. <author> M. Barr and J. Steinhorn. Kaffe, </author> <title> anyone? Implementing a Java Virtual Machine. </title> <booktitle> Embedded Systems Programming, </booktitle> <pages> pages 34-46, </pages> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: On the DEC, where native BLAS libraries were available through the dxml library [10], we measured performance with both the JBLAS classes and the native library. On the Sparcs, we ran two versions with kaffe <ref> [24, 5] </ref>: one with dynamic array bounds checking turned on and the other with this feature turned off. We turned off array bounds checking in kaffe by modifying the native instructions that its JIT compiler emits. We measured performance without array bounds checking for two reasons.
Reference: 6. <editor> F. Bassetti et al. </editor> <title> A comparison of performance-enhancing strategies for parallel numerical object-oriented frameworks. </title> <booktitle> In Proc. </booktitle> <address> ISCOPE'97, </address> <year> 1997. </year>
Reference-contexts: These may not be the best algorithms when run under the object model of Java. We have discussed several object-oriented programming idioms to implement numerical libraries efficiently. Future work needs to explore these and other techniques such as expression templates <ref> [6] </ref>. 2. Compiler changes. We noted in Section 4 several desirable optimizations that javac does not perform. Much work remains to be done here to develop better compilation techniques for Java. Budimlic and Kennedy [9] are exploring such optimizations using object inlining techniques. 3. Just-In-Time compilation.
Reference: 7. <author> A. J. C. Bik and D. B. Gannon. </author> <title> A note on native level 1 BLAS in Java. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 9(11) </volume> <pages> 1091-1099, </pages> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: For JLAPACK, we provided two versions: one implemented in Java, and the other employing vender-supplied native BLAS. The latter version provides Java wrappers around the Fortran BLAS routines, using the native method call mechanism of Java. Bik and Gannon <ref> [7] </ref> have shown that native methods can be used to achieve good performance, and our findings support their results. 3 JLAPACK JLAPACK and JBLAS are our Java implementations of the LAPACK and BLAS libraries, currently implementing the subset of the subroutines in both libraries that are used by the simple general
Reference: 8. <author> R. F. Boisvert et al. </author> <title> Developing numerical libraries in Java. </title> <booktitle> In Proc. ACM 1998 Workshop on Java for High Performance Network Computing, </booktitle> <pages> pages 35-44, </pages> <year> 1998. </year>
Reference-contexts: Then, all the method does is supply the vector's data member (by giving it a reference to its own data), and update its shape object. This approach eliminates unnecessary data copying and allows reuse of storage for temporary vectors and matrices. Boisvert et al. <ref> [8] </ref> discuss an implementation for numerical libraries in Java that does not encapsulate vectors and matrices. They use two-dimensional arrays to represent matrices, and store information describing the shape of vectors and matrices in local variables, similar to the Fortran version. <p> One version must handle the case where a vector is stored in a one-dimensional array, and another must handle the case where a vector is a column of a matrix, and is stored in a two dimensional array. They claim <ref> [8, p. 41] </ref>: If we are to provide the same level of functionality as the Fortran and C BLAS then we must provide several versions of each vector operation. While this may be true of implementations of BLAS primitives, this should not affect the interface visible to the programmer. <p> Unfortunately, every call to eltAt () and assignAt () must use the shape object to calculate the address of an element. Equations (1) and (2) demonstrate the cost of these calculations. Boisvert et al. <ref> [8] </ref> observe that the use of such methods is five times slower than an ordinary array access. We employ two mechanisms to overcome this overhead: aggregate operations and incremental access methods. Aggregate operations are operations performed on an entire vector or matrix at once. <p> The Java Numerical Toolkit <ref> [8] </ref> is a set of libraries for numerical computing in Java. Its initial version contains functionality such as elementary matrix and vector operations, matrix factorization, and the solution of linear systems. HP-Java [25] is an extension to Java, that allows parallel programming.
Reference: 9. <author> Z. Budimlic and K. Kennedy. </author> <title> Optimizing Java: </title> <journal> Theory and practice. Concurrency: Practice and Experience, </journal> <volume> 9(6) </volume> <pages> 445-463, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Future work needs to explore these and other techniques such as expression templates [6]. 2. Compiler changes. We noted in Section 4 several desirable optimizations that javac does not perform. Much work remains to be done here to develop better compilation techniques for Java. Budimlic and Kennedy <ref> [9] </ref> are exploring such optimizations using object inlining techniques. 3. Just-In-Time compilation. Current JIT compilers are in their early version, and have not been heavily optimized. As we discussed in Section 4, some do not take advantage of machine-specific optimizations and do not appear to schedule code effectively. 4.
Reference: 10. <institution> DIGITAL Extended Math Library. </institution> <note> http://www.digital.com/hpc/software/dxml.html. </note>
Reference-contexts: Testing Environment Table 1 lists the platforms we used for timing. We ran Fortran versions for all Unix platforms, using the -fast option when compiling the Fortran library. On the DEC, where native BLAS libraries were available through the dxml library <ref> [10] </ref>, we measured performance with both the JBLAS classes and the native library. On the Sparcs, we ran two versions with kaffe [24, 5]: one with dynamic array bounds checking turned on and the other with this feature turned off.
Reference: 11. <author> A. Dingle and T. H. Hildebrandt. </author> <title> Improving C++ performance using temporaries. </title> <booktitle> Computer, </booktitle> <pages> pages 31-41, </pages> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: For efficiency, an implementation can still provide specialized routines for common cases. 3.2 Limiting constructor calls Excessive object creation is a well-known source of performance loss in object-oriented programs. Therefore, we use a technique (similar to that described by Dingle and Hildebrandt <ref> [11] </ref>) to limit the number of temporary vector and matrices created. Such objects are used locally in methods of the JBLAS and JLAPACK classes, so it is natural to place them within these methods. However, we make them private static class members.
Reference: 12. <author> J. J. Dongarra et al. </author> <title> Algorithm 679: A set of level 3 basic linear algebra subprograms: Model implementaton and test programs. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16(1) </volume> <pages> 18-28, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 13. <author> J. J. Dongarra et al. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 14. <author> J. J. Dongarra et al. </author> <title> Algorithm 656: An extended set of basic linear algebra subprograms: Model implementaton and test programs. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 14(1) </volume> <pages> 18-32, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 15. <author> J. J. Dongarra et al. </author> <title> An extended set of basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 16. <author> D. R. Engler. </author> <title> VCODE: A retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proc. PLDI'96, </booktitle> <pages> pages 160-170, </pages> <year> 1996. </year>
Reference-contexts: As we discussed in Section 4, some do not take advantage of machine-specific optimizations and do not appear to schedule code effectively. 4. Architectural issues. Current trends in processor implementation adds significant instruction re-ordering capabilities to the hardware. Engler <ref> [16] </ref> conjectures that this may reduce or obviate the need for instruction scheduling by JIT compilers. This is a reasonable conjecture that needs to be tested. 5. Experimentation with other codes. LAPACK is obviously not representative of all numerical software.
Reference: 17. <author> D. Flanagan. </author> <title> Java In a Nutshell. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1997. </year>
Reference-contexts: Note how the shape parameters specify exactly where the data is stored. are used to retrieve the next element of a vector or the next column of a matrix, and are similar to the methods defined by the java.lang.Enumeration <ref> [17] </ref> interface. However, Enumeration does not handle primitive types, so we could not implement this functionality with the Enumeration interface. 3.4 Complex numbers Currently, Java does not provide a primitive type for complex numbers. However, complex numbers are required within the LAPACK library, so we provide two implementations for them.
Reference: 18. <author> Java Grande Forum. </author> <title> The Java Grande Forum charter document. </title> <address> http://www.npac.syr.edu/javagrande/jgfcharter.html. </address>
Reference-contexts: While it is beyond the scope of this paper to determine the best mechanism for including primitive complex numbers in Java, this issue is under consideration by the Java Grande Forum <ref> [18] </ref>, and must be resolved satisfactorily if Java is to be viable for numerical computing. 4 Performance Performance is an overarching concern for scientific computation. The Fortran version of LAPACK has been highly optimized and represents our target level of performance.
Reference: 19. <editor> C. L. Lawson et al. </editor> <title> Algorithm 539: Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5(3) </volume> <pages> 324-325, </pages> <month> Sep. </month> <year> 1979. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 20. <editor> C. L. Lawson et al. </editor> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <month> Sep. </month> <year> 1979. </year>
Reference-contexts: JLAPACK currently implements only the simple linear equation solver for general matrices (i.e., xGESV and the routines they require) with both blocking and nonblocking versions. LAPACK uses the Basic Linear Algebra Subroutines (BLAS) <ref> [13, 12, 15, 14, 20, 19] </ref> for many of its time-critical inner loops. Most high performance machines have BLAS libraries with machine-specific optimizations, called native BLAS. Generic Fortran 77 BLAS code is available and is distributed with LAPACK.
Reference: 21. <author> J. E. Moreira, S. P. Midkiff, and M. Gupta. </author> <title> From flop to megaflops: Java for technical computing. </title> <booktitle> In Proc. </booktitle> <address> LCPC'98, </address> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: HP-Java [25] is an extension to Java, that allows parallel programming. HPJava is somewhat similar to HPF and is designed for SPMD programming. Several projects are developing optimizers for Java. Moreira et al. <ref> [21] </ref> are developing a static compiler that optimizes array bounds checks and null pointer checks within loops. Adl-Tabatabai et al. [1] have developed a JIT compiler that performs a set of optimizations, including subexpression elimination, register allocation, and the elimination of array bounds checking.
Reference: 22. <author> A. C. Myers, J. A. Bank, and B. Liskov. </author> <title> Parameterized types for Java. </title> <booktitle> In Proc. POPL'97, </booktitle> <pages> pages 132-145, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Two language issues hinder the development of JLAPACK: the absence of parametric polymorphism and the absence of operator overloading. The absence of parametric polymorphism required us to create a version of the JLAPACK library for each data type, which results in code bloat and extra programmer effort. Several projects <ref> [2, 22, 23] </ref> have examined methods for providing parametric polymorphism, either by modifying the JVM or by a adding a preprocessing phase, and it is possible that the feature will be available in future versions of Java.
Reference: 23. <author> M. Odersky and P. Wadler. </author> <title> Pizza into Java: Translating theory into practice. </title> <booktitle> In Proc. POPL'97, </booktitle> <pages> pages 146-159, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Two language issues hinder the development of JLAPACK: the absence of parametric polymorphism and the absence of operator overloading. The absence of parametric polymorphism required us to create a version of the JLAPACK library for each data type, which results in code bloat and extra programmer effort. Several projects <ref> [2, 22, 23] </ref> have examined methods for providing parametric polymorphism, either by modifying the JVM or by a adding a preprocessing phase, and it is possible that the feature will be available in future versions of Java.
Reference: 24. <author> T. J. Wilkinson. </author> <note> The Kaffe homepage. http://www.transvirtual.com/kaffe.html. </note>
Reference-contexts: On the DEC, where native BLAS libraries were available through the dxml library [10], we measured performance with both the JBLAS classes and the native library. On the Sparcs, we ran two versions with kaffe <ref> [24, 5] </ref>: one with dynamic array bounds checking turned on and the other with this feature turned off. We turned off array bounds checking in kaffe by modifying the native instructions that its JIT compiler emits. We measured performance without array bounds checking for two reasons.
Reference: 25. <author> G. Zhang et al. </author> <title> Considerations in HPJava language design and implementation. </title> <booktitle> In Proc. </booktitle> <address> LCPC'98, </address> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The Java Numerical Toolkit [8] is a set of libraries for numerical computing in Java. Its initial version contains functionality such as elementary matrix and vector operations, matrix factorization, and the solution of linear systems. HP-Java <ref> [25] </ref> is an extension to Java, that allows parallel programming. HPJava is somewhat similar to HPF and is designed for SPMD programming. Several projects are developing optimizers for Java. Moreira et al. [21] are developing a static compiler that optimizes array bounds checks and null pointer checks within loops.
References-found: 25

