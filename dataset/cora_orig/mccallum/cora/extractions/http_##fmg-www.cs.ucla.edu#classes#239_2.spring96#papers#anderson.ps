URL: http://fmg-www.cs.ucla.edu/classes/239_2.spring96/papers/anderson.ps
Refering-URL: http://fmg-www.cs.ucla.edu/reiher/CS239_spring97.html
Root-URL: http://www.cs.ucla.edu
Title: Serverless Network File Systems  
Author: Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang 
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: This work is supported in part by the Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, IBM, Siemens Corporation, Sun Microsystems, and Xerox Corporation. Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship, Neefe by a National Science Foundation Graduate Research Fellowship, and Roselli by a Department of Education GAANN fellowship. The authors can be contacted at -tea, dahlin, neefe, patterson, drew, rywang-@CS.Berke-ley.EDU. Abstract In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
Abstract-found: 1
Intro-found: 1
Reference: [Ande95] <author> T. Anderson, D. Culler, D. Patterson, </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5464, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs <ref> [Doug91, Litz92, Ande95] </ref>. By increasing the peak processing power available to users, NOWs increase peak demands on the file system [Cyph93]. Unfortunately, current centralized file system designs fundamentally limit performance and availability since all read misses and all disk writes go through the central server.
Reference: [Bake91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proc. of the 13th Symp. on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Next generation networks not only enable serverlessness, they require it by allowing applications to place increasing demands on the file system. The I/O demands of traditional applications have been increasing over time <ref> [Bake91] </ref>; new applications enabled by fast networks such as multimedia, process migration, and parallel processing will further pressure file systems to provide increased performance. For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. <p> Unfortunately, small writes are common in many environments <ref> [Bake91] </ref>, and larger caches increase the percentage of writes in disk workload mixes over time. We expect cooperative caching using workstation memory as a global cache to further this workload trend. <p> Distributing Utilization Status xFS assigns the burden of maintaining each segments utilization status to the client that wrote the segment. This approach provides parallelism by distributing the bookkeeping, and it provides good locality; because clients seldom write-share data <ref> [Bake91, Kist92, Blaz93] </ref> a clients writes usually affect only local segments utilization status. We simulated this policy to examine how well it reduced the overhead of maintaining utilization information.
Reference: [Bake92] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 1022, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: must either reduce the fragment size (reducing the efficiency of the writes) or increase the size of the segment (increasing memory demands on the clients); even if the system were to increase the segment size, syncs would often force clients to write partial segments to disk, again reducing write efficiency <ref> [Bake92] </ref>. 2.4. Multiprocessor Cache Consistency Network file systems resemble multiprocessors in that both provide a uniform view of storage across the system, requiring both to track where blocks are cached. This information allows them to maintain cache consistency by invalidating stale cached copies. <p> the simulator assumes that blocks survive in clients write buffers for 30 seconds or until overwritten, whichever is sooner; this assumption allows the simulated system to avoid communication more often than a real system since it does not account for segments that are written to disk early due to syncs <ref> [Bake92] </ref>. (Unfortunately, syncs are not visible in our Auspex traces.) Finally, under the Distributed policy, each client tracks the status of blocks that it writes so that it needs no network messages when modifying a block for which it was the last writer. <p> Of course, a key to good NFS server performance is to efficiently implement synchronous writes; our prototype does not yet exploit the non-volatile RAM optimization found in most commercial NFS servers <ref> [Bake92] </ref>, so for best performance, NFS clients should mount these partitions using the unsafe option to allow xFS to buffer writes in memory. 7. xFS Prototype This section describes the state of the xFS prototype as of August 1995 and presents preliminary performance results measured on a 32 node cluster of <p> The NFS server uses a faster disk than the xFS storage servers, a 2.1 GB DEC RZ 28-VA with a peak bandwidth of 5 MB/s from the raw partition into memory. The NFS server also uses a Prestoserve NVRAM card that acts as a buffer for disk writes <ref> [Bake92] </ref>. We did not use an NVRAM buffer for the xFS machines, although xFSs log buffer provides similar performance benefits. A high-speed, switched Myrinet network [Bode95] connects the machines.
Reference: [Bake94] <author> M. Baker. </author> <title> Fast Crash Recovery in Distributed File Systems. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: for the newest log segment stored at that group), each client or manager only needs to contact N storage server groups, and all of the clients and managers can proceed in parallel, provided that they take steps to avoid recovery storms where many machines simultaneously contact a single storage server <ref> [Bake94] </ref>. We plan use randomization to accomplish this goal. Recovering the log checkpoint or rolling forward logs raises similar scaling issues. Although each manager or client must potentially contact all of the storage servers to read the logs, each log can be recovered in parallel. <p> Cache Consistency State After the managers have recovered and rolled forward the imap, they must recover the cache consistency state associated with the blocks they manage. xFS will use server-driven recovery <ref> [Bake94] </ref>. The manager contacts all of the systems clients, and they send the manager a list of the blocks that they are caching or for which they have write ownership from the indicated portion of the index number space.
Reference: [Basu95] <author> A. Basu, V. Buch, W. Vogels, and T. von Eicken. </author> <month> U-Net: </month>
Reference-contexts: In contrast, shared media networks such as Ethernet or FDDI allow only one client or server to transmit at a time. In addition, the move towards low latency network interfaces <ref> [vE92, Basu95] </ref> enables closer cooperation between machines than has been possible in the past. The result is that a LAN can be used as an I/O backplane, harnessing physically distributed processors, memory, and disks into a single system.
References-found: 5

