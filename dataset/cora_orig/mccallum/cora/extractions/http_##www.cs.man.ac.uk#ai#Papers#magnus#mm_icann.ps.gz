URL: http://www.cs.man.ac.uk/ai/Papers/magnus/mm_icann.ps.gz
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: The Dynamics of Matrix Momentum  
Author: Magnus Rattray and David Saad 
Address: B4 7ET, UK.  
Affiliation: Neural Computing Research Group, Aston University, Birmingham  
Abstract: We analyse the matrix momentum algorithm, which provides an efficient approximation to on-line Newton's method, by extending a recent statistical mechanics framework to include second order algorithms. We study the efficacy of this method when the Hessian is available and also consider a practical implementation which uses a single example estimate of the Hessian. The method is shown to provide excellent asymptotic performance, although the single example implementation is sensitive to the choice of training parameters. We conjecture that matrix momentum could provide efficient matrix inversion for other second order algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. </author> <title> Fabian. </title> <journal> Ann. Math. Statist., </journal> <volume> 39, </volume> <month> 1327 </month> <year> (1968). </year>
Reference-contexts: Second order methods, which incorporate information about the curvature of the mean error surface, have been shown to be asymptotically optimal (e.g., <ref> [1, 2] </ref>) but they are often expensive both computationally and in terms of storage space; for example, they may require averaging over the entire data set to determine the Hessian followed by a matrix inversion.
Reference: [2] <author> S. </author> <note> Amari Neural Computation 10(2) 251 (1998). </note>
Reference-contexts: Second order methods, which incorporate information about the curvature of the mean error surface, have been shown to be asymptotically optimal (e.g., <ref> [1, 2] </ref>) but they are often expensive both computationally and in terms of storage space; for example, they may require averaging over the entire data set to determine the Hessian followed by a matrix inversion. <p> This fixed point is an unstable transient fixed point for gradient descent [5] and it is therefore better to use gradient descent initially and to only switch on matrix momentum after escaping this fixed point. Other second order algorithms exist, such as natural gradient learning <ref> [2] </ref>, which overcome this problem and provide improved transient performance over gradient descent [10]. Matrix momentum may also provide an efficient matrix inversion method for these algorithms. <p> Hessian based methods are not appropriate during the transients of learning because there is a possibility of trapping in suboptimal fixed points [9]. It would therefore make sense to use other matrix pre-multipliers which are guaranteed positive definite, such as the Fisher information matrix used in natural gradient learning <ref> [2, 10] </ref> or the linearized Hessian [4] used in Gauss-Newton methods. Matrix momentum could easily provide an efficient inversion method in order to approximate the resulting algorithms. Acknowledgement This work was supported by the EPSRC grant GR/L19232.
Reference: [3] <author> G. B. Orr, T. K. </author> <booktitle> Leen Advances in Neural Information Processing Systems vol 9, </booktitle> <editor> ed M. C. Mozer, M. I. Jordan and T. </editor> <address> Petsche (Cambridge, MA: </address> <publisher> MIT Press, </publisher> <address> 1997) p 606 </address>
Reference-contexts: Orr & Leen <ref> [3, 4] </ref> have recently proposed a novel on-line algorithm which uses a momentum term with an adaptive matrix momentum parameter to approximate on-line Newton's method. They claim that this algorithm is asymptotically optimal and insensitive to the choice of external parameters. The aim of this paper is twofold. <p> In practice, the Hessian is not available on-line and we therefore use the same theoretical framework to examine the performance using a single example approximation to the Hessian, as suggested by Orr & Leen <ref> [3] </ref>, and consider its limitations. There are several advantages in conducting a theoretical study in the manner described here over a numerical one. Studying the average behaviour, using modest computational means, we perform an unbiased assessment of the algorithm which is insensitive to the choice of training examples. <p> The simplest such approximation is to use a single training example in order to estimate the Hessian <ref> [3] </ref>. <p> analytically, using methods from [6], but we have shown here that performance is certainly strongly dependent on parameter choice. 4 Conclusion and future work In this paper we extend a recently developed theoretical framework to accommodate on-line second order methods and in particular we solve the dynamics of matrix momentum <ref> [3] </ref>. This algorithm provides a very efficient approximation to on-line New-ton's method by avoiding explicit inversion of the Hessian and we show that the two methods are very close, if not equivalent, when the Hessian is known.
Reference: [4] <author> G. B. Orr, </author> <type> Ph.D. Dissertation, </type> <institution> Oregon Graduate Institute of Science & Technology (1995). </institution>
Reference-contexts: Orr & Leen <ref> [3, 4] </ref> have recently proposed a novel on-line algorithm which uses a momentum term with an adaptive matrix momentum parameter to approximate on-line Newton's method. They claim that this algorithm is asymptotically optimal and insensitive to the choice of external parameters. The aim of this paper is twofold. <p> Standard on-line momentum has been considered previously and has not been shown to be particularly useful (e.g. <ref> [4, 7, 8] </ref>) but it is instructive to consider this case first. <p> It would therefore make sense to use other matrix pre-multipliers which are guaranteed positive definite, such as the Fisher information matrix used in natural gradient learning [2, 10] or the linearized Hessian <ref> [4] </ref> used in Gauss-Newton methods. Matrix momentum could easily provide an efficient inversion method in order to approximate the resulting algorithms. Acknowledgement This work was supported by the EPSRC grant GR/L19232.
Reference: [5] <author> D. Saad, S. A. </author> <title> Solla, </title> <journal> Phys. Rev. Lett. </journal> <note> 74, 4337 (1995); Phys. Rev. E 52 4225 (1995). </note>
Reference-contexts: They claim that this algorithm is asymptotically optimal and insensitive to the choice of external parameters. The aim of this paper is twofold. We first employ a theoretical framework, recently developed for studying the dynamics of on-line learning <ref> [5] </ref>, to study the performance of an idealized version of matrix momentum in which the exact Hessian is available. <p> Angled brackets denote averages over inputs and the covariance matrix completely describes the mean state of the system. In the limit of large N we define a continuous time variable ff = =N and a coupled set of ordinary differential equations describes the overlap evolution under standard gradient descent <ref> [5] </ref>. These equations, representing an exact analytical solution for the average case, can be integrated numerically to obtain a solu tion of the dynamics. The generalization error can be written in terms of the overlaps and can thus be calculated once the dynamics have been solved. <p> We define a Markov process equivalent to equation (1) by introducing a new set of variables 1 J i = J i + N fi i ; i = fi i + j ffi i : (2) We can now proceed along the lines of <ref> [5] </ref> in order to derive a set of first order differential equations describing the evolution of a set of overlaps. <p> Choosing j ff = 1=ff is known to provide optimal asymptotic performance in this case. However, it has not been shown that the limiting behaviour described for standard momentum holds for a matrix momentum parameter. Substituting the above definitions into equations (2) and following the methods in <ref> [5] </ref> we find a set of differential equations for the overlaps as N ! 1, dQ ik = E ik + E ki ; dff dC ik = kj ff hffi i z k + ffi k z i i + k 2 j 2 ff hffi i ffi k i <p> The fields are distributed according to a multivariate Gaussian with the overlaps as covariances and all averages and generalization error derivatives can be calculated in closed form <ref> [5] </ref>. Matrix momentum, as defined above, is not particularly useful during the transients of learning in multilayer neural networks with over-lapping receptive fields, since both on-line Newton's method and matrix momentum can become trapped in a suboptimal fixed point of the dynamics [9]. <p> This fixed point is an unstable transient fixed point for gradient descent <ref> [5] </ref> and it is therefore better to use gradient descent initially and to only switch on matrix momentum after escaping this fixed point. Other second order algorithms exist, such as natural gradient learning [2], which overcome this problem and provide improved transient performance over gradient descent [10]. <p> The simplest such approximation is to use a single training example in order to estimate the Hessian [3]. The equations of motion for matrix momentum using this approximation can also be determined by the methods in <ref> [5] </ref> and the equations for Q and R are as in (4), while the equations for the other overlaps are, dC ik = kh (j ff ffi i OE i )z k + (j ff ffi k OE k )z i i + k 2 h (j ff ffi i OE
Reference: [6] <author> T. K. Leen, B. Schottky, D. </author> <booktitle> Saad Advances in Neural Information Processing Systems vol 10, </booktitle> <editor> ed M. I. Jordan, M. J. Kearns and S. A. </editor> <address> Solla (Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1998). </year>
Reference-contexts: Combining these results with recent work on the asymptotic dynamics of gradient descent could provide analytical results for optimal and maximal learning parameters <ref> [6] </ref>. 1 2 General framework We consider a map from an N -dimensional input space 2 &lt; N onto a scalar, realized through a model oe (J; ) = P K i=1 g (J i ), which can be viewed as a soft committee machine, where g (x) j erf (x= <p> For intermediate k (dot-dashed line) the performance is asymptotically close to optimal and certainly provides a significant improvement over gradient descent. Further work is required to determine the optimal and maximal values of k and j ff analytically, using methods from <ref> [6] </ref>, but we have shown here that performance is certainly strongly dependent on parameter choice. 4 Conclusion and future work In this paper we extend a recently developed theoretical framework to accommodate on-line second order methods and in particular we solve the dynamics of matrix momentum [3]. <p> The method is also reasonably stable to fluctuations caused by using a very crude single example approximation to the Hessian, as long as the algorithm parameters are chosen well. It should be reasonably straightforward to apply the results of <ref> [6] </ref> in order to determine optimal and maximal parameters in this case, in terms of task complexity and nonlinearity. However, to obtain more robust performance a better on-line approximation to the Hessian should probably be used.
Reference: [7] <author> W. Weigerinck, A. Komoda, T .Heskes J. </author> <note> Phys. A 27, 4425 (1994). </note>
Reference-contexts: Standard on-line momentum has been considered previously and has not been shown to be particularly useful (e.g. <ref> [4, 7, 8] </ref>) but it is instructive to consider this case first. <p> The above limits are related to those discussed in <ref> [7] </ref> and their results are consistent with the above observations. The latter scaling proves most appropriate for matrix momentum and is rigorously justified without resorting to adiabatic elimination.
Reference: [8] <author> A. Prugel-Bennett, </author> <note> unpublished notes, </note> <year> (1996). </year>
Reference-contexts: Standard on-line momentum has been considered previously and has not been shown to be particularly useful (e.g. <ref> [4, 7, 8] </ref>) but it is instructive to consider this case first. <p> elimination and we find that the dynamics of R and Q is simply equivalent to gradient descent with an effective learning rate of j eff = j=(1 fi) in this case. * More interesting dynamics is observed if we choose j O (1=N ) and 1 fi O (1=N ) <ref> [8] </ref>. In this case the overlaps all evolve on the same time-scale.
Reference: [9] <author> M. Rattray, D. Saad, </author> <title> `Incorporating curvature information into on-line learning' Proc. of the On-line Learning Themed Week, (Isaac Newton Institute, </title> <address> Cambridge, </address> <year> 1997). </year>
Reference-contexts: Matrix momentum, as defined above, is not particularly useful during the transients of learning in multilayer neural networks with over-lapping receptive fields, since both on-line Newton's method and matrix momentum can become trapped in a suboptimal fixed point of the dynamics <ref> [9] </ref>. This fixed point is an unstable transient fixed point for gradient descent [5] and it is therefore better to use gradient descent initially and to only switch on matrix momentum after escaping this fixed point. <p> In fig. 1 (a) we compare the asymptotic performance of idealized matrix momentum to on-line Newton's method for a two-node network learning an isotropic task in the presence of output noise with oe 2 = 0:01 (The dynamics for on-line Newton's method is solved in <ref> [9] </ref>). We use gradient descent initially, until after the transient fixed point described above, and then we use matrix momentum with j ff gradually reduced from 0:1 to the 1=ff decay which is known to be asymptotically optimal. <p> As k increases, the trajectory converges onto the on-line Newton's method result (solid line), as desired, and we approach the optimal asymptotic decay law (dot-dashed line) which is determined in <ref> [9] </ref>. <p> However, to obtain more robust performance a better on-line approximation to the Hessian should probably be used. Hessian based methods are not appropriate during the transients of learning because there is a possibility of trapping in suboptimal fixed points <ref> [9] </ref>. It would therefore make sense to use other matrix pre-multipliers which are guaranteed positive definite, such as the Fisher information matrix used in natural gradient learning [2, 10] or the linearized Hessian [4] used in Gauss-Newton methods.
Reference: [10] <author> M. Rattray, D. Saad, S. A. Solla, S. </author> <title> Amari `Natural gradient descent for on-line learning' (in preparation, </title> <year> 1998). </year>
Reference-contexts: Other second order algorithms exist, such as natural gradient learning [2], which overcome this problem and provide improved transient performance over gradient descent <ref> [10] </ref>. Matrix momentum may also provide an efficient matrix inversion method for these algorithms. <p> Hessian based methods are not appropriate during the transients of learning because there is a possibility of trapping in suboptimal fixed points [9]. It would therefore make sense to use other matrix pre-multipliers which are guaranteed positive definite, such as the Fisher information matrix used in natural gradient learning <ref> [2, 10] </ref> or the linearized Hessian [4] used in Gauss-Newton methods. Matrix momentum could easily provide an efficient inversion method in order to approximate the resulting algorithms. Acknowledgement This work was supported by the EPSRC grant GR/L19232.
References-found: 10

