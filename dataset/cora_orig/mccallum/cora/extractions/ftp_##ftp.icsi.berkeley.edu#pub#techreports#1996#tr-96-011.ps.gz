URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1996/tr-96-011.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1996.html
Root-URL: http://www.icsi.berkeley.edu
Title: A DSOM hierarchical model for reflexive processing: an application to visual trajectory classification  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Claudio M. Privitera and Lokendra Shastri 
Date: June 1996  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-96-011  
Abstract: Any intelligent system, whether human or robotic, must be capable of dealing with patterns over time. Temporal pattern processing can be achieved if the system has a short-term memory capacity (STM) so that different representations can be maintained for some time. In this work we propose a neural model wherein STM is realized by leaky integrators in a self-organizing system. The model exhibits compo-sitionality, that is, it has the ability to extract and construct progressively complex and structured associations in an hierarchical manner, starting with basic and primitive (temporal) elements. An important feature of the proposed model is the use of temporal correlations to express dynamic bindings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Benaim and L. Tomasini. </author> <title> Competitive and Self-Organizing algorithms based on the minimization of an information criterion. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <address> Amsterdam, 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Both biological plausibility and statistical properties of self-organizing process can be enhanced by using as the activation function the following normalized Gaussian or softmax function: u i (*) = U i (*) = P (3) where, G () is a gaussian with fixed variance <ref> [1] </ref>. In this case, the optimal estimation of a continuous smooth function f : &lt; K ! &lt; Q can be achieved by means of the following population coding: f (*) i where both the vectors i and Y i are learned using the classical parallel Hebbian rule above introduced.
Reference: [2] <author> N. Bernstein. </author> <title> The Co-ordination and Regulation of Movements. </title> <publisher> Pergamon, Oxford, </publisher> <year> 1967. </year>
Reference-contexts: Another phenomena where compositionality plays an ubiquitous and critical role is language where a sequence of acoustic input is mapped into a complex and high dimensional description of events and states, and motor control where abstract representation of motor programs are decomposed during movement generation into more primitive motor schemas <ref> [2] </ref>, [9]. A central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation [6], [26], [27].
Reference: [3] <author> E. </author> <title> Bienenstock. A model of neocortex. </title> <booktitle> Netwrok: Computation in Neural Systems, </booktitle> <volume> 6 </volume> <pages> 179-224, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Many cognitive processes must be compositional, that is, they must be capable of extracting and constructing progressively complex and structured representations in a hierarchical manner, starting with simple primitive elements [4], <ref> [3] </ref>, [26]. For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event. <p> Thus compositionality, binding, and temporal pattern processing are interrelated and seem to play an essential role in cognition <ref> [3] </ref>. In this work, compositionality is achieved by a hierarchical model of Self-Organizing Maps (SOM) and the temporal processing ability is achieved by exploiting neurons capable of supporting short-term memory (STM).
Reference: [4] <author> E. Bienenstock and S. Geman. </author> <title> Compositionality in neural systems. In M.A. Arbib, editor, </title> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 223-226. </pages> <publisher> The Mit Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Many cognitive processes must be compositional, that is, they must be capable of extracting and constructing progressively complex and structured representations in a hierarchical manner, starting with simple primitive elements <ref> [4] </ref>, [3], [26]. For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event.
Reference: [5] <author> I. Bierderman. Recognition-by-components: </author> <title> a theory of human image understanding. </title> <journal> Psychological Review, </journal> <volume> 94 </volume> <pages> 115-147, </pages> <year> 1987. </year>
Reference-contexts: For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event. In this case, primitive elements are combined into more complex associations until a complete meaningful representation of the visual input is achieved <ref> [5] </ref>, [8], [14].
Reference: [6] <author> S. L. Blesser, R. Coppola, and R. N. Nakamura. </author> <title> Episodic multiregional cortical coherence at multiple frequencies during visual task performance. </title> <journal> Nature, </journal> <volume> 366 </volume> <pages> 153-155, </pages> <year> 1993. </year>
Reference-contexts: A central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation <ref> [6] </ref>, [26], [27]. Recent neurological data highlights the potential role of the temporal structure of neural activity in the expression and propagation of dynamic bindings [28], [27].
Reference: [7] <author> D.C. Burr and J. Ross. </author> <title> Visual analysis during motion. In M.A. </title> <editor> Arbib and A.R. Hanson, editors, </editor> <title> Vision, Brain, </title> <booktitle> and Cooperative Computation, </booktitle> <pages> pages 187-208. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: All the elements of the map share the same visual field and the input activation function of these elements can be directly derived from some neurological hypothesis: for example <ref> [7] </ref> suggests the existence of motion detectors with elongated spatio-temporal receptive field tilted in a preferred direction.
Reference: [8] <author> D.D. Hoffman and W. Richards. </author> <title> Parts of recognition. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 65-96, </pages> <year> 1985. </year>
Reference-contexts: For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event. In this case, primitive elements are combined into more complex associations until a complete meaningful representation of the visual input is achieved [5], <ref> [8] </ref>, [14].
Reference: [9] <author> S.W. Keele, A. Cohen, and R. Ivry. </author> <title> Motor programs: concepts and issues. </title> <editor> In M. Jean-nerod, editor, </editor> <title> Attention and performance XIII. </title> <publisher> Lawrence Erlbaum Ass., </publisher> <address> Hillsdale, NJ,, </address> <year> 1990. </year>
Reference-contexts: phenomena where compositionality plays an ubiquitous and critical role is language where a sequence of acoustic input is mapped into a complex and high dimensional description of events and states, and motor control where abstract representation of motor programs are decomposed during movement generation into more primitive motor schemas [2], <ref> [9] </ref>. A central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation [6], [26], [27].
Reference: [10] <author> T. Kohonen. </author> <title> Self organizing formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference: [11] <author> T. Kohonen. </author> <title> Self organization and associative memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: The parameter and the activation function u i (*) together define many key properties of the map: for example, in a traditional Kohonen model ([10], <ref> [11] </ref>), u i (*) is a simple window function centered on the most resonant element so that each weight vector of the map is updated during each step of the learning phase on the basis of its proximity to the winner element: in this way the map preserves the topology of
Reference: [12] <author> T. Martinetz. </author> <title> Competitive hebbian learning rule forms perfectly topology preserving maps. </title> <editor> In S. Gielen and B. Kappen, editors, </editor> <booktitle> Proc. of the International Conference on ANN'93, </booktitle> <pages> pages 427-434, </pages> <address> Amsterdam, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: learning: in this case, the function u i (*) is not evaluated in the map but in the input vector space so that finally, the neighborhood between two elements is highlighted by the presence of a connection between them and not by their proximity in the lattice (see [13] and <ref> [12] </ref>).
Reference: [13] <author> T. Martinetz and K. Schulten. </author> <title> A 'Neural-Gas' network learns topologies. </title> <editor> In T. Ko-honen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <address> Amsterdam, 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: evolved during learning: in this case, the function u i (*) is not evaluated in the map but in the input vector space so that finally, the neighborhood between two elements is highlighted by the presence of a connection between them and not by their proximity in the lattice (see <ref> [13] </ref> and [12]).
Reference: [14] <author> T. Pavlidis. </author> <title> Structural pattern recognition: </title> <editor> primitives and juxtaposition. In S. Watan-abe, editor, </editor> <booktitle> Frontiers of Pattern Recognition, </booktitle> <pages> pages 421-451. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event. In this case, primitive elements are combined into more complex associations until a complete meaningful representation of the visual input is achieved [5], [8], <ref> [14] </ref>.
Reference: [15] <author> R. Plamondon and C.M. Privitera. </author> <title> A neural model for learning and generating rapid movement sequence. </title> <journal> Biological Cybernetics, </journal> <volume> 34(2) </volume> <pages> 117-30, </pages> <year> 1996. </year>
Reference-contexts: This temporal property of D-SOM can be exploited in the context of motor control, time series analysis, temporal sequence representation and recognition (see <ref> [15] </ref>, [19], [20], 2 Some basic simplification has been assumed in this case for the sake of exposition. The simplification is that the first reference point of the event in progress, is specific to that event.
Reference: [16] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <journal> Proc. of the IEEE, </journal> <volume> 78 </volume> <pages> 1481-1495, </pages> <year> 1990. </year>
Reference-contexts: It is worth highlighting that, the previous equation is a sum of radial basis function [18] and it can be also proved that Eq. (4) is a solution of the regularization problem <ref> [16] </ref>, [17].
Reference: [17] <author> T. Poggio and F. Girosi. </author> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982, </pages> <year> 1990. </year> <month> 20 </month>
Reference-contexts: It is worth highlighting that, the previous equation is a sum of radial basis function [18] and it can be also proved that Eq. (4) is a solution of the regularization problem [16], <ref> [17] </ref>.
Reference: [18] <author> M. J. D. Powell. </author> <title> Radial basis functions for multivariable interpolation: a review. </title> <editor> In J. C. Mason and M. G. Cox, editors, </editor> <title> Algorithms for Approximation. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year>
Reference-contexts: It is worth highlighting that, the previous equation is a sum of radial basis function <ref> [18] </ref> and it can be also proved that Eq. (4) is a solution of the regularization problem [16], [17].
Reference: [19] <author> C.M. Privitera and P. </author> <title> Morasso. The analysis of continuous temporal sequences by a map of sequential leaky integrators. </title> <booktitle> In Proceedings of IEEE International Conference on Neural Networks, </booktitle> <volume> volume 5, </volume> <pages> pages 3127-3130, </pages> <address> Orlando, Florida, </address> <year> 1994. </year>
Reference-contexts: Thus one can view each element as a recognizer of a complex event. In view of the above, each element is composed of a set of sub-elements | classically called taps <ref> [19] </ref> | each of which represents different critical points in the temporal evolution of the input vector. Specifically, if an element recognizes an event composed of m temporal features, it consists of m distinct taps. <p> The learning of tap weights can be done in a traditional Hebbian manner exploiting a unique vector 2 &lt; Kfim for each iteration. In <ref> [19] </ref> an alternative learning method has been presented where the taps are learned in real time using a sequence of input vectors. <p> This temporal property of D-SOM can be exploited in the context of motor control, time series analysis, temporal sequence representation and recognition (see [15], <ref> [19] </ref>, [20], 2 Some basic simplification has been assumed in this case for the sake of exposition. The simplification is that the first reference point of the event in progress, is specific to that event.
Reference: [20] <author> C.M. Privitera and P. </author> <title> Morasso. A neural model for the execution of symbolic motor programs. </title> <editor> In M. Marinaro and P. Morasso, editors, </editor> <booktitle> International Conference on Artficial Neural Networks, </booktitle> <pages> pages 254-257, </pages> <address> London, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This temporal property of D-SOM can be exploited in the context of motor control, time series analysis, temporal sequence representation and recognition (see [15], [19], <ref> [20] </ref>, 2 Some basic simplification has been assumed in this case for the sake of exposition. The simplification is that the first reference point of the event in progress, is specific to that event.
Reference: [21] <author> C.M. Privitera and R. Plamondon. </author> <title> A self-organizing neural network for learning and generating sequences of target-directed movements in the context of a delta-lognormal theory. </title> <booktitle> In Proc. IEEE Int. Conf. on Neural Networks IEEE ICNN'95 Perth, Australia, </booktitle> <volume> volume 4, </volume> <pages> pages 1999-2004, </pages> <address> Perth, Australia, </address> <year> 1995. </year>
Reference-contexts: Usually, during the first part of an input temporal stream, the resonance distribution observed is located in zones that could be far away from the final localization: this is because some different temporal events can share initial segments. 6 corresponding resonance distribution on the map is displayed. 7 [22], <ref> [21] </ref>).
Reference: [22] <author> C.M. Privitera, V. Sanguineti, and P. </author> <title> Morasso. Temporal sequences generation and computational maps. </title> <booktitle> In Proceedings of NeuroNimes'93: Neural Networks and their Industrial and Cognitive Applications, </booktitle> <pages> pages 55-64, </pages> <address> Nimes, France, </address> <year> 1993. </year>
Reference-contexts: Usually, during the first part of an input temporal stream, the resonance distribution observed is located in zones that could be far away from the final localization: this is because some different temporal events can share initial segments. 6 corresponding resonance distribution on the map is displayed. 7 <ref> [22] </ref>, [21]).
Reference: [23] <author> M. Reiss and J.G. Taylor. </author> <title> Storing temporal sequences. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 773-787, </pages> <year> 1991. </year>
Reference-contexts: Thus the membrane potential of an element is correlated with the temporal evolution of its input and the element can not only analyze a static input vector *, it can also perform a temporal integration of the function u i (*(t)) (hence the name, Leaky Integrator element (see for example <ref> [23] </ref>)). If this temporal integration depends on the occurrence of certain spatio-temporal features/sub-events in the evolving temporal input, then the activation of the element indicates the occurrence of an external event composed of these sub-events. Thus one can view each element as a recognizer of a complex event.
Reference: [24] <author> H. Ritter. </author> <title> Learning with the self-organizing map. </title> <editor> In T. Kohonen, K. Makisara, O. Sim-ula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <address> Amsterdam, 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: element of the map a second output vector Y i 2 &lt; Q (see Figure 2) so that the map can finally be seen as a system with two layers of weights defining a generic mapping f : &lt; K ! &lt; Q , where usually K &gt; Q ([25], <ref> [24] </ref>).
Reference: [25] <author> H. Ritter and K. Schulten. </author> <title> Extending kohonen's self-organizing mapping algorithm to learn ballistic movements. </title> <editor> In R. Eckmiller and C. Malsburg, editors, </editor> <booktitle> Neural Computers, </booktitle> <pages> pages 393-406, </pages> <address> Berlin, 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference: [26] <author> L. Shastri and V. Ajjanagadde. </author> <title> From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings using temporal synchrony. </title> <journal> Behavioral and brain sciences, </journal> <volume> 16 </volume> <pages> 417-494, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Many cognitive processes must be compositional, that is, they must be capable of extracting and constructing progressively complex and structured representations in a hierarchical manner, starting with simple primitive elements [4], [3], <ref> [26] </ref>. For example, visual recognition of complex objects and events requires the rapid assembly of diverse information about the subcomponents of the object or event. In this case, primitive elements are combined into more complex associations until a complete meaningful representation of the visual input is achieved [5], [8], [14]. <p> A central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation [6], <ref> [26] </ref>, [27]. Recent neurological data highlights the potential role of the temporal structure of neural activity in the expression and propagation of dynamic bindings [28], [27]. <p> A particularly promising hypothesis is that all information pertaining to a single entity is bound together as a result of the synchronous firing of the various nodes encoding information about this entity [30], <ref> [26] </ref>. Thus compositionality, binding, and temporal pattern processing are interrelated and seem to play an essential role in cognition [3]. In this work, compositionality is achieved by a hierarchical model of Self-Organizing Maps (SOM) and the temporal processing ability is achieved by exploiting neurons capable of supporting short-term memory (STM).
Reference: [27] <author> L. Shastri and D. Grannes. </author> <title> A connectionist treatment of negation and inconsistency. </title> <booktitle> In Proc. of Cognitive Science Society Conference (to appear), </booktitle> <address> San Diego, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: A central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation [6], [26], <ref> [27] </ref>. Recent neurological data highlights the potential role of the temporal structure of neural activity in the expression and propagation of dynamic bindings [28], [27]. <p> central requirement of compositionality and hierarchical processing is the maintenance and propagation of dynamic bindings between appropriate components of a multi-level and distributed representation [6], [26], <ref> [27] </ref>. Recent neurological data highlights the potential role of the temporal structure of neural activity in the expression and propagation of dynamic bindings [28], [27]. A particularly promising hypothesis is that all information pertaining to a single entity is bound together as a result of the synchronous firing of the various nodes encoding information about this entity [30], [26].
Reference: [28] <author> W. Singer. </author> <title> Synchronization of cortical activity and its putative role in information processing and learning. Annu. </title> <journal> Rev. Physiol., </journal> <volume> 55 </volume> <pages> 349-74, </pages> <year> 1993. </year>
Reference-contexts: Recent neurological data highlights the potential role of the temporal structure of neural activity in the expression and propagation of dynamic bindings <ref> [28] </ref>, [27]. A particularly promising hypothesis is that all information pertaining to a single entity is bound together as a result of the synchronous firing of the various nodes encoding information about this entity [30], [26].
Reference: [29] <author> D. F. Specht. </author> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 109-118, </pages> <year> 1990. </year>
Reference-contexts: In this sense, an input vector fed to the map involves an internal place-coded probability resonance distribution having the maximum peak just located on the most similar element (see for instance <ref> [29] </ref>). 3 manifold: this tile is centered around the weight vector i .
Reference: [30] <author> C. von der Malsburg. </author> <title> Am i thinking assemblies? Brain Theory, </title> <year> 1986. </year>
Reference-contexts: A particularly promising hypothesis is that all information pertaining to a single entity is bound together as a result of the synchronous firing of the various nodes encoding information about this entity <ref> [30] </ref>, [26]. Thus compositionality, binding, and temporal pattern processing are interrelated and seem to play an essential role in cognition [3].
Reference: [31] <author> D.L. Wang and M.A. Arbib. </author> <title> Timing and chunking in processing temporal order. </title> <journal> IEEE Trans. on System, Man, and Cybernetics, </journal> <volume> 23(4) </volume> <pages> 993-1009, </pages> <year> 1993. </year> <month> 21 </month>
Reference-contexts: The learning of tap weights can be done in a traditional Hebbian manner exploiting a unique vector 2 &lt; Kfim for each iteration. In [19] an alternative learning method has been presented where the taps are learned in real time using a sequence of input vectors. In <ref> [31] </ref> another method is proposed where taps are pre-determinated and the learning process only effects the connections between taps and neuron. three distinct time instants and the arrows on the left side of the Figure indicate three input temporal features of the map for each of the three instants: the exact
References-found: 31

