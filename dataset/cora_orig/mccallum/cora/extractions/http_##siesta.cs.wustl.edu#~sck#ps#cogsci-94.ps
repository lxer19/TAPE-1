URL: http://siesta.cs.wustl.edu/~sck/ps/cogsci-94.ps
Refering-URL: http://siesta.cs.wustl.edu/~sck/
Root-URL: 
Email: sck@cs.wustl.edu  johnson@cs.wustl.edu  barry@cs.wustl.edu  
Title: Recurrent Natural Language Parsing  
Author: Stan C. Kwasny Sahnny Johnson Barry L. Kalman 
Address: St. Louis, Missouri 63130-4899  Knox College Galesburg, Illinois 61401-4999  St. Louis, Missouri 63130-4899  
Affiliation: Department of Computer Science Washington University  Department of Mathematics Computer Science  Department of Computer Science Washington University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Christiansen, M. </author> <year> (1992). </year> <title> The (Non) Necessity of Recursion in Natural Language Processing. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 665-670). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Church, K. </author> <year> (1982). </year> <title> On Memory Limitations in Natural Language Processing. </title> <address> Bloomington, </address> <note> IN: Indiana University Linguistics Club. </note>
Reference: <author> Das, S., Giles, C. L. & Sun, G.Z. </author> <year> (1992). </year> <title> Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 791-796). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Simple recurrent networks possess the power of a finite state machine, but as such can approximate, to a finite degree, the processing requirements of a context free language. Unlike recurrent parsing networks described elsewhere <ref> (see, for example, Das et al., 1992) </ref>, our system contains no explicit internal or external stack. The stack's role is assumed to a sufficient degree by the recurrence in the network.
Reference: <author> J. L. Elman, J.L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14(2), </volume> <pages> 179-212. </pages>
Reference-contexts: Casting the rules as a neural network yielded significant advantages in robustness and we have studied lexical ambiguity, grammatical ill-formedness, lexical omission, and other properties of the design. Current Work In our current work, a simple recurrent network <ref> (Elman, 1990) </ref> is utilized which permits some degree of memory of past decisions.
Reference: <author> Finch, S. & Chater, N. </author> <year> (1992). </year> <title> Bootstrapping Syntactic Categories. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 820-825). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Kalman, B.L. </author> <year> (1990). </year> <title> Super linear learning in back propagation neural nets (Tech Rep WUCS-90-21). </title> <institution> St. Louis: Washington University, Department of Computer Science. </institution>
Reference-contexts: The first value is from training and second value is from testing. of 58-24-39 for training and 62-24-39 for testing. Training itself is performed using a modified conjugate-gradient method <ref> (Kalman, 1990) </ref> that has been implemented to utilize the parallelism available on a 20-processor Sun SPARC Center 2000 machine. Results Training proceeded until the number of errors encountered in the training set was small and further training increased the number of errors in the testing set.
Reference: <author> Kalman, B. L., Kwasny, S. C. & Abella, A. </author> <year> (1993). </year> <title> Decomposing input patterns to facilitate training. </title> <booktitle> In Proceedings of the World Congress on Neural Networks (pp. </booktitle> <address> III-503-III-506). Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Kwasny, S. C. & Kalman, B. L. </author> <year> (1990). </year> <title> Connectionism and determinism in a syntactic parser. </title> <journal> Connection Science, </journal> <volume> 2(1&2), </volume> <pages> 63-82. </pages>
Reference: <author> Kwasny, S. C. & Kalman, B. L. </author> <year> (1991). </year> <title> The case of the unknown word: Imposing syntactic constraints on words missing from the lexicon. </title> <booktitle> In Proceedings of the Third Midwest Artificial Intelligence and Cognitive Science Society Conference (pp. </booktitle> <pages> 46-50). </pages> <address> Carbondale, IL: </address> <booktitle> Midwest Artificial Intelligence and Cognitive Science Society. </booktitle>
Reference: <author> Kwasny, S. C. & Faisal, K. A. </author> <year> (1992). </year> <title> Symbolic parsing via sub-symbolic rules. </title> <editor> In J. Dinsmore (Ed.), </editor> <title> Closing the Gap: Symbolism vs. </title> <booktitle> Connectionism. </booktitle> <pages> (pp. 209-235). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> S. E. Lee, S. E. & Holt, B. R. </author> <year> (1992). </year> <title> Regression analysis of spectroscopic process data using a combined architecture of linear and nonlinear neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (pp. </booktitle> <address> IV-549-IV-554). Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: These activation patterns encode information about relevant past events so that current and future decisions can be influenced, much as the stack would influence symbolic processing. Since there is evidence that isolating the linearly separable relationship from the nonlinear part makes for more effective training <ref> (Lee and Holt, 1992) </ref>, and since parsing seems to require approximately linear processing for most of the steps, we adapted the recurrent network designed by Elman to include direct connections from input to output units. The parser is illustrated in Figure 2.
Reference: <author> Marcus, M. P. </author> <year> (1980). </year> <title> A theory of syntactic recognition for natural language. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Meeting this challenge is a major focus of our work. From the basic design of a deterministic natural language parser, we are introducing and evaluating connectionist techniques and evolving toward a fully connectionist language understanding system. Deterministic (wait-and-see) parsing <ref> (Marcus, 1980) </ref> is a good model for the design of a neural network based parser fore two reasons. First, it concentrates on syntax, which has the advantage of being relatively well understood and therefore leads to results which can be easily compared.
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. </title> <editor> In S. Pinker & J. Mehler (Eds), </editor> <booktitle> Connections and symbols. </booktitle> <pages> (pp. 73-193). </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Pulman, S. G. </author> <year> (1986). </year> <title> Grammars, parsers, and memory limitations. </title> <booktitle> Language and Cognitive Processes 2, </booktitle> <pages> 197-225. </pages>
Reference-contexts: Such a performance-oriented approach (as opposed to a competence-oriented one) is intentional. There is growing belief that : : : human parsing resources must be characterized as finite-state <ref> (Pulman, 1986) </ref> and that full recursion, as supported by context-free languages, may be unnecessary (Christiansen, 1992; Church, 1982). Simple recurrent networks possess the power of a finite state machine, but as such can approximate, to a finite degree, the processing requirements of a context free language.
Reference: <author> Weckerly, J. & Elman, J. L. </author> <year> (1992). </year> <title> A PDP approach to processing center-embedded sentences. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 414-419). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
References-found: 15

