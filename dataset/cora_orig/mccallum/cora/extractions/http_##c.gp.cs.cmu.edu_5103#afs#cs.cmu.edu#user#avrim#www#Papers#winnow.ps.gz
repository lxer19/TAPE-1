URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/winnow.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/avrim/www/Papers/pubs.html
Root-URL: http://www.cs.cmu.edu
Title: Empirical Support for Winnow and Weighted-Majority  
Keyword: Winnow, Weighted-Majority, Multiplicative algorithms.  
Note: Running head: Empirical Support for Winnow and Weighted-Majority  
Abstract: Algorithms: Results on a Calendar Scheduling Domain Abstract This paper describes experimental results on using Winnow and Weighted-Majority based algorithms on a real-world calendar scheduling domain. These two algorithms have been highly studied in the theoretical machine learning literature. We show here that these algorithms can be quite competitive practically, outperforming the decision-tree approach currently in use in the Calendar Apprentice system in terms of both accuracy and speed. One of the contributions of this paper is a new variant on the Winnow algorithm (used in the experiments) that is especially suited to conditions with string-valued classifications, and we give a theoretical analysis of its performance. In addition we show how Winnow can be applied to achieve a good accuracy/coverage tradeoff and explore issues that arise such as concept drift. We also provide an analysis of a policy for discarding predictors in Weighted-Majority that allows it to speed up as it learns. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Armstrong, R., Freitag, D., Joachims, T., and Mitchell, T. </author> <year> (1995). </year> <title> Webwatcher: A learning apprentice for the world wide web. </title> <booktitle> In 1995 AAAI Spring Symposium on Information Gathering from Heterogeneous Distributed Environments. </booktitle>
Reference-contexts: In fact, Armstrong et al. <ref> (Armstrong et al., 1995) </ref> report similarly good accuracy-coverage performance using Winnow for a Web link-prediction task. 4.5 Pruning The weighted-majority algorithm brings up the following exciting notion: by dropping poorly performing experts from consideration (neither asking for their predictions nor updating their weights), one can have an algorithm that speeds up
Reference: <author> Blum, A. </author> <year> (1992). </year> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 373-386. </pages>
Reference: <author> Blum, A., Hellerstein, L., and Littlestone, N. </author> <year> (1991). </year> <title> Learning in the presence of finitely or infinitely many irrelevant attributes. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 157-166, </pages> <address> Santa Cruz, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. and Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference: <author> Cesa-Bianchi, N., Freund, Y., Helmbold, D., Haussler, D., Schapire, R., and Warmuth, M. </author> <year> (1993). </year> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Annual ACM Symp. on the Theory of Computing, </booktitle> <pages> pages 382-391. </pages>
Reference: <author> Dent, L., Boticario, J., McDermott, J., Mitchell, T., and Zabowski, D. </author> <year> (1992). </year> <title> A personal learning apprentice. </title> <booktitle> In Proceedings of the 1992 National Conference on Artificial Intelligence. </booktitle>
Reference: <author> DeSantis, A., Markowsky, G., and Wegman, M. </author> <year> (1988). </year> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 29th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 110-119. </pages>
Reference: <author> Feller, W. </author> <year> (1968). </year> <title> An Introduction to Probability and its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition. </address>
Reference-contexts: The probability that either of these events ever happens is at most n t=1 1 X 1=t 2 &lt; ffi: One can improve the dependence on t in the above bound somewhat by arguing as in the proof of the law of the iterated logarithm <ref> (Feller, 1968) </ref>. The second theorem states that with a more aggressive policy, even though one may remove the optimal expert, one still can ensure that with high probability at least some near-optimal expert remains.
Reference: <author> Jourdan, J., Dent, L., McDermott, J., and Zabowski, D. </author> <year> (1991). </year> <title> Interfaces that learn: A learning apprentice for calendar management. </title> <type> Technical Report CMU-CS-91-135, </type> <institution> Carnegie Mellon University. </institution> <note> 18 Littlestone, </note> <author> N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference: <author> Littlestone, N. </author> <year> (1989). </year> <title> Mistake bounds and logarithmic linear-threshold learning algorithms. </title> <type> PhD thesis, </type> <address> U. C. Santa Cruz. </address>
Reference-contexts: Specialists are rewarded not so much for predicting correctly, but for predicting correctly in times when the global algorithm should have listened to them more carefully. The algorithm as described above can be viewed as a variant on the "balanced" version of Winnow <ref> (Littlestone, 1989) </ref>. In the "balanced" algorithm, one would maintain a vector of weights for each specialist, one weight for each possible output.
Reference: <author> Littlestone, N. </author> <year> (1991). </year> <title> Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> Santa Cruz, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Littlestone, N. and Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261. </pages>
Reference-contexts: In settling on their approach, the CAP designers also tried several other learning methods, including other decision tree variants and a neural-network algorithm, and found these to have equal or worse performance. 3 Description of the algorithms We consider two multiplicative-weight-updating learning algorithms: the Weighted-Majority algorithm <ref> (Littlestone and Warmuth, 1994) </ref> and a version of Littlestone's Winnow algorithm (Littlestone, 1988). These are both incremental algorithms. Upon receipt of an unlabeled example they make a prediction. <p> For example, if the AI-seminars are moved from Mondays to Fridays, in our approach this changes a specialist's behavior after 3 mistakes. In the balanced algorithm, this might cause a much larger number of mistakes, even if weights are lower-bounded as in <ref> (Littlestone and Warmuth, 1994) </ref>.
Reference: <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., and Zabowski, D. </author> <year> (1994). </year> <title> Experience with a personal learning assistant. </title> <journal> CACM, </journal> <volume> 37(7) </volume> <pages> 81-91. 19 </pages>
Reference-contexts: More specifically, CAP maintains a database of rules, sorted by observed performance. Each night, CAP builds a decision tree using the most recent 180 example, prunes this tree into a list of rules, and merges these rules into its database, updating its statistics <ref> (Mitchell et al., 1994) </ref>. <p> Strangely, for very high cutoffs, the performance in predicting day-of-week for User 1 decreases somewhat: the exact cause of this is unclear. The performance of Winnow here compares quite well with that of CAP <ref> (Mitchell et al., 1994) </ref>.
References-found: 13

