URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-719.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: Experimental Study of Minimum Cut Algorithms  
Author: by Matthew S. Levine David R. Karger Arthur C. Smith 
Degree: SUBMITTED TO THE DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE IN PARTIAL FULFILLMENT FOR THE DEGREE OF MASTER OF SCIENCE IN COMPUTER SCIENCE AT THE  All Rights Reserved Signature of Author  Certified by  Assistant Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Note: c fl1997  
Date: 1995  MAY 1997  May 9, 1997  
Address: Princeton University,  
Affiliation: A.B. Computer Science  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. K. Ahuja, J. B. Orlin, and R. E. Tarjan. </author> <title> Improved Time Bounds for the Maximum Flow Problem. </title> <journal> SIAM J. Comput., </journal> <volume> 18:939954, </volume> <year> 1989. </year>
Reference-contexts: Subsequently there was much progress in computing maximum flows, but no one has yet been able to prove a time bound better than O (nm) for any of the best algorithms <ref> [1, 9, 10, 25, 41] </ref>. Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows. <p> We then add one random cycle on all vertices, so the graph will be connected, and add the remaining m - n edges at random. Every edge added gets a random capacity. If the endpoints have different colors, the capacity is chosen uniformly at random from <ref> [1; 100] </ref>; otherwise the capacity is chosen uniformly at random from [1; 100P]. Following [48], we tested on 6 subfamilies. Our families are the same in spirit as those of [48], but we use larger problem sizes and we added some data points where we felt it was appropriate. <p> Every edge added gets a random capacity. If the endpoints have different colors, the capacity is chosen uniformly at random from [1; 100]; otherwise the capacity is chosen uniformly at random from <ref> [1; 100P] </ref>. Following [48], we tested on 6 subfamilies. Our families are the same in spirit as those of [48], but we use larger problem sizes and we added some data points where we felt it was appropriate. <p> The dotted line shows one of many near minimum cuts (value 2006). 4.1. EXPERIMENT DESIGN 69 * c the type of graph to generate (1 or 2). If c = 1, for each pair of vertices, with probability d, we include an edge with weight uniformly distributed in <ref> [1; 100] </ref>. If c = 2, we split the graph into two components, one containing vertices 1 through n=2 and the other containing vertices n=2 + 1 through n. Again, for each pair of vertices, we include an edge with probability d. <p> Again, for each pair of vertices, we include an edge with probability d. If the two vertices are in the same component, the edge weight is chosen uniformly from <ref> [1; 100n] </ref>, but if the vertices are in different components, the edge weight is chosen uniformly from [1; 100]. <p> Again, for each pair of vertices, we include an edge with probability d. If the two vertices are in the same component, the edge weight is chosen uniformly from [1; 100n], but if the vertices are in different components, the edge weight is chosen uniformly from <ref> [1; 100] </ref>.
Reference: [2] <author> R. J. Anderson and J. C. Setubal. </author> <title> Goldberg's Algorithm for the Maximum Flow in Perspective: a Computational Study. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: </title> <booktitle> First DIMACS Implementation Challenge, </booktitle> <pages> pages 118. </pages> <publisher> AMS, </publisher> <year> 1993. </year>
Reference-contexts: Meanwhile, we use the tests to find weaknesses in the implementation and devise heuristics to improve performance. Thus it is difficult to be sure when we are done. For HO we take advantage of implementation work on maximum flow algorithms <ref> [2, 13, 16, 17, 50] </ref>; for NI we take advantage of the work of Nagamochi et al. KS and K were both developed from scratch, so it remains possible that their inferior performance is due to the fact that we are the first to develop heuristics for them. <p> We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic [12, 16]. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 13, 16, 50] </ref> and is essential for the analysis of the Hao-Orlin algorithm.
Reference: [3] <author> D. L. Applegate and W. J. Cook. </author> <type> Personal communication. </type> <year> 1996. </year>
Reference-contexts: If edges of a network fail with some probability, it makes intuitive sense that the greatest danger of network disconnection is at a minimum cut. Minimum cuts also arise in information retrieval [6], compilers for parallel languages [7], and cutting-plane algorithms for the Traveling Salesman Problem (TSP) <ref> [3] </ref>. We have also received requests for our codes from researchers interested in routing in ATM networks and computational biology. 6 CHAPTER 1. INTRODUCTION 1.1 Previous Work The problem of finding a minimum cut has a long history. <p> Thus, cutting plane algorithms for the traveling salesman problem must solve a large number of minimum cut problems (see [43] for a survey of the area). We obtained some of the minimum cut instances that were solved by Applegate and Cook <ref> [3] </ref> in their TSP solver. These are clearly desirable test data, as they are from a real-world application. The Padberg-Rinaldi heuristics are very effective on the TSP instances.
Reference: [4] <author> T. Badics and R. Boros. </author> <title> Implementing a Maximum Flow Algorithm: Experiments with Dynamic Trees. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: </title> <booktitle> First DIMACS Implementation Challenge, </booktitle> <pages> pages 4364. </pages> <publisher> AMS, </publisher> <year> 1993. </year>
Reference-contexts: We eventually decided to select between the simple method and the fancy method on-line, based on the density of the graph. The Fancy Way We used an implementation of dynamic trees written by Tamas Badics <ref> [4] </ref> for the first DIMACS implementation challenge. This implementation uses splay trees, which is likely the most practical approach. We made some non-obvious changes from the theory description in implementing this method.
Reference: [5] <author> F. Barahona. </author> <title> Packing Spanning Trees. </title> <institution> Mathematics of Operations Research, 20(1):104115, </institution> <year> 1995. </year>
Reference-contexts: It is possible to give a strongly polynomial tree packing algorithm <ref> [24, 5] </ref>, but the time bounds are not better than O (nm). Karger finesses the 34 CHAPTER 2. BACKGROUND problem by showing that we can get by with a tree packing in a subgraph that does have a small minimum cut.
Reference: [6] <author> R. A. Botafogo. </author> <title> Cluster Analysis for Hypertext Systems. </title> <booktitle> In Proc. of the 16th Annual ACM SIGIR Conference of Res. and Dev. in Info. Retrieval, </booktitle> <pages> pages 116125, </pages> <year> 1993. </year>
Reference-contexts: An easy example is network reliability theory [37, 53]. If edges of a network fail with some probability, it makes intuitive sense that the greatest danger of network disconnection is at a minimum cut. Minimum cuts also arise in information retrieval <ref> [6] </ref>, compilers for parallel languages [7], and cutting-plane algorithms for the Traveling Salesman Problem (TSP) [3]. We have also received requests for our codes from researchers interested in routing in ATM networks and computational biology. 6 CHAPTER 1.
Reference: [7] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T. J. Sheffler. </author> <title> Array Distribution in Data-Parallel Programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, pages 7691. Lecture Notes in Computer Science series, </booktitle> <volume> vol. 896, </volume> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: An easy example is network reliability theory [37, 53]. If edges of a network fail with some probability, it makes intuitive sense that the greatest danger of network disconnection is at a minimum cut. Minimum cuts also arise in information retrieval [6], compilers for parallel languages <ref> [7] </ref>, and cutting-plane algorithms for the Traveling Salesman Problem (TSP) [3]. We have also received requests for our codes from researchers interested in routing in ATM networks and computational biology. 6 CHAPTER 1. INTRODUCTION 1.1 Previous Work The problem of finding a minimum cut has a long history.
Reference: [8] <author> C. S. Chekuri, A. V. Goldberg, D. R. Karger, M. S. Levine, and C. Stein. </author> <title> Experimental Study of Minimum Cut Algorithms. </title> <booktitle> In Proc. 8th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 324333, </pages> <year> 1997. </year>
Reference-contexts: Note that this paper represents joint work with Chandra Chekuri, Andrew Goldberg, David Karger, and Clifford Stein. A preliminary version appeared in SODA 97 <ref> [8] </ref>. The paper is organized as follows. In Chapter 2 we review the theory behind the minimum cut algorithms, including definitions, characterizations of the problem, and descriptions of the algorithms. In Chapter 3 we discuss general implementation issues and details of each algorithm in turn.
Reference: [9] <author> J. Cheriyan and T. Hagerup. </author> <title> A randomized maximum flow algorithm. </title> <booktitle> In Proc. 30th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 118123, </pages> <year> 1989. </year>
Reference-contexts: Subsequently there was much progress in computing maximum flows, but no one has yet been able to prove a time bound better than O (nm) for any of the best algorithms <ref> [1, 9, 10, 25, 41] </ref>. Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows.
Reference: [10] <author> J. Cheriyan, T. Hagerup, and K. Mehlhorn. </author> <title> Can a maximum flow be computed in o(nm) time? In M. </title> <editor> S. Paterson, editor, </editor> <booktitle> Proc. 17th ICALP, Lecture Notes in Computer Science #443, </booktitle> <pages> pages 235248, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year> <note> An extended abstract is also available as ALCOM-90-26, </note> <institution> ESPRIT II Basic Research Actions Program Project no. </institution> <address> 3075 (ALCOM). </address>
Reference-contexts: Subsequently there was much progress in computing maximum flows, but no one has yet been able to prove a time bound better than O (nm) for any of the best algorithms <ref> [1, 9, 10, 25, 41] </ref>. Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows.
Reference: [11] <author> J. Cheriyan and S. N. Maheshwari. </author> <title> Analysis of Preflow Push Algorithms for Maximum Netwrok Flow. </title> <note> SIAM J. Comput., 18:10571086, 1989. 124 BIBLIOGRAPHY </note>
Reference-contexts: This strategy, in combination with appropriate heuristics, seems to give the best results in practice [13]. It also permits a better bound on the number of non-saturating push operations: O (n 2 p m) <ref> [11] </ref>. HighestLabelPushRelabel (G; s; t) for all v, d (v) 0 saturate all arcs out of s while there exists an active vertex Discharge (an active vertex with maximum distance label) We now sketch the proof of this time bound. Call the time between successive relabels a phase.
Reference: [12] <author> B. V. Cherkassky. </author> <title> A Fast Algorithm for Computing Maximum Flow in a Network. </title> <editor> In A. V. Karzanov, editor, </editor> <booktitle> Collected Papers, </booktitle> <volume> Vol. </volume> <month> 3: </month> <title> Combinatorial Methods for Flow Problems, </title> <booktitle> pages 9096. The Institute for Systems Studies, </booktitle> <address> Moscow, </address> <year> 1979. </year> <title> In Russian. </title> <journal> English translation appears in AMS Trans., </journal> <volume> Vol. 158, </volume> <pages> pp. 2330, </pages> <year> 1994. </year>
Reference-contexts: We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic <ref> [12, 16] </ref>. This heuristic often speeds up push-relabel algorithms for the maximum flow problem [2, 13, 16, 50] and is essential for the analysis of the Hao-Orlin algorithm.
Reference: [13] <author> B. V. Cherkassky and A. V. Goldberg. </author> <title> On Implementing Push-Relabel Method for the Maximum Flow Problem. </title> <type> Technical Report STAN-CS-94-1523, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Meanwhile, we use the tests to find weaknesses in the implementation and devise heuristics to improve performance. Thus it is difficult to be sure when we are done. For HO we take advantage of implementation work on maximum flow algorithms <ref> [2, 13, 16, 17, 50] </ref>; for NI we take advantage of the work of Nagamochi et al. KS and K were both developed from scratch, so it remains possible that their inferior performance is due to the fact that we are the first to develop heuristics for them. <p> One possibility is the highest label strategy: discharge an active vertex with the highest distance label. This strategy, in combination with appropriate heuristics, seems to give the best results in practice <ref> [13] </ref>. It also permits a better bound on the number of non-saturating push operations: O (n 2 p m) [11]. <p> We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic [12, 16]. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 13, 16, 50] </ref> and is essential for the analysis of the Hao-Orlin algorithm. <p> It took us a long time to arrive at the strategies described here. We recommend comparing any new strategies against the ones we use. 48 CHAPTER 3. IMPLEMENTATION 3.3 Hao-Orlin Algorithm We based our implementation ho on the push-relabel max-flow code of Cherkassky and Gold-berg <ref> [13] </ref>. Thus several implementation decisions were made based experience from the max-flow context. Such decisions are likely appropriate, but not above question. We begin by discussing choices we made in the implementation that are not addressed by the algorithm. <p> Contractions tend not to happen in groups, so we use compact contraction everywhere in ho. Push-Relabel Strategy We chose to use the highest-label strategy to pick which nodes to discharge first. This strategy seems to give the best results in practice in the max-flow context <ref> [13] </ref>. For this reason we did not consider the fancy approach that uses dynamic trees. We use an array of buckets B [0 : : :2n - 1] to implement this efficiently.
Reference: [14] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1990. </year>
Reference-contexts: Dotted lines are pointers for the set-union data structure. 3.2. THE PADBERG-RINALDI HEURISTICS 45 SetUnionContract (v,w) union (v,w) We implement the disjoint-set union data structure by disjoint-set forests with path compression; see e.g. <ref> [14] </ref>. In this representation each set has a distinct representative, each vertex has a pointer towards (but not necessarily directly to) the representative of its set, and representatives point to themselves. We assume that the reader is familiar with the set-union data structure.
Reference: [15] <author> G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson. </author> <title> Solution of a Large-Scale Traveling Salesman Problem. </title> <type> Oper. </type> <institution> Res., 2:393410, </institution> <year> 1954. </year>
Reference-contexts: One set of inequalities that has been very useful is subtour elimination constraints, first introduced by Dantzig, Fulkerson, and Johnson <ref> [15] </ref>. The problem of identifying a subtour elimination constraint can be rephrased as the problem of finding a minimum cut in a graph with real-valued edge weights.
Reference: [16] <author> U. Derigs and W. Meier. </author> <title> Implementing Goldberg's Max-Flow Algorithm A Computational Investigation. ZOR Methods and Models of Operations Research, </title> <address> 33:383403, </address> <year> 1989. </year>
Reference-contexts: Meanwhile, we use the tests to find weaknesses in the implementation and devise heuristics to improve performance. Thus it is difficult to be sure when we are done. For HO we take advantage of implementation work on maximum flow algorithms <ref> [2, 13, 16, 17, 50] </ref>; for NI we take advantage of the work of Nagamochi et al. KS and K were both developed from scratch, so it remains possible that their inferior performance is due to the fact that we are the first to develop heuristics for them. <p> We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic <ref> [12, 16] </ref>. This heuristic often speeds up push-relabel algorithms for the maximum flow problem [2, 13, 16, 50] and is essential for the analysis of the Hao-Orlin algorithm. <p> We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic [12, 16]. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 13, 16, 50] </ref> and is essential for the analysis of the Hao-Orlin algorithm.
Reference: [17] <author> U. Derigs and W. Meier. </author> <title> An Evaluation of Algorithmic Refinements and Proper Data-Structures for the Preflow-Push Approach for Maximum Flow. </title> <booktitle> In ASI Series on Computer and System Sciences, </booktitle> <volume> volume 8, </volume> <pages> pages 209223. NATO, </pages> <year> 1992. </year>
Reference-contexts: Meanwhile, we use the tests to find weaknesses in the implementation and devise heuristics to improve performance. Thus it is difficult to be sure when we are done. For HO we take advantage of implementation work on maximum flow algorithms <ref> [2, 13, 16, 17, 50] </ref>; for NI we take advantage of the work of Nagamochi et al. KS and K were both developed from scratch, so it remains possible that their inferior performance is due to the fact that we are the first to develop heuristics for them.
Reference: [18] <author> E. A. Dinic, A. V. Karzanov, and M. V. Lomonosov. </author> <title> On the structure of a family of minimum weighted cuts in a graph. </title> <editor> In A. A. Fridman, editor, </editor> <booktitle> Studies in Discrete Optimization, </booktitle> <pages> pages 290306. </pages> <publisher> Nauka Publishers, </publisher> <year> 1976. </year>
Reference-contexts: Note that there may be more than one minimum cut. In fact, in a cycle where every edge has the same capacity, there are n of them. (The cycle is actually the worst case. This result is shown by Dinitz, Karzanov, and Lomonosov <ref> [18] </ref> and also follows easily from the correctness proof of KS.) Since we only look for one minimum cut, we sometimes fix one minimum cut and refer to it as the minimum cut. 2.1 Flow Based Approaches The first approach to solving the minimum cut problem was based on a related
Reference: [19] <author> J. Edmonds. </author> <title> Submodular functions, matroids, and certain polyhedra. </title> <booktitle> In Calgary International Conf. on Combinatorial Structures and their Applications, </booktitle> <pages> pages 6987, </pages> <address> New York, 1969. </address> <publisher> Gordon and Breach. </publisher>
Reference-contexts: Two theorems of Edmonds relate a-cuts and a-arborescences: Theorem 2.3.1 [20] In a directed graph the maximum number of edge-disjoint a-arborescences equals the minimum value of an a-cut. Theorem 2.3.2 <ref> [19] </ref> The edges of a directed graph can be partitioned into k a-arborescences if and only if they can be partitioned into k spanning trees where every vertex except a has in-degree k. We refer to a set of edge-disjoint trees as a tree packing.
Reference: [20] <editor> J. Edmonds. Edge-disjoint branchings. In R. Rustin, editor, </editor> <booktitle> Combinatorial Algorithms, </booktitle> <pages> pages 9196, </pages> <address> New York, 1972. </address> <publisher> Algorithmics Press. </publisher>
Reference-contexts: An a-cut is a cut; its value is the total capacity of edges crossing the partition from the side that includes a to the other. Two theorems of Edmonds relate a-cuts and a-arborescences: Theorem 2.3.1 <ref> [20] </ref> In a directed graph the maximum number of edge-disjoint a-arborescences equals the minimum value of an a-cut.
Reference: [21] <author> P. Elias, A. Feinstein, and C. E. Shannon. </author> <title> Note on Maximum Flow Through a Network. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-2:117199, </volume> <year> 1956. </year>
Reference-contexts: It was originally considered a harder variant of the minimum s-t cut problem, which places the further restriction that designated vertices s and t be on opposite sides of the partition. The well-known max-flowmin-cut theorem <ref> [22, 21] </ref> implies that an s-t minimum cut can be found by computing an s-t maximum flow. In 1961, Gomory and Hu showed how to solve the minimum cut problem with n - 1 s-t minimum cut computations. <p> An s-t cut is a cut that has s and t on opposite sides of the partition. 10 CHAPTER 2. BACKGROUND The minimum s-t cut, s;t (G), is the s-t cut of minimum value. The well-known max-flowmin-cut theorem <ref> [22, 21] </ref> implies that a minimum s-t cut can be found by computing the maximum flow between s and t.
Reference: [22] <author> L. R. Ford, Jr. and D. R. Fulkerson. </author> <title> Maximal Flow Through a Network. </title> <journal> Canadian Journal of Math., </journal> <volume> 8:399404, </volume> <year> 1956. </year>
Reference-contexts: It was originally considered a harder variant of the minimum s-t cut problem, which places the further restriction that designated vertices s and t be on opposite sides of the partition. The well-known max-flowmin-cut theorem <ref> [22, 21] </ref> implies that an s-t minimum cut can be found by computing an s-t maximum flow. In 1961, Gomory and Hu showed how to solve the minimum cut problem with n - 1 s-t minimum cut computations. <p> An s-t cut is a cut that has s and t on opposite sides of the partition. 10 CHAPTER 2. BACKGROUND The minimum s-t cut, s;t (G), is the s-t cut of minimum value. The well-known max-flowmin-cut theorem <ref> [22, 21] </ref> implies that a minimum s-t cut can be found by computing the maximum flow between s and t. <p> It is therefore natural to look for an augmenting trees algorithm to find a tree packing, analogous to the classical Ford-Fulkerson augmenting paths algorithm to find a maximum flow <ref> [22] </ref>. This is precisely what Gabow [23] gives. 2.3. TREE PACKINGS 31 The augmenting paths algorithm for maximum flow works by repeatedly finding a path from the source to the sink in the residual graph and sending as much flow along it as possible.
Reference: [23] <author> H. N. Gabow. </author> <title> A Matroid Approach to Finding Edge Connectivity and Packing Arbores-cences. </title> <institution> J. Comp. and Syst. Sci., 50:259273, </institution> <year> 1995. </year>
Reference-contexts: It is therefore natural to look for an augmenting trees algorithm to find a tree packing, analogous to the classical Ford-Fulkerson augmenting paths algorithm to find a maximum flow [22]. This is precisely what Gabow <ref> [23] </ref> gives. 2.3. TREE PACKINGS 31 The augmenting paths algorithm for maximum flow works by repeatedly finding a path from the source to the sink in the residual graph and sending as much flow along it as possible.
Reference: [24] <author> H. N. Gabow and H. H. Westermann. </author> <title> Forests, Frames and Games: Algorithms for Matroid Sums and Applications. </title> <journal> Algorithmica, </journal> <volume> 7(5):465497, </volume> <year> 1992. </year>
Reference-contexts: It is possible to give a strongly polynomial tree packing algorithm <ref> [24, 5] </ref>, but the time bounds are not better than O (nm). Karger finesses the 34 CHAPTER 2. BACKGROUND problem by showing that we can get by with a tree packing in a subgraph that does have a small minimum cut.
Reference: [25] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> A New Approach to the Maximum Flow Problem. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 35:921940, </volume> <year> 1988. </year>
Reference-contexts: Subsequently there was much progress in computing maximum flows, but no one has yet been able to prove a time bound better than O (nm) for any of the best algorithms <ref> [1, 9, 10, 25, 41] </ref>. Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows.
Reference: [26] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> Finding Minimum-Cost Circulations by Canceling Negative Cycles. </title> <booktitle> In Proc. 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 388397, </pages> <year> 1988. </year> <note> BIBLIOGRAPHY 125 </note>
Reference-contexts: Conveniently, however, the fastest current maximum flow algorithms and the Hao-Orlin minimum cut algorithm are both based on the push relabel method, so we review that method here. For a more detailed description see <ref> [26] </ref>. We begin with some additional definitions. The algorithm maintains a preflow, which is a relaxed version of a flow. <p> Another possibility is to use a FIFO queue to order discharge operations. In conjunction with dynamic trees, a sophisticated data-structure that makes it possible to do many non-saturating pushes at once, this method gives the best known time bound: O (nm log (n 2 =m)) <ref> [26] </ref>. We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic [12, 16]. <p> Our experience with this idea was that it usually just slowed down the code, so we stopped using it. Excess Detection We introduce a simple heuristic that often allows us to contract a vertex in the middle of a flow computation. The general results on the push-relabel method <ref> [26] </ref> imply that the excess at a vertex v is a lower bound on the capacity of the minimum s-v cut.
Reference: [27] <author> R. E. Gomory and T. C. Hu. </author> <title> Multi-terminal network flows. </title> <journal> J. SIAM, </journal> <volume> 9:551570, </volume> <year> 1961. </year>
Reference-contexts: GapRelabel (v) (conditions for Relabel apply) if v is the only vertex with distance label d (v) remove all w with d (w) d (v) else Relabel (v) 2.1.3 The Gomory-Hu Algorithm In 1961, Gomory and Hu <ref> [27] </ref> showed that s;t (G) for all n pairs of s and t could actually be computed using only n - 1 maximum flow computations. Their method immediately yields an algorithm for computing minimum cuts using only O (n) maximum flow computations.
Reference: [28] <author> D. Gusfield. </author> <title> Very simple methods for all pairs network flow analysis. </title> <journal> SIAM J. Comput., </journal> <volume> 19:143155, </volume> <year> 1990. </year>
Reference-contexts: We conclude that omitting PR tests from an implementation of a minimum cut algorithm would be a serious mistake. It is said that implementations using graph contraction are usually difficult to code (see e.g. <ref> [28] </ref>) and may be inefficient, but the gains of the Padberg-Rinaldi heuristics easily make contraction worth implementing. There are several possible directions for future work. On the implementation side, there are a few possibilities that should be explored. First, further experiments on using PR tests in ho should be done.
Reference: [29] <author> J. Hao. </author> <title> A Faster Algorithm for Finding the Minimum Cut of a Graph. </title> <type> Unpublished manuscript, </type> <year> 1991. </year>
Reference-contexts: Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows. Their algorithm (which we will call NI) runs in O (n (m + n log n)) time. In 1992, Hao and Orlin <ref> [29, 30] </ref> rejuvenated the flow approach by showing that clever modification of the Gomory-Hu algorithm implemented with a push-relabel maximum flow algorithm runs in time asymptotically equal to the time to compute one maximum flow: O (nm log (n 2 =m)). (We refer to this algorithm as HO.) Progress continued in
Reference: [30] <author> J. Hao and J. B. Orlin. </author> <title> A Faster Algorithm for Finding the Minimum Cut of a Graph. </title> <booktitle> In Proc. 3rd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 165174, </pages> <year> 1992. </year>
Reference-contexts: Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows. Their algorithm (which we will call NI) runs in O (n (m + n log n)) time. In 1992, Hao and Orlin <ref> [29, 30] </ref> rejuvenated the flow approach by showing that clever modification of the Gomory-Hu algorithm implemented with a push-relabel maximum flow algorithm runs in time asymptotically equal to the time to compute one maximum flow: O (nm log (n 2 =m)). (We refer to this algorithm as HO.) Progress continued in
Reference: [31] <author> J. Hao and J. B. Orlin. </author> <title> A Faster Algorithm for Finding the Minimum Cut in a Directed Graph. </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <address> 17:424446, </address> <year> 1994. </year>
Reference-contexts: This method allows us to amortize the work of the (n - 1) s-t cut computations to obtain a worst-case time bound that is asymptotically the same as the bound for one maximum flow computation. We give a brief description of this algorithm below. See <ref> [31] </ref> for details. Note that the algorithm given by Hao and Orlin applies to directed graphs, as did the original Gomory-Hu algorithm. As with GH, we ignore those details in this discussion. A key concept of the Hao-Orlin algorithm is that of a sleeping layer of vertices. <p> We have to be careful not to disturb the flow information when we do contractions as a result of the PR tests. Fortunately, it is easy to show (using <ref> [31] </ref>) that it is safe to contract an edge incident to the source, as long as we saturate any new outgoing capacity from source that this operation creates. So we can safely use the source PR strategy.
Reference: [32] <author> M. R. Henzinger and D. P. Williamson. </author> <title> On the Number of Small Cuts in a Graph. </title> <journal> Information Processing Letters, </journal> <volume> 59:4144, </volume> <year> 1996. </year>
Reference-contexts: Unfortunately, while it is conjectured that there are only O (n b2ffc ) ff-minimum cuts, only special cases have been proved. We will use two of these pieces: Lemma 3.6.1 [34] There are at most n 2ff ff-minimum cuts. Lemma 3.6.2 <ref> [32] </ref> For ff &lt; 3=2, there are at most 9n 2 ff-minimum cuts. We assume that the program will be given a parameter f, where we are supposed to succeed with probability at least 1 - 1=f, so we will use that parameter here.
Reference: [33] <author> D. R. Karger. </author> <title> Global Min-Cuts in RNC, and Other Ramifications of a Simple Min-Cut Algorithm. </title> <booktitle> In Proc. 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1993. </year>
Reference-contexts: In order to explain this algorithm, we start by describing a simpler algorithm of Karger <ref> [33] </ref>, which shows one of the key ideas. Consider our capacitated graph as an uncapacitated graph with multiple edges to represent capacitated edges. Pick an edge at random. Clearly the probability it is in the minimum cut is only =m.
Reference: [34] <author> D. R. Karger. </author> <title> Random sampling in cut, flow, and network design problems. </title> <booktitle> In Proc. 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 648657, </pages> <year> 1994. </year> <note> Submitted to Math. </note> <institution> of Oper. Res. </institution>
Reference-contexts: Estimating the Minimum Cut In order to perform the sampling step correctly, the algorithm needs to estimate the value of the minimum cut. In <ref> [34] </ref>, Karger gives two ways to resolve this problem. The first is to run Matula's linear time (2 + *)-approximation algorithm to get a good estimate (see Section 2.2.2). <p> There is no proof that it will be correct with the desired probability in all cases. This modification is in contrast to the other algorithms, where our heuristic changes did not affect correctness. For reference, we now give a reworking of Karger's analysis <ref> [34] </ref> that gives the best constants we know how to get. Some readers will want to skip this section. Recall that we sample each edge independently with probability p, and we need to bound the probability that any non-minimum cut samples to less than (1 - *)p edges. <p> We refer to a cut as ff-minimum if is has value at most ff. Unfortunately, while it is conjectured that there are only O (n b2ffc ) ff-minimum cuts, only special cases have been proved. We will use two of these pieces: Lemma 3.6.1 <ref> [34] </ref> There are at most n 2ff ff-minimum cuts. Lemma 3.6.2 [32] For ff &lt; 3=2, there are at most 9n 2 ff-minimum cuts. <p> There are tricks that can be played to reduce the running time of Gabow's algorithm, such as the divide and conquer variant proposed by Karger <ref> [34] </ref>, but it seems that for Gabow's algorithm to be practical, we need to either find a way to implicitly represent the trees, or we need to tighten the analysis of Karger's algorithm so that we do not need to pack so many. <p> With a theoretically justifiable sampling probability, the problem we have is that Gabow's algorithm needs to explicitly represent the trees, and there are too many. Perhaps some kind of scaling approach could work around this problem. Karger's divide and conquer approach <ref> [34] </ref>, which improves the asymptotic running time to O ( p m log n), might also improve performance.
Reference: [35] <author> D. R. Karger. </author> <title> Random Sampling in Graph Optimization Problems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA 94305, </address> <year> 1994. </year>
Reference-contexts: We now argue the running time, assuming that the total edge capacity is bounded by a polynomial in n; the results can be extended to arbitrary capacities <ref> [35] </ref>. Since the original graph had dn=2 edges, and the certificate has only dn=(2 + *), an (*) fraction of the edges must be eliminated at each step.
Reference: [36] <author> D. R. Karger. </author> <title> Using randomized sparsification to approximate minimum cut. </title> <booktitle> In Proc. 5th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 424432, </pages> <year> 1994. </year>
Reference-contexts: We now describe how to accomplish each step. Finding a Sparse Subgraph The subgraph can be found by taking a random sample of the edges. The following theorem of Karger captures the key property of a random sample: Theorem 2.3.4 <ref> [36] </ref> Consider edges with capacity greater than one to be multiple edges with capacity one.
Reference: [37] <author> D. R. Karger. </author> <title> A randomized fully polynomial approximation scheme for the all terminal network reliability problem. </title> <booktitle> In Proc. 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 1117, </pages> <year> 1995. </year>
Reference-contexts: This concept is more natural in pictures than in words; see Figure 1.1. Computation of minimum cuts is useful in various applications. An easy example is network reliability theory <ref> [37, 53] </ref>. If edges of a network fail with some probability, it makes intuitive sense that the greatest danger of network disconnection is at a minimum cut. Minimum cuts also arise in information retrieval [6], compilers for parallel languages [7], and cutting-plane algorithms for the Traveling Salesman Problem (TSP) [3]. <p> We begin by assuming that the upper bound ^ has been set to . At the end of this section, we justify our assumption. Our analysis is based on a network reliability analysis from <ref> [37] </ref>. That paper considers a graph in which each edge fails with probability p, and determines the probability that the graph remains connected. This problem is related to our objective as follows. <p> We ask whether deleting the uncontracted edges leaves us with a single component. In other words: we consider deleting every edge of G with probability 2 -d= , and ask whether the remaining (contracted) edges connect G. The following is proven in [45] (see also <ref> [37] </ref>), using the fact that among all graph with minimum cut , the graph most likely to become disconnected under random edge failures is a cycle: 28 CHAPTER 2. BACKGROUND Lemma 2.2.6 Let G have n edges and minimum cut .
Reference: [38] <author> D. R. Karger. </author> <title> Minimum Cuts in Near-Linear Time. </title> <booktitle> In Proc. 28th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 5663, </pages> <year> 1996. </year>
Reference-contexts: With probability at least 1 - 1=n, it finds all minimum cuts in O (n 2 log 3 n) time. Finally, in 1996, Karger <ref> [38] </ref> gave two closely related algorithms (K).
Reference: [39] <author> D. R. Karger and C. Stein. </author> <title> An ~ O(n 2 ) Algorithm for Minimum Cuts. </title> <booktitle> In Proc. 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 757765, </pages> <year> 1993. </year>
Reference-contexts: the Gomory-Hu algorithm implemented with a push-relabel maximum flow algorithm runs in time asymptotically equal to the time to compute one maximum flow: O (nm log (n 2 =m)). (We refer to this algorithm as HO.) Progress continued in 1993 with a randomized algorithm (KS) given by Karger and Stein <ref> [39, 40] </ref> . With probability at least 1 - 1=n, it finds all minimum cuts in O (n 2 log 3 n) time. Finally, in 1996, Karger [38] gave two closely related algorithms (K). <p> Unfortunately, the above algorithm does not run very fast. One iteration of a sequence of n - 1 contractions can take O (n 2 ) time; O (n 2 log n) iterations can take O (n 4 log n) time. However, Karger and Stein <ref> [39] </ref> point out that the highest probability of failure is when the graph is small. 24 CHAPTER 2. BACKGROUND In fact, if we contract down to n= p 2 nodes, Equation 2.6 says that the probability the minimum cut survives is at least one half. <p> Theorem 2.2.4 <ref> [39] </ref> The recursive contraction algorithm runs in O (n 2 log n) time and finds the minimum cut with probability (1= log n).
Reference: [40] <author> D. R. Karger and C. Stein. </author> <title> A new approach to the minimum cut problem. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 43(4):601640, </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: the Gomory-Hu algorithm implemented with a push-relabel maximum flow algorithm runs in time asymptotically equal to the time to compute one maximum flow: O (nm log (n 2 =m)). (We refer to this algorithm as HO.) Progress continued in 1993 with a randomized algorithm (KS) given by Karger and Stein <ref> [39, 40] </ref> . With probability at least 1 - 1=n, it finds all minimum cuts in O (n 2 log 3 n) time. Finally, in 1996, Karger [38] gave two closely related algorithms (K).
Reference: [41] <author> V. King, S. Rao, and R. Tarjan. </author> <title> A Faster Deterministic Maximum Flow Algorithm. </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <address> 17:447474, </address> <year> 1994. </year>
Reference-contexts: Subsequently there was much progress in computing maximum flows, but no one has yet been able to prove a time bound better than O (nm) for any of the best algorithms <ref> [1, 9, 10, 25, 41] </ref>. Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki [47] showed how to find a minimum cut without using maximum flows.
Reference: [42] <author> D. E. Knuth. </author> <title> Seminumerical Algorithms, </title> <booktitle> volume 2 of The Art of Computer Programming. </booktitle> <publisher> Addison Wesley, </publisher> <address> 2nd edition, </address> <year> 1981. </year>
Reference-contexts: Picking a number according to the Poisson distribution can be done such that the number of random numbers we need is the same as the value we output (see <ref> [42] </ref>); 3.6. KARGER'S ALGORITHM 55 since the expected value of a sampled edge is always at most O (log n), this method allows us to sample using O (m log n) random numbers, regardless of the magnitude of the capacities.
Reference: [43] <author> E. L. Lawler, J. K. Lenstra, A. H. G. R. Kan, and D. B. Shmoys. </author> <title> The Traveling Salesman Problem. </title> <publisher> Wiley & Sons, </publisher> <year> 1985. </year> <note> 126 BIBLIOGRAPHY </note>
Reference-contexts: The problem of identifying a subtour elimination constraint can be rephrased as the problem of finding a minimum cut in a graph with real-valued edge weights. Thus, cutting plane algorithms for the traveling salesman problem must solve a large number of minimum cut problems (see <ref> [43] </ref> for a survey of the area). We obtained some of the minimum cut instances that were solved by Applegate and Cook [3] in their TSP solver. These are clearly desirable test data, as they are from a real-world application. The Padberg-Rinaldi heuristics are very effective on the TSP instances.
Reference: [44] <author> T. Leong, P. Shor, and C. Stein. </author> <title> Implementation of a Combinatorial Multicommodity Flow Algorithm. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: </title> <booktitle> First DIMACS Implementation Challenge, </booktitle> <pages> pages 387406. </pages> <publisher> AMS, </publisher> <year> 1993. </year>
Reference-contexts: PST Algorithm We have done preliminary experiments with PST, but they are inconclusive. Previous implementation work using PST to find multicommodity flows <ref> [44] </ref> found that heuristic changes to the algorithm were crucial to good performance. We do not feel that we have worked enough with PST yet to include results on it in this study. It would definitely be interesting to know how it performs.
Reference: [45] <author> M. V. Lomonosov and V. P. Poleskii. </author> <title> Lower bound of network reliability. Problems of Information Transmission, </title> <address> 7:118123, </address> <year> 1971. </year>
Reference-contexts: We ask whether deleting the uncontracted edges leaves us with a single component. In other words: we consider deleting every edge of G with probability 2 -d= , and ask whether the remaining (contracted) edges connect G. The following is proven in <ref> [45] </ref> (see also [37]), using the fact that among all graph with minimum cut , the graph most likely to become disconnected under random edge failures is a cycle: 28 CHAPTER 2. BACKGROUND Lemma 2.2.6 Let G have n edges and minimum cut . <p> Lemma 2.2.8 Conditioned on the fact that a minimum cut has failed, the cycle is the most likely graph to partition into more than two pieces under random edge failures. Proof. A straightforward modification of <ref> [45] </ref>. Corollary 2.2.9 Conditioned on the fact that a minimum cut has failed, the probability a graph partitions into 3 or more pieces is at most np =2 . The following lemma is an immediate corollary.
Reference: [46] <author> D. W. Matula. </author> <title> A Linear Time 2 + * Approximation Algorithm for Edge Connectivity. </title> <booktitle> In Proc. 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 500504, </pages> <year> 1993. </year>
Reference-contexts: However, if we are willing to settle for an answer that is within a constant factor of the minimum cut, we can guarantee that the number of edges is reduced by a constant factor with each search. This result is due to Matula <ref> [46] </ref>. The algorithm is as follows: 2.2.
Reference: [47] <author> H. Nagamochi and T. Ibaraki. </author> <title> Computing Edge-Connectivity in Multigraphs and Capaci-tated Graphs. </title> <journal> SIAM J. Disc. Meth., </journal> <volume> 5:5466, </volume> <year> 1992. </year>
Reference-contexts: Hence we cannot give a bound better than O (n 2 m) for the Gomory-Hu algorithm. Gomory-Hu stood as the best algorithm for the problem until 1989, when Nagamochi and Ibaraki <ref> [47] </ref> showed how to find a minimum cut without using maximum flows. Their algorithm (which we will call NI) runs in O (n (m + n log n)) time. <p> This method requires n - 2 calls to the subroutine. So the question is whether there is a fast such subroutine. Nagamochi and Ibaraki <ref> [47] </ref> show that there is a surprisingly simple one. For the purposes of intuition, consider uncapacitated multigraphs. Suppose we have a graph 2.2. CONTRACTION 21 with minimum cut value one. Then the graph is connected, so we can find a spanning tree.
Reference: [48] <author> H. Nagamochi, T. Ono, and T. Ibaraki. </author> <title> Implementing an Efficient Minimum Capacity Cut Algorithm. </title> <journal> Math. Prog., </journal> <volume> 67:297324, </volume> <year> 1994. </year>
Reference-contexts: The recent burst of theoretical progress has outdated implementation experiments. In 1990, Padberg and Rinaldi published a study on practical implementation of Gomory-Hu [51], which is very valuable for the design of heuristics, but unfortunately came just before the theory breakthroughs. Nagamochi et al <ref> [48] </ref> confirm that NI often beats Gomory-Hu in practice. Nothing is known about the practical performance of the other new algorithms. 1.2 Our Contribution In this paper we address the question of the practical performance of minimum cut algorithms. We consider all of the contenders: NI, HO, KS, and K. <p> This strategy also extends to NI: apply the PR tests near the edge contracted after the last search. Both Padberg and Rinaldi [51] and Nagamochi et al <ref> [48] </ref> make use of this approach. Note that it could be similarly extended to KS as it was originally described, where we contract one edge at a time, but it does not make sense for our variant, where we contract many edges at once. <p> This reordering can be helpful because it may cause PR tests and/or excess detection tests to pass earlier than they would have. 3.4 Nagamochi-Ibaraki Algorithm NI required relatively little modification from its theory description. We just incorporated two heuristics given by Nagamochi et al. <ref> [48] </ref>, made some careful data structure choices, and incorporated PR tests. We differ from [48] on the latter two points. Nagamochi-Ono-Ibaraki Heuristics Nagamochi et al. [48] give a heuristic modification to NI that often helps to update the upper bound on the minimum cut. <p> We just incorporated two heuristics given by Nagamochi et al. <ref> [48] </ref>, made some careful data structure choices, and incorporated PR tests. We differ from [48] on the latter two points. Nagamochi-Ono-Ibaraki Heuristics Nagamochi et al. [48] give a heuristic modification to NI that often helps to update the upper bound on the minimum cut. The heuristic takes advantage of the fact that the set of visited nodes is connected, and therefore defines a cut. <p> We just incorporated two heuristics given by Nagamochi et al. <ref> [48] </ref>, made some careful data structure choices, and incorporated PR tests. We differ from [48] on the latter two points. Nagamochi-Ono-Ibaraki Heuristics Nagamochi et al. [48] give a heuristic modification to NI that often helps to update the upper bound on the minimum cut. The heuristic takes advantage of the fact that the set of visited nodes is connected, and therefore defines a cut. <p> In the end we chose to use a 4-ary heap. Note that this makes the theoretical worst case time bound on our implementation O (mn log n). PR Heuristics Nagamochi et al <ref> [48] </ref> incorporate the PR tests by applying a source test at the node created by the last contraction of a search. <p> Random Graphs with Heavy Components A natural type of graph on which to run a minimum cut algorithm is the type that is always drawn to exhibit the problem: two well connected components connected by low capacity edges. Nagamochi et al. <ref> [48] </ref> used a family of graphs that generalizes this idea. <p> Every edge added gets a random capacity. If the endpoints have different colors, the capacity is chosen uniformly at random from [1; 100]; otherwise the capacity is chosen uniformly at random from [1; 100P]. Following <ref> [48] </ref>, we tested on 6 subfamilies. Our families are the same in spirit as those of [48], but we use larger problem sizes and we added some data points where we felt it was appropriate. <p> If the endpoints have different colors, the capacity is chosen uniformly at random from [1; 100]; otherwise the capacity is chosen uniformly at random from [1; 100P]. Following <ref> [48] </ref>, we tested on 6 subfamilies. Our families are the same in spirit as those of [48], but we use larger problem sizes and we added some data points where we felt it was appropriate.
Reference: [49] <author> C. S. J. A. Nash-Williams. </author> <title> Edge disjoint spanning trees of finite graphs. </title> <journal> Journal of the London Mathematical Society, </journal> <volume> 36:445450, </volume> <year> 1961. </year>
Reference-contexts: If we try to consider undirected spanning trees, we get a theorem that is close, but has some slack in it. In particular, Nash-Williams shows: Theorem 2.3.3 <ref> [49] </ref> An undirected graph with minimum cut contains at least b=2c edge-disjoint spanning trees. Note that NI packs spanning trees (and forests), but it does so with different intent, as it does not attempt to find a maximum packing, but rather a maximal one.
Reference: [50] <author> Q. C. Nguyen and V. Venkateswaran. </author> <title> Implementations of Goldberg-Tarjan Maximum Flow Algorithm. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: First DIMACS Implementation Challenge, </title> <booktitle> pages 1942. AMS, </booktitle> <year> 1993. </year>
Reference-contexts: Meanwhile, we use the tests to find weaknesses in the implementation and devise heuristics to improve performance. Thus it is difficult to be sure when we are done. For HO we take advantage of implementation work on maximum flow algorithms <ref> [2, 13, 16, 17, 50] </ref>; for NI we take advantage of the work of Nagamochi et al. KS and K were both developed from scratch, so it remains possible that their inferior performance is due to the fact that we are the first to develop heuristics for them. <p> We did not implement this version, and the full description is rather involved, so we do not give it here. We assume that a relabel operation always uses the gap relabeling heuristic [12, 16]. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 13, 16, 50] </ref> and is essential for the analysis of the Hao-Orlin algorithm.
Reference: [51] <author> M. Padberg and G. Rinaldi. </author> <title> An Efficient Algorithm for the Minimum Capacity Cut Problem. </title> <journal> Math. Prog., </journal> <volume> 47:1936, </volume> <year> 1990. </year>
Reference-contexts: The recent burst of theoretical progress has outdated implementation experiments. In 1990, Padberg and Rinaldi published a study on practical implementation of Gomory-Hu <ref> [51] </ref>, which is very valuable for the design of heuristics, but unfortunately came just before the theory breakthroughs. Nagamochi et al [48] confirm that NI often beats Gomory-Hu in practice. <p> Finally we discuss an algorithm of Karger and Stein, which shows that guessing contractible edges is good enough for a high probability of success. 2.2.1 The Padberg-Rinaldi Heuristics In their implementation study of minimum cut algorithms, <ref> [51] </ref>, Padberg and Rinaldi introduced several local tests for identifying contractible edges. The point is to try to take option 1 of Gener-icContractCut whenever possible. Used in GH, every time a test finds a contractible edge, we save one maximum flow computation. <p> We refer to these as PR tests or PR heuristics. We say that a test passes if one of the conditions is satisfied, which implies that the edge is contractible. Recall for the following formulas that c (v) denotes the total capacity incident to vertex v. Lemma 2.2.3 <ref> [51] </ref> Let ^ be an upper bound on (G). <p> We will discuss this issue further in subsequent sections. 3.2 The Padberg-Rinaldi Heuristics It is not clear how to get the most benefit from the PR tests. One natural strategy, which is used by Padberg and Rinaldi <ref> [51] </ref>, is to keep applying them until no edges pass any of the tests. We 46 CHAPTER 3. IMPLEMENTATION refer to this approach as the exhaustive strategy. The problem is that this strategy takes too long. <p> This strategy also extends to NI: apply the PR tests near the edge contracted after the last search. Both Padberg and Rinaldi <ref> [51] </ref> and Nagamochi et al [48] make use of this approach. Note that it could be similarly extended to KS as it was originally described, where we contract one edge at a time, but it does not make sense for our variant, where we contract many edges at once. <p> Note there are also (n 2 ) cuts of value 2006. This modification is cumbersome to describe in words, but the picture is clear. See Figure 4.2. Family n DBLCYC 1024, 2048, 4096, 8192, 16384, 32768 PR One final problem family is one used by Padberg and Rinaldi <ref> [51] </ref>. Our only use of this family is to check the effectiveness of our PR strategies against those of Padberg and Rinaldi. This family includes two different types of graphs. The first type is a random graph with an expected density d. <p> Family n d c PR1 100,200,300,400 2 1 PR3 100,200,300,400 50 1 PR5 100,200,300,400,500,1000,1500,2000 2 2 PR7 100,200,300,400 50 2 As we wish to compare directly to Padberg and Rinaldi, these values are precisely those used by Padberg and Rinaldi in their paper <ref> [51] </ref>. 4.1.3 Codes As discussed in the previous chapter, for each algorithm we made numerous decisions about the implementation. While the process of implementation involved testing many of these decisions, there are far too many for us to attempt to present data on everything we tried.
Reference: [52] <author> S. A. Plotkin, D. Shmoys, and E. Tardos. </author> <title> Fast Approximation Algorithms for Fractional Packing and Covering. </title> <booktitle> In Proc. 32nd IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 495504, </pages> <year> 1991. </year>
Reference-contexts: One is to make our undirected graph directed and use Gabow's algorithm, which is de scribed above. Another possibility is to approximate a packing of undirected spanning trees with the fractional packing algorithm of Plotkin, Shmoys, and Tardos <ref> [52] </ref> (PST). This approach hast the theoretical advantage that in some cases we may find more than =2 trees, and in this case there will be a tree that uses only one minimum cut edge. We did not implement this method, so we do not describe it further.
Reference: [53] <author> A. Ramanathan and C. Colbourn. </author> <title> Counting Almost Minimum Cutsets with Reliability Applications. </title> <journal> Math. Prog., </journal> <volume> 39:253261, </volume> <year> 1987. </year>
Reference-contexts: This concept is more natural in pictures than in words; see Figure 1.1. Computation of minimum cuts is useful in various applications. An easy example is network reliability theory <ref> [37, 53] </ref>. If edges of a network fail with some probability, it makes intuitive sense that the greatest danger of network disconnection is at a minimum cut. Minimum cuts also arise in information retrieval [6], compilers for parallel languages [7], and cutting-plane algorithms for the Traveling Salesman Problem (TSP) [3].
Reference: [54] <author> D. D. Sleator and R. E. Tarjan. </author> <title> A Data Structure for Dynamic Trees. </title> <journal> J. Comput. System Sci., </journal> <volume> 26:362391, </volume> <year> 1983. </year>
Reference-contexts: We need only check vertices that fit one of the two conditions. It is not immediately clear that we will gain anything, because v (or a neighbor of a descendant of v) may have O (n) ancestors. So to be efficient, we use dynamic trees <ref> [54, 55] </ref>, which among other things, support the following two operations on a tree with values at the nodes (val (w) at node w): AddPath (v; x) add x to the value of every node on the path from v to the root.
Reference: [55] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 32(3):652686, </volume> <year> 1985. </year>
Reference-contexts: We need only check vertices that fit one of the two conditions. It is not immediately clear that we will gain anything, because v (or a neighbor of a descendant of v) may have O (n) ancestors. So to be efficient, we use dynamic trees <ref> [54, 55] </ref>, which among other things, support the following two operations on a tree with values at the nodes (val (w) at node w): AddPath (v; x) add x to the value of every node on the path from v to the root.
Reference: [56] <author> R. E. Tarjan. </author> <title> Applications of Path Compression on Balanced Trees. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 26(4):690715, </volume> <year> 1979. </year>
Reference-contexts: It is not clear that this change makes any difference. We do not bother to use a linear time algorithm for computing least common ancestors; rather we use the path compression algorithm of Tarjan <ref> [56] </ref>, which is wonderfully simple and runs in O (mff (m; n)) time, where ff (m; n) is a functional inverse of Ackermann's function.
References-found: 56

