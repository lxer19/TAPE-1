URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-445.ps.Z
Refering-URL: http://www.media.mit.edu/~szummer/
Root-URL: http://www.media.mit.edu
Email: szummer@media.mit.edu, picard@media.mit.edu  
Title: Image Classification  
Author: Martin Szummer and Rosalind W. Picard 
Web: http://www-white.media.mit.edu/~szummer/  
Address: Rm E15-384; 20 Ames St; Cambridge MA 02139; USA  
Affiliation: MIT Media Lab  
Note: Indoor-Outdoor  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 445 Appeared: IEEE Intl Workshop on Content-based Access of Image and Video Databases, Jan 1998 Abstract We show how high-level scene properties can be inferred from classification of low-level image features, specifically for the indoor-outdoor scene retrieval problem. We systematically studied the features: (1) histograms in the Ohta color space (2) multiresolution, simultaneous autoregressive model parameters (3) coefficients of a shift-invariant DCT. We demonstrate that performance is improved by computing features on sub-blocks, classifying these subblocks, and then combining these results in a way reminiscent of "stacking." State of the art single-feature methods are shown to result in about 75-86% performance, while the new method results in 90.3% correct classification, when evaluated on a diverse database of over 1300 consumer images provided by Kodak. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Myron Flickner, Harpreet Sawhney, et al. </editor> <title> Query by image and video content: The QBIC system. </title> <journal> IEEE Computer, </journal> <volume> 28(9) </volume> <pages> 23-32, </pages> <month> Sept </month> <year> 1995. </year>
Reference-contexts: Another important application is image retrieval. Let's say we would like to find a beach scene. A helpful step would be to limit the search to outdoor scenes. Unfortunately, this is not possible even in state-of-the-art image retrieval systems such as QBIC <ref> [1] </ref>, Virage [2] and VisualSEEk [3]. These systems are based mainly on color histograms and primitive texture measures. The user builds a query by selecting colors from a palette, a texture from a chart, and then indicates how to weight the color versus the texture.
Reference: [2] <author> Amarnath Gupta and Ramesh Jain. </author> <title> Visual information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 40(5), </volume> <year> 1997. </year> <note> http://www.virage.com/research. htm/vir_cacm.pdf. </note>
Reference-contexts: Another important application is image retrieval. Let's say we would like to find a beach scene. A helpful step would be to limit the search to outdoor scenes. Unfortunately, this is not possible even in state-of-the-art image retrieval systems such as QBIC [1], Virage <ref> [2] </ref> and VisualSEEk [3]. These systems are based mainly on color histograms and primitive texture measures. The user builds a query by selecting colors from a palette, a texture from a chart, and then indicates how to weight the color versus the texture.
Reference: [3] <author> J. R. Smith and S.F. Chang. Visualseek: </author> <title> a fully automated content-based image query system. </title> <booktitle> In ACM Multimedia, </booktitle> <pages> pages 87-98, </pages> <month> Nov </month> <year> 1996. </year>
Reference-contexts: Another important application is image retrieval. Let's say we would like to find a beach scene. A helpful step would be to limit the search to outdoor scenes. Unfortunately, this is not possible even in state-of-the-art image retrieval systems such as QBIC [1], Virage [2] and VisualSEEk <ref> [3] </ref>. These systems are based mainly on color histograms and primitive texture measures. The user builds a query by selecting colors from a palette, a texture from a chart, and then indicates how to weight the color versus the texture.
Reference: [4] <author> T. P. Minka and R. W. </author> <title> Picard. Interactive learning using a `society of models'. </title> <booktitle> In Proceedings of CVPR, </booktitle> <pages> pages 447-452, </pages> <address> San Francisco, CA, June 1996. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Query by image example enables the user to select one image and find other similar images, making it easier to specify the relevant color and texture query. Most systems still require the user to select weights for the different features. An exception to this is FourEyes <ref> [4] </ref>, which can learn the relevant feature combination based on several positive and negative examples. In an initial quick attempt to teach FourEyes to solve the indoor-outdoor classification problem using whole images with no specific subblock guidance, we did not meet with significant success.
Reference: [5] <author> Monika Gorkani and Rosalind W. </author> <title> Picard. Texture orientation for sorting photos at a glance. </title> <booktitle> In Proc. Int. Conf. Pat. Rec., </booktitle> <volume> volume I, </volume> <pages> pages 459-464, </pages> <address> Jerusalem, Israel, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: This new way is successful for accurately distinguishing indoor from outdoor scenes. 2 Background Several attempts at recognizing high-level scene properties using low-level features have been made. Gorkani and Picard <ref> [5] </ref> discriminate between photos of city scenes and photos of landscape scenes. They use a multiscale steerable pyramid to find dominant orientations in 4 fi 4 subblocks of the image.
Reference: [6] <author> Elaine C. Yiu. </author> <title> Image classification using color cues and texture orientation. </title> <type> Master's thesis, </type> <institution> MIT, dept EECS, </institution> <year> 1996. </year>
Reference-contexts: They use a multiscale steerable pyramid to find dominant orientations in 4 fi 4 subblocks of the image. The image is classified as a city scene if enough subblocks have strong dominant vertical orientation, or alternatively medium-strong vertical orientation and also horizontal orientation. Yiu <ref> [6] </ref> uses the same dominant orientation features and also color information to classify indoor and outdoor scenes. She uses nearest neighbor and support vector machine classifiers. The former classifier is better at color, the latter at dominant orientation.
Reference: [7] <author> Pamela R. Lipson. </author> <title> Context and Configuration Based Scene Classification. </title> <type> PhD thesis, </type> <institution> MIT, EECS dept, </institution> <year> 1996. </year>
Reference-contexts: Furthermore, the texture features used here give significantly better results than her dominant orientation detector. The work here also takes advantage of a spatial tessellation of the image, which we found provides a significant gain in performance. Instead of building a specific scene class detector, Lipson <ref> [7] </ref> describes a general scene query approach. Scenes are described by graphs representing relations between image regions. The relationships include relative color, spatial location, and highpass frequency content. Unfortunately, the templates have to be constructed manually for each scene layout.
Reference: [8] <author> Hong-Heather Yu and Wayne Wolf. </author> <title> Scenic classification methods for image and video databases. </title> <booktitle> In Proc. SPIE, Digital Image Storage and Archiving systems, </booktitle> <pages> pages 363-371, </pages> <year> 1995. </year> <note> http://www.ee. princeton.edu/~heathery/. </note>
Reference-contexts: These templates are also quite specific, which makes them fine for limited special cases such as "sky over mountain over lake" but difficult for the case considered here of capturing a broad concept like an outdoor scene. Yu <ref> [8] </ref> learns a statistical template from examples. She computes vector quantized color histograms for sub-blocks of the image. Then she trains a one-dimensional hidden Markov model along vertical or horizontal segments of specific scene layouts, such as sky-mountain-river scenes.
Reference: [9] <author> Y-I Ohta, T. Kanade, and T. Sakai. </author> <title> Color information for region segmentation. Comp. Graph. </title> <journal> and Img. Proc., </journal> <volume> 13 </volume> <pages> 222-241, </pages> <year> 1980. </year>
Reference-contexts: These features were 2 computed both for the whole image and for each sub-block of a 4 fi 4 image tessellation. The color feature is a color histogram, and has 32 bins per channel like our baseline. However, the three channels come from the Ohta color space <ref> [9] </ref>. The color axes of this space are the 3 largest eigenvectors of the RGB space, found through principal components analysis of a large selection of natural images.
Reference: [10] <author> Michael Swain and Dana Ballard. </author> <title> Color indexing. </title> <journal> Int. J. of Comp. Vis., </journal> (1):11-32, 1991. 
Reference-contexts: The change of color spaces, from RGB to Ohta, raises the performance of color histogram based recognition to 73.2%. Moreover, instead of using the Euclidean norm for measuring distances between histograms, we use the histogram intersection norm <ref> [10] </ref>.
Reference: [11] <author> Jianchang Mao and Anil K. Jain. </author> <title> Texture classification and segmentation using multiresolution simultaneous autoregressive models. </title> <journal> Pattern Recognition, </journal> <volume> 25(2) </volume> <pages> 173-188, </pages> <year> 1992. </year>
Reference-contexts: In the rest of the paper we exclusively apply the Ohta color space with histogram intersection. The texture features are computed using the multiresolution, simultaneous autoregressive model (MSAR) <ref> [11] </ref>. These are among the best texture features bench-marked on the Brodatz album [12]. The model constructs the best linear predictor of a pixel based on a noncausal neighborhood. The features are the weights of the predictor.
Reference: [12] <author> Rosalind W. Picard, Tanweer Kabir, and Fang Liu. </author> <title> Real-time recognition with the entire Brodatz texture database. </title> <booktitle> In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 638-639, </pages> <address> New York, </address> <month> June </month> <year> 1993. </year> <title> MIT Media Lab Perceptual Computing TR 215. </title>
Reference-contexts: In the rest of the paper we exclusively apply the Ohta color space with histogram intersection. The texture features are computed using the multiresolution, simultaneous autoregressive model (MSAR) [11]. These are among the best texture features bench-marked on the Brodatz album <ref> [12] </ref>. The model constructs the best linear predictor of a pixel based on a noncausal neighborhood. The features are the weights of the predictor. Three different neighborhoods at scales 2, 3, and 4 are used, and the weights are concatenated to yield a 15-dimensional vector, as in [12]. <p> the Brodatz album <ref> [12] </ref>. The model constructs the best linear predictor of a pixel based on a noncausal neighborhood. The features are the weights of the predictor. Three different neighborhoods at scales 2, 3, and 4 are used, and the weights are concatenated to yield a 15-dimensional vector, as in [12]. The Maha-lanobis norm is used to measure feature vector distance (covariances are estimated from several subwindow estimates). We extracted these features from gray scale images at two resolutions (half and quarter), using a suitable antialiasing filter.
Reference: [13] <author> Leo Breiman. </author> <title> Stacked regression. </title> <note> ftp: //ftp.stat.berkeley.edu/pub/users/breiman/ stacked.abstract, </note> <year> 1994. </year>
Reference-contexts: It is difficult to estimate covariances for such a large vector, and we encounter general curse-of-dimensionality problems. Instead, we chose to pursue a multi-stage classification approach, classifying the subblocks independently and then performing another classification on these answers (Figure 1). This is reminiscent of stacking <ref> [13] </ref> except that the subblock classifiers here were trained on their own data. Not surprisingly, the individual subblock classifiers are less accurate than a whole image classifier.

References-found: 13

