URL: ftp://flop.informatik.tu-muenchen.de/pub/fki/fki-207-95rev.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00228.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: hochreit@informatik.tu-muenchen.de  juergen@idsia.ch  
Title: LONG SHORT-TERM MEMORY  
Author: Jurgen Schmidhuber 
Date: Revised December 1996  
Web: http://www7.informatik.tu-muenchen.de/~hochreit  http://www.idsia.ch/~juergen  
Address: 80290 Munchen, Germany  Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: Sepp Hochreiter Fakultat fur Informatik Technische Universitat Munchen  IDSIA  
Pubnum: Technical Report FKI-207-95, Version 3.0  
Abstract: Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insufficient, decaying error back flow. We briefly review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, efficient method called "Long Short-Term Memory" (LSTM). LSTM can learn to bridge time lags in excess of 1000 steps by enforcing constant error flow through "constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM's update complexity per time step is O(W ), where W is the number of weights. In experimental comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex long time lag tasks that have never been solved by previous recurrent network algorithms. It works with local, distributed, real-valued, and noisy pattern representations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1994). </year> <title> Credit assignment through time: Alternatives to backpropagation. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 75-82. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite-state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages> <editor> de Vries, B. and Principe, J. C. </editor> <year> (1991). </year> <title> A theory for neural networks with time delays. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 162-168. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To correctly predict the symbol before last, the net has to remember the second symbol. Comparison. We compare LSTM to "Elman nets trained by Elman's training procedure" (ELM) <ref> (results taken from Cleeremans et al. 1989) </ref>, Fahlman's "Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed). <p> 4 blocks, size 1 264 0.5 97 9,500 LSTM 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" <ref> (results taken from Cleeremans et al. 1989) </ref>, "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM). Weight numbers in the first 4 rows are estimates | the corresponding papers don't provide all the technical details.
Reference: <author> Doya, K. and Yoshizawa, S. </author> <year> (1989). </year> <title> Adaptive neural oscillator using continuous-time backpropagation learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 375-385. </pages>
Reference: <author> Elman, J. L. </author> <year> (1988). </year> <title> Finding structure in time. </title> <type> Technical Report CRL 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference: <author> Fahlman, S. E. </author> <year> (1991). </year> <title> The recurrent cascade-correlation learning algorithm. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our LSTM architectures are selected quite arbitrarily. If nothing is known about the complexity of the given problem, a more systematic approach would be: start with a very small net consisting of one memory cell. If this does not work, try two cells, etc. Alternatively, use sequential network construction <ref> (e.g., Fahlman 1991) </ref>. Outline of experiments. * Experiment 1 focuses on a standard benchmark test for recurrent nets: the embedded Reber grammar. Since it allows for training sequences with short time lags, it is not a long time lag problem. <p> To correctly predict the symbol before last, the net has to remember the second symbol. Comparison. We compare LSTM to "Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al. 1989), Fahlman's "Recurrent Cascade-Correlation" (RCC) <ref> (results taken from Fahlman 1991) </ref>, and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed). It should be mentioned that Smith and Zipser actually make the task easier by increasing the probability of short time lag exemplars. <p> 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" <ref> (results taken from Fahlman 1991) </ref> and our new approach (LSTM). Weight numbers in the first 4 rows are estimates | the corresponding papers don't provide all the technical details. Only LSTM almost always learns to solve the task (only two failures out of 150 trials).
Reference: <author> Hochreiter, J. </author> <year> (1991). </year> <title> Untersuchungen zu dynamischen neuronalen Netzen. </title> <type> Diploma thesis, </type> <institution> Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution> <note> See www7.informatik.tu-muenchen.de/~hochreit. </note>
Reference-contexts: The most difficult task (Task 2c) involves hundreds of distractor symbols at random positions, and minimal time lags of 1000 steps. LSTM solves it, while BPTT and RTRL already fail in case of 10-step minimal time lags <ref> (see also, e.g., Hochreiter 1991 and Mozer 1992) </ref>.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Long short-term memory. </title> <type> Technical Report FKI-207-95, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: Application areas will include (1) time series prediction, (2) music composition, and (3) speech processing. 23 8 ACKNOWLEDGMENTS Thanks to Mike Mozer, Wilfried Brauer, Nic Schraudolph, and several anonymous referees for valuable comments and suggestions that helped to improve a previous version of this paper <ref> (Hochreiter and Schmidhuber 1995) </ref>. This work was supported by DFG grant SCHM 942/3-1 from "Deutsche Forschungsgemeinschaft".
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1996). </year> <title> Bridging long time lags by weight guessing and "Long Short-Term Memory". </title> <editor> In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, </editor> <booktitle> Spa-tiotemporal models in biological and artificial systems, </booktitle> <pages> pages 65-72. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, Netherlands. Serie: </address> <booktitle> Frontiers in Artificial Intelligence and Applications, </booktitle> <volume> Volume 37. 30 Hochreiter, </volume> <editor> S. and Schmidhuber, J. </editor> <year> (1997). </year> <title> LSTM can solve hard long time lag problems. </title> <booktitle> In Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address> <note> Presented at NIPS 96. </note>
Reference-contexts: See, e.g., Hochreiter (1991) and Mozer (1992). A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing. Guessing can outperform many long time lag algorithms. Recently we discovered <ref> (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) </ref> that many long time lag tasks used by previous authors can be solved much faster by simple random weight guessing than by the proposed algorithms. <p> See also Miller and Giles (1993) for additional work on multiplicative inputs. As we recently discovered, however, simple weight guessing solves some of Miller and Giles' problems more quickly than the algorithms they investigate <ref> (Schmidhuber and Hochreiter, 1996) </ref>. 6 DISCUSSION Limitations of LSTM. * The particularly efficient truncated backprop version of the LSTM algorithm won't easily solve problems similar to "strongly delayed XOR problems", where the goal is to compute the XOR of two widely separated inputs that previously occurred somewhere in a noisy sequence.
Reference: <author> Lang, K., Waibel, A., and Hinton, G. E. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43. </pages>
Reference-contexts: Other methods that seem practical for short time lags only are Time-Delay Neural Networks <ref> (Lang et al. 1990) </ref> and Plate's method (Plate 1993), which updates unit activations based on a weighted sum of old activations (see also de Vries and Principe 1991).
Reference: <author> Lin, T., Horne, B. G., Tino, P., and Giles, C. L. </author> <year> (1995). </year> <title> Learning long-term dependencies is not as difficult with NARX recurrent neural networks. </title> <institution> Technical Report UMIACS-TR-95-78 and CS-TR-3500, Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. </institution>
Reference: <author> Miller, C. B. and Giles, C. L. </author> <year> (1993). </year> <title> Experimental comparison of the effect of order in recurrent neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 849-872. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused back-propagation algorithm for temporal sequence recognition. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 349-381. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> Induction of multiscale temporal structure. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To deal with long time lags, Mozer (1992) uses time constants influencing the activation changes. However, for long time lags the time constants need external fine tuning <ref> (Mozer 1992) </ref>. Sun et al.'s alternative approach (1993) updates the activation of a recurrent unit by adding the old activation and the (scaled) current net input. The net input, however, tends to perturb the stored information, which again makes long-term storage impractical.
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269. </pages>
Reference: <author> Pearlmutter, B. A. </author> <year> (1995). </year> <title> Gradient calculations for dynamic recurrent neural networks: A survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(5) </volume> <pages> 1212-1228. </pages>
Reference: <author> Plate, T. A. </author> <year> (1993). </year> <title> Holographic recurrent networks. </title> <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 34-41. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other methods that seem practical for short time lags only are Time-Delay Neural Networks (Lang et al. 1990) and Plate's method <ref> (Plate 1993) </ref>, which updates unit activations based on a weighted sum of old activations (see also de Vries and Principe 1991). Lin et al. (1995) propose variants of time-delay networks called NARX networks; some of their problems can be solved quickly by simple weight guessing though.
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> Language induction by phase transition in dynamical recognizers. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 619-626. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Puskorius, G. V. and Feldkamp, L. A. </author> <year> (1994). </year> <title> Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 279-297. </pages>
Reference: <author> Ring, M. B. </author> <year> (1993). </year> <title> Learning sequential tasks by incrementally adding higher orders. </title> <editor> In S. J. Han-son, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 115-122. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1989). </year> <title> A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 403-412. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1992a). </year> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 243-248. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1992b). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference-contexts: To predict the final element, the net has to learn to store a representation of the first element for p time steps. We compare "Real-Time Recurrent Learning" for fully recurrent nets (RTRL), "Back-Propagation Through Time" (BPTT), the sometimes very successful 2-net "Neural Sequence Chunker" <ref> (CH, Schmidhuber 1992b) </ref>, and our new method (LSTM). In all cases, weights are initialized in [-0.2,0.2]. Due to limited computation time, training is stopped after 5 million sequence presentations. <p> Each layer has connections from all layers below. All units use the logistic activation function sigmoid in [0,1]. BPTT: same architecture as the one trained by RTRL. CH: both net architectures like RTRL's, but one has an additional output for predicting the hidden unit of the other one <ref> (see Schmidhuber 1992b for details) </ref>. LSTM: like with RTRL, but the hidden unit is replaced by a memory cell and an input gate (no output gate required). g is the logistic sigmoid, and h is the identity function h : h (x) = x; 8x.
Reference: <author> Schmidhuber, J. </author> <year> (1992c). </year> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 31 Schmidhuber, J. </address> <year> (1993). </year> <institution> Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Habilitationss--chrift, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: LSTM, however, always learned to solve the task. Comparing successful trials only, LSTM learned much faster. See Table 2 for details. It should be mentioned, however, that a hierarchical chunker can also always quickly solve this task <ref> (Schmidhuber 1992c, 1993) </ref>. 12 Task 2b: no local regularities. With the task above, the chunker sometimes learns to correctly predict the final element, but only because of predictable local regularities in the input stream that allow for compressing the sequence.
Reference: <author> Schmidhuber, J. and Hochreiter, S. </author> <year> (1996). </year> <title> Guessing can outperform many long time lag algorithms. </title> <type> Technical Report IDSIA-19-96, </type> <institution> IDSIA. </institution>
Reference-contexts: See, e.g., Hochreiter (1991) and Mozer (1992). A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing. Guessing can outperform many long time lag algorithms. Recently we discovered <ref> (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) </ref> that many long time lag tasks used by previous authors can be solved much faster by simple random weight guessing than by the proposed algorithms. <p> See also Miller and Giles (1993) for additional work on multiplicative inputs. As we recently discovered, however, simple weight guessing solves some of Miller and Giles' problems more quickly than the algorithms they investigate <ref> (Schmidhuber and Hochreiter, 1996) </ref>. 6 DISCUSSION Limitations of LSTM. * The particularly efficient truncated backprop version of the LSTM algorithm won't easily solve problems similar to "strongly delayed XOR problems", where the goal is to compute the XOR of two widely separated inputs that previously occurred somewhere in a noisy sequence.
Reference: <author> Silva, G. X., Amaral, J. D., Langlois, T., and Almeida, L. B. </author> <year> (1996). </year> <title> Faster training of recurrent networks. </title> <editor> In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, </editor> <booktitle> Spatiotemporal models in biological and artificial systems, </booktitle> <pages> pages 168-175. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, Netherlands. </address> <booktitle> Series: Frontiers in Artificial Intelligence and Applications, </booktitle> <volume> Volume 37. </volume>
Reference: <author> Smith, A. W. and Zipser, D. </author> <year> (1989). </year> <title> Learning sequential structures with the real-time recurrent learning algorithm. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(2) </volume> <pages> 125-131. </pages>
Reference-contexts: 0.1 100 21,730 LSTM 3 blocks, size 2 276 0.2 97 14,060 LSTM 4 blocks, size 1 264 0.5 97 9,500 LSTM 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL <ref> (results taken from Smith and Zipser 1989) </ref>, "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM).
Reference: <author> Sun, G., Chen, H., and Lee, Y. </author> <year> (1993). </year> <title> Time warping invariant neural networks. </title> <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 180-187. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watrous, R. L. and Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 406-414. </pages>
Reference: <author> Williams, R. J. </author> <year> (1989). </year> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science. </institution>
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501. </pages>
Reference-contexts: Net activations are reset after each processed input sequence. For comparisons with recurrent nets taught by gradient descent, we give results only for RTRL, except for comparison 2a, which also includes BPTT. Note, however, that untruncated BPTT <ref> (see, e.g., Williams and Peng 1990) </ref> computes exactly the same gradient as o*ine RTRL.
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1992). </year> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, Backpropagation: </editor> <booktitle> Theory, Architectures and Applications. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 32 </pages>
References-found: 33

