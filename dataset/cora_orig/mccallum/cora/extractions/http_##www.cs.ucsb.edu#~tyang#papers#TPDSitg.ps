URL: http://www.cs.ucsb.edu/~tyang/papers/TPDSitg.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Title: Heuristic Algorithms for Scheduling Iterative Task Computations on Distributed Memory Machines  
Author: Tao Yang Cong Fu 
Keyword: KEY WORDS: Scheduling, communication optimization, granularity, software pipelining, iterative task graphs, directed acyclic graphs.  
Address: Santa Barbara, CA 93106.  
Affiliation: Department of Computer Science University of California  
Abstract: Many partitioned scientific programs can be modeled as iterative execution of computational tasks and represented by iterative task graphs (ITGs). An ITG may or may not have dependence cycles. In this paper, we consider the symbolic scheduling of ITGs on distributed memory architectures with nonzero communication overhead and propose heuristic algorithms for scheduling both cyclic and acyclic ITGs without searching the entire iteration space. Our approach incorporates techniques of software pipelining, graph unfolding, directed acyclic graph (DAG) scheduling and load balancing. We analyze the asymptotic optimality of the algorithms to show that the derived schedules are competitive to the optimal solutions. We also study the sensitivity of scheduling performance on inaccurate weights. Finally we present the experimental results to demonstrate the effectiveness of the optimization techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Beguelin, J.J. Dongarra, G.A. Geist, R. Manchek, </author> <title> V.S. Sunderam, Graphical development tools for network-based concurrent supercomputing, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <year> 1991, </year> <month> pp.435-444. </month>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems [2, 17, 20, 24] and many graphical parallel programming tools use coarse-grain computation graphs to model user applications <ref> [1, 13] </ref>. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). An ITG represents a sequence of task computations where each iteration consists of the execution of a set of tasks. <p> The theoretical results help us to understand why the algorithms can deliver good performance. These algorithmic and theoretical results are verified through experiments for solving scientific problems which can be represented as ITGs. The algorithms could be useful for mapping applications in graphical parallel programming tools <ref> [1, 7, 13] </ref> and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL [17, 20, 24] and we expect that ITG algorithms will also be useful.
Reference: [2] <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <journal> in Parallel Processing Letters, </journal> <volume> Vol. 5, No. 4, </volume> <pages> pp. 527-538, </pages> <month> Dec, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems <ref> [2, 17, 20, 24] </ref> and many graphical parallel programming tools use coarse-grain computation graphs to model user applications [1, 13]. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). <p> For compiling Fortran and C, in order to make graph scheduling applicable, there is work to be done in generating coarse-grain task graphs <ref> [2] </ref>. The current algorithms still have restrictions. For example, we have not considered processor distances. It has been shown (e.g. [5]) in the modern processor architectures, the distance factor does not affect the cost of communication significantly because of the wormhole routing technology.
Reference: [3] <author> P. Chretienne, </author> <title> Cyclic scheduling with communication delays: a polynomial special case. </title> <month> Dec </month> <year> 1993. </year> <type> Tech Report, </type> <institution> LITP. </institution>
Reference-contexts: The optimal solution for a special class of ITGs has been studied in <ref> [3] </ref>. Our heuristic algorithm is designed for general ITGs. The optimal loop unfolding factor is derived so that the scheduling performance can be guaranteed compared to the optimal solution. We will show that the quality of DAG scheduling is critical to the performance of ITG scheduling.
Reference: [4] <author> V. Donaldson and J. Ferrante, </author> <title> Determining Asynchronous Pipeline Execution Times, </title> <type> Technical Report, </type> <institution> Department of Computer Science, UCSD, </institution> <year> 1995, </year> <booktitle> in Proceedings of IEEE IPPS'96 (10th Interational Parallel Processing Symposium). </booktitle>
Reference-contexts: However, this routing scheme may lead to communication contention if network traffic is heavy. We have not considered the overhead of memory accessing and caching effect [24]. Another interesting future work is to consider non-uniform iteration intervals as proposed in <ref> [4] </ref>. 18 Acknowledgment This work was supported by NSF RIA CCR-9409695 and INT-9533361.
Reference: [5] <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: For compiling Fortran and C, in order to make graph scheduling applicable, there is work to be done in generating coarse-grain task graphs [2]. The current algorithms still have restrictions. For example, we have not considered processor distances. It has been shown (e.g. <ref> [5] </ref>) in the modern processor architectures, the distance factor does not affect the cost of communication significantly because of the wormhole routing technology. However, this routing scheme may lead to communication contention if network traffic is heavy. We have not considered the overhead of memory accessing and caching effect [24].
Reference: [6] <author> H. El-Rewini, T. G. Lewis and H. H. Ali, </author> <title> Task Scheduling in Parallel and Distributed Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. <p> Our work has been motivated by the above research but takes into 1 consideration the characteristics of asynchronous parallelism and the impact of communication. We will examine how scheduling techniques for task and loop parallelism can be combined together for mapping iterative task computation on message-passing architectures. <ref> [6] </ref> presented an approach which uses a DAG scheduling algorithm for one iteration of the computation and computes the optimal unfolding for a special case. We will show how to derive a near-optimal unfolding factor for general ITGs. Load balancing and communication minimization are the important aspects of the optimizations.
Reference: [7] <author> C. Fu and T. Yang, </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations, </title> <booktitle> in Proc. of the 10th ACM International Conference on Supercomputing, Philadelphia, </booktitle> <pages> pp. 237-244, </pages> <month> May, </month> <year> 1996. </year>
Reference-contexts: When executing each individual task, the task receives the data items it needs, performs computation, and then sends out data produced for its children (using aggregate multicasting if possible). The style is similar to the one used in the RAPID and PYRROS systems <ref> [7, 27] </ref>. Next we examine the performance of 12 this code structure and provide an analysis on performance variation between the predicted parallel time and the actual run-time performance. <p> The theoretical results help us to understand why the algorithms can deliver good performance. These algorithmic and theoretical results are verified through experiments for solving scientific problems which can be represented as ITGs. The algorithms could be useful for mapping applications in graphical parallel programming tools <ref> [1, 7, 13] </ref> and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL [17, 20, 24] and we expect that ITG algorithms will also be useful. <p> These algorithmic and theoretical results are verified through experiments for solving scientific problems which can be represented as ITGs. The algorithms could be useful for mapping applications in graphical parallel programming tools [1, 7, 13] and run-time systems for irregular problems <ref> [7] </ref>. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL [17, 20, 24] and we expect that ITG algorithms will also be useful.
Reference: [8] <author> H. Gabow and R. Tarjan, </author> <title> Faster scaling algorithms for network problems, </title> <journal> SIAM J. Computing, </journal> <month> Oct </month> <year> 1989. </year>
Reference-contexts: These inequalities can be solved in a complexity of O ( p * ) using the shortest path algorithm <ref> [8] </ref> where constant * is the desired accuracy in finding the minimum value fi such that fi fl (G) fi fi fl (G) + *. For example, for Fig.1 (a), * is chosen as 0.5 and the value of fi is computed as 40. * Granularity.
Reference: [9] <author> F. Gasperoni and U. Schweigelshohn, </author> <title> Scheduling Loops on Parallel Processors: A simple algorithm with close to optimum performance. </title> <booktitle> Proc. of CONPAR 92 , pp. </booktitle> <pages> 613-624. </pages>
Reference-contexts: Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. In loop scheduling research, software pipelining <ref> [9, 12, 21] </ref> is an important technique proposed for exploiting instruction-level parallelism on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [16] have also been developed to allow a compiler to explore more parallelism. <p> But most of the previous software pipelining algorithms do not give the performance bound. In <ref> [9] </ref>, a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. <p> This transformation was first proposed in <ref> [9] </ref> for mapping graphs when communication is zero. In Section 4.6, we will show such a transformation is still valid for 7 our case by carefully designing the ITG schedule in Section 4.5. <p> The proof is based on the analysis in <ref> [9] </ref>. The main difference is that we consider communication delay and also we add a proof on resource constraint satisfaction. Theorem 2 The algorithm correctly produces a schedule for an ITG.
Reference: [10] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: In the task computation model [20], exploring data locality means to localize data communication between tasks by assigning them to the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication <ref> [10] </ref>. The optimal solution for a special class of ITGs has been studied in [3]. Our heuristic algorithm is designed for general ITGs. The optimal loop unfolding factor is derived so that the scheduling performance can be guaranteed compared to the optimal solution.
Reference: [11] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interpro-cessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: The ETF algorithm <ref> [11] </ref> has a similar performance bound with a complexity O (pv 2 ) while our algorithm has a lower complexity O ((v + e) log v). 4.5 Schedule Transformation The DAG scheduling produces processor assignment P roc (i) and the starting time fl i for each task T i in K
Reference: [12] <author> M. Lam, </author> <title> Software pipelining: an effective scheduling technique for VLIW machines, </title> <booktitle> ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. In loop scheduling research, software pipelining <ref> [9, 12, 21] </ref> is an important technique proposed for exploiting instruction-level parallelism on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [16] have also been developed to allow a compiler to explore more parallelism. <p> In Fig. 1 (d), the executions of task instances from different iterations are overlapped. for all iterations. * The idea of exploring parallelism within and across iterations has been proposed in the context of instruction-level software pipelining <ref> [12] </ref>. But most of the previous software pipelining algorithms do not give the performance bound. In [9], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed.
Reference: [13] <author> P. Newton and J. Browne, </author> <title> The CODE 2.0 graphical parallel programming language, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 167-177. </pages>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems [2, 17, 20, 24] and many graphical parallel programming tools use coarse-grain computation graphs to model user applications <ref> [1, 13] </ref>. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). An ITG represents a sequence of task computations where each iteration consists of the execution of a set of tasks. <p> The theoretical results help us to understand why the algorithms can deliver good performance. These algorithmic and theoretical results are verified through experiments for solving scientific problems which can be represented as ITGs. The algorithms could be useful for mapping applications in graphical parallel programming tools <ref> [1, 7, 13] </ref> and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL [17, 20, 24] and we expect that ITG algorithms will also be useful.
Reference: [14] <author> D. O'Hallaron, </author> <title> The ASSIGN parallel program generator, </title> <type> Tech. Report, </type> <institution> CS Dept, </institution> <address> CMU, CMU-CS-91-141, </address> <year> 1991. </year>
Reference-contexts: Two testing instances are both coarse grained, for matrices of sizes 600 fi 600 and 800 fi 800 with width 60. * Trellis. This is an FFT-based application example used in <ref> [14] </ref> for processing a sequence of signal information. The ITG for Trellis is acyclic and has mixed granularities. The structure of Trellis ITGs is demonstrated in Fig 6 (d) and two instances will be tested. * Randomly generated ITGs.
Reference: [15] <author> M. A. Palis, J.-C. Liou, Wei, D.S.L, </author> <title> A greedy task clustering heuristic that is provably good. </title> <booktitle> Proceedings of the 1994 International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN). IEEE 1994. p. </booktitle> <pages> 398-405. </pages>
Reference-contexts: Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time.
Reference: [16] <author> K. K. Parhi and D. G. Messerschmitt, </author> <title> Static rate-optimal scheduling of iterative dataflow programs via optimum unfolding, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40:2, </volume> <year> 1991, </year> <pages> pp. 178-195. </pages>
Reference-contexts: In loop scheduling research, software pipelining [9, 12, 21] is an important technique proposed for exploiting instruction-level parallelism on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques <ref> [16] </ref> have also been developed to allow a compiler to explore more parallelism. Our work has been motivated by the above research but takes into 1 consideration the characteristics of asynchronous parallelism and the impact of communication. <p> But most of the previous software pipelining algorithms do not give the performance bound. In [9], a near-optimal scheduling algorithm for pipelining with no communication delay is proposed. We need to extend this result to incorporate communication optimization. 4 * As we mentioned above, graph unfolding techniques <ref> [16] </ref> have been also developed to increase the number of tasks within each iteration, which gives a scheduler more flexibility to exploit parallelism. <p> This new graph is called G f and needs to be executed only bN=f c iterations. We use the unfolding algorithm proposed in <ref> [16] </ref> to construct G f . The new graph G f contains f fl v nodes such that each node T i in G corresponds to f nodes in G f : T i;1 ; T i;2 ; ; T i;f . <p> Notice the unfolded graph has the following property <ref> [16] </ref>: fi fl (G f ) = f fi fl (G). 4.3 Graph transformation We transform G f into a DAG so that the DAG scheduling technique could be applied.
Reference: [17] <author> S. S. Pande, D. P. Agrawal and J. Mauney, </author> <title> A scalable scheduling scheme for functional parallelism on distributed memory multiprocessor systems. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1995, </year> <note> vol.6, (no.4):388-99. </note>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems <ref> [2, 17, 20, 24] </ref> and many graphical parallel programming tools use coarse-grain computation graphs to model user applications [1, 13]. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). <p> Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. <p> The algorithms could be useful for mapping applications in graphical parallel programming tools [1, 7, 13] and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL <ref> [17, 20, 24] </ref> and we expect that ITG algorithms will also be useful. For compiling Fortran and C, in order to make graph scheduling applicable, there is work to be done in generating coarse-grain task graphs [2]. The current algorithms still have restrictions.
Reference: [18] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: contains all N fl v task instances T k i (1 i v, 1 k N ), and these tasks and their dependencies in E (G; N ) constitute a DAG. 1 Our results can be extended for inexact dependence values such as `+' and `*' used in the literature <ref> [18] </ref>. 2 * Each task T x in an ITG has a computation cost t x and there is a communication cost for sending a message from task T x on one processor to task T y on another processor.
Reference: [19] <author> R. Reiter, </author> <title> Scheduling parallel computations, </title> <journal> Journal of ACM, </journal> <month> Oct </month> <year> 1968, </year> <pages> pp. 590-599. 19 </pages>
Reference-contexts: The optimality of our solution is guaranteed by comparing it with the optimum that achieves the minimum time for the expanded graph E (G; N ). We will use the concept of integer-periodic scheduling <ref> [19] </ref> as in the instruction-level software pipelining and structure a schedule as follows: all instances of task T i are assigned to the same processor. <p> We summarize the information needed in computing the unfolding factor as follows. 5 * Smallest iteration interval. This is the smallest possible value fi p for any p when the communi- cation cost is zero <ref> [19] </ref>. It is defined as: fi fl (G) = max all cycles C in G t (C) ; where d (C) is the summation of dependence (edge) distances in this cycle C and t (C) is the summation of computation weights of the tasks in this cycle.
Reference: [20] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems <ref> [2, 17, 20, 24] </ref> and many graphical parallel programming tools use coarse-grain computation graphs to model user applications [1, 13]. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). <p> Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. <p> We will show how to derive a near-optimal unfolding factor for general ITGs. Load balancing and communication minimization are the important aspects of the optimizations. It is known that data locality must be explored for reducing unnecessary communication. In the task computation model <ref> [20] </ref>, exploring data locality means to localize data communication between tasks by assigning them to the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication [10]. The optimal solution for a special class of ITGs has been studied in [3]. <p> The algorithms could be useful for mapping applications in graphical parallel programming tools [1, 7, 13] and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL <ref> [17, 20, 24] </ref> and we expect that ITG algorithms will also be useful. For compiling Fortran and C, in order to make graph scheduling applicable, there is work to be done in generating coarse-grain task graphs [2]. The current algorithms still have restrictions.
Reference: [21] <author> V. H. Van Dongen, G. R. Gao and Q. Ning, </author> <title> A polynomial time method for optimal software pipelining. </title> <booktitle> Proc. of CONPAR 92, </booktitle> <pages> pp. 613-624. </pages>
Reference-contexts: Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. In loop scheduling research, software pipelining <ref> [9, 12, 21] </ref> is an important technique proposed for exploiting instruction-level parallelism on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [16] have also been developed to allow a compiler to explore more parallelism.
Reference: [22] <author> K. P. Wang, J. C. Bruch, Jr., </author> <title> An efficient iterative parallel finite element computational method, in the Mathematics of finite Elements and Applications., </title> <editor> J. R.Whiteman (Eds.), </editor> <year> 1994. </year>
Reference-contexts: Throughout this section, we will use the following tested cases. * A sparse linear system solver. This solver is based on the iterative SOR method which is still used in many engineering applications with some variation <ref> [22] </ref>, even new iterative methods such as CG are developed. We use submatrix partitioning and a partitioned sparse matrix usually contains many zero blocks. Notice that since the number of iterations for convergence is large, the convergence test can be conducted every few hundreds of iterations instead of every iteration.
Reference: [23] <author> P. Wang, Y.T. Yao, </author> <title> M.P. Tulin, An Efficient Numerical Tank for Nonlinear Water Waves based on the Multi-Subdomain Approach with BEM, </title> <booktitle> in Int. J. Numerical Methods in Fluids, </booktitle> <volume> Vol. 20, </volume> <pages> 383-392, </pages> <year> 1995. </year>
Reference-contexts: Two graphs are tested: one is a coarse grained grid of size 400 fi 400, another is a fine grained grid of size 200 fi 200. * A banded system solver. This problem is based on an application in <ref> [23] </ref>. The structure is cyclic as demonstrated in Fig. 6 (c), similar to the sparse SOR solver and but with some regularity. Two testing instances are both coarse grained, for matrices of sizes 600 fi 600 and 800 fi 800 with width 60. * Trellis.
Reference: [24] <author> R. Wolski and J. Feo, </author> <title> Program Partitioning for NUMA Multiprocessor Computer Systems, </title> <type> Tech. Report, </type> <institution> Lawrence Livermore Nat. Lab., </institution> <year> 1992. </year> <editor> J. </editor> <booktitle> of Parallel and Distributed Computing, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction Task graphs are important in representing parallel computations in compiler systems <ref> [2, 17, 20, 24] </ref> and many graphical parallel programming tools use coarse-grain computation graphs to model user applications [1, 13]. There are several representation schemes for computation graphs and in this paper we consider the scheduling of a class of graphs called iterative task graphs (ITGs). <p> Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. <p> The algorithms could be useful for mapping applications in graphical parallel programming tools [1, 7, 13] and run-time systems for irregular problems [7]. Graph scheduling algorithms (mainly for DAGs) have been successful in compiling functional languages such as SISAL <ref> [17, 20, 24] </ref> and we expect that ITG algorithms will also be useful. For compiling Fortran and C, in order to make graph scheduling applicable, there is work to be done in generating coarse-grain task graphs [2]. The current algorithms still have restrictions. <p> However, this routing scheme may lead to communication contention if network traffic is heavy. We have not considered the overhead of memory accessing and caching effect <ref> [24] </ref>. Another interesting future work is to consider non-uniform iteration intervals as proposed in [4]. 18 Acknowledgment This work was supported by NSF RIA CCR-9409695 and INT-9533361.
Reference: [25] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference-contexts: Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time.
Reference: [26] <author> T. Yang and A. Gerasoulis, </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 9, </volume> <pages> 951-967, </pages> <year> 1994. </year>
Reference-contexts: The result of the DAG scheduling is used for constructing the ITG schedule in Section 4.5. The PYRROS algorithm involves the following multiple stages in scheduling: 1) It clusters tasks with intensive communication using the DSC algorithm <ref> [26] </ref>. 2) It merges clusters to p processors using a load balancing algorithm. 3) It reorders tasks within each processor to overlap computation with communication. The algorithm has a complexity O ((v + e) log v). The size of the graph increases by f after unfolding. <p> We will derive the performance bound of the second algorithm, which is used in deriving the near-optimal unfolding factor. In Section 7, we conduct experiments to examine the impact of using this one-stage algorithm. In the one-stage approach, we use the idea of the DSC algorithm <ref> [26] </ref> but limit the number of clusters to p. The algorithm first computes the blevel value for each task where blevel (T x ) is the length of the longest path from this task T x to an exit node. <p> If a tie occurs, we will pick the latter one. The tlevel value of the selected task will be the latest starting time fl x of this task unless there are no processors available. The minimization procedure is also used in DSC ( <ref> [26] </ref>, page 958). The idea is to further reduce the starting time of the selected task by merging one or more parents of this examined task to one processor. In this way unnecessary communication could further be saved. The complexity of this algorithm is O ((v + e) log v).
Reference: [27] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism [28]. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. <ref> [6, 15, 17, 20, 24, 25, 27] </ref>. Using DAG scheduling algorithms for ITGs is not feasible since the number of iterations may be too large or may not even be known at compile-time. <p> The starting time fl i and the processor assignment P roc (i) of each task T i are provided. In mapping the kernel DAG, we use two algorithms: one is a multi-stage algorithm designed for the PYRROS system <ref> [27] </ref>, another is a one-stage algorithm. We will choose the smaller one between the two solutions produced by these algorithms. The result of the DAG scheduling is used for constructing the ITG schedule in Section 4.5. <p> When executing each individual task, the task receives the data items it needs, performs computation, and then sends out data produced for its children (using aggregate multicasting if possible). The style is similar to the one used in the RAPID and PYRROS systems <ref> [7, 27] </ref>. Next we examine the performance of 12 this code structure and provide an analysis on performance variation between the predicted parallel time and the actual run-time performance.
Reference: [28] <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines. </title> <booktitle> in Proc. of 24th Inter. Conference on Parallel Processing, Aug. 1995, </booktitle> <volume> Vol II, </volume> <pages> pp. 151-158. </pages>
Reference-contexts: An ITG represents a sequence of task computations where each iteration consists of the execution of a set of tasks. Task dependence may exist within an iteration and across iterations. Iterative task computations involve both task and loop parallelism <ref> [28] </ref>. There is a lot of research work on scheduling task parallelism based on directed acyclic graphs (DAG), e.g. [6, 15, 17, 20, 24, 25, 27].
References-found: 28

