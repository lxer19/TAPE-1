URL: ftp://ftp.icsi.berkeley.edu/pub/ai/jagota/vol1_2.ps
Refering-URL: http://www.icsi.berkeley.edu/~jagota/NCS/vol1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: mmisra@mines.edu  
Phone: Ph: (303)-273-3873 Fax: (303)-273-3875  
Title: Parallel Environments for Implementing Neural Networks  
Author: Manavendra Misra 
Web: WWW: http://www.mines.edu/fs home/mmisra/  
Address: Golden, CO 80401  
Affiliation: Dept of Mathematical and Computer Sciences Colorado School of Mines  
Abstract: As artificial neural networks (ANNs) gain popularity in a variety of application domains, it is critical that these models run fast and generate results in real time. Although a number of implementations of neural networks are available on sequential machines, most of these implementations require an inordinate amount of time to train or run ANNs, especially when the ANN models are large. One approach for speeding up the implementation of ANNs is to implement them on parallel machines. This paper surveys the area of parallel environments for the implementations of ANNs, and prescribes desired characteristics to look for in such implementations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Adaptive solutions inc web page. </institution> <note> http://www.asi.com/. </note>
Reference-contexts: Siemens built the SYNAPSE neurocomputer and its architecture is described in [81]. SYNAPSE uses 8 of Siemens' MA-16 matrix-matrix multiplier chips while the SYNAPSE 2 is a PC accelerator board with one MA-16 chip. Adaptive Solutions Inc. <ref> [1] </ref> markets special purpose ANN hardware under the name CNAPS [52]. CNAPS is based on the proprietary CNAPS-1064 Digital Parallel Processor chip that has 64 sub-processors operating in SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together.
Reference: [2] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> Communication Complexity of PRAMs. </title> <journal> Theoretical Computer Science, </journal> <volume> 71 </volume> <pages> 3-28, </pages> <year> 1990. </year>
Reference-contexts: The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM [19], BSP [98], C 3 [33], logP [8], L-PRAM <ref> [2] </ref>, H-PRAM [36], etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine. This theoretical analysis will also ensure that the parallel implementations generated by the environment will be efficient 1 .
Reference: [3] <author> D. M. Anthony. </author> <title> Reducing connectivity in compression networks. Neural Network Review, </title> <year> 1990. </year>
Reference-contexts: In addition to being biologically plausible, sparse networks have also been shown to be useful in applications <ref> [3] </ref>, so a number of researchers have looked at parallel implementations of sparse networks. Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem [54, 56, 57, 58, 59, 61].
Reference: [4] <author> V. C. Barbosa and P. Lima. </author> <title> On the distributed parallel simulation of Hopfield's neural networks. </title> <journal> Software, practice and experience, </journal> 20(10) 967-983, Oct 1990. 
Reference-contexts: The authors of [93] implement a model called EDANN (Entropy Driven ANN) on a transputer array and show an intersting application of ANNs in orientation extraction from images. Barbosa and Lima <ref> [4] </ref> present an Occam implementation of Hopfield Nets on distributed memory architectures. Fortuna et al [18] describe a simulator for cellular neural networks called PSIMCNN that is implemented on transputer based machines. An analysis of ANN implementations on transputers is presented in [67].
Reference: [5] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: Our final goal is to see if a parallel environment exists that allows easy mapping of various neural network models onto parallel machines. What are some of the desired features to look for in such an environment? The neural network literature presents a number of models inspired by biology <ref> [5, 35] </ref>. Each model has strengths and weaknesses, and one model is more appropriate than another for a given application. A parallel simulation environment should provide the user with a choice of models.
Reference: [6] <institution> Brainmaker home page. </institution> <note> http://www.calsci.com/home.htm. </note>
Reference-contexts: Some of these include PlaNet, UCLA-SFINX, Xerion, NeuroGraph, BrainMaker, Asprin-Migraines (also developed for the Cray family of vector supercomputers), and Pygmalion. Details about these simulators can be found in the Frequently Asked Questions of the newsgroup comp.ai.neural-nets [70]. Of these, BrainMaker is marketed by California Scientific Software <ref> [6] </ref> and is one of the more popular commercial programs. Its basic capabilities consist of training Back-propagation networks, but additional add-ons can be purchased. It comes with a utility called NetMaker which can import training data from a variety of formats, and thus simplify the process of creating the network. <p> Adaptive Solutions Inc. [1] markets special purpose ANN hardware under the name CNAPS [52]. CNAPS is based on the proprietary CNAPS-1064 Digital Parallel Processor chip that has 64 sub-processors operating in SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. BrainMaker <ref> [6] </ref> software has also been ported to run on the CNAPS PC cards. Nestor Inc. [69] marketed the NI1000 chip. This chip implements a network with Radial Basis Function neurons. It can store up to 1024 prototypes, with 256 dimensions, 5-bits per dimension.
Reference: [7] <author> L.-C. Chu and B. W. Wah. </author> <title> Optimal mapping of neural network learning on message passing multi-computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 319-339, </pages> <year> 1992. </year>
Reference-contexts: A discussion of the related literature follows below. Ghosh et al. [23] discuss the requirements to efficiently implement a generic neural network model on a multicomputer. The discussion includes mapping strategies, and an analysis of simulations of the mappings. In a similar vein, Chu and Wah <ref> [7, 101] </ref> describe optimal mapping of the learning process in multi-layer feed-forward networks on message-passing multicomputers. Predicted and actual results in applying this strategy to implement learning schemes like back-propagation on a network of Sun workstations and the Intel iPSC/2 Hypercube multicomputer are presented.
Reference: [8] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In 4 t h ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <year> 1993. </year>
Reference-contexts: Next, this analysis should investigate which parallel computation models can best exploit the parallelism in each model. The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM [19], BSP [98], C 3 [33], logP <ref> [8] </ref>, L-PRAM [2], H-PRAM [36], etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine.
Reference: [9] <author> Y. L. Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Hubbard, and L. D. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis [79], autonomous navigation [75], game playing [92], handwriting recognition <ref> [9] </ref>, image and signal processing [27, 88], etc. Another area that has benefited from ANN models is that of building intelligent vision models.
Reference: [10] <author> C. D'Autrechy, J. Reggia, G. Sutton, and S. Goodall. </author> <title> A general purpose simulation environment for developing connectionist models. Simulation, </title> <type> 51(1), </type> <month> July </month> <year> 1988. </year>
Reference-contexts: BrainMaker products are available for DOS, Windows, and Macintosh environments as well as the CNAPS parallel hardware from Adaptive Solutions Inc. [52]. A number of authors have worked in the area of developing concise languages for describing ANN architectures. These languages can then be used to create simulation environments <ref> [10] </ref> for both sequential as well as parallel machines. One effort in this regard was the Neuron Simulation Language [104] that can be used to describe single neurons, as well as networks of neurons. Another concise language (MDL) for ANN description is described in [91].
Reference: [11] <author> E. Deprit. </author> <title> Implementing recurrent back-propagation on the Connection Machine. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 295-314, </pages> <year> 1989. </year>
Reference-contexts: They present theoretical and experimental results to show that their technique performs quite well on the nCUBE and the CM-5. Other implementations on SIMD machines include Ranka et al [82] and Wilson [106]. Deprit <ref> [11] </ref> presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31].
Reference: [12] <author> K. I. Diamantara, D. L. Heine, and I. D. Scherson. </author> <title> Implementation of neural network algorithms on the P 3 parallel associative processor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 247-250, </pages> <year> 1990. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula [47], Scherson <ref> [12] </ref>, Shams [87], Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78]. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit).
Reference: [13] <author> O. Ekeberg, P. Hammarlund, B. Levin, and A. Lansner. Swim: </author> <title> A simulation environment for realistic neural network modeling. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 3. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM <ref> [13] </ref>, NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines.
Reference: [14] <author> A. El-Amawy and P. Kulasinghe. </author> <title> Algorithmic mapping of feedforward neural networks onto multiple bus systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(2) </volume> <pages> 130-136, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: An analysis of ANN implementations on transputers is presented in [67]. Wang and Wu [102, 107] show how to simulate five different NN models (ART1, ART2, feed-forward networks, recurrent networks, and Hopfield nets) on a shared memory vector multiprocessor (the Alliant FX/80). El-Amawy and Kulasinghe <ref> [14] </ref> present a method to implement feed-forward networks on an architecture called Multiple Bus System (MBS). An MBS has p processors, and b buses, with p b. The algorithm treats a feed-forward ANN as a feed-forward computational graph, and maps this graph onto an MBS.
Reference: [15] <author> C. Ernoult. </author> <title> Performance of backpropagation on a parallel tranputer-based machine. </title> <booktitle> In Neuro Nimes 88, </booktitle> <pages> pages 311-324, </pages> <year> 1988. </year>
Reference-contexts: The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31]. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2. In <ref> [15] </ref>, the authors show that a network of 2,480 neurons with 921,600 links simulated on 16 transputers runs 14 times faster than on one transputer.
Reference: [16] <author> H. Ernst, B. Mokry, and Z. Schreter. </author> <title> A transputer based general simulator for connectionist models. </title> <editor> In R. Eckmiller, G. Hartmann, and G. Hauske, editors, </editor> <booktitle> Parallel Processing in Neural Systems and Computers, </booktitle> <pages> pages 283-286. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year> <booktitle> Neural Computing Surveys vol 1, </booktitle> <pages> 48-60, </pages> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 55 </note>
Reference-contexts: Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2. In [15], the authors show that a network of 2,480 neurons with 921,600 links simulated on 16 transputers runs 14 times faster than on one transputer. The results in <ref> [16] </ref> use a four transputer network, and simulate a network with 74,996 connections to achieve a performance of 250,000 CUPS (connection updates per second).
Reference: [17] <author> C. Eswaran and K. V. Asari. </author> <title> Systolic array implementation of artificial neural networks. </title> <journal> Microprocessors and Microsystems, </journal> <volume> 18(8) </volume> <pages> 481-488, </pages> <year> 1994. </year>
Reference-contexts: Systolic implementation of associative memory NNs (such as the Hopfield network, the Bidirectional Associative Memory, Temporal Associative Memory) are presented in [50]. Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas <ref> [17] </ref>. Other systolic implementations are presented by Ramacher [80] and Jones et al [39]. A number of researchers have developed algorithms for implementing ANNs on specific machines. Zhang et al. describe an implementation of multi-layer feed-forward ANNs running Backpropagation on the Connection Machine-2.
Reference: [18] <author> L. Fortuna, G. Manganaro, and G. Nunnari. </author> <title> Parallel simulation of cellular neural networks. </title> <journal> Computers & Electrical Engineering, </journal> <volume> 22(1) </volume> <pages> 61-84, </pages> <year> 1996. </year>
Reference-contexts: The authors of [93] implement a model called EDANN (Entropy Driven ANN) on a transputer array and show an intersting application of ANNs in orientation extraction from images. Barbosa and Lima [4] present an Occam implementation of Hopfield Nets on distributed memory architectures. Fortuna et al <ref> [18] </ref> describe a simulator for cellular neural networks called PSIMCNN that is implemented on transputer based machines. An analysis of ANN implementations on transputers is presented in [67].
Reference: [19] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In 10-th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: Next, this analysis should investigate which parallel computation models can best exploit the parallelism in each model. The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM <ref> [19] </ref>, BSP [98], C 3 [33], logP [8], L-PRAM [2], H-PRAM [36], etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine.
Reference: [20] <author> G. Frazier. Ariel: </author> <title> A scalable multiprocessor for the simulation of neural networks. </title> <journal> Computer Architecture News, </journal> <volume> 18(1) </volume> <pages> 107-114, </pages> <month> Mar </month> <year> 1990. </year>
Reference-contexts: Turega [97] describes a special purpose architecture consisting of one conventional processor, and a number of very simple processor and memory nodes. Ariel <ref> [20] </ref> is an architecture built by Texas Instruments that uses fast DSPs, and very large semiconductor memories in order to simulate large ANNs. Siemens built the SYNAPSE neurocomputer and its architecture is described in [81].
Reference: [21] <author> K. Fukushima and S. Miyake. </author> <title> Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. </title> <journal> Pattern Recognition, </journal> <volume> 15(6) </volume> <pages> 455-469, </pages> <year> 1982. </year>
Reference-contexts: Developing intelligent artificial vision systems has proved to be a very challenging task, and looking towards the human visual system for inspiration has yielded exciting results <ref> [21, 54, 74, 99, 100] </ref>. These artificial models rely heavily on highly interconnected computational units functioning in parallel. The inspiration behind ANN models are biological models that are massively parallel, with many simple biological cells cooperating to solve problems.
Reference: [22] <author> R. D. Geller and D. W. Hammerstrom. </author> <title> A VLSI architecture for a neurocomputer using high-order predicates. </title> <booktitle> In Workshop on Computer Architecture for Pattern Analysis and Machine Intelligence, </booktitle> <pages> pages 153-161, </pages> <year> 1987. </year>
Reference-contexts: Another overview of hardware implementations of ANNs is presented in [48]. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller <ref> [22] </ref> and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation.
Reference: [23] <author> J. Ghosh and K. Hwang. </author> <title> Mapping Neural Networks onto Message-Passing Multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 291-330, </pages> <year> 1989. </year>
Reference-contexts: A number of researchers have contributed to the area of parallel implementations of ANNs by designing and analyzing algorithms to map specific ANN models on to specific parallel architectures. A discussion of the related literature follows below. Ghosh et al. <ref> [23] </ref> discuss the requirements to efficiently implement a generic neural network model on a multicomputer. The discussion includes mapping strategies, and an analysis of simulations of the mappings.
Reference: [24] <author> M. Glesner and W. Pochmuller. </author> <title> Neurocomputers-An overview of Neural Networks in VLSI. </title> <publisher> Chapman & Hall Neural Computing Series. Chapman and Hall, </publisher> <year> 1994. </year>
Reference-contexts: In this paper, we have concentrated on describing the work done in the latter area. However, a brief description of special purpose parallel hardware for ANN implementation is presented for completeness. A nice overview of special purpose hardware for ANNs is presented in the book by Glesner and Pochmuller <ref> [24] </ref>. Another overview of hardware implementations of ANNs is presented in [48]. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller [22] and Hammerstrom [34] have designed hardware to implement neural models.
Reference: [25] <author> N. Goddard. </author> <title> Rochester Connectionist Simulation Environment. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 10. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS <ref> [25] </ref>, and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines. Another interesting effort in this regard is described in [45] which talks about a simulation environment to simulate heterogeneous/hybrid ANNs.
Reference: [26] <author> N. H. Goddard, K. J. Lynne, T. Mintz, and L. Bukys. </author> <title> The Rochester Connectionist Simulator: User Manual. </title> <institution> University of Rochester, </institution> <type> Tech Report 233, </type> <year> 1989. </year>
Reference-contexts: Another excellent public domain sequential simulator has been developed at the University of Stuttgart. This simulator is called the Stuttgart Neural Network Simulator or SNNS [109, 110, 111]. Some ideas in this simulator were inspired by another sequential simulator, the Rochester Connectionist Simulator, RCS <ref> [26] </ref>. SNNS comes with an extensive and well written user's manual that makes it easy for a user to use and modify the software.
Reference: [27] <author> R. Gorman and T. Sejnowski. </author> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 75-89, </pages> <year> 1988. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis [79], autonomous navigation [75], game playing [92], handwriting recognition [9], image and signal processing <ref> [27, 88] </ref>, etc. Another area that has benefited from ANN models is that of building intelligent vision models. Since roughly 60% of the human brain is involved in interpreting visual input, it is not surprising that biologically inspired systems have proved to be useful in the field of Computer Vision.
Reference: [28] <author> K. F. Goser. </author> <title> Implementation of artificial neural networks into hardware: Concepts and limitations. </title> <booktitle> Mathematics and computers in simulation, </booktitle> 41(1/2):161-171, Jun 1996. 
Reference-contexts: A nice overview of special purpose hardware for ANNs is presented in the book by Glesner and Pochmuller [24]. Another overview of hardware implementations of ANNs is presented in [48]. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in <ref> [28] </ref>. Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation.
Reference: [29] <author> H. P. Graf and P. deVegar. </author> <title> A CMOS implementation of a neural network model. </title> <editor> In P. Losleben, editor, </editor> <booktitle> Advanced Research on VLSI, </booktitle> <pages> pages 351-367. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Another overview of hardware implementations of ANNs is presented in [48]. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. <ref> [29] </ref> have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation. Morgan et al. [62, 63] have used the Ring Array Processor (RAP) (designed for speech recognition tasks) for Connectionist applications.
Reference: [30] <author> K. A. Grajski. </author> <title> Neurocomputing using the MasPar MP-1. </title> <type> Technical Report 90-010, </type> <institution> Ford Aerospace, Advanced Dev. Dept., Mail Stop X-22, </institution> <address> San Jose, CA 95161-9041, </address> <year> 1990. </year>
Reference-contexts: Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar <ref> [30, 31] </ref>. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2. In [15], the authors show that a network of 2,480 neurons with 921,600 links simulated on 16 transputers runs 14 times faster than on one transputer.
Reference: [31] <author> K. A. Grajski, G. Chinn, C. Chen, C. Kusymail, and S. Tomboulian. </author> <title> Neural network simulation on the MasPar MP-1 massively parallel processor. </title> <booktitle> In Proceedings of the INNC, </booktitle> <address> Paris, France, </address> <year> 1990. </year>
Reference-contexts: Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar <ref> [30, 31] </ref>. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2. In [15], the authors show that a network of 2,480 neurons with 921,600 links simulated on 16 transputers runs 14 times faster than on one transputer.
Reference: [32] <author> S. N. Gupta, M. Zubair, and C. E. Grosch. </author> <title> Simulation of neural networks on massively parallel computer (DAP-510) using sparse matrix techniques. </title> <type> Technical report, </type> <institution> Dept of Computer Science, Old Dominion University, </institution> <address> VA 23529-0162, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem [54, 56, 57, 58, 59, 61]. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP <ref> [32] </ref>, and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [33] <author> S. E. Hambrusch and A. A. Khokhar. </author> <title> C 3 : A parallel model for coarse-grained machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2) </volume> <pages> 139-154, </pages> <month> Feb </month> <year> 1996. </year>
Reference-contexts: Next, this analysis should investigate which parallel computation models can best exploit the parallelism in each model. The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM [19], BSP [98], C 3 <ref> [33] </ref>, logP [8], L-PRAM [2], H-PRAM [36], etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine.
Reference: [34] <author> D. Hammerstrom. </author> <title> A VLSI architecture for high-performance, low-cost, on-chip learning. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 537-544, </pages> <year> 1990. </year>
Reference-contexts: Another overview of hardware implementations of ANNs is presented in [48]. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller [22] and Hammerstrom <ref> [34] </ref> have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation.
Reference: [35] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year> <booktitle> Neural Computing Surveys vol 1, </booktitle> <pages> 48-60, </pages> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 56 </note>
Reference-contexts: Our final goal is to see if a parallel environment exists that allows easy mapping of various neural network models onto parallel machines. What are some of the desired features to look for in such an environment? The neural network literature presents a number of models inspired by biology <ref> [5, 35] </ref>. Each model has strengths and weaknesses, and one model is more appropriate than another for a given application. A parallel simulation environment should provide the user with a choice of models.
Reference: [36] <author> T. Heywood and S. Ranka. </author> <title> A Practical Hierarchical Model of Parallel Computation: I. The Model. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 212-232, </pages> <year> 1992. </year>
Reference-contexts: The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM [19], BSP [98], C 3 [33], logP [8], L-PRAM [2], H-PRAM <ref> [36] </ref>, etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine. This theoretical analysis will also ensure that the parallel implementations generated by the environment will be efficient 1 .
Reference: [37] <author> M. Holler. </author> <title> Intel 80170NX Electrically Trainable Analog Neural Network (ETANN). </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks, </booktitle> <address> Washington D.C., </address> <booktitle> volume II, </booktitle> <pages> page 191, </pages> <year> 1989. </year>
Reference-contexts: This chip implements a network with Radial Basis Function neurons. It can store up to 1024 prototypes, with 256 dimensions, 5-bits per dimension. Another silicon implementation of ANNs came from Intel in the form of the 80170NX Electrically Trainable Analog Neural Network (ETANN) chip <ref> [37] </ref>. The ETANN implements an analog neural network with 64 inputs, 16 internal biases, and 64 neurons with sigmoidal transfer functions. It does not provide for on-chip learning. Emulation is done in software and the weights have to be downloaded to the chip.
Reference: [38] <author> M. James and D. Hoang. </author> <title> Design of low-cost, real-time simulation systems for large neural networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 221-235, </pages> <year> 1992. </year>
Reference-contexts: An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. <ref> [38] </ref> have also proposed special purpose hardware for neural network simulation. Morgan et al. [62, 63] have used the Ring Array Processor (RAP) (designed for speech recognition tasks) for Connectionist applications.
Reference: [39] <author> S. Jones, K. Sammut, and J. Hunter. </author> <title> Learning in linear systolic neural network engines: Analysis and implementation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(4):5844, </volume> <year> 1994. </year>
Reference-contexts: Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas [17]. Other systolic implementations are presented by Ramacher [80] and Jones et al <ref> [39] </ref>. A number of researchers have developed algorithms for implementing ANNs on specific machines. Zhang et al. describe an implementation of multi-layer feed-forward ANNs running Backpropagation on the Connection Machine-2.
Reference: [40] <author> H. Kato, H. Yoshizawa, H. Iciki, and K. Asakawa. </author> <title> A parallel neurocomputer architecture towards billion connection updates per second. </title> <booktitle> In International Joint Conference on Neural Networks IJCNN 90, </booktitle> <volume> volume II, </volume> <pages> pages 47-50, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: One approach to building special purpose hardware for ANNs is to use Digital Signal Processing (DSP) chips as the building blocks for such hardware. A parallel system using DSPs was developed at the Fujitsu Labs <ref> [40] </ref>. The Processing Element (PE) in this machine is the floating point DSP TMS320C30 from Texas Instruments. PEs are arranged in a linear array and an algorithm similar to S.Y.Kung's [43] is used for the implementation.
Reference: [41] <author> V. Kumar, S. Shekhar, and M. B. Amin. </author> <title> A scalable parallel formulation of the backpropagation algorithm for hypercubes and related architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(10) </volume> <pages> 1073-1090, </pages> <year> 1994. </year>
Reference-contexts: The authors describe how to implement a multiply-accumulate-rotate iteration for a fully connected network, using the 2-D mesh connections of the CM-2. Vipin Kumar et al. present a technique for mapping the Backpropagation learning algorithm on hypercubes and related architectures <ref> [41] </ref>. They present theoretical and experimental results to show that their technique performs quite well on the nCUBE and the CM-5. Other implementations on SIMD machines include Ranka et al [82] and Wilson [106]. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2.
Reference: [42] <author> H. T. Kung, D. A. Pomerleau, G. L. Gusciora, and D. S. Touretzky. </author> <title> How we got 17 million connections per second. </title> <booktitle> In International Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 143-150, </pages> <year> 1988. </year>
Reference-contexts: N. Hwang describe a scheme for designing special purpose systolic ring architectures to simulate neural nets. By recognizing that neural algorithms can be re-written as iterative matrix operations, the authors were able to directly apply techniques for mapping iterative matrix algorithms onto systolic architectures. H. T. Kung et al. <ref> [42] </ref> have reported results of implementing the Backpropagation learning algorithm on the CMU Warp. The Warp exploited coarse grained parallelism in the problem by mapping either partitions or copies of the network onto its ten systolic processors.
Reference: [43] <author> S. Y. Kung. </author> <title> Parallel architectures for artificial neural nets. </title> <booktitle> In International Conference on Systolic Arrays, </booktitle> <pages> pages 163-174, </pages> <year> 1988. </year>
Reference-contexts: There is an underlying similarity between the simple, special purpose computational units of a neural network, and the dedicated processing elements of a systolic array that apply a predefined computation on data elements as the data are pumped through the array. In <ref> [43, 44] </ref>, S. Y. Kung and J. N. Hwang describe a scheme for designing special purpose systolic ring architectures to simulate neural nets. <p> In [51], the authors provide a formal analysis of the systolic processing required to implement a Hopfield like network for solving a particular problem. The techniques used are similar in spirit to Kung and Hwang <ref> [43, 44] </ref>. Systolic implementation of associative memory NNs (such as the Hopfield network, the Bidirectional Associative Memory, Temporal Associative Memory) are presented in [50]. Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas [17]. <p> A parallel system using DSPs was developed at the Fujitsu Labs [40]. The Processing Element (PE) in this machine is the floating point DSP TMS320C30 from Texas Instruments. PEs are arranged in a linear array and an algorithm similar to S.Y.Kung's <ref> [43] </ref> is used for the implementation. Another DSP based machine is the MUSIC system (MUlti-Signal processor with Intelligent Communication) built at ETH [66, 65]. MUSIC too can serve as a special purpose NN machine. Special purpose implementations can be fast, efficient and cost effective.
Reference: [44] <author> S. Y. Kung and J. N. Hwang. </author> <title> A Unified Systolic Architecture for Artificial Neural Nets. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 358-387, </pages> <year> 1989. </year>
Reference-contexts: There is an underlying similarity between the simple, special purpose computational units of a neural network, and the dedicated processing elements of a systolic array that apply a predefined computation on data elements as the data are pumped through the array. In <ref> [43, 44] </ref>, S. Y. Kung and J. N. Hwang describe a scheme for designing special purpose systolic ring architectures to simulate neural nets. <p> In [51], the authors provide a formal analysis of the systolic processing required to implement a Hopfield like network for solving a particular problem. The techniques used are similar in spirit to Kung and Hwang <ref> [43, 44] </ref>. Systolic implementation of associative memory NNs (such as the Hopfield network, the Bidirectional Associative Memory, Temporal Associative Memory) are presented in [50]. Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas [17].
Reference: [45] <author> T. Lange. </author> <title> Simulation of heterogeneous neural networks on serial and parallel machines. </title> <journal> Parallel computing, </journal> <volume> 14(3) </volume> <pages> 287-303, </pages> <month> Aug </month> <year> 1990. </year>
Reference-contexts: The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines. Another interesting effort in this regard is described in <ref> [45] </ref> which talks about a simulation environment to simulate heterogeneous/hybrid ANNs. The environment is called DESCARTES (Development Environment for Simulating Connectionist ARchiTEctureS), and is written in LISP. The paper describes sequential, as well as an SIMD implementation on the Connection Machine-2.
Reference: [46] <author> R. Leighton and A. Wieland. </author> <title> The Asprin/Migraines software package. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 11. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines <ref> [46] </ref>. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines. Another interesting effort in this regard is described in [45] which talks about a simulation environment to simulate heterogeneous/hybrid ANNs.
Reference: [47] <author> W.-M. Lin, V. K. Prasanna, and K. W. Przytula. </author> <title> Algorithmic mapping of neural network models onto parallel SIMD machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(12) </volume> <pages> 1390-1401, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula <ref> [47] </ref>, Scherson [12], Shams [87], Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78]. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit).
Reference: [48] <author> C. Lindsey and T. Lindblad. </author> <title> Review of hardware neural networks: a user's perspective. </title> <booktitle> In Proceedings of ELBA94, </booktitle> <year> 1994. </year> <note> Also available from http://www1.cern.ch/NeuralNets/nnwInHepHard.html. </note>
Reference-contexts: However, a brief description of special purpose parallel hardware for ANN implementation is presented for completeness. A nice overview of special purpose hardware for ANNs is presented in the book by Glesner and Pochmuller [24]. Another overview of hardware implementations of ANNs is presented in <ref> [48] </ref>. An overview of how ANNs can be implemented in Ultra Large Scale Integration (ULSI) circuits is presented in [28]. Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model.
Reference: [49] <author> C. S. Lindsey, B. Denby, and T. Lindblad. </author> <title> Neural networks in hardware web page. </title> <address> http://www1.cern.ch/NeuralNets/nnwInHepHard.html. </address>
Reference-contexts: Thus, these implementations provide a good balance of speed and flexibility. We have therefore chosen to concentrate on general purpose implementations in this paper. Online current information about special purpose hardware implementations of ANNs can be found at <ref> [49] </ref>. 3 Desired Characteristics As is apparent from the above discussion, there has been a substantial amount of research done in the area of parallel implementations of ANNs.
Reference: [50] <author> K. Margaritis. </author> <title> On the systolic implementation of associative memory artificial neural networks. </title> <journal> Parallel Computing, </journal> <volume> 21(5) </volume> <pages> 825-840, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The techniques used are similar in spirit to Kung and Hwang [43, 44]. Systolic implementation of associative memory NNs (such as the Hopfield network, the Bidirectional Associative Memory, Temporal Associative Memory) are presented in <ref> [50] </ref>. Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas [17]. Other systolic implementations are presented by Ramacher [80] and Jones et al [39]. A number of researchers have developed algorithms for implementing ANNs on specific machines.
Reference: [51] <author> K. Margaritis and D. Evans. </author> <title> Systolic implementation of neural networks for searching sets of properties. </title> <journal> Parallel computing, </journal> <volume> 18(3) </volume> <pages> 325-334, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: H. T. Kung et al. [42] have reported results of implementing the Backpropagation learning algorithm on the CMU Warp. The Warp exploited coarse grained parallelism in the problem by mapping either partitions or copies of the network onto its ten systolic processors. In <ref> [51] </ref>, the authors provide a formal analysis of the systolic processing required to implement a Hopfield like network for solving a particular problem. The techniques used are similar in spirit to Kung and Hwang [43, 44].
Reference: [52] <author> H. McCartor. </author> <title> Back propagation implementation on the Adaptive Solutions CNAPS neurocomputer chip. </title> <editor> In R. L. et al., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 1028-1031. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year> <title> More details about CNAPS can be found at the Adaptive Solutions web site at http://www.asi.com. </title>
Reference-contexts: It comes with a utility called NetMaker which can import training data from a variety of formats, and thus simplify the process of creating the network. BrainMaker products are available for DOS, Windows, and Macintosh environments as well as the CNAPS parallel hardware from Adaptive Solutions Inc. <ref> [52] </ref>. A number of authors have worked in the area of developing concise languages for describing ANN architectures. These languages can then be used to create simulation environments [10] for both sequential as well as parallel machines. <p> Siemens built the SYNAPSE neurocomputer and its architecture is described in [81]. SYNAPSE uses 8 of Siemens' MA-16 matrix-matrix multiplier chips while the SYNAPSE 2 is a PC accelerator board with one MA-16 chip. Adaptive Solutions Inc. [1] markets special purpose ANN hardware under the name CNAPS <ref> [52] </ref>. CNAPS is based on the proprietary CNAPS-1064 Digital Parallel Processor chip that has 64 sub-processors operating in SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. BrainMaker [6] software has also been ported to run on the CNAPS PC cards.
Reference: [53] <author> E. Mesrobian, J. Skrzypek, A. Lee, and B. Ringer. </author> <title> A simulation environment for computational neuroscience. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 1. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year> <booktitle> Neural Computing Surveys vol 1, </booktitle> <pages> 48-60, </pages> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 57 </note>
Reference-contexts: A version of SNNS that uses training pattern parallelism has also been developed for the Intel Paragon (an MIMD distributed memory machine). A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX <ref> [53] </ref> environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines.
Reference: [54] <author> M. Misra. </author> <title> Implementation of Neural Networks on Parallel Architectures. </title> <type> PhD thesis, </type> <institution> Dept. of EE-Systems, University of Southern California, </institution> <year> 1992. </year>
Reference-contexts: Developing intelligent artificial vision systems has proved to be a very challenging task, and looking towards the human visual system for inspiration has yielded exciting results <ref> [21, 54, 74, 99, 100] </ref>. These artificial models rely heavily on highly interconnected computational units functioning in parallel. The inspiration behind ANN models are biological models that are massively parallel, with many simple biological cells cooperating to solve problems. <p> Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [55] <author> M. Misra and V. K. P. Kumar. </author> <title> Efficient VLSI Implementation of Iterative Solutions to Sparse Linear Systems. </title> <editor> In J. McCanny, J. McWhirter, and E. S. Jr., editors, </editor> <booktitle> Systolic Array Processors, </booktitle> <pages> pages 52-61. </pages> <publisher> Prentice Hall, </publisher> <year> 1989. </year> <booktitle> Proceedings of the 3rd Int. Conf. on Systolic Arrays. </booktitle>
Reference-contexts: In addition to being biologically plausible, sparse networks have also been shown to be useful in applications [3], so a number of researchers have looked at parallel implementations of sparse networks. Again, in a number of situations, parallel sparse matrix-vector techniques <ref> [55, 60] </ref> can be used to solve the problem [54, 56, 57, 58, 59, 61]. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64].
Reference: [56] <author> M. Misra and V. K. P. Kumar. </author> <title> Massive Memory Organizations for Implementing Neural Networks. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 259-264, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [57] <author> M. Misra and V. K. P. Kumar. </author> <title> Neural network simulation on a Reduced Mesh of Trees organization. </title> <booktitle> In SPIE/SPSE Symposium on Electronic Imaging, </booktitle> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [58] <author> M. Misra and V. K. P. Kumar. </author> <title> Implementation of Sparse Neural Networks on Fixed Size Arrays. </title> <editor> In M. A. Bayoumi, editor, </editor> <title> Parallel Algorithms and Architectures for DSP Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [59] <author> M. Misra and V. K. P. Kumar. </author> <title> Implementation of neural networks on parallel architectures. </title> <editor> In B. Soucek, editor, </editor> <title> Fast Learning and Invariant Object Recognition. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1992. </year> <note> In print. </note>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [60] <author> M. Misra, D. Nassimi, and V. K. Prasanna. </author> <title> Efficient VLSI implementation of iterative solutions to sparse linear systems. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 525-544, </pages> <year> 1993. </year>
Reference-contexts: In addition to being biologically plausible, sparse networks have also been shown to be useful in applications [3], so a number of researchers have looked at parallel implementations of sparse networks. Again, in a number of situations, parallel sparse matrix-vector techniques <ref> [55, 60] </ref> can be used to solve the problem [54, 56, 57, 58, 59, 61]. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64].
Reference: [61] <author> M. Misra and V. K. Prasanna. </author> <title> Implementation of neural networks on massive memory organizations. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 39(7) </volume> <pages> 476-480, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Again, in a number of situations, parallel sparse matrix-vector techniques [55, 60] can be used to solve the problem <ref> [54, 56, 57, 58, 59, 61] </ref>. Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP [64]. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [62] <author> N. Morgan, J. Beck, P. Kohn, J. Bilmes, E. Allman, and J. Beer. </author> <title> The RAP: a Ring Array Processor for Layered Network Calculations. </title> <booktitle> In Proc. of Intl. Conf. on Application Specific Array Processors, </booktitle> <address> Princeton, N.J., </address> <pages> pages 296-308, </pages> <year> 1990. </year>
Reference-contexts: Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation. Morgan et al. <ref> [62, 63] </ref> have used the Ring Array Processor (RAP) (designed for speech recognition tasks) for Connectionist applications. Nordstrom et al. [71] discuss the implementation of ANNs on existing machines, as well as the design of new architectures specifically tuned to ANN implementation.
Reference: [63] <author> N. Morgan, J. Beck, P. Kohn, J. Bilmes, E. Allman, and J. Beer. </author> <title> The Ring Array Processor: A multiprocessing peripheral for connectionist applications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 248-259, </pages> <year> 1992. </year>
Reference-contexts: Geller [22] and Hammerstrom [34] have designed hardware to implement neural models. Similarly, Graf et al. [29] have developed a CMOS implementation of a neural network model. James et al. [38] have also proposed special purpose hardware for neural network simulation. Morgan et al. <ref> [62, 63] </ref> have used the Ring Array Processor (RAP) (designed for speech recognition tasks) for Connectionist applications. Nordstrom et al. [71] discuss the implementation of ANNs on existing machines, as well as the design of new architectures specifically tuned to ANN implementation.
Reference: [64] <author> S. M. Muller and B. Gomes. </author> <title> Efficient mapping of randomly sparse neural networks on parallel vector supercomputers. </title> <booktitle> In Proceedings of the Sixth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Gupta et al. have approached the problem of implementing sparse ANNs on the DAP [32], and Muller et al. have done the same on the RAP <ref> [64] </ref>. A group of researchers have chosen systolic architectures for implementing ANNs.
Reference: [65] <author> U. Muller, A. Gunzinger, and W. Guggenbuhl. </author> <title> Fast neural net simulation with a DSP processor array. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(1) </volume> <pages> 203-213, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: PEs are arranged in a linear array and an algorithm similar to S.Y.Kung's [43] is used for the implementation. Another DSP based machine is the MUSIC system (MUlti-Signal processor with Intelligent Communication) built at ETH <ref> [66, 65] </ref>. MUSIC too can serve as a special purpose NN machine. Special purpose implementations can be fast, efficient and cost effective. However, these implementations offer very little flexibility.
Reference: [66] <author> U. A. Muller, B. Baumle, PeterKohler, and AntonGunzinger. </author> <title> Achieving supercomputer performance for neural net simulation with an array of digital signal processors. </title> <journal> IEEE micro, </journal> <volume> 12(5) </volume> <pages> 55-65, </pages> <month> Oct </month> <year> 1992. </year>
Reference-contexts: PEs are arranged in a linear array and an algorithm similar to S.Y.Kung's [43] is used for the implementation. Another DSP based machine is the MUSIC system (MUlti-Signal processor with Intelligent Communication) built at ETH <ref> [66, 65] </ref>. MUSIC too can serve as a special purpose NN machine. Special purpose implementations can be fast, efficient and cost effective. However, these implementations offer very little flexibility.
Reference: [67] <author> J. M. J. Murre. </author> <title> Transputers and neural networks: An analysis of implementation constraints and performance. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 284-292, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Barbosa and Lima [4] present an Occam implementation of Hopfield Nets on distributed memory architectures. Fortuna et al [18] describe a simulator for cellular neural networks called PSIMCNN that is implemented on transputer based machines. An analysis of ANN implementations on transputers is presented in <ref> [67] </ref>. Wang and Wu [102, 107] show how to simulate five different NN models (ART1, ART2, feed-forward networks, recurrent networks, and Hopfield nets) on a shared memory vector multiprocessor (the Alliant FX/80).
Reference: [68] <editor> J. M. J. Murre. Neurosimulators. In M. A. Arbib, editor, </editor> <booktitle> Handbook of Brain Research and Neural Networks. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: This will allow us to identify the features that should be carried over to parallel implementation environments. A significant amount of work has been done in developing simulation environments for ANNs on sequential machines. An earlier survey of sequential ANN simulators was provided in <ref> [68] </ref>. Historically, one of the more popular simulators was provided with the PDP book [84]. This simulator was ported to a number of sequential platforms, and provided the user with an opportunity to learn about ANNs, and use them to solve problems.
Reference: [69] <author> Nestor web site. </author> <note> http://www.nestor.com. </note>
Reference-contexts: Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. BrainMaker [6] software has also been ported to run on the CNAPS PC cards. Nestor Inc. <ref> [69] </ref> marketed the NI1000 chip. This chip implements a network with Radial Basis Function neurons. It can store up to 1024 prototypes, with 256 dimensions, 5-bits per dimension. Another silicon implementation of ANNs came from Intel in the form of the 80170NX Electrically Trainable Analog Neural Network (ETANN) chip [37].
Reference: [70] <institution> Neural Networks: </institution> <note> Frequently Asked Questions web page. http://www-leibniz.imag.fr/RESEAUX/osorio/faqs/FAQ.html#questions. Neural Computing Surveys vol 1, 48-60, 1996, http://www.icsi.berkeley.edu/~jagota/NCS 58 </note>
Reference-contexts: Some of these include PlaNet, UCLA-SFINX, Xerion, NeuroGraph, BrainMaker, Asprin-Migraines (also developed for the Cray family of vector supercomputers), and Pygmalion. Details about these simulators can be found in the Frequently Asked Questions of the newsgroup comp.ai.neural-nets <ref> [70] </ref>. Of these, BrainMaker is marketed by California Scientific Software [6] and is one of the more popular commercial programs. Its basic capabilities consist of training Back-propagation networks, but additional add-ons can be purchased.
Reference: [71] <author> T. Nordstrom and B. Svensson. </author> <title> Using and designing massively parallel computers for artificial neural networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 260-285, </pages> <year> 1992. </year>
Reference-contexts: James et al. [38] have also proposed special purpose hardware for neural network simulation. Morgan et al. [62, 63] have used the Ring Array Processor (RAP) (designed for speech recognition tasks) for Connectionist applications. Nordstrom et al. <ref> [71] </ref> discuss the implementation of ANNs on existing machines, as well as the design of new architectures specifically tuned to ANN implementation. Researchers at the University College London have built a 16-bit RISC processor with local memory and a communication unit on one chip for ANN implementation [73, 96].
Reference: [72] <author> R. C. O'Reilly, C. K. Dawson, and J. L. McClelland. </author> <title> PDP++ users manual. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1997. </year> <note> Available from http://www.cnbc.cmu.edu/PDP++/PDP++.html. </note>
Reference-contexts: This simulator was ported to a number of sequential platforms, and provided the user with an opportunity to learn about ANNs, and use them to solve problems. A newer version of this simulator (called PDP++) has been released by the developers <ref> [72] </ref>. This version provides the user with a graphical user interface (GUI) based on the InterViews toolkit through which the user can choose a particular ANN model to simulate. <p> The software comes with an extensive on-line manual, and has been ported to a number of sequential platforms. Executables for certain platforms, as well as source code is available freely <ref> [72] </ref>. Although parallel versions of the software are not being developed, the design of the simulator presents a number of desirable features that could be used in the development of a parallel environment. Another excellent public domain sequential simulator has been developed at the University of Stuttgart.
Reference: [73] <author> M. Pacheco and P. Treleaven. Neural-RISC: </author> <title> A Processor and Parallel Architecture for Neural Networks. </title> <booktitle> In International Joint Conference on Neural Networks IJCNN 92, </booktitle> <volume> volume II, </volume> <pages> pages 177-182, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Researchers at the University College London have built a 16-bit RISC processor with local memory and a communication unit on one chip for ANN implementation <ref> [73, 96] </ref>. Turega [97] describes a special purpose architecture consisting of one conventional processor, and a number of very simple processor and memory nodes. Ariel [20] is an architecture built by Texas Instruments that uses fast DSPs, and very large semiconductor memories in order to simulate large ANNs.
Reference: [74] <author> T. Poggio and S. Edelman. </author> <title> A neural network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343(6255) </volume> <pages> 263-266, </pages> <month> Jan 18 </month> <year> 1990. </year>
Reference-contexts: Developing intelligent artificial vision systems has proved to be a very challenging task, and looking towards the human visual system for inspiration has yielded exciting results <ref> [21, 54, 74, 99, 100] </ref>. These artificial models rely heavily on highly interconnected computational units functioning in parallel. The inspiration behind ANN models are biological models that are massively parallel, with many simple biological cells cooperating to solve problems.
Reference: [75] <author> D. A. Pomerleau. Alvinn: </author> <title> An autonomous land vehicle in a neural network. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 305-313. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis [79], autonomous navigation <ref> [75] </ref>, game playing [92], handwriting recognition [9], image and signal processing [27, 88], etc. Another area that has benefited from ANN models is that of building intelligent vision models.
Reference: [76] <author> K. W. Przytula and V. K. P. Kumar. </author> <title> Algorithmic mapping of neural networks models on parallel SIMD machines. </title> <booktitle> In International Conference on Application Specific Array Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna <ref> [76, 77] </ref>, Lin, Prasanna and Przytula [47], Scherson [12], Shams [87], Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78].
Reference: [77] <author> K. W. Przytula, W.-M. Lin, and V. K. P. Kumar. </author> <title> Partitioned implementation of neural networks on mesh connected array processors. </title> <booktitle> In Workshop on VLSI Signal Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna <ref> [76, 77] </ref>, Lin, Prasanna and Przytula [47], Scherson [12], Shams [87], Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78].
Reference: [78] <author> K. W. Przytula and V. K. Prasanna, </author> <title> editors. Digital Parallel Implementations of Neural Networks. </title> <publisher> Prentice Hall, </publisher> <month> Spring </month> <year> 1992. </year>
Reference-contexts: Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula [47], Scherson [12], Shams [87], Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna <ref> [78] </ref>. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit).
Reference: [79] <author> N. Qian and T. J. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis <ref> [79] </ref>, autonomous navigation [75], game playing [92], handwriting recognition [9], image and signal processing [27, 88], etc. Another area that has benefited from ANN models is that of building intelligent vision models.
Reference: [80] <author> U. Ramacher and J. Beichter. </author> <title> Systolic Architectures for Fast Emulation of Artificial Neural Networks. </title> <editor> In J. McCanny, J. McWhirter, and E. S. Jr., editors, </editor> <booktitle> Systolic Array Processors, </booktitle> <pages> pages 277-286. </pages> <publisher> Prentice Hall, </publisher> <year> 1989. </year> <booktitle> Proceedings of the 3rd Int. Conf. on Systolic Arrays. </booktitle>
Reference-contexts: Systolic implementation of associative memory NNs (such as the Hopfield network, the Bidirectional Associative Memory, Temporal Associative Memory) are presented in [50]. Eswaran et al describe a hardware implementation of Hopfield and Hamming Nets using systolic ideas [17]. Other systolic implementations are presented by Ramacher <ref> [80] </ref> and Jones et al [39]. A number of researchers have developed algorithms for implementing ANNs on specific machines. Zhang et al. describe an implementation of multi-layer feed-forward ANNs running Backpropagation on the Connection Machine-2.
Reference: [81] <author> U. Ramacher, W. Raab, J. Anlauf, J. Beichter, U. Hachmann, N. Bruls, M. Wesseling, E. Sicheneder, R. Manner, J. Grass, and A. Wurz. </author> <title> Multiprocessor and memory architecture of the neurocomputer SYNAPSE I. </title> <booktitle> In Digest ICANN '93, </booktitle> <pages> pages 1034-1039, </pages> <year> 1993. </year>
Reference-contexts: Ariel [20] is an architecture built by Texas Instruments that uses fast DSPs, and very large semiconductor memories in order to simulate large ANNs. Siemens built the SYNAPSE neurocomputer and its architecture is described in <ref> [81] </ref>. SYNAPSE uses 8 of Siemens' MA-16 matrix-matrix multiplier chips while the SYNAPSE 2 is a PC accelerator board with one MA-16 chip. Adaptive Solutions Inc. [1] markets special purpose ANN hardware under the name CNAPS [52].
Reference: [82] <author> S. Ranka, N. Asokan, R. Shankar, C. K. Mohan, and K. Mehrotra. </author> <title> A neural network simulator on the Connection Machine. </title> <booktitle> In Fifth IEEE International Symposium on Intelligent Control, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: Vipin Kumar et al. present a technique for mapping the Backpropagation learning algorithm on hypercubes and related architectures [41]. They present theoretical and experimental results to show that their technique performs quite well on the nCUBE and the CM-5. Other implementations on SIMD machines include Ranka et al <ref> [82] </ref> and Wilson [106]. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31].
Reference: [83] <author> C. R. Rosenberg and G. Blelloch. </author> <title> An implementation of network learning on the Connection Machine. </title> <editor> In D. Waltz and J. Feldman, editors, </editor> <title> Connectionist Models and their Implications. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1988. </year>
Reference-contexts: Other implementations on SIMD machines include Ranka et al [82] and Wilson [106]. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on <ref> [83] </ref> and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31]. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2.
Reference: [84] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> and the PDP Research Group. </title> <booktitle> Parallel Distributed Processing: Exploration in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Typically, the human brain consists of approximately 10 11 neurons, each with an average of 10 3 -10 4 connections. It is believed that the immense computing power of the brain is the result of the parallel and distributed computing performed by these neurons <ref> [84] </ref>. <p> A significant amount of work has been done in developing simulation environments for ANNs on sequential machines. An earlier survey of sequential ANN simulators was provided in [68]. Historically, one of the more popular simulators was provided with the PDP book <ref> [84] </ref>. This simulator was ported to a number of sequential platforms, and provided the user with an opportunity to learn about ANNs, and use them to solve problems. A newer version of this simulator (called PDP++) has been released by the developers [72].
Reference: [85] <author> P. Sajda, K. Sakai, S.-C. Yen, and L. Finkel. </author> <title> Nexus: A neural simulator for integrating top-down and bottom-up modeling. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 2. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A version of SNNS that uses training pattern parallelism has also been developed for the Intel Paragon (an MIMD distributed memory machine). A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus <ref> [85] </ref>, SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines.
Reference: [86] <author> T. J. Sejnowski and C. R. Rosenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion <ref> [86] </ref>, protein structure analysis [79], autonomous navigation [75], game playing [92], handwriting recognition [9], image and signal processing [27, 88], etc. Another area that has benefited from ANN models is that of building intelligent vision models.
Reference: [87] <author> S. Shams and K. W. Przytula. </author> <title> Mapping of neural networks onto programmable parallel machines. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <month> May </month> <year> 1990. </year> <booktitle> Neural Computing Surveys vol 1, </booktitle> <pages> 48-60, </pages> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 59 </note>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula [47], Scherson [12], Shams <ref> [87] </ref>, Tomboulian [94, 95]. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78]. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit).
Reference: [88] <author> P. Simpson. </author> <title> Artificial Neural Systems: Foundations, Paradigms, Applications and Implementations. </title> <publisher> Elmsford Press: Pergamon Press, </publisher> <year> 1990. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis [79], autonomous navigation [75], game playing [92], handwriting recognition [9], image and signal processing <ref> [27, 88] </ref>, etc. Another area that has benefited from ANN models is that of building intelligent vision models. Since roughly 60% of the human brain is involved in interpreting visual input, it is not surprising that biologically inspired systems have proved to be useful in the field of Computer Vision.
Reference: [89] <author> A. Singer. </author> <title> Implementations of artificial neural networks on the Connection Machine. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 305-315, </pages> <year> 1990. </year> <note> Also appears as Technical Report RL90-2 from Thinking Machines Corporation, </note> <institution> MA, USA. </institution>
Reference-contexts: Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31]. Singer <ref> [89] </ref> presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2. In [15], the authors show that a network of 2,480 neurons with 921,600 links simulated on 16 transputers runs 14 times faster than on one transputer.
Reference: [90] <author> J. Skrzypek, </author> <title> editor. Neural Network Simulation Environments. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A version of SNNS that uses training pattern parallelism has also been developed for the Intel Paragon (an MIMD distributed memory machine). A description of a number of sequential (and some parallel) ANN simulation environments is presented in <ref> [90] </ref> and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J.
Reference: [91] <author> J. Teeters. </author> <title> MDL: A system for fast simulation of large layered neural networks. </title> <journal> Simulation, </journal> <volume> 56(6) </volume> <pages> 369-379, </pages> <year> 1991. </year>
Reference-contexts: One effort in this regard was the Neuron Simulation Language [104] that can be used to describe single neurons, as well as networks of neurons. Another concise language (MDL) for ANN description is described in <ref> [91] </ref>. MDL allows the specification of a large network in just a couple of pages. Its parallel structure also allows highly optimized implementation on parallel machines. SLONN [103] can efficiently represent single neurons, small networks, as well as large networks.
Reference: [92] <author> G. Tesauro and T. J. Sejnowski. </author> <title> A neural network that learns to play backgammon. </title> <editor> In D. Z. Anderson, editor, </editor> <booktitle> Neural Information and Processing Systems (Denver 1987), </booktitle> <pages> pages 442-456. </pages> <institution> Americal Institute of Physics, </institution> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The result of following the design philosophy of massively interconnecting simple units has provided models that have proved to be successful in a number of applications, including text to speech conversion [86], protein structure analysis [79], autonomous navigation [75], game playing <ref> [92] </ref>, handwriting recognition [9], image and signal processing [27, 88], etc. Another area that has benefited from ANN models is that of building intelligent vision models.
Reference: [93] <author> T. Tollenaere, M. M. V. Hulle, and G. A. Orban. </author> <title> Parellel implementation and capabilities of entropy-driven artificial neural networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(3) </volume> <pages> 286-305, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: The results in [16] use a four transputer network, and simulate a network with 74,996 connections to achieve a performance of 250,000 CUPS (connection updates per second). The authors of <ref> [93] </ref> implement a model called EDANN (Entropy Driven ANN) on a transputer array and show an intersting application of ANNs in orientation extraction from images. Barbosa and Lima [4] present an Occam implementation of Hopfield Nets on distributed memory architectures.
Reference: [94] <author> S. Tomboulian. </author> <title> A system for routing arbitrary directed graphs on SIMD architectures. </title> <type> Technical Report ICASE Report No. 87-14, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula [47], Scherson [12], Shams [87], Tomboulian <ref> [94, 95] </ref>. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78]. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit). <p> Other implementations on SIMD machines include Ranka et al [82] and Wilson [106]. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and <ref> [94, 95] </ref>. Grajski et al. discuss ANN implementations on the MasPar [30, 31]. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2.
Reference: [95] <author> S. Tomboulian. </author> <title> Introduction to a system for implementing Neural Net connections on SIMD architectures. </title> <type> Technical Report ICASE No. 88-3, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Techniques from the field of parallel matrix computations can also be brought to bear on this problem. Algorithmic work that can be placed in this category includes that of Przytula and Prasanna [76, 77], Lin, Prasanna and Przytula [47], Scherson [12], Shams [87], Tomboulian <ref> [94, 95] </ref>. A collection of parallel algorithms to implement ANNs is available in the book edited by Przytula and Prasanna [78]. A number of the algorithms described above deal with fully connected ANNs (networks where each unit is connected to every other unit). <p> Other implementations on SIMD machines include Ranka et al [82] and Wilson [106]. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and <ref> [94, 95] </ref>. Grajski et al. discuss ANN implementations on the MasPar [30, 31]. Singer [89] presents a detailed comparison of five different techniques of mapping the Backpropagation algorithm onto the CM-2.
Reference: [96] <author> P. Treleaven, M. Pacheco, and M. Vellasco. </author> <title> VLSI Architectures for Neural Networks. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 8-27, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Researchers at the University College London have built a 16-bit RISC processor with local memory and a communication unit on one chip for ANN implementation <ref> [73, 96] </ref>. Turega [97] describes a special purpose architecture consisting of one conventional processor, and a number of very simple processor and memory nodes. Ariel [20] is an architecture built by Texas Instruments that uses fast DSPs, and very large semiconductor memories in order to simulate large ANNs.
Reference: [97] <author> M. Turega. </author> <title> A computer architecture to support neural net simulation. </title> <journal> The Computer Journal, </journal> <volume> 35(4) </volume> <pages> 350-360, </pages> <month> Aug </month> <year> 1992. </year>
Reference-contexts: Researchers at the University College London have built a 16-bit RISC processor with local memory and a communication unit on one chip for ANN implementation [73, 96]. Turega <ref> [97] </ref> describes a special purpose architecture consisting of one conventional processor, and a number of very simple processor and memory nodes. Ariel [20] is an architecture built by Texas Instruments that uses fast DSPs, and very large semiconductor memories in order to simulate large ANNs.
Reference: [98] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Next, this analysis should investigate which parallel computation models can best exploit the parallelism in each model. The parallel computational and communication complexity of each of the ANN architectures should be computed on the common models of parallel computation such as PRAM [19], BSP <ref> [98] </ref>, C 3 [33], logP [8], L-PRAM [2], H-PRAM [36], etc. This will provide researchers with an idea of the amount of parallelism that can be exploited when each model is implemented on the appropriate parallel machine.
Reference: [99] <author> C. von der Malsburg. </author> <title> Goal and architecture of nueral computers. </title> <editor> In Eckmiller, editor, Neurocomputers. </editor> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Developing intelligent artificial vision systems has proved to be a very challenging task, and looking towards the human visual system for inspiration has yielded exciting results <ref> [21, 54, 74, 99, 100] </ref>. These artificial models rely heavily on highly interconnected computational units functioning in parallel. The inspiration behind ANN models are biological models that are massively parallel, with many simple biological cells cooperating to solve problems.
Reference: [100] <author> C. von der Malsburg. </author> <title> Pattern recognition by labeled graph matching. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 141-148, </pages> <year> 1988. </year>
Reference-contexts: Developing intelligent artificial vision systems has proved to be a very challenging task, and looking towards the human visual system for inspiration has yielded exciting results <ref> [21, 54, 74, 99, 100] </ref>. These artificial models rely heavily on highly interconnected computational units functioning in parallel. The inspiration behind ANN models are biological models that are massively parallel, with many simple biological cells cooperating to solve problems.
Reference: [101] <author> B. W. Wah and L.-C. Chu. </author> <title> Efficient mapping of neural networks on multicomputers. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 234-238, </pages> <year> 1990. </year>
Reference-contexts: A discussion of the related literature follows below. Ghosh et al. [23] discuss the requirements to efficiently implement a generic neural network model on a multicomputer. The discussion includes mapping strategies, and an analysis of simulations of the mappings. In a similar vein, Chu and Wah <ref> [7, 101] </ref> describe optimal mapping of the learning process in multi-layer feed-forward networks on message-passing multicomputers. Predicted and actual results in applying this strategy to implement learning schemes like back-propagation on a network of Sun workstations and the Intel iPSC/2 Hypercube multicomputer are presented.
Reference: [102] <author> C.-J. Wang and C.-H. Wu. </author> <title> Parallel simulation of neural networks. </title> <booktitle> Simulation, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Barbosa and Lima [4] present an Occam implementation of Hopfield Nets on distributed memory architectures. Fortuna et al [18] describe a simulator for cellular neural networks called PSIMCNN that is implemented on transputer based machines. An analysis of ANN implementations on transputers is presented in [67]. Wang and Wu <ref> [102, 107] </ref> show how to simulate five different NN models (ART1, ART2, feed-forward networks, recurrent networks, and Hopfield nets) on a shared memory vector multiprocessor (the Alliant FX/80). El-Amawy and Kulasinghe [14] present a method to implement feed-forward networks on an architecture called Multiple Bus System (MBS).
Reference: [103] <author> D. Wang and C. Hsu. SLONN: </author> <title> A simulation language for modeling of neural networks. </title> <journal> Simulation, </journal> <volume> 55(2) </volume> <pages> 69-83, </pages> <month> Aug </month> <year> 1990. </year>
Reference-contexts: Another concise language (MDL) for ANN description is described in [91]. MDL allows the specification of a large network in just a couple of pages. Its parallel structure also allows highly optimized implementation on parallel machines. SLONN <ref> [103] </ref> can efficiently represent single neurons, small networks, as well as large networks. Before an environment is developed to efficiently implement ANN models on parallel computers, there has to be a theoretical analysis of the mapping of ANN models onto various parallel machine models.
Reference: [104] <author> A. Weitzenfeld and M. Arbib. NSL: </author> <title> Neural Simulation Language. </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 4. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) <ref> [104] </ref>, SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines. <p> A number of authors have worked in the area of developing concise languages for describing ANN architectures. These languages can then be used to create simulation environments [10] for both sequential as well as parallel machines. One effort in this regard was the Neuron Simulation Language <ref> [104] </ref> that can be used to describe single neurons, as well as networks of neurons. Another concise language (MDL) for ANN description is described in [91]. MDL allows the specification of a large network in just a couple of pages.
Reference: [105] <author> M. Wilson, U. Bhalla, J. Uhley, and J. Bower. </author> <title> GENESIS: A System for Simulating Neural Networks. </title> <editor> In D.S.Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year> <booktitle> Neural Computing Surveys vol 1, </booktitle> <pages> 48-60, </pages> <year> 1996, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 60 </note>
Reference-contexts: One area of effort not covered in this paper is that of simulation environments that attempt to simulate the operation of a single neuron in great detail (see for example <ref> [105] </ref>). These simulators model the individual neuron, and are therefore of great interest to neurobiologists.
Reference: [106] <author> S. S. Wilson. </author> <title> Neural computing on a one dimensional SIMD array. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 206-211, </pages> <year> 1989. </year>
Reference-contexts: They present theoretical and experimental results to show that their technique performs quite well on the nCUBE and the CM-5. Other implementations on SIMD machines include Ranka et al [82] and Wilson <ref> [106] </ref>. Deprit [11] presents results of implementing the Recurrent Backpropagation algorithm on the CM-2. The two algorithms used in this work are in turn based on [83] and [94, 95]. Grajski et al. discuss ANN implementations on the MasPar [30, 31].
Reference: [107] <author> C.-H. Wu, C.-J. Wang, and S. Sivasundaram. </author> <title> Neural network simulation on shared-memory vector multiprocessors. </title> <booktitle> Proceedings of Supercomputing 89, </booktitle> <address> Reno NV., </address> <pages> pages 197-203, </pages> <year> 1989. </year>
Reference-contexts: Barbosa and Lima [4] present an Occam implementation of Hopfield Nets on distributed memory architectures. Fortuna et al [18] describe a simulator for cellular neural networks called PSIMCNN that is implemented on transputer based machines. An analysis of ANN implementations on transputers is presented in [67]. Wang and Wu <ref> [102, 107] </ref> show how to simulate five different NN models (ART1, ART2, feed-forward networks, recurrent networks, and Hopfield nets) on a shared memory vector multiprocessor (the Alliant FX/80). El-Amawy and Kulasinghe [14] present a method to implement feed-forward networks on an architecture called Multiple Bus System (MBS).
Reference: [108] <author> A. Zell. </author> <title> Simulation Neuronaler Netze. </title> <publisher> Addison Wesley, </publisher> <address> Germany, </address> <year> 1994. </year> <note> Only available in German. </note>
Reference-contexts: A version of SNNS that uses training pattern parallelism has also been developed for the Intel Paragon (an MIMD distributed memory machine). A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and <ref> [108] </ref>. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS [112], RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines.
Reference: [109] <author> A. Zell et al. </author> <note> SNNS Manual. SNNS can be retrieved from ftp.informatik.uni-stuttgart.de from /pub/SNNS. </note>
Reference-contexts: Another excellent public domain sequential simulator has been developed at the University of Stuttgart. This simulator is called the Stuttgart Neural Network Simulator or SNNS <ref> [109, 110, 111] </ref>. Some ideas in this simulator were inspired by another sequential simulator, the Rochester Connectionist Simulator, RCS [26]. SNNS comes with an extensive and well written user's manual that makes it easy for a user to use and modify the software.
Reference: [110] <author> A. Zell, T. Korb, N. Mache, and T. Sommer. </author> <title> Recent developments of the SNNS Neural Network Simulator. </title> <booktitle> In Proceedings of the Applications of Neural Networks Conference, SPIE, </booktitle> <volume> volume 1294, </volume> <year> 1991. </year>
Reference-contexts: Another excellent public domain sequential simulator has been developed at the University of Stuttgart. This simulator is called the Stuttgart Neural Network Simulator or SNNS <ref> [109, 110, 111] </ref>. Some ideas in this simulator were inspired by another sequential simulator, the Rochester Connectionist Simulator, RCS [26]. SNNS comes with an extensive and well written user's manual that makes it easy for a user to use and modify the software.
Reference: [111] <author> A. Zell, T. Korb, T. Sommer, and R. Bayer. </author> <title> A neural network simulation environment. </title> <booktitle> In Proceedings of the Applications of Neural Networks Conference, SPIE, </booktitle> <volume> volume 1294, </volume> <pages> pages 534-544, </pages> <year> 1990. </year>
Reference-contexts: Another excellent public domain sequential simulator has been developed at the University of Stuttgart. This simulator is called the Stuttgart Neural Network Simulator or SNNS <ref> [109, 110, 111] </ref>. Some ideas in this simulator were inspired by another sequential simulator, the Rochester Connectionist Simulator, RCS [26]. SNNS comes with an extensive and well written user's manual that makes it easy for a user to use and modify the software.
Reference: [112] <author> A. Zell, N. Mache, R. Hubner, G. Mamier, M. Vogt, M. Schmalzl, and K.-U. Herrmann. </author> <title> SNNS (Stuttgart Neural Network Simulator). </title> <editor> In J. Skrzypek, editor, </editor> <title> Neural Network Simulation Environments, chapter 9. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: A description of a number of sequential (and some parallel) ANN simulation environments is presented in [90] and [108]. These include the UCLA-SFINX [53] environment, Nexus [85], SWIM [13], NSL (Neuron Simulation Language) [104], SNNS <ref> [112] </ref>, RCS [25], and Asprin/Migraines [46]. The book by J. Skrzypek therefore forms a good starting point for anyone interested in learning about software environments for simulating neural networks on sequential machines.
Reference: [113] <author> A. Zell, N. Mache, M. Vogt, and M. Huettel. </author> <title> Problems of massive parallelism in neural network simulation. </title> <booktitle> In Proceedings of the IEEE Int. Conf. on Neural Networks, </booktitle> <address> San Francisco, CA, </address> <booktitle> volume III, </booktitle> <pages> pages 1890-1895. </pages> <publisher> IEEE Press, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: Unlike the new version of the Neural Computing Surveys vol 1, 48-60, 1996, http://www.icsi.berkeley.edu/~jagota/NCS 50 PDP simulator, SNNS has a single kernel and different models are handled as switches in the same program. A parallel version of the SNNS program has been developed for the MasPar <ref> [113] </ref>. SNNS provides an option to train multiple copies of the same ANN, or different ANNs simultaneously on a number of networked workstations. A version of SNNS that uses training pattern parallelism has also been developed for the Intel Paragon (an MIMD distributed memory machine).
References-found: 113

