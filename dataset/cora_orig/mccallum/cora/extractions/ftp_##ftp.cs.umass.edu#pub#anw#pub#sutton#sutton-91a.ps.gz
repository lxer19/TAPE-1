URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-91a.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: sutton@gte.com  
Title: Planning by Incremental Dynamic Programming  
Author: Richard S. Sutton 
Address: Waltham, MA 02254  
Affiliation: GTE Laboratories Incorporated  
Date: 353-357, 1991 Morgan-Kaufmann  
Note: Appeared in Proceedings Ninth Conference on Machine Learning,  
Abstract: This paper presents the basic results and ideas of dynamic programming as they relate most directly to the concerns of planning in AI. These form the theoretical basis for the incremental planning methods used in the integrated architecture Dyna. These incremental planning methods are based on continually updating an evaluation function and the situation-action mapping of a reactive system. Actions are generated by the reactive system and thus involve minimal delay, while the incremental planning process guarantees that the actions and evaluation function will eventually be optimal|no matter how extensive a search is required. These methods are well suited to stochastic tasks and to tasks in which a complete and accurate model is not available. For tasks too large to implement the situation-action mapping as a table, supervised-learning methods must be used, and their capabilities remain a significant limitation of the approach.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. </author> <title> (1988) The Dynamic Structure of Everyday Life. </title> <type> PhD thesis, MIT Technical Report AI-TR-1085. </type>
Reference: <author> Agre, P. E., & Chapman, D. </author> <year> (1987) </year> <month> Pengi: </month> <title> An implementation of a theory of activity. </title> <booktitle> Proceedings of AAAI-87, </booktitle> <pages> 268-272. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <title> (1990a) Learning and sequential decision making. In Learning and Computational Neuroscience, </title> <editor> M. Gabriel and J.W. Moore (Eds.), </editor> <booktitle> pp. </booktitle> <pages> 539-602, </pages> <publisher> MIT Press. </publisher>
Reference: <author> Bellman, R. E. </author> <title> (1957) Dynamic Programming. </title> <publisher> Prince-ton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bertsekas, D. P. </author> <title> (1987) Dynamic Programming: Deterministic and Stochastic Models, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This approach to planning can be viewed as an appli-cation and extension of dynamic programming (DP). DP is a broad class of techniques developed in operations research for solving sequential decision problems such as routing and scheduling <ref> (e.g., Bertsekas, 1987) </ref>. These methods are in many ways very similar to AI search methods, but they differ in that they allocate memory to the entire state space instead of only to a tree of possibilities starting at the current state.
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <title> (1989) Parallel Distributed Processing: Numerical Methods, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Furthermore, for the tabular case, it has been proven that ^ V converges to V given only that all states continue to be updated by (5) <ref> (Bertsekas & Tsitsiklis, 1989) </ref>. Following Bertsekas and Tsitsiklis, we call this way of computing V asynchronous value iteration (AVI), because it is essentially an asynchronous version of a classic dynamic programming method known as value iteration.
Reference: <author> Dean, T., & Boddy, M. </author> <title> (1988) An analysis of time-dependent planning. </title> <booktitle> Proceedings AAAI-88, </booktitle> <pages> 49-54. </pages>
Reference-contexts: Other work has attempted to take an intermediate ground and manage the tradeoff flexibly at runtime <ref> (e.g., Dean & Boddy, 1988) </ref> The Dyna integrated architecture (Sutton, 1990) combines reactive systems and planning in a simple way. Actions are generated by a reactive system, but the mapping implemented by that system is continually adjusted by a planning process, as suggested by Figure 1c.
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, Volume II, </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Korf, R. E. </author> <title> (1990) Real-Time Heuristic Search. </title> <booktitle> Artificial Intelligence 42: </booktitle> <pages> 189-211. </pages>
Reference: <author> Laird, J. E., & Rosenbloom, P. S. </author> <title> (1990) Integrating planning, execution, and learning in Soar for external environments. </title> <booktitle> AAAI-90, </booktitle> <pages> 1022-1029. </pages>
Reference: <author> Lin, L.-J. </author> <title> (1991) Self-improving reactive agents: Case studies of reinforcement learning frameworks. </title> <booktitle> In: Proceedings of the International Conference on the Simulation of Adaptive Behavior, </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Mitchell, T. M. </author> <title> (1990) Becoming increasingly reactive. </title> <booktitle> AAAI-90, </booktitle> <pages> 1051-1058. </pages>
Reference: <author> Moore, A. W. </author> <title> (1990) Efficient Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Cambridge University Computer Science Department. </institution>
Reference: <author> Riolo, R. </author> <title> (1991) Lookahead planning and latent learning in a classifier system. </title> <booktitle> In: Proceedings of the International Conference on the Simulation of Adaptive Behavior, </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Rosenschein, S. J., & Kaelbling, L. P. </author> <title> (1986) The synthesis of machines with provable epistemic properties. </title> <booktitle> Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <pages> 83-98, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Russell, S. J. </author> <title> (1989) Execution architectures and compilation. </title> <booktitle> Proceedings of IJCAI-89, </booktitle> <pages> 15-20. </pages>
Reference-contexts: First, we generalize the idea of per-transition costs, to per-transition outcomes, called rewards. Rewards can be either desirable (positive) or undesirable (negative), where the latter are essentially the same as costs. The agent's objective is to choose actions so as to maximize the total reward received from the world <ref> (cf. Russell, 1989) </ref>. For example, a robot meant to find and collect soda cans might be given a positive reward for depositing a can in the recycling bin, a negative reward for colliding with anything or if its battery runs too low, and zero reward at all other times.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3, </volume> <pages> 210-229. </pages> <note> Reprinted in E. </note> <editor> A. Feigenbaum, & J. Feldman (Eds.), </editor> <booktitle> Computers and thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Schoppers, M. J. </author> <title> (1987) Universal plans for reactive robots in unpredictable domains. </title> <booktitle> Proceedings of IJCAI-87. </booktitle>
Reference: <author> Sutton, </author> <title> R.S. (1988) Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3: </booktitle> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <title> (1990) Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference-contexts: Other work has attempted to take an intermediate ground and manage the tradeoff flexibly at runtime (e.g., Dean & Boddy, 1988) The Dyna integrated architecture <ref> (Sutton, 1990) </ref> combines reactive systems and planning in a simple way. Actions are generated by a reactive system, but the mapping implemented by that system is continually adjusted by a planning process, as suggested by Figure 1c.
Reference: <author> Watkins, C. J. C. H. </author> <title> (1989) Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University Psychology Department. </institution>
Reference: <author> Werbos, P. J. </author> <title> (1987) Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <month> Jan-Feb. </month>
Reference: <author> Whitehead, S. D., Ballard, D.H. </author> <title> (1990) Active perception and reinforcement learning. </title> <booktitle> In: Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> 354-357, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Whitehead, S. D., Ballard, D.H. </author> <title> (in press) Learning to perceive and act by trial and error. </title> <booktitle> Machine Learning. </booktitle>
References-found: 24

