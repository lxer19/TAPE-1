URL: http://www.cs.washington.edu/homes/anderson/papers/jub.ps
Refering-URL: http://www.cs.washington.edu/homes/anderson/papers.html
Root-URL: 
Title: Computer Science Problems in Astrophysical Simulation  
Author: Richard Anderson 
Abstract: This paper presents a survey of current work on N-body simulation in astrophysics. The goals of the paper are to present several computer science problems that arise in N-body simulation, and to show how cross disciplinary collaboration can enrich computer science. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Hartmanis and Herbert Lin, </author> <title> editors. Computing the Future. </title> <publisher> National Academy Press, </publisher> <address> Washington, DC, </address> <year> 1992. </year>
Reference-contexts: First and foremost there is the intellectual argument that this collaboration will be mutually beneficial and will advance all of the involved disciplines. This argument is being formally made by organizations such as the National Research Council <ref> [1] </ref>. Currently, at least in the United States, funding opportunities such as the HPCC Initiative are promoting collaborative ventures.
Reference: [2] <author> W. Benz, R. L. Bowers, A. G. W. Cameron, and W. H. </author> <title> Press. Dynamic mass exchange in doubly degenerate binaries. I. 0.9 and 1.2 M fi stars. </title> <journal> The Astrophysical Journal, </journal> <volume> 348 </volume> <pages> 647-667, </pages> <year> 1990. </year>
Reference-contexts: Examples of smaller systems which are studied by simulation include the collision of galaxies and the mass exchange between orbiting white dwarf stars <ref> [2] </ref>. One of the key technical challenges in simulation arises from the necessity of using a large number of particles. The largest reported simulations (as of June, 1992), involve following the evolution of 17 million particles over 600 time steps [3]. <p> The bottom up up approach aims at grouping together points in a way that reflects the geometry of the particles. Independently, Benz et al. <ref> [2] </ref> and Jernigan and Porter [11] gave schemes where close together points are combined to form clusters. Although these data structures are much less understood than the top down approaches, they appear to perform well in practice [12]. <p> Can the theory of geometric separators [23] be used to build better trees? 7.2 Nearest Neighbor Trees An alternative to the top down approach is to build the tree bottom up by combining close together particles. This method was independently proposed by Benz et al. <ref> [2] </ref> and by Jernigan and Porter [11]. This method appears to be competitive with the top down approaches [12]. We define a nearest neighbor tree to be a tree that is formed by repeatedly collapsing mutually nearest neighbors 6 until a single point is left.
Reference: [3] <author> J. K. Salmon and M. S. Warren. </author> <title> Astrophysical n-body simulations using hierarchical tree data structures. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 570-576, </pages> <year> 1992. </year>
Reference-contexts: One of the key technical challenges in simulation arises from the necessity of using a large number of particles. The largest reported simulations (as of June, 1992), involve following the evolution of 17 million particles over 600 time steps <ref> [3] </ref>. The simulations were performed by Salmon and Warren using the 512 processor Intel Touchstone Delta (i860). The simulations ran in roughly 24 hours. These simulations were very large scale simulations where each particle represented over 10 10 solar masses, and each time step corresponded to over 10 million years. <p> The control structure of the N-body algorithms, made it a substantial challenge to get good performance on machines such as Crays, but eventually vectorized codes were developed. Currently, the main interest in implementation is achieving high performance on large parallel machines. The largest simulations that have been run <ref> [3] </ref> utilized the 512 processors Intel Touchstone Delta. The N-body problem potentially has a large amount of work that can be done independently and in parallel, especially when the number of particles is much larger than the number of available processors.
Reference: [4] <author> R. Carlberg, </author> <year> 1993. </year> <type> Personal Communication. </type>
Reference-contexts: These simulations were very large scale simulations where each particle represented over 10 10 solar masses, and each time step corresponded to over 10 million years. Smaller "routine" simulations run by astrophysicists may involve one million particles and run for one month on a fast workstation <ref> [4] </ref>. The reason that astrophysical simulations require such a large number of particles to achieve accurate results is that the problems involved have a very large dynamic range. This means that interesting events simultaneously occur on many different length scales.
Reference: [5] <author> E. Holmberg. </author> <title> On the clustering tendencies among the nebulae. II. a study of encounters between laboratory models of stellar systems by a new integration procedure. </title> <journal> The Astrophysical Journal, </journal> <volume> 94(3) </volume> <pages> 385-395, </pages> <year> 1941. </year>
Reference-contexts: Currently, a set of methods, referred to as tree-codes are the most popular for large scale simulations. These were independently discovered and developed by a number of researchers. 4.1 Analog algorithm The history of N-body algorithms dates back to a truly remarkable paper published in 1941 <ref> [5] </ref>. In this paper, Erik Holmberg, a Swedish physicist described an analog method for N-body simulation. His idea was to replace gravitational force with light, since both light and gravity obey an inverse square law.
Reference: [6] <author> R. W. Hockney and J. W. Eastwood. </author> <title> Computer Simulation Using Particles. </title> <editor> Adam Hilger, </editor> <year> 1988. </year>
Reference-contexts: The paper also discussed how errors such as reflected light from the table surface were measured and dealt with. 4.2 Mesh Algorithms The first computational method which gave a substantial improvement over direct computation was to use a mesh <ref> [6] </ref>. In this method, each particle is moved to the closest grid point. The force computation becomes a convolution which can be done with a FFT. This reduces the complexity of the computation to M log M where M is the number of grid points used.
Reference: [7] <author> J. E. Barnes and P. Hut. </author> <title> A hierarchical o(n log n) force-calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: Imple--mentations of this method are often referred to as tree-codes. The method was independently discovered by several researchers. The particular version that is most commonly used is the Barnes-Hut algorithm <ref> [7] </ref>. The starting point for the tree codes is to use a natural approximation in computing the force. Suppose that we want to compute the force exserted by a set of particles S = fp 1 ; : : : ; p k g on a particle x.
Reference: [8] <author> J. K. Salmon and M. S. Warren. </author> <title> Skeletons from the treecode closet. </title> <type> Technical report, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Salmon and Warren <ref> [8] </ref> consider other choices including methods which take into account the magnitude of the errors in using various approximations. Data Structures There is a tremendous flexibility in the choice of spatial data structure that could be used in the algorithm. <p> to run large simulations, physicists will continue to run simulations outside the range where error analysis gives reasonable bounds on the accuracy. 4 Galaxies would disappear in under one billion years of simulated time, while real galaxies have existed for ten billion years. 7 5.3 Exploding galaxies Salmon and Warren <ref> [8, 16] </ref> have documented a systematic error arising in the Barnes-Hut algorithm. The problem arises when a small galaxy collides with a larger galaxy. As the smaller galaxy approaches, it loses self gravitation and falls apart.
Reference: [9] <author> A. W. Appel. </author> <title> An efficient program for many-body simulation. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 6 </volume> <pages> 85-103, </pages> <year> 1985. </year>
Reference-contexts: The oct-tree is constructed by recursively subdividing the cubes into eight subcubes, splitting at the geometrically central point. The subdivi 4 sion continues until cubes contain fewer than two particles. Other data structures, including k d trees, used by Appel <ref> [9] </ref>, and fair-split trees, proposed by Callahan and Kosaraju [10], choose separating planes based upon the point set, where the subdivision does not necessarily create equal sized regions. The bottom up up approach aims at grouping together points in a way that reflects the geometry of the particles. <p> The large branching factor of the tree and the cubic dependence on cause the constant to be so large. 5 4.4 Fast Multipole It is possible to gain a theoretical improvement in N-body algorithms by allowing cluster-cluster approximations instead of just particle-cluster operations. Appel <ref> [9] </ref> introduced the use of cluster-cluster operations, and Greengard and Rokhlin [13] showed how the operations could be used to achieve high accuracy in linear time. <p> This can be achieved if processor allocation does not change dramatically between time steps. Another use of the correlation between time steps is to use an incremental data structure for the spatial decomposition tree instead of rebuilding from scratch. The implementation of Appel <ref> [9] </ref> and Jernigan and Porter [11] use incremental data structures. In some cases, such as oct-trees, the data structure construction is so efficient that it is not necessary to use an incremental version.
Reference: [10] <author> P. B. Callahan and S. R. Kosaraju. </author> <title> A decomposition of multi-dimensional point-sets with applications to k-nearest-neighbors and n-body potential fields. </title> <booktitle> In Proceedings of the 24th ACM Symposium on Theory of Computation, </booktitle> <pages> pages 546-555, </pages> <year> 1992. </year>
Reference-contexts: The oct-tree is constructed by recursively subdividing the cubes into eight subcubes, splitting at the geometrically central point. The subdivi 4 sion continues until cubes contain fewer than two particles. Other data structures, including k d trees, used by Appel [9], and fair-split trees, proposed by Callahan and Kosaraju <ref> [10] </ref>, choose separating planes based upon the point set, where the subdivision does not necessarily create equal sized regions. The bottom up up approach aims at grouping together points in a way that reflects the geometry of the particles. <p> Constructing nearest neighbor trees We have not considered the problem of how to build the nearest neighbor tree. Since the problem of finding a nearest neighbor pair in a point set can be solved in O (n log n) time <ref> [24, 25, 10] </ref>, there is an O (n 2 log n) algorithm for the problem. It is very likely that the result can be improved to O (n log n) time. The parallel complexity is also of interest.
Reference: [11] <author> J. G. Jernigan and D. H. Porter. </author> <title> A tree code with logarithmic reduction of force terms, hierarchical regularization of all variables and explicit accuracy controls. </title> <journal> The Astrophysical Journal Supplement, </journal> <volume> 71:871, </volume> <year> 1989. </year>
Reference-contexts: The bottom up up approach aims at grouping together points in a way that reflects the geometry of the particles. Independently, Benz et al. [2] and Jernigan and Porter <ref> [11] </ref> gave schemes where close together points are combined to form clusters. Although these data structures are much less understood than the top down approaches, they appear to perform well in practice [12]. <p> This method was independently proposed by Benz et al. [2] and by Jernigan and Porter <ref> [11] </ref>. This method appears to be competitive with the top down approaches [12]. We define a nearest neighbor tree to be a tree that is formed by repeatedly collapsing mutually nearest neighbors 6 until a single point is left. This naturally gives a binary tree. <p> Argument 1: If points are randomly distributed than a constant fraction of the points will be involved in mutual nearest neighbor pairs. These can be collapsed, reducing the number of pairs by a constant fraction, leading to logarithmic depth. <ref> [11] </ref> Argument 2: Let ffi denote the distance from p to its nearest neighbor. Suppose p is merged with all points within a radius of 2ffi. The nearest neighbor distance of the resulting point will be a least 2ffi. <p> This can be achieved if processor allocation does not change dramatically between time steps. Another use of the correlation between time steps is to use an incremental data structure for the spatial decomposition tree instead of rebuilding from scratch. The implementation of Appel [9] and Jernigan and Porter <ref> [11] </ref> use incremental data structures. In some cases, such as oct-trees, the data structure construction is so efficient that it is not necessary to use an incremental version.
Reference: [12] <author> J. Makino. </author> <title> Comparison of two different tree algorithms. </title> <journal> The Journal of Computational Physics, </journal> <volume> 88:393, </volume> <year> 1990. </year>
Reference-contexts: Independently, Benz et al. [2] and Jernigan and Porter [11] gave schemes where close together points are combined to form clusters. Although these data structures are much less understood than the top down approaches, they appear to perform well in practice <ref> [12] </ref>. Performance Although the performance of the particle-cluster algorithms is generally characterized as O (n log n), the actual run time does depend upon the distribution of the points. <p> The errors are not independent, and there is also conditioning between time steps, so a central limit theorem approximation is not valid. It is probably not possible to formally derive a distribution on the errors, although they have been observed to satisfy reasonable statistical properties <ref> [12] </ref>. A worst case analysis, which assumes a conspiracy of errors, yields an overly pessimistic bounded. <p> This method was independently proposed by Benz et al. [2] and by Jernigan and Porter [11]. This method appears to be competitive with the top down approaches <ref> [12] </ref>. We define a nearest neighbor tree to be a tree that is formed by repeatedly collapsing mutually nearest neighbors 6 until a single point is left. This naturally gives a binary tree. We give an algorithmic definition, which gives substantial flexibility in how the tree is constructed.
Reference: [13] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference-contexts: Appel [9] introduced the use of cluster-cluster operations, and Greengard and Rokhlin <ref> [13] </ref> showed how the operations could be used to achieve high accuracy in linear time. The idea for cluster-cluster evaluations is that if there are point sets A and B, which are well-separated, then a single force computation can be done between the sets.
Reference: [14] <author> L. Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1987. </year> <month> 13 </month>
Reference-contexts: In the three dimensional algorithm, to evaluate a cell of size D, all D cells in a cube of diameter 9D must be examined, which is 729 cells <ref> [14] </ref>. The force evaluation is also very expensive. In two dimensions, the coordinates can be represented with complex numbers, which greatly simplifies the computation of the functions. In three dimensions, a different technique, such as spherical harmonics is necessary [15].
Reference: [15] <author> F. Zhao. </author> <title> An O(n) algorithm for three--dimensional n-body simulations. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: The force evaluation is also very expensive. In two dimensions, the coordinates can be represented with complex numbers, which greatly simplifies the computation of the functions. In three dimensions, a different technique, such as spherical harmonics is necessary <ref> [15] </ref>. The fast multipole method is well suited for the case where high accuracy is required. Although it has large constant factors, additional accuracy is available at relatively low cost.
Reference: [16] <author> J. K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Both of these problems have been addressed in a number of implementations, and good performance has been achieved on several different machines. Machine utilization of over 80% has been reported on hypercubes <ref> [16] </ref>, and on the Stanford Dash [17]. 6 4.6 What is left to be done? Physicists desire to perform simulations which are orders of magnitude larger than what is feasible today. <p> to run large simulations, physicists will continue to run simulations outside the range where error analysis gives reasonable bounds on the accuracy. 4 Galaxies would disappear in under one billion years of simulated time, while real galaxies have existed for ten billion years. 7 5.3 Exploding galaxies Salmon and Warren <ref> [8, 16] </ref> have documented a systematic error arising in the Barnes-Hut algorithm. The problem arises when a small galaxy collides with a larger galaxy. As the smaller galaxy approaches, it loses self gravitation and falls apart. <p> In current algorithms, almost all of the work is done in actual force computation with construction of the tree data structure being only a small amount of the work. (Salmon <ref> [16] </ref> gives the cost of tree construction as only 2% of the sequential run time.) The load balancing problem is to assign the particles to the processors, so that each processor performs approximately the same amount of work. <p> Static schemes that have been used for the N-body problem include a top down method, Orthogonal Recursive Bisection <ref> [16] </ref> and a bottom up method, Costzones [19]. One idea that works well in load balancing is to keep track of the amount of work per particle done in each time step, and use it as an estimate of the work when load balancing is done. <p> The most difficult problem is to ensure that each processor has access to all necessary parts of the tree. This is a very difficult implementation problem. For example, the solution to this problem was the most complicated part of the Salmon implementation <ref> [16] </ref>. The problem is easily solved on a shared memory machine [17]. This specific example is used by proponents of shared memory in the on going discussion of the relative merits of shared memory versus message passing.
Reference: [17] <author> J. P. Singh. </author> <title> Parallel Hierarchical N-body Methods and Their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year> <note> Computer Systems Laboratory Technical Report CSL-TR-93-565. </note>
Reference-contexts: Both of these problems have been addressed in a number of implementations, and good performance has been achieved on several different machines. Machine utilization of over 80% has been reported on hypercubes [16], and on the Stanford Dash <ref> [17] </ref>. 6 4.6 What is left to be done? Physicists desire to perform simulations which are orders of magnitude larger than what is feasible today. Today's simulations are limited both in terms of total processing power (the speed and number of processors), as well as by the available memory. <p> This is a very difficult implementation problem. For example, the solution to this problem was the most complicated part of the Salmon implementation [16]. The problem is easily solved on a shared memory machine <ref> [17] </ref>. This specific example is used by proponents of shared memory in the on going discussion of the relative merits of shared memory versus message passing. A major step in developing abstractions for tree codes on message passing machines was taken by Bhatt et al. [20].
Reference: [18] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: Researchers are seeking general solutions to many problems in high performance computing. The N-body problem has become a popular application to study <ref> [18] </ref> since it has a less regular structure than many other scientific applications, and hence requires more general solution techniques. (One concern about the computer science work that uses the N-body problem as a benchmark is that it is concentrating on the Barnes-Hut algorithm.
Reference: [19] <author> J. P. Singh, C. Holt, T. Totsuka, A. Gupta, and J. L Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <year> 1992. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: Static schemes that have been used for the N-body problem include a top down method, Orthogonal Recursive Bisection [16] and a bottom up method, Costzones <ref> [19] </ref>. One idea that works well in load balancing is to keep track of the amount of work per particle done in each time step, and use it as an estimate of the work when load balancing is done.
Reference: [20] <author> S. Bhatt, M. Chen, C-Y. Lin, and P. Liu. </author> <title> Abstractions for parallel n-body simulations. </title> <booktitle> In Supercomputing, </booktitle> <year> 1992. </year> <note> Reference needs to be verified. </note>
Reference-contexts: This specific example is used by proponents of shared memory in the on going discussion of the relative merits of shared memory versus message passing. A major step in developing abstractions for tree codes on message passing machines was taken by Bhatt et al. <ref> [20] </ref>. They propose a pair of abstractions, Traverse and Deliver, which lead to a concise implementation of the Barnes-Hut algorithm. The key mechanism is to define a tree traversal which prompts the delivery of messages to the originating processors.
Reference: [21] <author> Calvin Lin and Lawrence Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> International Journal of Parallel Processing, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year>
Reference-contexts: It is essential that methods for porting programs pay very close attention to the efficiency of the resulting program. There are several ongoing efforts, such as the Orca Project at University of Washington <ref> [21, 22] </ref> which are developing systems which apply to various classes of scientific applications.
Reference: [22] <author> Lawrence Snyder. </author> <booktitle> Foundations of practical parallel programming languages. In Proceedings of the Second International Conference of the Austrian Center for Parallel Computation. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: It is essential that methods for porting programs pay very close attention to the efficiency of the resulting program. There are several ongoing efforts, such as the Orca Project at University of Washington <ref> [21, 22] </ref> which are developing systems which apply to various classes of scientific applications.
Reference: [23] <author> G. L. Miller and W. Thurston. </author> <title> Separators in two or three dimensions. </title> <booktitle> In Proceedings of the 22nd ACM Symposium on Theory of Computation, </booktitle> <pages> pages 300-307, </pages> <year> 1990. </year>
Reference-contexts: Is the high branching factor a detriment to the oct-tree based algorithm? Can a binary tree lead to fewer comparisons? 6. Can the theory of geometric separators <ref> [23] </ref> be used to build better trees? 7.2 Nearest Neighbor Trees An alternative to the top down approach is to build the tree bottom up by combining close together particles. This method was independently proposed by Benz et al. [2] and by Jernigan and Porter [11].
Reference: [24] <author> K. Clarkson. </author> <title> Fast algorithms for the all-nearest-neighbors problem. </title> <booktitle> In 24th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 226-232, </pages> <year> 1983. </year>
Reference-contexts: Constructing nearest neighbor trees We have not considered the problem of how to build the nearest neighbor tree. Since the problem of finding a nearest neighbor pair in a point set can be solved in O (n log n) time <ref> [24, 25, 10] </ref>, there is an O (n 2 log n) algorithm for the problem. It is very likely that the result can be improved to O (n log n) time. The parallel complexity is also of interest.
Reference: [25] <author> P. M. Vaidya. </author> <title> An optimal algorithm for the all-nearest-neighbors problem. </title> <booktitle> In 27th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 117-122, </pages> <year> 1986. </year>
Reference-contexts: Constructing nearest neighbor trees We have not considered the problem of how to build the nearest neighbor tree. Since the problem of finding a nearest neighbor pair in a point set can be solved in O (n log n) time <ref> [24, 25, 10] </ref>, there is an O (n 2 log n) algorithm for the problem. It is very likely that the result can be improved to O (n log n) time. The parallel complexity is also of interest.
Reference: [26] <author> P. B. Callahan. </author> <title> Optimal parallel all-nearest-neighbors using the well-separated pair decomposition. </title> <booktitle> In 34th Symposium on Foundations of Computer Science, </booktitle> <year> 1993. </year> <month> 14 </month>
Reference-contexts: It is very likely that the result can be improved to O (n log n) time. The parallel complexity is also of interest. There is not an obvious NC algorithm for the problem, even if the tree height is logarithmic. Since there are optimal bounds for nearest neighbors <ref> [26] </ref>, it is likely that highly efficient parallel algorithms exist for constructing nearest neighbor trees. 7.3 Simulation Algorithms The work on simulation has generally concentrated on getting a good solution to the N-body problem.
References-found: 26

