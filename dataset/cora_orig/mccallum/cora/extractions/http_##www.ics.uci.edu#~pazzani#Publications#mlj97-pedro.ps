URL: http://www.ics.uci.edu/~pazzani/Publications/mlj97-pedro.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pedrod@ics.uci.edu  pazzani@ics.uci.edu  
Title: On the Optimality of the Simple Bayesian Classifier under Zero-One Loss  
Author: PEDRO DOMINGOS MICHAEL PAZZANI Editor: Gregory Provan 
Keyword: Simple Bayesian classifier, naive Bayesian classifier, zero-one loss, optimal classification, induction with attribute dependences  
Address: Irvine, CA 92697  
Affiliation: Department of Information and Computer Science, University of California,  
Note: 1-30 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ben-Bassat, M., Klove, K. L., & Weil, M. H. </author> <year> (1980). </year> <title> Sensitivity analysis in Bayesian classification models: Multiplicative deviations. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2, </volume> <pages> 261-266. </pages>
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bias, </title> <type> variance and arcing classifiers (Technical Report 460). </type> <institution> Statistics Department, University of California at Berkeley, Berkeley, </institution> <address> CA. ftp://ftp.stat.berkeley.edu/- users/breiman/arcall.ps.Z. </address>
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> Proceedings of the Ninth European Conference on Artificial Intelligence. </booktitle> <address> Stockholm, Sweden: </address> <publisher> Pitman. </publisher>
Reference: <author> Clark, P., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8, Quinlan, 1993), instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction <ref> (CN2 version 6.1, Clark & Boswell, 1991) </ref>. A simple Bayesian classifier was implemented for these experiments. Three main issues arise here: how to handle numeric attributes, zero counts, and missing values.
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference-contexts: The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8, Quinlan, 1993), instance-based learning <ref> (PEBLS 2.1, Cost & Salzberg, 1993) </ref> and rule induction (CN2 version 6.1, Clark & Boswell, 1991). A simple Bayesian classifier was implemented for these experiments. Three main issues arise here: how to handle numeric attributes, zero counts, and missing values.
Reference: <editor> DeGroot, M. H. </editor> <booktitle> (1986). Probability and statistics (2nd ed.). </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The fourth line shows the confidence levels obtained by applying the more sensitive Wilcoxon test <ref> (DeGroot, 1986) </ref> to the 28 average accuracy differences obtained, and results in high confidence that the Bayesian classifier is more accurate than each of the other learners. The fifth line shows the average accuracy across all data sets, and again the Bayesian classifier performs the best.
Reference: <author> Dietterich, T. </author> <year> (1996). </year> <title> Statistical tests for comparing supervised classification learning algorithms (technical report). </title> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR. ftp://ftp.cs.orst.edu/pub/tgd/papers/stats.ps.gz. </address>
Reference: <author> Dougherty, J., Kohavi, R., & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 194-202). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: The classifier takes an unlabeled example and assigns it to a class. Many classifiers can be viewed as computing a set of discriminant functions of the example, one for each class, and assigning the example to the class whose function is maximum <ref> (Duda & Hart, 1973) </ref>. <p> If P (C i jE) is the probability that example E is of class C i , zero-one loss is minimized if, and only if, E is assigned to the class C k for which P (C k jE) is maximum <ref> (Duda & Hart, 1973) </ref>. In other words, using P (C i jE) as the discriminant functions f i (E) is the optimal classification procedure. <p> The simple Bayesian classifier is limited in expressiveness in that it can only create linear frontiers <ref> (Duda & Hart, 1973) </ref>. Therefore, even with many training examples and no noise, it does not approach 100% accuracy on some problems. Langley (1993) proposed the use of "recursive Bayesian classifiers" to address this limitation. <p> P (C X jE) is the accuracy of X on E. This definition reduces to Equation 6 when one class has probability 1 given E. Definition 2 The Bayes rate for an example is the lowest zero-one loss achievable by any classifier on that example <ref> (Duda & Hart, 1973) </ref>. Definition 3 A classifier is locally optimal for a given example iff its zero-one loss on that example is equal to the Bayes rate. <p> Then the following result is an immediate extension to the general nominal case of a well-known one for Boolean attributes <ref> (Duda & Hart, 1973) </ref>. Theorem 4 When all attributes are nominal, the Bayesian classifier is not globally optimal for classes that are not discriminable by linear functions of the corresponding features. <p> Then, by taking the logarithm of Equation 2, the Bayesian classifier is equivalent to a linear machine <ref> (Duda & Hart, 1973) </ref> whose discriminant function for class C i is log P (C i ) + j;k log P (A j = v jk jC i ) b jk (i.e., the weight of each Boolean fea ture is the log-probability of the corresponding attribute value given the class). <p> Since in nominal domains the basic Bayesian classifier cannot learn some linearly separable concepts, in these domains its range of optimality is a subset of the perceptron's, or of a linear machine's <ref> (Duda & Hart, 1973) </ref>. This leads to the following result. Let the Vapnik-Chervonenkis dimension, or VC dimension for short, be defined as in (Haussler, 1988). Corollary 2 In domains composed of a nominal attributes, the VC dimension of the simple Bayesian classifier is O (a). <p> In numeric domains, the Bayesian classifier is not restricted to linearly separable problems; for example, if classes are normally distributed, nonlinear boundaries and multiple disconnected regions can arise, and the Bayesian classifier is able to identify them <ref> (see Duda & Hart, 1973) </ref>. 6.2. Sufficient conditions In this section we establish the Bayesian classifier's optimality for some common concept classes.
Reference: <author> Flury, B., Schmid, M. J., & Narayanan, A. </author> <year> (1994). </year> <title> Error rates in quadratic discrimination with constraints on the covariance matrices. </title> <journal> Journal of Classification, </journal> <volume> 11, </volume> <pages> 101-120. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1996). </year> <title> On bias, variance, </title> <type> 0/1 loss, </type> <institution> and the curse-of-dimensionality (technical report). Department of Statistics, Stanford University, Stanford, </institution> <address> CA. ftp://playfair.stanford.- edu/pub/friedman/kdd.ps.Z. </address>
Reference-contexts: DOMINGOS AND M. PAZZANI It is well known that squared error loss can be decomposed into three additive components <ref> (Friedman, 1996) </ref>: the intrinsic error due to noise in the sample, the statistical bias (systematic component of the approximation error, or error for an infinite sample) and the variance (component of the error due to the approximation's sensitivity to the sample, or error due to the sample's finite size).
Reference: <author> Friedman, N., Geiger, D., & Goldszmidt, M. </author> <year> (1997). </year> <title> Bayesian network classifiers. </title> <booktitle> Machine Learning (this volume). </booktitle>
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <pages> 177-221. </pages>
Reference-contexts: This leads to the following result. Let the Vapnik-Chervonenkis dimension, or VC dimension for short, be defined as in <ref> (Haussler, 1988) </ref>. Corollary 2 In domains composed of a nominal attributes, the VC dimension of the simple Bayesian classifier is O (a). Proof: This result follows immediately from Theorem 4 and the fact that, given a attributes, the VC dimension of linear discriminant functions is O (a) (Haussler, 1988). <p> defined as in <ref> (Haussler, 1988) </ref>. Corollary 2 In domains composed of a nominal attributes, the VC dimension of the simple Bayesian classifier is O (a). Proof: This result follows immediately from Theorem 4 and the fact that, given a attributes, the VC dimension of linear discriminant functions is O (a) (Haussler, 1988). Thus, in nominal domains, the PAC-learning guarantees that apply to linear machines apply also to the Bayesian classifier.
Reference: <author> John, G., & Langley, P. </author> <year> (1995). </year> <title> Estimating continuous distributions in Bayesian classifiers. </title> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 338-345). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <month> CA. </month> <title> OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER 29 Kohavi, </title> <editor> R. </editor> <year> (1996). </year> <title> Scaling up the accuracy of naive-Bayes classifiers: A decision-tree hybrid. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 202-207). </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Proof: This follows directly from the definition of global optimality, and the fact that there exist m-of-n concepts for which the Bayesian classifier makes errors, even when the examples are noise-free (i.e., an example always has the same class) and the Bayes rate is therefore zero <ref> (e.g., 3-of-7, Kohavi, 1995) </ref>. 16 P. DOMINGOS AND M. PAZZANI Let P (AjC) represent the probability that an arbitrary attribute A is true given that the concept C is true, let a bar represent negation, and let all examples be equally probable.
Reference: <author> Kohavi, R., Becker, B., & Sommerfield, D. </author> <year> (1997). </year> <title> Improving simple Bayes (technical report). Data Mining and Visualization Group, </title> <institution> Silicon Graphics Inc., Mountain View, </institution> <address> CA. ftp://starry.- stanford.edu/pub/ronnyk/impSBC.ps.Z. </address>
Reference: <author> Kohavi, R., & Wolpert, D. H. </author> <year> (1996). </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 275-283). </pages> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kong, E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 313-321). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga (Ed.), </editor> <booktitle> Current Trends in Knowledge Acquisition. </booktitle> <address> Amsterdam, The Netherlands: </address> <publisher> IOS Press. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 206-219). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This section empirically tests this claim by comparing Pazzani's (1996) extension with one that differs from it solely by using the method for attribute dependence detection described in <ref> (Kononenko, 1991) </ref> and (Wan & Wong, 1989). In each case, the algorithm finds the single best pair of attributes to join by considering all possible joins. Two measures for determining the best pair were compared.
Reference: <author> Kubat, M., Flotzinger, D., & Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering patterns in EEG-Signals: Comparative study of a few methods. </title> <booktitle> Proceedings of the Eighth European Conference on Machine Learning (pp. </booktitle> <pages> 366-371). </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> Proceedings of the Eighth European Conference on Machine Learning (pp. </booktitle> <pages> 153-164). </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P., Iba, W., & Thompson, K. </author> <year> (1992). </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 223-228). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 399-406). </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Merz, C. J., Murphy, P. M., & Aha, D. W. </author> <year> (1997). </year> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. http:- //www.ics.uci.edu/ mlearn/MLRepository.html. </address>
Reference-contexts: This ensures the Bayesian classifier does not inadvertently have access to more information than the other algorithms, and if anything biases the results against it. Twenty-eight data sets from the UCI repository <ref> (Merz, Murphy & Aha,1997) </ref> were used in the study. Twenty runs were conducted for each data set, randomly selecting 2 3 of the data for training and the remainder for testing. Table 1 shows the average accuracies obtained.
Reference: <author> Niblett, T. </author> <year> (1987). </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> Proceedings of the Second European Working Session on Learning (pp. </booktitle> <pages> 67-78). </pages> <address> Bled, Yugoslavia: Sigma. </address>
Reference-contexts: A principled solution to this problem is to incorporate a small-sample correction into all probabilities, such as the Laplace correction <ref> (Niblett, 1987) </ref>. If n ijk is the number of times 6 P. DOMINGOS AND M.
Reference: <author> Pazzani, M. J. </author> <year> (1996). </year> <title> Searching for dependencies in Bayesian classifiers. </title> <editor> In D. </editor> <publisher> Fisher & H.-J. </publisher>
Reference: <author> Lenz (Eds.), </author> <title> Learning from data: </title> <journal> Artificial intelligence and statistics V (pp. </journal> <pages> 239-248). </pages> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Pazzani, M., Muramatsu, J., & Billsus, D. </author> <year> (1996). </year> <title> Syskill & Webert: Identifying interesting web sites. </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 54-61). </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Pazzani, M., & Sarrett, W. </author> <year> (1990). </year> <title> A framework for average case analysis of conjunctive learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 349-372. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction <ref> (C4.5 release 8, Quinlan, 1993) </ref>, instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction (CN2 version 6.1, Clark & Boswell, 1991). A simple Bayesian classifier was implemented for these experiments. Three main issues arise here: how to handle numeric attributes, zero counts, and missing values.
Reference: <author> Russek, E., Kronmal, R. A., & Fisher, L. D. </author> <year> (1983). </year> <title> The effect of assuming independence in applying Bayes' theorem to risk estimation and classification in diagnosis. </title> <journal> Computers and Biomedical Research, </journal> <volume> 16, </volume> <pages> 537-552. </pages>
Reference: <author> Sahami, M. </author> <year> (1996). </year> <title> Learning limited dependence Bayesian classifiers. </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 335-338). </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Singh, M., & Provan, G. M. </author> <year> (1995). </year> <title> A comparison of induction algorithms for selective and nonselective Bayesian classifiers. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 497-505). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, M., & Provan, G. M. </author> <year> (1996). </year> <title> Efficient learning of selective Bayesian network classifiers. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 453-461). </pages> <address> Bari, Italy: </address> <note> Morgan Kaufmann. 30 P. </note> <author> DOMINGOS AND M. PAZZANI Tibshirani, R. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules (technical report). </title> <institution> Department of Preventive Medicine and Biostatistics, University of Toronto, Toronto, </institution> <address> Ontario. http://utstat.toronto.edu/reports/tibs/biasvar.ps. </address>
Reference: <author> Wan, S. J., & Wong, S. K. M. </author> <year> (1989). </year> <title> A measure for concept dissimilarity and its applications in machine learning. </title> <booktitle> Proceedings of the International Conference on Computing and Information (pp. </booktitle> <pages> 267-273). </pages> <address> Toronto, Ontario: </address> <note> North-Holland. Received Date Accepted Date Final Manuscript Date </note>
Reference-contexts: This section empirically tests this claim by comparing Pazzani's (1996) extension with one that differs from it solely by using the method for attribute dependence detection described in (Kononenko, 1991) and <ref> (Wan & Wong, 1989) </ref>. In each case, the algorithm finds the single best pair of attributes to join by considering all possible joins. Two measures for determining the best pair were compared.
References-found: 37

