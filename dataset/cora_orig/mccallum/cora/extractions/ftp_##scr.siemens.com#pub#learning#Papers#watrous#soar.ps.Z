URL: ftp://scr.siemens.com/pub/learning/Papers/watrous/soar.ps.Z
Refering-URL: ftp://ftp.mbfys.kun.nl/snn/pub/pierre/connectionists.html
Root-URL: 
Title: Synthesize, Optimize, Analyze, Repeat (SOAR): Application of Neural Network Tools to ECG Patient Monitoring  
Author: Raymond Watrous Geoffrey Towell, and Martin S. Glassman 
Address: 755 College Road East, Princeton, NJ 08540  
Affiliation: Siemens Corporate Research  
Abstract: Results are reported from the application of tools for synthesizing, optimizing and analyzing neural networks to an ECG Patient Monitoring task. A neural network was synthesized from a rule-based classifier and optimized over a set of normal and abnormal heartbeats. The classification error rate on a separate and larger test set was reduced by a factor of 2. Sensitivity analysis of the synthesized and optimized networks revealed informative differences. Analysis of the weights and unit activations of the optimized network enabled a reduction in size of the network by a factor of 40% without loss of accuracy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58. </pages>
Reference-contexts: Introduction It has been pointed out that learning in complex domains is intractable without sufficient domain knowledge <ref> [1] </ref>. Consequently, the instantiation of domain knowledge in neural network models used in learning is of considerable interest. Once instantiated in a neural network model, domain knowledge can be refined by methods of nonlinear numerical optimization using gradient descent.
Reference: [2] <author> Kuhn, G. </author> <year> (1992). </year> <title> Joint optimization of classifier and feature space in speech recognition. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 4, </volume> <pages> pages 709-714. </pages>
Reference-contexts: Analysis Sensitivity Analysis Having instantiated the SME classifier in the form of a differentiable neural network, it became possible to compute the sensitivity of the classifier output to its input features. Following a method introduced by Kuhn <ref> [2] </ref>, the sensitivities were ordered by the value of the output unit; this allowed the sensitivities to be interpreted with respect to the correctness of the classifier.
Reference: [3] <author> Makhoul, J. </author> <year> (1991). </year> <title> Pattern recognition properties of neural networks. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Neural Networks for Signal Processing: Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 173-187. </pages> <booktitle> IEEE Signal Processing Society. </booktitle>
Reference-contexts: A subset of this data base was used for training purposes and the remainder for testing. It has been shown that minimization of the mean squared error (MSE) leads to an estimate of the posterior probability of the class given the input <ref> [3] </ref>. This measure includes the prior probability of the class, which is assumed to be represented in the distribution of the training samples. Since examples of arrhythmias are relatively rare in the standard ECG data bases, the prior probability of a normal beat is quite high.
Reference: [4] <author> Rumelhart, D. E., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., McClelland, J. L., </editor> <title> and the PDP research group, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Volume I Foundations, chapter 8. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The resulting network, depicted in Figure 1, yielded identical performance to the original set of rules. Optimization The gradient of the output of a neural network with respect to the parameters of the network can be efficiently computed <ref> [4] </ref>. Consequently, the network can be optimized by standard nonlinear methods of gradient descent. For the ECG application, there are several large data bases available, including the MIT/BIH database, which contains approximately 100,000 heartbeats that have been labeled by expert cardiologists.
Reference: [5] <author> Towell, G. G., Shavlik, J. W., and Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Synthesis It has been shown that a neural network can be synthesized from any set of Boolean clauses <ref> [5, 6] </ref>. That is, the architecture, unit offsets and interconnection strengths of a neural network can fl To appear in: Proceedings of the Workshop on Environmental and Energy Applications of Neural Networks, March 30-31, 1995, Richland, Washington.
Reference: [6] <author> Watrous, R. L. </author> <year> (1988). </year> <title> Speech Recognition using Connectionist Networks. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania. </institution>
Reference-contexts: Synthesis It has been shown that a neural network can be synthesized from any set of Boolean clauses <ref> [5, 6] </ref>. That is, the architecture, unit offsets and interconnection strengths of a neural network can fl To appear in: Proceedings of the Workshop on Environmental and Energy Applications of Neural Networks, March 30-31, 1995, Richland, Washington.
References-found: 6

