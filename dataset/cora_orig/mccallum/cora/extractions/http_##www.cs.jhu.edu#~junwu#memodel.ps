URL: http://www.cs.jhu.edu/~junwu/memodel.ps
Refering-URL: http://www.cs.jhu.edu/~junwu/
Root-URL: http://www.cs.jhu.edu
Title: A Maximum Entropy Language Model with Topic Sensitive Features  
Author: Jun Wu Advisors: Dr. Sanjeev Khudanpur and Dr. Frederick Jelinek 
Date: March 12, 1998  
Address: 3400 N. Charles Street Baltimore, MD 21218  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> I.Csiszar, </author> <title> Why least squares and maximum entropy? An axiomatic approach to inference. </title> <journal> Annals of Statistics, </journal> <volume> 19(4) </volume> <pages> 20232-2067, </pages> <year> 1991. </year>
Reference-contexts: All features, either Markov or non-Markov ones, give the probability distributions of words some constraints, and language modeling is used to find a good distribution that satisfies all constraints. According to the maximum entropy principle <ref> [12, 2, 1] </ref>, the best distribution should have maximum entropy under these constraints since we should make no additional assumptions other than what we know [5, 1]. <p> According to the maximum entropy principle [12, 2, 1], the best distribution should have maximum entropy under these constraints since we should make no additional assumptions other than what we know <ref> [5, 1] </ref>. To get topic sensitive features, we first classify the training data according to topic and extract from the data for each topic words whose frequencies within the topic deviate significantly from the overall distributions. <p> Several algorithm improvements employed to make the maximum entropy framework implementable are then presented in Section 3. We discuss topic classification and feature extraction in Section 4. Experimental results are presented in Section 6. We conclude with a summary and mention of future work. 2 Maximum Entropy Method <ref> [11, 1] </ref> 2.1 Basic Ideas The basic problem of statistical language modeling is to estimate a joint probability measurement: X; Y ! [0; 1] from observations &lt; x 1 ; y 1 &gt; ,..., &lt; x L ; y L &gt; where L is the length of training data. <p> Experimental results are presented in Section 6. We conclude with a summary and mention of future work. 2 Maximum Entropy Method [11, 1] 2.1 Basic Ideas The basic problem of statistical language modeling is to estimate a joint probability measurement: X; Y ! <ref> [0; 1] </ref> from observations &lt; x 1 ; y 1 &gt; ,..., &lt; x L ; y L &gt; where L is the length of training data. For example, in a trigram model, Y is the vocabulary set and X is the set of bigrams.
Reference: [2] <author> I. Csiszar, </author> <title> A geometric interpretation of Darroch and Ratcliff's generalized iterative scaling. </title> <journal> The Annals of Staticstics, </journal> <volume> 17(3) </volume> <pages> 1409-1413, </pages> <year> 1989. </year>
Reference-contexts: All features, either Markov or non-Markov ones, give the probability distributions of words some constraints, and language modeling is used to find a good distribution that satisfies all constraints. According to the maximum entropy principle <ref> [12, 2, 1] </ref>, the best distribution should have maximum entropy under these constraints since we should make no additional assumptions other than what we know [5, 1].
Reference: [3] <author> R. Florian, </author> <type> Personal Conversation. </type>
Reference-contexts: bigrams (~200 per w 1 ) and the topic-conditional unigrams (~1000) may be removed from Y x (separatedly for each t) and clubbed with the unigrams, considerably reducing the computational complexity, O (jXj jY x j). 4 Topic Sensitive Modeling 4.1 Classification of Conversations We use the clustering method in <ref> [3] </ref> to classify conversations into different topics. Let - X: Weighted unigram frequencies of all words in the conversa tion except stop words. - Y : Similar vector for the centroid.
Reference: [4] <author> R. Iyer, </author> <title> Using Out-of-Domain Data to Improve Statistical Language Models, </title> <type> Ph.D. Dissertation, </type> <institution> Boston University, </institution> <year> 1997. </year>
Reference-contexts: A more effective way of boosting the probability of relevant words may be via determination of semantic content of the document, eg., by topic detection. Efforts to construct individual language models for various topics and using a mixture of relevant models have been made <ref> [4] </ref>. These methods suffer from the drawback that the training data get fragmented, which makes the estimate of topic insensitive probabilities poor. We propose a unified model in which topic is a part of the word context or history used for prediction.
Reference: [5] <author> E. Jaynes, </author> <title> Where do we stand on maximum entropy? In R. </title> <editor> Levine and M. Tribus editors, </editor> <title> The Maximum Entropy Formalism. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978 </year>
Reference-contexts: According to the maximum entropy principle [12, 2, 1], the best distribution should have maximum entropy under these constraints since we should make no additional assumptions other than what we know <ref> [5, 1] </ref>. To get topic sensitive features, we first classify the training data according to topic and extract from the data for each topic words whose frequencies within the topic deviate significantly from the overall distributions.
Reference: [6] <author> F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. </author> <title> A Dynamic Language Model for Speech Recognition, </title> <booktitle> Proceedings of Speech and Natural Language DARPA Workshop, </booktitle> <pages> pages 293-95, </pages> <year> 1991. </year>
Reference-contexts: Some non-Markov features, such as lexicon triggers and cache language models, have been examined in the past <ref> [6, 9] </ref>. However determining lexicon trigger pairs is a difficult problem, and cache language models have limited effectiveness because they are essentially self-triggers the occurrence of words of a sentence boosts the probability of their own occurrence.
Reference: [7] <author> F. Jelinek and R. Mercer, </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <editor> In Edzard S. Gelsema and Laveen N. Kanal, editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pp 381-397, </pages> <address> Amsterdam, May 21-23 1980, </address> <publisher> North Holland. </publisher>
Reference: [8] <author> S. Katz, </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer, </title> <journal> IEEE, Trans on ASSP Vol.35, No.3, </journal> <volume> pp400-401, </volume> <month> March, </month> <year> 1987 </year>
Reference: [9] <author> R. Rosenfeld, </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach, </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie-Mellon University, </institution> <year> 1994. </year>
Reference-contexts: Some non-Markov features, such as lexicon triggers and cache language models, have been examined in the past <ref> [6, 9] </ref>. However determining lexicon trigger pairs is a difficult problem, and cache language models have limited effectiveness because they are essentially self-triggers the occurrence of words of a sentence boosts the probability of their own occurrence.
Reference: [10] <author> S. Pietra, V. Pietra, J. Lafferty, </author> <title> Inducing Features of Random Fields, </title> <publisher> CMU-CS-95-144. </publisher>
Reference: [11] <author> E. Ristad, </author> <title> Maximum Entropy Modeling Toolkit, </title> <year> 1997. </year>
Reference-contexts: Several algorithm improvements employed to make the maximum entropy framework implementable are then presented in Section 3. We discuss topic classification and feature extraction in Section 4. Experimental results are presented in Section 6. We conclude with a summary and mention of future work. 2 Maximum Entropy Method <ref> [11, 1] </ref> 2.1 Basic Ideas The basic problem of statistical language modeling is to estimate a joint probability measurement: X; Y ! [0; 1] from observations &lt; x 1 ; y 1 &gt; ,..., &lt; x L ; y L &gt; where L is the length of training data.
Reference: [12] <author> S. Khudanpur, </author> <title> Notes for the course Information Theoretic Methods in Statistics, </title> <booktitle> 1998. </booktitle> <pages> 17 </pages>
Reference-contexts: All features, either Markov or non-Markov ones, give the probability distributions of words some constraints, and language modeling is used to find a good distribution that satisfies all constraints. According to the maximum entropy principle <ref> [12, 2, 1] </ref>, the best distribution should have maximum entropy under these constraints since we should make no additional assumptions other than what we know [5, 1]. <p> X p (x; y) g i (x; y) (1) Given such a function class P, we want to find the distribution p fl (x; y) in P that maximizes the entropy H (p) where H (p) = &lt;x;y&gt; The solution p fl (x; y) has the exponential form <ref> [12] </ref>. So we consider the class R of exponential models m (yjx) defined over our features G. <p> R = fm : m (yjx) = Z (x) where r (y; x) = i g i (x;y) Z (x) = y The intersection of P and R contains the maximum entropy distribution P fl (x; y) and is unique <ref> [12] </ref>.
References-found: 12

