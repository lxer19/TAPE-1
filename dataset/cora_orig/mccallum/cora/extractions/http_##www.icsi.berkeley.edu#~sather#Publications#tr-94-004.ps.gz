URL: http://www.icsi.berkeley.edu/~sather/Publications/tr-94-004.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~sather/Publications/tr-94-004.html
Root-URL: http://www.icsi.berkeley.edu
Title: Near or Far  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Hermann Hartig 
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-94-004  
Abstract: To efficiently program massively parallel computers it is important to be aware of nearness or farness of references. It can be a severe performance bug if a reference that is meant to be near by a programmer turns out to be far. This paper presents a simple way to express nearness and farness in such a way that compile-time detection of such performance bugs becomes possible. It also allows for compile-time determination of nearness for many cases which can be used for compile time optimization techniques to overlap communication with processing. The method relies on the type system of a strongly typed object oriented language whose type rules are extended by three type coercion rules. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Cray t3d. </institution> <type> personal communication. </type>
Reference-contexts: The minimum latency difference currently <ref> [1] </ref> seems to be a factor of around five, while a factor in the order of ten to one hundred is the normal case.
Reference: [2] <institution> Concurrent object-oriented programming. Communications of the ACM, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Examples for this approach include Split-C [4] and most designs published in a recent special issue of CACM <ref> [2] </ref>. These efforts are either based on a message passing (e.g. OCCAM [9]) or a (logical) shared address space programming paradigm. The common denominator here is that the distribution of work is organized by the programmer. That makes possible to use even highly irregular and complex data structures.
Reference: [3] <author> D.C. Cann. </author> <title> The optimising sisal compiler: Version 123.0. </title> <type> Technical Report UCRL-MA-110080, </type> <institution> Lawrence Livermoore National Laboratory Man-aual, </institution> <year> 1992. </year>
Reference-contexts: Two major directions can be identified: * Compiler controlled: This direction is characterized by the employment of highly sophisticated compilers to organize and distribute parallel work over such architectures. Examples include the translation of functional programming languages <ref> [3] </ref> for those architectures, attempts to transfer well understood SIMD technology by translating sequential programs (e.g. FORTRAN [5]) as well as the extension of modern languages by "FORALL constructs". The com mon denominator of this direction is that compilers distribute the work and data structures.
Reference: [4] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishna-murty, S. Lumetta, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: That approach, however, is usually limited to regular data structures (e.g., arrays) that can be dis tributed in a more or less obvious manner. * Programmer controlled: Here the organization of work for a massively parallel computer is done by the "competent programmer". Examples for this approach include Split-C <ref> [4] </ref> and most designs published in a recent special issue of CACM [2]. These efforts are either based on a message passing (e.g. OCCAM [9]) or a (logical) shared address space programming paradigm. The common denominator here is that the distribution of work is organized by the programmer.
Reference: [5] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran. Language Spec., </title> <month> December </month> <year> 1992. </year>
Reference-contexts: Examples include the translation of functional programming languages [3] for those architectures, attempts to transfer well understood SIMD technology by translating sequential programs (e.g. FORTRAN <ref> [5] </ref>) as well as the extension of modern languages by "FORALL constructs". The com mon denominator of this direction is that compilers distribute the work and data structures.
Reference: [6] <author> S. Murer, J. Feldman, Chu-Cheow Lim, and Martina-Maria Seidel. psather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-028, ICSI, </type> <month> Dec </month> <year> 1993. </year>
Reference-contexts: Since the programming language Sather [7, 8] has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and <ref> [6] </ref>. Section 2.1 is, for the most part, taken literally from the Sather report [8]. 2.1 Sather Data structures in Sather are constructed from objects. Each object has a unique type which defines the operations which may be performed on it. <p> Two features of pSather are of interest to this paper, namely threads and the notion of object location. To avoid confusion, it must be pointed out that the Sather language extensions described in section 3 deviate from pSather, as defined by the current report <ref> [6] </ref>. (Serial) Sather extended with threads and the notion of location as found in pSather, and extended with few other features is used as the substrate to explain the contribution of this paper.
Reference: [7] <author> Stephen M. Omohundro. </author> <title> The Sather Language. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca., </address> <year> 1991. </year>
Reference-contexts: Then the extensions required to express and to check nearness are introduced and implications and limitations are discussed. Finally, some conclusions are drawn. 2 Sather and pSather The basic idea presented in this paper is to exploit properties of strongly typed object oriented languages. Since the programming language Sather <ref> [7, 8] </ref> has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and [6]. <p> Since the programming language Sather <ref> [7, 8] </ref> has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and [6]. Section 2.1 is, for the most part, taken literally from the Sather report [8]. 2.1 Sather Data structures in Sather are constructed from objects. Each object has a unique type which defines the operations which may be performed on it.
Reference: [8] <author> Stephen M. Omohundro. </author> <title> The Sather 1.0 Specification. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, Ca., </institution> <note> 1994 (in preparation). </note>
Reference-contexts: Then the extensions required to express and to check nearness are introduced and implications and limitations are discussed. Finally, some conclusions are drawn. 2 Sather and pSather The basic idea presented in this paper is to exploit properties of strongly typed object oriented languages. Since the programming language Sather <ref> [7, 8] </ref> has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and [6]. <p> Since the programming language Sather <ref> [7, 8] </ref> has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and [6]. Section 2.1 is, for the most part, taken literally from the Sather report [8]. 2.1 Sather Data structures in Sather are constructed from objects. Each object has a unique type which defines the operations which may be performed on it. <p> Since the programming language Sather [7, 8] has a clear, clean, and elegant type system, it is used as basis for the further explanation. This section essentially is a brief summary of [7, 8] and [6]. Section 2.1 is, for the most part, taken literally from the Sather report <ref> [8] </ref>. 2.1 Sather Data structures in Sather are constructed from objects. Each object has a unique type which defines the operations which may be performed on it. Each Sather variable has a declared type which determines the types of objects it may hold. <p> An example is a variable that counts how many times a certain routine of a class is called. In Sather, as in some other object oriented languages, there is no way to differentiate between these uses: Shared variables are common to all objects of a class <ref> [8] </ref>. For parallel programs, two interpretations of shared variables come to mind: * Variables are shared between all objects of a class that are located on the same node.
Reference: [9] <author> Dick Pountain. </author> <title> A tutorial introduction to OCCAM programming. Inmos Occam Manual. </title> <type> 8 </type>
Reference-contexts: Examples for this approach include Split-C [4] and most designs published in a recent special issue of CACM [2]. These efforts are either based on a message passing (e.g. OCCAM <ref> [9] </ref>) or a (logical) shared address space programming paradigm. The common denominator here is that the distribution of work is organized by the programmer. That makes possible to use even highly irregular and complex data structures. Examples include distributed trees, event driven simulation, and bounded tree search algorithms.
References-found: 9

