URL: ftp://ftp.cis.upenn.edu/pub/traumaid/papers/inc-markov.ps.gz
Refering-URL: http://www.cis.upenn.edu/~traumaid/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rwash@linc.cis.upenn.edu  
Title: Incremental Markov-Model Planning  
Author: Richard Washington 
Address: 200 South 33rd Street, 5th Floor Philadelphia, PA 19104-6389  
Affiliation: Philadelphia VAMC and University of Pennsylvania Department of Computer and Information Science  
Abstract: This paper presents an approach to building plans using partially observable Markov decision processes. The approach begins with a base solution that assumes full observability. The partially observable solution is incrementally constructed by considering increasing amounts of information from observations. The base solution directs the expansion of the plan by providing an evaluation function for the search fringe. We show that incremental observation moves from the base solution towards the complete solution, allowing the planner to model the uncertainty about action outcomes and observations that are present in real domains. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Beck and S. G. Pauker. </author> <title> The Markov process in medical prognosis. Medical Decision Making, </title> <booktitle> 3(4) </booktitle> <pages> 419-458, </pages> <year> 1983. </year>
Reference-contexts: The role of diagnostic tests for identifying the current state of the patient is absent from these models; when used, it appears in a decision tree, with the leaves being Markov models of the outcomes based on the results of a diagnostic procedure <ref> [1] </ref>. However, actual medical practice interleaves diagnostic and therapeutic actions so that the most appropriate procedure is done at each time (given the available knowledge).
Reference: [2] <author> C. Boutilier and R. Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <year> 1994. </year>
Reference-contexts: In fact, what the robot needs to do is calculate the optimal action to take in face of this uncertainty, and given the sensor information. This is exactly what POMDPs are designed to do. Existing research on Markov approaches rely either on FOMDPs that assume perfect sensor knowledge <ref> [6, 2] </ref>, or 0th-order approximations to POMDPs that ignore information-gathering actions [17]. The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3].
Reference: [3] <author> A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <year> 1994. </year>
Reference-contexts: Existing research on Markov approaches rely either on FOMDPs that assume perfect sensor knowledge [6, 2], or 0th-order approximations to POMDPs that ignore information-gathering actions [17]. The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models <ref> [13, 3] </ref>. Available approximations are only powerful enough to scale up to problems with tens of states [18, 11, 16]. However, even simple tasks can require thousands of states (see [17]). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. <p> In all cases, the value function quickly changes from the initial FOMDP-based estimate and then more slowly approaches the true POMDP value. This is true as the AO* search is expanded: tiger problem <ref> [3] </ref>. as the AO* search is expanded: tiny navigation problem [10]. as the AO* search is expanded: large naviga tion problem [10]. whether the absolute change in utility is large (Figures 3 and 4) or small (Figures 5 and 6). 6 Discussion We have shown how a POMDP decision rule can
Reference: [4] <author> M. E. Cowen, M. Chartrand, and W. F. Weitzel. </author> <title> A Markov model of the natural history of prostate cancer. </title> <journal> Journal of Clinical Epidemiology, </journal> <pages> pages 3-21, </pages> <year> 1994. </year>
Reference-contexts: In medical diagnosis, the underlying patient state can be modeled as a Markov process, with medical procedures and natural processes moving the patient from state to state with some probability. These probabilities are reported in the medical literature (e.g., see <ref> [4] </ref>) and can be used to produce Markov models of disease progression, modeling such features as survival rate [4] and cost [7]. However, in these models the patient's state is modeled probabilistically based on reported occurrences in the general population or a specific patient population. <p> These probabilities are reported in the medical literature (e.g., see <ref> [4] </ref>) and can be used to produce Markov models of disease progression, modeling such features as survival rate [4] and cost [7]. However, in these models the patient's state is modeled probabilistically based on reported occurrences in the general population or a specific patient population.
Reference: [5] <author> A. Curran and K. J. Kyriakopolous. </author> <title> Sensor-based localization for wheeled mobile robots. </title> <booktitle> In Proccedings, IEEE International Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: Those that do work often rely on specific features or models of the robot or environment that rarely generalize <ref> [12, 5] </ref>. In fact, what the robot needs to do is calculate the optimal action to take in face of this uncertainty, and given the sensor information. This is exactly what POMDPs are designed to do.
Reference: [6] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nichol-son. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 35-74, </pages> <year> 1995. </year>
Reference-contexts: In fact, what the robot needs to do is calculate the optimal action to take in face of this uncertainty, and given the sensor information. This is exactly what POMDPs are designed to do. Existing research on Markov approaches rely either on FOMDPs that assume perfect sensor knowledge <ref> [6, 2] </ref>, or 0th-order approximations to POMDPs that ignore information-gathering actions [17]. The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3].
Reference: [7] <author> M. C. Fahs, J. Mandelblatt, C. Schechter, and C. Muller. </author> <title> Cost effectiveness of cervical cancer screening for the elderly. </title> <journal> Annals of Internal Medicine, </journal> <volume> 117(6) </volume> <pages> 520-527, </pages> <year> 1992. </year>
Reference-contexts: These probabilities are reported in the medical literature (e.g., see [4]) and can be used to produce Markov models of disease progression, modeling such features as survival rate [4] and cost <ref> [7] </ref>. However, in these models the patient's state is modeled probabilistically based on reported occurrences in the general population or a specific patient population.
Reference: [8] <author> Y. K. Hwang and N. Ahuja. </author> <title> Gross motion planning| a survey. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(3), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: A mobile robot relies on its sensor information to infer its location in the world, and then uses that location to calculate the best route to take. Given perfect informa tion, there is a wide array of approaches that work well for navigation <ref> [15, 9, 8] </ref>, but in reality very few work well when the robot is unsure of its location. Those that do work often rely on specific features or models of the robot or environment that rarely generalize [12, 5].
Reference: [9] <author> K. Kedem and M. Sharir. </author> <title> An efficient motion planning algorithm for a convex rigid polygonal object in 2-dimensional polygonal space. </title> <journal> Discrete Computational Geometry, </journal> <volume> 5 </volume> <pages> 43-75, </pages> <year> 1990. </year>
Reference-contexts: A mobile robot relies on its sensor information to infer its location in the world, and then uses that location to calculate the best route to take. Given perfect informa tion, there is a wide array of approaches that work well for navigation <ref> [15, 9, 8] </ref>, but in reality very few work well when the robot is unsure of its location. Those that do work often rely on specific features or models of the robot or environment that rarely generalize [12, 5].
Reference: [10] <author> M. L. Littman, A. Cassandra, and L. P. Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> San Fran-cisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Define a value function of a state distribution as a weighted sum of the FOMDP value functions: V F () = i2N (this is the approach suggested in <ref> [10] </ref> and [17]). <p> In fact, the FOMDP solution can be used if necessary to decide on an action in the absence of an expanded POMDP path, using a weighting to decide on the best action, as in <ref> [10] </ref>. As the AND-OR tree is expanded, the best action is increasingly influenced by the observations and decreasingly by the FOMDP, thus incrementally shifting from a FOMDP model to a POMDP model. State S8 is the goal state. <p> In all cases, the value function quickly changes from the initial FOMDP-based estimate and then more slowly approaches the true POMDP value. This is true as the AO* search is expanded: tiger problem [3]. as the AO* search is expanded: tiny navigation problem <ref> [10] </ref>. as the AO* search is expanded: large naviga tion problem [10]. whether the absolute change in utility is large (Figures 3 and 4) or small (Figures 5 and 6). 6 Discussion We have shown how a POMDP decision rule can be incrementally constructed using a FOMDP solution of the core <p> This is true as the AO* search is expanded: tiger problem [3]. as the AO* search is expanded: tiny navigation problem <ref> [10] </ref>. as the AO* search is expanded: large naviga tion problem [10]. whether the absolute change in utility is large (Figures 3 and 4) or small (Figures 5 and 6). 6 Discussion We have shown how a POMDP decision rule can be incrementally constructed using a FOMDP solution of the core process.
Reference: [11] <author> W. S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3]. Available approximations are only powerful enough to scale up to problems with tens of states <ref> [18, 11, 16] </ref>. However, even simple tasks can require thousands of states (see [17]). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. First, we briefly review POMDPs.
Reference: [12] <author> R. Mandelbaum and M. Mintz. </author> <title> A confidence set approach to mobile robot localization. </title> <booktitle> In Proceedings of the 1996 Conference on Multisensor Fusion and Integration, </booktitle> <year> 1996. </year>
Reference-contexts: Those that do work often rely on specific features or models of the robot or environment that rarely generalize <ref> [12, 5] </ref>. In fact, what the robot needs to do is calculate the optimal action to take in face of this uncertainty, and given the sensor information. This is exactly what POMDPs are designed to do.
Reference: [13] <author> G. E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Existing research on Markov approaches rely either on FOMDPs that assume perfect sensor knowledge [6, 2], or 0th-order approximations to POMDPs that ignore information-gathering actions [17]. The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models <ref> [13, 3] </ref>. Available approximations are only powerful enough to scale up to problems with tens of states [18, 11, 16]. However, even simple tasks can require thousands of states (see [17]). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. <p> Finally we discuss our initial results on a simple robot navigation problem and some other problems from the literature. 2 Partially Observable Markov Deci sion Processes In this section we briefly review Markov processes, and in particular POMDPs. We will borrow the notation of <ref> [13] </ref>, adding or changing only as required for the problem at hand; the reader can refer there for a more complete explanation of the framework. We assume that the underlying process, the core process, is described by a finite-state, stationary Markov chain.
Reference: [14] <author> N. J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: Given the AND-OR tree of actions and observations and an evaluation function, with the goal of maximizing the utility of the chosen plan of actions, we can invert the utilities (making them disutilities). This now presents the goal of minimizing the disutility of the plan, so the AO* algorithm <ref> [14] </ref> can be used to expand the tree incrementally. The AO* algorithm still involves a non-deterministic choice of which AND branch along the current best path to further expand (since that does not fall out directly from the evaluations, unlike in A*).
Reference: [15] <author> C. O'Dunlaing and C. K. Yap. </author> <title> A "retraction" method for planning the motion of a disc. </title> <journal> Journal of Algorithms, </journal> <volume> 6 </volume> <pages> 104-111, </pages> <year> 1985. </year>
Reference-contexts: A mobile robot relies on its sensor information to infer its location in the world, and then uses that location to calculate the best route to take. Given perfect informa tion, there is a wide array of approaches that work well for navigation <ref> [15, 9, 8] </ref>, but in reality very few work well when the robot is unsure of its location. Those that do work often rely on specific features or models of the robot or environment that rarely generalize [12, 5].
Reference: [16] <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <year> 1995. </year>
Reference-contexts: The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3]. Available approximations are only powerful enough to scale up to problems with tens of states <ref> [18, 11, 16] </ref>. However, even simple tasks can require thousands of states (see [17]). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. First, we briefly review POMDPs.
Reference: [17] <author> R. Simmons and S. Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <year> 1995. </year>
Reference-contexts: This is exactly what POMDPs are designed to do. Existing research on Markov approaches rely either on FOMDPs that assume perfect sensor knowledge [6, 2], or 0th-order approximations to POMDPs that ignore information-gathering actions <ref> [17] </ref>. The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3]. Available approximations are only powerful enough to scale up to problems with tens of states [18, 11, 16]. <p> Available approximations are only powerful enough to scale up to problems with tens of states [18, 11, 16]. However, even simple tasks can require thousands of states (see <ref> [17] </ref>). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. First, we briefly review POMDPs. Then we show how observations can be incrementally added to improve a plan. <p> Define a value function of a state distribution as a weighted sum of the FOMDP value functions: V F () = i2N (this is the approach suggested in [10] and <ref> [17] </ref>).
Reference: [18] <author> C. C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: The POMDP literature, on the other hand, makes it obvious that precise solutions to POMDPs are available for only trivially small models [13, 3]. Available approximations are only powerful enough to scale up to problems with tens of states <ref> [18, 11, 16] </ref>. However, even simple tasks can require thousands of states (see [17]). The approach presented here shows that an FOMDP solution can produce estimates that lead to an incremental POMDP solution. First, we briefly review POMDPs.
References-found: 18

