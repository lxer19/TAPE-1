URL: http://www.cs.tamu.edu/people/borgesl/europ.ps
Refering-URL: http://www.cs.tamu.edu/people/borgesl/
Root-URL: http://www.cs.tamu.edu
Title: A Parallel Solver for Extreme Eigenpairs  
Author: Leonardo Borges and Suely Oliveira 
Address: College Station, TX 77843-3112, USA.  
Affiliation: Computer Science Department, Texas A&M University,  
Abstract: In this paper a parallel algorithm for finding a group of extreme eigenvalues is presented. The algorithm is based on the well known Davidson method for finding one eigenvalue of a matrix. Here we incorporate knowledge about the structure of the subspace through the use of an arrowhead solver which allows more parallelization in both the original Davidson and our new version. In our numerical results various preconditioners (diagonal, multigrid and ADI) are compared. The performance results presented are for the Paragon but our implementation is portable to machines which provide MPI and BLAS.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK [12] routines, and later implementations [13, 15] use LAPACK <ref> [1] </ref> or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15]. Here we explore the relationship between two successive matrices S k which allows us to represent S k through an arrowhead matrix.
Reference: 2. <author> L. Borges and S. Oliveira. </author> <title> A parallel Davidson-type algorithm for several eigen-values. </title> <journal> Journal of Computational Physics. </journal> <note> To appear. </note>
Reference-contexts: In Section 5 we present numerical and performance results for the parallel implementation on the Paragon. Further results about the parallel algorithm and other numerical results are presented in <ref> [2] </ref>. 2 The Davidson Algorithm Two of the most popular iterative methods for large symmetric eigenvalue problems are Lanczos and Davidson algorithms. <p> This observation reduces the computational work by eliminating the outer loop from the two nested loops in the full MGS algorithm. 4.2 The Arrowhead Relationship Between Matrices S k As pointed in <ref> [2, 10] </ref>, the relationship between S k and S k1 can be used to show that S k is explicitly similar to an arrowhead matrix ~ S k of the form ~ S k = fl k1 ~s k k s kk ; (1) where ~s k = Y T k1 <p> Finally, the ADI method aggregate the advantages of the previous preconditioners in the sense that it is more effective and less expensive than MG. More details about the preconditioners used here can be found in <ref> [2] </ref> and its references. Table 1. <p> Note that the estimated optimal number of processors is not far from the actual optimal. The model for our estimates is presented in <ref> [2] </ref>. Fig. 1. Actual and estimated times for equation (2). Results for two different matrix sizes performance on the Paragon are shown. To conclude, we compare the performance of the parallel DSE with PAR-PACK [7], a parallel implementation of ARPACK 1 . <p> We used the regular mode when running PARPACK. The problem was solved by using 4, 8, and 16 processors to obtain relative resid-uals kAu uk=kuk of order less than 10 5 . We show our theoretical analysis for the parallel algorithm in <ref> [2] </ref>. Other numerical results for the sequential DSE algorithm, including examples showing the behavior of the algorithm for eigenvalues with multiplicity greater than one, were presented in [10]. Fig. 2. Running times for DSE and PARPACK using 4, 8, and 16 processors on the Paragon.
Reference: 3. <author> G. Cisneros, M. Berrondo, and C. F. Brunge. DVDSON: </author> <title> A subroutine to evaluate selected sets of eigenvalues and eigenvectors of large symmetric matrices. </title> <journal> Compu. Chem., </journal> <volume> 10 </volume> <pages> 281-291, </pages> <year> 1986. </year>
Reference-contexts: ( 0 ) 2 , and the angle between the exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works <ref> [3, 4, 17] </ref> adopt EISPACK [12] routines, and later implementations [13, 15] use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15].
Reference: 4. <author> G. Cisneros and C. F. Brunge. </author> <title> An improved computer program for eigenvector and eigenvalues of large configuration iteraction matrices using the algorithm of Davidson. </title> <journal> Compu. Chem., </journal> <volume> 8 </volume> <pages> 157-160, </pages> <year> 1984. </year>
Reference-contexts: ( 0 ) 2 , and the angle between the exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works <ref> [3, 4, 17] </ref> adopt EISPACK [12] routines, and later implementations [13, 15] use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15].
Reference: 5. <author> M. Crouzeix, B. Philippe, and M. Sadkane. </author> <title> The Davidson method. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15(1) </volume> <pages> 62-76, </pages> <year> 1994. </year>
Reference-contexts: Because of our choice for m, note that in step 2.c dim S will be always bigger or equal to j. 3 Convergence Rate A proof of convergence (but without a rate estimate) for the Davidson algorithm is given in Crouzeix, Philippe and Sadkane <ref> [5] </ref>. A bound on the convergence rate was first presented in [10]. The complete proof is shown in Oliveira [9]. Let A be the given matrix whose eigenvalues and eigenvectors are wanted.
Reference: 6. <author> E. R. Davidson. </author> <title> The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. </title> <journal> J. Comp. Phys., </journal> <volume> 17 </volume> <pages> 87-94, </pages> <year> 1975. </year>
Reference-contexts: Although in the original formulation M is the diagonal matrix (diag (A) I) 1 <ref> [6] </ref>, the Generalized Davidson (GD) algorithm allows the incorporation of different operators for M . The DSE algorithm can be summarized as follows.
Reference: 7. <author> K. Maschhoff and D. Sorensen. </author> <title> A portable implementation of ARPACK for dis-tributed memory parallel architectures. </title> <booktitle> In Proceedings of Copper Mountain Conference on Iterative Methods, </booktitle> <month> April 9-13 </month> <year> 1996. </year>
Reference-contexts: The model for our estimates is presented in [2]. Fig. 1. Actual and estimated times for equation (2). Results for two different matrix sizes performance on the Paragon are shown. To conclude, we compare the performance of the parallel DSE with PAR-PACK <ref> [7] </ref>, a parallel implementation of ARPACK 1 . Figure 2 presents the total running times for both algorithms for the problem described above.
Reference: 8. <author> D. P. O'Leary and G. W. Stewart. </author> <title> Computing the eigenvalues and eigenvectors of symmetric arrowhead matrices. </title> <journal> J. Comp. Phys., </journal> <volume> 90 </volume> <pages> 497-505, </pages> <year> 1990. </year>
Reference-contexts: Thus, given the eigenvalues fl k1 and eigenvectors Y k1 of S k1 , matrix ~ S k can be used to find the eigenvalues fl k of S k . Arrowhead eigensolvers <ref> [8, 11] </ref> are highly parallelizable and typically perform O (k 2 ) operations, instead of the usual O (k 3 ) effort of algorithms for dense matrices S. 5 Numerical Results In our numerical results we employ three kind of preconditiners: diagonal pre-conditioner (as in the original Davidson), multigrid and ADI.
Reference: 9. <author> S. Oliveira. </author> <title> On the convergence rate of a preconditioned algorithm for eigenvalue problems. </title> <note> Submitted. </note>
Reference-contexts: The choice of preconditioner is an important issue in eliminating convergence to the wrong eigenvalue [14] In the next section, we describe the Davidson algorithm and our version for computing several eigen-values. In <ref> [9] </ref> Oliveira presented convergence rates for Davidson type algorithm dependent on the type of preconditioner. These results are summarized here in Section 3. Section 4 addresses parallelization strategies discussing the data distribution in a MIMD architecture, and a fast solver for the projected subspace eigenproblem. <p> A bound on the convergence rate was first presented in [10]. The complete proof is shown in Oliveira <ref> [9] </ref>. Let A be the given matrix whose eigenvalues and eigenvectors are wanted. The preconditioner M is given for one step, and Davidson's algorithm is used with u k being the current computed approximate eigenvector.
Reference: 10. <author> S. Oliveira. </author> <title> A convergence proof of an iterative subspace method for eigenval-ues problem. </title> <editor> In F. Cucker and M. Shub, editors, </editor> <booktitle> Foundations of Computational Mathematics Selected Papers, </booktitle> <pages> pages 316-325. </pages> <publisher> Springer, </publisher> <month> January </month> <year> 1997. </year> <title> (selected). </title>
Reference-contexts: 1 Introduction A large number of scientific applications rely on the computation of a few eigen-values for a given matrix A. Typically they require the lowest or highest eigen-values. Our algorithm (DSE) is based on the Davidson algorithm, but calculates various eigenvalues through implicit shifting. DSE was first presented in <ref> [10] </ref> under the name RDME to express its ability to identify eigenvalues with multiplicity bigger than one. The choice of preconditioner is an important issue in eliminating convergence to the wrong eigenvalue [14] In the next section, we describe the Davidson algorithm and our version for computing several eigen-values. <p> A bound on the convergence rate was first presented in <ref> [10] </ref>. The complete proof is shown in Oliveira [9]. Let A be the given matrix whose eigenvalues and eigenvectors are wanted. The preconditioner M is given for one step, and Davidson's algorithm is used with u k being the current computed approximate eigenvector. <p> This observation reduces the computational work by eliminating the outer loop from the two nested loops in the full MGS algorithm. 4.2 The Arrowhead Relationship Between Matrices S k As pointed in <ref> [2, 10] </ref>, the relationship between S k and S k1 can be used to show that S k is explicitly similar to an arrowhead matrix ~ S k of the form ~ S k = fl k1 ~s k k s kk ; (1) where ~s k = Y T k1 <p> We show our theoretical analysis for the parallel algorithm in [2]. Other numerical results for the sequential DSE algorithm, including examples showing the behavior of the algorithm for eigenvalues with multiplicity greater than one, were presented in <ref> [10] </ref>. Fig. 2. Running times for DSE and PARPACK using 4, 8, and 16 processors on the Paragon.
Reference: 11. <author> S. Oliveira. </author> <title> A new parallel chasing algorithm for transforming arrowhead matrices to tridiagonal form. </title> <journal> Mathematics of Computation, </journal> <volume> 67(221) </volume> <pages> 221-235, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: Thus, given the eigenvalues fl k1 and eigenvectors Y k1 of S k1 , matrix ~ S k can be used to find the eigenvalues fl k of S k . Arrowhead eigensolvers <ref> [8, 11] </ref> are highly parallelizable and typically perform O (k 2 ) operations, instead of the usual O (k 3 ) effort of algorithms for dense matrices S. 5 Numerical Results In our numerical results we employ three kind of preconditiners: diagonal pre-conditioner (as in the original Davidson), multigrid and ADI.
Reference: 12. <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix eigensystem routines: EISPACK guide. </title> <booktitle> Number 6 in Lecture Notes Comput. </booktitle> <publisher> Sci. Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <note> second edition, </note> <year> 1976. </year>
Reference-contexts: and the angle between the exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK <ref> [12] </ref> routines, and later implementations [13, 15] use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15].
Reference: 13. <author> A. Stathopoulos and C. F. Fischer. </author> <title> A Davidson program for finding a few selected extreme eigeinpairs of a large, sparse, real, symmetric matrix. </title> <journal> Comp. Phys. Comm., </journal> <volume> 79 </volume> <pages> 268-290, </pages> <year> 1994. </year>
Reference-contexts: exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK [12] routines, and later implementations <ref> [13, 15] </ref> use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15]. Here we explore the relationship between two successive matrices S k which allows us to represent S k through an arrowhead matrix. <p> solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK [12] routines, and later implementations <ref> [13, 15] </ref> use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15]. Here we explore the relationship between two successive matrices S k which allows us to represent S k through an arrowhead matrix.
Reference: 14. <author> A. Stathopoulos, Y. Saad, and C. F. Fisher. </author> <title> Robust preconditioning of large, sparse, symmetric eigenvalue problems. </title> <journal> J. Comp. and Appl. Mathematics, </journal> <volume> 64 </volume> <pages> 197-215, </pages> <year> 1995. </year>
Reference-contexts: DSE was first presented in [10] under the name RDME to express its ability to identify eigenvalues with multiplicity bigger than one. The choice of preconditioner is an important issue in eliminating convergence to the wrong eigenvalue <ref> [14] </ref> In the next section, we describe the Davidson algorithm and our version for computing several eigen-values. In [9] Oliveira presented convergence rates for Davidson type algorithm dependent on the type of preconditioner. These results are summarized here in Section 3.
Reference: 15. <author> V. M. Umar and C. F. Fischer. </author> <title> Multitasking the Davidson algorithm for the large, sparse eigenvalue problem. </title> <journal> Int. J. Supercomput. Appl., </journal> <volume> 3 </volume> <pages> 28-53, </pages> <year> 1989. </year>
Reference-contexts: exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK [12] routines, and later implementations <ref> [13, 15] </ref> use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15]. Here we explore the relationship between two successive matrices S k which allows us to represent S k through an arrowhead matrix. <p> solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works [3, 4, 17] adopt EISPACK [12] routines, and later implementations <ref> [13, 15] </ref> use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15]. Here we explore the relationship between two successive matrices S k which allows us to represent S k through an arrowhead matrix.
Reference: 16. <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1963. </year>
Reference-contexts: In the case of a Diagonal preconditioner this would correspond to scaling the system and then solving. Multigrid and ADI pre-conditioners are more complex and for that we refer the reader to <ref> [16, 18, 19] </ref>. In our implementation level 1, 2 and 3 BLAS and the Message Passing Interface (MPI) library were used for easy portability. The computational results in this section were obtained with a finite difference approximation for u + gu = f (2) on a unit square domain.
Reference: 17. <author> J. Weber, R. Lacroix, and G. Wanner. </author> <title> The eigenvalue problem in configuration iteration calculations: A computer program based on a new derivation of the algorithm of Davidson. </title> <journal> Compu. Chem., </journal> <volume> 4 </volume> <pages> 55-60, </pages> <year> 1980. </year>
Reference-contexts: ( 0 ) 2 , and the angle between the exact and computed eigenvector goes to zero ultimately geometrically with convergence factor bounded by 0 . 4 Parallel Implementation Previous implementations for the Davidson algorithm solve the eigenvalue problem in subspace S by using algorithms for dense matrices: early works <ref> [3, 4, 17] </ref> adopt EISPACK [12] routines, and later implementations [13, 15] use LAPACK [1] or reductions to tridiagonal form. Partial parallelization is obtained through the matrix-vector operations and sparse format storage for matrix A [13, 15].
Reference: 18. <author> J. R. Westlake. </author> <title> A handbook of numerical matrix inversion and solution of linear equations. </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: In the case of a Diagonal preconditioner this would correspond to scaling the system and then solving. Multigrid and ADI pre-conditioners are more complex and for that we refer the reader to <ref> [16, 18, 19] </ref>. In our implementation level 1, 2 and 3 BLAS and the Message Passing Interface (MPI) library were used for easy portability. The computational results in this section were obtained with a finite difference approximation for u + gu = f (2) on a unit square domain.
Reference: 19. <author> D. M. Young. </author> <title> Iterative Solution of Large Linear Systems. </title> <publisher> Academic Press, </publisher> <year> 1971. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In the case of a Diagonal preconditioner this would correspond to scaling the system and then solving. Multigrid and ADI pre-conditioners are more complex and for that we refer the reader to <ref> [16, 18, 19] </ref>. In our implementation level 1, 2 and 3 BLAS and the Message Passing Interface (MPI) library were used for easy portability. The computational results in this section were obtained with a finite difference approximation for u + gu = f (2) on a unit square domain.
References-found: 19

