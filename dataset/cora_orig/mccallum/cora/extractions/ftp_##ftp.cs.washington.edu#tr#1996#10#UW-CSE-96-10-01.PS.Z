URL: ftp://ftp.cs.washington.edu/tr/1996/10/UW-CSE-96-10-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: lamarca@parc.xerox.com ladner@cs.washington.edu  
Title: The Influence of Caches on the Performance of Sorting  
Author: Anthony LaMarca Richard E. Ladner 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: We investigate the effect that caches have on the performance of sorting algorithms both experimentally and analytically. To address the performance problems that high cache miss penalties introduce we restructure heap-sort, mergesort and quicksort in order to improve their cache locality. For all three algorithms the improvement in cache performance leads to a reduction in total execution time. We also investigate the performance of radix sort. Despite the extremely low instruction count incurred by this linear sorting algorithm, its relatively poor cache performance results in worse overall performance than the efficient comparison based sorting algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Blelloch, C. Plaxton, C. Leiserson, S Smith, B. Maggs, and M. Zagha. </author> <title> A comparison of sorting algorithms for the connection machine. </title> <booktitle> In Proceedings of the 3rd ACM Symposium on Parallel Algorithms & Architecture, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: To fix this inefficiency, a single multi-partition pass can be used to divide the full set into a number of subsets which are likely to be cache sized or smaller. Multi-partitioning is used in parallel sorting algorithms to divide a set into subsets for multiple processors <ref> [1, 13] </ref> in order to quickly balance the load. We choose the number of pivots so that the number of subsets larger than the cache is small with sufficiently high probability.
Reference: [2] <author> S. Carlsson. </author> <title> An optimal algorithm for deleting the root of a heap. </title> <journal> Information Processing Letters, </journal> <volume> 37(2) </volume> <pages> 117-120, </pages> <year> 1991. </year>
Reference-contexts: In addition, we employ standard optimizations to reduce instruction count. The literature contains a number of optimizations that reduce the number of comparisons performed for both adds and removes <ref> [5, 2, 10] </ref>, but in practice these do not improve performance and we do not include them in the base heapsort. 4.1 Memory Optimized Heapsort. To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [3] <author> D. Clark. </author> <title> Cache performance of the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction. Since the introduction of caches, main memory has continued to grow slower relative to processor cycle times. The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the Al-phaServer 8400 <ref> [3, 7] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance.
Reference: [4] <author> E. Coffman and P. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Collectively, all of the processes represent the accesses to the entire virtual address space and hence represent the algorithm's overall memory behavior. Collective analysis assumes that the references to memory satisfy the independent reference assumption <ref> [4] </ref>. In this model each access is independent of all previous accesses, that is, the system is memoryless.
Reference: [5] <author> J. De Graffe and W. Kosters. </author> <title> Expected heights in heaps. </title> <journal> BIT, </journal> <volume> 32(4) </volume> <pages> 570-579, </pages> <year> 1992. </year>
Reference-contexts: In addition, we employ standard optimizations to reduce instruction count. The literature contains a number of optimizations that reduce the number of comparisons performed for both adds and removes <ref> [5, 2, 10] </ref>, but in practice these do not improve performance and we do not include them in the base heapsort. 4.1 Memory Optimized Heapsort. To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [6] <author> W. Feller. </author> <title> An Introduction to Probability Theory and its Applications. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: Feller shows that if k points are placed randomly in a range of length 1, the chance of a resulting subrange being of size x or greater is exactly (1 x) k <ref> [6, Vol. 2, Pg. 22] </ref>. Let n be the total number of keys, B the number of keys per cache block, and C the capacity of the cache in blocks. In multi-quicksort we partition the input array into 3n=(BC) pieces, requiring (3n=(BC)) 1 pivots.
Reference: [7] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction. Since the introduction of caches, main memory has continued to grow slower relative to processor cycle times. The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the Al-phaServer 8400 <ref> [3, 7] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance.
Reference: [8] <author> Robert W. Floyd. </author> <title> Treesort 3. </title> <journal> Communications of the ACM, </journal> <volume> 7(12):701, </volume> <year> 1964. </year>
Reference-contexts: With n keys, building the heap takes O (n log n) steps, and removing them in sorted order takes O (n log n) steps. In 1965 Floyd proposed an improved technique for building a heap with better average case performance and a worst case of O (n) steps <ref> [8] </ref>. As a base heapsort algorithm, we follow the recommendations of algorithm textbooks and use a binary heap constructed using Floyd's method. In addition, we employ standard optimizations to reduce instruction count.
Reference: [9] <author> E. H. </author> <title> Friend. </title> <journal> Journal of the ACM, </journal> <volume> 3:152, </volume> <year> 1956. </year>
Reference-contexts: The first pass accumulates counts of the number of keys with each radix. The counts are used to determine the offsets in the keys of each radix in the destination array. The second pass moves the source array to the destination array according to the offsets. Friend <ref> [9] </ref> suggested an improvement to reduce the number of passes over the source array, by accumulating the counts for the (i + 1)-st iteration at the same time as moving keys during the i-th iteration. This requires a second count array of size 2 r .
Reference: [10] <author> G. Gonnet and J. Munro. </author> <title> Heaps on heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15(4) </volume> <pages> 964-971, </pages> <year> 1986. </year>
Reference-contexts: In addition, we employ standard optimizations to reduce instruction count. The literature contains a number of optimizations that reduce the number of comparisons performed for both adds and removes <ref> [5, 2, 10] </ref>, but in practice these do not improve performance and we do not include them in the base heapsort. 4.1 Memory Optimized Heapsort. To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [11] <author> J. Hennesey and D. Patterson. </author> <booktitle> Computer Architecture </booktitle>
Reference-contexts: Throughout this study we have assumed that a block of contiguous pages in the virtual address space map to a block of contiguous pages in the cache. This is only guaranteed to be true when caches are virtually indexed rather than physically indexed <ref> [11] </ref>. Unfortunately, the caches on all five of our test machines are physically indexed.
References-found: 11

