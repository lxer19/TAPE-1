URL: http://www.cs.columbia.edu/~mauricio/papers/merge-purge.ps
Refering-URL: http://www.cs.columbia.edu/~mauricio/
Root-URL: 
Email: fmauricio,salg@cs.columbia.edu  
Title: The Merge/Purge Problem for Large Databases  
Author: Mauricio A. Hernandez Salvatore J. Stolfo 
Address: New York, NY 10027  
Affiliation: Department of Computer Science, Columbia University,  
Abstract: Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Closure over the results of independent runs considering alternative primary key attributes in each pass. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The more corrupted the data, more runs might be needed to capture the matching records. The transitive closure, however, is executed on pairs of tuple id's, each at most 30 bits, and fast solutions to compute transitive closure exist <ref> [1] </ref>. From observing real world scenarios, the size of the data set over which the closure is computed is at least one order of magnitude smaller than the corresponding database of records, and thus does not contribute a large cost.
Reference: [2] <author> M. A. Bickel. </author> <title> Automatic Correction to Misspelled Names: a Fourth-generation Language Approach. </title> <journal> Communications of the ACM, </journal> <volume> 30(3) </volume> <pages> 224-228, </pages> <year> 1987. </year>
Reference-contexts: Since we only have a corpus for the names of the cities in the U.S.A. (18670 different names), we only attempted correcting the spelling of the city field. We chose the algorithm described by Bickel in <ref> [2] </ref> for its simplicity and speed. Although not shown in the results presented in this paper, the use of spell corrector over the city field improved the percent of correctly found duplicated records by only 1.5% - 2.0%.
Reference: [3] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: In [9], we describe the sorted-neighborhood method as a generalization of band joins and provide an alternative algorithm for the sorted-neighborhood method based on the duplicate elimination algorithm described in <ref> [3] </ref>. This duplicate elimination algorithms takes advantage of the fact that "matching" records will come together during different phases of the Sort phase. Due to space limitations, we will not describe this alternative solution here.
Reference: [4] <author> H. M. Dewan, M. A. Hernandez, K. Mok, and S. Stolfo. </author> <title> Predictive Load Balancing of Parallel Hash-Joins over Heterogeneous Processors in the Presence of Data Skew. </title> <booktitle> In Proc. 3rd Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 40-49, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: It then redistributes the clusters among processors using a longest processing time first [8] strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained. In <ref> [4] </ref> we detailed the load balancing algorithm in the context of parallel database joins. The time results for the clustering method are depicted in figure 6 (b).
Reference: [5] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> An Evaluation of Non-Equijoin Algorithms. </title> <booktitle> In Proc. 17th Int'l. Conf. on Very Large Databases, </booktitle> <pages> pages 443-452, </pages> <address> Barcelona, Spain, </address> <year> 1991. </year>
Reference-contexts: The first record in the window slides out of the window (see figure 1). Sorting and then merging within a window is the essential approach of a Sort Merge Band Join as described by DeWitt <ref> [5] </ref>. As described in that paper, the sort and merge phase can be combined in one pass.
Reference: [6] <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year>
Reference-contexts: The results displayed in section 3 are based upon edit distance computation since the outcome of the program did not vary much among the different distance functions for the particular databases used in our study. For the purpose of experimental study, we wrote an OPS5 <ref> [6] </ref> rule program consisting of 26 rules for this particular domain of employee records and was tested repeatedly over relatively small databases of records.
Reference: [7] <author> S. Ghandeharizadeh. </author> <title> Physical Database Design in Multiprocessor Database Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin Madi-son, </institution> <year> 1990. </year>
Reference-contexts: Clustering data as described above raises the issue of how well partitioned the data is after clustering. We use an approach that closely resembles the multidimensional partitioning strategy of <ref> [7] </ref>. If the data from which the n-attribute key is extracted is distributed uniformly over its domain, then we can expect all clusters to have approximately the same number of records in them.
Reference: [8] <author> R. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17 </volume> <pages> 416-429, </pages> <year> 1969. </year>
Reference-contexts: The coordinator processor keeps track of how many records it sent to each processor (and cluster) and therefore it knows, at the end of the clustering stage, how balanced the partition is. It then redistributes the clusters among processors using a longest processing time first <ref> [8] </ref> strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained. In [4] we detailed the load balancing algorithm in the context of parallel database joins.
Reference: [9] <author> M. A. Hernandez. </author> <title> A Generalization of Band-Joins and the Merge/Purge Problem. </title> <type> Technical Report CUCS-005-1995, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The differences from this previous work and ours in the use of a complex function (the equational theory) to determine if records under consideration "match", and our concern for the accuracy of the computed result since matching records may not appear within a common "band". In <ref> [9] </ref>, we describe the sorted-neighborhood method as a generalization of band joins and provide an alternative algorithm for the sorted-neighborhood method based on the duplicate elimination algorithm described in [3].
Reference: [10] <author> W. Kent. </author> <title> The Breakdown of the Information Model in Multi-Database Systems. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 20(4) </volume> <pages> 10-15, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In this paper we study a familiar instance of the semantic integration problem <ref> [10] </ref> or the instance identification problem [14], called the merge/purge problem. Here we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. For instance, one month is a typical business cycle in certain direct marketing operations.
Reference: [11] <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical changes, to complete change of last names and addresses. When setting the parameters for the kind of typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [11] </ref>. <p> Since misspellings are introduced by the database generator, we explored the possibility of improving the results by running a spelling correction program over some fields. Spelling correction algorithms have received a large amount of attention for decades <ref> [11] </ref>. Most of the spelling correction algorithms we considered use a corpus of correctly spelled words from which the correct spelling is selected.
Reference: [12] <author> D. P. Miranker, B. Lofaso, G. Farmer, A. Chandra, and D. </author> <title> Brant. On a TREAT-based Production System Compiler. </title> <booktitle> In Proc. 10th Int'l Conf. on Expert Systems, </booktitle> <pages> pages 617-630, </pages> <year> 1990. </year>
Reference-contexts: We chose to use string data in this study (e.g., names, addresses) for pedagogical reasons (after all everyone gets "faulty" 2 At the time the system was built, the public domain OPS5 compiler was simply too slow for our experimental purposes. Another OPS5C compiler <ref> [12] </ref> was not available to us in time for these studies. The OPS5C compiler produces code that is reportedly many times faster than previous compilers. We captured this speed advantage for our study here by hand recoding our rules in C. junk mail).
Reference: [13] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 233-242, </pages> <year> 1994. </year>
Reference-contexts: In this case, at least three passes would be needed, one pass for conditioning the data and preparing keys, at least a second pass, likely more, for a high speed sort like, for example, the AlphaSort <ref> [13] </ref>, and a final pass for window processing and application of the rule program for each record entering the sliding window. Depending upon the complexity of the rule program, the last pass may indeed be the dominant cost. <p> Of course, doubling the speed of the workstations and utilizing the various RAID-based striping optimizations to double disk I/O speeds discussed in <ref> [13] </ref> and elsewhere (which is certainly possible today since the HP processors and disks used here are slow compared to, for example, Alpha workstations with modern RAID-disk technology) would produce a total time that is at least half the estimated time, i.e. within 3-4 days. 5 Conclusion The sorted neighborhood method
Reference: [14] <author> Y. R. Wang and S. E. Madnick. </author> <title> The Inter-Database Instance Identification Problem in Integrating Autonomous Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: 1 Introduction In this paper we study a familiar instance of the semantic integration problem [10] or the instance identification problem <ref> [14] </ref>, called the merge/purge problem. Here we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. For instance, one month is a typical business cycle in certain direct marketing operations.
References-found: 14

