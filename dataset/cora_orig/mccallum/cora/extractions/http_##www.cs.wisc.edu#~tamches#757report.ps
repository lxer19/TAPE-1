URL: http://www.cs.wisc.edu/~tamches/757report.ps
Refering-URL: http://www.cs.wisc.edu/~tamches/tamches.html
Root-URL: 
Title: Techniques and Tools for Distributed Shared Memory Performance Improvement  
Author: Mark D. Callaghan and Ariel M. Tamches 
Note: 1.0 Introduction  
Address: Wisconsin, Madison  
Affiliation: Department of Computer Sciences University of  
Abstract: Distributed shared memory (DSM) systems hide the details of communication for parallel applications by providing a shared virtual memory space kept coherent via message passing. Although easy to program, performance is poor when memory contention is high. Performance can be improved by detecting and predicting future memory access patterns, such as migratory, producer-consumer, and grouped reads. These patterns are interesting because mechanisms exist to improve their performanceprovided the patterns can be detected. We have investigated run-time detection techniques that require no input from the programmer and no source code modifications. The performance improvement from these techniques has several fundamental limitations, which may indicate that automated techniques are not the best approach for improving DSM performance. We have also developed powerful and exible tools to visualize the behavior of DSM programs on Stache, a fine-grained DSM system running on a 32-node CM-5. We used these tools to analyze the behavior of several large parallel programs. DSM systems allow distributed memory message passing machines to be programmed without worrying about communication details [Tem94]. For the purposes of this paper, a DSM system maintains shared memory in software on distributed memory message passing machines. Two consequences of hiding communication details are a simpler programming model, and a more complex performance tuning model. The programming model for DSM applications is simpler than the model for message passing applications because communication is implicit in DSM applications. The implicit management of communication provided by a DSM system has a performance cost associated with it. There are two aspects to the performance cost. First, there is a communication overhead with respect to the amount of communication required by a message passing application for a similar exchange of information. Second, the programmer loses the opportunity to explicitly schedule long latency communication operations. The overhead for a DSM application is the amount of communication required to exchange information, less the amount of communication required for a similar operation using a message passing system. This overhead can be viewed at a microscopic level by counting the number of messages to share a variable that has been updated. A macroscopic view can compare 
Abstract-found: 1
Intro-found: 1
Reference: [Ben90] <author> J. K. Bennett, J. B. Carter and W. Zwaenepoel, </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures, </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Analysis tools must deal with an information overload, and be able to present results for hundreds of thousands of blocks simultaneously. New protocols are difficult to simulate without a DSM system that is maintained in software. Munin provides cache coherence protocols optimized for specific memory access patterns <ref> [Ben90] </ref>. Munin supports granularity at the size of a virtual memory page. The large size used for coherence may obsure memory access patterns. Migratory and producer-consumer patterns were among the types described. SM-prof graphically presents summaries of cache coherence events [Smp95].
Reference: [Cox93] <author> A. L. Cox and R. J. Fowler, </author> <title> Adaptive Cache Coherency for Detecting Migratory Shared Data, </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Archi tecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference: [Cul93] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> Parallel Programming in Split-C, </title> <booktitle> Proceedings of Supercomputing 93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Access control is enforced by modifying the ECC bits of the memory in which the blocks of shared memory reside. A 32 node CM-5 served as the distributed memory message passing machine. Two applications, appbt [NAS91] and em3d <ref> [Cul93] </ref>, were used to test run time memory access pattern detection and to generate traces. Appbt and em3d were interesting in that application specific protocols [Fal94] have been written that significantly improve their performance compared to a naive DSM version.
Reference: [DDS94] <author> F. Dahlgren, M. Dubois, and P. Stenstrm, </author> <title> Combined Performance Gains of Simple Cache Protocol Extensions, </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The schemes do not avoid the write penalty for blocks when the migratory pattern exists on a subset of the processors that access the block. Protocol extensions that adapt to migratory, producer-consumer, and sequential memory access patterns are described in <ref> [DDS94] </ref>. The applications used were more amenable to optimization than the applications that we used. The prefetching scheme is limited to prefetching contiguous cache blocks.
Reference: [Egg90] <author> S. J. Eggers and R. H. Katz, </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1988. </year>
Reference: [Fal94] <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood, </author> <title> Application-Specific Protocols for User-Level Shared Memory, </title> <booktitle> Proceed ings of Supercomputing 94, </booktitle> <month> November, </month> <year> 1994. </year>
Reference-contexts: Naive DSM systems perform communication operations as a result of access faults due to loads and stores, which will be performed serially, rather than in parallel. The process of improving the performance of applications for a DSM system is described in <ref> [Fal94] </ref>. The authors developed application specific protocols and were able to get the DSM applications to run as fast as message passing versions. The application specific protocols reduced the DSM overhead by using message passing codes. This optimization also scheduled the long latency operations to occur in parallel. <p> A 32 node CM-5 served as the distributed memory message passing machine. Two applications, appbt [NAS91] and em3d [Cul93], were used to test run time memory access pattern detection and to generate traces. Appbt and em3d were interesting in that application specific protocols <ref> [Fal94] </ref> have been written that significantly improve their performance compared to a naive DSM version. The optimized applications provide a much better standard for performance comparison than the unoptimized versions. <p> What is happening here? appbt+busy is a naive DSM implementation of a program with tremendously high contention for an array of shared busy-wait lock variables. Clearly, the sequence of six pixels represents that array. appbt+sigwait was developed by <ref> [Fal94] </ref> to alleviate the DSM thrashing of appbt+busy using signal and wait queue primitives; a node successfully completing an update signals the node waiting at the head of the queue (using Tempest active messages), who then proceeds with its calculations. Notice two striking aspects to this display.
Reference: [Hill93] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D.A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Sys tems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November, </month> <year> 1993. </year>
Reference-contexts: As another example, noting a high rate of invalidations of read-write blocks on some node and high values of read misses on another might lead to speculation that a checkin operation <ref> [Hill93] </ref> should be performed when done writing certain blocks. (We would also like to know which blocks are the ones we should concentrate our study on; such a tool is discussed in the next section.) While CM-5 application is running, Barchart conceptually updates values for each node every time a new <p> Acquiring exclusive block access while handling Stache Read Misses could avoid these faults. There are also a high number of invalidations for blocksboth shared-mode and exclusive-mode. We cannot avoid these events without an update protocol or source code hints such as checkin/checkout <ref> [Hill93] </ref>. Barchart (left) clearly reveals a producer-consumer sharing pattern in em3d. The home nodes for producer blocks are the only writers to them. The high number of coherence events is due to three factors. <p> High block miss rates can occur for several reasons; typical cases include contention for synchronization variables, migratory data sharing patterns, and producer/consumer data sharing patterns. DSM assistance can take the form of prefetching (in read-only or exclusive mode), acquiring exclusive access on a Read Miss, and checkin <ref> [Hill93] </ref>. The Hotblocks visualization tool provides a framework for identifying which blocks have high miss rates. This contrasts with the Barchart visualization tool of the previous section, which provides a framework for identifying which nodes have high miss rates.
Reference: [Hol94] <author> J. K. Hollingsworth, B. P. Miller, and J. Cargille. </author> <title> Dynamic Program Instrumentation for Scalable Performance Tools. </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May, </month> <year> 1994. </year>
Reference-contexts: As presently defined, replay files will not scale; we cannot afford to continue appending entries for each time interval, because long-running programs would create arbitrarily growing replay logs. We borrowed a trick from the Paradyn system <ref> [Hol94] </ref> to solve this problem. A fixed-size (say, 5000-entry) replay log is allocated for each node. When a nodes replay log fills up, we fold it.
Reference: [NAS91] <author> D. Bailey, J. Barton, T. Lasinski and H. Simon, </author> <title> The NAS Parallel Benchmarks, </title> <type> Tech nical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Stache is implemented using Blizzard-E, an implementation of the Tempest interface for a CM-5. Access control is enforced by modifying the ECC bits of the memory in which the blocks of shared memory reside. A 32 node CM-5 served as the distributed memory message passing machine. Two applications, appbt <ref> [NAS91] </ref> and em3d [Cul93], were used to test run time memory access pattern detection and to generate traces. Appbt and em3d were interesting in that application specific protocols [Fal94] have been written that significantly improve their performance compared to a naive DSM version.
Reference: [Smp95] <author> M. Brorsson, SM-prof: </author> <title> A Tool to Visualise and Find Cache Coherence Performance Bottlenecks in Multiprocessor Programs, </title> <booktitle> To appear in ACM Sigmetrics and Performance 95. </booktitle> <pages> 20 </pages>
Reference-contexts: Munin supports granularity at the size of a virtual memory page. The large size used for coherence may obsure memory access patterns. Migratory and producer-consumer patterns were among the types described. SM-prof graphically presents summaries of cache coherence events <ref> [Smp95] </ref>. Some types of access patterns can be inferred from the visualizations. However, per cache block information is not provided, and access patterns that apply to a subset of cache blocks may be obscured.
Reference: [Ste93] <author> P. Stenstrm, M. Brorsson, and L. Sandberg, </author> <title> An Adaptive Cache Coherence Protocol Optimized for Migratory Sharing, </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The cache block sizes were 32 for em3d and 128 for appbt. The results and schemes are described below. Original is the behavior for an unmodified version of Stache. 5.1.1 Migratory-s The scheme used to detect migratory access is described in <ref> [Ste93] </ref>.
Reference: [Sto95] <author> T. M. Chilimbi, T. Ball, S. G. Eick, J. R. Larus, StormWatch: </author> <title> A Tool for Visualizing Memory System Protocols, </title> <note> Submitted for publication, Supercomputing 95. </note>
Reference-contexts: However, per cache block information is not provided, and access patterns that apply to a subset of cache blocks may be obscured. StormWatch displays event traces, and makes the trace display manageable by filtering events that a user is not interested in <ref> [Sto95] </ref>. However, patterns may only be interesting when they occur thousands of times and such a pattern may be difficult to see. Counts of events may be more suitable for some problems.
Reference: [Tem94] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood, Tempest and Typhoon: </author> <title> User-Level Shared Memory, </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: 1.0 Introduction DSM systems allow distributed memory message passing machines to be programmed without worrying about communication details <ref> [Tem94] </ref>. For the purposes of this paper, a DSM system maintains shared memory in software on distributed memory message passing machines. Two consequences of hiding communication details are a simpler programming model, and a more complex performance tuning model. <p> An application using our optimizations would still perform twice as slow as a message passing application with respect to communication. 4.0 Methodology We used Stache to provide sequentially-consistent shared memory <ref> [Tem94] </ref>. It uses a write-invalidate protocol and supports cache coherency for blocks of 32 bytes or larger. Stache is implemented using Blizzard-E, an implementation of the Tempest interface for a CM-5. <p> The optimized applications provide a much better standard for performance comparison than the unoptimized versions. The applications were also interesting because they displayed different memory access patterns which will be described. 5.0 Run Time Detection An existing cache coherence protocol, Stache <ref> [Tem94] </ref>, was extended to detect migratory access patterns, producer-consumer access patterns, and grouped reads. It was also modified to issue prefetch requests and to support the acquisition of blocks with exclusive access while handling read misses.
References-found: 13

