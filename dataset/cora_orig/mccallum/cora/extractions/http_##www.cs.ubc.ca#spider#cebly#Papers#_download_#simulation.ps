URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/simulation.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: email: cheuk,cebly@cs.ubc.ca  
Title: Structured Arc Reversal and Simulation of Dynamic Probabilistic Networks  
Author: Adrian Y. W. Cheuk and Craig Boutilier 
Address: Vancouver, BC, CANADA, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Date: July, 1997  
Note: To appear, Proc. Thirteenth Conf. on Uncertainty in AI (UAI-97),Providence,  
Abstract: We present an algorithm for arc reversal in Bayesian networks with tree-structured conditional probability tables, and consider some of its advantages, especially for the simulation of dynamic probabilistic networks. In particular, the method allows one to produce CPTs for nodes involved in the reversal that exploit regularities in the conditional distributions. We argue that this approach alleviates some of the overhead associated with arc reversal, plays an important role in evidence integration and can be used to restrict sampling of variables in DPNs. We also provide an algorithm that detects the dynamic irrelevance of state variables in forward simulation. This algorithm exploits the structured CPTs in a reversed network to determine, in a time-independent fashion, the conditions under which a variable does or does not need to be sampled.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1104-1111, Montreal. </address>
Reference-contexts: We will focus on decision trees in this paper. We do not delve further into the details of CSI or the use of tree-structured CPTs in general (see [2] for further details). We do note that tree-structured CPTs and CSI have been exploited in decision-making <ref> [1] </ref>, knowledge ac quisition [9] and learning [6]. The integration of CSI with other well-known BN methods promises to make it even more pervasive. <p> DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models [11], control problems such as robot or vehicle monitoring and control [12, 10], planning [5] and sequential decision problems <ref> [18, 1] </ref>. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory. This is illustrated schematically in Figure 6 (following [10]).
Reference: [2] <author> C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <address> UAI-96, pp.115-123, Portland, OR. </address>
Reference-contexts: 1 Introduction Recent investigations have explored the extension of the types of independence that can be represented in Bayesian networks (BNs). Specifically, the conditional independence of variables given a certain context (or instantiation of variables) has been proposed as a way of making BN specification and inference more tractable <ref> [8, 15, 2] </ref>. This context-specific independence (CSI) is often represented by the use of structured representations of the conditional probability tables (CPTs) for the network. <p> This context-specific independence (CSI) is often represented by the use of structured representations of the conditional probability tables (CPTs) for the network. While a variable is directly dependent on all of its parents, structured CPT representations, such as decision trees <ref> [2] </ref> or rules [15], capture the fact that (direct) dependence on certain parents does not hold given particular instantiations of others. <p> Let X ; Y ; Z; C be pairwise disjoint sets of variables. We say X and Y are contextually independent <ref> [2] </ref> given Z and the context c 2 val (C) if P (X j Z; c; Y ) = P (X j Z; c) whenever P (Y ; Z; c) &gt; 0: Thus, the independence relation between X and Y need not hold for all values val (C). <p> In this example, the CPT for A is encoded with 5 distinct entries rather than the 16 required by the usual tabular representation. We note that simple extensions of d-separation can be used to find global CSI relations <ref> [2] </ref>. It is suggested in [2] that CPTs can be encoded using appropriate compact function representations that make explicit such local CSI relations. We will focus on decision trees in this paper. <p> In this example, the CPT for A is encoded with 5 distinct entries rather than the 16 required by the usual tabular representation. We note that simple extensions of d-separation can be used to find global CSI relations <ref> [2] </ref>. It is suggested in [2] that CPTs can be encoded using appropriate compact function representations that make explicit such local CSI relations. We will focus on decision trees in this paper. We do not delve further into the details of CSI or the use of tree-structured CPTs in general (see [2] for further details). <p> It is suggested in <ref> [2] </ref> that CPTs can be encoded using appropriate compact function representations that make explicit such local CSI relations. We will focus on decision trees in this paper. We do not delve further into the details of CSI or the use of tree-structured CPTs in general (see [2] for further details). We do note that tree-structured CPTs and CSI have been exploited in decision-making [1], knowledge ac quisition [9] and learning [6]. The integration of CSI with other well-known BN methods promises to make it even more pervasive.
Reference: [3] <author> C. Boutilier and M. Goldszmidt. </author> <title> The frame problem and Bayesian network action representations. </title> <booktitle> Proc. 11th Cana-dian Conf. on AI, </booktitle> <address> pp.69-83, Toronto, </address> <year> 1996. </year>
Reference-contexts: Of the total 233 CPT entries represented, only 210 required explicit computation. Our experiments with other similar DPNs suggest that this savings is commonplace. This seems especially true in the evaluation of policies, where actions or decisions play a predominant role. As argued in <ref> [3] </ref>, the representation of action effects often admits a considerable amount of CSI. Apart from the potential savings it provides during network restructuring, another advantage offered by TSAR is the ability to determine irrelevant variables dynamically. Irrelevance can be viewed at the network level.
Reference: [4] <author> R. E. Bryant. </author> <title> Graph-based algorithms for boolean function manipulation. </title> <journal> IEEE Trans. Comp., </journal> <volume> C-35(8):677-691, </volume> <year> 1986. </year>
Reference-contexts: This structure describes the dependence of each variable on other variables within the same or previous slice. Essentially, these are directed acyclic graphs generated from the CPTs for the variables in question, and are similar to binary decision diagrams (BDDs) <ref> [4] </ref>. The sample graphs for the seven variables are shown in Figure 9. <p> Advantages include the reduction in (space and computational) overhead for reversal, and the ple graphs can use some of the efficient procedures designed for BDD manipulation <ref> [4] </ref>. Furthermore, this process can be terminated early if we ever find that the irrelevant condition for a variable with respect to any graph is false, or if the conjunction of conditions for any (incremental) subset of the graphs is inconsistent: the variable must then be sampled no matter what.
Reference: [5] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: In particular, the problems associated with increasing the number of parents a node hasa fact that makes reversal sometimes problematicis mitigated by the use of structured CPTS. We describe the relevance of our approach to stochastic simulation of dynamic probabilistic networks (DPNs) <ref> [5, 11, 10] </ref>. DPNs form an important class of BNs for modeling dynamical systems and sequential decision processes. Because of their size, exact methods are often rejected in favor of simulation techniques. <p> for A to be expressed with only 30 distinct entries, and requires that Equation 2 be evaluated only 20 times (i.e., at the leaves of marked subtrees). 6 4 TSAR in the Simulation of DPNs Dynamic probabilistic networks (DPNs) are a particular form of BN used to model temporally-extended systems <ref> [5, 11, 10] </ref>. Intuitively, we imagine a number of state variables whose values vary over time, allowing the network to be organized in slices consisting of a set of variables at a particular point in time. <p> DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models [11], control problems such as robot or vehicle monitoring and control [12, 10], planning <ref> [5] </ref> and sequential decision problems [18, 1]. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory. This is illustrated schematically in Figure 6 (following [10]).
Reference: [6] <author> N. Friedman and M. Goldszmidt. </author> <title> Learning Bayesian networks with local structure. </title> <address> UAI-96, pp.252-262, Portland, OR. </address>
Reference-contexts: We do not delve further into the details of CSI or the use of tree-structured CPTs in general (see [2] for further details). We do note that tree-structured CPTs and CSI have been exploited in decision-making [1], knowledge ac quisition [9] and learning <ref> [6] </ref>. The integration of CSI with other well-known BN methods promises to make it even more pervasive.
Reference: [7] <author> R. Fung and K. Chang. </author> <title> Weighing and integrating evidence for stochastic simulation in bayesian networks. </title> <address> UAI-89, pp.209-219, Windsor. </address>
Reference-contexts: DPNs form an important class of BNs for modeling dynamical systems and sequential decision processes. Because of their size, exact methods are often rejected in favor of simulation techniques. In the case of DPNs, arc reversal or evidence integration <ref> [7] </ref> is extremely important; this case has been made forcefully [10]. However, even partial evidence integration can cause a large blowup in the size of CPTs; hence structured arc reversal can play an important role. <p> Arc reversal is an important technique for BNs and influence diagrams, and plays a significant role in the evaluation of BNs through stochastic simulation <ref> [7, 10] </ref>, as we describe in the next section. The basic arc reversal operation is relatively straightforward. Consider a network where variable A is a parent of O. <p> Because of the size of DPNs, exact solution of a DPN is impractical in most settings. Therefore simulation models are often preferred. However, traditional methods such as likelihood weighting <ref> [17, 7] </ref> will be extremely unsuitable in DPNs exhibiting the schematic structure of Figure 6. Because they are sinks in the network, the sensor variables (which provide the only evidence) are unable to influence the course of the simulation. <p> The reversal is only partial, however, since sensor variables in the reversed network will generally have as parents state variables from slice t 1. 7 Unfortunately, evidence integration can be expensive. Indeed, Fung and Chang <ref> [7] </ref> suggest that, while evidence integration can help convergence of simulation methods tremendously, the computational cost of arc reversal may prove to be a practical obstacle to its applicability. <p> Given such a specific query, a simulation trial need not sample F t1 since this cannot impact A t . The irrelevance of F t1 to A t is dictated by the structure of the DPN. Indeed, Fung and Chang <ref> [7] </ref> propose irrelevance of this type as a means of speeding simulation, though they caution that the overhead involved may offset any savings. 8 8 See also [14] for a discussion of this type of relevance. To appear, Proc.
Reference: [8] <author> D. Geiger and D. Heckerman. </author> <booktitle> Advances in probabilistic reasoning. </booktitle> <address> UAI-91, pp.118-126, Los Angeles. </address>
Reference-contexts: 1 Introduction Recent investigations have explored the extension of the types of independence that can be represented in Bayesian networks (BNs). Specifically, the conditional independence of variables given a certain context (or instantiation of variables) has been proposed as a way of making BN specification and inference more tractable <ref> [8, 15, 2] </ref>. This context-specific independence (CSI) is often represented by the use of structured representations of the conditional probability tables (CPTs) for the network.
Reference: [9] <author> S. Glesner and D. Koller. </author> <title> Constructing flexible dynamic belief networks from first-order probabilistic knowledge bases. </title> <address> ECSQARU '95, pp.217-226. </address>
Reference-contexts: We will focus on decision trees in this paper. We do not delve further into the details of CSI or the use of tree-structured CPTs in general (see [2] for further details). We do note that tree-structured CPTs and CSI have been exploited in decision-making [1], knowledge ac quisition <ref> [9] </ref> and learning [6]. The integration of CSI with other well-known BN methods promises to make it even more pervasive.
Reference: [10] <author> K. Kanazawa, D. Koller, and S. Russell. </author> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.346-351, Montreal. </address>
Reference-contexts: In particular, the problems associated with increasing the number of parents a node hasa fact that makes reversal sometimes problematicis mitigated by the use of structured CPTS. We describe the relevance of our approach to stochastic simulation of dynamic probabilistic networks (DPNs) <ref> [5, 11, 10] </ref>. DPNs form an important class of BNs for modeling dynamical systems and sequential decision processes. Because of their size, exact methods are often rejected in favor of simulation techniques. <p> DPNs form an important class of BNs for modeling dynamical systems and sequential decision processes. Because of their size, exact methods are often rejected in favor of simulation techniques. In the case of DPNs, arc reversal or evidence integration [7] is extremely important; this case has been made forcefully <ref> [10] </ref>. However, even partial evidence integration can cause a large blowup in the size of CPTs; hence structured arc reversal can play an important role. We also show how the reversed DPNs can exploit the structured CPTs in simulation through the detection of irrelevance of variables dynamically. <p> Arc reversal is an important technique for BNs and influence diagrams, and plays a significant role in the evaluation of BNs through stochastic simulation <ref> [7, 10] </ref>, as we describe in the next section. The basic arc reversal operation is relatively straightforward. Consider a network where variable A is a parent of O. <p> for A to be expressed with only 30 distinct entries, and requires that Equation 2 be evaluated only 20 times (i.e., at the leaves of marked subtrees). 6 4 TSAR in the Simulation of DPNs Dynamic probabilistic networks (DPNs) are a particular form of BN used to model temporally-extended systems <ref> [5, 11, 10] </ref>. Intuitively, we imagine a number of state variables whose values vary over time, allowing the network to be organized in slices consisting of a set of variables at a particular point in time. <p> DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models [11], control problems such as robot or vehicle monitoring and control <ref> [12, 10] </ref>, planning [5] and sequential decision problems [18, 1]. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory. <p> We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory. This is illustrated schematically in Figure 6 (following <ref> [10] </ref>). We note that the set of state and sensor variables need not be disjoint, and that state variables could include decision variables whose values are set by the controller (possibly depending on the values of previous state variables). <p> Because they are sinks in the network, the sensor variables (which provide the only evidence) are unable to influence the course of the simulation. As demonstrated convincingly by Kanazawa, Koller and Russell <ref> [10] </ref>, straightforward simulation will often get off track very quickly, leading to trials with negligible (or zero) weight. They suggest the use of (partial) evidence integration in order to keep the simulation close to reality.
Reference: [11] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <address> UAI-92, pp.121-129, Stan-ford. </address>
Reference-contexts: In particular, the problems associated with increasing the number of parents a node hasa fact that makes reversal sometimes problematicis mitigated by the use of structured CPTS. We describe the relevance of our approach to stochastic simulation of dynamic probabilistic networks (DPNs) <ref> [5, 11, 10] </ref>. DPNs form an important class of BNs for modeling dynamical systems and sequential decision processes. Because of their size, exact methods are often rejected in favor of simulation techniques. <p> for A to be expressed with only 30 distinct entries, and requires that Equation 2 be evaluated only 20 times (i.e., at the leaves of marked subtrees). 6 4 TSAR in the Simulation of DPNs Dynamic probabilistic networks (DPNs) are a particular form of BN used to model temporally-extended systems <ref> [5, 11, 10] </ref>. Intuitively, we imagine a number of state variables whose values vary over time, allowing the network to be organized in slices consisting of a set of variables at a particular point in time. <p> DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models <ref> [11] </ref>, control problems such as robot or vehicle monitoring and control [12, 10], planning [5] and sequential decision problems [18, 1]. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables.
Reference: [12] <author> A. E. Nicholson and J. M. Brady. </author> <title> Sensor validation using dynamic belief networks. </title> <publisher> UAI-92, pp.207-214, Stanford. </publisher>
Reference-contexts: DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models [11], control problems such as robot or vehicle monitoring and control <ref> [12, 10] </ref>, planning [5] and sequential decision problems [18, 1]. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory.
Reference: [13] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: These assertions are local in that they refer specifically to a node and its parents in the graph. Additional conditional independence relations of a more global nature can be determined efficiently using the graphical criterion of d-separation <ref> [13] </ref>. To represent the distribution P , we need only, in addition to the graph, specify for each variable X i , a conditional probability table (CPT) encoding P (x i j (X i )) for each possible value of the variables in fX i ; (X i )g. (See [13] <p> <ref> [13] </ref>. To represent the distribution P , we need only, in addition to the graph, specify for each variable X i , a conditional probability table (CPT) encoding P (x i j (X i )) for each possible value of the variables in fX i ; (X i )g. (See [13] for details.) Apart from the usual strong independence relations encoded in BNs, we are often interested in independence between variables that holds only in certain contexts. Let X ; Y ; Z; C be pairwise disjoint sets of variables.
Reference: [14] <author> K. Poh and E. Horvitz. </author> <title> A graph-theoretic analysis of information value. </title> <address> UAI-96, pp.427-435, Portland, OR. </address>
Reference-contexts: The irrelevance of F t1 to A t is dictated by the structure of the DPN. Indeed, Fung and Chang [7] propose irrelevance of this type as a means of speeding simulation, though they caution that the overhead involved may offset any savings. 8 8 See also <ref> [14] </ref> for a discussion of this type of relevance. To appear, Proc. Thirteenth Conf. on Uncertainty in AI (UAI-97),Providence, July, 1997 We focus on a specific problem: assume a DPN has been given and that a certain subset of state variables has been designated as immediately relevant.
Reference: [15] <author> D. Poole. </author> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artif. Intel., </journal> <volume> 64(1) </volume> <pages> 81-129, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Recent investigations have explored the extension of the types of independence that can be represented in Bayesian networks (BNs). Specifically, the conditional independence of variables given a certain context (or instantiation of variables) has been proposed as a way of making BN specification and inference more tractable <ref> [8, 15, 2] </ref>. This context-specific independence (CSI) is often represented by the use of structured representations of the conditional probability tables (CPTs) for the network. <p> This context-specific independence (CSI) is often represented by the use of structured representations of the conditional probability tables (CPTs) for the network. While a variable is directly dependent on all of its parents, structured CPT representations, such as decision trees [2] or rules <ref> [15] </ref>, capture the fact that (direct) dependence on certain parents does not hold given particular instantiations of others.
Reference: [16] <author> R. D. Shachter. </author> <title> Evaluating influence diagrams. Op. </title> <journal> Res., </journal> <volume> 33(6) </volume> <pages> 871-882, </pages> <year> 1986. </year>
Reference-contexts: In this paper, we develop a version of the arc reversal algorithm for networks with tree-structured CPTs. Arc reversal <ref> [16] </ref> is an important technique for manipulating BNs, and our approach demonstrates that structured CPTs can be exploited considerably. This allows smaller CPTs to be produced with less computational effort, and produces reversed networks that retain substantial structure in their CPTs; this structure can then be exploited in inference. <p> The integration of CSI with other well-known BN methods promises to make it even more pervasive. We now consider one such combination of tree-structured CPTs with a BN manipulation algorithm. 3 Tree-Structured Arc Reversal 3.1 Arc Reversal with Unstructured CPTs Arc reversal <ref> [16] </ref> is a technique for restructuring a BN so that the arc between two nodes has its directionality reversed, while still correctly representing the original distribution. <p> The structure of resulting network is illustrated in Figure 1 (b). We use the notations old (A) and new (A) to refer to A's parents before and after reversal, respectively (similarly for O). The expressions for the new CPT entries are <ref> [16] </ref>: P (Ojx; y; z) = a2val (A) P (Ajx; y; z; O) = P (Ojx; y; z) Note that each term in Equation 1 is in an original CPT, as are the terms in the numerator of Equation 2, while the denominator is simply an entry in the new CPT
Reference: [17] <author> R. D. Shachter and M. A. Peot. </author> <title> Simulation approaches to general probabilistic inference in belief networks. </title> <address> UAI-89, pp.221-231, Windsor. </address>
Reference-contexts: Because of the size of DPNs, exact solution of a DPN is impractical in most settings. Therefore simulation models are often preferred. However, traditional methods such as likelihood weighting <ref> [17, 7] </ref> will be extremely unsuitable in DPNs exhibiting the schematic structure of Figure 6. Because they are sinks in the network, the sensor variables (which provide the only evidence) are unable to influence the course of the simulation.
Reference: [18] <author> J. A. Tatman and R. D. Shachter. </author> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Trans. Sys., Man and Cyber., </journal> <volume> 20(2) </volume> <pages> 365-379, </pages> <year> 1990. </year>
Reference-contexts: DPNs can be used to model dynamical systems generally, and specifically can be applied to time series models [11], control problems such as robot or vehicle monitoring and control [12, 10], planning [5] and sequential decision problems <ref> [18, 1] </ref>. We often distinguish certain variables within a particular slice as state variables, and others as sensor variables. It is generally only sensor variables that are observable and provide evidence of the system's trajectory. This is illustrated schematically in Figure 6 (following [10]).
References-found: 18

