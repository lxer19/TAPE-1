URL: http://www.cs.huji.ac.il/labs/learning/Papers/psa_ml.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Email: danar@cs.huji.ac.il  singer@cs.huji.ac.il NAFTALI TISHBY tishby@cs.huji.ac.il  
Title: The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length  
Author: DANA RON YORAM SINGER 
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science, Hebrew University,  
Abstract: We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata. The learning algorithm is motivated by real applications in man-machine interaction such as handwriting and speech recognition. Conventionally used fixed memory Markov and hidden Markov models have either severe practical or theoretical drawbacks. Though general hardness results are known for learning distributions generated by sources with similar structure, we prove that our algorithm can indeed efficiently learn distributions generated by our more restricted sources. In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time and sample complexity. We present two applications of our algorithm. In the first one we apply our algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth <ref> [1] </ref> study the problem of training HMMs. The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. <p> Probabilistic Finite Automata A Probabilistic Finite Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Q fi ! Q is the transition function, fl : Q fi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. The functions fl and must satisfy the following conditions: for every q 2 Q, P q2Q (q) = 1. <p> Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Q fi ! Q is the transition function, fl : Q fi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. The functions fl and must satisfy the following conditions: for every q 2 Q, P q2Q (q) = 1. <p> The nodes of the tree are labeled by pairs (s; fl s ) where s is the string associated with the `walk' starting from that node and ending in the root of the tree, and fl s : ! <ref> [0; 1] </ref> is the next symbol probability function related with s. We require that for every string s labeling a node in the tree, P 2 fl s () = 1.
Reference: 2. <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Similar definitions can be considered for other distance measures such as the variation and the quadratic distances. Note that the KL-divergence bounds the variation distance as follows <ref> [2] </ref>: D KL [P 1 jjP 2 ] 1 2 1 . Since the L 1 norm bounds the L 2 norm, the last bound holds for the quadratic distance as well.
Reference: 3. <author> A.P. Dempster, N.M Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc., </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: Dana Ron would like to thank the support of the Eshkol fellowship. Yoram Singer would like to thank the Clore Foundation for its support. Notes 1. A common maximum likelihood estimation procedure for HMMs is based on the EM algorithm <ref> [3] </ref>. The algorithm is guaranteed to converge to a local maximum. There is not any known algorithm which converges to a global maximum. 2.
Reference: 4. <author> J.A. Fill. </author> <title> Eigenvalue bounds on convergence to stationary for nonreversible Markov chains, with an application to exclusion process. </title> <journal> Annals of Applied Probability, </journal> <volume> 1 </volume> <pages> 62-87, </pages> <year> 1991. </year>
Reference-contexts: We show that this convergence rate can be bounded using the expansion properties of a weighted graph related to U M [13] or more generally, using algebraic properties of U M , namely, its second largest eigenvalue <ref> [4] </ref>. 4. Emulation of PSAs by PSTs In this section we show that for every PSA there exists an equivalent PST which is not much larger. This allows us to consider the PST equivalent to our target PSA, whenever it is convenient.
Reference: 5. <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1993. </year>
Reference-contexts: Hoffgen [7] studies families of distributions related to the ones studied in this paper, but his algorithms depend exponentially and not polynomially on the order, or memory length, of the distributions. Freund et. al. <ref> [5] </ref> point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard.
Reference: 6. <author> D. Gillman and M. Sipser. </author> <title> inference and minimization of hidden markov chains. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-158, </pages> <year> 1994. </year>
Reference-contexts: The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. Gill-man and Sipser <ref> [6] </ref> study the problem of exactly inferring an (ergodic) HMM over a binary alphabet when the inference algorithm can query a probability oracle for the long-term probability of any binary string. They prove that inference is hard: any algorithm for inference must make exponentially many oracle calls.
Reference: 7. <author> K.-U. Hoffgen. </author> <title> Learning and robust learning of product distributions. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-106, </pages> <year> 1993. </year>
Reference-contexts: As noted earlier, the size of full order L models 4 DANA RON, YORAM SINGER, NAFTALI TISHBY is exponential in L and hence, if we want to capture more than very short term memory dependencies in the sequences, then these models are clearly not practical. Hoffgen <ref> [7] </ref> studies families of distributions related to the ones studied in this paper, but his algorithms depend exponentially and not polynomially on the order, or memory length, of the distributions.
Reference: 8. <author> F. Jelinek. </author> <title> A fast sequential decoding algorithm using a stack. </title> <journal> IBM J. Res. Develop., </journal> <volume> 13 </volume> <pages> 675-685, </pages> <year> 1969. </year>
Reference-contexts: Moreover, the maximization of Equation (10a) can be performed efficiently by using a dynamic programming (DP) scheme, also known as the Viterbi algorithm [22]. This scheme requires O (jQj fi n) operations. If jQj is large, then approximation schemes to the optimal DP, such as the stack decoding algorithm <ref> [8] </ref> can be employed. Using similar methods it is also possible to correct errors when insertions and deletions of symbols occur as well. We tested the algorithm by taking a text from Jenesis and corrupting it in two ways. First, we altered every letter (including blanks) with probability 0:2.
Reference: 9. <author> F. Jelinek. </author> <title> Markov source modeling of text generation. </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1983. </year>
Reference-contexts: These observations suggests modeling such sequences by Markov chains of order L &gt; 1, (also known as n-gram models <ref> [9] </ref>), where the order is the memory length of the model, or alternatively, by Hidden Markov Models (HMM). These statisti 2 DANA RON, YORAM SINGER, NAFTALI TISHBY cal models capture rich families of sequence distributions and moreover, they give efficient procedures both for generating sequences and for computing their probabilities. <p> In this case the `hidden' part of the HMM is not needed and the additional parameters just complicate the model unnecessarily. Natural simpler alternatives, which are often used as well, are order L Markov chains or n-gram models <ref> [9] </ref>. As noted earlier, the size of full order L models 4 DANA RON, YORAM SINGER, NAFTALI TISHBY is exponential in L and hence, if we want to capture more than very short term memory dependencies in the sequences, then these models are clearly not practical.
Reference: 10. <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1985. </year>
Reference-contexts: 1. Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech <ref> [10] </ref>, [14], and statistical models of biological sequences such as DNA and proteins [12]. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function.
Reference: 11. <author> M. Kearns, Y.Mansour, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> On the learnabil-ity of discrete distributions. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Furthermore, such a hypothesis can be obtained from a single sample sequence if the sequence length is also polynomial in a parameter related to the correlation length (or mixing rate) of the target machine. Despite an intractability result concerning the learnability of distributions generated by Probabilistic Finite Automata <ref> [11] </ref> (described in Section 1.1), our restricted model can be learned in a PAC-like sense efficiently. This has not been shown so far for any of the more popular sequence modeling algorithms. 1.1. <p> Freund et. al. [5] point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard. Kearns et. al. <ref> [11] </ref> show that PFAs are not efficiently learnable under the assumption that there is no efficient algorithm for learning noisy parity functions in the PAC model. <p> Right: A prediction suffix tree. The prediction probabilities of the symbols `0' and `1', respectively, are depicted beside the nodes, in parentheses. 3. The Learning Model The learning model described in this paper is similar in spirit to that introduced in <ref> [11] </ref> in the sense that our approach is also motivated by the PAC model for learning boolean concepts from labeled examples. The two models differ slightly in their assumptions on the input to the learning algorithm and in the requirements from its output.
Reference: 12. <author> A. Krogh, S.I. Mian, and D. Haussler. </author> <title> A hidden markov model that finds genes in E. coli DNA. </title> <type> Technical Report UCSC-CRL-93-16, </type> <institution> University of California at Santa-Cruz, </institution> <year> 1993. </year>
Reference-contexts: The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech [10], [14], and statistical models of biological sequences such as DNA and proteins <ref> [12] </ref>. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function. Namely, the statistical correlation between two symbols, or two subsequences of symbols, which are some distance t apart in the sequence, decreases very rapidly with t. <p> The DNA `alphabet' is composed of four nucleotides denoted by: A,C,T,G. The models were constructed based on 250 different DNA strands from each type, their lengths ranging from 20 bases to several thousands. The PSAs built are rather small compared to the HMM model described in <ref> [12] </ref>: the PSA that models the coding regions has 65 states and the PSA that models the intergenic regions has 81 states. We tested the performance of the models by calculating the log-likelihood of the two models obtained on test data drawn from intergenic regions. <p> The latter property combined with the results mentioned above indicate that the PSA model might be used used when performing tasks such as DNA gene locating. However, we should stress that we have done only a preliminary step in this direction and the results obtained in <ref> [12] </ref> as part of a complete parsing system are better. intergenic regions and a PSA trained on data taken from coding regions. The test data was taken from intergenic regions. In 90% of the cases the likelihood of the first PSA was higher.
Reference: 13. <author> M. Mihail. </author> <title> Conductance and convergence of Markov chains A combinatorial treatment of expanders. </title> <booktitle> In Proceedings 30th Annual Conference on Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: When given one sample string, the given string must be long enough so as to ensure convergence of the probability of visiting a state to the stationary probability. We show that this convergence rate can be bounded using the expansion properties of a weighted graph related to U M <ref> [13] </ref> or more generally, using algebraic properties of U M , namely, its second largest eigenvalue [4]. 4. Emulation of PSAs by PSTs In this section we show that for every PSA there exists an equivalent PST which is not much larger.
Reference: 14. <author> A. Nadas. </author> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Trans. on ASSP, </journal> <volume> 32(4) </volume> <pages> 859-861, </pages> <year> 1984. </year> <type> 22 DANA RON, YORAM SINGER, </type> <institution> NAFTALI TISHBY </institution>
Reference-contexts: 1. Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech [10], <ref> [14] </ref>, and statistical models of biological sequences such as DNA and proteins [12]. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function.
Reference: 15. <author> L.R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Related Work The most powerful (and perhaps most popular) model used in modeling natural sequences is the Hidden Markov Model (HMM) [16], for which there exists a maximum likelihood estimation procedure 1 which is widely used in many applications <ref> [15] </ref>. From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth [1] study the problem of training HMMs. The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs.
Reference: 16. <author> L.R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: This has not been shown so far for any of the more popular sequence modeling algorithms. 1.1. Related Work The most powerful (and perhaps most popular) model used in modeling natural sequences is the Hidden Markov Model (HMM) <ref> [16] </ref>, for which there exists a maximum likelihood estimation procedure 1 which is widely used in many applications [15]. From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth [1] study the problem of training HMMs.
Reference: 17. <author> J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: Kearns et. al. [11] show that PFAs are not efficiently learnable under the assumption that there is no efficient algorithm for learning noisy parity functions in the PAC model. The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in <ref> [17] </ref> and have been used for other tasks such as universal data compression [17], [18], [23], [24]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24]. <p> The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in <ref> [17] </ref> and have been used for other tasks such as universal data compression [17], [18], [23], [24]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24].
Reference: 18. <author> J. Rissanen. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 32(4) </volume> <pages> 526-532, </pages> <year> 1986. </year>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [17] and have been used for other tasks such as universal data compression [17], <ref> [18] </ref>, [23], [24]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24].
Reference: 19. <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: According to Lemma 1, and the bounds computed for all the relevant parameters, this bound is as required in the theorem statement. 7. Applications A slightly modified version of our learning algorithm was applied and tested on various problems such as: correcting corrupted text, predicting DNA bases <ref> [19] </ref>, and part-of-speech disambiguation resolving [21]. We are still exploring the applicative possibilities of the algorithm. Here we demonstrate how to correct corrupted text and how to build a simple model for DNA strands. 7.1.
Reference: 20. <author> K.E. Rudd. </author> <title> Maps, genes, sequences, and computers: An Escherichia coli case study. </title> <journal> ASM News, </journal> <volume> 59 </volume> <pages> 335-341, </pages> <year> 1993. </year>
Reference-contexts: Building A Simple Model for E.coli DNA DNA strands are composed of sequences of protein coding genes and `fillers' between those regions named intergenic regions. Locating the coding genes is necessary, prior to any further DNA analysis. Using manually segmented data of E. coli <ref> [20] </ref> 20 DANA RON, YORAM SINGER, NAFTALI TISHBY we built two different PSAs, one for the coding regions and one for the intergenic regions.
Reference: 21. <author> H. Schutze and Y. Singer. </author> <title> Part-of-Speech tagging using a variable memory Markov model. </title> <booktitle> In Proceedings of ACL 32'nd, </booktitle> <year> 1994. </year>
Reference-contexts: Applications A slightly modified version of our learning algorithm was applied and tested on various problems such as: correcting corrupted text, predicting DNA bases [19], and part-of-speech disambiguation resolving <ref> [21] </ref>. We are still exploring the applicative possibilities of the algorithm. Here we demonstrate how to correct corrupted text and how to build a simple model for DNA strands. 7.1.
Reference: 22. <author> A.J. </author> <title> Viterbi. Error bounds for convulutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <year> 1967. </year>
Reference-contexts: Note that the sum (10c) can be computed efficiently in a recursive manner. Moreover, the maximization of Equation (10a) can be performed efficiently by using a dynamic programming (DP) scheme, also known as the Viterbi algorithm <ref> [22] </ref>. This scheme requires O (jQj fi n) operations. If jQj is large, then approximation schemes to the optimal DP, such as the stack decoding algorithm [8] can be employed. Using similar methods it is also possible to correct errors when insertions and deletions of symbols occur as well.
Reference: 23. <author> M.J. Weinberger, A. Lempel, and J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite-memory sources. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 38 </volume> <pages> 1002-1014, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [17] and have been used for other tasks such as universal data compression [17], [18], <ref> [23] </ref>, [24]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24].
Reference: 24. <author> F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. </author> <title> The context tree weighting method: Basic properties. </title> <journal> IEEE Trans. Inform. Theory, </journal> <note> 1993. Submitted for publication. </note>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [17] and have been used for other tasks such as universal data compression [17], [18], [23], <ref> [24] </ref>. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24]. <p> slightly different form) in [17] and have been used for other tasks such as universal data compression [17], [18], [23], <ref> [24] </ref>. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [24]. This paper describes an efficient sequential procedure for universal data compression for PSTs by using a much larger model class.
References-found: 24

