URL: http://www.cs.iastate.edu/~parekh/papers/ICML.ps
Refering-URL: http://www.cs.iastate.edu/~parekh/resume.html
Root-URL: 
Email: fparekh|honavarg@cs.iastate.edu  
Title: Learning DFA from Simple Examples  
Author: Rajesh Parekh and Vasant Honavar 
Address: 226 Atanasoff Hall  Ames IA 50011. U.S.A.  
Affiliation: Department of Computer Science  Iowa State University  
Abstract: We present a framework for learning DFA from simple examples. We show that efficient PAC learning of DFA is possible if the class of distributions is restricted to simple distributions where a teacher might choose examples based on the knowledge of the target concept. This answers an open research question posed in Pitt's seminal paper: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? Our approach uses the RPNI algorithm for learning DFA from labeled examples. In particular, we describe an efficient learning algorithm for exact learning of the target DFA with high probability when a bound on the number of states (N ) of the target DFA is known in advance. When N is not known, we show how this algorithm can be used for efficient PAC learning of DFAs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> 1981. </year> <title> A note on the number of queries needed to indentify regular languages. </title> <booktitle> Information and Control 51 </booktitle> <pages> 76-87. </pages>
Reference-contexts: An-gluin has shown that given a live-complete set of examples (that contains a representative string for each live state of the target DFA) and a knowledgeable teacher to answer membership queries it is possible to exactly learn the target DFA <ref> (Angluin 1981) </ref>. In a later paper, Angluin has relaxed the requirement of a live-complete set and has designed a polynomial time inference algorithm using both membership and equivalence queries (Angluin 1987).
Reference: <author> Angluin, D. </author> <year> 1987. </year> <title> Learning regular sets from queries and counterexamples. </title> <booktitle> Information and Computation 75 </booktitle> <pages> 87-106. </pages>
Reference-contexts: In a later paper, Angluin has relaxed the requirement of a live-complete set and has designed a polynomial time inference algorithm using both membership and equivalence queries <ref> (Angluin 1987) </ref>. The RPNI algorithm is a framework for identifying a DFA consistent with a given sample S in polynomial time (Oncina & Garca 1992).
Reference: <author> Chomsky, N. </author> <year> 1956. </year> <title> Three models for the description of language. </title> <booktitle> PGIT 2(3) </booktitle> <pages> 113-124. </pages>
Reference: <author> Denis, F.; D'Halluin, C.; and Gilleron, R. </author> <year> 1996. </year> <title> Pac learning with simple examples. </title> <booktitle> STACS'96 Proceedings of the 13 th Annual Symposium on the Theoretical Aspects of Computer Science 231-242. </booktitle>
Reference: <author> Dupont, P.; Miclet, L.; and Vidal, E. </author> <year> 1994. </year> <title> What is the search space of the regular inference? In Proceedings of the Second International Colloquium on Grammatical Inference (ICGI'94), </title> <type> 25-37. </type>
Reference: <author> Dupont, P. </author> <year> 1996. </year> <title> Incremental regular inference. </title>
Reference-contexts: Further, if the sample is a characteristic sample for the target DFA the algorithm is guaranteed to return a canonical representation of the target DFA. Our description of RPNI algorithm is based on the explanation given in <ref> (Dupont 1996) </ref>. The algorithm constructs a prefix tree acceptor P T A (S + ) for the examples in S + . <p> Against this background, empirical evaluation of the performance of the proposed algorithms using examples that come from natural domains is clearly of interest. In the incremental version of the RPNI algorithm <ref> (Dupont 1996) </ref> the learner maintains a hypothesis that is consistent with all labeled examples seen thus far and modifies it whenever a new inconsistent example is observed.
Reference: <editor> In Miclet, L., and Higuera, C., eds., </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> 222-237. </pages> <address> Montpellier, France: </address> <publisher> Springer. </publisher>
Reference: <author> Gold, E. M. </author> <year> 1978. </year> <title> Complexity of automaton identification from given data. </title> <booktitle> Information and Control 37(3) </booktitle> <pages> 302-320. </pages>
Reference-contexts: Exact learning of the target DFA from an arbitrary presentation of labeled examples is a hard problem <ref> (Gold 1978) </ref>. Gold has shown that the problem of identification of the minimum state DFA consistent with a presentation S comprising of a finite non-empty set of positive examples S + and possibly a finite non-empty set of negative examples S is N P -hard.
Reference: <author> Hopcroft, J., and Ullman, J. </author> <year> 1979. </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Ad-dison Wesley. </publisher>
Reference: <author> Kearns, M., and Valiant, L. G. </author> <year> 1989. </year> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the 21 st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> 433-444. </pages> <address> New York: </address> <publisher> ACM. </publisher>
Reference-contexts: Further, under certain cryptographic assumptions, Kearns and Valiant show that an efficient algorithm for learning DFA would entail efficient algorithms for solving the following problems that are known to be hard: breaking the RSA cryptosystem, factoring Blum integers, and detecting quadratic residues <ref> (Kearns & Valiant 1989) </ref>. The PAC model's requirement of learnability under all conceivable distributions is often considered too stringent. Pitt's paper has identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? (Pitt 1989). <p> Some of the negative results in approximate identification of DFA are derived by showing that an efficient algorithm for learning DFA would entail algorithms for solving known hard problems such as learning boolean formulae (Pitt & Warmuth 1988) and breaking the RSA cryptosystem <ref> (Kearns & Valiant 1989) </ref>. It would be interesting to explore the implications of our results on efficient learning of DFA from simple examples on these problems. Acknowledgements The authors wish to thank Dr. Jack Lutz for introducing Kolmogorov Complexity to them and Dr.
Reference: <author> Lang, K. J. </author> <year> 1992. </year> <title> Random dfa's can be approximately learned from sparse uniform sample. </title> <booktitle> In Proceedings of the 5th ACM workshop on Computational Learning Theory, </booktitle> <pages> 45-52. </pages>
Reference-contexts: Using a variant of Trakhtenbrot and Barzdin's algorithm, Lang has empirically demonstrated that random DFAs are approximately learnable from a sparse uniform sample <ref> (Lang 1992) </ref>. However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. Several concept classes are efficiently PAC learnable under restricted classes of distributions while their learnability under the distribution free model is not known (Li & Vitanyi 1991).
Reference: <author> Li, M., and Vitanyi, P. </author> <year> 1991. </year> <title> Learning simple concepts under simple distributions. </title> <journal> SIAM Journal of Computing 20 </journal> <pages> 911-935. </pages>
Reference-contexts: However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. Several concept classes are efficiently PAC learnable under restricted classes of distributions while their learnability under the distribution free model is not known <ref> (Li & Vitanyi 1991) </ref>. Li and Vitanyi have proposed a model for PAC learning with simple examples wherein the examples are drawn according to the Solomonoff-Levin universal distribution. They have shown that learnability under the universal distribution implies learnability under a broad class of simple distributions. <p> Concept classes such as log n-term DNF and simple k-reversible DFA are PAC learnable with simple examples whereas their PAC learnability in the standard sense is unknown <ref> (Li & Vitanyi 1991) </ref>. Further, the learning system might be aided by a benign teacher who knows the target concept and uses this knowledge in selecting the examples. This scheme due to (Denis, D'Halluin, & Gilleron 1996) is called the PACS model. <p> Li and Vitany i have shown that a concept class is efficiently learnable under the universal distribution if and only if it is efficiently learnable under each simple distribution <ref> (Li & Vitanyi 1991) </ref> provided the sampling is done according to the universal distribution. This raises the possibility of using sampling under the universal distribution to learn under all computable distributions. However, the universal distribution is not computable. <p> However, the universal distribution is not computable. Whether one can instead get by with a polynomially computable approximation of the universal distribution remains an open question. It is known that the universal distribution for the class of polynomially-time bounded simple distributions is computable in exponential time <ref> (Li & Vitanyi 1991) </ref>. This raises a number of interesting possibilities for learning under simple distributions. A related question of interest has to do with the nature of environments that can be modeled by simple distributions.
Reference: <author> Li, M., and Vitanyi, P. </author> <year> 1993. </year> <title> An Introduction to Kol--mogorov Complexity and its Applications. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: It can be shown that K (ff) jffj + . Finally, since K denotes the prefix Kolmogorov complexity it can be shown by Kraft's inequality that P ff2 fl 2 K (ff) 1. The reader is referred to <ref> (Li & Vitanyi 1993) </ref> for a detailed descrip tion of Kolmogorov complexity and related topics. Universal Distribution The set of programs for a string ff relative to a Turing Machine M is defined as: P ROG M (ff) = f j M () = ffg. <p> Thus, for an enumerable probability distribution P there is a constant 2 N such that for all strings ff; m (ff) P (ff). The coding theorem due independently to Schnorr, Levin and Chaitin <ref> (Li & Vitanyi 1993) </ref> states that 9 2 N 8ff m M (ff) = 2 K (x) : Intuitively this means that if there are several programs for a string ff on some machine M then there is a short program for ff on the Universal Machine (i.e. ff has a
Reference: <author> Oncina, J., and Garca, P. </author> <year> 1992. </year> <title> Inferring regular languages in polynomial update time. </title> <editor> In Perez, N. et al., ed., </editor> <booktitle> Pattern Recognition and Image Analysis, </booktitle> <pages> 49-61. </pages> <publisher> World Scientific. </publisher>
Reference-contexts: In a later paper, Angluin has relaxed the requirement of a live-complete set and has designed a polynomial time inference algorithm using both membership and equivalence queries (Angluin 1987). The RPNI algorithm is a framework for identifying a DFA consistent with a given sample S in polynomial time <ref> (Oncina & Garca 1992) </ref>. If S is a superset of a characteristic set for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to target. Pitt has surveyed several approaches for approximate identification of DFA (Pitt 1989). <p> A sample S = S + [ S is said to be characteristic with respect to a regular language L (G) (with a canonical acceptor A) if it satisfies the following two conditions <ref> (Oncina & Garca 1992) </ref>: * 8ff 2 N (L (G)); if ff 2 L (G) then ff 2 S + else 9fi 2 fl such that fffi 2 S + . * 8ff 2 S p (L (G)); 8fi 2 N (L (G)); if L (G) ff 6= L (G) fi <p> We present a framework for PAC learning from simple examples drawn according to the universal distribution. The RPNI Algorithm The regular positive and negative inference (RPNI) algorithm <ref> (Oncina & Garca 1992) </ref> is a polynomial time algorithm for identification of a DFA consistent with a given sample S = S + [ S . <p> The RPNI algorithm <ref> (Oncina & Garca 1992) </ref> is guaranteed to return a DFA that is equivalent to the target DFA provided the set of examples given to the algorithm includes a characteristic set.
Reference: <author> Pao, T., and Carr, J. </author> <year> 1978. </year> <title> A solution of the syntactic induction-inference problem for regular languages. </title> <booktitle> Computer Languages 3 </booktitle> <pages> 53-64. </pages>
Reference-contexts: The set of all derived automata obtained by systematically merging the states of A represents a lattice of FSA <ref> (Pao & Carr 1978) </ref>. This lattice is ordered by the grammar cover relation .
Reference: <author> Parekh, R., and Honavar, V. G. </author> <year> 1993. </year> <title> Efficient learning of regular languages using teacher supplied positive examples and learner generated queries. </title> <booktitle> In Proceedings of the Fifth UNB Conference on AI, </booktitle> <pages> 195-203. </pages>
Reference: <author> Pitt, L., and Warmuth, M. K. </author> <year> 1988. </year> <title> Reductions among prediction problems: on the difficulty of predicting automata. </title> <booktitle> In Proceedings of the 3 rd I.E.E.E. Conference on Structure in Complexity Theory, </booktitle> <pages> 60-69. </pages>
Reference-contexts: Under the standard complexity theoretic assumption P 6= N P , Pitt and Warmuth have shown that no polynomial time algorithm can be guaranteed to produce a DFA with at most n (1*)loglog (n) states from a set of labeled examples corresponding to a DFA with n states <ref> (Pitt & Warmuth 1988) </ref>. Efficient learning algorithms for identification of DFA assume that additional information is provided to the learner. <p> Some of the negative results in approximate identification of DFA are derived by showing that an efficient algorithm for learning DFA would entail algorithms for solving known hard problems such as learning boolean formulae <ref> (Pitt & Warmuth 1988) </ref> and breaking the RSA cryptosystem (Kearns & Valiant 1989). It would be interesting to explore the implications of our results on efficient learning of DFA from simple examples on these problems. Acknowledgements The authors wish to thank Dr.
Reference: <author> Pitt, L., and Warmuth, M. K. </author> <year> 1989. </year> <title> The minimum consistency dfa problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the 21 st ACM Symposium on the Theory of Computing, </booktitle> <pages> 421-432. </pages> <publisher> ACM. </publisher>
Reference-contexts: If S is a superset of a characteristic set for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to target. Pitt has surveyed several approaches for approximate identification of DFA <ref> (Pitt 1989) </ref>. Valiant's distribution-independent model of learning (also called the PAC model) (Valiant 1984) is widely used for learning several different concept classes approximately. <p> Even approximate learnability is proven to be a hard problem. Pitt and Warmuth have shown that the problem of polynomially approximate predictabil ity of the class of DFA is hard <ref> (Pitt & Warmuth 1989) </ref>. They make use of prediction preserving reductions to show that if DFAs are polynomially approximately predictable then so are other known hard to predict concept classes such as boolean formulas. <p> The PAC model's requirement of learnability under all conceivable distributions is often considered too stringent. Pitt's paper has identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? <ref> (Pitt 1989) </ref>. Using a variant of Trakhtenbrot and Barzdin's algorithm, Lang has empirically demonstrated that random DFAs are approximately learnable from a sparse uniform sample (Lang 1992). However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. <p> PAC Learning Model The PAC learning model (Valiant 1984) describes a probabilistic framework for approximate learning of concept classes from labeled examples. We present the definition of PAC-learning that is appropriate for learning DFA <ref> (Pitt 1989) </ref>. Let X denote the sample space defined as the set of all strings fl . Let x X denote a concept. For our purpose, x is a regular language. <p> Assume that there is an unknown and arbitrary but fixed distribution D according to which the examples of the target concept are drawn. In the context of learning DFA, D is restricted to a probability distribution on strings of fl of length at most m. Definition: <ref> (Pitt 1989) </ref> DFAs are PAC-identifiable iff there exists a (possibly randomized) algorithm A such that on input of any parameters * and ffi, for any DFA M of size N , for any number m, and for any probability distribution D on strings of fl of length at most m, if
Reference: <author> Pitt, L. </author> <year> 1989. </year> <title> Inductive inference, dfas and computational complexity. In Analogical and Inductive Inference, </title> <booktitle> Lecture Notes in Artificial Intelligence 397, </booktitle> <pages> 18-44. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: If S is a superset of a characteristic set for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to target. Pitt has surveyed several approaches for approximate identification of DFA <ref> (Pitt 1989) </ref>. Valiant's distribution-independent model of learning (also called the PAC model) (Valiant 1984) is widely used for learning several different concept classes approximately. <p> Even approximate learnability is proven to be a hard problem. Pitt and Warmuth have shown that the problem of polynomially approximate predictabil ity of the class of DFA is hard <ref> (Pitt & Warmuth 1989) </ref>. They make use of prediction preserving reductions to show that if DFAs are polynomially approximately predictable then so are other known hard to predict concept classes such as boolean formulas. <p> The PAC model's requirement of learnability under all conceivable distributions is often considered too stringent. Pitt's paper has identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? <ref> (Pitt 1989) </ref>. Using a variant of Trakhtenbrot and Barzdin's algorithm, Lang has empirically demonstrated that random DFAs are approximately learnable from a sparse uniform sample (Lang 1992). However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. <p> PAC Learning Model The PAC learning model (Valiant 1984) describes a probabilistic framework for approximate learning of concept classes from labeled examples. We present the definition of PAC-learning that is appropriate for learning DFA <ref> (Pitt 1989) </ref>. Let X denote the sample space defined as the set of all strings fl . Let x X denote a concept. For our purpose, x is a regular language. <p> Assume that there is an unknown and arbitrary but fixed distribution D according to which the examples of the target concept are drawn. In the context of learning DFA, D is restricted to a probability distribution on strings of fl of length at most m. Definition: <ref> (Pitt 1989) </ref> DFAs are PAC-identifiable iff there exists a (possibly randomized) algorithm A such that on input of any parameters * and ffi, for any DFA M of size N , for any number m, and for any probability distribution D on strings of fl of length at most m, if
Reference: <author> Trakhtenbrot, B., and Barzdin, Y. </author> <year> 1973. </year> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> Amsterdam: North Holland Publishing Company. </publisher>
Reference-contexts: Trakhtenbrot and Barzdin have described a polynomial time algorithm for constructing the smallest DFA consistent with a complete labeled sample i.e., a sample that includes all strings up to a particular length and the corresponding label that states whether the string is accepted by the target or not <ref> (Trakhtenbrot & Barzdin 1973) </ref>. An-gluin has shown that given a live-complete set of examples (that contains a representative string for each live state of the target DFA) and a knowledgeable teacher to answer membership queries it is possible to exactly learn the target DFA (Angluin 1981).
Reference: <author> Valiant, L. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27 </journal> <pages> 1134-1142. </pages>
Reference-contexts: Pitt has surveyed several approaches for approximate identification of DFA (Pitt 1989). Valiant's distribution-independent model of learning (also called the PAC model) <ref> (Valiant 1984) </ref> is widely used for learning several different concept classes approximately. When adapted to the problem of learning DFA, the goal of a PAC learning algorithm is to obtain in polynomial time, with high probability, a DFA that is approximately correct when compared to the target DFA. <p> PAC Learning Model The PAC learning model <ref> (Valiant 1984) </ref> describes a probabilistic framework for approximate learning of concept classes from labeled examples. We present the definition of PAC-learning that is appropriate for learning DFA (Pitt 1989). Let X denote the sample space defined as the set of all strings fl . Let x X denote a concept.
References-found: 21

