URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/thesis-Baker.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: http://www.cs.berkeley.edu
Title: Fast Crash Recovery in Distributed File Systems  
Author: by Mary Louise Gray Baker 
Degree: M.S. (University of California at Berkeley) 1988 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor John Ousterhout, Chair Professor Randy Katz Professor Rainer Sachs  
Date: 1984  1994  
Address: Berkeley)  
Affiliation: A.B. (University of California at  
Abstract-found: 0
Intro-found: 1
Reference: [Accett86] <author> Mike Accetta, Robert Baron, William Bolosky, David Golub, Richard Rashid, Avadis Tevanian, and Michael Young. </author> <title> Mach: A New Kernel Foundation for UNIX Development. </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <pages> pages 93-112, </pages> <month> June, </month> <year> 1986. </year>
Reference-contexts: In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach <ref> [Accett86] </ref>. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures. Their main market is on-line transaction processing.
Reference: [Anonym88] <author> Anonymous. </author> <title> A Measure of Transaction Processing Power. Tandem Corporation Technical Report 85.1. </title> <booktitle> In Readings in Database Systems, edited by Michael Stonebraker, </booktitle> <pages> pages 300-312. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Eliminating this communication by using the recovery box is an easy way to fix this problem. To measure POSTGRES recovery times, Mark Sullivan ran a debit/credit benchmark based on TP1 <ref> [Anonym88] </ref>, but to expedite the measurements he used a much smaller database than the actual TP1 benchmark requires. A single POSTGRES DBMS managed the database from a Sprite file server. Ten POSTGRES client processes running on a single Sprite client machine generated the transactions.
Reference: [Babaog90] <author> Ozalp Babaoglu. </author> <title> Fault-Tolerant Computing Based on Mach. </title> <booktitle> ACM Operating Systems Review, </booktitle> <pages> pages 27-39, </pages> <month> January, </month> <year> 1990. </year>
Reference-contexts: In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes <ref> [Babaog90] </ref> on Mach [Accett86]. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures. Their main market is on-line transaction processing.
Reference: [Baker91a] <author> Mary Baker and John Ousterhout. </author> <title> Availability in the Sprite Distributed File System. </title> <booktitle> ACM Operating Systems Review, </booktitle> <pages> pages 95-98, </pages> <month> April, </month> <year> 1991. </year>
Reference: [Baker91b] <author> Mary Baker, John Hartman, Michael Kupfer, Ken Shirriff and John Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <month> October, </month> <year> 1991. </year>
Reference-contexts: This makes the new data available quickly for other clients polling the file, but it increases the delay for the client application, and it increases the load on the network and the file server. Most files are only open for a short time <ref> [Baker91b] </ref>, so the write-through-on-close policy means that clients write back their dirty data almost immediately. This ties their write performance to the speed of the servers disk. <p> The file remains uncacheable until all clients have closed it. Turning off client caching during concurrent write-sharing may add latency to individual client read and write requests, but concurrent sharing is so rare that using this simple mechanism does not significantly affect overall system performance <ref> [Baker91b] </ref>. Given this explanation of Sprites cache consistency policy, there are still questions to answer. <p> Because Sprite clients send all open and close requests through to the server, we are able to trace much of the cache behavior of individual clients. More information about theses traces can be found in <ref> [Baker91b] </ref>. Table 1-1 presents the results of these simulations. A 60-second refresh interval would have resulted in many uses of stale data each hour, and one-half of all users would have accessed stale data over a 24-hour period.
Reference: [Baker92a] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> Proceedings of the Summer 1992 USENIX Conference, </booktitle> <pages> pages 31-43, </pages> <month> June, </month> <year> 1992. </year>
Reference-contexts: Software error type distributions. This table shows the distribution of software errors analyzed by Mark Sullivan in studies of error reports from the IBM MVS and 4.1/4.2 BSD UNIX operating systems [Sulli93a] <ref> [Baker92a] </ref>. The results columns give the percentage of errors that fall into each category. The two studies used different classification schemes, but both list addressing errors - the errors most likely to corrupt recovery box memory. The other error classes are described in the text. <p> This section describes how one application, an experimental version of the POSTGRES database management system (DBMS) [Stoneb86], uses the recovery box. 102 In this experimental version, written by Mark Sullivan <ref> [Baker92a] </ref>, POSTGRES runs as an application program on a Sprite file server and responds to requests from client programs running on other Sprite machines. POSTGRES accesses the recovery box using the system call interface described in section 5.2.1. 5.8.1.
Reference: [Baker92b] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <month> October, </month> <year> 1992. </year>
Reference-contexts: If the recovery box is stored in the servers volatile main memory, the contents will be lost after a power failure. One solution is to use a half-megabyte or so of non-volatile RAM for the recovery box. This is less costly than an uninterruptable power supply <ref> [Baker92b] </ref>. However, our experience has been that the client workstations also lose power during a power failure. In this case, theres no point in preserving their state on the server across the failure, because they must all be rebooted from scratch anyway.
Reference: [Banatr89] <author> J. P. Banatre, M. Banatre, G. Muller. </author> <title> Architecture of Fault-Tolerant Multiprocessor Workstations. </title> <booktitle> Proceedings of the Second Workshop on Workstation Operating Systems, </booktitle> <pages> pages 20-24, </pages> <month> September, </month> <year> 1989. </year>
Reference: [Bartle81] <author> J. Bartlett. </author> <title> A NonStop Kernel. </title> <booktitle> Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <month> December, </month> <year> 1981. </year>
Reference: [Bartle90] <author> Wendy Bartlett, Richard Carr, Dave Garcia, Jim Gray, Robert Horst, Robert 123 Jardine, Dan Lenoski, Dix McGuire, and Joel Bartlett. </author> <title> Fault Tolerance in Tandem Computer Systems. </title> <type> Tandem Technical Report 90.5, Tandem Part Number 40666, </type> <month> March, </month> <year> 1990. </year>
Reference: [Birman84] <author> Kenneth P. Birman, Amr El Abbadi, Wally Dietrich, Thomas Joseph, and Thomas Raeuchle. </author> <title> An Overview of the Isis Project. </title> <type> Technical Report number TR 84-642, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> October </month> <year> 1984. </year>
Reference: [Birman89] <author> Kenneth Birman and Keith Marzullo. </author> <title> The ISIS Distributed Programming Toolkit and the Meta Distributed Operating System. </title> <journal> SUN Technology, </journal> <volume> volume 2, number 1, </volume> <pages> pages 90-104, </pages> <month> Summer </month> <year> 1989. </year>
Reference: [Birman91] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight Causal and Atomic Group Multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 9, number 3, </volume> <pages> pages 272-314, </pages> <month> August, </month> <year> 1991. </year>
Reference: [Birrel82] <author> Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder. Grapevine: </author> <title> An Exercise in Distributed Computing. </title> <journal> Communications of the ACM, </journal> <volume> volume 25, number 4, </volume> <pages> pages 260-274, </pages> <month> April, </month> <year> 1982. </year>
Reference-contexts: In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine <ref> [Birrel82] </ref>, LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures.
Reference: [Birrel87] <author> [Andrew D. Birrell, Michael B. Jones, and Edward P. Wobber. </author> <title> A Simple and Efficient Implementation for Small Databases. </title> <booktitle> Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 149-154, </pages> <year> 1987. </year>
Reference: [Blasge79] <author> M. Blasgen, J. Gray, M. Mitoma, and T. Price. </author> <title> The Convoy Phenomenon. </title> <journal> Operating Systems Review, </journal> <volume> volume 13, number 2, </volume> <month> April, </month> <year> 1979. </year>
Reference-contexts: This is an interesting example of a phenomenon seen elsewhere in distributed systems: global events (in this case the server reboots) can cause unexpected and undesirable synchronization of different elements of a distributed system. The results are similar to those found in the convoy phenomenon <ref> [Blasge79] </ref>. clients initiate recovery with the server during the same second. At 84 seconds, another 15 clients initiate recovery with the server.
Reference: [Borg83] <author> Anita Borg, Jim Baumbach, and Sam Glazer. </author> <title> A Message System Supporting Fault Tolerance. </title> <booktitle> Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pages 90-99, </pages> <month> November </month> <year> 1983. </year>
Reference: [Borg89] <author> A. Borg and W. Blau and W. Graetsch and F. Herrman and W. Oberle. </author> <title> Fault Tolerance Under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 7, number 1, </volume> <month> February, </month> <year> 1989. </year>
Reference: [Bruell88] <author> G. Bruell, A. Z. Spector, R. Pausch. Camelot, </author> <title> a Flexible, Distributed Transaction Processing System. </title> <booktitle> Thirty-third IEEE Computer Society International Conference (COMPCON), </booktitle> <pages> pages 432-437, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: While LOCUS and QuickSilver provide transactions as a part of the operating system (embedded transaction support), some systems provide transaction services in an environment or language built on top of the operating system. Transactional systems that I do not describe include Camelot <ref> [Bruell88] </ref>, Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1.
Reference: [Cherit84] <author> D. R. Cheriton. </author> <title> The V Kernel: a Software Base for Distributed Systems. </title> <journal> IEEE Software, </journal> <volume> volume 1, number 2, </volume> <pages> pages 19-42, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V <ref> [Cherit84] </ref>. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability.
Reference: [Cherit93] <author> David R. Cheriton and Dale Skeen. </author> <title> Understanding the Limitations of Causally and Totally Ordered Communication. </title> <booktitle> Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <month> December, </month> <year> 1993. </year>
Reference: [Dasgup88] <author> P. Dasgupta, R. LeBlanc, W. Appelbe. </author> <title> The Clouds Distributed Operating System: Functional Description, Implementation Details and Related Work. </title> <booktitle> Eighth International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <pages> pages 13-17, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds <ref> [Dasgup88] </ref>, Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability.
Reference: [DeWitt84] <author> David J. DeWitt, Randy H. Katz, Frank Olken, Leonard D. Shapiro, Michael R. 124 Stonebraker, David Wood. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> Proceedings of the 1984 ACM SIGMOD Annual Meeting, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1984. </year>
Reference: [Dougli91] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software - Practice and Experience, </journal> <volume> volume 21, number 8, </volume> <month> August, </month> <year> 1991. </year>
Reference-contexts: All of the workstations in the cluster run the Sprite network operating system. Sprite is largely UNIX-compatible, and most of the applications running on the cluster are standard UNIX applications. In addition, Sprite provides process migration <ref> [Dougli91] </ref> which allows users to off-load jobs easily to idle machines in the cluster.
Reference: [Dunlap86] <author> Kevin J. Dunlap. </author> <title> Name Server Operations Guide for BIND. Release 4.3, Unix System Managers Manual. </title> <booktitle> Printed by the USENIX Association. </booktitle> <month> April, </month> <year> 1986. </year>
Reference-contexts: These applications could use the recovery box in the same way that file servers do to avoid disk I/Os and communication with clients during recovery. An example of such an application is a network name server, such as the Berkeley Internet Name Domain Server <ref> [Dunlap86] </ref>. A name server is a distributed database that allows clients to name objects in a distributed system. The server is a daemon called named that responds to queries on a network port.
Reference: [Gait90] <author> Jason Gait. </author> <title> A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> volume 33, number 1, </volume> <pages> pages 81-86, </pages> <month> January, </month> <year> 1990. </year>
Reference-contexts: Besides Sprites use of the recovery box, I am aware of only two other file systems that have considered this technique: the Phoenix In-Memory File System <ref> [Gait90] </ref>, and the Harp file system [Liskov91]. However, Harp currently does not actually implement the technique [Johnso93]. I describe these two systems in this section. 31 1.3.1. Phoenix Phoenix is an in-memory file system intended mainly for diskless computers with battery-powered memory.
Reference: [Garcia92] <author> Hector Garcia-Molina and Kenneth Salem. </author> <title> Main Memory Database Systems: An Overview. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> volume 4, number 6, </volume> <month> December, </month> <year> 1992. </year>
Reference: [Giffor88] <author> David K. Gifford, Roger M. Needham, and Michael D. Schroeder. </author> <title> The Cedar File System. </title> <journal> Communications of the ACM, </journal> <volume> volume 31, number 3, </volume> <pages> pages 288-298, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar <ref> [Giffor88] </ref>, Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1. <p> While LOCUS and QuickSilver provide transactions as a part of the operating system (embedded transaction support), some systems provide transaction services in an environment or language built on top of the operating system. Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar <ref> [Giffor88] </ref>, Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability.
Reference: [Gray89] <author> Cary G. Gray and David R. Cheriton. Leases: </author> <title> An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. </title> <booktitle> Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 202-210, </pages> <month> December, </month> <year> 1989. </year>
Reference-contexts: Sprite clients do not cache directories. Third, Echo allows clients to cache modified file attributes as well as dirty file data. This speeds up operations that examine the status of files. To guarantee cache consistency, Echo uses a token scheme with leases <ref> [Gray89] </ref>. A lease is a time-out period associated with a token and agreed upon by both the server and the client. Token managers on Echo file servers grant and revoke tokens to maintain cache consistency in the same way DEcorum token managers do.
Reference: [Gray88] <author> Jim Gray. </author> <title> The Transaction Concept: Virtues and Limitations. </title> <booktitle> Lecture Notes in Computer Science, Conference Proceedings, Netherlands, 1981. In Readings in Database Systems, edited by Michael Stonebraker, </booktitle> <pages> pages 140-150. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: This is because transactions make it easier to handle redundancy and failure recovery. Operations executed as transactions are atomic (either all the changes are made or none is made), durable (the changes survive failures), isolated (serializable), and consistent (the transformation is correct) <ref> [Gray88] </ref>. Transactions ease handling of system redundancy, because they can ensure that the replicated portions remain consistent. For example, a system that uses replicated file servers for availability may 28 update both servers simultaneously as a single transaction. Because transactions are atomic, the state on both servers will remain identical.
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability Between 1985 and 1990. </title> <type> Tandem Technical Report 90.1. Part number 33579, </type> <month> January, </month> <year> 1990. </year>
Reference-contexts: Error Statistics The most important issue for storing the recovery box in main memory is whether it will survive most server crashes intact. Statistics on the frequencies of different types of system outages indicate that this should be true for three reasons. These statistics were collected by Jim Gray <ref> [Gray90] </ref> and Mark Sullivan [Sulli93a] in Tandem, MVS, and UNIX systems. First, the majority of failures are due to software failures rather than permanent hardware problems that require long downtimes to fix. Second, the majority of software errors do not corrupt memory. <p> Together, these statistics suggest that most failures will leave the recovery box data undamaged, as detailed below. Published data about the frequency of different kinds of outages is scarce, but a study of Tandem systems shows that faulty software is now responsible for most failures <ref> [Gray90] </ref>. Over time, Tandem systems have experienced fewer outages caused by hardware failures. The number of software failures, on the other hand, has remained constant. Table 5-1 shows the percentages of each source of outage. <p> Two studies that categorize the types of operating system software errors indicate that most soft Table 5-1. Distribution of outage types. This table shows the distribution of types of outages by fatal fault occurring in Tandem systems between 1985-1990 <ref> [Gray90] </ref>. Software failures are caused by errors in the software. Operator errors are mistakes made by humans responsible for maintaining the machines. Hardware failures are caused by problems with the hardware. Environment failures are caused by oods, fires, and long power outages.
Reference: [Gray93] <author> Jim Gray, and Andreas Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in <ref> [Gray93] </ref> and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability. Transparent handling of replicated files is a major goal in LOCUS. Users can choose the desired degree of replication for files.
Reference: [Hagman86] <author> Robert B. Hagmann. </author> <title> A Crash Recovery Scheme for a Memory-Resident Database System. </title> <journal> IEEE Transactions on Computers, </journal> <volume> volume 35, number 9, </volume> <month> September </month> <year> 1986. </year>
Reference: [Hartma93] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <month> December, </month> <year> 1993. </year>
Reference-contexts: For example, some systems use their replicated processors only if a primary processor fails. In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra <ref> [Hartma93] </ref>, and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1.
Reference: [Haskin88] <author> Roger Haskin, Yoni Malachi, Wayne Sawdon, and Gregory Chan. </author> <title> Recovery Management in QuickSilver. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 6, number 1, </volume> <pages> pages 82-108, </pages> <month> February, </month> <year> 1988. </year>
Reference-contexts: Instead, these systems craft individual operations to be atomic as necessary. In this section I describe two systems that use transactions in different ways to provide recoverable file systems. LOCUS [Muelle83][Popek85][Walker83] uses transactions to keep data consistent across replicas, while QuickSilver <ref> [Haskin88] </ref> uses transactions simply to provide clean recovery of non-replicated resources. While LOCUS and QuickSilver provide transactions as a part of the operating system (embedded transaction support), some systems provide transaction services in an environment or language built on top of the operating system.
Reference: [Hisgen89] <author> Andy Hisgen, Andrew Birrell, Timothy Mann, Michael Schroeder and Garret Swart. </author> <title> Availability and Consistency Tradeoffs in the Echo Distributed File 125 System. </title> <booktitle> Proceedings of the Second Workshop on Workstation Operating Systems, </booktitle> <month> September, </month> <year> 1989. </year>
Reference-contexts: The following sections describe how three other file systems recover their distributed cache state. The choice of recovery mechanism in the first two systems was inuenced by Sprite. Spritely NFS [Mogul92][Mogul93][Sriniv89] uses server-driven recovery, and DEcorum [Kazar90] uses client-driven recovery. The third system, Echo <ref> [Hisgen89] </ref>[- Mann93], instead maintains a backup server with a copy of the distributed state. This technique is similar to transparent recovery, except that the stable storage for the servers state is actually the main memory of another server. Unfortunately, the following sections provide only limited recovery performance data.
Reference: [Howard88] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 6, number 1, </volume> <pages> pages 51-81, </pages> <month> February, </month> <year> 1988. </year>
Reference-contexts: In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew <ref> [Howard88] </ref>, Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1.
Reference: [Jewett91] <author> D. Jewett. </author> <title> Integrity S2: A Fault-Tolerant UNIX Platform. </title> <booktitle> Digest 21st International Symposium on Fault-Tolerant Computing, </booktitle> <address> June,1991. </address>
Reference-contexts: Error Repair To provide high availability, some systems attempt to detect and repair errors on-line. One name for this technique is error repair. It is also called forward error recovery. Tandems Integrity S2 <ref> [Jewett91] </ref> and AT&Ts 5ESS (electronic switching system) [Smith81] [Toy92a] are examples of systems that detect their own faults and repair themselves on-line, as described below. The appeal of error repair is that it does not incur the performance overhead of redundant hardware and software.
Reference: [Johnso93] <author> Paul Johnson. </author> <type> Personal Communication. </type> <month> June 18, </month> <year> 1993. </year>
Reference-contexts: Besides Sprites use of the recovery box, I am aware of only two other file systems that have considered this technique: the Phoenix In-Memory File System [Gait90], and the Harp file system [Liskov91]. However, Harp currently does not actually implement the technique <ref> [Johnso93] </ref>. I describe these two systems in this section. 31 1.3.1. Phoenix Phoenix is an in-memory file system intended mainly for diskless computers with battery-powered memory. To protect the file system, Phoenix uses a copy-on-write and checkpoint scheme. <p> The designers have so far not implemented the technique, because they do a complete reboot after a crash, and their computers (a collection of MicroVax 3500s) perform a memory check that overwrites the contents of memory <ref> [Johnso93] </ref>. 1.4. Summary An important part of recovery in distributed file systems is the recovery of distributed cache state information. <p> The second problem is that the boot PROMs of some machines modify or clear memory. For example, the memory check initiated by the boot PROM of the MicroVax 3500 modifies a word for every 1024 words in memory <ref> [Johnso93] </ref>. If it is not possible to turn off such activities as the memory check, it may be impossible to find a large enough area of physical memory that remains unmodified by the boot PROM.
Reference: [Juszcz89] <author> Chet Juszczak. </author> <title> Improving the Performance and Correctness of an NFS Server. </title> <booktitle> Proceedings of the Winter 1989 USENIX Conference, </booktitle> <pages> pages 56-63, </pages> <month> February, </month> <year> 1989. </year>
Reference: [Kazar90] <author> Michael L. Kazar, Bruce W. Leverett, Owen T. Anderson, Vasilis Apostolides, Beth A. Bottos, Sailesh Chutani, Craig F. Everhart, W. Anthony Mason, Shu-Tsui Tu and Edward R. Zayas. </author> <title> DEcorum File System Architectural Overview. </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 151-163, </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: The third section describes systems that store state information in main memory for fast crash recovery, as does the recovery box, described in chapter 5. 1.1. The Distributed Cache State Recovery Problem File systems such as Sprite, Spritely NFS [Mogul92][Sriniv89][Mogul93], DEcorum <ref> [Kazar90] </ref>, and Echo [Hisgen89][Mann93] all share a need to recover distributed cache state. The need arises, because the systems allow file data caching on client workstations and also guarantee consistency amongst the client caches. <p> The following sections describe how three other file systems recover their distributed cache state. The choice of recovery mechanism in the first two systems was inuenced by Sprite. Spritely NFS [Mogul92][Mogul93][Sriniv89] uses server-driven recovery, and DEcorum <ref> [Kazar90] </ref> uses client-driven recovery. The third system, Echo [Hisgen89][- Mann93], instead maintains a backup server with a copy of the distributed state. This technique is similar to transparent recovery, except that the stable storage for the servers state is actually the main memory of another server. <p> Spritely NFS makes a strong argument in favor of stateful systems. It shows that it is possible to make a few changes to a stateless protocol to turn it into a stateful one and gain performance and cache consistency advantages without introducing great complexity. 1.1.6. DEcorum Transarcs DEcorum <ref> [Kazar90] </ref> file system for the Distributed Computing Environment is based on a previous file system called AFS [Howard88][Satyan90] but has made several modifications to provide higher performance and stricter cache consistency guarantees.
Reference: [Kazar93] <author> Michael L. Kazar. </author> <type> Personal Communication, </type> <month> June, </month> <year> 1993. </year>
Reference-contexts: If a DEcorum server crashes, it loses its token information and must recover it. DEcorum uses an implementation of the client-driven recovery technique described in chapter 3, but with more attention paid to recovery after network partitions <ref> [Kazar93] </ref>. Clients detect when servers return after a crash or network partition. They then make calls to the server, and the server responds with an indication of whether it truly crashed and lost the token information. <p> The problem with the second approach is that it guarantees that state recovery will always take at least 30 seconds. Despite the extra time required by this approach, this is how the DEcorum file system (described in chapter 2) currently avoids cache consistency violations in its client-driven recovery implementation <ref> [Kazar93] </ref>.
Reference: [Kistle91] <author> James J. Kistler, and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-225, </pages> <month> October, </month> <year> 1991. </year>
Reference-contexts: A year-long trace of file activity in AFS [Satyan90], a distributed system with over 400 users, also shows some sharing. In AFS, the chances of a file being modified by two different users in less than a day are between 0.09% and 0.72%, depending upon the type of file <ref> [Kistle91] </ref>. While the measurements show less sharing than in Sprite, they still indicate a significant percentage of possible consistency errors in the absence of an effective cache consistency policy.
Reference: [Kleima86] <author> S. R. Kleiman, Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX. </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <pages> pages 238-247, </pages> <month> June, </month> <year> 1986. </year>
Reference-contexts: A client must not read or write cached file data until it has received the appropriate token from the file server. As in Sprite, the servers token manager intercepts all file open and close requests (and all other client calls through the Vnode <ref> [Kleima86] </ref> interface). The token manager keeps track of which clients have which type of tokens for which files.
Reference: [Lai89] <author> Nick Lai. </author> <type> Personal Communication, </type> <month> Fall, </month> <year> 1989. </year>
Reference-contexts: Thus only the changes from the primary copy need to be applied to the replicas to merge partitions. Besides complexity, a disadvantage of LOCUS is that it is slow <ref> [Lai89] </ref>. Maintaining consistency between file replicas is one source of performance overhead. The system enforces file consistency guarantees across machines with distributed transactions implemented with a two-phase commit protocol. Thus an update to a file requires messages to and from any hosts storing replicas.
Reference: [Lin90] <author> Ting-Ting Y. Lin and Daniel P. Siewiorek. </author> <title> Error Log Analysis: Statistical Modeling and Heuristic Trend Analysis. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> volume 39, number 4, </volume> <month> October, </month> <year> 1990. </year>
Reference-contexts: Table 5-1 shows the percentages of each source of outage. In 1989, software errors accounted for 62% of Tandem system failures, while only 7% were caused by hardware. In other environments as well, software errors and other transient errors account for the majority of failures <ref> [Lin90] </ref>. The direction of change in these failure statistics is part of the motivation for the fast recovery approach. <p> Finally, in highly reliable systems, disks are monitored for soft (correctable) errors. If these become frequent, the disk should be replaced. By monitoring the error trends it is usually possible to replace the faulty disk before it causes a permanent error <ref> [Lin90] </ref>. There are two remaining problems with the Sprite implementation of LFS that have an effect on the measurements in this dissertation. The first is that we lack an LFS disk boot program.
Reference: [Liskov88] <author> Barbara Liskov. </author> <title> Distributed Programming in Argus. </title> <journal> Communications of the ACM, </journal> <pages> pages 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: While LOCUS and QuickSilver provide transactions as a part of the operating system (embedded transaction support), some systems provide transaction services in an environment or language built on top of the operating system. Transactional systems that I do not describe include Camelot [Bruell88], Argus <ref> [Liskov88] </ref>, TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1.
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <month> October, </month> <year> 1991. </year>
Reference-contexts: Besides Sprites use of the recovery box, I am aware of only two other file systems that have considered this technique: the Phoenix In-Memory File System [Gait90], and the Harp file system <ref> [Liskov91] </ref>. However, Harp currently does not actually implement the technique [Johnso93]. I describe these two systems in this section. 31 1.3.1. Phoenix Phoenix is an in-memory file system intended mainly for diskless computers with battery-powered memory. To protect the file system, Phoenix uses a copy-on-write and checkpoint scheme. <p> The server issues an RPC to the backup and waits to receive its reply, before responding to the client. This technique is similar to that used by HARP <ref> [Liskov91] </ref> to make its file system updates on backup servers. 78 There are several advantages of this technique.
Reference: [Long91] <author> D. Long, J. Carroll and C. Park. </author> <title> A Study of the Reliability of Internet Sites. </title> <booktitle> Proceedings of the Tenth Symposium on Reliable Distributed Systems, </booktitle> <month> September, </month> <year> 1991. </year>
Reference-contexts: The techniques described here are applicable to other varieties of state information such as name and address translation data for network name services and client/server connection information for distributed databases. Finally, frequent server reboots are a problem for many systems, and not just Sprite. Measurements from Internet sites <ref> [Long91] </ref> indicate that UNIX machines fail on average once every two weeks. 1.2. Outline of Dissertation This section outlines the thesis and lists its overall results by chapter. The next chapter of this dissertation gives more motivation and background. <p> Figure 5-1 is a pie chart showing this calculation of random addressing errors. Using measurements from Internet sites <ref> [Long91] </ref> that indicate that UNIX machines fail on average once every two weeks, we can expect one to two failures per year due to random addressing errors. Even then, the error does not necessarily mean the recovery box will be damaged.
Reference: [Mackle91] <author> Rick Macklem. </author> <title> Lessons Learned Tuning the 4.3 BSD Reno Implementation of the NFS Protocol. </title> <booktitle> Proceedings of the Winter 1991 USENIX Conference, </booktitle> <pages> pages 53-64, </pages> <month> January, </month> <year> 1991. </year> <month> 126 </month>
Reference: [Mann93] <author> Timothy Mann, Andrew Birrell, Andy Hisgen, Charles Jerian, and Garret Swart. </author> <title> A Coherent Distributed File Cache with Directory Write-behind. </title> <note> Digital SRC Research Report Number 103, </note> <month> June, </month> <year> 1993. </year>
Reference-contexts: Echos designers chose not to implement a recovery mechanism such as Sprites that would allow the servers to retrieve the token information from the clients. One of the reasons is that Echo authenticates client requests more carefully than Sprite <ref> [Mann93] </ref>; it would have to authenticate the token information received from clients after a crash. However, Echos designers believe that a mechanism such as Sprites would have been useful: Weighing the advantages and disadvantages, we prefer token replication as the first line of defense against server crashes. <p> However, we encountered enough double server crashes in Echo to convince us that token recovery would have been useful as a second line of defense. It would have considerably reduced the disruption to users in these cases <ref> [Mann93] </ref>. 1.2. Providing High Availability This section describes traditional fault-tolerant techniques used to provide very high (non-stop) availability. The categories of techniques are error repair, redundant (replicated) hardware and software, and transactions. For each category, I describe a few systems that use the technique.
Reference: [McKusi84] <author> M. McKusick, W. Joy, S. Leffler, and R. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 2, number 3. </volume> <pages> pages 181-197, </pages> <month> August, </month> <year> 1984. </year>
Reference-contexts: Ive thus been able to take much more accurate start-up measurements than I could have otherwise. 6.3. Using LFS The lengthiest step in rebooting most UNIX-like operating systems is checking the consistency of the file system on disk. For file systems such as the BSD Fast File System (FFS) <ref> [McKusi84] </ref> and the original Sprite file system (OFS), this operation is carried out by the fsck file system check program. In Sprite, fsck requires about 40 minutes to check five gigabytes of disk.
Reference: [Mogul92] <author> Jeffrey C. Mogul. </author> <title> A Recovery Protocol for Spritely NFS. </title> <booktitle> USENIX File Systems Workshop Proceedings, </booktitle> <pages> pages 93-109, </pages> <month> May, </month> <year> 1992. </year>
Reference-contexts: With server-driven recovery, the root server can recover first with the other servers, freeing up their resources more quickly. These combined advantages are likely to make server-driven recovery the distributed cache state recovery technique of choice for many systems. For example, Spritely NFS <ref> [Mogul92] </ref> [Sriniv89] has chosen to implement server-driven recovery, as described in chapter 2. This chapter has the following organization. The first section describes the design and implementation of server-driven recovery. Section 4.2 lists some disadvantages of this recovery tech 58 nique as compared to client-driven recovery.
Reference: [Mogul93] <author> Jeffrey C. Mogul. </author> <title> Recovery in Spritely NFS. </title> <institution> Digital Western Research Laboratory Research Report 93/2, </institution> <month> June, </month> <year> 1993. </year>
Reference: [Muelle83] <author> E. Mueller, et al. </author> <title> A Nested Transaction Mechanism for LOCUS. </title> <booktitle> Proceedings of the Ninth Symposium on Operating System Principles, </booktitle> <month> October, </month> <year> 1983. </year>
Reference: [Nelson88] <author> M. Nelson, B. Welch, and J. Ousterhout, </author> <title> Caching in the Sprite Network File System. </title> <journal> Transactions on Computer Systems, </journal> <volume> volume 6, number 1, </volume> <pages> pages 134-154, </pages> <month> February, </month> <year> 1988. </year>
Reference-contexts: This means the application does not block until the write is finished, so its performance is not limited by the write operation. Sprites delayed write-back policy provides better application performance that the write-through policy used by NFS. Comparisons between NFS and Sprite in <ref> [Nelson88] </ref> made on Sun-3/50 and Sun-3/75 workstations show that Sprite is at least 30 to 40% faster, due in large part to its writing policies. More recent measurements in [Ouste90a] on faster workstations show that Sprite is now 50 to 100% faster.
Reference: [Ouster88] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite Network Operating System. </title> <journal> IEEE Computer, </journal> <volume> volume 6, number 1, </volume> <pages> pages 23-36, </pages> <month> February, </month> <year> 1988. </year>
Reference-contexts: This contrasts with traditional UNIX file systems that require tens of minutes to ensure file system consistency after a failure. By combining fast state recovery, LFS, and other fast recovery techniques, the Sprite Distributed File System <ref> [Ouster88] </ref> recovers from crashes in under 30 seconds. The faster a system recovers, the more available it is. In the limit, as recovery time approaches zero, a system with fast crash recovery is indistinguishable from a system that never crashes at all.
Reference: [Ouste90a] <author> John K. Ousterhout. </author> <booktitle> Why Arent Operating Systems Getting Faster as Fast as Hardware? Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: Comparisons between NFS and Sprite in [Nelson88] made on Sun-3/50 and Sun-3/75 workstations show that Sprite is at least 30 to 40% faster, due in large part to its writing policies. More recent measurements in <ref> [Ouste90a] </ref> on faster workstations show that Sprite is now 50 to 100% faster.
Reference: [Ouste90b] <author> John K. Ousterhout. </author> <title> The Role of Distributed State. </title> <booktitle> Proceedings of the 25th Anniversary Symposium, </booktitle> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September, </month> <year> 1990. </year>
Reference: [Patter88] <author> David A. Patterson, Garth Gibson and Randy H. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> Proceedings of the 1988 ACM SIGMOD, </booktitle> <pages> pages 109-116, </pages> <month> June, </month> <year> 1988. </year>
Reference-contexts: Thus the cost of the system is higher than its processing capacity warrants. 1.2.2.4. Zebra Zebra is an example of a high-performance network file system that uses replication for availability but wastes very little system capacity. To do this, Zebra uses two techniques from RAID <ref> [Patter88] </ref>: striping for fast file access, and parity for high availability of file data. These techniques are illustrated in Figure 1-6. In Zebra, file striping means that different blocks of a file are stored on different storage servers.
Reference: [Popek85] <author> Gerald J. Popek and Bruce J. Walker, </author> <title> editors. The LOCUS Distributed System Architecture. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference: [Pu86] <author> C. Pu, J. D. Noe, and A. Proudfoot. </author> <title> Regeneration of Replicated Objects: A Technique and its Eden Implementation. </title> <booktitle> Proceedings of the Second International Conference on Data Engineering, </booktitle> <pages> pages 175-187, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden <ref> [Pu86] </ref>, Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures. <p> Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden <ref> [Pu86] </ref>, SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability.
Reference: [Reed81] <author> D. Reed and L. Svobodova. SWALLOW: </author> <title> A Distributed Data Storage System for a Local Network. </title> <booktitle> In Networks for Computer Communications, </booktitle> <pages> pages 355-373, </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference-contexts: In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS [Walker83], and SWALLOW <ref> [Reed81] </ref>. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures. Their main market is on-line transaction processing. <p> Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW <ref> [Reed81] </ref>, and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability.
Reference: [Reuter84] <author> Andreas Reuter. </author> <title> Performance Analysis of Recovery Techniques. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> volume 9, number 4, </volume> <pages> pages 526-559, </pages> <month> December, </month> <year> 1984. </year>
Reference: [Rodehe91] <author> Thomas L. Rodeheffer and Michael D. Schroeder. </author> <title> Automatic Reconfiguration 127 in Autonet. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 183-197, </pages> <month> October, </month> <year> 1991. </year>
Reference-contexts: This replication scheme provides better availability than Sprite, but it is not simple and it costs more, because it uses extra hardware. Besides the cost of the extra servers and disks, the scheme uses special disk hardware and a high-bandwidth, low-latency network called Autonet <ref> [Rodehe91] </ref>. Echo disks are equipped with hardware that recognizes at most one server at a time as the disks owner. This is necessary, because it is possible for a backup server to try to take over for a primary that is merely slow but not actually down.
Reference: [Rosenb91] <author> M. Rosenblum and J. K. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <booktitle> Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15, </pages> <month> October, </month> <year> 1991. </year> <journal> Also published as Transactions on Computer Systems, </journal> <volume> volume 10, number 1, </volume> <pages> pages 26-52, </pages> <month> February, </month> <year> 1992. </year>
Reference-contexts: By combining fast state recovery with other fast recovery techniques, we can significantly improve overall recovery times. In particular, a disk storage manager such as LFS (the Log-structured File System) <ref> [Rosenb91] </ref> can recover the consistency of the file system on disk in only a few seconds. This contrasts with traditional UNIX file systems that require tens of minutes to ensure file system consistency after a failure. <p> In Sprite, fsck requires about 40 minutes to check five gigabytes of disk. We have reduced the time to check and recover the file system to about three seconds by switching to Men-del Rosenblums Log-Structured File System (LFS) <ref> [Rosenb91] </ref>. It was this switch that made it reasonable to consider speeding up the other aspects of recovery. Without this switch, other improvements would largely be unnoticed.
Reference: [Rosenb92] <author> Mendel Rosenblum. </author> <type> Personal Communication. </type> <year> 1992. </year>
Reference-contexts: The amount of time to roll forward from the log depends on the number of updates performed since the last checkpoint. Three seconds is a generously large estimate for a single disk, and multiple disks will be checked in parallel <ref> [Rosenb92] </ref>. The second part of recovering the file system is checking its logical structure, which can be corrupted by a media failure. Fsck performs this task, while Sprite LFS recovery does not.
Reference: [Salem86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Crash Recovery Mechanisms for Main Storage Database Systems. </title> <institution> CS-TR-034-86, Princeton University, Princeton, NJ, </institution> <year> 1986. </year>
Reference: [Sandbe85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> Proceedings of the Summer 1985 USENIX Conference, </booktitle> <pages> pages 119-130, </pages> <month> June, </month> <year> 1985. </year>
Reference-contexts: This change in technology has required a corresponding improvement in the performance and behavior of operating systems designed for distributed environments. Early network file systems were slow, often because they lacked tools to manage and to take advantage of their distributed resources. For example, Suns Network File System, NFS <ref> [Sandbe85] </ref>, is slower than more modern systems, because its file servers do not keep distributed state that describes how client workstations use their files. In contrast, modern distributed systems incorporate techniques for managing distributed resources efficiently and transparently. Distributed state is a key management tool. <p> However, the file server can lose this information if it crashes. Recovering this information after a server crash is the distributed cache state recovery problem. The following section explains the problem in more detail, organized as follows. For comparison, I first describe an older file system, NFS <ref> [Sandbe85] </ref>, that has made an opposite set of tradeoffs from Sprites. NFS has no state recovery problem, because it limits client file caching and provides only weak cache consistency. The section then lists the performance benefits Sprite gets 7 from caching file data, including modified (or dirty) data, on clients.
Reference: [Satyan90] <author> M. Satyanarayanan. </author> <title> Scalable, Secure, and Highly Available Distributed File Access. </title> <journal> IEEE Computer, </journal> <volume> volume 23, number 5, </volume> <month> May, </month> <year> 1990. </year>
Reference-contexts: A year-long trace of file activity in AFS <ref> [Satyan90] </ref>, a distributed system with over 400 users, also shows some sharing. In AFS, the chances of a file being modified by two different users in less than a day are between 0.09% and 0.72%, depending upon the type of file [Kistle91].
Reference: [Satyan93] <author> M. Satyanarayanan, Henry H. Mashburn, Puneet Kumar, David C. Steere, and James J. Kistler. </author> <title> Lightweight Recoverable Virtual Memory. </title> <booktitle> Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 146-160, </pages> <month> December, </month> <year> 1993. </year>
Reference-contexts: The design proposed storing the tail of the transaction log in the non-volatile memory. However, the actual implementation of POSTGRES pushes the log and data pages to disk on every transaction commit [Sulli93b]. Another similar technique is Recoverable Virtual Memory (RVM) <ref> [Satyan93] </ref>. RVM is a very lightweight transactional facility designed for UNIX applications with persistent data structures whose updates must be fault-tolerant.
Reference: [Seltz93a] <author> Margo Seltzer. </author> <title> File System Performance and Transaction Support. </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, </institution> <address> U. C. Berkeley. </address> <note> Available as Electronics Research Laboratory, </note> <institution> College of Engineering, University of California at Berkeley Memorandum No. UCB/ERL M93/1, </institution> <month> January, </month> <year> 1993. </year>
Reference-contexts: Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS [Specto85], Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and <ref> [Seltz93a] </ref>. 1.2.3.1. LOCUS LOCUS provides a UNIX-compatible, transparently distributed file system with automatic file replication for high availability. Transparent handling of replicated files is a major goal in LOCUS. Users can choose the desired degree of replication for files. <p> It was this switch that made it reasonable to consider speeding up the other aspects of recovery. Without this switch, other improvements would largely be unnoticed. There are two steps to recovering most file systems <ref> [Seltz93a] </ref>; checking the physical consistency of the file system on disk, and checking its logical structure. Both fsck and Sprite LFS recovery perform the first step.
Reference: [Seltz93b] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, and Carl Staelin. </author> <title> An Implementation of a Log-Structured File System for UNIX. </title> <booktitle> Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 307-326, </pages> <month> January, </month> <year> 1993. </year>
Reference-contexts: Fsck performs this task, while Sprite LFS recovery does not. For example, a media failure or software bug can corrupt the contents of a directory, so that files become inaccessible. This leaves the file system logically incorrect. The BSD implementation of LFS <ref> [Seltz93b] </ref> (a more recent LFS implementation than Sprites) considers this an important aw and thus performs the logical consistency check in background after roll-forward recovery from the log.
Reference: [Siegel89] <author> Alex Siegel, Kenneth Birman, and Keith Marzullo. Deceit: </author> <title> A Flexible Distributed File System. </title> <type> Technical report 89-1042. </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1989. </year>
Reference-contexts: ISIS also has an automatic distributed checkpoint mechanism. Although many applications have been built with ISIS, the most interesting in the context of this thesis is the Deceit file system <ref> [Siegel89] </ref>. Deceit is an NFS-compatible distributed file system that provides the user with per-file control over reliability and performance. For example, a user may choose higher reliability with less performance for some critical files, and higher performance with less reliability for often-accessed files.
Reference: [Siewio92] <author> Daniel P. Siewiorek and Robert S. Swarz. </author> <title> Reliable Computer Systems. </title> <note> DEC Press, 2nd edition, </note> <year> 1992. </year>
Reference: [Smith81] <author> W. B. Smith and F. T. Andrews. </author> <title> No. 5ESS Overview. </title> <booktitle> Proceedings of the Tenth International Switching Symposium, </booktitle> <address> Montreal, Canada, </address> <month> September, </month> <year> 1981. </year>
Reference-contexts: Error Repair To provide high availability, some systems attempt to detect and repair errors on-line. One name for this technique is error repair. It is also called forward error recovery. Tandems Integrity S2 [Jewett91] and AT&Ts 5ESS (electronic switching system) <ref> [Smith81] </ref> [Toy92a] are examples of systems that detect their own faults and repair themselves on-line, as described below. The appeal of error repair is that it does not incur the performance overhead of redundant hardware and software.
Reference: [Specto85] <author> A. Z. Spector, J. Butcher, D. S. Daniels, D. J. Duchamp, J. L. Eppinger, C. E. Fineman, A. Heddaya, and P. M. Schwarz. </author> <title> Support for Distributed Transactions in the TABS Prototype. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> volume 11, number 6, </volume> <pages> pages 520-530, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: While LOCUS and QuickSilver provide transactions as a part of the operating system (embedded transaction support), some systems provide transaction services in an environment or language built on top of the operating system. Transactional systems that I do not describe include Camelot [Bruell88], Argus [Liskov88], TABS <ref> [Specto85] </ref>, Cedar [Giffor88], Clouds [Dasgup88], Eden [Pu86], SWALLOW [Reed81], and V [Cherit84]. Tandem and Stratus, described previously, are also transactional systems. Further comparison of embedded transaction systems can be found in [Gray93] and [Seltz93a]. 1.2.3.1.
Reference: [Sriniv89] <author> V. Srinivasan and Jeffrey C. Mogul. Spritely NFS: </author> <title> Experiments with Cache 128 Consistency Protocols. </title> <booktitle> Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 45-57, </pages> <month> December, </month> <year> 1989. </year>
Reference-contexts: With server-driven recovery, the root server can recover first with the other servers, freeing up their resources more quickly. These combined advantages are likely to make server-driven recovery the distributed cache state recovery technique of choice for many systems. For example, Spritely NFS [Mogul92] <ref> [Sriniv89] </ref> has chosen to implement server-driven recovery, as described in chapter 2. This chapter has the following organization. The first section describes the design and implementation of server-driven recovery. Section 4.2 lists some disadvantages of this recovery tech 58 nique as compared to client-driven recovery.
Reference: [Stoneb86] <author> M. Stonebraker and L. Rowe. </author> <title> The Design of POSTGRES. </title> <booktitle> Proceedings of the Fifth ACM SIGMOD Conference, </booktitle> <month> June, </month> <year> 1986. </year>
Reference-contexts: Besides its performance benefits, an advantage of the recovery box is that it can be used for safe storage by other parts of the operating system and by user-level applications such as databases. 75 Later in this chapter I describe how a distributed version of the POSTGRES database <ref> [Stoneb86] </ref> uses the recovery box for fast crash recovery. The recovery box is generally useful for any system that wishes to preserve state information across failures, if it is possible to isolate the state updates, and if the state items have the following properties. <p> This section describes how one application, an experimental version of the POSTGRES database management system (DBMS) <ref> [Stoneb86] </ref>, uses the recovery box. 102 In this experimental version, written by Mark Sullivan [Baker92a], POSTGRES runs as an application program on a Sprite file server and responds to requests from client programs running on other Sprite machines.
Reference: [Stoneb87] <author> M. Stonebraker. </author> <title> The Design of the POSTGRES Storage System. </title> <booktitle> Readings 13th International Conference on Very Large Databases, </booktitle> <address> Brighton, England, </address> <month> September, </month> <year> 1987. </year>
Reference-contexts: In a conventional database management system, recovery includes the cost of write-ahead log processing (recovering disk state) in addition to client connection reestablishment. However, POSTGRES has an unconventional storage manager that maintains consistency of the data on disk without requiring write-ahead log processing <ref> [Stoneb87] </ref>, so it does not need the recovery box to avoid this costly recovery step. Without the recovery box, connection reestablishment is driven by the clients. When the database manager fails, all transactions executing on behalf of clients are aborted and all connection state held at the server is lost. <p> Authentication of the client is reverified when the DBMS receives the next message from the client. POSTGRES does not use the recovery box to store any of the state associated with its storage system. Storage system performance optimizations requiring non-volatile RAM are discussed in <ref> [Stoneb87] </ref>; for example, to reduce commit latency, committed data can be stored in non-volatile RAM instead of on disk. But this technique requires the operating system to guarantee that data stored in non-volatile memory be permanent.
Reference: [Stratu89] <author> Stratus Computer, Inc. </author> <title> VOS Transaction Processing Facility Guide. Part Number R215-00, </title> <month> November, </month> <year> 1989. </year>
Reference: [Sulliv90] <author> Mark Sullivan. </author> <title> Unpublished survey of software errors reported in 4.1 and 4.2 BSD UNIX. </title> <year> 1990. </year>
Reference-contexts: These two studies are summarized in Table 5-2 and are described below. The BSD UNIX study <ref> [Sulliv90] </ref> divides errors into synchronization (47%), exception-handling (12%), addressing (12%), and miscellaneous (29%) errors. Synchronization errors include problems such as deadlock or waiting endlessly for a signal from some event. Exception-handling errors are those that occur in code for handling other errors, including transient hardware errors.
Reference: [Sulliv91] <author> Mark Sullivan and Ram Chillarege. </author> <title> Software Defects and Their Impact on System Availability - A Study of Field Failures in Operating Systems. </title> <booktitle> Digest 21st International Symposium on Fault Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Addressing errors are those for which the software uses an incorrect memory address. They are the errors most likely to corrupt memory, since the software may modify bytes at the wrong location. For UNIX, addressing errors account for only 12% of errors. The MVS study <ref> [Sulliv91] </ref> classifies errors in terms of low-level programming errors, of which 41% were control problems, 30% were addressing errors, 8% were data miscalculations, and 21% were miscellaneous errors. Control errors include such problems as deadlock, in which the program stops without corrupting anything but transient state.
Reference: [Sulli93a] <author> Mark Sullivan. </author> <title> System Support for Software Fault Tolerance in Highly Available Database Management Systems. </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, University of California at Berkeley. </institution> <note> Available as Electronics Research Laboratory, </note> <institution> College of Engineering, </institution> <address> U. C. </address> <note> Berkeley Memorandum No. UCB/ERL M93/5, </note> <month> January, </month> <year> 1993 </year>
Reference-contexts: Statistics on the frequencies of different types of system outages indicate that this should be true for three reasons. These statistics were collected by Jim Gray [Gray90] and Mark Sullivan <ref> [Sulli93a] </ref> in Tandem, MVS, and UNIX systems. First, the majority of failures are due to software failures rather than permanent hardware problems that require long downtimes to fix. Second, the majority of software errors do not corrupt memory. <p> Software error type distributions. This table shows the distribution of software errors analyzed by Mark Sullivan in studies of error reports from the IBM MVS and 4.1/4.2 BSD UNIX operating systems <ref> [Sulli93a] </ref> [Baker92a]. The results columns give the percentage of errors that fall into each category. The two studies used different classification schemes, but both list addressing errors - the errors most likely to corrupt recovery box memory. The other error classes are described in the text.
Reference: [Sulli93b] <author> Mark Sullivan. </author> <type> Personal Communication, </type> <month> July 21, </month> <year> 1993. </year>
Reference-contexts: The design proposed storing the tail of the transaction log in the non-volatile memory. However, the actual implementation of POSTGRES pushes the log and data pages to disk on every transaction commit <ref> [Sulli93b] </ref>. Another similar technique is Recoverable Virtual Memory (RVM) [Satyan93]. RVM is a very lightweight transactional facility designed for UNIX applications with persistent data structures whose updates must be fault-tolerant.
Reference: [Tandem90] <author> Tandem. Sales brochures and price lists, </author> <month> October, </month> <year> 1990. </year>
Reference-contexts: Depending on how much customers are willing to pay, they can buy as much fault-tolerance as they want from Tandem. Tandem can even replicate entire geographic sites to prevent major disasters from bringing companies to a halt. However, Tandem systems can cost tens of millions of dollars <ref> [Tandem90] </ref>, and it is not easy to write fault-tolerant applications using their process pairs directly. In many computing environments the cost of this approach is too high. 1.2.2.2. TARGON/32 TARGON/32 is a UNIX-based system intended for an on-line transaction processing environment.
Reference: [Thomps87] <author> J. G. Thompson. </author> <title> Efficient Analysis of Caching Systems. </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, University of California at Berkeley. </institution> <note> Available as EECS technical report number UCB/CSD 87/374, </note> <month> October, </month> <year> 1987. </year>
Reference: [Thomps89] <author> James G. Thompson and Alan Jay Smith. </author> <title> Efficient (Stack) Algorithms for Analysis of Write-Back and Sector Memories. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> volume 7, number 1, </volume> <pages> pages 78-116, </pages> <month> February </month> <year> 1989. </year>
Reference: [Toy92a] <author> L. C. </author> <title> Toy. Part II: Large-Scale Real-Time Program Retrofit Methodology in AT&T 5ESS Switch. In Reliable Computer Systems, </title> <editor> Daniel P. Siewiorek and Robert S. </editor> <booktitle> Swarz, </booktitle> <pages> pages 574-586, </pages> <note> DEC Press, 2nd edition, </note> <year> 1992. </year>
Reference-contexts: Error Repair To provide high availability, some systems attempt to detect and repair errors on-line. One name for this technique is error repair. It is also called forward error recovery. Tandems Integrity S2 [Jewett91] and AT&Ts 5ESS (electronic switching system) [Smith81] <ref> [Toy92a] </ref> are examples of systems that detect their own faults and repair themselves on-line, as described below. The appeal of error repair is that it does not incur the performance overhead of redundant hardware and software.
Reference: [Toy92b] <author> W. N. </author> <title> Toy. Part I: Fault-Tolerant Design of AT&T Telephone Switching System Processors. In Reliable Computer Systems, </title> <editor> Daniel P. Siewiorek and Robert S. </editor> <booktitle> Swarz, </booktitle> <pages> pages 533-574, </pages> <note> DEC Press, 2nd edition, </note> <year> 1992. </year>
Reference-contexts: It can be difficult to determine if all possible errors have been assigned recovery code. It can also be difficult to determine if the recovery code is correct. 21 AT&Ts 5ESS [Smith81][Toy92a] <ref> [Toy92b] </ref> is another example of a system that combines error repair with redundant hardware and software. 5ESS detects and repairs faults in both its hardware and software, and can even make hardware and software upgrades, all without interruption in service.
Reference: [Walker83] <author> Walker, Popek, English, Kline and Thie. </author> <title> The LOCUS Distributed Operating System. </title> <booktitle> Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pages 49-70, </pages> <month> October, </month> <year> 1983. </year>
Reference-contexts: In other systems, the extra processors run different jobs and therefore contribute to the overall processing power. In this section I describe Tandem [Bartle81][Bartle90], Stratus [Stratu89][Webber92], TAR-GON/32 [Borg83][Borg89], Zebra [Hartma93], and ISIS [Birman84][Birman89]. Some other systems using replication are Andrew [Howard88], Cedar [Giffor88], Eden [Pu86], Grapevine [Birrel82], LOCUS <ref> [Walker83] </ref>, and SWALLOW [Reed81]. There is also some work on replicated recoverable processes [Babaog90] on Mach [Accett86]. 1.2.2.1. Tandem NonStop Systems Tandems NonStop Systems are some of the best-known highly available systems and use hardware and software redundancy to tolerate both hardware and software failures.
Reference: [Webber92] <editor> Steven Webber. </editor> <booktitle> The Stratus Architecture. In Reliable Computer Systems, </booktitle> <editor> Daniel P. Siewiorek and Robert S. </editor> <booktitle> Swarz, </booktitle> <pages> pages 648-670, </pages> <note> DEC Press, 2nd edition, 1992. 129 </note>
Reference: [Welch86] <author> Brent Welch. </author> <title> The Sprite Remote Procedure Call System. </title> <type> Technical Report Number UCB/CSD 86/302, </type> <institution> Computer Science Division, </institution> <address> U. C. Berkeley, </address> <month> June, </month> <year> 1986. </year>
Reference-contexts: Sprite clients do this using a low-level mechanism that monitors message traffic. Hosts communicate with each other in Sprite using kernel-to-kernel remote procedure calls (RPCs) <ref> [Welch86] </ref>. A client issues a request of the file server by sending it an RPC. One of several RPC server processes in the servers kernel handles the request. The server then sends the results of the request back to the client in a response to the RPC. <p> List of blocked RPCs. This table lists RPCs that are blocked for all clients during recovery. Most of these RPCs change the distributed cache state or other file system state and could cause cache consistency violations or other inconsistencies during recovery. For more information about these RPCs, please see <ref> [Welch86] </ref> and [Welch90] RPC Purpose open Open a file or other object. read Read from a file or other object. write Write to a file or other object. close Close a file or other object. unlink Unlink a file or other object. rename Rename a file or other object. mkdir Create <p> Table 4-2. List of Unblocked RPCs. This table lists RPCs that are not blocked for clients during recovery. Most of these RPCs are needed as part of recovery. For more information about these RPCs, please see <ref> [Welch86] </ref> and [Welch90] 67 This combination of restrictions and complications means that there are many types of locks held simultaneously during recovery and care is required to avoid deadlock. There is an exclusive access lock (monitor lock) around some of the device code to protect the device data structures.
Reference: [Welch88] <author> Brent B. Welch and John K. Ousterhout. </author> <title> Pseudo Devices: User-Level Extensions to the Sprite File System. </title> <booktitle> Proceedings of the Summer 1988 USENIX Conference, </booktitle> <pages> pages 37-49, </pages> <month> June, </month> <year> 1988. </year>
Reference-contexts: Clients next recover their I/O handles. I/O handles store information about objects in the file system, such as files and directories, devices, pipes, and pseudo devices. (A pseudo-device is a named object in the file system that is actually implemented by a user-level software process <ref> [Welch88] </ref>.) An I/O handle is similar to a UNIX inode, but also includes information such as the number of open references (streams) that the client has for an object, and whether the streams are open for reading or writing. <p> State information sent from clients to the server for recovery. For each type of handle, this table gives the information sent by clients to the server for crash recovery. Important fields are described in the text. Pseudo-devices are devices implemented by user-level processes. Please see <ref> [Welch88] </ref> for further details. 40 In particular, the purpose of the servers file I/O handles is to maintain cache consistency among the clients. The server must know which clients have open references or cached dirty blocks for files.
Reference: [Welch90] <author> Brent Ballinger Welch. </author> <title> Naming, State Management, and User-Level Extensions in the Sprite Distributed File System. </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, University of California at Berkeley. </institution> <note> Available as EECS technical report number UCB/CSD 90/567, </note> <month> April, </month> <year> 1990. </year>
Reference-contexts: Chapters 3 through 5 describe and evaluate three different techniques for recovering distributed state. Chapter 3 covers the first technique, called client-driven recovery. In this form of recovery, first designed and implemented by Brent Welch <ref> [Welch90] </ref>, client workstations of the file server keep track of which of the servers files and other objects they have cached or opened. When they detect that the file server has crashed and rebooted, the clients send their state information through to the file server. <p> This table lists RPCs that are blocked for all clients during recovery. Most of these RPCs change the distributed cache state or other file system state and could cause cache consistency violations or other inconsistencies during recovery. For more information about these RPCs, please see [Welch86] and <ref> [Welch90] </ref> RPC Purpose open Open a file or other object. read Read from a file or other object. write Write to a file or other object. close Close a file or other object. unlink Unlink a file or other object. rename Rename a file or other object. mkdir Create a directory. <p> Table 4-2. List of Unblocked RPCs. This table lists RPCs that are not blocked for clients during recovery. Most of these RPCs are needed as part of recovery. For more information about these RPCs, please see [Welch86] and <ref> [Welch90] </ref> 67 This combination of restrictions and complications means that there are many types of locks held simultaneously during recovery and care is required to avoid deadlock. There is an exclusive access lock (monitor lock) around some of the device code to protect the device data structures.
References-found: 95

