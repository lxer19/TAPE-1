URL: http://www.cs.duke.edu/~jsv/Papers/Vit87.seq_sampling.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node69.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: (page 1) An Efficient Algorithm for Sequential Random Sampling Une Methode Efficace pour Former Sequentiellement
Author: Jeffrey Scott Vitter ; 
Abstract: We examine several methods for drawing a sequential random sample of n records from a file containing N records. Method D, which was introduced in [10], is recommended for general use. The algorithm is online (so that CPU time can be overlapped with I/O), has a small constant memory requirement, and is easy to program. An improved implementation is given in the Appendix. Resume. Nous considerons plusieures methodes pour former sequentiellement un echantillon de n elements d'un fichier de taille N. Nous recomman-dons hh la Methode D ii , qui etait introduite par [10], pour un usage general. Le calcul peut ^etre fait en parallele avec les entrees/sorties, il n'y a pas besoin de beaucoup de memoire, et l'algorithme est facile a pro grammer. Une implementation optimisee est donnee dans l'Appendice.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. H. Ahrens and U. Dieter. </author> <title> Computer Methods for Sampling from the Exponential and Normal Distributions. </title> <journal> Communications of the ACM, </journal> <volume> 15, </volume> <month> 10 (October </month> <year> 1972), </year> <pages> 873-882. </pages>
Reference-contexts: It is possible to generate an exponential random variate faster than by computing ln U , where U is a uniform random variate, by direct techniques coded in assembly language <ref> [1] </ref>, [9]. If such a routine is available, all three methods can be sped up. The way to do that for Method D is described in the Appendix.
Reference: 2. <author> J. H. Ahrens and U. Dieter. </author> <title> Sequential Random Sampling. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11, </volume> <month> 2 (June </month> <year> 1985), </year> <pages> 157-169. </pages>
Reference-contexts: The implementation is easy to program (it is given in the Appendix). Method D solves the open problem presented in [9, ex. 3.4.2-8]. Two interesting twopass methods for sequential random sampling, called Methods SG and SG*, were developed by Ahrens and Dieter <ref> [2] </ref>. Methods SG and SG* were quoted in [2] to be faster than Method D by factors of 4-5 and 2-2.5, respectively. It turns out that an inferior implementation of Method D was used in the testing, not the one given explicitly on page 716 in [10]. <p> Method D solves the open problem presented in [9, ex. 3.4.2-8]. Two interesting twopass methods for sequential random sampling, called Methods SG and SG*, were developed by Ahrens and Dieter <ref> [2] </ref>. Methods SG and SG* were quoted in [2] to be faster than Method D by factors of 4-5 and 2-2.5, respectively. It turns out that an inferior implementation of Method D was used in the testing, not the one given explicitly on page 716 in [10]. We have redone the experiments with a better implementation. <p> This requires some extra storage, and the program goes o*ine from time to time to do the random number generation. 3. Comparisons In this section we compare the performance of Method D with that of the twopass random sampling schemes Methods SG and SG* from <ref> [2] </ref>. The latter two algorithms are not online, but do run in O (n) time. Let us begin with a sketch of how Methods SG and SG* work. <p> The programs were compiled and run on an IBM 3081 mainframe computer; optimization level 1 was used, since it produced the fastest code. A Pascal-like implementation of Method D is given in the Appendix; the FORTRAN 77 version is a direct translation of it. The comparisons given in <ref> [2] </ref> involving Method D are misleading, because the timing experiments use the inferior implementation of Method D in [6], not the simpler and more efficient version given on page 716 in [10] or the improved version given in the Appendix of this paper. <p> If such a routine is available, all three methods can be sped up. The way to do that for Method D is described in the Appendix. There are several published nonsequential random sampling methods (for example, see [9], [4], and <ref> [2] </ref>), which typically use a hash table of size O (n). Sequential samples can be obtained in a second pass by sorting, also in O (n) time.
Reference: 3. <author> J. L. Bentley. </author> <title> Personal communication (April 1983). </title> <note> See page 713 of [10]. </note>
Reference-contexts: The final n indices are then chosen via Method S (or better yet, via Method A) using a different sequence of pseudorandom numbers. The resulting algorithm is called Method SG*. Methods SG and SG* are very similar to the clever algorithms proposed by Bentley <ref> [3] </ref>. The main difference is that Bentley obtains a sorted random sample of real numbers by repeatedly generating random variates X using (2.3); the parameters n and N change dynamically as the sampling progresses. The sample of reals is then truncated to integers to complete pass 1.
Reference: 4. <author> J. Ernvall and O. Nevalainen. </author> <title> An Algorithm for Unbiased Random Sampling. </title> <journal> The Computer Journal, </journal> <volume> 25, </volume> <month> 1 (January </month> <year> 1982), </year> <pages> 45-47. </pages>
Reference-contexts: If such a routine is available, all three methods can be sped up. The way to do that for Method D is described in the Appendix. There are several published nonsequential random sampling methods (for example, see [9], <ref> [4] </ref>, and [2]), which typically use a hash table of size O (n). Sequential samples can be obtained in a second pass by sorting, also in O (n) time.
Reference: 5. <author> C. T. Fan, M. E. Muller, and I. Rezucha. </author> <title> Development of Sampling Plans by Using Sequential (Item by Item) Selection Techniques and Digital Computers. </title> <journal> American Statistical Assn. Journal, </journal> <month> 57 (June </month> <year> 1962), </year> <pages> 387-402. </pages>
Reference-contexts: Conclusions are given in Section 4. Efficient and improved implementations of Method D and another method, called Method A, appear in the Appendix. 2. Method D We begin by discussing the simplest of all sequential random sampling methods, due to Fan, Muller, and Rezucha <ref> [5] </ref> and Jones [7], which is called Method S by Knuth [9]. An independent uniform random variate (from the unit interval) is generated for each record in the file in order to determine whether the record should be chosen for the sample.
Reference: 6. <author> H. Gehrke. Einfache sequentielle Stichprobenentnahme. </author> <title> Diplomarbeit, </title> <address> Universitat Kiel, Kiel, West Germany (August 1984). </address>
Reference-contexts: A Pascal-like implementation of Method D is given in the Appendix; the FORTRAN 77 version is a direct translation of it. The comparisons given in [2] involving Method D are misleading, because the timing experiments use the inferior implementation of Method D in <ref> [6] </ref>, not the simpler and more efficient version given on page 716 in [10] or the improved version given in the Appendix of this paper.
Reference: 7. <author> T. G. Jones. </author> <title> A Note on Sampling a Tape File. </title> <journal> Communications of the ACM, </journal> <volume> 5, </volume> <month> 6 (June </month> <year> 1962), </year> <month> 343. </month>
Reference-contexts: Conclusions are given in Section 4. Efficient and improved implementations of Method D and another method, called Method A, appear in the Appendix. 2. Method D We begin by discussing the simplest of all sequential random sampling methods, due to Fan, Muller, and Rezucha [5] and Jones <ref> [7] </ref>, which is called Method S by Knuth [9]. An independent uniform random variate (from the unit interval) is generated for each record in the file in order to determine whether the record should be chosen for the sample.
Reference: 8. <author> J. Kawarasaki and M. Sibuya. </author> <title> Random Numbers for Simple Random Sampling Without Replacement. </title> <journal> Keio Math. Sem. Rep, </journal> <volume> No. 7 (1982), </volume> <pages> 1-9. </pages>
Reference-contexts: The Appendix also gives an efficient version of Method A, which improves the running time of the basic Method S by a factor of 3-4. Other algorithms for sequential random sampling are described and analyzed in [10] and <ref> [8] </ref>. Appendix In this appendix we present Pascal-like implementations for Methods A and D. Variables of type real should be double precision so that roundoff error is insignificant, even when N is very large. Roughly log 10 N + 1 digits of precision will suffice.
Reference: 9. <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming. Volume 2: Seminumerical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (second edition 1981). </address>
Reference-contexts: The memory requirement of the program is a small constant. 3. The implementation is easy to program (it is given in the Appendix). Method D solves the open problem presented in <ref> [9, ex. 3.4.2-8] </ref>. Two interesting twopass methods for sequential random sampling, called Methods SG and SG*, were developed by Ahrens and Dieter [2]. Methods SG and SG* were quoted in [2] to be faster than Method D by factors of 4-5 and 2-2.5, respectively. <p> Efficient and improved implementations of Method D and another method, called Method A, appear in the Appendix. 2. Method D We begin by discussing the simplest of all sequential random sampling methods, due to Fan, Muller, and Rezucha [5] and Jones [7], which is called Method S by Knuth <ref> [9] </ref>. An independent uniform random variate (from the unit interval) is generated for each record in the file in order to determine whether the record should be chosen for the sample. <p> It is possible to generate an exponential random variate faster than by computing ln U , where U is a uniform random variate, by direct techniques coded in assembly language [1], <ref> [9] </ref>. If such a routine is available, all three methods can be sped up. The way to do that for Method D is described in the Appendix. There are several published nonsequential random sampling methods (for example, see [9], [4], and [2]), which typically use a hash table of size O <p> a uniform random variate, by direct techniques coded in assembly language [1], <ref> [9] </ref>. If such a routine is available, all three methods can be sped up. The way to do that for Method D is described in the Appendix. There are several published nonsequential random sampling methods (for example, see [9], [4], and [2]), which typically use a hash table of size O (n). Sequential samples can be obtained in a second pass by sorting, also in O (n) time.
Reference: 10. <author> J. S. Vitter. </author> <title> Faster Methods for Random Sampling. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <month> (July </month> <year> 1984), </year> <pages> 703-718. </pages>
Reference-contexts: I., USA. An earlier version of this manuscript appeared as Brown University Technical Report CS-86-12. 2 / Sequential Random Sampling In this paper we reaffirm the efficiency and practicality of Method D introduced in <ref> [10] </ref> and present some implementation improvements. We recommend it as the method of choice for sequential random sampling. The main features of Method D are: 1. It is online|that is, it requires no preprocessing and can generate each element of the sample in constant expected time. 2. <p> Methods SG and SG* were quoted in [2] to be faster than Method D by factors of 4-5 and 2-2.5, respectively. It turns out that an inferior implementation of Method D was used in the testing, not the one given explicitly on page 716 in <ref> [10] </ref>. We have redone the experiments with a better implementation. Method D uses more CPU time than Method SG in most cases, but always less than Method SG*. <p> The mean value of S (n; N ) is (N n)=(n+1), and the standard deviation is roughly equal to the mean. The full derivations of most of the results presented in this section appear in <ref> [10] </ref>. Method S can be expressed in this framework: each value of the skip distance S (n; N ) is generated in O (S + 1) time using S + 1 uniform random variates. <p> a) using a call to the exponential and logarithm library functions.) Our choice of parameters is given below: g (x) = &lt; n x n1 0, otherwise; c = N n + 1 h (s) = &gt; &lt; n s n1 0, otherwise. (2:4) The resulting algorithm is proved in <ref> [10] </ref> to run in O (n) time, on the average. The running time can be cut by more than half by clever implementation. <p> When 1=n is sufficiently small with respect to n=N , the average number of times that Step D2 is done before acceptance is reached can be reduced by using a different distribution for X, namely, the geometric distribution given in <ref> [10] </ref>. The second optimization comes into play when n is sufficiently small and when X cannot be generated via (2.9). An alternative to (2.3) is to obtain X directly by generating n independent uniform real numbers from the unit interval, selecting the smallest, and multiplying it by N . <p> The comparisons given in [2] involving Method D are misleading, because the timing experiments use the inferior implementation of Method D in [6], not the simpler and more efficient version given on page 716 in <ref> [10] </ref> or the improved version given in the Appendix of this paper. Methods S and A, which are not included in the above tables, use roughly 12N and 3:5N + 8n microseconds of CPU time, respectively, to complete the sampling. <p> The Appendix also gives an efficient version of Method A, which improves the running time of the basic Method S by a factor of 3-4. Other algorithms for sequential random sampling are described and analyzed in <ref> [10] </ref> and [8]. Appendix In this appendix we present Pascal-like implementations for Methods A and D. Variables of type real should be double precision so that roundoff error is insignificant, even when N is very large. Roughly log 10 N + 1 digits of precision will suffice.
References-found: 10

