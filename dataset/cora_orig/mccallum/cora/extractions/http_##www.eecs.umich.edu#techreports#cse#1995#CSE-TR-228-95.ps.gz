URL: http://www.eecs.umich.edu/techreports/cse/1995/CSE-TR-228-95.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse95.html
Root-URL: http://www.eecs.umich.edu
Email: blj@umich.edu  
Title: Optimization of Mass Storage Hierarchies of more than a factor of two. Historically, much emphasis
Author: Bruce Jacob 
Note: 1.0 Introduction mance  [9]; however, little attention has been paid to how all the  
Address: Michigan  
Affiliation: Advanced Computer Architecture Lab EECS Department, University of  Tech  
Pubnum: Report CSE-TR-228-95  
Abstract: May 12, 1994 Optimization is often a question of where one should put ones money in improving performance. As far as large storage hierarchies go, intuition suggests (and common practice supports) adding as much as is affordable of the fastest technology available. Many cache hierarchy studies have shown that this is often not the optimal approach, and we show that for mass storage hierarchies it always tends to be the wrong approach. For large data sets, as is the case for network file servers, a machine with no RAM and several gigabytes of disk performs 30% faster than a machine with no disk and a cost-equivalent amount of RAM. This paper presents a mathematical analysis of the optimization of an I/O hierarchy, as well as trace driven simulations of a network file server in support of the analysis. Over the past several years, there has been a substantial increase in the speed and capacity demands placed on computer memory systems. Great strides have been made in the capacity of mass storage devices such as magnetic disk, tape, and optical media, but improvements in the speed of these devices has not kept up with the improvements in CPU speeds and semiconductor memory. Caching is used to hide the deficiency, but the widening gap between semiconductor memory and magnetic storage is making it easy to lose much performance through poor system configuration. Larger miss costs need to be offset by higher hit rates, inducing system administrators to buy more and more main memory for workstations; this is not necessarily the correct thing to do, especially in the case of file servers. In this environment, it is very important to spend ones money wisely, as the cost figures are staggering and $5,000 misplaced can result in a difference in perfor number of cache levels increase with an increasing data set size [5][6][14], and yet as our systems grow larger we continue to pour RAM into systems instead of adding to the caches on disk, or even more levels in the hierarchy. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Advertisements, </author> <note> Computer Shopper , April 1994. </note>
Reference-contexts: Optimization of Mass Storage Hierarchies 2.1 Volatile Semiconductor Memory Volatile semiconductor memory, typically DRAM, is what is primarily used as the top level of the storage hierarchy. DRAMs can be accessed in tens to hundreds of nanoseconds (typically 80), and cost approximately $30 per megabyte <ref> [1] </ref>. DRAMS have a transfer rate of roughly 160 MB/s, derived from a latency to first access on a page of 80 ns and subse quent accesses to the same page are on the order of 25 ns [12]. <p> Typical bandwidth numbers for disk are around 10 MB/s, average latencies are around 10 ms, and for large drives the cost is $0.50 per MB <ref> [1] </ref>. Optimization of Mass Storage Hierarchies 2.5 Magnetic Tape Traditionally, magnetic tape has been used as a backup file system not very well integrated with the rest of the devices in the hierarchy.
Reference: 2. <author> Antonelli, C.J. and Honeyman, P., </author> <title> Integrating Mass Storage and File Systems, </title> <booktitle> Twelfth IEEE Symposium on Mass Storage Systems , pp. </booktitle> <pages> 133-138, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: As storage requirements become greater, the use of more cost-effective storage media, such as magnetic tape and optical disks, become an integral part of the storage hierarchy. This paper assumes a system configuration similar to that described in Antonelli, et al. <ref> [2] </ref>; a large tertiary storage device housing the entire file system, with RAM and magnetic disks acting as L1 and L2 caches on top. The following sections describe the various technologies used in storage hierarchies, including their strengths, their weaknesses, their costs and their performance numbers. <p> Most installations actually use magnetic disks not as a cache to the file system but to store the entire file system itself. As suggested in <ref> [2] </ref> and in talks given by Antonelli this tends to be an increasing waste of precious resources as file systems get larger. Although the access time for magnetic disks have been improving at a very slow rate, capacity continues to grow rapidly.
Reference: 3. <author> Baker, M., Asami, S., Deprit, E., Ousterhout, J. and Seltzer , M., </author> <title> NonVolatile Memory for Fast Reliable File Systems, </title> <booktitle> Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pp. 20-22, </pages> <month> October </month> <year> 1992. </year> <title> Optimization of Mass Storage Hierarchies </title>
Reference-contexts: Once an item of data is in nonvolatile RAM it is as safe as on disk and need not be ushed out just to ensure its integrity. The result is faster writes through the use of NVRAM <ref> [3] </ref>.
Reference: 4. <author> Blumson, S., Honeyman, P ., Ragland, T. E. and Stolarchuk, M. T., </author> <title> AFS Server Logging, </title> <type> CITI Technical Report 93-10, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: the minimum increment to spend on an upgrade would be $256, which should buy roughly 1/2 GB of disk space or 8 MB of RAM. 4.1 Workload Description The data that was used for the workload in the tracedriven hierarchy simulations was collected by CITI via their logging AFS server <ref> [4] </ref>. The only data that the server sees are those accesses not serviced from the clients local cache. We use approximately one months worth of trace data captured from a server named marge from April 14, 1992 through May 8, 1992.
Reference: 5. <author> Chow, C. K., </author> <title> On Optimization of Storage Hierarchies, </title> <journal> IBM Journal of Research and Development , Vol. </journal> <volume> 18, No. 3, </volume> <month> May </month> <year> 1974. </year>
Reference-contexts: This makes it very easy to find closed form solutions to the optimization problem. 3.1 Welchs Analysis Welch took a model of a memory hierarchy from Chow <ref> [5] </ref> and concluded that if you have a fixed budget and a known set of probabilities of hitting any given level of a memory hierarchy, you can find the optimal appropriation of funds to the various hierarchy levels, so that the average effective access time is minimized.
Reference: 6. <author> Chow, C. K., </author> <title> Determination of Caches Capacity and its Matching Storage Hierarchy, </title> <journal> IEEE Transactions on Computers , Vol C-25, </journal> <volume> No. 2, </volume> <month> February </month> <year> 1976. </year>
Reference: 7. <author> Copeland, G., Keller, T., Krishnamurthy, R. and Smith, M., </author> <title> The Case for Safe RAM, </title> <booktitle> Proceedings of the Fifteenth International Conference on Very Large Databases , pp. </booktitle> <pages> 327-335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Currently, NVRAMs have similar access times to DRAMs, but slightly higher costs per megabyte. The most common type of NVRAM is battery-backed DRAM. This configuration obviously has the same access latency of regular DRAM, but with the additional cost of the battery or uninterruptible power supply. Safe RAM <ref> [7] </ref> is implemented in this form of NVRAM. The advantage that nonvolatile memory holds over volatile RAM is that it can truly be used in place of disk.
Reference: 8. <author> Drapeau, A. L. and Katz, R. H., </author> <title> Striped Tape Arrays, </title> <booktitle> Twelfth IEEE Symposium on Mass Storage Systems pp. </booktitle> <pages> 257-265, </pages> <month> April, </month> <year> 1993. </year>
Reference-contexts: Another class of research involves the reconfiguring of traditional I/O devices in novel ways, rather than adding new types. Combining devices such as magnetic disk drives into disk arrays exploits parallelism yielding an increase in band width [10]. Tape striping performs a similar optimization for magnetic tape drives <ref> [8] </ref>. However, in the many papers on making large storage systems faster, there has been little mention of the enormous wealth of research done to optimize cache hierarchies. <p> Assuming the higher levels of the hierarchy service most of the file accesses, the average access time for accessing data in a storage system backed by magnetic tape can remain reasonable. Similar to magnetic disks, tape striping has been proposed to improve the bandwidth of magnetic tape devices <ref> [8] </ref>. Bandwidth numbers for cartridge tape drives range from 100 KB/s to 50 MB/s, average seek times are around 10 sec to 40 sec, depending upon the technology and capacity of the cartridge [8]. 2.6 Optical and Magneto-Optical Disk Slow to become mainstream for a variety of reasons, optical disks nevertheless <p> Similar to magnetic disks, tape striping has been proposed to improve the bandwidth of magnetic tape devices <ref> [8] </ref>. Bandwidth numbers for cartridge tape drives range from 100 KB/s to 50 MB/s, average seek times are around 10 sec to 40 sec, depending upon the technology and capacity of the cartridge [8]. 2.6 Optical and Magneto-Optical Disk Slow to become mainstream for a variety of reasons, optical disks nevertheless have the potential to replace magnetic tape as the tertiary storage media of choice.
Reference: 9. <author> Garcia-Molina, H., Park, A., and Rogers, L., </author> <title> Performance Through Memory, </title> <booktitle> Proceedings of the 1987 ACM SIGMETRICS Conference, </booktitle> <pages> pp. 122-131, </pages> <month> May, </month> <year> 1987. </year>
Reference-contexts: Historically, much emphasis and attention has been paid to optimizing cache hierarchies [5][6][11][14][19], to managing large stores of data, and to having enough memory in a system <ref> [9] </ref>; however, little attention has been paid to how all the optimizations fit together.
Reference: 10. <author> Katz R. H., Gibson, G. A. and Patterson, D. A., </author> <title> Disk System Architectures for High Performance Comput ing, </title> <booktitle> Proceedings of the IEEE , pp. </booktitle> <pages> 1842-1858, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Another class of research involves the reconfiguring of traditional I/O devices in novel ways, rather than adding new types. Combining devices such as magnetic disk drives into disk arrays exploits parallelism yielding an increase in band width <ref> [10] </ref>. Tape striping performs a similar optimization for magnetic tape drives [8]. However, in the many papers on making large storage systems faster, there has been little mention of the enormous wealth of research done to optimize cache hierarchies. <p> Although the access time for magnetic disks have been improving at a very slow rate, capacity continues to grow rapidly. Striping data among several disks in a disk array, such as in RAID <ref> [10] </ref> can improve the bandwidth of magnetic disk devices, but the single-request latency remains high for small requests. Typical bandwidth numbers for disk are around 10 MB/s, average latencies are around 10 ms, and for large drives the cost is $0.50 per MB [1]. <p> We shall see that the first dollar will always be spent upon disk. This agrees with results from <ref> [10] </ref>, [18], and [19]; when the amount of money to spend on the cache is small, capacity counts for more than speed.
Reference: 11. <author> MacDonald, J. E. and Sigworth, K. L., </author> <title> Storage Hierarchy Optimization Procedure, </title> <journal> IBM Journal of Research and Development , Vol. </journal> <volume> 19, No. 2, </volume> <month> March </month> <year> 1975. </year>
Reference: 12. <institution> Mitsubishi Electric, Data Book: Semiconductor Memories/RAM </institution>
Reference-contexts: DRAMS have a transfer rate of roughly 160 MB/s, derived from a latency to first access on a page of 80 ns and subse quent accesses to the same page are on the order of 25 ns <ref> [12] </ref>.
Reference: 13. <author> Nakagomi, T., Holzbach, M., Van Meter , R. and Ranade, S., </author> <title> ReDefining the Storage Hierarchy: An Ultra Fast Magneto-Optical Disk Drive, </title> <booktitle> Twelfth IEEE Symposium on Mass Storage Systems , April 1993. </booktitle>
Reference-contexts: The enormity of I/O space is changing the way computer systems are being used, designed, and opti mized. There have been a number of research projects exploring this new frontier of computing. Nakagomi, et al, <ref> [13] </ref> suggested replacing the entire hierarchy with a single large, fast magneto-optical array, trading the top level access time for a much higher bandwidth. <p> Optical disks have a price-per-bit similar to magnetic tape and allow efficient, random access to large volumes of data and could very well prove to be the more effective of the two [15]. In fact, Nakagomi, et al. <ref> [13] </ref> suggest replacing everything from the magnetic disk array on down to the tape backup devices with one large (terabyte-sized) magneto-optical device.
Reference: 14. <author> Przybylski, S. A., </author> <title> Cache and Memor y Hierarchy Design: a Performance-Dir ected Approach , Morgan Kauf-mann, </title> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: gigabytes of disk cache and Remember that this paper is concerned with server machines, which is analogous to looking at level-2 caches; the program traces seen by a L2 cache have been filtered by the L1 cache and exhibit less locality of reference, driving miss ratios up by large factors <ref> [14] </ref>. It is not surprising, then, that in order to improve performance of a server machine, size counts more than speed. <p> In order to simplify implementation, writes were treated similarly to reads in a read-modify-write manner. Future studies will deal more with this issue. Przybylski ignores writes altogether in his cache analysis <ref> [14] </ref>, but it is not the case that writes can be simply dispensed with in an I/O hierarchy, as the cost must be accounted for somewhere. 4.2.1 Memory Object The memory module keeps lists of blocks of user data in a roughly LRU-type set associative cache configuration.
Reference: 15. <author> Quinlan, S., </author> <title> A Cached WORM File System, </title> <journal> Software Practice and Experience , pp. </journal> <pages> 1289-1299, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Optical disks have a price-per-bit similar to magnetic tape and allow efficient, random access to large volumes of data and could very well prove to be the more effective of the two <ref> [15] </ref>. In fact, Nakagomi, et al. [13] suggest replacing everything from the magnetic disk array on down to the tape backup devices with one large (terabyte-sized) magneto-optical device.
Reference: 16. <author> Ranade, S., </author> <title> Mass Storage Technologies , Meckler Publishig, </title> <address> Westport, CT, </address> <year> 1991. </year>
Reference-contexts: Optical disk bandwidth figures range from less than 1 MB/s to just over 10 MB/s, average seek times range from 60 ms for smaller drives to 7 seconds for large jukeboxes <ref> [16] </ref>. 3.0 Mathematical Analysis Quite a bit has been published on the optimization of cache hierarchies. Chow showed that the optimum number of cache levels scales with the logarithm of the capacity of the cache hierarchy [5][6].
Reference: 17. <author> Redfield, R. and Willenbring, J., </author> <booktitle> Holostore Technology for Higher Levels of Memory Hierarchy , Eleventh IEEE Symposium on Mass Storage Systems , pp. </booktitle> <pages> 155-159, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several papers [3][7][20] suggest the use of nonvolatile RAM to improve I/O performance by providing a reliable write-buffer to reduce physical write traffic and improve response time. Finally, future devices such as holostore <ref> [17] </ref> are projected to fit nicely in between semiconductor memory and magnetic disk in the storage hierarchy. Holostore technology offers capacity that approaches the density of magnetic disks with access times closer to semiconductor memory. <p> The storage capacity of holostore is competitive with magnetic or optical disk; since it is a 3-D storage media, its volumetric storage density is extremely high <ref> [17] </ref>. Unfortunately, it is not yet available. 2.4 Magnetic Disk Magnetic disks are most commonly used for online storage. Access latencies for magnetic disks make them barely fast enough to be considered online, and they are still far too slow to communicate directly with a processor.
Reference: 18. <author> Rege, S. L., </author> <title> Cost, Performance, and Size Tradeoffs for Different Levels in a Memory Hierarchy , Computer Vol. </title> <journal> 9, </journal> <volume> No. 4, </volume> <month> April </month> <year> 1976. </year>
Reference-contexts: We shall see that the first dollar will always be spent upon disk. This agrees with results from [10], <ref> [18] </ref>, and [19]; when the amount of money to spend on the cache is small, capacity counts for more than speed.
Reference: 19. <author> Welch, T., </author> <title> Memory Hierarchy Confi guration Analysis, </title> <journal> IEEE Transactions on Computers , Vol. C-27, </journal> <volume> No. 5, </volume> <month> May </month> <year> 1978. </year>
Reference-contexts: Rege and Garcia-Molina demonstrated that it is often better to have more of a slower device than less of a faster device [9][18]. Welch showed that the optimal size of each level must be proportional to the amount of time spent servicing requests out of that level <ref> [19] </ref>. This paper builds upon previous cache studies with one major twist. Previous studies have been able to find solutions for optimal hierarchy configurations, but the results contained dependencies upon the cache configuration; the number of levels or the sizes of the levels or the hit rates of the levels. <p> This optimal appropriation leads to such a balanced memory hierarchy when the proportion of money spent at a given level is equal to the proportion of time spent at that level servicing requests <ref> [19] </ref>. <p> We shall see that the first dollar will always be spent upon disk. This agrees with results from [10], [18], and <ref> [19] </ref>; when the amount of money to spend on the cache is small, capacity counts for more than speed.
Reference: 20. <author> Wu, M. and Zwaeneopoel, W., eNVy: </author> <title> A NonVolatile, Main Memory Storage System, </title> <month> March, </month> <year> 1994. </year>
References-found: 20

