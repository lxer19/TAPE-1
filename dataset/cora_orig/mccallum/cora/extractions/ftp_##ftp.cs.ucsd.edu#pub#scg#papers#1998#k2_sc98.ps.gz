URL: ftp://ftp.cs.ucsd.edu/pub/scg/papers/1998/k2_sc98.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: Communication overlap in multi-tier parallel algorithms  
Author: Scott B. Baden and Stephen J. Fink 
Address: La Jolla, CA 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Note: To appear: Conf. Proc. SC '98, Orlando FL 1  
Abstract: Hierarchically organized multicomputers such as SMP clusters offer new opportunities and new challenges for high-performance computation, but realizing their full potential remains a formidable task. We present a hierarchical model of communication targeted to block-structured, bulk-synchronous applications running on dedicated clusters of symmetric multiprocessors. Our model supports node-level rather processor-level communication as the fundamental operation, and is optimized for aggregate patterns of regular section moves rather than point-to-point messages. These two capabilities work synergistically. They provide flexibility in overlapping communication and overcome deficiencies in the underlying communication layer on systems where inter-node communication bandwidth is at a premium. We have implemented our communication model in the KeLP2.0 run time library. We present empirical results for five applications running on a cluster of Digital AlphaServer 2100's. Four of the applications were able to overlap communication on a system which does not support overlap via non-blocking message passing using MPI. Overall performance improvements due to our overlap strategy ranged from 12% to 28%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> An efficient parallel algorithm for the 3-d FFT NAS parallel benchmark, </title> <booktitle> in Proc. of SHPCC `94, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 129-133. </pages>
Reference-contexts: Nevertheless, we were able to improve performance with overlap-about 15% on 8 nodes- using a pipelining strategy due to Agarwal et al. <ref> [1] </ref>. We scaled the problem size with the number of nodes, with 2 20 unknowns per node. Data were distributed with a 1D block decomposition. Despite the use of overlap, performance for this application is poor.
Reference: [2] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> An integrated runtime and compile-time approach for paral-lelizing structured and block structured applications, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> 6 </month> <year> (1995). </year>
Reference-contexts: Too, users may need to define application-specific collective operations that are not supported by their favorite message passing layer. Since communication requirements can be known in advance, we may use advance knowledge to optimize communica 2 tion at run time, e.g. inspector-executor analysis <ref> [2] </ref>, though other optimizations are possible [15]. For a variety of reasons, non-blocking communication may be ineffective in realizing communication overlap. Multi-phase communication algorithms, such as dimension exchange or hypercube broadcast algorithms, require a strict ordering of messages.
Reference: [3] <author> B. Alpern, L. Carter, and J. Ferrante, </author> <title> Modeling parallel computers as memory hierarchies, in Programming Models for Massively Parallel Computers, </title> <editor> W. K. Giloi, S. Jahnichen, and B. D. Shriver, eds., </editor> <publisher> IEEE Computer Society Press, </publisher> <month> Sept. </month> <year> 1993, </year> <pages> pp. 116-23. </pages>
Reference-contexts: The hierarchical flavor of our communication model reflects the reality that off-node bandwidth of the network interface is a scare resource, severely limiting collective rate at which processors on a given node may communicate. In this spirit, it is similar to the PMH model described by Alpern et al. <ref> [3] </ref>. The collective behavior implemented by the Mover matches the requirements of many scientific computations, in which communication, even if irregular, is highly stylized. Communication patterns may often be known in advance, and involve many communicating pairs of processors.
Reference: [4] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga, </author> <title> The NAS parallel benchmarks, </title> <type> Tech. Rep. </type> <institution> RNR-94-007, NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: This is true in part because we increase memory bus utilization when we increase processor utilization, and in part because variations in communication times over the ATM switch introduce uncertainty into our load balancing estimates. Interrupt overhead is likely another factor. 5.2.2 NAS-MG The NAS-MG multigrid benchmark <ref> [4] </ref> solves Poisson's equation in 3D using a multigrid V-cycle [8]. Multigrid carries out computation at a series of levels and each level defines a grid at a successively coarser resolution.
Reference: [5] <author> S. Balay, W. D. Gropp, L. C. McInnes, and B. R. Smith, </author> <title> Efficient management of parallelism in object-oriented numerical software libraries, in Modern Software Tools in Scientific Computing, </title> <editor> E. Arge, A. M. Bruaset, and H. P. Langtangen, eds., </editor> <publisher> Birkhauser Press, </publisher> <year> 1997. </year>
Reference: [6] <author> J. Beecroft, M. Homewook, and M. McLaren, </author> <title> Meiko cs-2 interconnect elan-elite design, </title> <booktitle> Parallel Computing, 20 (1994), </booktitle> <pages> pp. 1627-1638. </pages>
Reference-contexts: Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon [31], later on the Wisconsin Wind Tunnel [33], Meiko CS-2 <ref> [6] </ref>, SHRIMP [7], and Proteus [38]. Sawdey et al. [34] describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications. We implemented five applications and ran them on a cluster of Digital AlphaServer 2100's.
Reference: [7] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg, </author> <title> Virtual memory mapped network interface for the SHRIMP mul-ticomputer, </title> <booktitle> in Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago,IL, </address> <month> April </month> <year> 1994, </year> <pages> pp. 142-153. </pages>
Reference-contexts: Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon [31], later on the Wisconsin Wind Tunnel [33], Meiko CS-2 [6], SHRIMP <ref> [7] </ref>, and Proteus [38]. Sawdey et al. [34] describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications. We implemented five applications and ran them on a cluster of Digital AlphaServer 2100's.
Reference: [8] <author> W. L. Briggs, </author> <title> A Multigrid Tutorial, </title> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: Interrupt overhead is likely another factor. 5.2.2 NAS-MG The NAS-MG multigrid benchmark [4] solves Poisson's equation in 3D using a multigrid V-cycle <ref> [8] </ref>. Multigrid carries out computation at a series of levels and each level defines a grid at a successively coarser resolution. We parallelized each level of this stencil-based computation much as we did redBlack3D, using a pre-fetching scheme which delayed computation on an inner annulus. we parallelized redblack3D.
Reference: [9] <author> S. Chakrabarti, E. Deprit, E.-J. Im, J. Jones, A. Krishnamurthy, C.-P. Wen, and K. Yelick, Multipol: </author> <title> A distributed data structure library, </title> <booktitle> in Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> Jul. </month> <year> 1995. </year>
Reference: [10] <author> K. Chandy and C. Kesselman, </author> <title> Compositional C++: Compositional parallel programming, </title> <booktitle> in Fifth International Workshop of Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Note that KeLP structured loops contrast sharply with unstructured thread programming, where the programmer must explicitly manage synchronization between individual threads. CC++ <ref> [10] </ref> provides a programming model with both types of parallel control constructs. Like the CC++ structured parallel loops, the KeLP iterators simplify the expression of parallelism, but restrict the forms of parallel control flow available to the programmer.
Reference: [11] <author> C. Chang, A. Sussman, and J. Saltz, </author> <title> Support for distributed dynamic data structures in C++, </title> <type> Tech. Rep. </type> <institution> CS-TR-3266, University of Maryland, </institution> <year> 1995. </year>
Reference: [12] <author> D. Culler, A. Dusseau, S. Goldstein, A. Kr-ishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> Parallel programming in Split-C, </title> <booktitle> in Proc. Supercomputing, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference: [13] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eiken, </author> <title> LogP: Towards a realistic model of parallel computation, </title> <booktitle> in Proceedings of the Fourth Symposium on Principle and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 1-12. </pages>
Reference-contexts: This model directly reflects the structure of many commercial distributed-memory parallel computers, as well as formal parallel computation models, such as CTA [35], BSP [39], and LogP <ref> [13] </ref>. These single-tier models may be characterized as having two levels of control flow: a collective level, and a node level.
Reference: [14] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang, </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 (1994), </volume> <pages> pp. 462-479. </pages>
Reference: [15] <author> S. J. Fink, </author> <title> Hierarchical Programming for Block-Structured Scientific Calculations, </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1998. </year>
Reference-contexts: Communication patterns may often be known in advance, and involve many communicating pairs of processors. We have implemented the Mover model as part of the C++ run time library called KeLP2.0 <ref> [16, 15] </ref>. In addition to supporting collective, asynchronous, hierarchical communication, KeLP2.0 provides other vital ingredients: hierarchical decompositions and control flow. We chose to implement the Mover to run on a spare SMP processor. <p> Too, users may need to define application-specific collective operations that are not supported by their favorite message passing layer. Since communication requirements can be known in advance, we may use advance knowledge to optimize communica 2 tion at run time, e.g. inspector-executor analysis [2], though other optimizations are possible <ref> [15] </ref>. For a variety of reasons, non-blocking communication may be ineffective in realizing communication overlap. Multi-phase communication algorithms, such as dimension exchange or hypercube broadcast algorithms, require a strict ordering of messages. Overlap strategies based on non-blocking communication must poll, which complicates the user code and compromises performance. <p> We experimented with an alternative implementation of the Mover, that employed non-blocking message passing calls in lieu of an extra thread. This strategy failed to to improve performance-and in some cases actually resulted in lower performance <ref> [15] </ref>. We compared performance of our KeLP applications with explicit message passing versions written in MPI. Two of these-NPB2.1 benchmarks NAS-MG and NAS-FT-were down-loaded and used without modification. Another-RedBlack3D-was carefully and painstakingly optimized. <p> It implements matrix multiplication as a series of blocked outer products over distributed matrices. We developed a straightforward dual-tier adaptation of SUMMA, as well as a new SUMMA variant that explicitly overlaps communication and computation <ref> [15] </ref>. This pipelined algorithm carries out a series of broadcasts as shown in Fig. 9. The broadcasts involve a panel of data rather than the entire block of data assigned to the processor, that is, a vertical or horizontal slice.
Reference: [16] <author> S. J. Fink and S. B. Baden, </author> <title> Runtime support for multi-tier programming of block-structured applications on SMP clusters, </title> <booktitle> in Proceedings of 1997 International Scientific Computing in Object-Oriented Parallel Environments Conference, </booktitle> <address> Marina del Rey, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: Communication patterns may often be known in advance, and involve many communicating pairs of processors. We have implemented the Mover model as part of the C++ run time library called KeLP2.0 <ref> [16, 15] </ref>. In addition to supporting collective, asynchronous, hierarchical communication, KeLP2.0 provides other vital ingredients: hierarchical decompositions and control flow. We chose to implement the Mover to run on a spare SMP processor. <p> If so, the most heavily loaded node does not overlap communication and computation. 5.2.4 NAS FT The NAS FT benchmark incurs a costly transpose that limits the performance of the application on an SMP cluster <ref> [16, 28] </ref>. Nevertheless, we were able to improve performance with overlap-about 15% on 8 nodes- using a pipelining strategy due to Agarwal et al. [1]. We scaled the problem size with the number of nodes, with 2 20 unknowns per node. Data were distributed with a 1D block decomposition.
Reference: [17] <author> S. J. Fink, S. B. Baden, and S. R. Kohn, </author> <title> Efficient run-time support for irregular block-structured applications, </title> <journal> J. Parallel Distrib. Comput., </journal> <year> (1998). </year>
Reference-contexts: We propose a hierarchical programming model, which we have implemented as the KeLP2.0 framework. KeLP2.0 builds on its single-tier predecessor, KeLP1.0 <ref> [17] </ref>. Like its predecessor, KeLP2.0 is a C++ class library. We next give an overview of the multi-tier KeLP2.0 Programming model. We will refer to KeLP2.0 as KeLP from now on. Control flow. Under KeLP, we characterize an SMP cluster as follows. <p> A MotionPlan is a first class communication schedule, and it describes a set of block copies between two XArrays, which will execute as an atomic operation. The programmer constructs the Motion-Plan describing the desired communication pattern with the help of a Region calculus of geometric operations <ref> [17] </ref>. Once the MotionPlan has been constructed, the Mover may be built. We pass the MotionPlan to the Mover constructor, along with the two XArrays to which the communication activity is to be bound.
Reference: [18] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum, </author> <title> A high-performance, portable implementation of the MPI message passing interface standard, </title> <type> tech. rep., </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1997. </year> <note> http://www.mcs.anl.gov/mpi/mpich/. </note>
Reference-contexts: Each SMP has four Alpha 21064A processors, each processor has a 4MB direct-mapped L2 cache. For inter-node communication, we rely on MPICH 1.0.12 <ref> [18] </ref> over an OC-3 ATM switch. KeLP was implemented in C++, but serial numeric kernels employed in our applications were implemented in Fortran 77, and use double precision arithmetic. We compiled C++ using gcc v2.7.2, with compiler option -O2.
Reference: [19] <author> F. M. Hayes, </author> <title> Design of the AlphaServer multiprocessor sever systems, </title> <journal> Digital Technical Journal, </journal> <volume> 6 (1994), </volume> <pages> pp. 8-19. </pages>
Reference-contexts: As a compromise, KeLP defines node and processor level waits on a Mover, that provide additional flexibility. 4 Implementation We implemented KeLP2.0 on a cluster of Digital AlphaServer 2100's <ref> [19] </ref> running Digital UNIX 4.0. Each SMP has four Alpha 21064A processors, each processor has a 4MB direct-mapped L2 cache. For inter-node communication, we rely on MPICH 1.0.12 [18] over an OC-3 ATM switch.
Reference: [20] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> Nov. </month> <year> 1994. </year>
Reference: [21] <author> L. Kale and S. Krishnan, CHARM++: </author> <title> a portable concurrent object oriented system in C++, </title> <booktitle> in Proceedings of OOPSLA, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference: [22] <author> C. Kamath, R. Ho, and D. P. Manley, DXML: </author> <title> a high-performance scientific subroutine library, </title> <journal> Digital Technical Journal, </journal> <volume> 6 (1994), </volume> <pages> pp. 44-56. 14 </pages>
Reference-contexts: The matrices were distributed with a 2D block distribution. The panel size b was 100 in all runs, and was determined experimentally. We use the dgemm kernels from the Digital Extended Math Library (DXML) <ref> [22] </ref>. On this platform, DXML obtains 160 MFLOPS per processor for matrix sizes that do not fit in the L2 cache. We always select problem sizes that do not in in the L2 cache.
Reference: [23] <author> G. Kiczales, J. Lamping, A. Mendhekar, C. Maeda, C. Lopes, J.-M. Longtier, and J. Irwin, </author> <title> Aspect-oriented programming, </title> <type> Tech. Rep. </type> <note> SPL97-008 P9710042, Xerox PARC, </note> <month> February </month> <year> 1997. </year>
Reference-contexts: This encapsulation works synergistically with our scheme for overlapping communication and hides much of the underlying complexity. In particular, the keLP Mover separates the expression of correct programs from optimizations affecting performance. This type of separation of concerns results in easier-to-develop, more maintainable code <ref> [23] </ref>. Our implementation of KeLP2.0 worked around many limitations of an aging platform and and raises questions about performance tradeoffs on modern designs.
Reference: [24] <author> S. R. Kohn and S. B. Baden, </author> <title> Irregular coarse-grain data parallelism under LPARX, </title> <journal> J. Scientific Programming, </journal> <month> 5 </month> <year> (1996). </year>
Reference: [25] <author> M. Lauria, S. Pakin, and A. A. Chien, </author> <title> Efficient layering for high speed communication: Fast messages 2.x, </title> <booktitle> in Proc. 7th High Perf. Distributed Computing Conf. </booktitle> <address> (HPDC7), </address> <month> July </month> <year> 1998. </year>
Reference-contexts: For example, MPICH does not intercept communication via shared memory, and we had to implement this capability inside the Mover. Too, Fast messages, which provide for message streams, could reduce the amount of copying, even for non-contiguous data <ref> [25] </ref>. We did not multi-thread the Mover, but the design of KeLP admits this possibility. However, it is unlikely that the Alpha has the memory and communication bandwidth to support concurrency within communication. 5 Performance 5.1 Overview We next assess the benefits of communication overlap expressed by the KeLP Mover.
Reference: [26] <author> B.-H. Lim, P. Heidelberger, P. Pattnaik, and M. Snir, </author> <title> Message proxies for efficient, protected communication on smp clusters, </title> <booktitle> in Proceedings of the Third International Symposium on High-Performance Computer Architecture, </booktitle> <address> San Antonio, TX, February 1997, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 116-27. </pages>
Reference-contexts: Lim et al. describe the message proxy, a trusted agent running as a kernel extension on a spare SMP processor to provide protected access to the network interface <ref> [26] </ref>. When a process sends a message, it communicates with the proxy running on its node via shared memory. The proxy o*oads most of the work involved in managing communication but requires a set of shared data structures for each pair of communicating processes.
Reference: [27] <author> C. Lin and L. Snyder, </author> <title> ZPL:an array sublanguage, </title> <booktitle> in Proc. Languages and Compilers for Parallel Computing, 6th Int'l Workshop, </booktitle> <editor> U. Banerjee, D. Gelern-ter, A. Nicolau, and D. Padua, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1994, </year> <pages> pp. 96-114. </pages>
Reference: [28] <author> S. S. Lumetta, A. M. Mainwaring, and D. E. Culler, </author> <title> Multi-protocol active messages on a cluster of smps, </title> <booktitle> in Proc. SC97, </booktitle> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: The proxy o*oads most of the work involved in managing communication but requires a set of shared data structures for each pair of communicating processes. The Multi-protocol Active Message, described by Lumetta et al. <ref> [28] </ref>, employs shared memory to intercept on-node messages. The cost of communication is significantly lower if the processors happen to be on the same node, reflecting a hierarchical cost model. As noted by the authors, this cost model may be used to improve performance through carefully chosen data decompositions. <p> In some cases (RedBlack3D and NAS FT) there were significant differences between the MPI and KeLP2.0 non-overlapped versions. However, our version of MPI was not optimized for shared memory on the node, and we expect that MPI performance would improve with an enhanced message passing layer <ref> [28] </ref>. Perhaps the non-overlapped KeLP version gives us a truer picture of performance with an improved message passing implementation. In addition we present single node performance, in Fig. 5. As the Figure shows, per-processor performance degrades as we add more processors. <p> If so, the most heavily loaded node does not overlap communication and computation. 5.2.4 NAS FT The NAS FT benchmark incurs a costly transpose that limits the performance of the application on an SMP cluster <ref> [16, 28] </ref>. Nevertheless, we were able to improve performance with overlap-about 15% on 8 nodes- using a pipelining strategy due to Agarwal et al. [1]. We scaled the problem size with the number of nodes, with 2 20 unknowns per node. Data were distributed with a 1D block decomposition.
Reference: [29] <author> Message-Passing Interface Standard, </author> <title> MPI: A message-passing interface standard, </title> <institution> University of Tennessee, Knoxville, TN, </institution> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: These single-tier models may be characterized as having two levels of control flow: a collective level, and a node level. For example, a SPMD message passing program, perhaps written in MPI <ref> [29] </ref>, contains a number of collective operations, such as reductions, barriers and broadcasts, interspersed within a node level program. The node-level instructions form separate threads of control which execute independently. Snyder clearly articulates the two-level approach in the XYZ program levels of the Phase Abstractions programming model [36].
Reference: [30] <author> R. Parsons and D. Quinlan, </author> <title> Run-time recognition of task parallelism within the P++ parallel array class library, </title> <booktitle> in Proc. Scalable Parallel Libraries Conference, </booktitle> <month> October </month> <year> 1994, </year> <pages> pp. 77-86. </pages>
Reference: [31] <author> P. Pierce and G. Regnier, </author> <title> The Paragon implementation of the NX message passing interface, </title> <booktitle> in Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994, </year> <pages> pp. 184-190. </pages>
Reference-contexts: This policy decision was made out of expediency, and could be changed without affecting the correctness of the user's code. Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon <ref> [31] </ref>, later on the Wisconsin Wind Tunnel [33], Meiko CS-2 [6], SHRIMP [7], and Proteus [38]. Sawdey et al. [34] describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications.
Reference: [32] <author> J. Rantakokko, </author> <title> A framework for partitioning domains with inhomogeneous workload, </title> <type> Tech. Rep. 8, </type> <institution> Royal Institute of Technology and Uppsala University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: In the following experiments, we solve Poisson's equation over a grid structure covering the geometrical shape of Lake Superior, obtained from Yingxin Pang. Fig. 8 shows the resultant grids and partitioning assignments generated by Pang's process, which employed a heuristic developed by Rantakokko <ref> [32] </ref>. The data in Fig. 4c reveal that the performance due to overlap is virtually non-existent. Further examination reveals that the computation (and hence communication) are severely load imbalanced.
Reference: [33] <author> S. K. Reinhardt, R. W. Pfile, and D. A. Wood, </author> <title> Decoupled hardware support for distributed shared memory, </title> <booktitle> in Proceedings of the 23rd Annual International Conference on Computer Architecture, </booktitle> <address> Philadelphia,PA, </address> <month> May </month> <year> 1996, </year> <pages> pp. 34-43. </pages>
Reference-contexts: Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon [31], later on the Wisconsin Wind Tunnel <ref> [33] </ref>, Meiko CS-2 [6], SHRIMP [7], and Proteus [38]. Sawdey et al. [34] describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications. We implemented five applications and ran them on a cluster of Digital AlphaServer 2100's.
Reference: [34] <author> A. C. Sawdey, M. T. O'Keefe, and W. B. Jones, </author> <title> A general programming model for developing scalable ocean circulation applications, </title> <booktitle> in Proceedings of the ECMWF Workshop on the Use of Parallel Processors in Meteorology, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon [31], later on the Wisconsin Wind Tunnel [33], Meiko CS-2 [6], SHRIMP [7], and Proteus [38]. Sawdey et al. <ref> [34] </ref> describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications. We implemented five applications and ran them on a cluster of Digital AlphaServer 2100's.
Reference: [35] <author> L. Snyder, </author> <title> Type architectures, shared memory, and the corollary of modest potential, </title> <booktitle> Annual Review of Computer Science, 1 (1986), </booktitle> <pages> pp. 289-317. </pages> <booktitle> [36] , Foundations of practical parallel programming languages, in Portability and Performance of Parallel Processing, </booktitle> <editor> T. Hey and J. Ferrante, eds., </editor> <publisher> John Wiley and Sons, </publisher> <year> 1993. </year>
Reference-contexts: This model directly reflects the structure of many commercial distributed-memory parallel computers, as well as formal parallel computation models, such as CTA <ref> [35] </ref>, BSP [39], and LogP [13]. These single-tier models may be characterized as having two levels of control flow: a collective level, and a node level.
Reference: [37] <author> A. Sohn and R. Biswas, </author> <title> Communication studies of DMP and SMP machines, </title> <type> Tech. Rep. </type> <institution> NAS-97-004, NAS, </institution> <year> 1997. </year>
Reference-contexts: Overlap strategies based on non-blocking communication must poll, which complicates the user code and compromises performance. In other cases, the network interface may not be able to realize communication overlap except under certain narrowly defined conditions, which may not be realistic <ref> [37] </ref>. Or, the message layer may not be implemented to take advantage of the network interface's capabilities. For example, the message co-processor may not be able to handle message linearization, which which must then be executed on a compute processor, slowing down computation.
Reference: [38] <author> A. K. Somani and A. M. Sansano, </author> <title> Minimizing overhead in parallel algorithms through overlapping communication/computation, </title> <type> Tech. Rep. 97-8, </type> <institution> ICASE, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Others have recognized the benefits of using a spare processor when the marginal loss of a single CPU is small. The technique was first explored on the Intel Paragon [31], later on the Wisconsin Wind Tunnel [33], Meiko CS-2 [6], SHRIMP [7], and Proteus <ref> [38] </ref>. Sawdey et al. [34] describe a compiler for the Fortran-P programming language, which uses a spare SMP, and is specialized for grid-based applications. We implemented five applications and ran them on a cluster of Digital AlphaServer 2100's.
Reference: [39] <author> L. G. Valiant, </author> <title> A bridging model for parallel computation, </title> <journal> Communications of the ACM, </journal> <volume> 33 (1990), </volume> <pages> pp. 103-111. </pages>
Reference-contexts: This model directly reflects the structure of many commercial distributed-memory parallel computers, as well as formal parallel computation models, such as CTA [35], BSP <ref> [39] </ref>, and LogP [13]. These single-tier models may be characterized as having two levels of control flow: a collective level, and a node level.
Reference: [40] <author> R. van de Geign and J. Watts, SUMMA: </author> <title> Scalable universal matrix multiplication algorithm, </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 9 (1997), </volume> <pages> pp. 255-74. </pages>
Reference-contexts: On a single node, the code obtains speedups of 1.73, 2.38, and 2.57 on 2, 3 , 4 four Alpha processors, respectively. 5.2.5 SUMMA SUMMA implements a fast matrix multiply algorithm due to van de Geijn and Watts <ref> [40] </ref>. It implements matrix multiplication as a series of blocked outer products over distributed matrices. We developed a straightforward dual-tier adaptation of SUMMA, as well as a new SUMMA variant that explicitly overlaps communication and computation [15]. <p> We always select problem sizes that do not in in the L2 cache. The "MPI" results use the C+MPI SUMMA version made publicly available by van de Geijn and Watts and described in <ref> [40] </ref>, which we modified code to call the DXML kernel. The pipelined MPI implementation is able to overlap communication with computation and edges out the non-overlapped KeLP2.0 implementation on 8 nodes.

References-found: 39

