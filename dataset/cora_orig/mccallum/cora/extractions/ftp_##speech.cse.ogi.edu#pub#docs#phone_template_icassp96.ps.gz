URL: ftp://speech.cse.ogi.edu/pub/docs/phone_template_icassp96.ps.gz
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/cole.html
Root-URL: http://www.cse.ogi.edu
Email: (neena@cse.ogi.edu)  
Title: CREATING SPEAKER-SPECIFIC PHONETIC TEMPLATES WITH A SPEAKER-INDEPENDENT PHONETIC RECOGNIZER: IMPLICATIONS FOR VOICE DIALING  
Author: Neena Jain, Ronald Cole and Etienne Barnard 
Address: 20000 N.W. Walker Road, P.O. Box 91000, Portland, OR 97291-1000, USA,  
Affiliation: Center for Spoken Language Understanding, Oregon Graduate Institute of Science and Technology,  
Abstract: We present a new approach to speaker dependent template generation which uses dramatically less storage to represent a speaker's words, with minimal degradation in recognition accuracy. In this approach, the symbolic string produced by a speaker-independent phonetic recognizer is used to represent utterances. We investigate effective procedures for template generation, and compare the results of these procedures to templates represented by acoustic parameters for utterances produced with different telephone handsets. The use of speaker-specific templates led to a reduction of about 1:500 in data-storage requirements with comparable recognition accuracy. In also compare recognition performance for speaker-specific and speaker-independent templates, and for combinations of the two. The results showed that combining speaker-specific and speaker-independent templates produces better recognition performance than either alone. A voice dialing system is described which incorporates the speaker-specific templates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Jain, "Ms-thesis: </author> <title> A new approach for voice-dialing," </title> <type> tech. rep., </type> <institution> Oregon Graduate Institute, </institution> <year> 1995. </year> <note> http://www.cse.ogi.edu/CSLU/cslu.publications.html. </note>
Reference-contexts: The models created during registration are used for performing these actions. Fig 1 shows the basic structure of the application process. The speaker verification component, used for providing security, is not discussed here; a detailed description can be found in <ref> [1] </ref>. 3. SPEAKER-SPECIFIC MODELS In this section, we describe the method for generating the speaker-specific models. Our speaker-specific models represent utterances as sequences of phonemes rather than acoustic parameters. A speaker-independent phonetic front end is used to generate the string of phonemes.
Reference: [2] <author> H. Hermansky, N. Morgan, A. Bayya, and P.Kohn, </author> <title> "Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)," </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> vol. 87, no. 4, </volume> <pages> pp. 1367-1370, </pages> <year> 1991. </year>
Reference-contexts: The following steps produce a phonetic transcription: 1. Data Capture: Telephone speech is sampled at 8 kHz. Unnecessary silence at the beginning and end of the utterance is removed by end-point detection. 2. Feature Extraction: Seventh order RASTA features <ref> [2] </ref> are computed for every 10ms of speech using a 10ms window. This yields eight coefficients per frame.
Reference: [3] <author> E. Barnard, R. Cole, M. Fanty, and P. Vermeulen, </author> <title> "Real-world speech recognition with neural networks," </title> <booktitle> in Applications and Science of artificial neural networks, (Orlando), </booktitle> <pages> pp. 524-537, </pages> <booktitle> SPIE, </booktitle> <volume> Vol. 2492, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Frame Classification: Classification of 10ms frames is performed using a context-dependent three layered MLP, trained with conjugate-gradient optimization. Details of the neural network architecture and training procedures are described in <ref> [3] </ref>. The network was trained on phonetically labeled speech data from the OGI-TS corpus [4]. To account for the influence of context on the current frame, RASTA features from a window of 168 msec, centered on the current frame, are computed.
Reference: [4] <author> Ronald Cole, Mark Fanty, Mike Noel and Terri Lander, </author> <title> "Telephone Speech Corpus Development at CSLU," </title> <booktitle> in Proc. ICSLP, </booktitle> <volume> vol. 4, </volume> <pages> pp. 1815-1818, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Frame Classification: Classification of 10ms frames is performed using a context-dependent three layered MLP, trained with conjugate-gradient optimization. Details of the neural network architecture and training procedures are described in [3]. The network was trained on phonetically labeled speech data from the OGI-TS corpus <ref> [4] </ref>. To account for the influence of context on the current frame, RASTA features from a window of 168 msec, centered on the current frame, are computed. <p> The Viterbi search maps frame scores to phonemes. It is constrained in two ways bigram grammar and duration. The bigram grammar consists of the transition probabilities of each phoneme being followed by every other phoneme. These values were computed from the large phonetically transcribed OGI stories database <ref> [4] </ref>. The minimum and maximum durations of each phoneme are also used for constraining the search. The Viterbi search algorithm uses the grammar and the duration constraints to find the maximum scoring phonetic sequence given the frame-by-frame output scores. <p> In the second system, 20th order LPC cepstral coefficients computed for every 10 msec of speech are quantized by using Vector Quantization to form the reference template. The codebook used for quantizing was generated from the OGI stories database <ref> [4] </ref>. 2 codebooks, of size 1024, were used for quantizing the lower and the upper 10 LPC coefficients. 2000 bits are needed to represent one second of speech. During testing, the unknown speech vector was quantized and then matched with all the reference templates using DTW.
Reference: [5] <author> L. R. Rabiner and J. G. Wilpon, </author> <title> "Some performance benchmarks for isolated word speech recognition systems," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 2, </volume> <pages> pp. 343-357, </pages> <year> 1987. </year>
Reference-contexts: Effect of Number of Templates Chosen 5. COMPARISON WITH DTW Dynamic Time Warping (DTW) with LPC features can be considered as one of the state-of-the art classification system for speaker-dependent recognition <ref> [5] </ref>. DTW is an efficient method of achieving non-linear normalization of the time axis of the speech signal to account for the fluctuations in the speaking rate within a speaker.
Reference: [6] <author> M. F. Spiegel, M. J. Macchi, and K. D. Gollhardt, </author> <title> "Synthesis of names by a demisyllable-based speech synthesizer(Orator)," </title> <booktitle> Proc. Eurospeech, </booktitle> <pages> pp. 117-120, </pages> <year> 1989. </year>
Reference-contexts: In the first experiment, we compare speaker-specific templates with speaker-independent models. The speaker-independent models are generated by two methods ama-chine and a phonetician. Orator <ref> [6] </ref> is a commercial text-to-speech synthesizer which also generates the pronunciation models. In the second experiment, we combine the speaker-specific models with the speaker-independent models (from Orator) for each speaker. In condition 1, the best model for each phrase was combined with the speaker-independent model.
References-found: 6

