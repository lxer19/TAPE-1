URL: http://charm.cs.uiuc.edu/version2/papers/DP_INTL92.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/DP_INTL92.html
Root-URL: http://www.cs.uiuc.edu
Email: email: kornkven@cs.uiuc.edu  email: kale@cs.uiuc.edu  
Phone: tel: (217)-333-5827  tel: (217)-244-0094  
Title: Dynamic Adaptive Scheduling in an Implementation of a Data Parallel Language  
Author: Edward A. Kornkven Laxmikant V. Kale 
Address: Urbana, IL 61801  Urbana, IL 61801  
Affiliation: Department of Computer Science University Of Illinois  Department of Computer Science University Of Illinois  
Abstract: In the execution of a parallel program, it is desirable for all processors dedicated to the program to be kept fully utilized. However, a program that employs a lot of message-passing might spend a considerable amount of time waiting for messages to arrive. In order to mitigate this efficiency loss, instead of blocking execution for every message, we would rather overlap that communication time with other computation. This paper presents an approach to accomplishing this overlap in a systematic manner when compiling a data parallel language targeted for MIMD computers. 
Abstract-found: 1
Intro-found: 1
Reference: [Cha92] <institution> Parallel Programming Laboratory, University of Illinois Department of Computer Science, Urbana, Illinois. </institution> <note> The CHARM(3.0) Programming Language Manual, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: Clearly, then, ordering these statements for optimal overlap is difficult and not amenable to a completely static approach. An alternative dynamic approach is the topic of this paper. 3 Dagger and Charm Charm <ref> [Kal90, Cha92] </ref> is a machine-independent run-time system which runs on a variety of shared-memory and distributed-memory parallel computers and supports an explicitly parallel C-like language. Charm supports the dynamic creation, manipulation, and scheduling of small tasks called chares.
Reference: [Gur93] <author> A. Gursoy. Dagger: </author> <title> Combining the benefits of synchronous and asynchronous communication styles. </title> <booktitle> Submitted to 1993 International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Branch office chares are useful for describing a computation that is similar on all processors-such as the array processing in DP. They are also useful for implementing distributed data structures and for defining interfaces between parallel modules. Dagger <ref> [Gur93] </ref> runs on top of Charm and is a tool for performing computations that can be rep resented by directed graphs. <p> The reader is referred to <ref> [Gur93] </ref> for more details. The DDG as previously described is nearly sufficient to support the translation of IF and loop statements. We augment the DDG with a new node type (the diamond) to represent IF and loop tests-these statements require some special additional code to be generated.
Reference: [Hig92] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification (Draft), </title> <address> 1.0 edition, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Finally, we discuss related research in section 8 and summarize our results in section 9. 2 The DP Source Language The source language that we wish to compile is a subset of High Performance Fortran (HPF) <ref> [Hig92] </ref> which we call DP. Like other languages in its class, its distinguishing features include the ability of source statements to operate on entire arrays or sections of arrays at a time, and a large set of intrinsic functions for manipulating those arrays. <p> We will avoid details of declaring arrays and distributing them onto processors. It will be assumed in the examples discussed here that all arrays are the same size and shape, and that corresponding elements are stored on the same processor. The reader is referred to <ref> [Hig92] </ref> for further details of HPF. Some statements will be able to execute entirely locally on the processors-the first assignment statement in Figure 1 is an example. Generally, however, data will have to be shared between processors at some time during execution of the program, necessitating interprocessor communication.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, , and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 139-176. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1992. </year>
Reference-contexts: However, the Kali compiler must schedule its receive statements statically whereas our schedule is able to be dynamically determined because of our underlying message-driven virtual machine. Some other examples of projects in which efforts are made to overlap communication and computation include Fortran D <ref> [HKT92] </ref> and the Crystallizing Fortran Project [LC91]. All of these efforts must rely on statically scheduling program tasks. Of course, the benefits of message-driven execution extend to the entire program (not just to communication/computation overlap) and serve to simplify the entire scheduling task.
Reference: [Kal90] <author> L.V. Kale. </author> <title> The Chare Kernel parallel programming language and system. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 17-25, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Clearly, then, ordering these statements for optimal overlap is difficult and not amenable to a completely static approach. An alternative dynamic approach is the topic of this paper. 3 Dagger and Charm Charm <ref> [Kal90, Cha92] </ref> is a machine-independent run-time system which runs on a variety of shared-memory and distributed-memory parallel computers and supports an explicitly parallel C-like language. Charm supports the dynamic creation, manipulation, and scheduling of small tasks called chares.
Reference: [Koe91] <author> C. Koelbel. </author> <title> Compile-time generation of regular communications patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year> <title> Analyzes data distribution and access patterns for distributed data. </title>
Reference-contexts: What distinguishes our work is that the order in which these tasks are performed is determined at run-time according to our adaptive schedule. We know of no other machine-independent run-time system that offers the asynchronous, message-driven virtual machine to which we compile. <ref> [Koe91] </ref>, for example, describes the generation of code for performing communication for distributed arrays in the Kali language. In that paper, mention is made of the fact that interleaving communication and computation can be done to improve efficiency.
Reference: [LC91] <author> Jingke Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-375, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Some other examples of projects in which efforts are made to overlap communication and computation include Fortran D [HKT92] and the Crystallizing Fortran Project <ref> [LC91] </ref>. All of these efforts must rely on statically scheduling program tasks. Of course, the benefits of message-driven execution extend to the entire program (not just to communication/computation overlap) and serve to simplify the entire scheduling task.

References-found: 7

