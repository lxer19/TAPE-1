URL: http://www.cs.tamu.edu/faculty/amato/Courses/626/models/model-survey.ps
Refering-URL: http://www.cs.tamu.edu/faculty/amato/Courses/626/modelrefs.html
Root-URL: http://www.cs.tamu.edu
Title: Models of Parallel Computation: A Survey and Synthesis  
Author: B.M. Maggs L.R. Matheson R.E. Tarjan 
Note: Research partially supported by NSF National Young Investigator Award and ARPA Grant Nos. F33615-93-1-1330, N00014-91-J-1698, N00014-92-J-1799 Princeton research partially supported by NSF grant no. CCR-8920505 and Office of Naval Research Contract No. N0014-91-J-1463  
Address: Pittsburgh, Pa 15213 Princeton, NJ 08540  Princeton, NJ 08540  
Affiliation: Carnegie-Mellon University NEC Research Institute Princeton University  NEC Research Institute  
Abstract: In the realm of sequential computing the random access machine has successufully provided an underlying model of computation that promoted consistency and coordination among algorithm developers, computer architects and language experts. In the realm of parallel computing, however, there has been no similar success. The need for such a unifying parallel model or set of models is heightened by the greater demand for performance and the greater diversity among machines. Yet the modeling of parallel computing still seems to be mired in controversy and chaos. This paper is an excerpt from a study which presents broad range of models of parallel computation and the different roles they serve in algorithm, language and machine design. The objective is to better understand which model characteristics are important to each design community in order to elucidate the requirements of a unifying paradigm. As an impetus for discussion, we conclude by suggesting a model of parallel computation which is consistent with a model design philosophy which balances simplicity and descriptivity with prescriptivity. Space constraints allow only the presentation of the survey of abstract computational models. It is our hope that the introduction provides insights into the rich array of relevant issues in other disciplines, inspiring the interested reader to examine the full paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aggarwal, A., Alpern, B., Chandra, A., Snir, M., </author> <title> "A Model for Hierarchical Memory", </title> <booktitle> Proc. of the 19th Annual ACM Symp. on Theory of Computing, ACM, </booktitle> <pages> pp. 305-314, </pages> <month> May </month> <year> (1987). </year>
Reference-contexts: In most such models the concept of random access is irretrievably altered. Two early examples of serial hierarchical memory models are the HMM <ref> [1] </ref> model and the BT [2] model. In the HMM model there are k levels of memory each of which contains 2 k memory locations; access to memory location x takes f (x) time for some function f .
Reference: [2] <author> Aggarwal, A., Chandra, A., and Snir, M., </author> <title> "Hierarchical Memory with Block Transfer", </title> <booktitle> Proc. of the 28th Annual IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pp. 204-216, </pages> <year> (1987). </year>
Reference-contexts: In most such models the concept of random access is irretrievably altered. Two early examples of serial hierarchical memory models are the HMM [1] model and the BT <ref> [2] </ref> model. In the HMM model there are k levels of memory each of which contains 2 k memory locations; access to memory location x takes f (x) time for some function f .
Reference: [3] <author> Aggarwal, A., Chandra, A., and Snir, M., </author> <title> "Communications Complexity of PRAMs", </title> <journal> Theoretical Computer Science , Vol. </journal> <volume> 71, </volume> <pages> pp. 3-28, </pages> <year> (1990). </year>
Reference-contexts: Another PRAM variant which arbitrates memory access is the Module Parallel Computer (MPC) [27]. In this model shared global memory is broken into m modules. Only one memory access can occur within each module per time step. The LPRAM (or Local-memory PRAM) proposed in <ref> [3] </ref> augments the CREW PRAM by associating with each processor an unlimited amount of local private memory. The QRQW PRAM [16] provides an intermediary, a queue, to arbitrate and manage memory accesses, while charging a memory access cost which is a function of the queue length. <p> It was suggested that the LPRAM could be aumented by charging a cost of l units to access global memory <ref> [3] </ref>. An elaboration of this model, the BPRAM [4], augmented this by charging l units for the first message from global memory and a variable cost, b, for each additional memory access in a contiguous block.
Reference: [4] <author> Aggarwal. A., Chandra, A., and Snir, M., </author> <title> "On Communications Latencies in PRAM Computations", </title> <booktitle> Proc. of the 1st Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-21, </pages> <year> (1989). </year>
Reference-contexts: It was suggested that the LPRAM could be aumented by charging a cost of l units to access global memory [3]. An elaboration of this model, the BPRAM <ref> [4] </ref>, augmented this by charging l units for the first message from global memory and a variable cost, b, for each additional memory access in a contiguous block. Thus the BPRAM provides incentives for one level of reference locality and for block transfer, a form of data parallelism.
Reference: [5] <author> Akl, S., </author> <title> "Memory Access in Models of Parallel Computation: From Folklore to Synergy and Beyond", </title> <booktitle> Proc. of the 2nd Workshop on Algorithms and Data Structures, </booktitle> <publisher> Springer-Verlag, Berlin, </publisher> <pages> pp. 92-104, </pages> <year> (1991). </year>
Reference-contexts: In these models data movement is a valued resource, and the models provide performance rewards for exploiting both types of data movement parallelism: block transfer and parallel transfer. A perspective on the potential benefits of data movement 5 parallelism can be found in <ref> [5] </ref> and a survey of paral-lel hierarchical memory models can be found in [37]. For problems which involve the movement of large amounts of data these models may be particularly effective in producing efficient algorithms. An example of algorithm design using these models can be found in [38].
Reference: [6] <author> Barnoy, A., and Kipnis, S., </author> <title> "Designing Algorithms in the Postal Model for Message Passing Systems", </title> <booktitle> Proc. of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 13-22, </pages> <year> (1992). </year> <month> 9 </month>
Reference-contexts: Later referred to as the Distributed Memory Model (DMM), it posits private memory modules associated with processors in a bounded degree network. Computation and nearest neighbor communication require one time step. Another more recent example of a distributed memory model is the Postal Model <ref> [6] </ref> deriving its name from an analogy to the US mail system. In this model to accomplish a non-local memory access a processor posts a message into the network and goes about its business (posting other messages) while the first is being delivered.
Reference: [7] <author> Blelloch, G., </author> <title> "Scans as Primitive Operations", </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <pages> pp. 1526-1538, </pages> <month> November </month> <year> (1989). </year>
Reference-contexts: The examples above include some of the proposed modifications to the PRAM which better reflect the characteristics of actual computers. Many other variants have been developed which enhance the standard PRAM. Some for example include additional unit-cost primitive operations such as scans <ref> [7] </ref>. Such models can be considered to be designed to be prescriptive, suggesting features, some more realizable than others, which could potentially make a parallel platform efficient for a given class of problems. <p> For example, in <ref> [7] </ref> Blelloch et al. develop a detailed model of the CM-2 by Thinking Machines Inc., a fine-grained hypercube-connected massively parallel computer, in order to better understand which sorting algorithms and implementations perform best on this platform.
Reference: [8] <author> Blelloch, G., Leiserson, C., Maggs, B., Plaxton, G., Smith, S., Zagha, M., </author> <title> "A Comparison of Sorting Algorithms for the Connection Machine CM-2", </title> <booktitle> Proc. of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 3-16, </pages> <year> (1991). </year>
Reference: [9] <author> Cook, S., and Reckhow, R., </author> <title> "Time Boundend Random Access Machines", </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> Vol. 7, </volume> <pages> pp. 354-375, </pages> <year> (1973). </year>
Reference-contexts: In each domain translation from problem to computational algorithm requires a model of computation. For example, the development of serial computing has produced, among others, the widely accepted Von Neu-mann model, expressed elegantly in the random access machine model (RAM) <ref> [9] </ref>. Such a computational model 1 must clearly define an execution engine powerful enough to produce a solution to the relevant class of problems. In addition such a model needs to reflect the salient computing characteristics of practical computing platforms.
Reference: [10] <author> Cole, R., and Zajicek, O., </author> <title> "The APRAM: Incorporating Asynchrony into the PRAM Model", </title> <booktitle> Proc. of the 1st Annual ACM Symposium on Parallel Algoritms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <year> (1989). </year>
Reference-contexts: The standard PRAM posits a rigid execution pattern in which all processors are synchronized by a global clock. Several variants ease this restriction. Examples which allow asynchronous execution with irregular synchronization points include the APRAM <ref> [10] </ref> and the Asynchronous PRAM [15]. Periodic synchronization between intervals of asynchronous execution is incorporated in the XPRAM [34]. While these models incorporate synchronization, they do not charge an explicit cost.
Reference: [11] <author> Cole, R., and Zajicek, O., </author> <title> "The Expected Value of Asynchrony", </title> <booktitle> Proc. of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 85-94, </pages> <year> (1990). </year>
Reference: [12] <author> Culler, D., Karp, R., Patterson, D., Sahay, A., Schauser, K., Santos, E., von Eicken, T., </author> <title> "LogP: Towards a Realistic Model of Parallel Computation", </title> <booktitle> Proc. of the ACM SIG-PLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pp. 1-12, </pages> <year> (1993). </year>
Reference-contexts: Another example of a bridging model which has focused on more accurately reflecting existing machine attributes is the LogP model <ref> [12] </ref>. This model, though closely related to the BSP model, is distinct in two ways. First, it models asynchronous execution.
Reference: [13] <author> Dally, W., Keckler, S., Carter, N., Chang, A., Fillo, M., Lee, W., </author> <title> "M-Machine Architecture v1.0", MIT Concurrent VLSI Architecture Memo 58, </title> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> (1994). </year> <title> [14] de Torre, </title> <editor> P., and Kruskal, C., </editor> <title> "Towards a Single Model of Efficient Computation in Real Machines", </title> <booktitle> Proc. of Parallel Architectures and Languages Europe (PARLE91), Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> (1991). </year>
Reference-contexts: In response to the incentives to reduce communication overhead in parallel computers several machines have been envisioned in which more of the network communication mechanisms are handled in hardware, e.g. the M-machine <ref> [13] </ref>. Such decreases in the overhead of a communication increase the relative importance of network topology. Yet contention-based latency is a complicated function of topology and the communication pattern, and while toplogical considerations may provide performance optimizations, adding topology to a simple abstract machine model complicates it.
Reference: [15] <author> Gibbons, P., </author> <title> "A More Practical PRAM Model", </title> <booktitle> Proc. of the 1st Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <year> (1989). </year>
Reference-contexts: The standard PRAM posits a rigid execution pattern in which all processors are synchronized by a global clock. Several variants ease this restriction. Examples which allow asynchronous execution with irregular synchronization points include the APRAM [10] and the Asynchronous PRAM <ref> [15] </ref>. Periodic synchronization between intervals of asynchronous execution is incorporated in the XPRAM [34]. While these models incorporate synchronization, they do not charge an explicit cost.
Reference: [16] <author> Gibbons, P., Matias, Y., and Ramachandran, V., </author> <title> "The QRQW PRAM: Accounting for Contention in Parallel Algorithms", </title> <booktitle> Proc. of the 6th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 638-648, </pages> <year> (1994), </year> <note> to appear. </note>
Reference-contexts: Only one memory access can occur within each module per time step. The LPRAM (or Local-memory PRAM) proposed in [3] augments the CREW PRAM by associating with each processor an unlimited amount of local private memory. The QRQW PRAM <ref> [16] </ref> provides an intermediary, a queue, to arbitrate and manage memory accesses, while charging a memory access cost which is a function of the queue length. These arbitration devices all tend to provide the incentive to avoid target memory location contention. 2.Synchronization.
Reference: [17] <author> Goodrich, M., </author> <title> "Parallel Algorithms Column 1: Models of Computation", </title> <journal> SIGACT News, </journal> <volume> vol. 24, </volume> <pages> pp. 16-21, </pages> <month> Decem-ber </month> <year> (1993). </year>
Reference: [18] <author> Heywood, T., and Ranka, S., </author> <title> "A Practical Hierarchical Model of Parallel Computation: 1. The Model", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 16, </volume> <pages> pp. 212-232, </pages> <year> (1992). </year>
Reference: [19] <author> Hightower, W., Prins, J., and Reif, J., </author> <title> "Implementations of Randomized Sorting Algorithms on Large Parallel Machines", </title> <booktitle> Proc. of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-167, </pages> <year> (1992). </year>
Reference-contexts: The model incorporates the cost of an arithmetic operation, a nearest neighbor communication, a general communication and a scan operation. A similar analysis is performed using a model developed to reflect the MasPar MP1 computer in <ref> [19] </ref>. This model, also used to draw conclusions about the efficiency of alternative sorting algorithms, employs a very similar set of parameters.
Reference: [20] <author> Jaja, J., </author> <title> "An Introduction to Parallel Algorithms", </title> <publisher> Addison-Wesley, </publisher> <year> (1992). </year>
Reference-contexts: In its simplest form the parallel random access machine (PRAM) model <ref> [20] </ref> posits a set of P processors, with global shared memory, executing the same program in lockstep. Though there is some variability between PRAM definitions, the standard PRAM is a MIMD computer where each processor can execute its own instruction stream. <p> Presented below is a representative subset that illustrate which practical machine characteristics have been the focus of efforts to improve the PRAM. 1.Memory Access. Among basic variants of the PRAM, the most powerful is the CRCW PRAM <ref> [20] </ref> which allows concurrent reading or writing of any memory location, with some rule to resolve concurrent writes, such as randomization, prioritization, or combining. Concurrent memory access at unit cost is perhaps the most egregious aspect of this model, and many variants have been developed to restrict this idealization. <p> Concurrent memory access at unit cost is perhaps the most egregious aspect of this model, and many variants have been developed to restrict this idealization. The EREW and CREW PRAMs <ref> [20] </ref> restrict access to a given memory location to one processor at a time, either for both reads and writes (EREW) or just for writes (CREW). This preserves unit access cost but enforces some notion of serial access.
Reference: [21] <author> Kruskal, C., Rudolph, L., and Snir, M., </author> <title> "A Complexity Theory of Efficient Parallel Algorithms", </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 71, </volume> <pages> pp. 95-132, </pages> <year> (1990). </year>
Reference: [22] <author> Leighton, T. </author> <title> "Introduction to Parallel Architectures: Arrays, Trees, Hypercubes", </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1992). </address>
Reference-contexts: These models provide design incentives for efficient data mappings and communication routings. There are at least as many models as there are proposed network topologies. A survey and analysis with an exhaustive bibliography can be found in <ref> [22] </ref>. While the standard PRAM presents an uncluttered and appealing design platform, it presents little opportunity for optimization with respect to practical machine attributes. Each of the classes of models discussed subsequently introduces some practical aspect to the user, thereby conferring some responsiblity for optimization.
Reference: [23] <author> Leiserson, C., and Maggs, B., </author> <title> "Communication-Efficient Parallel Algorithms for Distributed Random-Access Machines", </title> <journal> Algorithmica, </journal> <volume> Vol. 3, </volume> <pages> pp. 53-77, </pages> <year> (1988). </year>
Reference-contexts: Another example of a PRAM variant which assumes two classes of memory and includes a mechanism for assigning a non-unit cost to a remote access is the DRAM <ref> [23] </ref>. The DRAM is important because it eliminates the paradigm of global shared memory and replaces it with only private distributed memory. While the topology of the communication network is ignored, the DRAM incorporates the notion of limited bandwidth.
Reference: [24] <author> Liu, P., Aiello, W., and Bhatt, S., </author> <title> "An Atomic Model for Message Passing", </title> <booktitle> Proc. of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 154-163, </pages> <year> (1993). </year>
Reference-contexts: This model is notable for its explicit mechanism and incentive for latency hiding. The model is strictly a model of communication and provides no description of computation. A related model which includes computation but introduces quite a bit more complexity is the Atomic Model for Message Passing developed in <ref> [24] </ref>. Low-Level Models Many abstract models have been recently developed which incorporate a more detailed view of the machine components and behavior.
Reference: [25] <author> Mansfield, E., "Microeconomics", </author> <title> Third Edition, </title> <publisher> Norton and Co., </publisher> <address> New York, </address> <year> (1979). </year>
Reference-contexts: Thus the PRAM provides a measure of the ideal parallel time complexity. Stalwart PRAM critics do not share this favorable view. Instead they point to economic externality arguments <ref> [25] </ref>. Without associating a reasonable and practical cost to the use of a resource (computational parallelism), there is an incentive to use the resource abusively.
Reference: [26] <author> Martel, C., Subramonian, R., and Park, A., </author> <title> "Asynchronous PRAM Algorithms for List Ranking and Transitive Closure", </title> <booktitle> Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> (1990). </year>
Reference: [27] <author> Melhorn, K., and Vishkin, U., </author> <title> "Randomized and Deterministic Simulations of PRAMs by Parallel Machines with Restricted Granularity of Parallel Memory", </title> <journal> Acta Infor-matica, </journal> <volume> vol. 21, </volume> <pages> pp. 339-374, </pages> <year> (1984). </year>
Reference-contexts: This preserves unit access cost but enforces some notion of serial access. Another PRAM variant which arbitrates memory access is the Module Parallel Computer (MPC) <ref> [27] </ref>. In this model shared global memory is broken into m modules. Only one memory access can occur within each module per time step. The LPRAM (or Local-memory PRAM) proposed in [3] augments the CREW PRAM by associating with each processor an unlimited amount of local private memory.
Reference: [28] <author> McColl, W., </author> <title> "General Purpose Parallel Computing", </title> <type> Technical Report, </type> <institution> NEC Research Institute, Princeton, NJ, </institution> <month> April </month> <year> (1992). </year>
Reference: [29] <author> Siegal H., Abraham, S., Bain, W., Batcher, K., Casavant, T., DeGroot, D., Dennis, J., Douglas, D., Feng, T., Good-man, J., Huang, A., Jordan, H., Jump, R., Patt, Y., Smith, A., Smith, J., Snyder, L., Stone, H., Tuck, R., and Wah, B., </author> <title> "Report of the Purdue Workshop on Grand Challenges in Computer Architecture for the Support of High Performance Computing", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 16, </volume> <pages> pp. 199-211, </pages> <year> (1992). </year>
Reference-contexts: While the rewards for consistency are fairly apparent, much less apparent is the means of achieving this consistency. To date there seems to be some consensus on the importance of the problem <ref> [29] </ref> yet there has been no single model or set of models which has achieved widespread acc-ceptance and managed to achieve this consistancy. Serial vs.
Reference: [30] <author> Snyder, L., </author> <title> "Type Architectures, Shared Memory and the Corollary of Modest Potential", </title> <booktitle> Annual Review of Computer Science, Annual Review, </booktitle> <publisher> Inc. </publisher> <pages> pp. 289-318, </pages> <year> (1986). </year>
Reference-contexts: To accomplish this we first present a selected survey of models from several of these disciplines. The survey is presented within a simple logical framework, first proposed by Snyder in <ref> [30] </ref>. This framework allows the wide array of models used in computer science to be viewed somewhat systematically. The solution to any given task begins with the design of a set of steps (an algorithm) which will realize the computational solution to an abstract problem specification. <p> In [35] Valiant provides compelling arguments for an abstract parallel machine model which provides a unifying and consis-tant design paradigm to facilitate portable parallel algorithm design and program translation. In an early multidisciplinary effort Snyder proposed one possible bridging model, the Candidate Type Architecture model <ref> [30] </ref>. This model posits a finite number of sequential von Neumann computers executing asynchronously, with a global synchronization mechanism, connected in a network of fixed bounded degree. The model specifies communication cost, but synchronization, achieved through the global controller, is free, and there are no bandwidth constraints.
Reference: [31] <author> Stromberg, </author> <title> K.,"An Introduction to Classical Real Analysis", </title> <publisher> Wadsworth, </publisher> <address> Belmont, California, </address> <year> (1981). </year>
Reference-contexts: Even the early Greek mathematicians, who were primarily interested in geometry, dealt with the issues of modeling. Indeed, Pythagoras discovered that the set of rational numbers (ratios of whole numbers) were inadequate to describe the length of a diagonal of a square having sides of length 1 <ref> [31] </ref>. This made it necessary to augment the rationals to create a more powerful and expressive tool: the real numbers. Since the time of the Greeks, the art and science of modeling has mushroomed.
Reference: [32] <author> Upfal, E., </author> <title> "Efficient Schemes for Parallel Communication", </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> Vol. 31, </volume> <pages> pp. 507-517, </pages> <year> (1984). </year>
Reference-contexts: A fundamental paradigm shift, however, was the introduction of models in which modules of memory are associated with processors. An early example of this type of model is used by Upfal in <ref> [32] </ref>. Later referred to as the Distributed Memory Model (DMM), it posits private memory modules associated with processors in a bounded degree network. Computation and nearest neighbor communication require one time step.
Reference: [33] <author> Valiant, L., </author> <title> "A Bridging Model for Parallel Computation", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 33, </volume> <pages> pp. 103-111, </pages> <year> (1990). </year>
Reference-contexts: Bridging Models The notion of a bridging model was effectively captured by Valiant when he described the von Neumann model as "a connecting bridge that enables programs to run efficiently on machines from the diverse and chaotic world of hardware" <ref> [33] </ref>. In [35] Valiant provides compelling arguments for an abstract parallel machine model which provides a unifying and consis-tant design paradigm to facilitate portable parallel algorithm design and program translation. In an early multidisciplinary effort Snyder proposed one possible bridging model, the Candidate Type Architecture model [30].
Reference: [34] <author> Valiant, L., </author> <title> "General Purpose Parallel Architectures", </title> <editor> van Leeuwen, J., ed. </editor> <booktitle> Handbook of Computer Science, </booktitle> <publisher> MIT Press, </publisher> <year> (1990). </year>
Reference-contexts: Several variants ease this restriction. Examples which allow asynchronous execution with irregular synchronization points include the APRAM [10] and the Asynchronous PRAM [15]. Periodic synchronization between intervals of asynchronous execution is incorporated in the XPRAM <ref> [34] </ref>. While these models incorporate synchronization, they do not charge an explicit cost. Although the only cost is implicit, (the loss of processor utilization while waiting for other processors to complete) these models still provide an incentive to synchronize only when necessary. 3.Latency.
Reference: [35] <author> Valiant, L., </author> <title> "Why BSP Computers", </title> <type> Technical Report TR-26-92, </type> <institution> Aiken Computation Laboratory, Harvard University, </institution> <address> Cambridge, MA (1992). </address>
Reference-contexts: Bridging Models The notion of a bridging model was effectively captured by Valiant when he described the von Neumann model as "a connecting bridge that enables programs to run efficiently on machines from the diverse and chaotic world of hardware" [33]. In <ref> [35] </ref> Valiant provides compelling arguments for an abstract parallel machine model which provides a unifying and consis-tant design paradigm to facilitate portable parallel algorithm design and program translation. In an early multidisciplinary effort Snyder proposed one possible bridging model, the Candidate Type Architecture model [30].
Reference: [36] <author> Mansour, Y., Nisan, N., Vishkin, U., </author> <title> "Trade-offs Between Communication Throughput and Parallel Time", </title> <booktitle> Proc. of the 26th Symposium on Theory of Computing, </booktitle> <address> Montreal, Quebec, Canada, </address> <pages> pp. 372-380, </pages> <year> 1994. </year>
Reference-contexts: While the function is somewhat complicated it attempts to 4 provide scheduling incentives to respect limited access to non-local data. A recent PRAM variant proposed in <ref> [36] </ref>, the PRAM (m) incorporates bandwidth limitations by restricting the size of global shared memory to m memory locations. The model is a CRCW PRAM except that in any given step only m accesses can be serviced. 5.Primitives.
Reference: [37] <author> Vitter, J., </author> <title> "Efficient Memory Access in Large Scale Computation", </title> <booktitle> Proc. of the 1991 Symposium on Theoretical Aspects of Computer Science (STACS), Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 26-41, </pages> <year> (1991). </year>
Reference-contexts: A perspective on the potential benefits of data movement 5 parallelism can be found in [5] and a survey of paral-lel hierarchical memory models can be found in <ref> [37] </ref>. For problems which involve the movement of large amounts of data these models may be particularly effective in producing efficient algorithms. An example of algorithm design using these models can be found in [38].
Reference: [38] <author> Vitter, J., and Shriver, E., </author> <title> "Algorithms for Parallel Memory I: Two Level Memories", and "Algorithms for Parallel Memory II: Hierarchical Multilevel Memories", </title> <type> Technical Reports, </type> <institution> CS-1993-01 and CS-1993-02, Department of Computer Science, Duke University, </institution> <month> January </month> <year> (1993). </year> <note> (to appear in a special issue of Algorithmica on the subject of large scale memories; a summarized version of this research was presented at the 22nd Annual Symposium on Theory of Computing, </note> <institution> Baltimore, MD, </institution> <month> May </month> <year> 1990). </year> <month> 10 </month>
Reference-contexts: For problems which involve the movement of large amounts of data these models may be particularly effective in producing efficient algorithms. An example of algorithm design using these models can be found in <ref> [38] </ref>. Network Models Both classes of models discussed above ignore the possible impacts of the topology of the communication network. Network models of parallel computation reflect a focus of concern in the early generation of parallel computers.
References-found: 37

