URL: http://www.mpik-tueb.mpg.de/people/personal/bs/papers/kdd.ps.gz
Refering-URL: http://www.mpik-tueb.mpg.de/people/personal/bs/svm.html
Root-URL: 
Email: schoel@big.att.com cjcb@big.att.com vlad@big.att.com  
Title: Extracting Support Data for a Given Task  
Author: Bernhard Scholkopf Chris Burges Vladimir Vapnik 
Address: 101 Crawfords Corner Road Holmdel NJ 07733 USA  
Affiliation: AT&T Bell Laboratories  
Abstract: We report a novel possibility for extracting a small subset of a data base which contains all the information necessary to solve a given classification task: using the Support Vector Algorithm to train three different types of handwritten digit classifiers, we observed that these types of classifiers construct their decision surface from strongly overlapping small ( 4%) subsets of the data base. This finding opens up the possibility of compressing data bases significantly by disposing of the data which is not important for the solution of a given task. In addition, we show that the theory allows us to predict the classifier that will have the best generalization ability, based solely on performance on the training set and characteristics of the learning machines. This finding is important for cases where the amount of available data is limited. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boser, B. E.; Guyon, I. M.; and Vapnik, V. </author> <year> 1992. </year> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh ACM 144-152. </address>
Reference-contexts: Maximizing (11) then requires the computation of dot products ((x) (x i )) in a high-dimensional space. Under certain conditions <ref> (Boser, Guyon & Vapnik, 1992) </ref>, these expensive calculations can be reduced significantly by using a function K such that ((x) (x i )) = K (x; x i ): We thus get decision functions of the form f (x) = sgn i=1 ! Experimental Results In our experiments, we used the
Reference: <author> Bromley, J.; and Sackinger, E. </author> <year> 1991. </year> <title> Neural-network and k-nearest-neighbor classifiers. </title> <type> Technical Report 11359-910819-16TM, </type> <institution> AT&T. </institution>
Reference-contexts: They should be compared with values achieved on the same database with a five-layer neural net (LeCun et al., 1990), 5.1%, a two-layer neural net, 5.9%, and the human performance, 2.5% <ref> (Bromley & Sackinger, 1991) </ref>. Table 1: Performance for three different types of classifiers, constructed with the support vector algorithm by choosing different functions K in (13) and (14). Given are raw errors (i.e. no rejections allowed) on the test set.
Reference: <author> Cortes, C.; and Vapnik, V. </author> <year> 1995. </year> <title> Support Vector Networks. </title> <note> To appear in Machine Learning. </note>
Reference: <author> Le Cun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. J. </author> <year> 1990. </year> <title> Handwritten digit recognition with a backpropagation network. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> 396-404, </pages> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: We used a US postal database of 9300 handwritten digits (7300 for training, 2000 for testing) <ref> (cf. LeCun et al., 1990) </ref>. Each digit is a 16fi16 vector with entries between 1 and 1. Preprocessing consisted in smoothing with a Gaussian kernel of width = 0:75. <p> Performance for Various Types of Classifiers The results for the three different functions K are summarized in Table 1. They should be compared with values achieved on the same database with a five-layer neural net <ref> (LeCun et al., 1990) </ref>, 5.1%, a two-layer neural net, 5.9%, and the human performance, 2.5% (Bromley & Sackinger, 1991). Table 1: Performance for three different types of classifiers, constructed with the support vector algorithm by choosing different functions K in (13) and (14).
Reference: <author> Vapnik, V. </author> <year> 1995. </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, forthcoming. </publisher>
Reference-contexts: In the case where the available training data is limited, it is important to have a means for achieving the best possible generalization by controlling characteristics of the learning machine. We use a bound of statistical learning theory <ref> (Vapnik, 1995) </ref> to predict the degree which yields the best generalization for polynomial classifiers. In the next Section, we follow Vapnik (1995), Boser, Guyon & Vapnik (1992), and Cortes & Vapnik (1995) in briefly recapitulating this algorithm and the idea of Structural Risk Minimization that it is based on. <p> Then we define a decision function f w;b by f w;b : B x 1 ;:::;x r ! f1g; The possibility of introducing a structure on the set of hyperplanes is based on the fact <ref> (Vapnik, 1995) </ref> that the set ff w;b : kwk Ag has a VC-dimension h satis fying h R 2 A 2 : (5) Note. Dropping the condition kwk A leads to a set of functions whose VC-dimension equals N + 1, where N is the dimensionality of Z.
References-found: 5

