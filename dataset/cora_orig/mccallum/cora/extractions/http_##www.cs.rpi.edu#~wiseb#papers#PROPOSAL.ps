URL: http://www.cs.rpi.edu/~wiseb/papers/PROPOSAL.ps
Refering-URL: http://www.cs.rpi.edu/~wiseb/mybib.html
Root-URL: http://www.cs.rpi.edu
Title: Beyond Multimedia: An Architecture for True Multimodal Interfaces A Research Proposal  Advisory Committee  
Author: G. Bowden Wise Prof. Ephraim P. Glinert, chair, Prof. Mukkai S. Krishnamoorthy, Prof. Edwin H. Rogers, Prof. David B. Boles, 
Date: December 3, 1993  
Address: Troy, NY 12180  
Affiliation: Dept. of Computer Science Rensselaer Polytechnic Institute  Computer Science  Computer Science  Computer Science  Psychology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Tony Feldman, </author> <title> editor. Virtual Reality '91: Impacts and Applications. </title> <publisher> Meckler Publishing, </publisher> <month> June </month> <year> 1991. </year> <title> Held in London. </title>
Reference-contexts: Research into so-called virtual reality has demonstrated the potential for computers to communicate with humans using additional senses such as touch <ref> [1, 2] </ref>, but in this research we will focus our attention to just the two modalities which support the highest bandwidth for information transmission. How to deal with even this restricted problem effectively and efficiently is not yet known.
Reference: [2] <author> Ken Pimentel and Kevin Teixeira. </author> <title> Virtual Reality: Through the Looking Glass. Wind-crest Books, </title> <booktitle> first edition, </booktitle> <year> 1993. </year>
Reference-contexts: Research into so-called virtual reality has demonstrated the potential for computers to communicate with humans using additional senses such as touch <ref> [1, 2] </ref>, but in this research we will focus our attention to just the two modalities which support the highest bandwidth for information transmission. How to deal with even this restricted problem effectively and efficiently is not yet known. <p> Metawidgets must have an associated set of widgets to represent their information in a variety of modalities. Because all of our widgets are inherited from the same base class BaseWidget, a set of representations may be represented as an array of BaseWidgets. BaseWidget widgetRepresentations <ref> [2] </ref>; Each metawidget will also share a common interface definitions. Methods will be needed to calculate and retrieve cognitive resource values for the current widget representation of the metawidget. Other methods will be needed to creating and deleting representations as well as transforming a representation from one to another.
Reference: [3] <author> Meera M. Blattner, Ephraim P. Glinert, Joaqium A. Jorge, and Gary R. Ormsby. Metawidgets: </author> <title> Towards a theory of multimodal interface design. </title> <booktitle> In Proc. </booktitle> <address> COMP-SAC'92, </address> <publisher> Chicago, </publisher> <pages> pages 115-120. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September 22-25 </month> <year> 1992. </year>
Reference-contexts: A multimodal interface needs a mechanism for describing sets of representations for each information object, whether visual or aural or both, and for selecting among those representations. Glinert and Blattner have recently proposed such a class of user interface objects, which they call "metawidgets" <ref> [3, 4] </ref>, but to date no systems have actually made use of this technology. The research presented here will extend the work of Glinert and Blattner, by using metawidgets as fundamental entities for interacting with users in an actual multimodal interface. <p> When the the visual and auditory properties are derived from the same data, the two modalities reinforce one another. Alternatively, when different data drive the two modalities, independently, the number of dimensions of data the icon can represent is maximized. Blattner, Greenberg, and Kamegai <ref> [3] </ref> used scientific audiolization to visualize turbulence. Turbulence is a particularly difficult phenomena to perceive visually. By using sound, many of the complex phenomena of turbulence, such as viscosity, density, temperature, speed of flow, direction of flow, and vortex size could be heard. <p> We propose the use of metawidgets <ref> [3, 4] </ref> as the fundamental building block for multimodal user interfaces. Metawidgets will contain a list of representations they present as well as methods for selecting among them. from toolkit widgets.
Reference: [4] <author> Ephraim P. Glinert and Meera M. Blattner. </author> <title> Programming the multimedia interface. </title> <booktitle> In Proc. 1st ACM Int. Conf. on Multimedia (MULTIMEDIA'93), Anaheim, </booktitle> <pages> pages 189-197. </pages> <publisher> ACM Press, </publisher> <month> August 2-6 </month> <year> 1993. </year>
Reference-contexts: A multimodal interface needs a mechanism for describing sets of representations for each information object, whether visual or aural or both, and for selecting among those representations. Glinert and Blattner have recently proposed such a class of user interface objects, which they call "metawidgets" <ref> [3, 4] </ref>, but to date no systems have actually made use of this technology. The research presented here will extend the work of Glinert and Blattner, by using metawidgets as fundamental entities for interacting with users in an actual multimodal interface. <p> We propose the use of metawidgets <ref> [3, 4] </ref> as the fundamental building block for multimodal user interfaces. Metawidgets will contain a list of representations they present as well as methods for selecting among them. from toolkit widgets.
Reference: [5] <author> Kenneth J. Kokjer. </author> <title> The information capacity of the human fingertip. </title> <journal> IEEE Transactions on Systems, Man,and Cybernetics, </journal> <volume> SMC-17(1):100-102, </volume> <month> January/February </month> <year> 1987. </year>
Reference-contexts: Vision has a much higher bandwidth for information reception than our other senses <ref> [5] </ref> which is one reason graphics has been the dominant form of presentation in most computer systems. Hearing follows vision in terms of bandwidth, but is two magnitudes slower. Another major difference between vision and hearing is their physical dimension of existence.
Reference: [6] <author> Sara A. Bly, William Buxton (moderator), Steven P. Frysinger, David Lunney, Dou-glass L. Mansur, Joseph J. Mezrich, and Robert C. Morrison. </author> <title> Communicating with sound. </title> <booktitle> In CHI'85 Proceedings, </booktitle> <address> New York, </address> <pages> pages 115-119. </pages> <publisher> ACM SIGCHI, ACM Press, </publisher> <month> April 14-18 </month> <year> 1985. </year>
Reference-contexts: A sound on the other hand only exists briefly, perhaps a few seconds. Once the sound has passed, it is gone and we cannot replay it. The transient nature of sound makes it a well-suited medium for conveying information about system state and presenting discrete messages. Bly <ref> [6] </ref> discusses many of the advantages of sound as audio cues: * Audio cues can be processed in parallel.
Reference: [7] <author> Dave Kansas. </author> <title> The icon crisis: Tiny pictures cause confusion. The Wall Street Journal, </title> <institution> page B1, </institution> <month> November 17 </month> <year> 1993. </year>
Reference-contexts: Icons are often constructed using abstract symbols making it difficult to determine their meaning. Icons have also become so common that some symbols are reused in different applications with different underlying meanings, leading to even more confusion <ref> [7] </ref>. Well designed audio cues may convey the meaning more efficiently than an icon. 9 * Audio cues will not interfere with the visual display. Graphical computer displays are already overloaded with information.
Reference: [8] <author> William Buxton. </author> <title> Introduction to this special issue on nonspeech audio. </title> <journal> Human-Computer Interaction, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1989. </year>
Reference-contexts: Conveying information using sound will help decrease the number of visual messages. * Sound can be used where vision is unavailable. Visually impaired users have much difficulty or are unable to interpret information presented visually. Presenting information using sound will help make computers more accessible to impaired users. Buxton <ref> [8] </ref> indicates that, functionally, sound provides one of three general types of information: 1. Alarms and warnings. These messages take priority over other information. Their purpose is to interrupt any ongoing task and alert the user to something that requires immediate attention.
Reference: [9] <author> Mark H. Ashcraft. </author> <title> Human Memory and Cognition. </title> <publisher> Harper Collins, </publisher> <year> 1989. </year>
Reference-contexts: However, only the theories of attention will 10 be discussed briefly here, for they provide the most relevance to multimodal interfaces. For more information on the theories of psychology, consult Ashcraft's excellent introduction to cognitive psychology <ref> [9] </ref>, books on human factors [10, 11, 12, 13], and journals devoted to human factors, such as the journal Human Factors. 2.2.1 Theories of Attention At any given moment we are performing a variety of tasks.
Reference: [10] <author> Peter H. Lindsay and Donald A. Norman. </author> <title> Human Information Processing: An Introduction to Psychology. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: However, only the theories of attention will 10 be discussed briefly here, for they provide the most relevance to multimodal interfaces. For more information on the theories of psychology, consult Ashcraft's excellent introduction to cognitive psychology [9], books on human factors <ref> [10, 11, 12, 13] </ref>, and journals devoted to human factors, such as the journal Human Factors. 2.2.1 Theories of Attention At any given moment we are performing a variety of tasks.
Reference: [11] <author> Stuart K. Card, Thomas P. Moran, and Allen Newell. </author> <title> The Psychology of Human-Computer Interaction. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1983. </year>
Reference-contexts: However, only the theories of attention will 10 be discussed briefly here, for they provide the most relevance to multimodal interfaces. For more information on the theories of psychology, consult Ashcraft's excellent introduction to cognitive psychology [9], books on human factors <ref> [10, 11, 12, 13] </ref>, and journals devoted to human factors, such as the journal Human Factors. 2.2.1 Theories of Attention At any given moment we are performing a variety of tasks.
Reference: [12] <author> Barry H. Kantowitz and Robert D. Sorkin. </author> <title> Human Factors: Understanding People-System Relationships. </title> <publisher> John Wiley & Sons, </publisher> <year> 1983. </year>
Reference-contexts: However, only the theories of attention will 10 be discussed briefly here, for they provide the most relevance to multimodal interfaces. For more information on the theories of psychology, consult Ashcraft's excellent introduction to cognitive psychology [9], books on human factors <ref> [10, 11, 12, 13] </ref>, and journals devoted to human factors, such as the journal Human Factors. 2.2.1 Theories of Attention At any given moment we are performing a variety of tasks.
Reference: [13] <author> Peter A. Hancock. </author> <title> Human Factors Psychology. </title> <publisher> North Holland, </publisher> <year> 1987. </year> <month> 48 </month>
Reference-contexts: However, only the theories of attention will 10 be discussed briefly here, for they provide the most relevance to multimodal interfaces. For more information on the theories of psychology, consult Ashcraft's excellent introduction to cognitive psychology [9], books on human factors <ref> [10, 11, 12, 13] </ref>, and journals devoted to human factors, such as the journal Human Factors. 2.2.1 Theories of Attention At any given moment we are performing a variety of tasks.
Reference: [14] <author> Martin L. Fracker and Christopher D. Wickens. </author> <title> Resources, confusions, and compati-bility in dual-axis tracking: Displays, controls, and dynamics. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 15(1) </volume> <pages> 80-96, </pages> <year> 1989. </year>
Reference-contexts: Below we will examine multiple resource theory because it provides some insight into how attention is affected when information is perceived through several modalities simultaneously. For further information on these theories see Fracker's review <ref> [14] </ref>. Multiple resource theory [15] contends that when multiple tasks are performed together, their performance will deteriorate if they compete for the same mental resources. Attention can be thought of as a resource that can be distributed to different stages of processing depending on task demands.
Reference: [15] <editor> Christopher D. Wickens. Attention. In Peter A. Hancock, editor, </editor> <booktitle> Human Factors Psychology, </booktitle> <pages> pages 29-80. </pages> <publisher> North Holland, </publisher> <year> 1987. </year>
Reference-contexts: Below we will examine multiple resource theory because it provides some insight into how attention is affected when information is perceived through several modalities simultaneously. For further information on these theories see Fracker's review [14]. Multiple resource theory <ref> [15] </ref> contends that when multiple tasks are performed together, their performance will deteriorate if they compete for the same mental resources. Attention can be thought of as a resource that can be distributed to different stages of processing depending on task demands.
Reference: [16] <author> Joseph Rothstein. MIDI: </author> <title> A Comprehensive Introduction, volume 7 of The Computer Music and Digital Audio Series. </title> <publisher> A-R Editions, </publisher> <year> 1992. </year>
Reference-contexts: Two visual messages or two aural messages must use the same resources. 2.3 Techniques for Nonspeech Audio Earlier computer systems were very limited in their sound output capability. At best, a few tones of differing frequency and duration could be created. The introduction of MIDI <ref> [16] </ref> made it possible for computers to create the sounds of musical instruments.
Reference: [17] <author> Meera M. Blattner, Denise A. Sumikawa, and Robert M. Greenberg. Earcons and icons: </author> <title> Their structure and common design principles. </title> <editor> In Ephraim P. Glinert, editor, </editor> <booktitle> Visual Programming Environments: Applications and Issues, </booktitle> <pages> pages 582-606. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year> <note> Also published in Human-Computer Interaction, Volume 4, Number 1, </note> <year> 1989, </year> <pages> pages 11-44. </pages>
Reference-contexts: Mapping the data to sound enables the data to be "heard." For example, the height of the bars in a bar graph can be translated to frequency. More examples of this technique will be presented in Section 2.4.1. 2.3.2 Earcons Blattner, Sumikawa, and Greenberg <ref> [17] </ref> studied the use of sound in the human-computer interface as an alternative mechanism for presenting information. <p> Sounds in a computer interface must be easy to learn and recognize. The systematic structure of earcons reduces the number of different sounds users must remember and facilitates recall. This is what makes earcons so powerful. 17 Sumikawa [20] and later Blattner <ref> [17] </ref> published a set of guidelines for the construction of earcons, but neither of them actually tested the use of earcons in an implementation. Barfield, Rosenberg, and Levasseur [21] were the first to conduct an experiment using earcons.
Reference: [18] <author> William W. Gaver. </author> <title> Auditory icons: Using sound in computer interfaces. </title> <journal> Human-Computer Interaction, </journal> <volume> 2 </volume> <pages> 167-177, </pages> <year> 1986. </year>
Reference-contexts: Representational icons can be thought of as natural sounds that sound like a real object of which we are familiar (e.g., a telephone ringing, or a scraping sound). Representational icons are very similar to Gaver's concept of auditory icons <ref> [18] </ref> which will be discussed more fully in the following section. Abstract earcons, on the other hand, are unfamiliar sounds having no relationship to a real world sound. Semi-abstract earcons may have some resemblance to a real world sound, but also contain synthetic sounds.
Reference: [19] <author> Jospeh Kerman. </author> <title> Listen. </title> <publisher> Worth Publishers, Inc., </publisher> <address> brief edition, </address> <year> 1987. </year>
Reference-contexts: Rhythm and pitch are the fixed parameters of motives, and timbre, register, and dynamics are the variable parameters of motives. Briefly, we will define each of these parameters. For a more thorough discussion of these musical terms see an introductory text on music, such as Kerman <ref> [19] </ref>. Rhythm relates to the timing and weighting of the notes and gives music a sense of forward movement. Pitch is related to the frequency of the note. In western tonal music, there are eight octaves of 12 pitches each for a total of 96 pitches.
Reference: [20] <author> Denise A. Sumikawa. </author> <title> Guidelines for the integration of audio cues into computer interfaces. </title> <type> Technical Report UCRL-53656, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1985. </year> <note> Also published as Master's thesis. </note>
Reference-contexts: Sounds in a computer interface must be easy to learn and recognize. The systematic structure of earcons reduces the number of different sounds users must remember and facilitates recall. This is what makes earcons so powerful. 17 Sumikawa <ref> [20] </ref> and later Blattner [17] published a set of guidelines for the construction of earcons, but neither of them actually tested the use of earcons in an implementation. Barfield, Rosenberg, and Levasseur [21] were the first to conduct an experiment using earcons.
Reference: [21] <author> Woodrow Barfield. </author> <title> The use of icons, earcons, and commands in the design of an online hierarchical menu. </title> <journal> IEEE Transactions on Professional Communication, </journal> <volume> 34(2) </volume> <pages> 101-108, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This is what makes earcons so powerful. 17 Sumikawa [20] and later Blattner [17] published a set of guidelines for the construction of earcons, but neither of them actually tested the use of earcons in an implementation. Barfield, Rosenberg, and Levasseur <ref> [21] </ref> were the first to conduct an experiment using earcons. In their experiment they tried to determine whether icons or commands are better for menus and whether the addition of sound (earcons) make any difference to menu recognition.
Reference: [22] <author> Stephen A. Brewster, Peter C. Wright, and Alistair D. N. Edwards. </author> <title> An evaluation of earcons for use in auditory human-computer interfaces. </title> <booktitle> In INTERCHI'93, Amsterdam, </booktitle> <pages> pages 222-227. </pages> <publisher> ACM Press, </publisher> <month> April 24-29 </month> <year> 1993. </year>
Reference-contexts: The sounds they used for earcons were very simple consisting of simple changes in pitch to distinguish menu items. In addition, there was not much variation among menu items. A more complex set of sounds may have revealed better performance. Brewster <ref> [22] </ref> conducted several experiments on the effectiveness of earcons. In one experiment, earcons were constructed in different ways to determine if structured sounds were better than unstructured sounds for communicating information and also to determine if musical sounds were better than synthetic ones.
Reference: [23] <author> William W. Gaver. </author> <title> What in the world do we hear?: An ecological approach to auditory event perception. </title> <journal> Ecological Psychology, </journal> <volume> 5(1) </volume> <pages> 1-29, </pages> <year> 1993. </year>
Reference-contexts: Auditory icons can also be used to represent multidimensional data. For instance, the magnitude of some data value could be represented by the size of a virtual sound-producing object. Gaver describes algorithms for implementing parameterized auditory icons for a variety of impact, bouncing, breaking, scraping, and machine sounds <ref> [23] </ref>. 2.3.4 Comparison of Earcons and Auditory Icons From the above discussion, it appears that auditory icons would result in better performance than earcons because they are more familiar and easier to remember. Jones and Furner [24] compared the use of earcons and auditory icons by performing two experiments.
Reference: [24] <author> S. D. Jones and S. M. Furmer. </author> <title> The construction of audio icons and information cues for human-computer dialogues. </title> <editor> In E. D. Megaw, editor, </editor> <booktitle> Contemporary Ergonomics 1989: Proceedings of the Ergonomics Society's 1989 Annual Conference. </booktitle> <publisher> Taylor & Francis, </publisher> <year> 1989. </year>
Reference-contexts: Jones and Furner <ref> [24] </ref> compared the use of earcons and auditory icons by performing two experiments. In the first experiment, subjects were given typical interface commands (e.g., delete or copy) along with a sample earcon, an auditory icon, or synthesized speech. Subjects preferred earcons over auditory icons.
Reference: [25] <author> Durand R. Begault. </author> <title> Perceptual effects of synthetic reverberation on three-dimensional audio systems. </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 40(11) </volume> <pages> 895-904, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: In both experiments, subjects scored highest with speech. 2.3.5 Spatial Sound and Virtual Reality In our everyday interaction with the world we not only hear the different qualities of sounds, such as pitch and timbre, but we can also determine what direction the sound originates from. Virtual reality systems <ref> [25, 26, 27, 28] </ref> utilize sophisticated equipment to generate sounds so that we perceive them as coming from different points in space. These systems are still too costly to be used in user interfaces, but may later prove to be another important way to present information to computer users.
Reference: [26] <author> Durand R. Begault and Elizabeth M. Wenzel. </author> <title> Techniques and applications for bin-aural sound manipulation in human-machine interfaces. </title> <journal> The International Journal of Aviation Psychology, </journal> <volume> 2(1) </volume> <pages> 1-22, </pages> <year> 1992. </year>
Reference-contexts: In both experiments, subjects scored highest with speech. 2.3.5 Spatial Sound and Virtual Reality In our everyday interaction with the world we not only hear the different qualities of sounds, such as pitch and timbre, but we can also determine what direction the sound originates from. Virtual reality systems <ref> [25, 26, 27, 28] </ref> utilize sophisticated equipment to generate sounds so that we perceive them as coming from different points in space. These systems are still too costly to be used in user interfaces, but may later prove to be another important way to present information to computer users.
Reference: [27] <author> Eilizabeth M. Wenzel, Frederic L. Wightman, and Doris J. Kistler. </author> <title> Localization with non-individualized virtual acoustic displays. </title> <booktitle> In CHI'91 Proceedings, </booktitle> <address> New York, </address> <pages> pages 351-359. </pages> <publisher> ACM SIGCHI, ACM Press, </publisher> <month> April 27 May 2 </month> <year> 1991. </year>
Reference-contexts: In both experiments, subjects scored highest with speech. 2.3.5 Spatial Sound and Virtual Reality In our everyday interaction with the world we not only hear the different qualities of sounds, such as pitch and timbre, but we can also determine what direction the sound originates from. Virtual reality systems <ref> [25, 26, 27, 28] </ref> utilize sophisticated equipment to generate sounds so that we perceive them as coming from different points in space. These systems are still too costly to be used in user interfaces, but may later prove to be another important way to present information to computer users.
Reference: [28] <author> Elizabeth M. Wenzel. </author> <title> Three-dimensional virtual acoustic displays. </title> <editor> In Merra M. Blat-tner and Roger B. Dannenberg, editors, </editor> <booktitle> Multimedia Interface Design, Frontier Series, </booktitle> <pages> pages 257-288. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: In both experiments, subjects scored highest with speech. 2.3.5 Spatial Sound and Virtual Reality In our everyday interaction with the world we not only hear the different qualities of sounds, such as pitch and timbre, but we can also determine what direction the sound originates from. Virtual reality systems <ref> [25, 26, 27, 28] </ref> utilize sophisticated equipment to generate sounds so that we perceive them as coming from different points in space. These systems are still too costly to be used in user interfaces, but may later prove to be another important way to present information to computer users.
Reference: [29] <author> Gary S. Kendall. </author> <title> Visualization by ear: Auditory imagery for scientific visualization and virtual reality. </title> <journal> Computer Music Journal, </journal> <volume> 15(4) </volume> <pages> 70-73, </pages> <month> Winter </month> <year> 1991. </year>
Reference-contexts: However, even modern day tools such as three-dimensional color graphics are limited when working with complex phenomena of many dimensions that are not well related to three-dimensional objects in space and time <ref> [29] </ref>. The process of displaying the data is called data representation. Traditionally, scientific visualization has relied on computer graphics and visual data representation. As humans we utilize many of our senses to experience the world around us. <p> Traditionally, scientific visualization has relied on computer graphics and visual data representation. As humans we utilize many of our senses to experience the world around us. The recent advances in multimedia and virtual reality have opened up other ways to interpret data. A number of researchers <ref> [30, 31, 29] </ref> have suggested that sound take on a more important role in the study of these complex phenomena through the use of auditory data representation and scientific audiolization.
Reference: [30] <author> Bill Buxton. </author> <title> Using our ears: An introduction to the use of nonspeech audio cues. </title> <editor> In Edward J. Farrell, editor, </editor> <title> Extracting Meaning from Complex Data: Processing, Display, </title> <booktitle> Interaction (Proceedings - SPIE The International Society for Optical Engineering), </booktitle> <pages> pages 124-127. SPIE, </pages> <month> February 14-16 </month> <year> 1990. </year>
Reference-contexts: Traditionally, scientific visualization has relied on computer graphics and visual data representation. As humans we utilize many of our senses to experience the world around us. The recent advances in multimedia and virtual reality have opened up other ways to interpret data. A number of researchers <ref> [30, 31, 29] </ref> have suggested that sound take on a more important role in the study of these complex phenomena through the use of auditory data representation and scientific audiolization.
Reference: [31] <author> Robert S. Hotchkiss and Cheryl L. Wampler. </author> <booktitle> The auditorialization of scientific information. ACM Proceedings?, </booktitle> <year> 1991. </year> <pages> Pages 453-461. </pages>
Reference-contexts: Traditionally, scientific visualization has relied on computer graphics and visual data representation. As humans we utilize many of our senses to experience the world around us. The recent advances in multimedia and virtual reality have opened up other ways to interpret data. A number of researchers <ref> [30, 31, 29] </ref> have suggested that sound take on a more important role in the study of these complex phenomena through the use of auditory data representation and scientific audiolization. <p> Finally, by assigning sounds to individual language tokens, programmers can hear their program during compilation. Listening to such sounds may identify syntactic errors within the program. Hotchkiss and Wampler <ref> [31] </ref> also show how program auralization can be used to listen to a program in execution. A unique frequency is assigned to each function within a program. During execution a note is played for a duration related to the length of time the program remains in each function.
Reference: [32] <author> J. M. Chambers, M. V. Mathews, and F. R. Moore. </author> <title> Auditory data inspection. </title> <institution> AT&T Bell Laboratories Technical Memorandum, (74-1214-20), </institution> <year> 1974. </year>
Reference-contexts: Other parameters that are often used are timbre, attack envelope, and direction (spatial location). Some examples of the use of sound for visualization will be discussed next. One of the earliest investigations of auditory data representation was performed by Chambers, Matthews, and Moore <ref> [32] </ref>. Multiple parameters of sound (e.g., frequency, spectral content, amplitude modulation) were used to encode dimensions of multidimensional data that were not displayed on a conventional scatterplot. The auditorily-enhanced scatterplot improved the classification of multivariate data.
Reference: [33] <author> E. S. Yeung. </author> <title> Pattern recognition by audio representation of multivariate analytical data. </title> <journal> Analytical Chemistry, </journal> <volume> 52(7) </volume> <pages> 1120-1123, </pages> <year> 1980. </year>
Reference-contexts: Multiple parameters of sound (e.g., frequency, spectral content, amplitude modulation) were used to encode dimensions of multidimensional data that were not displayed on a conventional scatterplot. The auditorily-enhanced scatterplot improved the classification of multivariate data. Yeung <ref> [33] </ref> explored the use of sound as an alternative to graphic presentation of data. In his experiment, subjects were asked to classify mineral samples using 7-dimensional 20 chemical data encoded in sound.
Reference: [34] <author> J. J. Mezrich, S. Frysinger, and R. Silvjanovski. </author> <title> Dynamic representation of multivari-ate time series data. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 79(385) </volume> <pages> 34-40, </pages> <year> 1984. </year>
Reference-contexts: The subjects reported an accuracy of 98% after training. An example of using sound to reinforce the visual display is the examination of multi-variate time series data by Mezrich, Frysinger, and Slivjanovski <ref> [34] </ref> in which an auditory-visual technique was compared to three visual techniques. In the auditory-visual technique, data were presented visually by representing each variable as a pair of vertical lines that got longer and further apart as the value of a variable increased.
Reference: [35] <author> Georges Grinstein and Stuart Smith. </author> <title> The perceptualization of scientific data. </title> <editor> In Ed-ward J. Farrell, editor, </editor> <title> Extracting Meaning from Complex Data: Processing, Display, </title> <booktitle> Interaction (Proceedings - SPIE The International Society for Optical Engineering), </booktitle> <pages> pages 190-191. SPIE, </pages> <month> February 14-16 </month> <year> 1990. </year>
Reference-contexts: The overlaid graphs and auditory-visual graphs yielded much better performance than the separate or vertically-aligned graphs. Subjects performed better with the auditory-visual graphs when the data is presented over a long enough span. Grinstein and Smith <ref> [35] </ref> developed an iconographic display that enables the texture of data to be perceived. An iconographic approach associates an icon with each single datum. Each icon has visual and aural attributes which are driven by the data.
Reference: [36] <author> Marcus H. Brown and John Hershberger. </author> <title> Color and sound in algorithm animation. </title> <type> Technical Report 76a, </type> <institution> DEC System Research Center, </institution> <address> 130 Lytton Avenue, Palo Alto, CA 94301, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition to understanding the intricacies of the algorithm, designers must also know how to effectively present the animation to the user. The Zeus system, developed by Brown and Hershberger <ref> [36, 37] </ref>, uses both sound and color graphics for algorithm animation. Whenever a particular variable is encountered in an algorithm, a tone is generated whose pitch is linearly related to the variable's value. Sound is used in four different ways in Zeus. Sound can reinforce what is already displayed visually.
Reference: [37] <author> Marc H. Brown. </author> <title> An introduction to zeus: Audiovisualization of some elementary sequential and parallel sorting algorithms. </title> <booktitle> In CHI'92 Proceedings, </booktitle> <address> New York, </address> <pages> pages 663-664. </pages> <publisher> ACM SIGCHI, ACM Press, </publisher> <month> May 3-7 </month> <year> 1992. </year> <month> 50 </month>
Reference-contexts: In addition to understanding the intricacies of the algorithm, designers must also know how to effectively present the animation to the user. The Zeus system, developed by Brown and Hershberger <ref> [36, 37] </ref>, uses both sound and color graphics for algorithm animation. Whenever a particular variable is encountered in an algorithm, a tone is generated whose pitch is linearly related to the variable's value. Sound is used in four different ways in Zeus. Sound can reinforce what is already displayed visually.
Reference: [38] <author> Christopher DiGiano and Ronald M. Baecker. </author> <title> Program auralization: Sound enhance-ments to the programming environment. </title> <booktitle> In Graphics Interface '92, </booktitle> <address> Palo Alto. </address> <publisher> Morgan Kaufmann Publishers, </publisher> <month> May 11-15, </month> <year> 1992 1992. </year>
Reference-contexts: Listening to the sound produced by an algorithm enables a pattern to be heard. These patterns form unique "signatures" for some algorithms. Another technique for analyzing the behavior of computer programs is called program auralization <ref> [38] </ref>. This refers to the use of nonspeech sound for the understanding and effective use of computer programs. DiGiano [38] identifies several classes of program information that are suitable for mapping to sound and suggest how to add auralization capabilities. <p> These patterns form unique "signatures" for some algorithms. Another technique for analyzing the behavior of computer programs is called program auralization <ref> [38] </ref>. This refers to the use of nonspeech sound for the understanding and effective use of computer programs. DiGiano [38] identifies several classes of program information that are suitable for mapping to sound and suggest how to add auralization capabilities. These concepts are illustrated in a sound-enhanced programming system called LogoMedia.
Reference: [39] <author> William W. Gaver, Randall B. Smith, and Tim O'Shea. </author> <title> Effective sounds in complex systems: </title> <booktitle> The ARKola simulation. In CHI'91 Conference Proceedings: Reaching Through Technology, </booktitle> <pages> pages 85-90. </pages> <publisher> ACM SIGCHI, ACM Press: Addison-Wesley, </publisher> <month> April 27 May 2 </month> <year> 1991. </year>
Reference-contexts: The sounds produced create a "symphony" of sounds 22 which sound like classical canonic composition. These symphonies can indicate defects in the code. 2.4.3 Collaborative Computing A very interesting application is the use of sound in collaborative environments. Gaver <ref> [39] </ref> incorporated his concept of auditory icons into a complex, multi-user simulation system called ARKola. The simulation system was built using the Shared Alternate Reality Kit or SharedARK [40], which allows virtual environments, such as laboratories and factories, to be constructed on a computer and later simulated.
Reference: [40] <author> William W. Gaver and Randall B. Smith. </author> <title> Auditory icons in large-scale collaborative environments. </title> <editor> In D. Diaper, editor, </editor> <booktitle> Human Computer Interaction - INTERACT'90, </booktitle> <pages> pages 735-740. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Gaver [39] incorporated his concept of auditory icons into a complex, multi-user simulation system called ARKola. The simulation system was built using the Shared Alternate Reality Kit or SharedARK <ref> [40] </ref>, which allows virtual environments, such as laboratories and factories, to be constructed on a computer and later simulated. Multiple users are able to use the system simultaneously to manipulate objects in the virtual environment.
Reference: [41] <author> William W. Gaver. </author> <title> The SonicFinder: An interface that uses auditory icons. </title> <journal> Human-Computer Interaction, </journal> <volume> 4 </volume> <pages> 67-94, </pages> <year> 1989. </year>
Reference-contexts: Researchers began to look at using the auditory channel to circumvent the overload of the visual system. We examine a few of these systems next. Gaver incorporated auditory icons into the Apple Macintosh user interface by developing an auditory interface called the SonicFinder <ref> [41] </ref>. Events in the interface (e.g., selecting an icon) are mapped to sound-producing events (e.g., tapping an object). The SonicFinder uses information that is available in the existing interface to trigger and control the playback of sampled sounds from everyday events.
Reference: [42] <author> Lester L. Ludwig, Natalio Pincever, and Michael Cohen. </author> <title> Extending the notion of a window system to audio. </title> <booktitle> Computer, </booktitle> <pages> pages 66-72, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: With the increasing importance of audio in computer applications (multimedia and otherwise), users also need to be able to manage the presentation and organization of a multitude of (possibly simultaneous) audio sources. Without such management the audio sources may be confused or even lost. Ludwig, Pincever, and Cohen <ref> [42] </ref> suggest the use of 24 audio windows to enable users to manage their audio streams. Users are able to organize audio sources spatially in one, two, or three dimensions; the sources can, thus, be perceived to be at different locations in a line, plane, and three space.
Reference: [43] <author> Jerome Elkind. </author> <title> The incidence of disabilities in the united states. </title> <booktitle> Human Factors, </booktitle> <volume> 32(4) </volume> <pages> 397-405, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The systems developed here will also benefit the normal user population and help bring multimodal computing into the mainstream. Elkind <ref> [43] </ref> analyzed data from the 1990 Census and discovered that there are approximately 92 million people within the United States with some sort of impairment; this is over a third of the total U.S. population.
Reference: [44] <author> Richard S. Schwerdtfeger. </author> <title> Making the GUI talk. </title> <journal> BYTE, </journal> <pages> pages 118-128, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The textual output could then easily be converted to another form of presentation, such as speech, so that it could be perceived. Screen readers, which translate ASCII text to synthetic speech, are readily available for text based interfaces <ref> [44] </ref>. There are a variety of visual impairments. Some visually impaired users only require that the graphical or textual output be enlarged. Users with more severe impairments require alternative nonvisual mechanisms for perceiving the information, such as, synthesized speech, tactile displays, braille displays, and braille hardcopy.
Reference: [45] <author> Doris Aaronson and Paul Gabias. </author> <title> Computer use by the visually impaired. Behavior Research Methods, </title> <journal> Instruments & Computers, </journal> <volume> 19(2) </volume> <pages> 275-282, </pages> <year> 1987. </year>
Reference-contexts: These adaptations allow their users to access information from any application running on their computer provided the output was presented on the screen. A description of the multitude of adaptations are discussed by Aaronson <ref> [45] </ref>, Brown [46], De l'Aune [47], Hjelmquist [48], and Schreier [49]. Another way to adapt computers is to design application-specific software that can be used by impaired users.
Reference: [46] <author> Judith R. Brown and Steve Cunningham. </author> <title> Programming the User Interface: Principles and Examples. </title> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference-contexts: These adaptations allow their users to access information from any application running on their computer provided the output was presented on the screen. A description of the multitude of adaptations are discussed by Aaronson [45], Brown <ref> [46] </ref>, De l'Aune [47], Hjelmquist [48], and Schreier [49]. Another way to adapt computers is to design application-specific software that can be used by impaired users.
Reference: [47] <author> William Del'Aune. </author> <title> Computers: Their genesis, use and potential. </title> <journal> Journal of Visual Impairment & Blindness, </journal> <pages> pages 401-407, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: These adaptations allow their users to access information from any application running on their computer provided the output was presented on the screen. A description of the multitude of adaptations are discussed by Aaronson [45], Brown [46], De l'Aune <ref> [47] </ref>, Hjelmquist [48], and Schreier [49]. Another way to adapt computers is to design application-specific software that can be used by impaired users. Vener's Magnex [50] is an editor for the Commodore Amiga personal computer that uses a "cross-hair" cursor to help people locate the current typing position.
Reference: [48] <author> Erland Hjelmquist, Bengt Jansson, and Gunilla Torell. </author> <title> Computer-oriented technology for blind readers. </title> <journal> Journal of Visual Impairment & Blindness, </journal> <pages> pages 210-215, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These adaptations allow their users to access information from any application running on their computer provided the output was presented on the screen. A description of the multitude of adaptations are discussed by Aaronson [45], Brown [46], De l'Aune [47], Hjelmquist <ref> [48] </ref>, and Schreier [49]. Another way to adapt computers is to design application-specific software that can be used by impaired users. Vener's Magnex [50] is an editor for the Commodore Amiga personal computer that uses a "cross-hair" cursor to help people locate the current typing position.
Reference: [49] <author> Elliot M. Schreier. </author> <title> The future of access technology for blind and visually impaired people. </title> <journal> Journal of Visual Impairment & Blindness, </journal> <pages> pages 520-523, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: These adaptations allow their users to access information from any application running on their computer provided the output was presented on the screen. A description of the multitude of adaptations are discussed by Aaronson [45], Brown [46], De l'Aune [47], Hjelmquist [48], and Schreier <ref> [49] </ref>. Another way to adapt computers is to design application-specific software that can be used by impaired users. Vener's Magnex [50] is an editor for the Commodore Amiga personal computer that uses a "cross-hair" cursor to help people locate the current typing position.
Reference: [50] <author> Avram R. Vener and Ephraim P. Glinert. Magnex: </author> <title> A text editor for the visually impaired. </title> <type> Technical Report 89-5, </type> <institution> Rensselaer Polytechnic Institute, Department of Computer Science, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1989. </year> <month> 51 </month>
Reference-contexts: A description of the multitude of adaptations are discussed by Aaronson [45], Brown [46], De l'Aune [47], Hjelmquist [48], and Schreier [49]. Another way to adapt computers is to design application-specific software that can be used by impaired users. Vener's Magnex <ref> [50] </ref> is an editor for the Commodore Amiga personal computer that uses a "cross-hair" cursor to help people locate the current typing position. The cross-hair utilizes a special diagonal line that originates from the top left-hand corner of the screen, which enables the eye to rapidly find the appropriate character.
Reference: [51] <author> Alistair D. N. Edwards. </author> <title> Soundtrack: An auditory interface for blind users. </title> <journal> Human--Computer Interaction, </journal> <volume> 4 </volume> <pages> 45-66, </pages> <year> 1989. </year>
Reference-contexts: The cross-hair utilizes a special diagonal line that originates from the top left-hand corner of the screen, which enables the eye to rapidly find the appropriate character. Edwards developed a word processor with an auditory interface, called Soundtrack <ref> [51] </ref>. The interface to Soundtrack consists of several screens each overlaid with a grid layout of non-overlapping auditory objects. An auditory object is defined by its spatial location, a name, an action, and a tone.
Reference: [52] <author> D. Lunney and R. C. Morrison. </author> <title> High technology laboratory aids for visually handicapped chemistry students. </title> <journal> Journal of Chemical Education, </journal> <volume> 58(3) </volume> <pages> 228-231, </pages> <year> 1981. </year>
Reference-contexts: These techniques are effective, but are bulky and cumbersome to use. Lunney <ref> [52] </ref> developed an elaborate system for presenting infrared spectra data using sound. Each spectrum is presented using three different techniques in sequence. In the first technique, the pitch of a tone is proportional to the frequency location of the infrared peak it represents.
Reference: [53] <author> Douglas L. Mansur, Meera M. Blattner, and Kenneth L. Joy. Sound-Graphs: </author> <title> A numerical data analysis method for the blind. </title> <type> Technical Report UCRL-53548, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> California 94550, </address> <year> 1984. </year>
Reference-contexts: In an informal experiment, they found that identical matches were reliably made from a set of spectra produced by twelve organic compounds. A more common way to represent data is with two-dimensional x-y graphs. Mansur, Blattner, and Joy <ref> [53] </ref> developed an auditory representation of x-y graphs called Sound-Graphs. The x-y data can be obtained from actual data or from a mathematical function. The x values are linearly mapped into a time interval from 0 to 3 seconds. The y values are scaled into frequency (pitch) values.
Reference: [54] <author> Doug Griffith. </author> <title> Computer access for persons who are blind or visually impaired: </title> <booktitle> Human factors issues. Human Factors, </booktitle> <volume> 32(4) </volume> <pages> 467-475, </pages> <year> 1990. </year>
Reference-contexts: Earlier screen readers which rely on textual output, therefore, cannot be used with GUIs. Nonvisual mechanisms 28 are needed to enable GUIs to be used by the visually impaired or blind. Two examples of nonvisual GUIs are outSPOKEN and inTOUCH <ref> [54] </ref> which provide auditory and haptic interfaces to the Apple Macintosh respectively. The first program, outSPOKEN allows the user to read the icons and menus of the Macintosh by means of synthetic speech. inTOUCH allows the Optacon tactile display to be used with the Macintosh.
Reference: [55] <author> Elizabeth D. Mynatt and W. Keith Edwards. </author> <title> Mapping GUIs to auditory interfaces. </title> <booktitle> In UIST'92, </booktitle> <pages> pages 61-70. </pages> <publisher> ACM Press, </publisher> <month> November 15-18 </month> <year> 1992. </year>
Reference-contexts: The Optacon generates tactile images of the icons and windows enabling the user to feel them. Another example of an auditory GUI intended for visually impaired or blind users is Mercator, a system developed by Mynatt and Edwards <ref> [55] </ref> to run in an X Windows environment. The auditory interface is constructed from the graphical interface in a transparent manner. The Mercator architecture retrieves information about the underlying graphical user interface of the application while the application is running.
Reference: [56] <author> Aaron Marcus and Andries van Dam. </author> <title> User-interface developments for the nineties. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 49-57, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: To manage this inherent complexity, the trend in user interface development has been to use successive levels of abstraction by layering the code that makes up the user interface and applications <ref> [56, 57] </ref>. There are several layers on top of a base graphics library provided by the underlying window system, as shown in Figure 8. The bottom layer contains the base graphics library (e.g., XLib, Microsoft Windows).
Reference: [57] <author> Emden R. Gansner and John H. Reppy. </author> <title> A foundation for user interface construction. </title> <editor> In Brad A. Myers, editor, </editor> <booktitle> Languages for User Interfaces, </booktitle> <pages> pages 239-259. </pages> <publisher> Jones and Bartlett, </publisher> <year> 1992. </year> <month> 52 </month>
Reference-contexts: To manage this inherent complexity, the trend in user interface development has been to use successive levels of abstraction by layering the code that makes up the user interface and applications <ref> [56, 57] </ref>. There are several layers on top of a base graphics library provided by the underlying window system, as shown in Figure 8. The bottom layer contains the base graphics library (e.g., XLib, Microsoft Windows).
References-found: 57

