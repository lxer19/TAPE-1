URL: http://www.iro.umontreal.ca/~feeley/papers/pscw92.ps.gz
Refering-URL: http://www.iro.umontreal.ca/~feeley/
Root-URL: http://www.iro.umontreal.ca
Email: feeley@iro.umontreal.ca  
Title: A Message Passing Implementation of Lazy Task Creation  
Author: Marc Feeley 
Address: Montreal, Quebec, CANADA  
Affiliation: Dept. d'informatique et de recherche operationnelle Universite de Montreal  
Abstract: This paper describes an implementation technique for Mul-tilisp's future construct aimed at large shared-memory multiprocessors. The technique is a variant of lazy task creation. The original implementation of lazy task creation described in [ Mohr, 1991 ] relies on efficient shared memory to distribute tasks between processors. In contrast, we propose a task distribution method based on a message passing paradigm. Its main advantages are that it is simpler to implement, has a lower cost for locally run tasks, and allows full caching of the stack on cache incoherent machines. Benchmarks on a 32 processor BBN TC2000 show that our method is more efficient than the original implementation by as much as a factor of 2. 
Abstract-found: 1
Intro-found: 1
Reference: [ Appel, 1989 ] <author> A. W. Appel. </author> <title> Allocation without locking. </title> <journal> Software Practice and Experience, </journal> <volume> 19(7) </volume> <pages> 703-705, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The method used by Gambit is to detect interrupts via polling and to never poll for interrupts inside the continuation pop sequence. There are other methods that have no direct overhead. For example, in the instruction interpretation method <ref> [ Appel, 1989 ] </ref> the interrupt handler checks to see if the interrupted instruction is in an "uninterruptible" section (i.e. a continuation pop sequence). If it is, the rest of the section is interpreted by the interrupt handler before the interrupt is serviced.
Reference: [ Bilardi and Nicolau, 1989 ] <author> G. Bilardi and A. Nicolau. </author> <title> Adaptive bitonic sorting: An optimal parallel algorithm for shared-memory machines. </title> <journal> SIAM Journal of Computing, </journal> <volume> 12(2) </volume> <pages> 216-228, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The bend in the curve is consequently more pronounced for programs which spend a high proportion of their time accessing the heap (e.g. abisort, allpairs, mm, and tridiag). * abisort Sort 16384 integers using the adaptive bitonic sort algorithm <ref> [ Bilardi and Nicolau, 1989 ] </ref> . * allpairs Compute the shortest path between all pairs of 117 nodes using a parallel version of Floyd's algorithm. * mm Multiply two matrices of integers (50 by 50). * scan Compute the parallel prefix sum of a vector of 32768 integers (in place).
Reference: [ Feeley and Miller, 1990 ] <author> M. Feeley and J. S. Miller. </author> <title> A parallel virtual machine for efficient Scheme compilation. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The measurement of S, H, and C was done by running the programs on a cache-incoherent NUMA shared-memory multiprocessor (the BBN butterfly TC2000) with the Gambit system <ref> [ Feeley and Miller, 1990 ] </ref> . All these programs were run on a single processor. The run time of each program was measured in three different settings. The first run was with the stack and heap located in non-cached local memory.
Reference: [ Feeley, 1993a ] <author> M. Feeley. </author> <title> An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Brandeis University Department of Computer Science, </institution> <year> 1993. </year> <note> Available as publication #869 from departe-ment d'informatique et recherche operationnelle de l'Universite de Montreal. </note>
Reference-contexts: The lock operations, which are expensive on some machines, can be avoided by using "software lock" algorithms such as [ Peterson, 1981 ] . Algorithms specially tailored for LTC are described in [ Mohr, 1991 ] and <ref> [ Feeley, 1993a ] </ref> . In addition, TAIL can be cached in a register since it is only mutated by the LTQ's owner. Another trick is to indicate that a continuation is no longer available by clearing the corresponding entry in the LTQ. <p> The relative importance of suboptimally caching the stack will thus decrease as the programs spend more and more time being idle and/or accessing remote memory. A more detailed analysis of the SM and MP protocols, including experiments on a 90 processor BBN GP1000, can be found in <ref> [ Feeley, 1993a ] </ref> . 7 Conclusion We have proposed a message-passing protocol for implementing lazy task creation. The performance of this protocol was compared with the original protocol which relies on an efficient shared memory.
Reference: [ Feeley, 1993b ] <author> M. Feeley. </author> <title> Polling efficiently on stock hardware. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: If it is, the rest of the section is interpreted by the interrupt handler before the interrupt is serviced. Other zero cost techniques are described in <ref> [ Feeley, 1993b ] </ref> . Thirdly, it is no longer necessary to save the future's return address on the LTQ. Instead, the return address can be found in the first stack frame above the stolen continuation 3 .
Reference: [ Goldman and Gabriel, 1988 ] <author> R. Goldman and R. P. Gabriel. </author> <title> Preliminary results with the initial implementation of Qlisp. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 143-152, </pages> <address> Snowbird, UT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Other systems have an even higher cost (both portable standard Lisp on the BBN butterfly GP1000 [ Swanson et al., 1988 ] and QLisp on an Alliant FX/8 <ref> [ Goldman and Gabriel, 1988 ] </ref> take roughly 1400 instructions). Due to this high cost, the overhead of exposing parallelism (i.e. of adding futures to a sequential program) will depend strongly on the task granularity.
Reference: [ Halstead, 1984 ] <author> R. Halstead. </author> <title> Implementation of Multilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 9-17, </pages> <address> Austin, TX, </address> <month> August </month> <year> 1984. </year>
Reference: [ Halstead, 1985 ] <author> R. Halstead. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> In ACM Trans. on Prog. Languages and Systems, </journal> <pages> pages 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Published in: Halstead, R., Ito T. (editors), ``Parallel Symbolic Computing: Languages, Systems, and Applications'', Springer-Verlag Lecture Notes in Computer Science 748, November, 1993, pp. 94-107. Multilisp <ref> [ Halstead, 1985 ] </ref> extends the Scheme [ IEEE Std 1178-1990, 1991 ] programming language with a simple and elegant parallel programming paradigm. Parallelism is specified explicitly in the program by the use of the future special form.
Reference: [ IEEE Std 1178-1990, 1991 ] <editor> IEEE Std 1178-1990. </editor> <title> IEEE Standard for the Scheme Programming Language. </title> <publisher> Institute of Electrical and Electronic Engineers, Inc., </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Published in: Halstead, R., Ito T. (editors), ``Parallel Symbolic Computing: Languages, Systems, and Applications'', Springer-Verlag Lecture Notes in Computer Science 748, November, 1993, pp. 94-107. Multilisp [ Halstead, 1985 ] extends the Scheme <ref> [ IEEE Std 1178-1990, 1991 ] </ref> programming language with a simple and elegant parallel programming paradigm. Parallelism is specified explicitly in the program by the use of the future special form. The expression (future expr) is called a future and expr is its body. Parallelism follows a tasking model.
Reference: [ Kranz et al., 1989 ] <author> D. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In ACM SIGPLAN '89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Transferring the tasks suspended on the placeholder to the work queue. The total cost of these operations can easily be in the hundreds of machine instructions. The performance of existing implementations of ETC seem to confirm this. The Mul-T system was carefully designed to minimize the cost of ETC <ref> [ Kranz et al., 1989 ] </ref> and it takes roughly 130 machine instructions per task on the Encore Multimax (the actual cost depends on the number of closed variables, their location etc.). <p> The performance of fine grain programs will be poor due to the relatively small proportion of the total time spent doing useful work. 2 Lazy Task Creation Lazy task creation (LTC) is an alternative implementation of futures proposed in <ref> [ Kranz et al., 1989 ] </ref> and subsequently implemented and studied by Eric Mohr on an Encore Multimax [ Mohr, 1991 ] .
Reference: [ Miller, 1988 ] <author> J. S. Miller. </author> <title> Implementing a Scheme-based parallel processing system. </title> <journal> International Journal of Parallel Processing, </journal> <volume> 17(5), </volume> <month> October </month> <year> 1988. </year>
Reference: [ Mohr, 1991 ] <author> E. Mohr. </author> <title> Dynamic Partitioning of Parallel Lisp Programs. </title> <type> PhD thesis, </type> <institution> Yale University Department of Computer Science, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: be poor due to the relatively small proportion of the total time spent doing useful work. 2 Lazy Task Creation Lazy task creation (LTC) is an alternative implementation of futures proposed in [ Kranz et al., 1989 ] and subsequently implemented and studied by Eric Mohr on an Encore Multimax <ref> [ Mohr, 1991 ] </ref> . The implementation of LTC described in this section is essentially that used by Mohr and will be called the shared-memory (SM) protocol because it assumes the existence of a global shared memory. <p> The lock operations, which are expensive on some machines, can be avoided by using "software lock" algorithms such as [ Peterson, 1981 ] . Algorithms specially tailored for LTC are described in <ref> [ Mohr, 1991 ] </ref> and [ Feeley, 1993a ] . In addition, TAIL can be cached in a register since it is only mutated by the LTQ's owner. Another trick is to indicate that a continuation is no longer available by clearing the corresponding entry in the LTQ.
Reference: [ Peterson, 1981 ] <author> G. L. Peterson. </author> <title> Myths about the mutual exclusion problem. </title> <journal> Information Processing Letters, </journal> <volume> 12(3) </volume> <pages> 115-116, </pages> <year> 1981. </year>
Reference-contexts: The lock operations, which are expensive on some machines, can be avoided by using "software lock" algorithms such as <ref> [ Peterson, 1981 ] </ref> . Algorithms specially tailored for LTC are described in [ Mohr, 1991 ] and [ Feeley, 1993a ] . In addition, TAIL can be cached in a register since it is only mutated by the LTQ's owner.
Reference: [ Steinberg et al., 1986 ] <author> S. Steinberg, D. Allen, L. Bagnall, and C. Scott. </author> <title> The Butterfly Lisp system. </title> <booktitle> In Proc. 1986 AAAI, volume 2, </booktitle> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference: [ Swanson et al., 1988 ] <author> M. Swanson, R. Kessler, and G. Lindstrom. </author> <title> An implementation of portable standard Lisp on the BBN Butterfly. </title> <booktitle> In Conference Record of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 132-141, </pages> <address> Snowbird, UT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Other systems have an even higher cost (both portable standard Lisp on the BBN butterfly GP1000 <ref> [ Swanson et al., 1988 ] </ref> and QLisp on an Alliant FX/8 [ Goldman and Gabriel, 1988 ] take roughly 1400 instructions). Due to this high cost, the overhead of exposing parallelism (i.e. of adding futures to a sequential program) will depend strongly on the task granularity.
Reference: [ Zorn et al., 1988 ] <author> B. Zorn, P. Hilfinger, K. Ho, J. Larus, and L. Semenzato. </author> <title> Features for multiprocessing in SPUR Lisp. </title> <type> Technical Report Report UCB/CSD 88/406, </type> <institution> University of California, Computer Science Division (EECS), </institution> <month> March </month> <year> 1988. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
References-found: 16

