URL: ftp://ftp.cs.rochester.edu/pub/u/jag/iastedARC96.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Title: Visual Servoing using Trust Region Methods and Estimation of the Full Coupled Visual-Motor Jacobian  
Author: Martin Jagersand 
Keyword: Robot control, Adaptive control, Real time computer vision.  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Note: In Proc. of IASTED Applications of Control and Robotics,  
Email: jag@cs.rochester.edu  
Date: 1996, p105-108  
Web: http://www.cs.rochester.edu/u/jag  
Abstract: We present a novel algorithm for visual servoing, capable of learning the robot kinematics and camera calibration. The approach differs from previous work in that a full coupled Jacobian is estimated online without prior models, and that a trust region method is used, improving stability and convergence of the controller. We present experimental results on the positioning accuracy and convergence of this controller showing an up to 5 fold improvement in repeatability on PUMA 761 and 762 robots and successful estimation of the Jacobian for 3, 6 and 12 controlled DOF with highly non-linear transfer functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> Dynamic Sensor Based Control of Robots with Visual Feedback J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 8] </ref>, but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable [9]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> So far we have only considered the static problem. In a real robot, unless moved in a look-and-move <ref> [1] </ref> fashion, we also need to consider how to control velocities and acceleration. With the above approach we can control velocities and accelerations by placing additional constraints on ff k .
Reference: [2] <author> Feddema J. T. Lee G. C. S. </author> <title> Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> This is not appropriate in most manipulation settings, where the calibration movements would interfere with the task. Partial modeling of the viewing geometry using an ARMAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 5] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. We instead seek an online method, which estimates the Jacobian by just observing the process, without introducing any extra calibration movements. <p> are called inbaddning [19] or Homotopy methods [17], and are used to improve the convergence range on difficult problems for a variety of numerical methods. 3.2 Implementation Issues Achieving dynamic stability together with reasonably fast performance on typical robot arms using visual servoing directly on the joint motors is hard <ref> [2] </ref>. We solve this problem by using the standard joint feedback controllers as an inner, fast control loop. The visual controller then controls an idealized, nicer robot.
Reference: [3] <author> Conkie A. Chongstitvatana P. </author> <title> An Uncalibrated Stereo Visual Servo System DAITR#475, </title> <address> Edinburgh 1990 </address>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [3, 16, 13] </ref>) is y fl y = KJ x (5) where K is a gain matrix.
Reference: [4] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> Eye-to-Hand Coordination for vision guided Robot Control Applications Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6].
Reference: [5] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> This is not appropriate in most manipulation settings, where the calibration movements would interfere with the task. Partial modeling of the viewing geometry using an ARMAX model and estimating only one or a few parameters (e.g. depth) has also been tried <ref> [2, 5] </ref>. This however restricts the camera-robot configurations and environments to structured, easy to model settings. We instead seek an online method, which estimates the Jacobian by just observing the process, without introducing any extra calibration movements.
Reference: [6] <author> Harris M. </author> <title> Vision Guided Part Alignment with Degraded Data DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [16, 7, 6] </ref>. Up to now work has concentrated on the low level servoing capable of obtaining a single visual alignment, over a relatively smooth and uncomplex region of the visual-motor transfer function.
Reference: [7] <editor> Hollinghurst N. Cipolla R. Uncalibrated Stereo Hand-Eye Co ordination Brit. </editor> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [16, 7, 6] </ref>. Up to now work has concentrated on the low level servoing capable of obtaining a single visual alignment, over a relatively smooth and uncomplex region of the visual-motor transfer function.
Reference: [8] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 8] </ref>, but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable [9]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> Experimentally we have been able to solve some difficult manipulation problems with this technique, such as control in 12 DOF of a non rigid link <ref> [8] </ref>. In many of these problems we found that, without the visual space planning, the control would terminate in a local min imum, or in a singularity. <p> These experiments are described in more detail in our technical report <ref> [8] </ref>. On a PUMA 761 we found that repeatability is 35 % better under visual servo control than under standard joint control. On a worn PUMA 762, with significant backlash, we got a repeatability improvement of 5 times with the visual control. The Utah/MIT dextrous hand has 16 controllable DOF's.
Reference: [9] <author> Jagersand M. </author> <title> Perception level control for uncalibrated hand eye coordination and motor actions Thesis proposal, </title> <institution> University of Rochester, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In this paper we present three main contributions: 1. A trust region method is used to give convergence for difficult visual-motor transfer functions. fl Supported by Sverige-Amerika Stiftelsen and Fulbright. y Also Chalmers, 412 96 Goteborg, Sweden 1 For a review of this work we direct the reader to <ref> [9] </ref> or [12]. 2 Typically one, the distance between robot and camera along the cameras optical axis, the visual depth. 2. <p> Visual features can be drawn from a large class of visual measurements [1, 8], but we have found that the ones which can be represented as points positions or point vectors in camera space are suitable <ref> [9] </ref>. We track features such as boundary discontinuities (lines,corners) and surface markings. Redundant visual perceptions (m n) are desirable as they are used to constrain the raw visual sensory information. <p> Positioning accuracy increased up to 4 times with m = 16 compared to m = 4. We have tried using the visual servoing in solving several complex, real world tasks, such as playing checkers, setting a table, solving a kids puzzle and changing a light bulb <ref> [9] </ref>. Visual space programming is different from conventional robot programming in that commands are given in image space rather than world space. This makes user friendly programmer interfaces easy to implement.
Reference: [10] <author> Kutulakos K. Jagersand M. </author> <title> Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination Workshop on vision for robots In conjunction with IROS 1995. </title>
Reference: [11] <author> Jagersand M. Nelson R. </author> <title> Visual Space Task Specification, </title> <booktitle> Planning and Control In Proc on IEEE Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> More importantly, working in image feature space allows us to do image space trajectory planning. We will briefly describe this here, but as it ties in more with the vision part of a visual servoing system than the robot control we direct the reader to <ref> [11] </ref>, in which we describe a method for high level visual space task specification, planning, and trajectory generation. In visual servoing, the task goals are given in image space. When solving complete tasks the image information is also used to plan trajectories (eg. to avoid obstacles). <p> The bootstrapping works because the visual feature motions are correlated <ref> [11] </ref>. Redundant visual measures are beneficial, as they reduce errors due to tracking and visual goal specification. In a 3 DOF positioning task we tried using between m = 4 and 16 6 Electronic m-peg videos of the demonstrations in this section are accessible through the Internet WWW. <p> This makes user friendly programmer interfaces easy to implement. We have tried having the robot operator: (1) Draw visual sketches of the desired movements. (2) Point out objects and alignments in video images. (3) Show an image sequence depicting the task (see <ref> [11] </ref>). In fig.2 the PUMA robot solves a kid's puzzle under visual control. The operator points in an image, using the computer mouse, directing which piece goes where. The program decomposes this into transportation, alignment and insertion movements, and plans trajectories in visual space (white lines in fig.).
Reference: [12] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Con trol PhD thesis U of Melbourne 1994. </title>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> A trust region method is used to give convergence for difficult visual-motor transfer functions. fl Supported by Sverige-Amerika Stiftelsen and Fulbright. y Also Chalmers, 412 96 Goteborg, Sweden 1 For a review of this work we direct the reader to [9] or <ref> [12] </ref>. 2 Typically one, the distance between robot and camera along the cameras optical axis, the visual depth. 2. <p> The model is valid around the current system configuration x k , and described by the image <ref> [12] </ref> or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as is exploited in visual feedback control but also highly constrains the possible visual changes to the subspace y k+1 &lt; m of y k+1 = <p> In normal operation smooth movements are achieved by continuously keeping the setpoint somewhat ahead of the robot, and thus leading it around like if it was on a rubber leach. A better account of the dynamics issues in this type of visual control can be found in <ref> [12] </ref>. 4 Experiments 6 We have evaluated our visual servoing controller by: (1) Testing the repeatability and convergence of positioning. (2) Using it as a component in solving several complex manipulation tasks. These experiments are described in more detail in our technical report [8].
Reference: [13] <author> Hosoda K. Asada M. </author> <title> Versatile Visual Servoing without Knowledge of True Jacobian Proc. </title> <booktitle> IROS 1994. </booktitle>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6]. <p> We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [3, 16, 13] </ref>) is y fl y = KJ x (5) where K is a gain matrix.
Reference: [14] <author> Fuentes O. Nelson R. </author> <title> Morphing hands and virtual tools TR# 551, </title> <institution> Dept of CS, U. of Rochester 1994. </institution>
Reference-contexts: The Utah/MIT dextrous hand has 16 controllable DOF's. The four fingers form a parallel kinematic chain when grasping an object. Fine manipulation of an object in the hand is much more difficult than with a robot arm <ref> [14] </ref>. Manipulating a rigid object in 6 DOF using the visual servo control we note a 73 % improvement in repeatability compared to Cartesian space joint feedback control. We have evaluated the model estimation in 3, 6 and 12 controlled DOF.
Reference: [15] <author> B. H. Yoshimi P. K. </author> <title> Allen Active, </title> <booktitle> Uncalibrated Visual Ser voing ARPA IUW, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction Using visual input to control robot manipulators allows more flexible and robust robot behaviors than traditional position based control. Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. <ref> [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] </ref> 1 . Visual models suitable for specifying simple visual alignments have also been studied [16, 7, 6].
Reference: [16] <author> Hager G. </author> <title> Calibration-Free Visual Control Using Projective Invariance In Proc. </title> <booktitle> of 5:th ICCV 1995. </booktitle>
Reference-contexts: Recently uncalibrated visual servo-ing control has been demonstrated in a variety of settings e.g. [1, 3, 2, 4, 5, 8, 11, 12, 13, 15] 1 . Visual models suitable for specifying simple visual alignments have also been studied <ref> [16, 7, 6] </ref>. Up to now work has concentrated on the low level servoing capable of obtaining a single visual alignment, over a relatively smooth and uncomplex region of the visual-motor transfer function. <p> We need a control system capable of turning these goal perceptions into motor actions; in our system joint movements. A simple control law, in some form occuring in most visual servoing research (e.g. <ref> [3, 16, 13] </ref>) is y fl y = KJ x (5) where K is a gain matrix.
Reference: [17] <author> Garcia, </author> <title> Zangwill Pathways to solutions, fixed points, </title> <editor> and equi libria, </editor> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: In many of these problems we found that, without the visual space planning, the control would terminate in a local min imum, or in a singularity. In numerical analysis techniques like this one are called inbaddning [19] or Homotopy methods <ref> [17] </ref>, and are used to improve the convergence range on difficult problems for a variety of numerical methods. 3.2 Implementation Issues Achieving dynamic stability together with reasonably fast performance on typical robot arms using visual servoing directly on the joint motors is hard [2].
Reference: [18] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, </title> <editor> sec ond ed. </editor> <year> 1987 </year>
Reference-contexts: In this basis the correction term in eq. 4 is zero except for the first column. Thus 4 Updating formulas of rank 1 and 2 are most common. The motivation of popular rank 2 formulas, such as the Davidson, Fletcher, Powell (DFP) <ref> [18] </ref> is that they preserve symmetric (X i = ~ i ~ T i ) positive definiteness of J, and thus guarantees that the search direction Dx of a quasi-Newton type controller is a descent direction for . <p> not a viable solution, as moves may be over a large part of the robot workspace, and using a single, experimentally found ff is likely to be inefficient, as well as require a lot of trials to find that ff. 3.1.1 Trust region We adopt a trust region method 5 <ref> [20, 18] </ref> which adapts the maximum step length ff automatically by taking as long steps as is possible or desirable, while maintaining convergence. <p> accumulation point x 1 satisfying r (x 1 ) = 0 and x T r 2 (x 1 )x 0; 8x 2 &lt; n is guaranteed for any function , rather than only for a quadratic function, as in the case of the unmodified quasi-Newton method in eq. 6 (See <ref> [18] </ref>). For the quasi-Newton method to work we need the starting point x 0 to be much closer to the goal x fl , than for the trust region method. The trust region method is efficient.
Reference: [19] <author> Gustafsson I. </author> <note> Till ampad Optimeringsl ara Komp., </note> <institution> Inst. for Inf. Beh., </institution> <note> Chalmers 1991. </note>
Reference-contexts: In many of these problems we found that, without the visual space planning, the control would terminate in a local min imum, or in a singularity. In numerical analysis techniques like this one are called inbaddning <ref> [19] </ref> or Homotopy methods [17], and are used to improve the convergence range on difficult problems for a variety of numerical methods. 3.2 Implementation Issues Achieving dynamic stability together with reasonably fast performance on typical robot arms using visual servoing directly on the joint motors is hard [2]. <p> The robot controller uses these to predict how to move to achieve new goals. We have showed how to improve a standard, Newton-type visual servoing algorithm. We use a trust region method to achieve convergence for difficult transfer functions, and inbaddning <ref> [19] </ref> or homotopy methods to transform a positioning task on a non-convex domain of the transfer function to a series of smaller tasks, each on a smaller convex domain.
Reference: [20] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, Pren tice Hall, </title> <type> 199x, preprint. </type>
Reference-contexts: Even for a convex problem ( convex) it is not guaranteed to be convergent <ref> [20] </ref>. There is also the problem of selecting an ff so that the method converges, and does it reasonably quickly. Previous work has overcome these problems by only making a single, small distance move within a relatively smooth and well scaled region of f . <p> not a viable solution, as moves may be over a large part of the robot workspace, and using a single, experimentally found ff is likely to be inefficient, as well as require a lot of trials to find that ff. 3.1.1 Trust region We adopt a trust region method 5 <ref> [20, 18] </ref> which adapts the maximum step length ff automatically by taking as long steps as is possible or desirable, while maintaining convergence. <p> However the values of the parameters affect how precise the controller follows the trajectory, and they are selected differently for coarse transportation and fine manipulation moves, trading accuracy for time. Typical suitable values are d lower = 0:1 and d upper = 0:7 <ref> [20] </ref>. The trust region method has very strong convergence properties.
References-found: 20

