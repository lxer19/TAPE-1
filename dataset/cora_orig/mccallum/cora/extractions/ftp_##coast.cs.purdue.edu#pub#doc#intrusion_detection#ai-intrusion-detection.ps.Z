URL: ftp://coast.cs.purdue.edu/pub/doc/intrusion_detection/ai-intrusion-detection.ps.Z
Refering-URL: http://www.cs.purdue.edu/coast/archive/data/categ24.html
Root-URL: http://www.cs.purdue.edu
Title: Artificial Intelligence and Intrusion Detection: Current and Future Directions  
Author: Jeremy Frank 
Keyword: Artificial Intelligence, Intrusion Detection, Feature Selection.  
Note: ARPA DOD/DABT63-93-C-0045  
Address: Davis, CA. 95616  
Affiliation: Division of Computer Science University of California at Davis  
Email: frank@cs.ucdavis.edu  
Phone: (916) 752-2149  
Date: June 9, 1994  
Abstract: Intrusion Detection systems (IDSs) have previously been built by hand. These systems have difficulty successfully classifying intruders, and require a significant amount of computational overhead making it difficult to create robust real-time IDS systems. Artificial Intelligence techniques can reduce the human effort required to build these systems and can improve their performance. Learning and induction are used to improve the performance of search problems, while clustering has been used for data analysis and reduction. AI has recently been used in Intrusion Detection (ID) for anomaly detection, data reduction and induction, or discovery, of rules explaining audit data. We survey uses of artificial intelligence methods in ID, and present an example using feature selection to improve the classification of network connections. The network connection classification problem is related to ID since intruders can create "private" communications services undetectable by normal means. We also explore some areas where AI techniques may further improve IDSs. 
Abstract-found: 1
Intro-found: 1
Reference: [Ba] <author> R. Baldwin. "Kuang: </author> <title> Rule-Based Security Checking." COPS documentation, </title> <booktitle> MIT,Lab For Computer Science Programming Systems Research Group, </booktitle> <year> 1989. </year>
Reference: [BuCa] <author> W. Buntine, R. Caruna. </author> <title> "Introduction to IND and Recursive Partitioning." </title> <institution> IND Documentation, NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: Neural networks are characterized by highly connected networks which are trained on a set of data in the hopes that the network will correctly classify future examples [WeKu]. Another example of a classifier is a Decision Tree <ref> [BuCa] </ref> [WeKu]. Decision trees are constructed by finding ways to separate the data into two or more groups. We then separate each of these groups in turn, until we have small groups of examples left. <p> Since our task is to correctly classify future examples based on the training examples, we used the classification error rate of a decision tree to evaluate each set of features <ref> [BuCa] </ref> [WeKu]. This error rate is computed by counting the misclassifications on a test data set which is independent from the training set [BuCa]. We tested representative algorithms from each of 3 broad classes of search algorithms. Backward sequential search begins with the full set of features. <p> is to correctly classify future examples based on the training examples, we used the classification error rate of a decision tree to evaluate each set of features <ref> [BuCa] </ref> [WeKu]. This error rate is computed by counting the misclassifications on a test data set which is independent from the training set [BuCa]. We tested representative algorithms from each of 3 broad classes of search algorithms. Backward sequential search begins with the full set of features. At each stage of the search, each feature in the remaining set is removed.
Reference: [ChKe] <author> P. Cheeseman, J. Kelley, M. Self, J. Stutz, W. Taylor, D. Freeman. </author> <title> "Autoclass: A Bayesian Classification System. </title> <booktitle> Proceedings of the 5th International Conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: We plan to use clustering techniques such as Autoclass <ref> [ChKe] </ref> to explore patterns in audit and network data. One way clustering can be used in ID is by giving an overview of complex data. Another is by considering each cluster in turn and analyzing interesting characteristics of each cluster.
Reference: [De] <author> D. Denning. </author> <title> "An Intrusion Detection Model." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol SE-13, </volume> <pages> no.2, </pages> <year> 1987. </year>
Reference-contexts: Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] <ref> [De] </ref> [DeBe] [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account.
Reference: [DeBe] <author> H. Debar, M Becker, D. Siboni. </author> <title> "A Neural Network Component for an Intrusion Detection System." </title> <booktitle> Proceedings, IEEE Symposium on Research in Computer Security and Privacy, </booktitle> <year> 1992. </year>
Reference-contexts: In systems such as DIDS [SnBr], MIDAS [SeSh], TIM [TeCh] and NSM [HeDi], data filtering is done using heuristic or ad hoc methods, which can be viewed as expert rules for filtering. Other systems filter data in a more adaptive or dynamic way. <ref> [DeBe] </ref> present a filtering system based on a neural network which acts to filter data which does not fit an observed trend. They assume that user activity contains notable trends that can be detected, and that there are correlations among the collected audit data. <p> IDES rules are encoded in an expert system shell. As information is gathered, the expert determines whether or not any rules have been satisfied, then chooses the most appropriate rule to select [LuJa]. <ref> [DeBe] </ref> propose an expert system in connection with a neural network. The neural network component reports anomalies to the expert system, which also employs data not used by the net. The expert contains a rule base similar to that used in IDES, with known attacks and system policy information. <p> Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] <ref> [DeBe] </ref> [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account.
Reference: [Do] <author> J. Doak. </author> <title> "Intrusion Detection: The Application of Feature Selection, A Comparison of Algorithms, and the Application of a Wide Area Network Analyzer." </title> <type> PhD. Thesis, </type> <institution> Department of Computer Science, University of California, Davis, </institution> <year> 1992 </year>
Reference-contexts: Thus, feature selection can be used to find features most indicative of misuse, or can be used to distinguish between types of misuse. <ref> [Do] </ref> and [SiSk] have performed comparisons of a variety of 3 feature selection techniques, and [Do] tested several techniques on simulated computer attack data to explore the possibility of using feature selection to improve intrusion detection techniques. <p> Thus, feature selection can be used to find features most indicative of misuse, or can be used to distinguish between types of misuse. <ref> [Do] </ref> and [SiSk] have performed comparisons of a variety of 3 feature selection techniques, and [Do] tested several techniques on simulated computer attack data to explore the possibility of using feature selection to improve intrusion detection techniques. <p> At each stage of the search, each feature in the remaining set is removed. The best feature to eliminate from the set is determined by comparing the error rates of the classifiers created using the resulting feature sets. Backward Sequential Search runs in polynomial time <ref> [Do] </ref>. Beam search is a type of best-first search which uses a bounded queue to limit the scope of the search. The queue is ordered from best-state to worst-state, with the best state placed in the front of the queue. <p> If there is no limit on the length of the queue, then Beam Search takes exponential time to complete but if the queue length is 1 then Beam Search takes polynomial time. Thus accuracy can be increased if the queue length is increased <ref> [Do] </ref>. In Random Generation Plus Sequential Selection, we perform several sequential selections from different places in the search space. As mentioned, the goal is to avoid picking the first good feature set seen on the assumption that other good feature sets are also available. <p> To do so, we generate a random feature set, then perform backward and forward sequential selection on the state. Random Generation runs in polynomial time but is more expensive than Backwards Sequential Selection <ref> [Do] </ref>. 5.3 Algorithm Performance The table below shows the size, classification error rates, and number of states searched for each problem. Note that with 7 features the total number of subsets of these features is 128.
Reference: [He] <author> T. Heberlein. </author> <title> "Haystack's Analysis: A Brief Description." Internal Document, </title> <institution> University of California, Davis, </institution> <year> 1991. </year>
Reference-contexts: Along with the choice of number of percentile points, the positions of the percentiles can also be located. For instance, [La] uses 2 percentiles and splits the categories at the 50th percentile. The Bernoulli vector used in Haystack uses x = 2 and splits at the 90th percentile. <ref> [He] </ref> Wisdom&Sense [LiVa] also performs clustering of numerical data. <p> It also provides the network with contextual inputs that audit data does not provide, and ensures that the network does not train on intrusive behavior. Other IDSs employing expert systems are Haystack <ref> [He] </ref>, AudES [Ts], MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] [DeBe] [LiVa]. <p> Windows extend across logins so that information across login sessions can be maintained in the profiles. PRAD provides a means for including changing legitimate user behavior in profiles after legitimacy is ascertained. Other systems employing statistical anomaly detection are MIDAS [SeSh] , IDES [JaVa], NADIR [JaDu] and Haystack <ref> [He] </ref>. 4.3 Rule-Based Induction In contrast to expert systems, rule-based systems automatically develop rules to explain the historical data they collect. Rules are modified over the lifetime of a system in order to keep the rule set accurate and manageable.
Reference: [He2] <author> T. Heberlein, </author> <title> Private Communication, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: On UNIX systems a connection is characterized by the source port and destination port numbers. Certain ports are reserved for different services; e.g. telnet uses port 23. However, an intruder can hide network connections by strategically placing the servers that receive the connections on different ports <ref> [He2] </ref>. The mapping of ports to services is internal to a single machine; an intruder could also change the port map. Thus we would like to be able to identify the type of connection made without referring to port numbers.
Reference: [HeDi] <author> L. Heberlein, G. Dias, K. Levitt, B. Mukherjee, J Wood, D. Wolber. </author> <title> "A Network Security Monitor." </title> <booktitle> Proceedings, IEEE Symposium on Research in Computer Security and Privacy, </booktitle> <year> 1990. </year>
Reference-contexts: This has the advantage of decreasing storage requirements and reducing processing time. However, filtering may throw out useful data, and so must be done carefully. In systems such as DIDS [SnBr], MIDAS [SeSh], TIM [TeCh] and NSM <ref> [HeDi] </ref>, data filtering is done using heuristic or ad hoc methods, which can be viewed as expert rules for filtering. <p> Our data consisted of 15,947 connections from one local area network during one week of normal use. 5.1 NSM Features We collected information about network connections using NSM <ref> [HeDi] </ref>. NSM returns data about each connection.
Reference: [JaDu] <author> K. Jackson, D. DuBois, C. Stallings. </author> <title> "An Expert System Application for Network Intrusion Detection." </title> <year> 1991 </year>
Reference-contexts: It also provides the network with contextual inputs that audit data does not provide, and ensures that the network does not train on intrusive behavior. Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS [SeSh] and NADIR <ref> [JaDu] </ref>. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] [DeBe] [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account. <p> Windows extend across logins so that information across login sessions can be maintained in the profiles. PRAD provides a means for including changing legitimate user behavior in profiles after legitimacy is ascertained. Other systems employing statistical anomaly detection are MIDAS [SeSh] , IDES [JaVa], NADIR <ref> [JaDu] </ref> and Haystack [He]. 4.3 Rule-Based Induction In contrast to expert systems, rule-based systems automatically develop rules to explain the historical data they collect. Rules are modified over the lifetime of a system in order to keep the rule set accurate and manageable.
Reference: [JaVa] <author> H. Javitz, A. Valdes. </author> <title> "The SRI IDES Statistical Anomaly Detector." </title> <booktitle> Proceedings, IEEE Symposium on Research in Computer Security and Privacy, </booktitle> <year> 1991. </year>
Reference-contexts: Windows extend across logins so that information across login sessions can be maintained in the profiles. PRAD provides a means for including changing legitimate user behavior in profiles after legitimacy is ascertained. Other systems employing statistical anomaly detection are MIDAS [SeSh] , IDES <ref> [JaVa] </ref>, NADIR [JaDu] and Haystack [He]. 4.3 Rule-Based Induction In contrast to expert systems, rule-based systems automatically develop rules to explain the historical data they collect. Rules are modified over the lifetime of a system in order to keep the rule set accurate and manageable.
Reference: [LaBe] <author> L. Lankewicz, M. Benard. </author> <title> "Real-time Anomaly Detection Using a Non-Parametric Pattern Recognition Approach." </title> <booktitle> Proceedings 7th Annual Computer Security Applications Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Exemplar methods build a representative of each cluster throughout the clustering process. Distance clustering uses a distance measure to establish membership in a cluster. Conceptual clustering requires that an object meet necessary and sufficient conditions for cluster membership. PRAD <ref> [LaBe] </ref> uses k-nearest neighbor (knn) clustering to reduce data. To perform knn clustering, x percentiles of the distribution are determined. The data is reduced to one of the values 1 to x. Thus each of n data elements is clustered with k = n x+1 neighboring data points.
Reference: [La] <author> L. Lankewicz. </author> <title> "A Non-Parametric Pattern Recognition to Anomaly Detection." </title> <type> PhD. Thesis, </type> <institution> Dept. of Computer Science, Tulane University, </institution> <year> 1992. </year>
Reference-contexts: The data is reduced to one of the values 1 to x. Thus each of n data elements is clustered with k = n x+1 neighboring data points. Along with the choice of number of percentile points, the positions of the percentiles can also be located. For instance, <ref> [La] </ref> uses 2 percentiles and splits the categories at the 50th percentile. The Bernoulli vector used in Haystack uses x = 2 and splits at the 90th percentile. [He] Wisdom&Sense [LiVa] also performs clustering of numerical data. <p> Anomaly detection can be difficult since the concept of normal can change over time. Furthermore, normalcy can be established with respect to different time frames. For example, a system can establish session, daily and weekly trends. PRAD <ref> [La] </ref> learns profiles of resource usage, time information, and directory access patterns. Profiles are analyzed with respect to login sessions and time windows, and performance in windows is weighted over time. Windows extend across logins so that information across login sessions can be maintained in the profiles.
Reference: [LiVa] <author> G. Liepens, H. Vacaro. </author> <title> "Anomaly Detection: Purpose and Framework." </title> <booktitle> Proceedings, 12th National Computer Security Conference, </booktitle> <year> 1989. </year>
Reference-contexts: For instance, [La] uses 2 percentiles and splits the categories at the 50th percentile. The Bernoulli vector used in Haystack uses x = 2 and splits at the 90th percentile. [He] Wisdom&Sense <ref> [LiVa] </ref> also performs clustering of numerical data. <p> Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] [DeBe] <ref> [LiVa] </ref>. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account. <p> All rules can either be used to signal anomalies, or the most appropriate rules to use may be determined <ref> [LiVa] </ref>. TIM's rules remain in the rule base only if they are highly predictive or confirmed by many observations. Prediction is calculated using an entropy model. The user must specify the behavior TIM is trying to predict.
Reference: [LuJa] <author> T. Lunt, R. Jaganathan, R. Lee, A. Whitehurst, S. Listgarten. </author> <title> "Knowledge-Based Intrusion Detection." </title> <booktitle> Proceedings of the AI Systems in Government Conference, </booktitle> <year> 1989. </year>
Reference-contexts: IDES rules are encoded in an expert system shell. As information is gathered, the expert determines whether or not any rules have been satisfied, then chooses the most appropriate rule to select <ref> [LuJa] </ref>. [DeBe] propose an expert system in connection with a neural network. The neural network component reports anomalies to the expert system, which also employs data not used by the net. The expert contains a rule base similar to that used in IDES, with known attacks and system policy information.
Reference: [LuTa] <author> T. Lunt, A. Tamaru, F. Gilham, R. Jaganathan, P. Neuman, C. Jalali. "IDES: </author> <title> A Progress Report." </title> <booktitle> Proceedings of the Sixth Annual Computer Security Applications Conference, </booktitle> <year> 1990. </year>
Reference-contexts: Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use <ref> [LuTa] </ref> [De] [DeBe] [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account.
Reference: [Ma] <author> R. Maxion, </author> <title> "Anomaly Detection for Diagnosis." </title> <address> FCTS, </address> <year> 1990 </year>
Reference-contexts: An anomaly detection system contains three distinct phases: 1. Abstract local information 2. Evolve background information from local abstractions 3. Establish anomaly background boundaries. <ref> [Ma] </ref> discusses smoothing raw data to eliminate reliance on outlying data points, blending data using an exponential method to weight historical data higher than current data, and finding and blending the variation in behavior to establish a tolerance level for network anomaly detection.
Reference: [RiSc] <author> C. Riesbeck, R. Schank. </author> <title> Inside Case-Based Reasoning. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference: [SeSh] <author> M. Sebring, E. Shellhouse, M. Hanna, R. Whitehurst. </author> <title> "Expert Systems in Intrusion Detection: A Case Study." </title> <booktitle> Proceedings of the 11th National Computer Security Conference, </booktitle> <year> 1988. </year>
Reference-contexts: Some data may not be useful to the IDS and thus can be eliminated before processing. This has the advantage of decreasing storage requirements and reducing processing time. However, filtering may throw out useful data, and so must be done carefully. In systems such as DIDS [SnBr], MIDAS <ref> [SeSh] </ref>, TIM [TeCh] and NSM [HeDi], data filtering is done using heuristic or ad hoc methods, which can be viewed as expert rules for filtering. <p> It also provides the network with contextual inputs that audit data does not provide, and ensures that the network does not train on intrusive behavior. Other IDSs employing expert systems are Haystack [He], AudES [Ts], MIDAS <ref> [SeSh] </ref> and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] [DeBe] [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account. <p> Windows extend across logins so that information across login sessions can be maintained in the profiles. PRAD provides a means for including changing legitimate user behavior in profiles after legitimacy is ascertained. Other systems employing statistical anomaly detection are MIDAS <ref> [SeSh] </ref> , IDES [JaVa], NADIR [JaDu] and Haystack [He]. 4.3 Rule-Based Induction In contrast to expert systems, rule-based systems automatically develop rules to explain the historical data they collect. Rules are modified over the lifetime of a system in order to keep the rule set accurate and manageable.
Reference: [ShDi] <author> J. Shavlik, T. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kauffman, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: The search process itself 2 is the subject of continuing research in the AI community. Humans are also able to generalize or abstract from large amounts of information by a process called discovery or clustering. Data clustering techniques are used to group data together according to some criteria <ref> [ShDi] </ref>. Clustering is used to discover hidden patterns in data that humans might miss. 3 Data Reduction for Intrusion Detection Due to the massive amount of audit data available, classification by hand is impossible.
Reference: [SiSk] <author> W. Siedlecki, J. Sklansky. </author> <title> "On Automatic Feature Selection." </title> <journal> International Journal of Artificial Intelligence, vol.2, no.2, </journal> <year> 1988. </year>
Reference-contexts: Further, some features may be redundant since the information they add is contained in other features. Extra features can increase computation time, and can impact the accuracy of an IDS. Feature selection improves classification by searching for the subset of features which best classifies the training data <ref> [SiSk] </ref> . In the ID domain, features are derived from information sources used to detect intrusions, and training instances are derived from detected intrusion attempts as well as normal behavior. <p> Thus, feature selection can be used to find features most indicative of misuse, or can be used to distinguish between types of misuse. [Do] and <ref> [SiSk] </ref> have performed comparisons of a variety of 3 feature selection techniques, and [Do] tested several techniques on simulated computer attack data to explore the possibility of using feature selection to improve intrusion detection techniques.
Reference: [SnBr] <author> S. Snapp, J. Bretano, G. Dias, T. Goan, L. Heberlein, C. Ho, K. Levitt, B. Mukherjee, S. Smaha, T. Grance, D. Teal, D. Mansur. "DIDS: </author> <title> Motivation, Architecture and an Early Prototype." </title> <booktitle> Proceedings of the 14th National Computer Security Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Some data may not be useful to the IDS and thus can be eliminated before processing. This has the advantage of decreasing storage requirements and reducing processing time. However, filtering may throw out useful data, and so must be done carefully. In systems such as DIDS <ref> [SnBr] </ref>, MIDAS [SeSh], TIM [TeCh] and NSM [HeDi], data filtering is done using heuristic or ad hoc methods, which can be viewed as expert rules for filtering.
Reference: [TeCh] <author> H. Teng, K. Chen, S. Lu. </author> <title> "Adaptive Real-Time Anomaly Detection Using Inductively Generated Sequential Patterns." </title> <booktitle> Proceedings, IEEE Symposium on Research in Computer Security and Privacy, </booktitle> <year> 1990. </year>
Reference-contexts: This has the advantage of decreasing storage requirements and reducing processing time. However, filtering may throw out useful data, and so must be done carefully. In systems such as DIDS [SnBr], MIDAS [SeSh], TIM <ref> [TeCh] </ref> and NSM [HeDi], data filtering is done using heuristic or ad hoc methods, which can be viewed as expert rules for filtering. <p> Both short term and long term patterns are checked against for anomalies. Rules also support instantiation. TIM allows the user to enter rules describing either patterns or abstractions of the audit data. <ref> [TeCh] </ref> 5 An Experiment Using Feature Selection A growing problem in intrusion detection is network-based intrusion detection. Since computer systems are increasingly network dependent it is imperative to protect both local and regional networks.
Reference: [Th] <author> C. Thornton, </author> <title> "A Computational Model for the Data Compression Metaphor." </title> <booktitle> Proceedings of the 3d International Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Artificial Intelligence researchers have noted the close relationship between learning and data compression. Discovering the generalization of a concept is in essence finding a more compact representation of the set of objects, and a hierarchical clustering algorithm can be used for inductive generalization <ref> [Th] </ref>. Statistical clustering measures the probability that each example is in a given cluster. Exemplar methods build a representative of each cluster throughout the clustering process. Distance clustering uses a distance measure to establish membership in a cluster.
Reference: [Ts] <author> G. Tsudik. </author> <title> "AudES An Expert System for Security Auditing." </title> <booktitle> `Proceedings of AAAI Conference on Innovative Applications in AI, </booktitle> <year> 1988 </year>
Reference-contexts: It also provides the network with contextual inputs that audit data does not provide, and ensures that the network does not train on intrusive behavior. Other IDSs employing expert systems are Haystack [He], AudES <ref> [Ts] </ref>, MIDAS [SeSh] and NADIR [JaDu]. 4.2 Anomaly Detection Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use [LuTa] [De] [DeBe] [LiVa]. In many cases this is a valid assumption, as in the attacker who breaks into a legitimate user's account.
Reference: [WeKu] <author> S. Weiss, </author> <title> C Kulikowski. Computer Systems That Learn. </title> <publisher> Morgan Kauffman, </publisher> <address> California, </address> <year> 1991. </year>
Reference-contexts: Humans excel at tasks such as learning, or gaining the ability to perform tasks from examples and training. An expert system handles problems using a computer model of expert human reasoning. However, most expert systems must undergo continuous maintenance to perform well <ref> [WeKu] </ref>. Other systems can acquire knowledge from a set of training instances. These training instances can be questions and correct answer pairs, or problems and the steps of a solution. Rule Based Induction derives rules which explain the training instances more clearly than a mathematical or statistical analysis of data [WeKu]. <p> <ref> [WeKu] </ref>. Other systems can acquire knowledge from a set of training instances. These training instances can be questions and correct answer pairs, or problems and the steps of a solution. Rule Based Induction derives rules which explain the training instances more clearly than a mathematical or statistical analysis of data [WeKu]. Classifier systems attempt to learn how to classify future examples from a set of training data. An example of a system that can be used as a classifier is a Neural Network, which uses a model of biological systems to perform classification. <p> Neural networks are characterized by highly connected networks which are trained on a set of data in the hopes that the network will correctly classify future examples <ref> [WeKu] </ref>. Another example of a classifier is a Decision Tree [BuCa] [WeKu]. Decision trees are constructed by finding ways to separate the data into two or more groups. We then separate each of these groups in turn, until we have small groups of examples left. <p> Neural networks are characterized by highly connected networks which are trained on a set of data in the hopes that the network will correctly classify future examples <ref> [WeKu] </ref>. Another example of a classifier is a Decision Tree [BuCa] [WeKu]. Decision trees are constructed by finding ways to separate the data into two or more groups. We then separate each of these groups in turn, until we have small groups of examples left. <p> Figure 1 shows how a tree used to classify network connections is constructed. The goal of Feature Selection is to reduce the amount of information required to make good predictions, and to improve the error rate of classifiers <ref> [WeKu] </ref>. This is accomplished by searching subsets of features, or information sources, and testing the ability of those features to classify the training instances. The search process itself 2 is the subject of continuing research in the AI community. <p> Since our task is to correctly classify future examples based on the training examples, we used the classification error rate of a decision tree to evaluate each set of features [BuCa] <ref> [WeKu] </ref>. This error rate is computed by counting the misclassifications on a test data set which is independent from the training set [BuCa]. We tested representative algorithms from each of 3 broad classes of search algorithms. Backward sequential search begins with the full set of features.
References-found: 26

