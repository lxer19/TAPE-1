URL: http://www.eecs.umich.edu/PPP/ieeetps93.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Approaching a Machine-Application Bound in Delivered Performance on Scientific Code 1  
Author: W. Mangione-Smith T. P. Shih S. G. Abraham E. S. Davidson 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: We have developed a performance bounding methodology that explains the performance of loop-dominated scientific applications on particular systems. We model the throughput of key hardware units that are common bottlenecks in concurrent machines. The four units currently used are: memory interface, floating-point, instruction issue, and a "dependence unit" which is used to model the effects of performance-limiting recurrences. We propose a workload characterization, and derive upper bounds on the performance of specific machine-workload pairs. Comparing delivered performance with bounds focuses attention on areas for improvement and indicates how much improvement might be attainable. A detailed analysis and performance improvement effort for the IBM RS/6000, using the Livermore Fortran Kernels 1-12 to represent the target workload, produces a lower bound of average 1.27 clocks per floating-point operation (CPF), whereas machine peak performance is 0.5 CPF and the V2.01 Fortran compiler attains only 2.43 CPF. Code improvements in this study have achieved 1.36 CPF, increasing the harmonic mean steady-state inner loop performance to 97.6% of the MFLOPS bound. Subsequently the V2.02 compiler achieved 1.75 CPF, and 1.60 with carefully chosen preprocessing. A goal-directed 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Windheiser and W. Jalby, </author> <title> "Behavioral Characterization of Decoupled Access/Execute Architectures," </title> <booktitle> in 1991 ACM International Conference on Supercomputing, </booktitle> <pages> pp. 28-39, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: An effective performance evaluation technique can provide insight for each of these groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. <ref> [1, 2] </ref>), the timing of large applications (e.g. [3, 4]), or analytic performance models (e.g. [5]). <p> Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 <ref> [1] </ref> fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 floating-point store with address updated automatically). 14 the codes on RS/6000 Model 730 with AIX 3.1.5 Operating <p> We used Goblin to disassemble the compiled code and interleaved these instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 <ref> [1] </ref> fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4,
Reference: [2] <author> J. S. Emer and D. W. Clark, </author> <title> "A Characterization of Processor Performance in the VAX-11/780," </title> <booktitle> in Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 301-309, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: An effective performance evaluation technique can provide insight for each of these groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. <ref> [1, 2] </ref>), the timing of large applications (e.g. [3, 4]), or analytic performance models (e.g. [5]). <p> Thus we only consider dependent FPU arithmetic instructions. 13 a. Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n b. Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) <ref> [2] </ref> fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 c. Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] <p> <ref> [2] </ref> fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 c. Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 floating-point store with address updated automatically). 14 the codes on RS/6000 Model 730 with AIX 3.1.5 Operating System and V2.01 compiler. shows the number of
Reference: [3] <author> J. P. Singh, W. Dietrich-Webber, and A. Gupta, </author> <title> "Splash: Stanford Parallel Applications for Shared-Memory," </title> <type> Tech. Rep. 596, </type> <institution> Stanford, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: An effective performance evaluation technique can provide insight for each of these groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [1, 2]), the timing of large applications (e.g. <ref> [3, 4] </ref>), or analytic performance models (e.g. [5]). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance. <p> Thus we only consider dependent FPU arithmetic instructions. 13 a. Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n b. Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 <ref> [3] </ref> fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 c. <p> Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 <ref> [3] </ref> fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 floating-point store with address updated automatically). 14 the codes on RS/6000 Model 730 with AIX 3.1.5 Operating System and V2.01 compiler. shows the number of such RAW stall cycles per iteration <p> We used Goblin to disassemble the compiled code and interleaved these instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 <ref> [3] </ref> fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved
Reference: [4] <author> M. Berry, D. Chen, P. Koss, and D. Kuck, </author> <title> "The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers," </title> <type> CSRD Report 827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> 36 November </month> <year> 1988. </year>
Reference-contexts: An effective performance evaluation technique can provide insight for each of these groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [1, 2]), the timing of large applications (e.g. <ref> [3, 4] </ref>), or analytic performance models (e.g. [5]). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance.
Reference: [5] <author> T. N. Mudge and H. B. Al-Sadoun, </author> <title> "A Semi-Markov Model for the Performance of Multiple-Bus Systems," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 34, </volume> <pages> pp. 934-942, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [1, 2]), the timing of large applications (e.g. [3, 4]), or analytic performance models (e.g. <ref> [5] </ref>). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance. <p> Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n b. Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 <ref> [5] </ref> fp5 = R stfdu fp0, X (K) [7] bc CL.2 c. <p> Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 <ref> [5] </ref> fp1 = Q stfdu fp2, X (K) [6] bc CL.2 floating-point store with address updated automatically). 14 the codes on RS/6000 Model 730 with AIX 3.1.5 Operating System and V2.01 compiler. shows the number of such RAW stall cycles per iteration for the compiled code. <p> instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 <ref> [5] </ref> fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved with 1U, 3U, 5U from the next even iteration to form the execution sequence 1, 1U, 3,
Reference: [6] <author> J. E. Smith, </author> <title> "Decoupled Access/Execute Architecture Computer Architectures," </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pp. 289-308, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Two logical processes are operating in this loop: the Access process moves data between the CPU and the memory, while the Execute process operates on that data <ref> [6] </ref>. While this example code is vectorizable because of the independence between the iterations of the inner loop, non-vectorizable code may also exhibit such access-execute parallelism. The typical inner product method of writing a matrix multiplication iterates over K in the inner loop. <p> Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) <ref> [6] </ref> bc CL.2 floating-point store with address updated automatically). 14 the codes on RS/6000 Model 730 with AIX 3.1.5 Operating System and V2.01 compiler. shows the number of such RAW stall cycles per iteration for the compiled code. <p> fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) <ref> [6] </ref> fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved with 1U, 3U, 5U from the next even iteration to form the execution sequence 1, 1U, 3, 3U, 5, 5U, thereby eliminating the two RAW
Reference: [7] <author> W. H. Mangione-Smith, S. G. Abraham, and E. S. Davidson, </author> <title> "Architectural vs. Delivered Performance of the IBM RS/6000 and the Astronautics ZS-1," </title> <booktitle> in Proc. Twenty-Fourth Hawaii International Conference on System Sciences, </booktitle> <pages> pp. 397-408, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Section 3 addresses this issue by considering the relationship between operation latencies and register requirements. 2 Bounding Optimum Performance - IBM RS/6000 This section develops a method of finding an upper bound on optimum performance for a scientific application on a highly concurrent processor. <ref> [7, 8] </ref> The methodology is illustrated 6 by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 (figure 1) on a set of scientific code kernels. <p> Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) <ref> [7] </ref> bc CL.2 c.
Reference: [8] <author> W. H. Mangione-Smith, S. G. Abraham, and E. S. Davidson, </author> <title> "A Performance Comparison of the IBM RS/6000 and the Astronautics ZS-1," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 39-46, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Section 3 addresses this issue by considering the relationship between operation latencies and register requirements. 2 Bounding Optimum Performance - IBM RS/6000 This section develops a method of finding an upper bound on optimum performance for a scientific application on a highly concurrent processor. <ref> [7, 8] </ref> The methodology is illustrated 6 by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 (figure 1) on a set of scientific code kernels.
Reference: [9] <author> R. R. Oehler and R. D. Groves, </author> <title> "IBM RISC System/6000 Processor Architecture," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 23-36, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: These operations can be executed at best at the rate of one operation per clock in the RS/6000 Floating-Point Unit (FPU) <ref> [9] </ref>. In our workload, These are combinable multiply-add triad operations (f ma ), uncombinable adds (f a ), and uncombinable multiplies (f m ). Add and multiply operators that appear in the loop body are checked to see if they are combinable into one of the RS/6000's three-operand multiply-add instructions.
Reference: [10] <author> G. F. Grohoski, </author> <title> "Machine Organization of the IBM RISC System/6000 Processor," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 37-58, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Thus essential stores (see t m ) are counted here as well. * t m for the memory port activity, which requires at least one clock per essential floating-point load (l fl ) and floating-point store (s fl ) <ref> [10] </ref>. Counting only essential loads and stores requires inter-iteration dependence analysis. For m iterations of the inner loop, the number of distinct array elements that appear on the left hand side of assignment statements will generally be of the form am + b.
Reference: [11] <author> H. Zima and B. Chapman, </author> <title> Supercompilers for Parallel and Vector Computers. </title> <address> New York: </address> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [21], loop interchange <ref> [11] </ref> (see matrix multiplication example in section 1.5) and cyclic reduction [12] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload.
Reference: [12] <author> H. S. Stone, </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison Wesley, 3 ed., </publisher> <year> 1993. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [21], loop interchange [11] (see matrix multiplication example in section 1.5) and cyclic reduction <ref> [12] </ref> (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload.
Reference: [13] <author> F. H. McMahon, </author> <title> "The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range," </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> December </month> <year> 1986. </year> <month> 37 </month>
Reference-contexts: The goal of this research and this case study is to evaluate performance relative to this simple bound, to find mechanisms for approaching the bound performance, and to explain the gaps between achievable and bound performance when they do occur. The first 12 Livermore Fortran Kernels <ref> [13] </ref> are used as the application workload for the experimental study in this paper. Table 1 lists the essential workload and the bound for LFK 1-12.
Reference: [14] <author> H. S. Warren Jr., </author> <title> "Predicting Execution Time on the IBM RISC RS/6000," </title> <type> Tech. Rep. </type> <institution> IBM Research Report RC 16994(#75050), IBM, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: By analyzing the compiled code, we have identified four major compiler-related issues that depressed the delivered performance: redundant loads and stores, read after write (RAW) dependence stalls, floating-point store vs. write-back conflicts <ref> [14] </ref>, and a fine grain code scheduling problem. These issues are discussed in this section. Some model and measurement refinement issues are deferred to the subsequent section. 2.1.1 Remove redundant loads/stores Redundant memory loads occur when data that are reused in later iterations are not saved in register files. <p> the stalls caused by RAW dependence among FPU arithmetic instructions. 2 In RS/6000, the second instruction of two consecutive dependent FPU arithmetic instructions will typically be stalled for one clock before entering the FPU pipeline (FPU pipeline latency for multiply-add is usually 2, sometimes more when data exceptions are possible) <ref> [14] </ref>. These stall clocks are added to the time, t f , required in the FPU.
Reference: [15] <author> C. Stephens, B. Cogswell, J. Heinlein, G. Palmer, and J. P. Shen, </author> <title> "Instruction Level Profiling and Evaluation of the IBM RS/6000," </title> <booktitle> in International Symp. on Computer Architecture, </booktitle> <pages> pp. 180-189, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The loop is unrolled twice and different registers are used for odd and even iterations. Redundant stores are simply removed and final stores are inserted after the loop. Since there is no official disassembler for the RS/6000, nor an executable assembly code output from the compiler, we used Goblin <ref> [15] </ref> to disassemble the compiled code, and performed the necessary modifications on that code. To illustrate the first method, consider the following example.
Reference: [16] <author> C. Eisenbeis, W. Jalby, and A. </author> <title> Lichnewsky, "Squeezing More CPU Performance Out of a Cray-2 by Vector Block Scheduling," </title> <booktitle> in Proc. of Supercomputing '88, </booktitle> <pages> pp. 237-246, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: However, for loops such as LFK7 and 8, which have more floating-point arithmetic instructions than essential memory operations, eliminating all redundant memory operations does not guarantee higher performance. Moreover, when register reallocation is difficult, necessitating a large degree of unrolling or register moves through the FPU pipeline <ref> [16] </ref>, the burden on the FPU can increase. Consequently, only one memory reference was removed in LFK7, and three in LFK8. All the redundant memory operations were removed in the other loops. <p> All jobs use the same schedule with the origin shifted in time by a constant amount for successive jobs. Rau [17] and Hsu [27] used the reservation table approach to develop CS. More recent work is found in <ref> [16, 19, 28] </ref>. In a CS schedule, all loop iterations have identical code schedules. A new iteration begins every iteration interval (II) clocks, often before some instructions from prior iterations have been issued.
Reference: [17] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard, "Efficient Code Generation for Horizontal Architectures: </title> <booktitle> Compiler Techniques and Architectural Support," in Proc. Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 131-139, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [17, 18, 19] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, 15 except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop. <p> Registers requirements are considered only for optimum performance schedules. 26 3.1 Cyclic Scheduling Compilers that use cyclic instruction scheduling (CS) <ref> [17] </ref> algorithms for loop code are able to generate code schedules that reach optimum performance for a range of programs 3 . <p> All jobs use the same schedule with the origin shifted in time by a constant amount for successive jobs. Rau <ref> [17] </ref> and Hsu [27] used the reservation table approach to develop CS. More recent work is found in [16, 19, 28]. In a CS schedule, all loop iterations have identical code schedules. <p> Memory load operations reserve the fixed-point unit, and store operations reserve both the floating-point unit and the fixed-point unit. Each 3 Cyclic scheduling is one form of software pipelining <ref> [17, 18] </ref>. The terms Cyclic Scheduling, Polycyclic Scheduling, and Software Pipelining are often used to describe similar techniques. 27 instruction is defined by a resource reservation template and a latency (or several latencies to different classes of readers of its result).
Reference: [18] <author> R. F. Touzeau, </author> <title> "A Fortran Compiler for the FPS-164 Scientific Computer," </title> <booktitle> in Proc. ACM SIGPLAN '84 Symp. on Compiler Construction, </booktitle> <pages> pp. 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [17, 18, 19] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, 15 except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop. <p> Memory load operations reserve the fixed-point unit, and store operations reserve both the floating-point unit and the fixed-point unit. Each 3 Cyclic scheduling is one form of software pipelining <ref> [17, 18] </ref>. The terms Cyclic Scheduling, Polycyclic Scheduling, and Software Pipelining are often used to describe similar techniques. 27 instruction is defined by a resource reservation template and a latency (or several latencies to different classes of readers of its result).
Reference: [19] <author> J. Tang, E. S. Davidson, and J. Tong, </author> <title> "Polycyclic Vector Scheduling vs. Chaining on 1-Port Vector Supercomputers," </title> <booktitle> in Proc. of Supercomputing '88, </booktitle> <pages> pp. 122-129, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [17, 18, 19] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, 15 except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop. <p> All jobs use the same schedule with the origin shifted in time by a constant amount for successive jobs. Rau [17] and Hsu [27] used the reservation table approach to develop CS. More recent work is found in <ref> [16, 19, 28] </ref>. In a CS schedule, all loop iterations have identical code schedules. A new iteration begins every iteration interval (II) clocks, often before some instructions from prior iterations have been issued.
Reference: [20] <author> C. B. Hall and K. O'Brien, </author> <title> "Performance Characteristics of Architectural Features of the IBM RS/6000," </title> <booktitle> in ASPLOS, </booktitle> <pages> pp. 303-309, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: and iteration count overhead, ignored instruction types, certain dependencies, and hardware interlocks are expected to have little or no effect on the performance of scientific inner loops due to the success of the RS/6000 branch handling and other hardware design features in masking their effects through buffering and concurrent execution <ref> [20] </ref>. However, there is some outer loop overhead to compute the loop limit and load it into the count register before the loop begins and to execute other code, if any, that is not part of an inner loop body.
Reference: [21] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> "Experimentally Characterizing the Behavior of Multiprocessor Memory Systems: A Case Study," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 16, </volume> <pages> pp. 216-223, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality <ref> [21] </ref>, loop interchange [11] (see matrix multiplication example in section 1.5) and cyclic reduction [12] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. <p> In this section, we present a simple methodology that can be developed further to predict real memory performance. This method uses the idea of load/store kernels to learn about the cache structure and calibrate its performance <ref> [21] </ref>. The basic program used here is: REAL*8 X (ARRAY SIZE) : TS = MCLOCK () DO 1 L = 1, LOOPS ( LOAD/STORE KERNEL ) 1 CONTINUE TE = MCLOCK () In this study, we ran the kernel many times to get steady state performance. <p> the cache characteristics. average time required to load an 8-byte data word depends on the array size and stride. 23 When ARRAY SIZE is less than 8K, the average access rate is one clock cycle per data element, and we conclude that all references are in cache (the cache region <ref> [21] </ref>). The RS/6000 Model 730 being measured in fact has a 64 Kbyte cache (= 8K*8 bytes). However, when ARRAY SIZE &gt; 8K, some misses occur (beginning of the transition region [21]). <p> is one clock cycle per data element, and we conclude that all references are in cache (the cache region <ref> [21] </ref>). The RS/6000 Model 730 being measured in fact has a 64 Kbyte cache (= 8K*8 bytes). However, when ARRAY SIZE &gt; 8K, some misses occur (beginning of the transition region [21]). On cache misses, assume initially that the referenced line will replace the least recently used line in the set. If ARRAY SIZE exceeds the cache size by less than the size of one full cache column (one line of each associative set), some data is never replaced. <p> In this region, since the effective access time grows linearly with ARRAY SIZE, we conclude that the initial assumption is correct. Since the access time and hence the miss rate, levels off at ARRAY SIZE 10K (the memory region <ref> [21] </ref>), we can conclude that a column is 2K * 8 bytes and that the cache must be 4-way set associative, as documented [22]. The figure also shows that the average miss penalty depends on STRIDE.
Reference: [22] <author> W. R. Hardell, D. A. Hicks, L. C. Howell Jr., W. E. Maule, R. Montoye, and D. P. </author> <title> 38 Tuttle, "Data Cache and Storage Control Units," </title> <booktitle> in IBM RISC System/6000 Techno--logy, </booktitle> <pages> pp. 44-50, </pages> <institution> IBM Corporation, </institution> <year> 1990. </year>
Reference-contexts: Since the access time and hence the miss rate, levels off at ARRAY SIZE 10K (the memory region [21]), we can conclude that a column is 2K * 8 bytes and that the cache must be 4-way set associative, as documented <ref> [22] </ref>. The figure also shows that the average miss penalty depends on STRIDE. Strides 16 and 32 have the same performance, implying that the cache line size must be 16 * 8 = 128 bytes, as documented for the Model 730.
Reference: [23] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: We will assume that CS is used to obtain optimum performance code schedules with the minimum register requirements of any such schedule, even though this problem is known to be intractable <ref> [23, 24] </ref>. In order to find an optimum performance schedule with the minimum register requirements, exhaustive search techniques were used [25]. CS is derived from Patel's work on the design of hardware pipelines [26]. <p> The RS/6000 has depth-independent templates for the instructions modeled here under normal operating conditions. 4 The problem of packing instruction templates from a kernel into an MRT of a given size is identical to the bin-packing problem, which is known to be NP-complete <ref> [23] </ref>. Since the RS/6000 is modeled as a DIT machine, the instruction templates do not need to be modified to model a machine with longer pipelines, and so there exists one bin packing problem for each kernel to be scheduled, independent of the pipeline depths. 3.4 Register Requirements vs.
Reference: [24] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1987. </year>
Reference-contexts: We will assume that CS is used to obtain optimum performance code schedules with the minimum register requirements of any such schedule, even though this problem is known to be intractable <ref> [23, 24] </ref>. In order to find an optimum performance schedule with the minimum register requirements, exhaustive search techniques were used [25]. CS is derived from Patel's work on the design of hardware pipelines [26].
Reference: [25] <author> W. H. Mangione-Smith, </author> <title> Performance Bounds and Buffer Space Requirements for Concurrent Processors. </title> <type> PhD thesis, </type> <institution> EECS Dept., Univ. of Mich., </institution> <address> Ann Arbor, MI, </address> <year> 1992. </year>
Reference-contexts: In order to find an optimum performance schedule with the minimum register requirements, exhaustive search techniques were used <ref> [25] </ref>. CS is derived from Patel's work on the design of hardware pipelines [26]. A pipeline is modeled as a set of segments and a "job" (here, one entire loop iteration) for the pipeline is modeled as a reservation table showing the segment usage (schedule) in time. <p> The estimate is exact when s u (modM II) = 1, and the actual value is monotonically nondecreasing in s u . Easily calculated upper and lower bounds on the register requirement serve to bound the error of this estimate <ref> [25, 29] </ref>. Two important observations can be drawn from equation 3.2. First, increasing either latency or instruction issue bandwidth tends to increase register requirements. Increasing the pipeline depth for a function unit will increase the lifetime of each register that the function unit writes to.
Reference: [26] <author> J. H. Patel and E. S. Davidson, </author> <title> "Improving the Throughput of a Pipeline by Insertion of Delays," </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 159-164, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: In order to find an optimum performance schedule with the minimum register requirements, exhaustive search techniques were used [25]. CS is derived from Patel's work on the design of hardware pipelines <ref> [26] </ref>. A pipeline is modeled as a set of segments and a "job" (here, one entire loop iteration) for the pipeline is modeled as a reservation table showing the segment usage (schedule) in time.
Reference: [27] <author> P. Y. Hsu, </author> <title> Highly Concurrent Scalar Processing. </title> <type> PhD thesis, </type> <institution> Coordinated Science Laboratory Report #CSG-49, University of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference-contexts: All jobs use the same schedule with the origin shifted in time by a constant amount for successive jobs. Rau [17] and Hsu <ref> [27] </ref> used the reservation table approach to develop CS. More recent work is found in [16, 19, 28]. In a CS schedule, all loop iterations have identical code schedules. A new iteration begins every iteration interval (II) clocks, often before some instructions from prior iterations have been issued. <p> More recent work is found in [16, 19, 28]. In a CS schedule, all loop iterations have identical code schedules. A new iteration begins every iteration interval (II) clocks, often before some instructions from prior iterations have been issued. A modulo reservation table (MRT) <ref> [27] </ref> with II rows keeps track of the resource requirements of all concurrently executing loops. For example, an RS/6000 MRT (at a level of detail that is similar to the performance bound) contains three columns modeling the shared resources: fixed-point unit, floating-point unit, and branch unit.
Reference: [28] <author> W. H. Mangione-Smith, S. G. Abraham, and E. S. Davidson, </author> <title> "Vector Register Design for Polycyclic Vector Scheduling," </title> <booktitle> in Proc. Fourth Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 154-163, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: All jobs use the same schedule with the origin shifted in time by a constant amount for successive jobs. Rau [17] and Hsu [27] used the reservation table approach to develop CS. More recent work is found in <ref> [16, 19, 28] </ref>. In a CS schedule, all loop iterations have identical code schedules. A new iteration begins every iteration interval (II) clocks, often before some instructions from prior iterations have been issued.
Reference: [29] <author> W. Mangione-Smith, S. G. Abraham, and E. S. Davidson, </author> <title> "Register Requirements of Pipelined Processors," </title> <booktitle> in Proc. International Conference on Supercomputing, </booktitle> <pages> pp. 260 - 271, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The epilogue code simply writes the result of the last iteration to memory. 3.2 Register Requirements for Cyclic Schedules We briefly review some of our previously published results <ref> [29] </ref>. <p> The estimate is exact when s u (modM II) = 1, and the actual value is monotonically nondecreasing in s u . Easily calculated upper and lower bounds on the register requirement serve to bound the error of this estimate <ref> [25, 29] </ref>. Two important observations can be drawn from equation 3.2. First, increasing either latency or instruction issue bandwidth tends to increase register requirements. Increasing the pipeline depth for a function unit will increase the lifetime of each register that the function unit writes to.
Reference: [30] <author> B. R. Rau, M. S. Schlansker, and D. W. L. Yen, </author> <title> "The Cydra 5 Stride-Insensitive Memory System," </title> <booktitle> in Proc. of International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> I-242-246, </address> <month> August </month> <year> 1989. </year> <month> 39 </month>
Reference-contexts: The new code will stall for ffi F fl M II fewer cycles on cache misses. The Cydra 5 compilers used this technique <ref> [30] </ref>. 4 Concluding Remarks This work is based on the premise that the proper focus of performance evaluation studies for scientific computers is on optimum rather than expected performance. Optimum performance studies and metrics have earned a bad reputation because it is easy to derive trivial or unachievable performance bounds.
Reference: [31] <author> E. L. Boyd and E. S. Davidson, </author> <title> "Hierarchical Performance Modeling with MACS: A Case Study of the Convex C-240," </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year> <month> 40 </month>
Reference-contexts: Such a goal-directed compiler would use a problem-solving approach in its search for object code improvement. An enabling technology for such a compiler, is the MACS performance model <ref> [31] </ref>. MACS (Machine, Application, Compiler, and Schedule) bounds provide a rich hierarchy of bounds and metrics for possible use by the compiler, system designers, and application developers. The bound presented here models the machine implementation and the application as characterized by the essential operations in the high level code.
References-found: 31

