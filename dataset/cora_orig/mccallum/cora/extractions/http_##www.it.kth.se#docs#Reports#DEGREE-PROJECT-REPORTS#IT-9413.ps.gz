URL: http://www.it.kth.se/docs/Reports/DEGREE-PROJECT-REPORTS/IT-9413.ps.gz
Refering-URL: http://www.it.kth.se/docs/Reports/DEGREE-PROJECT-REPORTS/
Root-URL: http://www.it.kth.se
Title: A An Evaluation of the CMMD Library and an Efficient Parallel Implementation of the RTRL
Author: Ulf Johansson Gunnar Franzen 
Date: 940226  
Affiliation: Royal Institute of Technology Department of Teleinformatics  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ronald J. Williams and David Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computing, </journal> <volume> 1 </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Its disadvantage is that the computational complexity of the algorithm makes it impractical to use for training of networks with a large number of neurons. 3.3.2 Algorithm Definition This section presents an overview of the RTRL-algorithm definition. A complete definition can be found in <ref> [1] </ref>. Let the network have n units, with m external input lines. The n-tuple of unit outputs at time t are denoted by the vector y (t) and the m-tuple of external inputs by the vector x (t). <p> *pk, **pj, ***p, *pk_new, **pj_new, ***p_new; double *wj, **w; double *y; double *fd; double time; int i, j, k, l; char *datafile; FILE *fptr; /* Parse command line arguments */ if (argc != 3) - fprintf (stderr, "USAGE: %s &lt;num_nodes&gt; &lt;datafile&gt;"n", argv [0]); exit (1); - n = atoi (argv <ref> [1] </ref>); datafile = argv [2]; /* Allocate memory */ pk = (double *)malloc (n*n*n*sizeof (double)); pj = (double **)malloc (n*n*sizeof (double *)); p = (double ***)malloc (n*sizeof (double **)); pk_new = (double *)malloc (n*n*n*sizeof (double)); pj_new = (double **)malloc (n*n*sizeof (double *)); p_new = (double ***)malloc (n*sizeof (double **)); wj = <p> (); void compute_p (); void print_result (); /***** Function main () *****/ main (argc, argv) int argc; char **argv; - /* Parse command line arguments */ if (argc != 4) - CMMD_fset_io_mode (stderr, CMMD_sync_bc); fprintf (stderr, "USAGE: %s &lt;num_neurons&gt; &lt;num_proc.&gt; &lt;input_file&gt;"n", argv [0]); exit (1); - n = atoi (argv <ref> [1] </ref>); part_size = atoi (argv [2]); input_file = argv [3]; bin_file = argv [4]; /* Set global and local parameters */ CMMD_reset_partition_size (part_size); file_desc = CMMD_global_open (bin_file, O_RDWR | O_CREAT | O_TRUNC, 0666); my_address = CMMD_self_address (); ma = my_address; /* Detemine the number of virtual processors on this node */
Reference: [2] <author> Jenq-Neng Hwang and Sun-Yuan Kung. </author> <title> Parallel algorithms/ architectures for neural networks. </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 1 </volume> <pages> 221-251, </pages> <year> 1989. </year>
Reference-contexts: In a classical pipelined system only the results flow. 3.2 Mapping of algorithms on systolic arrays Because ANN algorithms are CPU bound 2 by nature, they should map well to systolic arrays. A method for mapping a whole range of ANN algorithms to systolic arrays is presented in <ref> [2] </ref>. In [3], an algorithm transformation called time-space mapping is proposed for mapping an algorithm in the nested loop form, e.g., the RTRL algorithm, to a systolic array. The time-space mapping method is based on the fact that the order of execution of loop body instances can be changed. <p> **pj_new, ***p_new; double *wj, **w; double *y; double *fd; double time; int i, j, k, l; char *datafile; FILE *fptr; /* Parse command line arguments */ if (argc != 3) - fprintf (stderr, "USAGE: %s &lt;num_nodes&gt; &lt;datafile&gt;"n", argv [0]); exit (1); - n = atoi (argv [1]); datafile = argv <ref> [2] </ref>; /* Allocate memory */ pk = (double *)malloc (n*n*n*sizeof (double)); pj = (double **)malloc (n*n*sizeof (double *)); p = (double ***)malloc (n*sizeof (double **)); pk_new = (double *)malloc (n*n*n*sizeof (double)); pj_new = (double **)malloc (n*n*sizeof (double *)); p_new = (double ***)malloc (n*sizeof (double **)); wj = (double *)malloc (n*n*sizeof (double)); <p> print_result (); /***** Function main () *****/ main (argc, argv) int argc; char **argv; - /* Parse command line arguments */ if (argc != 4) - CMMD_fset_io_mode (stderr, CMMD_sync_bc); fprintf (stderr, "USAGE: %s &lt;num_neurons&gt; &lt;num_proc.&gt; &lt;input_file&gt;"n", argv [0]); exit (1); - n = atoi (argv [1]); part_size = atoi (argv <ref> [2] </ref>); input_file = argv [3]; bin_file = argv [4]; /* Set global and local parameters */ CMMD_reset_partition_size (part_size); file_desc = CMMD_global_open (bin_file, O_RDWR | O_CREAT | O_TRUNC, 0666); my_address = CMMD_self_address (); ma = my_address; /* Detemine the number of virtual processors on this node */ if ( n &gt; part_size
Reference: [3] <author> Dan I. Moldovan and Jose A. B. Fortes. </author> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume C-35, No. 1 </volume> <pages> 1-12, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: A method for mapping a whole range of ANN algorithms to systolic arrays is presented in [2]. In <ref> [3] </ref>, an algorithm transformation called time-space mapping is proposed for mapping an algorithm in the nested loop form, e.g., the RTRL algorithm, to a systolic array. The time-space mapping method is based on the fact that the order of execution of loop body instances can be changed. <p> This technique is derived from the linear time-space mapping for systolic algorithm synthesis <ref> [3] </ref>. The nonlinearity is introduced by using the modulo operator. The technique aims to map an algorithm into a 1-D array, in this case, a ring structure. The standard serial algorithm corresponding to Eq.10 is a nested loop of depth 4, implying a 4-D index space. <p> main () *****/ main (argc, argv) int argc; char **argv; - /* Parse command line arguments */ if (argc != 4) - CMMD_fset_io_mode (stderr, CMMD_sync_bc); fprintf (stderr, "USAGE: %s &lt;num_neurons&gt; &lt;num_proc.&gt; &lt;input_file&gt;"n", argv [0]); exit (1); - n = atoi (argv [1]); part_size = atoi (argv [2]); input_file = argv <ref> [3] </ref>; bin_file = argv [4]; /* Set global and local parameters */ CMMD_reset_partition_size (part_size); file_desc = CMMD_global_open (bin_file, O_RDWR | O_CREAT | O_TRUNC, 0666); my_address = CMMD_self_address (); ma = my_address; /* Detemine the number of virtual processors on this node */ if ( n &gt; part_size ) - vppp =
Reference: [4] <author> Richard P. Lippmann. </author> <title> An introduction to Computing with Neural Nets, </title> <journal> IEEE ASSP Magazine: </journal> <pages> 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: (argc, argv) int argc; char **argv; - /* Parse command line arguments */ if (argc != 4) - CMMD_fset_io_mode (stderr, CMMD_sync_bc); fprintf (stderr, "USAGE: %s &lt;num_neurons&gt; &lt;num_proc.&gt; &lt;input_file&gt;"n", argv [0]); exit (1); - n = atoi (argv [1]); part_size = atoi (argv [2]); input_file = argv [3]; bin_file = argv <ref> [4] </ref>; /* Set global and local parameters */ CMMD_reset_partition_size (part_size); file_desc = CMMD_global_open (bin_file, O_RDWR | O_CREAT | O_TRUNC, 0666); my_address = CMMD_self_address (); ma = my_address; /* Detemine the number of virtual processors on this node */ if ( n &gt; part_size ) - vppp = n / part_size; if
Reference: [5] <author> Tomas Nordstrom and Bertil Svensson. </author> <title> Using and Designing Massively Computers for Artificial Neural Networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14: </volume> <pages> 260-285, </pages> <year> 1992 </year>
Reference: [6] <author> H. T. Kung. </author> <title> Why systolic architechtures? Computer, </title> <booktitle> 15: </booktitle> <pages> 37-46, </pages> <month> January. </month> <year> 1982. </year>
Reference: [7] <author> Jose A. B. Fortes and Benjamin W. Wah. </author> <title> Systolic arrays, from concept to implementation Computer, </title> <booktitle> 7: </booktitle> <pages> 12-17, </pages> <month> July. </month> <year> 1987. </year>
Reference: [8] <author> John Hertz, Anders Krogh and Richard G. Palmer. </author> <title> Introduction to the Theory of Neural Computing, p 184-186. </title>
Reference: [9] <author> Igor Z. Milosavljevic and Marwan A. Jabri. </author> <title> Mappings of Real Time Recurrent Learning to SIMD Processor Arrays. </title> <booktitle> Proceedings of the Fifth Australian Conference on Neural Networks, </booktitle> <month> Januari-Februari </month> <year> 1994. </year>
Reference-contexts: This implementation is written in the C language and the CMMD features are used for message passing among the processors. To evaluate the efficiency of this implementation it is then compared to an existing implementation of the algorithm, written in the C* programming language. A mapping technique described in <ref> [9] </ref> is used to map the RTRL algorithm in an efficient way. This is the same mapping technique used for the C*-implementation. 1.3 Report Layout The structure of the report is as follows. First, an introduction to Artificial Neural Networks is presented, followed by a description of the RTRL algorithm. <p> By assigning the vectors v, b 1 , b 2 in a proper way it is possible to achieve independence among points lying on the same ring, make the p ij -matrices stationary and map all p ij -matrices with the same i to the same processor. In <ref> [9] </ref> it is shown that, for an example with 3 nodes, i.e., n = 3, if v = 6 1 1 7 2 4 0 3 5 and b 2 = 6 0 0 7 it is possible to create rings of index points that are independent and can therefore be
Reference: [10] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <month> January </month> <year> 1992. </year>
Reference: [11] <institution> Thinking Machines Corporation. </institution> <note> C* Programming Guide, </note> <month> May </month> <year> 1993. </year>
Reference: [12] <institution> Thinking Machines Corporation. </institution> <note> CMMD Reference Manual, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: This is done by evaluating the message passing features of the CM Message passing Library (CMMD), supplied by Thinking Machines Corporation <ref> [12] </ref>. Especially the possibilities to overlap communication/computation are evaluated. Then a ANN algorithm, the Real Time Recurrent Learning (RTRL) algorithm, is implemented on the CM-5. This implementation is written in the C language and the CMMD features are used for message passing among the processors.
Reference: [13] <institution> Thinking Machines Corporation. </institution> <note> CMMD User's Guide, </note> <month> May </month> <year> 1993. </year> <month> 73 </month>
References-found: 13

