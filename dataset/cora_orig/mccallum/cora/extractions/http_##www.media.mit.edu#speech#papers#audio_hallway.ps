URL: http://www.media.mit.edu/speech/papers/audio_hallway.ps
Refering-URL: http://www.media.mit.edu/speech/sig_papers.html
Root-URL: http://www.media.mit.edu
Email: geek@media.mit.edu  
Title: Audio Hallway: a Virtual Acoustic Environment for Browsing  
Author: Chris Schmandt 
Keyword: digitized speech, virtual environments, spatial audio, auditory user interface.  
Affiliation: M.I.T. Media Laboratory  
Abstract: This paper describes the Audio Hallway, a virtual acoustic environment for browsing collections of related audio files. The user travels up and down the Hallway by head motion, passing rooms alternately on the left and right sides. Emanating from each room is an auditory collage of braided audio which acoustically indicates the contents of the room. Each room represents a broadcast radio news story, and the contents are a collection of individual sound bites or actualities related to that story. Upon entering a room, the individual sounds comprising that story are arrayed spatially in front of the listener, with auditory focus controlled by head rotation. The main design challenge for the Audio Hallway is adequately controlling the auditory interface to position sounds so that spatial memory can facilitate navigation and recall in the absence of visual cues. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Begault. </author> <title> 3D Sound for Virtual Reality and Multimedia. </title> <publisher> Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: Computational techniques allow for spatial presentation of sound information by processing the source audio into left ear and right ear signals through the Head Related Transfer Function (HRTF), which introduces the inter-aural phase, frequency, and amplitude differences as heard by each ear <ref> [13, 1] </ref>. Such spatialized audio is presented over headphones, and was originally developed for virtual reality applications such as in [4].
Reference: [2] <author> A. Bregman. </author> <title> Auditory Scene Analysis: the Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: A number of cues contribute to the streaming effect, including frequency and temporal characteristics of the sound and the direction from which it arrives at our ears <ref> [2] </ref>. Computational techniques allow for spatial presentation of sound information by processing the source audio into left ear and right ear signals through the Head Related Transfer Function (HRTF), which introduces the inter-aural phase, frequency, and amplitude differences as heard by each ear [13, 1]. <p> Several aspects of auditory streaming indicate that at least some of this difficulty is inherent in our perceptual systems. The first is that visual cues are known to be strong components of our ability to separate audio into streams <ref> [2, 8] </ref>. Another is that the history of the sound is an additional streaming cue. Our braided audio sounds violate the history cue, because they shift every few seconds between sounds which, although semantically related, are acoustically different, coming from different talkers in different acoustic ambiance and background noise.
Reference: [3] <author> E. Cherry. </author> <title> Some experiments in the recognition of speech, with one and two ears. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 25 </volume> <pages> 975-979, </pages> <year> 1953. </year>
Reference-contexts: Audio Hallway is one of a series of projects exploiting computer generated simultaneous and spatialized audio presentation. Simultaneous listening, involving attending to multiple sources of audio at the same time, has been studied for some 1 time. Cherry <ref> [3] </ref> performed early experimental work on the cocktail party effect, i.e., that we appear to be able to switch our attention and focus it on one of several several simultaneously presented sounds. The selective attention ability is predicated on our capacity to cognitively group the arriving sounds into streams.
Reference: [4] <author> S. Fisher E. Wenzel C. Coler and M. McGeevy. </author> <title> Virtual interface environment for workstations. </title> <booktitle> Proceedings of the Human Factors Society, </booktitle> <year> 1988. </year>
Reference-contexts: Such spatialized audio is presented over headphones, and was originally developed for virtual reality applications such as in <ref> [4] </ref>. In a Bellcore project, non-spatial audio was used in an interesting acoustic environment for audio window systems, in which sounds of varying levels of priority or importance were enhanced or muffled [10].
Reference: [5] <author> R. Want E. Mynatt, M. </author> <title> Back. Designing Audio Aura. </title> <booktitle> In Proceedings of CHI99, </booktitle> <pages> pages 566-572, </pages> <address> New York, 1998. </address> <publisher> ACM. </publisher>
Reference-contexts: Using a sinusoidal amplitude envelope instead of a step function avoids jarring discontinuities and aids in perceiving the braided audio stream as a single, internally related acoustic entity. Sudden acoustic transitions draw attention to themselves <ref> [5] </ref>. It is important that the sometimes disparate components of the audio braid be perceived as a single whole, a cluster, which can be ascribed a particular location relative to other clusters; this is an aspect of our larger theme of spatial recall.
Reference: [6] <author> G. Furnas. </author> <title> The FishEye view: a new look at structured files. </title> <booktitle> In Proceedings of CHI86, </booktitle> <address> New York, 1986. </address> <publisher> ACM. </publisher>
Reference-contexts: In this figure only, size of the sounds represents their playback amplitude. be distinguished, they are distorted horizontally in a manner motivated by Furnas' work on Fisheye Lenses <ref> [6] </ref>. As the user's head rotates, a virtual lens moves across the audio sources, so that a small movement of the head results in a greater, distorted movement of the sources. This allows the user to bring a single sound into focus and also scan the sounds fairly rapidly.
Reference: [7] <author> M. Kobayashi and C. Schmandt. </author> <title> Dynamic soundscape: mapping time to space for audio browsing. </title> <booktitle> In Proceedings of CHI97, </booktitle> <address> New York, 1997. </address> <publisher> ACM. </publisher>
Reference-contexts: Spatialized audio was used in non-VR situations in AudioStreamer [9], which presented three simultaneous audio streams of news stories to the left, right, and in front of the listener, and enhanced selective attention when the user leaned toward one of them. It was also used in SoundScape <ref> [7] </ref> in which multiple versions of the same sound orbited about the listener, with rotational angle being a function of time offset into the sound, and with the user able to reach up into the orbiting sound and re-position it for playback.
Reference: [8] <author> B. Moore. </author> <title> An Introduction to the Psychology of Hearing. </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: Several aspects of auditory streaming indicate that at least some of this difficulty is inherent in our perceptual systems. The first is that visual cues are known to be strong components of our ability to separate audio into streams <ref> [2, 8] </ref>. Another is that the history of the sound is an additional streaming cue. Our braided audio sounds violate the history cue, because they shift every few seconds between sounds which, although semantically related, are acoustically different, coming from different talkers in different acoustic ambiance and background noise.
Reference: [9] <author> A. Mullins and C. Schmandt. </author> <title> Audio Streamer: Exploiting simultaneity for listening. </title> <booktitle> In Proceedings of CHI96, </booktitle> <address> New York, 1996. </address> <publisher> ACM. </publisher>
Reference-contexts: In a Bellcore project, non-spatial audio was used in an interesting acoustic environment for audio window systems, in which sounds of varying levels of priority or importance were enhanced or muffled [10]. Spatialized audio was used in non-VR situations in AudioStreamer <ref> [9] </ref>, which presented three simultaneous audio streams of news stories to the left, right, and in front of the listener, and enhanced selective attention when the user leaned toward one of them.
Reference: [10] <author> L. Ludwig N. Pincever and M. Cohen. </author> <title> Extending the notion of a window system to audio. </title> <journal> IEEE Computer, </journal> <volume> 23(8) </volume> <pages> 185-188, </pages> <year> 1987. </year>
Reference-contexts: In a Bellcore project, non-spatial audio was used in an interesting acoustic environment for audio window systems, in which sounds of varying levels of priority or importance were enhanced or muffled <ref> [10] </ref>. Spatialized audio was used in non-VR situations in AudioStreamer [9], which presented three simultaneous audio streams of news stories to the left, right, and in front of the listener, and enhanced selective attention when the user leaned toward one of them.
Reference: [11] <author> G. Stalton and M. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: As this was not the case, we took advantage of the transcripts merely to group actualities into stories and determine the number of stories. Summarization is provided via the braided audio technique described below. We use the SMART text retrieval engine <ref> [11] </ref> to correlate the stories. SMART is ordinarily used to match a query text against one or more documents in a previously indexed corpus of documents. We use it in a slightly different manner to cluster the actualities, where each cluster ideally represents one story.
Reference: [12] <author> L. Stifelman. </author> <title> A paper-based audio notebook. </title> <booktitle> In Proceedings of CHI97, </booktitle> <address> New York, 1997. </address> <publisher> ACM. </publisher>
Reference-contexts: Fortunately the news actualities are usually carefully edited at the ABC newsroom in New York; this makes them succinct and acoustically "dense". If they consisted of less edited speech, it might prove useful to select phrase boundaries (as was done, for example, in Stifelman's Audio Notebook <ref> [12] </ref>) for emphasis in the braiding. The braided audio is meant to convey the general topic of a story and some of the level of its emotional content.
Reference: [13] <author> E. Wenzel. </author> <title> Localization using nonindividualized head-related transfer fuctions. </title> <journal> Journal of the Acoustical Society of America, </journal> <month> July </month> <year> 1993. </year>
Reference-contexts: Computational techniques allow for spatial presentation of sound information by processing the source audio into left ear and right ear signals through the Head Related Transfer Function (HRTF), which introduces the inter-aural phase, frequency, and amplitude differences as heard by each ear <ref> [13, 1] </ref>. Such spatialized audio is presented over headphones, and was originally developed for virtual reality applications such as in [4].
References-found: 13

