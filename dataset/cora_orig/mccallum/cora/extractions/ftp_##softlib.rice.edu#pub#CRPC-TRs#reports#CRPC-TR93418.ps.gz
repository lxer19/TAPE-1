URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93418.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: LAPACK Working Note 58 The Design of Linear Algebra Libraries for High Performance Computers block-cyclic
Author: Jack Dongarra yz and David Walker 
Note: The  are reviewed. This work was supported in part by the NSF under Grant No. ASC-9005933 and by ARPA and ARO under contract number DAAL03-91-C-0047.  
Address: Knoxville, TN 37996  Oak Ridge, TN 37831  
Affiliation: yDepartment of Computer Science University of Tennessee  zMathematical Sciences Section Oak Ridge National Laboratory  
Abstract: This paper discusses the design of linear algebra libraries for high performance computers. Particular emphasis is placed on the development of scalable algorithms for MIMD distributed memory concurrent computers. A brief description of the EISPACK, LINPACK, and LAPACK libraries is given, followed by an outline of ScaLAPACK, which is a distributed memory version of LAPACK currently under development. The importance of block-partitioned algorithms in reducing the frequency of data movement between different levels of hierarchical memory is stressed. The use of such algorithms helps reduce the message startup costs on distributed memory concurrent computers. Other key ideas in our approach are the use of distributed versions of the Level 3 Basic Linear Algebra Subgrams (BLAS) as computational building blocks, and the use of Basic Linear Algebra Communication Subprograms (BLACS) as communication building blocks. Together the distributed BLAS and the BLACS can be used to construct higher-level algorithms, and hide many details of the parallelism from the application developer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, A. Benzoni, J. J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn. </author> <title> LAPACK for distributed memory architectures: Progress report. </title> <booktitle> In Parallel Processing for Scientific Computing, Fifth SIAM Conference. </booktitle> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [2] <author> E. Anderson and J. Dongarra. </author> <title> Results from the initial release of LAPACK. </title> <note> Technical Report LAPACK working note 16, </note> <institution> Computer Science Department, University of Tennessee, Knoxville, TN, </institution> <year> 1989. </year>
Reference-contexts: LAPACK addresses this problem by reorganizing the algorithms to use block matrix operations, such as matrix multiplication, in the innermost loops [3, 14]. These block operations can be optimized for each architecture to account for the memory hierarchy <ref> [2] </ref>, and so provide a transportable way to achieve high efficiency on diverse modern machines. Here we use the term "transportable" instead of "portable" because, for fastest possible performance, LAPACK requires that highly optimized block matrix operations be already implemented on each machine.
Reference: [3] <author> E. Anderson and J. Dongarra. </author> <title> Evaluating block algorithm variants in LAPACK. </title> <note> Technical Report LAPACK working note 19, </note> <institution> Computer Science Department, University of Tennessee, Knoxville, TN, </institution> <year> 1990. </year> <month> 36 </month>
Reference-contexts: LAPACK addresses this problem by reorganizing the algorithms to use block matrix operations, such as matrix multiplication, in the innermost loops <ref> [3, 14] </ref>. These block operations can be optimized for each architecture to account for the memory hierarchy [2], and so provide a transportable way to achieve high efficiency on diverse modern machines.
Reference: [4] <author> C. C. Ashcraft. </author> <title> The distributed solution of linear systems using the torus wrap data map ping. </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-147, Boeing Computer Services, </institution> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [5] <author> C. C. Ashcraft. </author> <title> A taxonamy of distributed dense LU factorization methods. </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-161, Boeing Computer Services, </institution> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [6] <author> M. Barnett, D. G. Payne, and R. van de Geijn. </author> <title> Broadcasting on meshes with worm-hole routing. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <month> April </month> <year> 1993. </year> <note> Submitted to Supercomputing '93. </note>
Reference-contexts: Thus, the way in which processes are assigned to processors can affect performance if some assignments result in differing amounts of contention. Logarithmic contention-free broadcast algorithms have been developed for processors connected as a two-dimensional mesh <ref> [6, 51] </ref>, so on such machines process (p; q) is usually mapped to the processor at position (p; q) in the mesh of processors. <p> An alternative is to use a spanning tree algorithm, of which there are several varieties. The complexity of the ring algorithm is linear in the number of processes involved, whereas that of spanning tree algorithms is logarithmic (for example, see <ref> [6] </ref>). Thus, considered in isolation, the spanning tree algorithms are preferable to a ring algorithm. However, in a spanning tree algorithm, a process may take part in several of the logarithmic steps, and in some implementations these algorithms act as a barrier.
Reference: [7] <author> W. S. Brainerd, C. H. Goldbergs, and J. C. Adams. </author> <title> Programmers Guide to Fortran 90. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Loosely speaking, we can say that L 0 and L 1 are broadcast along the rows of the template. This type of data movement is the same as that performed by the Fortran 90 routine SPREAD <ref> [7] </ref>. The broadcast of U 1 to all processes in the same template column is very similar.
Reference: [8] <author> R. P. Brent. </author> <title> The LINPACK benchmark for the Fujitsu AP 1000. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 128-135. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent <ref> [8] </ref>, Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a logical process
Reference: [9] <author> R. P. Brent. </author> <title> The LINPACK benchmark on the AP 1000: Preliminary report. </title> <booktitle> In Proceedings of the 2nd CAP Workshop, </booktitle> <month> NOV </month> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [10] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker. </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: operations, their use tends to promote high efficiency on many high-performance computers, particularly if specially coded implementations are provided by the manufacturer. 1.1.4 ScaLAPACK The ScaLAPACK software library, scheduled for completion by the end of 1994, will extend the LAPACK library to run scalably on MIMD, distributed memory, concurrent computers <ref> [10, 11] </ref>. For such machines the memory hierarchy includes the off-processor memory of other processors, in addition to the hierarchy of registers, cache, and local memory on each processor. <p> Following on from this, we will experiment with object-based interfaces for LAPACK and ScaLAPACK, with the goal of developing interfaces compatible with Fortran 90 <ref> [10] </ref> and C++ [24]. 1.2 Target Architectures The EISPACK and LINPACK software libraries were designed for supercomputers used in the 1970s and early 1980s, such as the CDC-7600, Cyber 205, and Cray-1. These machines featured multiple functional units pipelined for good performance [43]. <p> parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers <ref> [10, 25] </ref>. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a logical process template, a position, (b; d), in a logical array of blocks local to
Reference: [11] <author> J. Choi, J. J. Dongarra, and D. W. Walker. </author> <title> The design of scalable software libraries for distributed memory concurrent computers. </title> <editor> In J. J. Dongarra and B. Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> 1993. </year>
Reference-contexts: operations, their use tends to promote high efficiency on many high-performance computers, particularly if specially coded implementations are provided by the manufacturer. 1.1.4 ScaLAPACK The ScaLAPACK software library, scheduled for completion by the end of 1994, will extend the LAPACK library to run scalably on MIMD, distributed memory, concurrent computers <ref> [10, 11] </ref>. For such machines the memory hierarchy includes the off-processor memory of other processors, in addition to the hierarchy of registers, cache, and local memory on each processor.
Reference: [12] <author> E. Chu and A. George. </author> <title> Gaussian elimination with partial pivoting and load balancing on a multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 65-74, </pages> <year> 1987. </year>
Reference-contexts: For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George <ref> [12] </ref>, Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global
Reference: [13] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C: Version 0.9. </title> <type> Technical report, </type> <institution> Computer Science Division - EECS, University of California, Berkeley, </institution> <address> CA 94720, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: On distributed memory concurrent computers, data must be moved between processors. This is usually done by explicit calls to message passing routines, although parallel language extensions such as Coherent Parallel C [31] and Split-C <ref> [13] </ref> do the message passing implicitly. The question arises, "How can we achieve sufficient control over these three factors to obtain the levels of performance that machines can offer?" The answer is through use of the BLAS.
Reference: [14] <author> J. Demmel. </author> <title> LAPACK: A portable linear algebra library for supercomputers. </title> <booktitle> In Proceedings of the 1989 IEEE Control Systems Society Workshop on Computer-Aided Control System Design, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: For most computers, this simply means producing machine-language versions. However, the code can also take advantage of more exotic architectural features, such as vector operations. Further details about the BLAS are presented in Section 2. 1.1.3 LAPACK LAPACK <ref> [14] </ref> provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems. <p> LAPACK addresses this problem by reorganizing the algorithms to use block matrix operations, such as matrix multiplication, in the innermost loops <ref> [3, 14] </ref>. These block operations can be optimized for each architecture to account for the memory hierarchy [2], and so provide a transportable way to achieve high efficiency on diverse modern machines.
Reference: [15] <author> J. J. Dongarra. </author> <title> Increasing the performance of mathematical software through high-level mod ularity. </title> <booktitle> In Proc. Sixth Int. Symp. Comp. Methods in Eng. & Applied Sciences, </booktitle> <address> Versailles, France, </address> <pages> pages 239-248. </pages> <publisher> North-Holland, </publisher> <year> 1984. </year>
Reference-contexts: Vectorization. Designing vectorizable algorithms in linear algebra is usually straightfor ward. Indeed, for many computations there are several variants, all vectorizable, but with different characteristics in performance (see, for example, <ref> [15] </ref>). Linear algebra algorithms can approach the peak performance of many machines|principally because peak performance depends on some form of chaining of vector addition and multiplication operations, and this is just what the algorithms require. <p> We mentioned earlier that for many linear algebra computations there are several algorithmic variants, often referred to as i-, j-, and k-variants, according to a convention introduced in <ref> [15] </ref> and used in [36]. The same is true of the corresponding block algorithms.
Reference: [16] <author> J. J. Dongarra. </author> <note> LAPACK Working Note 34: Workshop on the BLACS. </note> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-134, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> May </month> <year> 1991. </year> <note> (LA-PACK Working Note #34). </note>
Reference-contexts: The fundamental building blocks of the ScaLAPACK library are distributed memory versions of the Level 2 and Level 3 BLAS, and a set of Basic Linear Algebra Communication Subprograms (BLACS) <ref> [16, 26] </ref> for communication tasks that arise frequently in parallel linear algebra computations. In the ScaLAPACK routines, all interprocessor communication occurs within the distributed BLAS and the BLACS, so the source code of the top software layer of ScaLAPACK looks very similar to that of LAPACK. <p> In addition to standardizing general communication primitives, it may also be advantageous to establish standards for problem-specific constructs in commonly occurring areas such as linear algebra. The BLACS (Basic Linear Algebra Communication Subprograms) <ref> [16, 26] </ref> is a package that provides the same ease of use and portability for MIMD message-passing linear algebra communication that the BLAS [17, 18, 45] provide for linear algebra computation.
Reference: [17] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year> <month> 37 </month>
Reference-contexts: The effect can be seen for matrices of quite small order, and for large orders the savings are quite significant. Finally, improved efficiency can be achieved by coding a set of BLAS <ref> [17] </ref> to take advantage of the special features of the computers on which LINPACK is being run. For most computers, this simply means producing machine-language versions. However, the code can also take advantage of more exotic architectural features, such as vector operations. <p> There are now three levels of BLAS: Level 1 BLAS [45]: for vector operations, such as y ffx + y Level 2 BLAS [18]: for matrix-vector operations, such as y ffAx + fiy Level 3 BLAS <ref> [17] </ref>: for matrix-matrix operations, such as C ffAB + fiC. Here, A, B and C are matrices, x and y are vectors, and ff and fi are scalars. <p> The BLACS (Basic Linear Algebra Communication Subprograms) [16, 26] is a package that provides the same ease of use and portability for MIMD message-passing linear algebra communication that the BLAS <ref> [17, 18, 45] </ref> provide for linear algebra computation. Therefore, we recommend that future software for dense linear algebra on MIMD platforms consist of calls to the BLAS for computation and calls to the BLACS for communication.
Reference: [18] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An extended set of Fortran basic linear algebra subroutines. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: There are now three levels of BLAS: Level 1 BLAS [45]: for vector operations, such as y ffx + y Level 2 BLAS <ref> [18] </ref>: for matrix-vector operations, such as y ffAx + fiy Level 3 BLAS [17]: for matrix-matrix operations, such as C ffAB + fiC. Here, A, B and C are matrices, x and y are vectors, and ff and fi are scalars. <p> The BLACS (Basic Linear Algebra Communication Subprograms) [16, 26] is a package that provides the same ease of use and portability for MIMD message-passing linear algebra communication that the BLAS <ref> [17, 18, 45] </ref> provide for linear algebra computation. Therefore, we recommend that future software for dense linear algebra on MIMD platforms consist of calls to the BLAS for computation and calls to the BLACS for communication.
Reference: [19] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. Van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1991. </year>
Reference-contexts: No extra floating-point operations nor extra working storage are required for either of these simple block algorithms. (See Gallivan et al. [33] and Dongarra et al. <ref> [19] </ref> for surveys of algorithms for dense linear algebra on high-performance computers.) Table 3 illustrates the speed of the LAPACK routine for LU factorization of a real matrix, SGETRF in single precision on CRAY machines, and DGETRF in double precision on all other machines.
Reference: [20] <author> J. J. Dongarra and E. Grosse. </author> <title> Distribution of mathematical software via electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 403-407, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: We call this the "mail-order software" model of portability, since it reflects the model used by software servers such as netlib <ref> [20] </ref>. This notion of portability is quite demanding. It requires that all relevant properties of the computer's arithmetic and architecture be discovered at runtime within the confines of a Fortran code.
Reference: [21] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey, and D. W. Walker. </author> <title> A proposal for a user-level message passing interface in a distributed memory environment. </title> <type> Technical Report TM-12231, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Thus, when communicating L 1 and U 1 the blocks to be broadcast would be amalgamated into a single message, which requires a buffer copy. The emerging Message Passing Interface (MPI) standard <ref> [21] </ref> provides support for noncontiguous messages, so in the future the need to avoid buffer copies will not be of such concern to the application developer. 7.2 Tradeoffs between Load Balance and Communication Latency We have discussed the mapping of the logical hierarchical memory to physical memory.
Reference: [22] <author> J. J. Dongarra, Peter Mayes, and Giuseppe Radicati di Brozolo. </author> <title> The IBM RISC System/6000 and linear algebra operations. Supercomputer, </title> <address> 44(VIII-4):15-30, </address> <year> 1991. </year>
Reference-contexts: The LINPACK algorithms can easily be restructured to use Level 2 BLAS, though restructuring has little effect on performance for matrices of very narrow bandwidth. It is also possible to use Level 3 BLAS, at the price of doing some extra work with zero elements outside the band <ref> [22] </ref>.
Reference: [23] <author> J. J. Dongarra and S. Ostrouchov. </author> <title> LAPACK block factorization algorithms on the Intel iPSC/860. </title> <type> Technical Report CS-90-115, </type> <institution> University of Tennessee at Knoxville, Computer Science Department, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [24] <author> J. J. Dongarra, R. Pozo, and D. W. Walker. </author> <title> An object oriented design for high performance linear algebra on distributed memory architectures. </title> <booktitle> In Proceedings of the Object Oriented Numerics Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Following on from this, we will experiment with object-based interfaces for LAPACK and ScaLAPACK, with the goal of developing interfaces compatible with Fortran 90 [10] and C++ <ref> [24] </ref>. 1.2 Target Architectures The EISPACK and LINPACK software libraries were designed for supercomputers used in the 1970s and early 1980s, such as the CDC-7600, Cyber 205, and Cray-1. These machines featured multiple functional units pipelined for good performance [43]. <p> Ease-of-use is also enhanced if implementation details are largely hidden from the user, for example, through the use of an object-based interface to the library <ref> [24] </ref>. 1.3.3 Range-Of-Use Range-of-use may be gauged by how numerically stable the algorithms are over a range of input problems, and the range of data structures the library will support.
Reference: [25] <author> J. J. Dongarra, R. van de Geijn, and D. W. Walker. </author> <title> A look at scalable dense linear algebra li braries. </title> <booktitle> In IEEE, editor, Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 372-379. </pages> <publisher> IEEE Publishers, </publisher> <year> 1992. </year>
Reference-contexts: parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers <ref> [10, 25] </ref>. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a logical process template, a position, (b; d), in a logical array of blocks local to
Reference: [26] <author> J. J. Dongarra and R. A. van de Geijn. </author> <title> Two-dimensional basic linear algebra communication subprograms. </title> <note> Technical Report LAPACK working note 37, </note> <institution> Computer Science Department, University of Tennessee, Knoxville, TN, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The fundamental building blocks of the ScaLAPACK library are distributed memory versions of the Level 2 and Level 3 BLAS, and a set of Basic Linear Algebra Communication Subprograms (BLACS) <ref> [16, 26] </ref> for communication tasks that arise frequently in parallel linear algebra computations. In the ScaLAPACK routines, all interprocessor communication occurs within the distributed BLAS and the BLACS, so the source code of the top software layer of ScaLAPACK looks very similar to that of LAPACK. <p> In addition to standardizing general communication primitives, it may also be advantageous to establish standards for problem-specific constructs in commonly occurring areas such as linear algebra. The BLACS (Basic Linear Algebra Communication Subprograms) <ref> [16, 26] </ref> is a package that provides the same ease of use and portability for MIMD message-passing linear algebra communication that the BLAS [17, 18, 45] provide for linear algebra computation.
Reference: [27] <author> J. J. Dongarra and R. A. van de Geijn. </author> <title> Reduction to condensed form for the eigenvalue problem on distributed memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 973-982, </pages> <year> 1992. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [28] <author> J. Du Croz and M. Pont. </author> <title> The development of a floating-point validation package. </title> <editor> In M. J. Irwin and R. Stefanelli, editors, </editor> <booktitle> Proceedings of the 8th Symposium on Computer Arithmetic, </booktitle> <address> Como, Italy, May 19-21, 1987. </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1987. </year>
Reference-contexts: For example, if it is important to know the overflow threshold for scaling purposes, it must be determined at runtime without overflowing, since overflow is generally fatal. Such demands have resulted in quite large and sophisticated programs <ref> [28, 44] </ref> which must be modified frequently to deal with new architectures and software releases. This "mail-order" notion of software portability also means that codes generally must be written for the worst possible machine expected to be used, thereby often degrading performance on all others.
Reference: [29] <author> T. H. Dunigan. </author> <title> Communication performance of the Intel Touchstone Delta mesh. </title> <type> Technical Report TM-11983, </type> <institution> Oak Ridge National Laboratory, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: If this condition is satisfied, the assignment of processes to processors can still affect performance by influencing the communication overhead. On recent distributed memory machines, such as the Intel Delta and CM-5, the time to send a single message between two processors is largely independent of their physical location <ref> [29, 48, 49] </ref>, and hence the assignment of processes to processors does not have much direct effect on performance. However, when a collective communication task, such as a broadcast, is being done, contention for physical resources can degrade performance.
Reference: [30] <author> A. Edelman. </author> <title> Large dense numerical linear algebra in 1993: The parallel computing influence. </title> <journal> International Journal Supercomputer Applications, </journal> <note> 1993. Accepted for publication. </note>
Reference-contexts: These systems are always symmetric and complex, but not Hermitian. For further information on various methods for solving large dense linear algebra problems that arise in computational fluid dynamics, see the report by Alan Edelman <ref> [30] </ref>. 4.2 Derivation of a Block Algorithm for LU Factorization Suppose the M fi N matrix A is partitioned as shown in Figure 5, and we seek a factorization A = LU , where the partitioning of L and U is also shown in Figure 5.
Reference: [31] <author> E. W. Felten and S. W. Otto. </author> <title> Coherent parallel C. </title> <editor> In G. C. Fox, editor, </editor> <booktitle> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 440-450. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: On distributed memory concurrent computers, data must be moved between processors. This is usually done by explicit calls to message passing routines, although parallel language extensions such as Coherent Parallel C <ref> [31] </ref> and Split-C [13] do the message passing implicitly. The question arises, "How can we achieve sufficient control over these three factors to obtain the levels of performance that machines can offer?" The answer is through use of the BLAS.
Reference: [32] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: The way the data are distributed (or decomposed) over the memory hierarchy of a computer is of fundamental importance to these factors. Concurrent efficiency, *, is defined as the concurrent speedup per processor <ref> [32] </ref>, where the concurrent speedup is the execution time, T seq , for the best sequential algorithm running on one processor of the concurrent computer, divided by the execution time, T , of the parallel algorithm running on N p processors. <p> We shall assume 0 m &lt; M and 0 p &lt; P . 16 Two common decompositions are the block and the cyclic decompositions <ref> [55, 32] </ref>.
Reference: [33] <author> K. Gallivan, R. Plemmons, and A. Sameh. </author> <title> Parallel algorithms for dense linear algebra com putations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: No extra floating-point operations nor extra working storage are required for either of these simple block algorithms. (See Gallivan et al. <ref> [33] </ref> and Dongarra et al. [19] for surveys of algorithms for dense linear algebra on high-performance computers.) Table 3 illustrates the speed of the LAPACK routine for LU factorization of a real matrix, SGETRF in single precision on CRAY machines, and DGETRF in double precision on all other machines.
Reference: [34] <author> A. Geist and M. Heath. </author> <title> Matrix factorization on a hypercube multiprocessor. </title> <editor> In M. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors, </booktitle> <year> 1986, </year> <pages> pages 161-180, </pages> <address> Philadelphia, PA, </address> <year> 1986. </year> <institution> Society for Industrial and Applied Mathematics. </institution>
Reference-contexts: For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath <ref> [34] </ref>, Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is
Reference: [35] <author> A. Geist and C. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine <ref> [35] </ref>, Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position,
Reference: [36] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: We mentioned earlier that for many linear algebra computations there are several algorithmic variants, often referred to as i-, j-, and k-variants, according to a convention introduced in [15] and used in <ref> [36] </ref>. The same is true of the corresponding block algorithms.
Reference: [37] <author> A. Gupta and V. Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year> <note> Also available as technical report TR 90-20 from the Computer Science Department, </note> <institution> University of Minnesota, </institution> <address> Minneapolis, MN 55455. </address>
Reference-contexts: For iterative routines, such as eigensolvers, the number of iterations, and hence the execution time, depends not only on the problem size, but also on other characteristics of the input data, such as condition number. A parallel algorithm is said to be scalable <ref> [37] </ref> if the concurrent efficiency depends on the problem size and number of processors only through their ratio. This ratio is simply the problem size per processor, often referred to as the granularity.
Reference: [38] <author> R. Harrington. </author> <title> Origin and development of the method of moments for field computation. </title> <journal> IEEE Antennas and Propagation Magazine, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: The underlying differential equation may vary, depending on the specific problem. In the design of stealth aircraft, the principal equation is the Helmholtz equation. To solve this equation, researchers use the method of moments <ref> [38, 56] </ref>. In the case of fluid flow, the problem often involves solving the Laplace or Poisson equation. Here, the boundary integral solution is known as the panel method [40, 41], so named from the quadrilaterals that discretize and approximate a structure such as an airplane.
Reference: [39] <author> B. Hendrickson and D. Womble. </author> <title> The torus-wrap mapping for dense matrix computations on massively parallel computers. </title> <type> Technical Report SAND92-0792, </type> <institution> Sandia National Laboratories, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble <ref> [39] </ref>, Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a logical process template, a position, (b;
Reference: [40] <author> J. L. Hess. </author> <title> Panel methods in computational fluid dynamics. </title> <journal> Annual Reviews of Fluid Mechan ics, </journal> <volume> 22 </volume> <pages> 255-274, </pages> <year> 1990. </year>
Reference-contexts: To solve this equation, researchers use the method of moments [38, 56]. In the case of fluid flow, the problem often involves solving the Laplace or Poisson equation. Here, the boundary integral solution is known as the panel method <ref> [40, 41] </ref>, so named from the quadrilaterals that discretize and approximate a structure such as an airplane. Generally, these methods are called boundary element methods.
Reference: [41] <author> J. L. Hess and M. O. Smith. </author> <title> Calculation of potential flows about arbitrary bodies. </title> <editor> In D. Kuche mann, editor, </editor> <booktitle> Progress in Aeronautical Sciences, </booktitle> <volume> Volume 8. </volume> <publisher> Pergamon Press, </publisher> <year> 1967. </year>
Reference-contexts: To solve this equation, researchers use the method of moments [38, 56]. In the case of fluid flow, the problem often involves solving the Laplace or Poisson equation. Here, the boundary integral solution is known as the panel method <ref> [40, 41] </ref>, so named from the quadrilaterals that discretize and approximate a structure such as an airplane. Generally, these methods are called boundary element methods.
Reference: [42] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 1.0, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) <ref> [42] </ref>, and has been previously used, in one form or another, by several researchers (see [1, 4, 5, 9, 23, 27, 50, 52, 54] for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [43] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers. </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, UK, </address> <year> 1981. </year>
Reference-contexts: These machines featured multiple functional units pipelined for good performance <ref> [43] </ref>. The CDC-7600 was basically a high-performance scalar computer, while the Cyber 205 and Cray-1 were early vector computers. The development of LAPACK in the late 1980s was intended to make the EISPACK and LINPACK libraries run efficiently on shared memory, vector supercomputers.
Reference: [44] <author> W. Kahan. </author> <note> Paranoia. Available from netlib [20]. </note>
Reference-contexts: For example, if it is important to know the overflow threshold for scaling purposes, it must be determined at runtime without overflowing, since overflow is generally fatal. Such demands have resulted in quite large and sophisticated programs <ref> [28, 44] </ref> which must be modified frequently to deal with new architectures and software releases. This "mail-order" notion of software portability also means that codes generally must be written for the worst possible machine expected to be used, thereby often degrading performance on all others.
Reference: [45] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: The question arises, "How can we achieve sufficient control over these three factors to obtain the levels of performance that machines can offer?" The answer is through use of the BLAS. There are now three levels of BLAS: Level 1 BLAS <ref> [45] </ref>: for vector operations, such as y ffx + y Level 2 BLAS [18]: for matrix-vector operations, such as y ffAx + fiy Level 3 BLAS [17]: for matrix-matrix operations, such as C ffAB + fiC. <p> The BLACS (Basic Linear Algebra Communication Subprograms) [16, 26] is a package that provides the same ease of use and portability for MIMD message-passing linear algebra communication that the BLAS <ref> [17, 18, 45] </ref> provide for linear algebra computation. Therefore, we recommend that future software for dense linear algebra on MIMD platforms consist of calls to the BLAS for computation and calls to the BLACS for communication.
Reference: [46] <author> C. Leiserson. </author> <title> Fat trees: Universal networks for hardware-efficient supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):892-901, </volume> <year> 1985. </year>
Reference-contexts: Each CM-5 node is a Sparc processor and up to 4 associated vector processors. Point-to-point communication between nodes is supported by a data network with the topology of a "fat tree" <ref> [46] </ref>. Global communication operations, such as synchronization and reduction, are supported by a separate control network. The IBM SP1 system is based on the same RISC chip used in the IBM RS/6000 workstations and uses a multistage switch to connect processors.
Reference: [47] <author> W. Lichtenstein and S. L. Johnsson. </author> <title> Block-cyclic dense linear algebra. </title> <type> Technical Report TR-04-92, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde [55], Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson <ref> [47] </ref>, and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a logical process template, a position, (b; d), in a logical
Reference: [48] <author> M. Lin, D. Du, A. E. Klietz, and S. Saroff. </author> <title> Performance evaluation of the CM-5 interconnection network. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1992. </year>
Reference-contexts: If this condition is satisfied, the assignment of processes to processors can still affect performance by influencing the communication overhead. On recent distributed memory machines, such as the Intel Delta and CM-5, the time to send a single message between two processors is largely independent of their physical location <ref> [29, 48, 49] </ref>, and hence the assignment of processes to processors does not have much direct effect on performance. However, when a collective communication task, such as a broadcast, is being done, contention for physical resources can degrade performance.
Reference: [49] <author> R. Ponnusamy, A. Choudhary, and G. Fox. </author> <title> Communication overhead on CM-5: An experi mental performance evaluation. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 108-115. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year> <month> 39 </month>
Reference-contexts: If this condition is satisfied, the assignment of processes to processors can still affect performance by influencing the communication overhead. On recent distributed memory machines, such as the Intel Delta and CM-5, the time to send a single message between two processors is largely independent of their physical location <ref> [29, 48, 49] </ref>, and hence the assignment of processes to processors does not have much direct effect on performance. However, when a collective communication task, such as a broadcast, is being done, contention for physical resources can degrade performance.
Reference: [50] <author> Y. Saad and M. H. Schultz. </author> <title> Parallel direct methods for solving banded linear systems. </title> <type> Technical Report YALEU/DCS/RR-387, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1985. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [51] <author> S. R. Seidel. </author> <title> Broadcasting on linear arrays and meshes. </title> <type> Technical Report TM-12356, </type> <institution> Oak Ridge National Laboratory, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Thus, the way in which processes are assigned to processors can affect performance if some assignments result in differing amounts of contention. Logarithmic contention-free broadcast algorithms have been developed for processors connected as a two-dimensional mesh <ref> [6, 51] </ref>, so on such machines process (p; q) is usually mapped to the processor at position (p; q) in the mesh of processors.
Reference: [52] <author> A. Skjellum and A. Leung. </author> <title> LU factorization of sparse, unsymmetric, Jacobian matrices on multicomputers. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 328-337. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [53] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: The nodes each have at least 16 Mbytes of memory, and are connected by a high-speed network with the topology of a two-dimensional mesh. The CM-5 from Thinking Machines Corporation <ref> [53] </ref> supports both SIMD and MIMD programming models, and may have up to 16k processors, though the largest CM-5 currently installed has 1024 processors. Each CM-5 node is a Sparc processor and up to 4 associated vector processors.
Reference: [54] <author> R. A. van de Geijn. </author> <title> Massively parallel LINPACK benchmark on the Intel Touchstone Delta and iPSC/860 systems. </title> <institution> Computer Science report TR-91-28, Univ. of Texas, </institution> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [42], and has been previously used, in one form or another, by several researchers (see <ref> [1, 4, 5, 9, 23, 27, 50, 52, 54] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 9.
Reference: [55] <author> E. F. Van de Velde. </author> <title> Data redistribution and concurrency. </title> <journal> Parallel Computing, </journal> <volume> 16, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: We shall assume 0 m &lt; M and 0 p &lt; P . 16 Two common decompositions are the block and the cyclic decompositions <ref> [55, 32] </ref>. <p> For further details of the optimization of parallel LU factorization algorithms for specific concurrent machines, together with timing results, the reader is referred to the work of Chu and George [12], Geist and Heath [34], Geist and Romine [35], Van de Velde <ref> [55] </ref>, Brent [8], Hendrickson and Womble [39], Lichtenstein and Johnsson [47], and Dongarra and co-workers [10, 25]. 7.1 Mapping Logical Memory to Physical Memory In Section 5, a logical (or virtual) matrix decomposition was described in which the global index (m; n) is mapped to a position, (p; q), in a
Reference: [56] <author> J. J. H. Wang. </author> <title> Generalized Moment Methods in Electromagnetics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The underlying differential equation may vary, depending on the specific problem. In the design of stealth aircraft, the principal equation is the Helmholtz equation. To solve this equation, researchers use the method of moments <ref> [38, 56] </ref>. In the case of fluid flow, the problem often involves solving the Laplace or Poisson equation. Here, the boundary integral solution is known as the panel method [40, 41], so named from the quadrilaterals that discretize and approximate a structure such as an airplane.
Reference: [57] <author> J. Wilkinson and C. Reinsch. </author> <title> Handbook for Automatic Computation: Volume II Linear Algebra. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1971. </year> <month> 40 </month>
Reference-contexts: EISPACK is primarily based on a collection of Algol procedures developed in the 1960s and collected by J. H. Wilkinson and C. Reinsch in a volume entitled Linear Algebra in the Handbook for Automatic Computation <ref> [57] </ref> series. This volume was not designed to cover every possible method of solution; rather, algorithms were chosen on the basis of their generality, elegance, accuracy, speed, or economy of storage.
References-found: 57

