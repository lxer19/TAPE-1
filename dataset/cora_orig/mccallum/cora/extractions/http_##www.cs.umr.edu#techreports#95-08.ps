URL: http://www.cs.umr.edu/techreports/95-08.ps
Refering-URL: http://www.cs.umr.edu/techreports/
Root-URL: 
Title: Parallel Fast Multipole Algorithm using MPI  
Author: Daniel Okunbor and Eric Jui-Lin Lu 
Date: June 21, 1995  
Address: Rolla, MO 65401  
Affiliation: Computer Science Department University of Missouri Rolla  
Abstract: The simulation of many-body, many-particle system has a wide range of applications in area such as biophysics, chemistry, astrophysics, etc. It is known that the force calculation contributes ninety percent of the simulation time. This is mainly due to the fact that the total number of interactions in the force is O(N 2 ), where N is the number of particles in the system. The fast multipole algorithm, proposed by Greengard and Rokhlin, reduces the time complexity of the force calculation to O(N ). We implement the fast multipole algorithm, using MPI, based on optimal communication scheme which minimizes the communication and synchronization overhead. The parallel fast multipole algorithm presented here is scalable and portable.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. A. Board, L.V. Kale, K. Schulten, R. D. Skeel, and T. Schlick, </author> <title> "Modeling biomolecules: Larger scales, longer duration", </title> <journal> IEEE Computational Science and Engineering, </journal> <pages> pp. 19-30, </pages> <month> Winter </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The simulation of many-body, many-particle system has a wide range of applications. It has been used extensively in biophysics and chemistry to investigate the dynamics of biomolecules <ref> [1] </ref>, in astrophysics to study the chaotic characteristics of the galactic system [2]. Simulation of this nature typically requires long time integrations and every time step during the simulation involves force calculations and advancing the positions of the particles in the system.
Reference: [2] <author> Appel A., </author> <title> "An efficient program for many-body simulation", </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 85-103, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction The simulation of many-body, many-particle system has a wide range of applications. It has been used extensively in biophysics and chemistry to investigate the dynamics of biomolecules [1], in astrophysics to study the chaotic characteristics of the galactic system <ref> [2] </ref>. Simulation of this nature typically requires long time integrations and every time step during the simulation involves force calculations and advancing the positions of the particles in the system. The execution time of the force calculation contributes ninety percent of the total compute time during a single step. <p> The algorithms that have received a lot of popularity are the particle-particle, particle mesh (PPPM) algorithm developed by Barnes fl You can contact the authors at okunbor@cs.umr.edu and/or jlu@cs.umr.edu. 1 and Hut [3] and Appel <ref> [2] </ref> and the fast multipole algorithm (FMA) or particle-particle, cluster-cluster algorithm developed by Greengard and Rokhlin [4]. The particle-particle, particle mesh algorithm uses center of mass to collectively represent the interaction force of the particles within the cluster with particles that are relatively distanced from this cluster.
Reference: [3] <author> Barnes J. and Hut P., </author> <title> "A hierachical O(N log N) force calculation algorithm", </title> <journal> Nature, </journal> <volume> vol. 324, </volume> <pages> pp. 446-449, </pages> <year> 1986. </year>
Reference-contexts: The algorithms that have received a lot of popularity are the particle-particle, particle mesh (PPPM) algorithm developed by Barnes fl You can contact the authors at okunbor@cs.umr.edu and/or jlu@cs.umr.edu. 1 and Hut <ref> [3] </ref> and Appel [2] and the fast multipole algorithm (FMA) or particle-particle, cluster-cluster algorithm developed by Greengard and Rokhlin [4]. The particle-particle, particle mesh algorithm uses center of mass to collectively represent the interaction force of the particles within the cluster with particles that are relatively distanced from this cluster.
Reference: [4] <author> L. Greengard and V. Rokhlin, </author> <title> "A fast algorithm for particle simulations", </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 73, </volume> <pages> pp. 325-348, </pages> <year> 1987. </year> <month> 9 </month>
Reference-contexts: algorithms that have received a lot of popularity are the particle-particle, particle mesh (PPPM) algorithm developed by Barnes fl You can contact the authors at okunbor@cs.umr.edu and/or jlu@cs.umr.edu. 1 and Hut [3] and Appel [2] and the fast multipole algorithm (FMA) or particle-particle, cluster-cluster algorithm developed by Greengard and Rokhlin <ref> [4] </ref>. The particle-particle, particle mesh algorithm uses center of mass to collectively represent the interaction force of the particles within the cluster with particles that are relatively distanced from this cluster. This algorithm combined with hierarchical tree structure, shown in to O (N log N ). <p> This paper is organized as follows. In Section 2, we explain as briefly as possible the fast multipole algorithm in respective of the space dimension. The mathematical details would not be discussed, since they can be found in several papers <ref> [4, 7, 8] </ref>. Section 3 is concerned with different data parallel paradigms used. The questions relating parallel FMA are addressed. The data parallel approach discussed here is our attempt to parallelize FMA in 2D. It is anticipated that this approach can be applied to FMA in 3D. <p> Assign each particle into its corresponding cell. * Step 1: Compute multipole expansion coefficients (MECs) <ref> [4] </ref> for each cell at the finest level h 1. <p> `1 cells. * Step 2: For each level ` from level h 2 to level 2, form MECs for each cell by shifting its children cells at level ` + 1. * Step 3: For each level from level 2 to level h 2, (1) construct local expansion coefficients (LECs) <ref> [4] </ref> for each cell i from the MECs of the cells in the interaction list, the shaded cells in Figure 4, of cell i; and (2) shift LECs of cell i into its children cells. * Step 4: For each cell at the finest level, compute its LECs from the MECs
Reference: [5] <author> L. Greengard and W. D. Gropp, </author> <title> "A parallel version of the fast multipole method", </title> <journal> Computer and Mathematics with Applications, </journal> <volume> vol. 20, no. 7, </volume> <pages> pp. 63-71, </pages> <year> 1990. </year>
Reference-contexts: Empirical results in two dimensions (2D) indicated that the sequential FMA outperformed the sequential direct method when N ' 180. The hierarchical tree structure of both algorithms make them good candidates for parallel computation. There have been increased interests in this direction. Greengard and Gropp <ref> [5] </ref> attempted to program the parallel version of FMA in two dimensions for SIMD architecture. Board, et al. [6] have done a tremendous amount of the parallelization of FMA in three dimensions (3D).
Reference: [6] <author> James F. Leathrum Jr. and John A. Board Jr., </author> <title> "The parallel fast multipole algorithm in three dimensions", </title> <type> Tech. Rep. Technical Report, </type> <institution> Electrical Engineering Department, Duke University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The hierarchical tree structure of both algorithms make them good candidates for parallel computation. There have been increased interests in this direction. Greengard and Gropp [5] attempted to program the parallel version of FMA in two dimensions for SIMD architecture. Board, et al. <ref> [6] </ref> have done a tremendous amount of the parallelization of FMA in three dimensions (3D). Although 2 programming these algorithms involves an enormous amount of work, progress in this area has been very outstanding. Most computer programs written for two and three dimensions target specific platforms.
Reference: [7] <author> J. Carrier, L. Greengard, and V. Rokhlin, </author> <title> "A fast adaptive multipole algorithm for particle simulations", </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 699-686, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This paper is organized as follows. In Section 2, we explain as briefly as possible the fast multipole algorithm in respective of the space dimension. The mathematical details would not be discussed, since they can be found in several papers <ref> [4, 7, 8] </ref>. Section 3 is concerned with different data parallel paradigms used. The questions relating parallel FMA are addressed. The data parallel approach discussed here is our attempt to parallelize FMA in 2D. It is anticipated that this approach can be applied to FMA in 3D.
Reference: [8] <author> Jacob Katzenelson, </author> <title> "Computational structure of the n-body problem", </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 787-815, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: This paper is organized as follows. In Section 2, we explain as briefly as possible the fast multipole algorithm in respective of the space dimension. The mathematical details would not be discussed, since they can be found in several papers <ref> [4, 7, 8] </ref>. Section 3 is concerned with different data parallel paradigms used. The questions relating parallel FMA are addressed. The data parallel approach discussed here is our attempt to parallelize FMA in 2D. It is anticipated that this approach can be applied to FMA in 3D.
Reference: [9] <author> Dimitri P. Bertsekas and John N. Tsitsiklis, </author> <title> Parallel and Distributed Computation: Numerical Methods, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1989. </year>
Reference-contexts: The MPI version supports three communication schemes: (1) broadcasting multipole expansions (MEs) to all other processors, (2) butterfly switching MEs to all processors, and (3) optimal neighboring communication. Butterfly-switch communication scheme can be found in <ref> [9] </ref>. Figure 6 shows its communication pattern for 4 processors. Results obtained from PFMA on a 16-node iPSC860 show the total communication overhead, including synchronization overhead, reduces by 67 and 89 percent, compared to the native broadcast scheme, when butterfly switch and optimal neighboring communication, respectively, are used.
Reference: [10] <author> J.P. Singh, C. Holt, J.L. Hennessy, and A. Gupta, </author> <title> "A parallel adaptive fast multipole method", </title> <booktitle> in Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 54-65. 10 </pages>
Reference-contexts: Although the parallel FMA works fine in a uniform (or close to uniform) distributed system, we expect it will get performance hit if the system is extremely non-uniform. Singh et al. <ref> [10] </ref> had developed a parallel adaptive 2D program. We are looking for a way which can effectively deal with non-uniformly distributed systems in both 2D and 3D.
References-found: 10

