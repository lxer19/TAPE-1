URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/caruana/pub/papers/aaaisymp91.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/caruana/pub/richhome.html
Root-URL: 
Email: chrisman@cs.cmu.edu caruana@cs.cmu.edu  carriker@ece.cmu.edu  
Title: Intelligent Agent Design Issues: Internal Agent State and Incomplete Perception  
Author: Lonnie Chrisman Rich Caruana Wayne Carriker 
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Computer Science Electrical and Computer  Carnegie Mellon University  
Date: Nov 15-17, 1991  
Note: In AAAI Fall Symposium Series: Sensory Aspects of Robotic Intelligence. Monterey, CA;  Engineering  
Abstract: Given the present diversity of architectures for intelligent agents it is important that we start identifying those environmental characteristics that impact architecture design. This would benefit architecture designers and those trying to understand and evaluate existing architectures. Unfortunately. the Turing Tarpit discourages precise characterizations. Nonetheless, we attempt here to make a precise characterization. We explore the situation where immediate perception of all pertinent aspects of the world is incomplete. Under this ubiquitous condition, we claim that internal state is necessary to compensate for sensory limitations. We identify types of internal state and examine the relationship between the characteristics of the environment and the appropriateness of each type of internal state for an agent. 
Abstract-found: 1
Intro-found: 1
Reference: [Agre & Chapman, 1987a] <author> Phil Agre and David Chapman, Pengi: </author> <title> An Implementation of a Theory of Activity. </title> <booktitle> In Sixth National Conference on Artificial Intelligence; 1987. </booktitle>
Reference-contexts: Expectation Utilizing Agents can be thought of as Plan Using Agents if the reader accepts a more general definition of planning than that usually implied in AI, such as what <ref> [Agre & Chapman, 1987a] </ref> and [Agre & Chapman, 1987b] call small-p planning. Expectation planning does not require that agents formulate new plans. It is possible that the agent retrieves plans from a precompiled library, as exemplified by PRS [Georgeff & Ingrand, 1989].
Reference: [Agre & Chapman, 1987b] <author> Phil Agre and David Chapman, </author> <title> From Reaction to Participation. In DARPA Planning Workshop: Panel on Interleaving Planning and Execution; Santa Cruz, </title> <address> CA October,1987. </address>
Reference-contexts: Expectation Utilizing Agents can be thought of as Plan Using Agents if the reader accepts a more general definition of planning than that usually implied in AI, such as what [Agre & Chapman, 1987a] and <ref> [Agre & Chapman, 1987b] </ref> call small-p planning. Expectation planning does not require that agents formulate new plans. It is possible that the agent retrieves plans from a precompiled library, as exemplified by PRS [Georgeff & Ingrand, 1989]. Expectation usage can be more general than a strict procedure following description implies.
Reference: [Brooks, 1986] <author> Rodney Brooks, </author> <title> A Robust Layered Control System For a Mobile Robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-2, No. </volume> <month> 1; March </month> <year> 1986. </year>
Reference-contexts: We use the term purely reactive agent to refer to an agent whose actions are a function of current percepts only. Note that most implemented reactive systems actually do choose their actions on the basis of some internal state in addition to their immediate percepts (cf. <ref> [Brooks, 1986] </ref>, [Brooks, 1990], [Kaelbling & Rosenschein, 1990]). We justify our use of this term by asserting that the essence of reactivity accepts as its ideal what we have termed the purely reactive agent.
Reference: [Brooks, 1990] <author> Rodney Brooks, </author> <title> Elephants Dont Play Chess. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> Vol. 6, No. </volume> <pages> 1-2, </pages> <address> pg. </address> <month> 3-15; </month> <year> 1990. </year>
Reference-contexts: We use the term purely reactive agent to refer to an agent whose actions are a function of current percepts only. Note that most implemented reactive systems actually do choose their actions on the basis of some internal state in addition to their immediate percepts (cf. [Brooks, 1986], <ref> [Brooks, 1990] </ref>, [Kaelbling & Rosenschein, 1990]). We justify our use of this term by asserting that the essence of reactivity accepts as its ideal what we have termed the purely reactive agent.
Reference: [Brooks, 1991] <author> Rodney Brooks, </author> <title> Intelligence Without Reason. </title> <booktitle> International Joint Conference on Artificial Intelligence; August 1991. </booktitle>
Reference-contexts: Proponents of the reactive and situated action paradigms often tout the slogan the world is its own best model <ref> [Brooks, 1991] </ref>. Even if this is true, it ceases to be operational once the world can no longer be perceived completely at every instant in time.
Reference: [Chrisman & Simmons, 1991] <author> Lonnie Chrisman and Reid Simmons, </author> <title> Sensible Planning: Focusing Perceptual Attention. </title> <booktitle> In Ninth National Conference on Artificial Intelligence; 1991. </booktitle>
Reference-contexts: Computational (or energy) costs of sensing: When sensing operations are expensive or the number of sensing operations is large, an agent may have to intelligently forgo potentially relevant observations <ref> [Chrisman & Simmons, 1991] </ref>, [Dean, 1990]. 5. Mutually exclusive sensing: In some domains (e.g. X-rays or destructive sensing), certain sensing operations may prohibit the use of other relevant observations.
Reference: [Dean, 1990] <author> Thomas Dean, </author> <title> Planning Under Uncertainty and Time Pressure. In Proceedings of the Workshop On Innovative Approaches to Planning, Scheduling, and Control, </title> <address> pg. 390-395; Nov, </address> <year> 1990. </year>
Reference-contexts: Computational (or energy) costs of sensing: When sensing operations are expensive or the number of sensing operations is large, an agent may have to intelligently forgo potentially relevant observations [Chrisman & Simmons, 1991], <ref> [Dean, 1990] </ref>. 5. Mutually exclusive sensing: In some domains (e.g. X-rays or destructive sensing), certain sensing operations may prohibit the use of other relevant observations.
Reference: [Drummond, 1991] <author> Mark Drummond, John Bresina, and Smadar Kedar, </author> <title> The Entropy Reduction Engine: Integrating Planning, Scheduling, and Control. </title> <booktitle> AAAI Spring Symposium Series; 1991. </booktitle>
Reference-contexts: For example, the agent might select a schedule of behaviors rather than a detailed procedure. It sets up the initial behavior (e.g. head for toolshed) and an expectation condition that, when met, signals a switch to a second set of reactive rules (cf. ERE strategies <ref> [Drummond, 1991] </ref>). Such an agent performs reactively while following a scheduled sequence of expectations. In the toolshed example, the Expectation Utilizing Agent, retrieves a plan to: a. go to the toolshed, b. pick up a small phillips screwdriver, and then c. return to the broken object.
Reference: [Firby, 1987] <author> R. James Firby, </author> <title> An Investigation into Reactive Planning in Complex Domains. </title> <booktitle> In Sixth National Conference on Artificial Intelligence; 1987. </booktitle>
Reference-contexts: Thus, contingency state should be viewed as a store for all potential future decisions. At the moment an important piece of data is observed, the agent makes all potential future decisions that depend upon that observation. Firbys posting of a RAP (reactive action package) <ref> [Firby, 1987] </ref> is, perhaps, one of the best examples of setting contingency state to be implemented so far.
Reference: [Georgeff & Ingrand, 1989] <author> Michael P. Georgeff and Francois Felix Ingrand, </author> <title> Decision-Making in an Embedded Reasoning System. </title> <booktitle> In Eleventh International Joint Conference on Artificial Intelligence; Detroit, </booktitle> <address> MI; 1989. </address>
Reference-contexts: Expectation planning does not require that agents formulate new plans. It is possible that the agent retrieves plans from a precompiled library, as exemplified by PRS <ref> [Georgeff & Ingrand, 1989] </ref>. Expectation usage can be more general than a strict procedure following description implies. For example, the agent might select a schedule of behaviors rather than a detailed procedure.
Reference: [Kaelbling & Rosenschein, 1990] <author> Leslie Pack Kaelbling and Stanley J. Rosenschein, </author> <title> Action and Planning in Embedded Agents. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> Vol. 6, No. </volume> <pages> 1-2, </pages> <address> pg. </address> <month> 35- 48; </month> <year> 1990. </year>
Reference-contexts: Note that most implemented reactive systems actually do choose their actions on the basis of some internal state in addition to their immediate percepts (cf. [Brooks, 1986], [Brooks, 1990], <ref> [Kaelbling & Rosenschein, 1990] </ref>). We justify our use of this term by asserting that the essence of reactivity accepts as its ideal what we have termed the purely reactive agent. <p> tools, when there is uncertainty about the tools that will be present in the toolshed, and when it is easy to remember the condition of the broken object (either because the object is simple or because there are a small number of possible broken states for it to be in). <ref> [Kaelbling & Rosenschein, 1990] </ref> view [internal] state in the agent as carrying information content by virtue of its objective correlation with the environment, and thus their work provides an example of a primarily Model Reactive architecture. 3.2 Expectation Utilizing Agent Agents that use only expectations about future world states to compensate
References-found: 11

