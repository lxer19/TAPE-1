URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/friedman.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/
Root-URL: http://www.cs.wisc.edu
Title: A CHARACTERIZATION OF PROLOG EXECUTION  
Author: by MARK ANDREW FRIEDMAN 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science) at the  
Date: 1992  
Affiliation: UNIVERSITY OF WISCONSIN-MADISON  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. Furukawa, R. Nakajima, A. Yonezawa, S. Goto, and A. Aoyama, </author> <title> ``Problem Solving and Inference Mechanisms,'' </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems. </booktitle> <address> Tokyo, Japan, </address> <pages> pp. 131-138, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: We list original contributions that we make to Prolog implementation research. 1.1. Logic Programming and Prolog Logic programming became the subject of increased attention when the Japanese Fifth Generation Computer Systems (FGCS) Project announced that Prolog would serve as the basis for its principal programming language <ref> [1] </ref>. The FGCS project succeeded in demonstrating that a framework of computer hardware and software could be constructed as a predicate logic machine.
Reference: [2] <author> K. Furukawa and T. Yokoi, </author> <title> ``Basic System Software,'' </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems. </booktitle> <pages> pp. 37-57, </pages> <year> 1984. </year>
Reference-contexts: The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language <ref> [2, 78] </ref> with new hardware architectures to support these languages. The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. <p> KL0 is translated into an internal table type of machine code by a simplistic assembler. Execution of the assembled code requires a complicated microprogrammed interpreter. The operating system of the PSI-I is the Sequential Inference Machine Programming and Operating System (SIMPOS) written in KL0 <ref> [2] </ref> The PSI-II is the successor to the PSI-I. Its design objectives were to reduce the amount and size of hardware and to improve performance in comparison to its predecessor, and to serve as an element of a parallel inference machine. The PSI-II is a desk-side logic programming workstation.
Reference: [3] <editor> K. Fuchi and K. Furukawa, </editor> <booktitle> ``The Role of Logic Programming in the Fifth Generation Computer Systems Project,'' in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> Berlin, Germany, </address> <pages> pp. 1-24, </pages> <year> 1986. </year>
Reference-contexts: The Personal Sequential Inference (PSI) Machine The Japanese Fifth Generation Computer Systems (FGCS) Project was announced in 1981 to develop a new generation of computers suited to knowledge information processing and centered around logic programming <ref> [77, 3, 4] </ref>. The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language [2, 78] with new hardware architectures to support these languages.
Reference: [4] <author> K. Furukawa, </author> <title> ``Fifth Generation Computer Project: Current Research Activity and Future Plans,'' </title> <booktitle> in Proceedings of the International Joint Conference on Theory and Practice of Software Development. </booktitle> <address> Pisa, Italy, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: The Personal Sequential Inference (PSI) Machine The Japanese Fifth Generation Computer Systems (FGCS) Project was announced in 1981 to develop a new generation of computers suited to knowledge information processing and centered around logic programming <ref> [77, 3, 4] </ref>. The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language [2, 78] with new hardware architectures to support these languages.
Reference: [5] <author> T. P. Dobry, A. M. Despain, and Y. N. Patt, </author> <title> ``Performance Studies of a Prolog Machine Architecture,'' </title> <booktitle> in Proceedings of the 12th Annual International Symposium on Computer Architecture. </booktitle> <pages> pp. 180-190, </pages> <month> Decem-ber </month> <year> 1985. </year>
Reference-contexts: It is more complete (in terms of the quality of benchmarks, the amount and presentation of fundamental statistics, and the precision of cost calculations) than any work that has been presented of which we are aware. We compare our measurements to Prolog profiles reported by others <ref> [5, 56, 57] </ref> and to studies of other languages [58-61] when these comparisons lend insight towards implementing Prolog efficiently. <p> The benchmarks we have developed are considerably larger and more representative of real applications than benchmarks that predominantly appear in the literature <ref> [38, 5, 7] </ref>. We h ave developed the Simple Instruction Set Machine for Prolog Execution (SIMPLE) to generate our results. The SIMPLE system includes an architectural specification, a compiler, and a simulator. <p> Special Purpose Prolog Machines Shortly after Warren specified his abstract machine model, several groups initiated projects to design special-purpose machines that implemented the WAM instruction set and memory organization in hardware. These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) <ref> [41, 5] </ref>, the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> A large portion of the project involved the design of architectures and compilers for sequential Prolog execution. The first achievements were the Programmed Logic Machine (PLM) developed by Tep Dobry <ref> [41, 5] </ref> and the PLM compiler developed by Peter Van Roy [62]. The PLM is a microprogrammed engine implementing the Warren Abstract Machine. Built in 1985, using five hundred TTL chips, it was the first hardware implementation of the WAM. <p> The PLM versus SPUR results are based upon a number of simulations using the PLM and SPUR simulators with fourteen standard benchmarks <ref> [5] </ref>. Using techniques, assumptions, and simplifications similar to those of Dobry, several low-level statistics (that is, instruction counts, memory references, and cache misses) are recorded and used to arrive at first-order approximations in the reported results. <p> Quality of the Benchmarks An important and much needed contribution that we add to Prolog implementation research is a new benchmark suite. The benchmarks most often cited in the literature are the eight Warren benchmarks [38] and the six PLM benchmarks <ref> [5] </ref>. These benchmarks form the basis for results in the Tick, PLM, PSI, SPUR, and LOW RISC studies. Unfortunately, these benchmarks are small-sized Prolog programs. As a group, they do not contain much variety and do not contain a full complement of the use of the language. <p> Costs of WAM Operations and Built-In Predicate Operations Dobry measured the dynamic frequencies of WAM operations and approximated the number of cycles that operations required on the PLM to obtain a rough estimate or weight of execution cost for WAM operations for the Warren and PLM benchmarks <ref> [5] </ref>. Although our benchmarks contain a greater variety and use of built-in predicates (that results in twice the frequency of escape operations), they share similar frequencies (though somewhat more evenly distributed) of static and dynamic distributions within the WAM operations with these benchmark suites.
Reference: [6] <author> J. W. Mills, </author> <title> ``A High Performance LOW RISC Machine for Logic Programming,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 6, </volume> <pages> pp. 179-212, </pages> <year> 1989. </year>
Reference-contexts: Several research efforts followed exploring the design of special-purpose, high-performance processors that directly implemented the instruction set of Warren's abstract machine [40-42]. As the trends of conventional language architecture design moved to that of reduced-instruction-set-computer (RISC) technology, Prolog architecture researchers looked to improve Prolog through RISC implementation techniques <ref> [43, 6, 44-48] </ref>. Recently, researchers have begun investigating techniques of global optimization specifically suited for Prolog compilation [49-53]. The most recent and promising Prolog implementation, the Berkeley Abstract Machine (BAM) [7], coupled with the Aquarius compiler [54, 55], combines a RISC architecture with a global optimizing compiler. <p> The RISC approach has been very beneficial to conventional architecture design [63, 64]. As we have seen in recent Prolog developments <ref> [6, 7] </ref>, application of similar design techniques targeted to Prolog lead to results that are as profitable. Because our architecture is similar to existing machines and because its instructions are easy to understand, our results are readily applied to the design of other machines. 7 1.3. <p> We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group <ref> [6] </ref>. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. <p> steered by the work of a group at the University of California at Berkeley that compared the performance of Prolog running on the PLM to Prolog executing on SPUR (Symbolic Programming Using RISC) [43, 84] and a Prolog RISC architecture proposed by the LOW RISC group at Arizona State University <ref> [85, 86, 6] </ref>. 2.3.1. <p> The SIMPLE to PLM static code-size ratio of twenty-five is greater than similar ratios reported by other researchers. The SPUR PLM implementation reports a SPUR to PLM static code-size ratio of seventeen [43]. Mills estimates LOW RISC code to be seven times the size as code on the PLM <ref> [6] </ref>. BAM code is only three times larger than PLM code [7].
Reference: [7] <author> B. K. Holmer, B. Sano, M. Carlton, P. Van Roy, R. Haygood, W. R. Bush, A. M. Despain, J. M. Pendleton, and T. Dobry, </author> <title> ``Fast Prolog with an Extended General Purpose Architecture,'' </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture. </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Recently, researchers have begun investigating techniques of global optimization specifically suited for Prolog compilation [49-53]. The most recent and promising Prolog implementation, the Berkeley Abstract Machine (BAM) <ref> [7] </ref>, coupled with the Aquarius compiler [54, 55], combines a RISC architecture with a global optimizing compiler. <p> The benchmarks we have developed are considerably larger and more representative of real applications than benchmarks that predominantly appear in the literature <ref> [38, 5, 7] </ref>. We h ave developed the Simple Instruction Set Machine for Prolog Execution (SIMPLE) to generate our results. The SIMPLE system includes an architectural specification, a compiler, and a simulator. <p> The RISC approach has been very beneficial to conventional architecture design [63, 64]. As we have seen in recent Prolog developments <ref> [6, 7] </ref>, application of similar design techniques targeted to Prolog lead to results that are as profitable. Because our architecture is similar to existing machines and because its instructions are easy to understand, our results are readily applied to the design of other machines. 7 1.3. <p> In chapter 2, we provide background information on other researchers' work on implementations of Prolog dividing these efforts into five areas: Warren's abstract machine model of Prolog execution, special-purpose Prolog machines, reduced-instruction-set-computer implementations of Prolog, global optimizations for Prolog compilation, and the most recent development, the Berkeley Abstract Machine <ref> [7] </ref>. We briey overview the Warren Abstract Machine [39]. We survey Tick's overlapped Prolog processor [65], the Berkeley Programmed Logic Machine (PLM) [41], and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) [66, 42]. <p> We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor <ref> [7] </ref>, Chapter 3 contains a presentation of our research methodology, tools and benchmarks. We describe the SIMPLE architecture, the organization of the SIMPLE compiler, and the use of SIMPLE to emulate the Warren Abstract Machine. <p> Research in the design of the Berkeley Abstract Machine (BAM) architecture has suggested similar instructions to improve performance <ref> [7] </ref>. (4) We investigate the behavior of our benchmarks to determine how much improvement may be achieved through global analysis compiler techniques for a variety of optimizations. Mellish [49], and Debray and Warren [51] have developed methods that automatically determine mode information and determinism. <p> Mode information is also useful in further analysis of the program. It can assist in the detection of deterministic predicates. 2.4.2. The Aquarius Compiler Peter Van Roy's extensive work on Prolog compilation led to the development of the Aquarius compiler [54, 55] for the Berkeley Abstract Machine (BAM) processor <ref> [7] </ref>. Van Roy includes significant dataow analysis and global optimizations in his compilation scheme to the point where he feels it is less useful to think in terms of the Warren Abstract Machine. <p> The philosophy in the design of the Berkeley Abstract Machine (BAM) <ref> [7] </ref> is that of extending a general-purpose architecture to support Prolog without compromising general-purpose performance by providing support through compiler optimization and essential low-level operations. <p> The SPUR PLM implementation reports a SPUR to PLM static code-size ratio of seventeen [43]. Mills estimates LOW RISC code to be seven times the size as code on the PLM [6]. BAM code is only three times larger than PLM code <ref> [7] </ref>. <p> The search spaces in our programs are broader showing larger frequencies of use of choice point operations. Recently, a new set of benchmarks have appeared in the literature <ref> [104, 7] </ref> which we refer to as the BAM benchmarks. Within the BAM benchmarks are a few programs more similar in size statically or dynamically to our programs. Table 3.13 compares the static and dynamic instruction counts, memory reference count, and memory usage of the different benchmark suites.
Reference: [8] <author> R. A. Kowalski, </author> <title> ``Algorithm = Logic + Control,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 22, </volume> <pages> pp. 424-436, </pages> <month> July </month> <year> 1979. </year>
Reference: [9] <author> R. A. Kowalski, </author> <title> in Logic for Problem Solving. </title> <publisher> Amsterdam, Holland: North Holland, </publisher> <year> 1979. </year>
Reference: [10] <author> J. A. Robinson, </author> <title> ``Logic Programming -- Past, Present and future,'' </title> <journal> Journal of New Generation Computing, </journal> <volume> vol. 1, </volume> <pages> pp. 107-124, </pages> <year> 1983. </year>
Reference: [11] <author> R. A. Kowalski, </author> <title> ``The Early Years of Logic Programming,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 38-43, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Logic programming is a programming paradigm founded on simple, first-order logic, yet it is powerful enough to facilitate the development of advanced applications [8-10]. This idea was first explored by Kowalski and Colmerauer in the early 1970's <ref> [11, 12] </ref>. Prolog is the most popular logic programming language. Introductions are provided in tutorials by Clocksin and Mellish [13], and Sterling and Shapiro [14], and in a pictorial overview by Colmerauer [15]. <p> All of these features combine with the conciseness of Prolog to make it an ideal language for rapid prototyping. In all, Prolog is an important foundation for future languages. Unfortunately, the earliest Prolog systems, implemented as interpreters <ref> [11] </ref>, executed slowly and required unacceptable amounts of working storage. The unavailability of fast computers with large main memories and the unavailability of optimizing compilers hindered the establishment of Prolog as a practical language. <p> We spent a significant amount of time gathering and modifying our benchmark code to produce a diverse and realistic set of examples. CHAPTER 2 Background 2.1. The Warren Abstract Machine Prolog originated in the 1970's through efforts by Robert Kowalski and his expertise in logic and automatic theorem proving <ref> [11] </ref>, Alain Colmerauer and his interests in natural language processing [12], and Phillipe Roussel and his work in implementing Prolog systems. Early Prolog interpreters were not favorably received due to their slowness and large storage requirements. It was David H. D.
Reference: [12] <author> J. Cohen, </author> <title> ``A View of the Origins and Development of Prolog,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 26-37, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Logic programming is a programming paradigm founded on simple, first-order logic, yet it is powerful enough to facilitate the development of advanced applications [8-10]. This idea was first explored by Kowalski and Colmerauer in the early 1970's <ref> [11, 12] </ref>. Prolog is the most popular logic programming language. Introductions are provided in tutorials by Clocksin and Mellish [13], and Sterling and Shapiro [14], and in a pictorial overview by Colmerauer [15]. <p> CHAPTER 2 Background 2.1. The Warren Abstract Machine Prolog originated in the 1970's through efforts by Robert Kowalski and his expertise in logic and automatic theorem proving [11], Alain Colmerauer and his interests in natural language processing <ref> [12] </ref>, and Phillipe Roussel and his work in implementing Prolog systems. Early Prolog interpreters were not favorably received due to their slowness and large storage requirements. It was David H. D.
Reference: [13] <author> L. Sterling, W. F. Clocksin, and C. S. Mellish, </author> <title> in Programming in Prolog. </title> <address> Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: This idea was first explored by Kowalski and Colmerauer in the early 1970's [11, 12]. Prolog is the most popular logic programming language. Introductions are provided in tutorials by Clocksin and Mellish <ref> [13] </ref>, and Sterling and Shapiro [14], and in a pictorial overview by Colmerauer [15]. The language is especially suited for symbolic programming in natural language analysis, artificial intelligence, and database applications, as well as for applications such as compiler writing [16], algebraic manipulation, and theorem proving. <p> An appreciation of how an interpreter executes a Prolog program would be sufficient depth for understanding our presentation. For a review of this material and the terminology that we use, the reader is referred to Clocksin and Mellish <ref> [13] </ref>. 9 1.4. Original Contributions The main original contributions that we add to research in Prolog implementation are as follows: (1) Our characterization study is of value to a wide range of researchers including those interested in Prolog abstract machine models, Prolog compiler design, and Prolog architectures. <p> Hexconversion reads a file of decimal numerals and produces a conversion table showing each decimal numeral and its equivalent hexadecimal representation. The primes benchmark uses the Sieve of Eratosthenes algorithm [102] to generate a table of prime numbers. It is adapted from the solution given by Clocksin and Mellish <ref> [13] </ref>. Queens generates all solutions to the eight-queens problem [99]. Its search strategy improves upon the generate and test technique through use of an agenda. The routes benchmark plans routes between different cities based on different criteria using a database of highway information, bus routes, and ight information.
Reference: [14] <author> L. Sterling and E. Shapiro, </author> <title> in The Art of Prolog. </title> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This idea was first explored by Kowalski and Colmerauer in the early 1970's [11, 12]. Prolog is the most popular logic programming language. Introductions are provided in tutorials by Clocksin and Mellish [13], and Sterling and Shapiro <ref> [14] </ref>, and in a pictorial overview by Colmerauer [15]. The language is especially suited for symbolic programming in natural language analysis, artificial intelligence, and database applications, as well as for applications such as compiler writing [16], algebraic manipulation, and theorem proving.
Reference: [15] <author> A. Colmerauer, </author> <title> ``Prolog in 10 Figures,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 28, </volume> <pages> pp. 1296-1324, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This idea was first explored by Kowalski and Colmerauer in the early 1970's [11, 12]. Prolog is the most popular logic programming language. Introductions are provided in tutorials by Clocksin and Mellish [13], and Sterling and Shapiro [14], and in a pictorial overview by Colmerauer <ref> [15] </ref>. The language is especially suited for symbolic programming in natural language analysis, artificial intelligence, and database applications, as well as for applications such as compiler writing [16], algebraic manipulation, and theorem proving.
Reference: [16] <author> D. H. Warren, </author> <title> ``Logic Programming and Compiler Writing,'' </title> <journal> Software Practice and Experience, </journal> <volume> vol. 10, </volume> <pages> pp. 97-125, </pages> <year> 1980. </year>
Reference-contexts: Introductions are provided in tutorials by Clocksin and Mellish [13], and Sterling and Shapiro [14], and in a pictorial overview by Colmerauer [15]. The language is especially suited for symbolic programming in natural language analysis, artificial intelligence, and database applications, as well as for applications such as compiler writing <ref> [16] </ref>, algebraic manipulation, and theorem proving. Successful examples of Prolog's use are in natural language understanding [17-20], expert systems [21-23], knowledge representation [24], the automatic generation of plans [25], database design [26, 27], expert database systems [28-30], and computer aided design (CAD) [31-34].
Reference: [17] <author> F. Pereira, </author> <title> ``Logic for Natural Language Analysis,'' </title> <type> Technical Note 275, </type> <institution> Artificial Intelligence Center, SRI International, </institution> <address> Menlo Park, California, </address> <month> January </month> <year> 1983. </year>
Reference: [18] <author> A. Porto and M. Filgueiras, </author> <title> ``Natural Language Semantics: A Logic Programming Approach,'' </title> <booktitle> in Proceedings of the 1984 International Symposium on Logic Programming. </booktitle> <address> Atlantic City, New Jersey, </address> <pages> pp. 228-232, </pages> <month> February </month> <year> 1984. </year> <pages> 122 123 </pages>
Reference: [19] <editor> V. Dahl and P. Saint-Dizier, </editor> <booktitle> in Natural Language Understanding and Logic Programming. </booktitle> <address> Amsterdam, </address> <publisher> Holland: North Holland, </publisher> <year> 1985. </year>
Reference: [20] <author> F. Pereira and S. M. Shieber, </author> <title> in Prolog and Natural-Language Analysis. Stanford, California: Center for the Study of Language and Information, </title> <year> 1987. </year>
Reference-contexts: The compiler benchmark is expanded from the Algol-like compiler described in Warren's thesis [38] and run with compilations of twelve short procedures. Cryptoarithmetic solves cryptoarithmetic problems utilizing an incremental generate and test search strategy [101]. Grammar is a natural language parser based on a small context-free grammar <ref> [20] </ref>. Hexconversion reads a file of decimal numerals and produces a conversion table showing each decimal numeral and its equivalent hexadecimal representation. The primes benchmark uses the Sieve of Eratosthenes algorithm [102] to generate a table of prime numbers.
Reference: [21] <author> A. Littleford, </author> <title> ``A MYCIN-like Expert System in Prolog,'' </title> <booktitle> in Proceedings of the Second International Logic Programming Conference. </booktitle> <address> Uppsala, Sweden, </address> <pages> pp. 289-300, </pages> <year> 1984. </year>
Reference: [22] <author> D. R. Brough and I. F. Alexander, </author> <title> ``The Fossil Expert System,'' </title> <journal> Expert Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 76-83 </pages> . 
Reference: [23] <author> D. B. Searls and K. M. Norton, </author> <title> ``Logic-Based Configuration with a Semantic Network,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 8, </volume> <pages> pp. 53-73, </pages> <year> 1990. </year>
Reference: [24] <author> C. J. Rawlings, W. R. Taylor, J. Nyakairu, J. Fox, and M. J. E. Sternberg, </author> <title> ``Using Prolog to Represent and Reason about Protein Structure,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 536-543, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The language is especially suited for symbolic programming in natural language analysis, artificial intelligence, and database applications, as well as for applications such as compiler writing [16], algebraic manipulation, and theorem proving. Successful examples of Prolog's use are in natural language understanding [17-20], expert systems [21-23], knowledge representation <ref> [24] </ref>, the automatic generation of plans [25], database design [26, 27], expert database systems [28-30], and computer aided design (CAD) [31-34]. Programming in Prolog is distinguished from programming in conventional languages in that Prolog is a declarative language.
Reference: [25] <author> C. Fellenstein, C. Green, L. Palmer, A. Walker, and D. Wyler, </author> <title> ``A Prototype Manufacturing Knowledge Base in SYLLOG,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 29, </volume> <pages> pp. 413-421, </pages> <year> 1985. </year>
Reference-contexts: Successful examples of Prolog's use are in natural language understanding [17-20], expert systems [21-23], knowledge representation [24], the automatic generation of plans <ref> [25] </ref>, database design [26, 27], expert database systems [28-30], and computer aided design (CAD) [31-34]. Programming in Prolog is distinguished from programming in conventional languages in that Prolog is a declarative language.
Reference: [26] <editor> H. Gallaire, J. Minker, and J. Nicolas, </editor> <title> ``Logic and Databases: A Deductive Approach,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 16, </volume> <pages> pp. 153-185, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Successful examples of Prolog's use are in natural language understanding [17-20], expert systems [21-23], knowledge representation [24], the automatic generation of plans [25], database design <ref> [26, 27] </ref>, expert database systems [28-30], and computer aided design (CAD) [31-34]. Programming in Prolog is distinguished from programming in conventional languages in that Prolog is a declarative language.
Reference: [27] <author> J. W. Lloyd and R. W. T opor, </author> <title> ``A Basis for Deductive Database Systems,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 2, </volume> <pages> pp. 93-109, </pages> <year> 1985. </year>
Reference-contexts: Successful examples of Prolog's use are in natural language understanding [17-20], expert systems [21-23], knowledge representation [24], the automatic generation of plans [25], database design <ref> [26, 27] </ref>, expert database systems [28-30], and computer aided design (CAD) [31-34]. Programming in Prolog is distinguished from programming in conventional languages in that Prolog is a declarative language.
Reference: [28] <author> C. Zaniolo, </author> <title> ``Prolog: A Database Query Language for All Seasons,'' </title> <booktitle> in Proceedings of the First International Workshop on Expert Database Systems. </booktitle> <address> Kiawah Island, South Carolina, </address> <month> October </month> <year> 1984. </year>
Reference: [29] <author> S. Ceri, G. Gottlob, and G. Wiederhold, </author> <title> ``Interfacing Relational Databases and Prolog Efficiently,'' </title> <booktitle> in Proceedings of the First International Conference on Expert Database Systems. </booktitle> <address> Charleston, South Carolina, </address> <pages> pp. 141-153, </pages> <month> April </month> <year> 1986. </year>
Reference: [30] <author> Y. Ioannidis, J. Chen, M. A. Friedman, and M. Tsangaris, </author> <title> ``BERMUDA - An Architectural Perspective on Interfacing Prolog to a Database Machine,'' </title> <booktitle> in Proceedings of the Expert Database Systems Conference. </booktitle> <year> 1988. </year>
Reference: [31] <author> R. Gupta, </author> <title> ``Test-Pattern Generation for VLSI Circuits in a Prolog Environment,'' </title> <booktitle> in Proceedings of the Third International Conference in Logic Programming. </booktitle> <pages> pp. 528-535, </pages> <month> July </month> <year> 1986. </year>
Reference: [32] <author> W. R. Bush, G. Cheng, P. C. McGeer, and A. Despain, </author> <title> ``Experience with Prolog as a Hardware Specification Language,'' </title> <booktitle> in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <month> August </month> <year> 1987. </year>
Reference: [33] <author> W. F. Clocksin, </author> <title> ``Logic Programming and Digital Circuit Analysis,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 4, </volume> <pages> pp. 59-82, </pages> <year> 1987. </year>
Reference: [34] <author> P. B. Reintjes, ``Aunt: </author> <title> A Universal Netlist Translator,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 8, </volume> <pages> pp. 5-19, </pages> <year> 1990. </year>
Reference: [35] <author> J. A. Robinson, </author> <title> ``A Machine-Oriented Logic Based on the Resolution Principle,'' </title> <journal> Journal of the ACM, </journal> <volume> vol. 12, </volume> <pages> pp. 23-44, </pages> <year> 1965. </year>
Reference-contexts: Prolog searches through the originally supplied set of facts and relationships and applies them to attempt to solve a problem. This ability to automatically support the computation process is based on the principles of unification and resolution discovered by Robinson <ref> [35] </ref>. Resolution is a rule of inference of first-order predicate logic.
Reference: [36] <author> N. J. Nilsson, </author> <booktitle> in Principles of Artificial Intelligence. </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: It is within this process that data structures are created as a solution is obtained to the program's problem. An introduction to first-order predicate calculus and resolution-based theorem proving can be found in Nilsson <ref> [36] </ref>. Mathematical questions regarding the properties of Prolog's procedural meaning with respect to logic are analyzed by Lloyd [37]. In the remainder of this thesis, we refrain from discussing theoretical aspects of logic programming.
Reference: [37] <author> J. W. Lloyd, </author> <booktitle> in Foundations of Logic Programming. </booktitle> <address> B erlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: An introduction to first-order predicate calculus and resolution-based theorem proving can be found in Nilsson [36]. Mathematical questions regarding the properties of Prolog's procedural meaning with respect to logic are analyzed by Lloyd <ref> [37] </ref>. In the remainder of this thesis, we refrain from discussing theoretical aspects of logic programming. The characteristic that gives Prolog it's primary advantages over procedural languages is the separation between its logical and control components.
Reference: [38] <author> D. H. Warren, </author> <title> ``Applied Logic -- Its Use and Implementation as a Programming Tool,'' </title> <institution> University of Edin-burgh, </institution> <year> 1977, </year> <type> PhD Dissertation. </type>
Reference-contexts: The development of Prolog was slowed by an absence of interesting examples to demonstrate its novelty and the existence of better compilers and environments for the then more mature language Lisp that had proven its value in symbolic processing. The development of the first Prolog compiler by Warren <ref> [38] </ref> and his later refined definition of an abstract Prolog instruction set and memory organization [39] provided the first steps toward efficient Prolog implementation. Several research efforts followed exploring the design of special-purpose, high-performance processors that directly implemented the instruction set of Warren's abstract machine [40-42]. <p> The benchmarks we have developed are considerably larger and more representative of real applications than benchmarks that predominantly appear in the literature <ref> [38, 5, 7] </ref>. We h ave developed the Simple Instruction Set Machine for Prolog Execution (SIMPLE) to generate our results. The SIMPLE system includes an architectural specification, a compiler, and a simulator. <p> Early Prolog interpreters were not favorably received due to their slowness and large storage requirements. It was David H. D. Warren's thesis work developing a Prolog compiler that led to the acceptance of Prolog and established the basis for practical systems <ref> [38, 67] </ref>. Warren refined his ideas and developed an abstract Prolog engine. Known as the Warren Abstract Machine (WAM) [39], this model has become the standard for implementing Prolog systems. The Warren Abstract Machine defines an abstract instruction set and memory organization. <p> It includes eighty megabytes of main memory, an eight-kilobyte, two-set, set-associative, mixed instruction and data cache, and the ability to support sixty-four concurrent processes. The PSI-I's model of execution is Warren's original model as developed in his thesis <ref> [38] </ref>. KL0 is translated into an internal table type of machine code by a simplistic assembler. Execution of the assembled code requires a complicated microprogrammed interpreter. <p> The concept of modes was introduced by Warren as a way of describing the ways in which a predicate is used in a Prolog program <ref> [38] </ref>. A simplistic view of modes is that a particular argument within a procedure may always have an instantiated term passed to it, or may always have an uninstantiated term passed to it, or both situations may be possible. <p> It includes the creation and search of two binary trees [99] and a technique to implement updatable variables adapted from a suggestion by O'Keefe for implementing updatable arrays [100]. The compiler benchmark is expanded from the Algol-like compiler described in Warren's thesis <ref> [38] </ref> and run with compilations of twelve short procedures. Cryptoarithmetic solves cryptoarithmetic problems utilizing an incremental generate and test search strategy [101]. Grammar is a natural language parser based on a small context-free grammar [20]. <p> Quality of the Benchmarks An important and much needed contribution that we add to Prolog implementation research is a new benchmark suite. The benchmarks most often cited in the literature are the eight Warren benchmarks <ref> [38] </ref> and the six PLM benchmarks [5]. These benchmarks form the basis for results in the Tick, PLM, PSI, SPUR, and LOW RISC studies. Unfortunately, these benchmarks are small-sized Prolog programs. <p> The concept of modes was introduced by Warren as a way to specify how a procedure argument is to be used as a means to further improve performance through further specialized code <ref> [38] </ref>. The performance improvements obtained from programmer mode specification encouraged the development of compile-time analysis techniques to automatically determine procedure argument modes that expand beyond simple input/output parameter specification to include more detailed information such as type, dereferencing, and deterministic modes.
Reference: [39] <author> D. H. Warren, </author> <title> ``An Abstract Prolog Instruction Set,'' </title> <type> Technical Note 309, </type> <institution> Artificial Intelligence Center, SRI International, </institution> <address> Menlo Park, California, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: The development of the first Prolog compiler by Warren [38] and his later refined definition of an abstract Prolog instruction set and memory organization <ref> [39] </ref> provided the first steps toward efficient Prolog implementation. Several research efforts followed exploring the design of special-purpose, high-performance processors that directly implemented the instruction set of Warren's abstract machine [40-42]. <p> We briey overview the Warren Abstract Machine <ref> [39] </ref>. We survey Tick's overlapped Prolog processor [65], the Berkeley Programmed Logic Machine (PLM) [41], and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) [66, 42]. <p> It was David H. D. Warren's thesis work developing a Prolog compiler that led to the acceptance of Prolog and established the basis for practical systems [38, 67]. Warren refined his ideas and developed an abstract Prolog engine. Known as the Warren Abstract Machine (WAM) <ref> [39] </ref>, this model has become the standard for implementing Prolog systems. The Warren Abstract Machine defines an abstract instruction set and memory organization. The WAM description which follows includes details and enhancements to Warren's original definition which were incorporated as part of the Programmed Logic Machine (PLM) [41]. <p> When the first procedure argument is a variable, execution passes through the switch operation and both clauses are tried. For a fuller description of the Warren Abstract Machine, the reader is referred to Warren's original specification <ref> [39] </ref>. A description of the PLM variation of the abstract machine is given by Dobry [41]. Additional 24 references are by David S. Warren [68] and by Hassan Ait-Kaci [69].
Reference: [40] <author> E. Tick and D. H. Warren, </author> <title> ``Towards a Pipelined Prolog Processor,'' </title> <booktitle> in Proceedings of the 1984 International Symposium on Logic Programming. </booktitle> <pages> pp. 29-40, </pages> <year> 1984. </year> <month> 124 </month>
Reference-contexts: Special Purpose Prolog Machines Shortly after Warren specified his abstract machine model, several groups initiated projects to design special-purpose machines that implemented the WAM instruction set and memory organization in hardware. These included Tick's pipelined Prolog processor <ref> [65, 40] </ref>, the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> Tick's Pipelined Prolog Processor Working with Warren, Evan Tick designed a machine to determine the maximum Prolog performance attainable by a sequential machine for a cost comparable to high-performance Lisp machines <ref> [65, 40] </ref>. Using the register organization and memory organization of the WAM, Tick's design uses a microprogrammed organization to implement the complex WAM operations as machine instructions. Tick's objectives were a fast cycle time and the ability to issue one microinstruction per cycle when data dependencies and machine resources allow.
Reference: [41] <author> T. P. Dobry, Y. N. Patt, and A. M. Despain, </author> <title> ``Design Decisions Inuencing the Microarchitecture for a Prolog Machine,'' </title> <booktitle> in Proceedings of the 17th Annual Microprogramming Workshop of the IEEE Computer Society. </booktitle> <month> October </month> <year> 1984. </year>
Reference-contexts: We briey overview the Warren Abstract Machine [39]. We survey Tick's overlapped Prolog processor [65], the Berkeley Programmed Logic Machine (PLM) <ref> [41] </ref>, and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) [66, 42]. We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. <p> The Warren Abstract Machine defines an abstract instruction set and memory organization. The WAM description which follows includes details and enhancements to Warren's original definition which were incorporated as part of the Programmed Logic Machine (PLM) <ref> [41] </ref>. In the WAM, each predicate of a Prolog program is compiled into a single procedure. A Prolog goal is interpreted as a procedure call with the arguments of the goal serving as the arguments of the procedure call. <p> For structures, the first element of this sequence is the constant specifying its functor. The structure's arguments occupy successive locations in memory terminated by nil. For lists, elements may occupy consecutive memory locations or the list may be broken in pieces which are chained through cdr-coding <ref> [41] </ref>. In cdr-coding, a location which contains a list tag with the special cdr-bit set to one serves as a continuation pointer to the next piece of the list. A list is terminated by nil. A list may also be non-terminated by a variable element containing a set cdr-bit. <p> For a fuller description of the Warren Abstract Machine, the reader is referred to Warren's original specification [39]. A description of the PLM variation of the abstract machine is given by Dobry <ref> [41] </ref>. Additional 24 references are by David S. Warren [68] and by Hassan Ait-Kaci [69]. The first of these reports describes the abstract machine in stages building from a description of the implementation of a procedural language. <p> Special Purpose Prolog Machines Shortly after Warren specified his abstract machine model, several groups initiated projects to design special-purpose machines that implemented the WAM instruction set and memory organization in hardware. These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) <ref> [41, 5] </ref>, the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> A large portion of the project involved the design of architectures and compilers for sequential Prolog execution. The first achievements were the Programmed Logic Machine (PLM) developed by Tep Dobry <ref> [41, 5] </ref> and the PLM compiler developed by Peter Van Roy [62]. The PLM is a microprogrammed engine implementing the Warren Abstract Machine. Built in 1985, using five hundred TTL chips, it was the first hardware implementation of the WAM.
Reference: [42] <author> H. Nakashima and K. Nakajima, </author> <title> ``Hardware Architecture of the Sequential Inference Machine: </title> <booktitle> PSI-II,'' in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: We briey overview the Warren Abstract Machine [39]. We survey Tick's overlapped Prolog processor [65], the Berkeley Programmed Logic Machine (PLM) [41], and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) <ref> [66, 42] </ref>. We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. <p> These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines <ref> [70, 66, 42] </ref>, the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> Alternative implementations for stack-overow checking can include a combination of software and hardware or hardware-exclusive solutions. An example of the former is the multi-stack management scheme of the PSI machines that eliminates the need for stack-overow checking by allowing stacks to grow to any size <ref> [80, 42] </ref>. The memory system of the PSI is similar to traditional virtual memory systems. Each stack area is implemented as a virtually-addressed memory segment built from a pool of fixed-size pages.
Reference: [43] <author> G. Borriello, A. R. Cherenson, P. B. Danzig, and M. N. Nelson, </author> <title> ``RISCS vs. CISCS for Prolog: A Case Study,'' </title> <booktitle> in Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <address> Palo Alto, California, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Several research efforts followed exploring the design of special-purpose, high-performance processors that directly implemented the instruction set of Warren's abstract machine [40-42]. As the trends of conventional language architecture design moved to that of reduced-instruction-set-computer (RISC) technology, Prolog architecture researchers looked to improve Prolog through RISC implementation techniques <ref> [43, 6, 44-48] </ref>. Recently, researchers have begun investigating techniques of global optimization specifically suited for Prolog compilation [49-53]. The most recent and promising Prolog implementation, the Berkeley Abstract Machine (BAM) [7], coupled with the Aquarius compiler [54, 55], combines a RISC architecture with a global optimizing compiler. <p> We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group <ref> [43] </ref> and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. <p> Attention turned to the design and tuning of RISC instruction sets for Prolog steered by the work of a group at the University of California at Berkeley that compared the performance of Prolog running on the PLM to Prolog executing on SPUR (Symbolic Programming Using RISC) <ref> [43, 84] </ref> and a Prolog RISC architecture proposed by the LOW RISC group at Arizona State University [85, 86, 6]. 2.3.1. <p> Machine The proximity of work on RISC architectures at Berkeley, specifically the design of the SPUR (Symbolic Processing Using RISCs) processor, to Berkeley's PLM project led to a study by Borriello, Cherenson, Danzig, and Nelson comparing the expected performance of Prolog running on the PLM to Prolog executed on SPUR <ref> [43] </ref>. 37 SPUR is a high-performance, multiprocessor, personal workstation designed for general-purpose processing with support for Lisp and oating point computations [84]. <p> The SIMPLE to PLM static code-size ratio of twenty-five is greater than similar ratios reported by other researchers. The SPUR PLM implementation reports a SPUR to PLM static code-size ratio of seventeen <ref> [43] </ref>. Mills estimates LOW RISC code to be seven times the size as code on the PLM [6]. BAM code is only three times larger than PLM code [7].
Reference: [44] <author> D. A. Patterson and C. H. Sequin, </author> <title> ``A VLSI RISC Computer,'' </title> <journal> IEEE Computer Magazine, </journal> <volume> vol. 15, </volume> <pages> pp. 8-21, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: SPUR extends the work of the RISC <ref> [44] </ref> and SOAR [87] architectures as a reduced-instruction-set computer with extensions for tagged data, a large mixed instruction and data cache, and a tightly-coupled coprocessor interface. The architecture uses forty-bit internal words that contain an eight-bit tag.
Reference: [45] <author> G. Radin, </author> <title> ``The 801 Minicomputer,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 27, </volume> <pages> pp. 237-246, </pages> <month> May </month> <year> 1983. </year>
Reference: [46] <author> J. L. Hennessy, N. Jouppi, F. Baskett, and J. Gill, </author> <title> ``The MIPS Machine,'' </title> <booktitle> in Digest of Papers Compcon Spring 1982. </booktitle> <pages> pp. 2-7, </pages> <month> February </month> <year> 1982. </year> <title> [47] ``HP Precision Architecture and Instruction Set,'' Manual Part Number: </title> <type> 09740-90014, </type> <institution> Hewlett-Packard Company, Palo Alto, California, </institution> <month> April </month> <year> 1989. </year> <title> [48] ``Series 10000 Technical Reference Library: Processors and Instruction Set,'' Order Number: </title> <institution> 011720-A00, Apollo Computer Inc., Chelmsford, Massachussetts, </institution> <month> September </month> <year> 1988. </year>
Reference: [49] <author> C. S. Mellish, </author> <title> ``Some Global Optimizations for a Prolog Compiler,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 1, </volume> <pages> pp. 43-66, </pages> <year> 1985. </year>
Reference-contexts: We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination <ref> [49, 51] </ref> and on automatic detection of determinism [50, 52]. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. <p> Research in the design of the Berkeley Abstract Machine (BAM) architecture has suggested similar instructions to improve performance [7]. (4) We investigate the behavior of our benchmarks to determine how much improvement may be achieved through global analysis compiler techniques for a variety of optimizations. Mellish <ref> [49] </ref>, and Debray and Warren [51] have developed methods that automatically determine mode information and determinism. <p> This method has been studied extensively and developed into a practical compilation tool [96, 97]. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish <ref> [49, 50] </ref> on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1. Mellish, Debray, and Warren In logic, it is frequently the case that there are alternative ways to show that some theorem follows from a set of axioms. <p> Mellish, Debray, and Warren have been able to automatically detect parts of programs that are restricted in terms of determinacy and procedure argument directionality <ref> [52, 51, 49, 50] </ref>. A deterministic predicate is a Prolog procedure whose definition and use determine that it is never possible for a goal involving that predicate to return more than one possible solution. Deterministic procedures will never backtrack and find alternative solutions.
Reference: [50] <author> C. S. Mellish, </author> <title> ``Abstract Interpretation of Prolog Programs,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 463-474, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism <ref> [50, 52] </ref>. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. We describe the SIMPLE architecture, the organization of the SIMPLE compiler, and the use of SIMPLE to emulate the Warren Abstract Machine. <p> This method has been studied extensively and developed into a practical compilation tool [96, 97]. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish <ref> [49, 50] </ref> on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1. Mellish, Debray, and Warren In logic, it is frequently the case that there are alternative ways to show that some theorem follows from a set of axioms. <p> Mellish, Debray, and Warren have been able to automatically detect parts of programs that are restricted in terms of determinacy and procedure argument directionality <ref> [52, 51, 49, 50] </ref>. A deterministic predicate is a Prolog procedure whose definition and use determine that it is never possible for a goal involving that predicate to return more than one possible solution. Deterministic procedures will never backtrack and find alternative solutions.
Reference: [51] <author> S. K. Debray and D. S. Warren, </author> <title> ``Automatic Mode Inference for Logic Programs,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 5, </volume> <pages> pp. 207-229, </pages> <year> 1988. </year>
Reference-contexts: We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination <ref> [49, 51] </ref> and on automatic detection of determinism [50, 52]. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. <p> Mellish [49], and Debray and Warren <ref> [51] </ref> have developed methods that automatically determine mode information and determinism. <p> This method has been studied extensively and developed into a practical compilation tool [96, 97]. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren <ref> [52, 51] </ref> and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1. <p> Mellish, Debray, and Warren have been able to automatically detect parts of programs that are restricted in terms of determinacy and procedure argument directionality <ref> [52, 51, 49, 50] </ref>. A deterministic predicate is a Prolog procedure whose definition and use determine that it is never possible for a goal involving that predicate to return more than one possible solution. Deterministic procedures will never backtrack and find alternative solutions.
Reference: [52] <author> S. K. Debray and D. S. Warren, </author> <title> ``Detection and Optimization of Functional Computations in Prolog,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 490-504, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism <ref> [50, 52] </ref>. We review the Aquarius compiler [55] and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. We describe the SIMPLE architecture, the organization of the SIMPLE compiler, and the use of SIMPLE to emulate the Warren Abstract Machine. <p> This method has been studied extensively and developed into a practical compilation tool [96, 97]. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren <ref> [52, 51] </ref> and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1. <p> Mellish, Debray, and Warren have been able to automatically detect parts of programs that are restricted in terms of determinacy and procedure argument directionality <ref> [52, 51, 49, 50] </ref>. A deterministic predicate is a Prolog procedure whose definition and use determine that it is never possible for a goal involving that predicate to return more than one possible solution. Deterministic procedures will never backtrack and find alternative solutions.
Reference: [53] <author> P. Van Roy, </author> <title> ``Improving the Execution Speed of Compiled Prolog with Modes, Clause Selection, and Determinism,'' </title> <booktitle> in Proceedings of the International Joint Conference on Theory and Practice of Software Development. Pisa, Italy, </booktitle> <pages> pp. 111-125, </pages> <month> March </month> <year> 1987. </year>
Reference: [54] <author> P. Van Roy, </author> <title> ``An Intermediate Language to Support Prolog's Unification,'' </title> <booktitle> in Proceedings of the North American Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 1148-1164, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Recently, researchers have begun investigating techniques of global optimization specifically suited for Prolog compilation [49-53]. The most recent and promising Prolog implementation, the Berkeley Abstract Machine (BAM) [7], coupled with the Aquarius compiler <ref> [54, 55] </ref>, combines a RISC architecture with a global optimizing compiler. <p> Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler <ref> [54, 55] </ref>. 2.4.1. Mellish, Debray, and Warren In logic, it is frequently the case that there are alternative ways to show that some theorem follows from a set of axioms. This leads to the notion of nondeterminism in logic programming. <p> Mode information is also useful in further analysis of the program. It can assist in the detection of deterministic predicates. 2.4.2. The Aquarius Compiler Peter Van Roy's extensive work on Prolog compilation led to the development of the Aquarius compiler <ref> [54, 55] </ref> for the Berkeley Abstract Machine (BAM) processor [7]. Van Roy includes significant dataow analysis and global optimizations in his compilation scheme to the point where he feels it is less useful to think in terms of the Warren Abstract Machine.
Reference: [55] <author> P. Van Roy and A. M. Despain, </author> <title> ``The Benefits of Global Dataow Analysis for an Optimizing Prolog Compiler,'' </title> <booktitle> in Proceedings of the North American Conference on Logic Programming. </booktitle> <year> 1990. </year>
Reference-contexts: Recently, researchers have begun investigating techniques of global optimization specifically suited for Prolog compilation [49-53]. The most recent and promising Prolog implementation, the Berkeley Abstract Machine (BAM) [7], coupled with the Aquarius compiler <ref> [54, 55] </ref>, combines a RISC architecture with a global optimizing compiler. <p> We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. We review the Aquarius compiler <ref> [55] </ref> and the Berkeley Abstract Machine (BAM) processor [7], Chapter 3 contains a presentation of our research methodology, tools and benchmarks. We describe the SIMPLE architecture, the organization of the SIMPLE compiler, and the use of SIMPLE to emulate the Warren Abstract Machine. <p> Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler <ref> [54, 55] </ref>. 2.4.1. Mellish, Debray, and Warren In logic, it is frequently the case that there are alternative ways to show that some theorem follows from a set of axioms. This leads to the notion of nondeterminism in logic programming. <p> Mode information is also useful in further analysis of the program. It can assist in the detection of deterministic predicates. 2.4.2. The Aquarius Compiler Peter Van Roy's extensive work on Prolog compilation led to the development of the Aquarius compiler <ref> [54, 55] </ref> for the Berkeley Abstract Machine (BAM) processor [7]. Van Roy includes significant dataow analysis and global optimizations in his compilation scheme to the point where he feels it is less useful to think in terms of the Warren Abstract Machine. <p> The philosophy in the design of the Berkeley Abstract Machine (BAM) [7] is that of extending a general-purpose architecture to support Prolog without compromising general-purpose performance by providing support through compiler optimization and essential low-level operations. The development of the BAM processor and the Aquarius compiler <ref> [55] </ref> proceeded simultaneously with this shared philosophy so that the features of one complement the features of the other. In the BAM, most Prolog specific operations are done satisfactorily in software. Only a crucial set of features need be supported by the architecture to achieve efficient Prolog performance.
Reference: [56] <author> H. Touati and A. Despain, </author> <title> ``An Empirical Study of the Warren Abstract Machine,'' </title> <booktitle> in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <pages> pp. 192-204, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: It is more complete (in terms of the quality of benchmarks, the amount and presentation of fundamental statistics, and the precision of cost calculations) than any work that has been presented of which we are aware. We compare our measurements to Prolog profiles reported by others <ref> [5, 56, 57] </ref> and to studies of other languages [58-61] when these comparisons lend insight towards implementing Prolog efficiently.
Reference: [57] <author> R. Ona, H. Shimuzu, K. Masuda, and M. Aso, </author> <title> ``Analysis of Sequential Prolog Programs,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 2, </volume> <pages> pp. 119-141, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: It is more complete (in terms of the quality of benchmarks, the amount and presentation of fundamental statistics, and the precision of cost calculations) than any work that has been presented of which we are aware. We compare our measurements to Prolog profiles reported by others <ref> [5, 56, 57] </ref> and to studies of other languages [58-61] when these comparisons lend insight towards implementing Prolog efficiently.
Reference: [58] <author> J. L. Hennessy and D. A. Patterson, </author> <title> in Computer Architecture: A Quantitative Approach. </title> <address> San Mateo, Cali-fornia: </address> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1989. </year>
Reference-contexts: These include compiler analysis techniques such as instruction reordering, static branch prediction, loop unrolling, and trace scheduling, as well as architectural and implementation enhancements such as dynamic instruction scheduling, data forwarding, delayed loads and delayed branches, dynamic branch prediction, additional logic for fast branch decision computation, and multithreading <ref> [58, 109] </ref>.
Reference: [59] <author> P. Steenkiste and J. Hennessy, </author> <title> ``Lisp on a Reduced-Instruction-Set Processor,'' </title> <booktitle> Proceedings of the 1986 Conference on Lisp and Functional Programming, </booktitle> <pages> pp. 192-201, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: It is during the dereferencing of objects, when following list and structure pointers, and when accessing elements of a list or structure that internal references are generated to the global stack and local stack. reported by Steenkiste and Hennessy <ref> [59] </ref>. This figure shows that instruction frequency differences between Prolog and Pascal magnify most differences between symbolic processing and numerical processing when they are compared to the instruction frequency differences of Lisp and Pascal. All three languages use the same percentages of computational instructions (move, arithmetic, logical and shift).
Reference: [60] <author> P. Steenkiste and J. Hennessy, </author> <title> ``Lisp on a Reduced-Instruction-Set Processor: Characterization and Optimization,'' </title> <journal> IEEE Computer Magazine, </journal> <volume> vol. 21, </volume> <pages> pp. 34-44, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Dynamic tag handling, being unique to symbolic languages such as Lisp and Prolog, does not occur in procedural languages. Steenkiste and Hennessy report Lisp to use twenty-one percent of its dynamic instructions for tag-handling <ref> [60] </ref>. Both Prolog and Lisp have a strong need to break apart objects, examine their types, and glue objects together. The larger percentage of tag-handling operations in Prolog is created by the manipulation of tagged data during the large portion of execution involved in unification.
Reference: [61] <author> E. Tick, </author> <title> ``Memory Performance of Lisp and Prolog Programs,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 642-649, </pages> <month> July </month> <year> 1986. </year> <month> 125 </month>
Reference: [62] <author> P. Van Roy, </author> <title> ``A Prolog Compiler for the PLM,'' </title> <type> Report No. </type> <institution> UCB/CSD 84/203, University of California, Berkeley, California, </institution> <month> November </month> <year> 1984, </year> <type> Master's Thesis. </type>
Reference-contexts: We h ave developed the Simple Instruction Set Machine for Prolog Execution (SIMPLE) to generate our results. The SIMPLE system includes an architectural specification, a compiler, and a simulator. The front-end of our compiler is the Programmed Logic Machine (PLM) compiler developed by Van Roy at Berkeley <ref> [62] </ref> that translates Prolog code into a representation of the Warren Abstract Machine (WAM). The back-end of the SIMPLE compiler implements the WAM model using the instruction set, register set, and memory organization of the SIMPLE architecture. <p> A large portion of the project involved the design of architectures and compilers for sequential Prolog execution. The first achievements were the Programmed Logic Machine (PLM) developed by Tep Dobry [41, 5] and the PLM compiler developed by Peter Van Roy <ref> [62] </ref>. The PLM is a microprogrammed engine implementing the Warren Abstract Machine. Built in 1985, using five hundred TTL chips, it was the first hardware implementation of the WAM. <p> The SIMPLE compiler translates Prolog into SIMPLE instructions as diagrammed in figure 3.1. The front-end of the compiler was obtained from Berkeley. It is Van Roy's original PLM compiler <ref> [62] </ref> that translates Prolog into a WAM instruction set. We use the first three modules of Van Roy's compiler. An input module reads a Prolog program and filters it into a group of clauses for each predicate. A clause compilation module compiles each of the individual clauses.
Reference: [63] <author> D. A. Patterson, </author> <title> ``Reduced Instruction Set Computers,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 28, </volume> <pages> pp. 8-21, </pages> <month> Jan-uary </month> <year> 1985. </year>
Reference-contexts: Suggested enhancements involve small modifications to the instruction set that are justified by their frequency of use and non-disruptive to the overall performance and optimizations at the abstract machine level that lend synergy to the architecture. The RISC approach has been very beneficial to conventional architecture design <ref> [63, 64] </ref>. As we have seen in recent Prolog developments [6, 7], application of similar design techniques targeted to Prolog lead to results that are as profitable.
Reference: [64] <author> J. Hennessy, </author> <title> ``VLSI RISC Processors,'' </title> <booktitle> VLSI Systems Design, </booktitle> <volume> vol. 1, </volume> <pages> pp. 22-32, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Suggested enhancements involve small modifications to the instruction set that are justified by their frequency of use and non-disruptive to the overall performance and optimizations at the abstract machine level that lend synergy to the architecture. The RISC approach has been very beneficial to conventional architecture design <ref> [63, 64] </ref>. As we have seen in recent Prolog developments [6, 7], application of similar design techniques targeted to Prolog lead to results that are as profitable.
Reference: [65] <author> E. Tick, </author> <title> ``An Overlapped Prolog Processor,'' </title> <type> Technical Note 308, </type> <institution> Artificial Intelligence Center, SRI International, </institution> <address> Menlo Park, California, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: We briey overview the Warren Abstract Machine [39]. We survey Tick's overlapped Prolog processor <ref> [65] </ref>, the Berkeley Programmed Logic Machine (PLM) [41], and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) [66, 42]. <p> Special Purpose Prolog Machines Shortly after Warren specified his abstract machine model, several groups initiated projects to design special-purpose machines that implemented the WAM instruction set and memory organization in hardware. These included Tick's pipelined Prolog processor <ref> [65, 40] </ref>, the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> Tick's Pipelined Prolog Processor Working with Warren, Evan Tick designed a machine to determine the maximum Prolog performance attainable by a sequential machine for a cost comparable to high-performance Lisp machines <ref> [65, 40] </ref>. Using the register organization and memory organization of the WAM, Tick's design uses a microprogrammed organization to implement the complex WAM operations as machine instructions. Tick's objectives were a fast cycle time and the ability to issue one microinstruction per cycle when data dependencies and machine resources allow.
Reference: [66] <author> K. Taki, M. Yokota, A. Yamamoto, H. Nishikawa, S. Uchida, H. Nakashima, and A. Mitsuishi, </author> <title> ``Hardware Design and Implementation of the Personal Sequential Inference Machine (PSI),'' </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems. </booktitle> <year> 1984. </year>
Reference-contexts: We briey overview the Warren Abstract Machine [39]. We survey Tick's overlapped Prolog processor [65], the Berkeley Programmed Logic Machine (PLM) [41], and the Fifth Generation Computer Systems Project's Personal Sequential Inference machines (the PSI-I and PSI-II) <ref> [66, 42] </ref>. We review the work that generated interest in Prolog RISC architectures by the Symbolic Processing Using RISC (SPUR) group [43] and the LOW RISC group [6]. We discuss global analysis compiler techniques on automatic mode determination [49, 51] and on automatic detection of determinism [50, 52]. <p> These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines <ref> [70, 66, 42] </ref>, the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. The Personal Sequential Inference Machine I (PSI-I) <ref> [70, 80, 66, 81, 82] </ref> is a single-user, self-contained, multiprocess machine with KL0 as its target language. The PSI-I is built from two thousand high-speed Schottky TTL MSI circuits and MOS RAM integrated circuits on twelve printed circuit boards running at a two-hundred-nanosecond clock speed.
Reference: [67] <author> D. H. D. Warren and L. M. Perreira, </author> <title> ``Prolog - The Language and its Implementation Compared with Lisp,'' </title> <booktitle> in Proceedings of the Symposium on Artificial Intelligence and Programming Languages. </booktitle> <pages> pp. 109-115, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: Early Prolog interpreters were not favorably received due to their slowness and large storage requirements. It was David H. D. Warren's thesis work developing a Prolog compiler that led to the acceptance of Prolog and established the basis for practical systems <ref> [38, 67] </ref>. Warren refined his ideas and developed an abstract Prolog engine. Known as the Warren Abstract Machine (WAM) [39], this model has become the standard for implementing Prolog systems. The Warren Abstract Machine defines an abstract instruction set and memory organization.
Reference: [68] <author> D. S. Warren, </author> <title> ``The Runtime Environment for a Prolog Compiler using a Copy Algorithm,'' </title> <type> Technical Report 83/052, </type> <institution> Computer Science Department, SUNY at Stony Brook, Stony Brook, </institution> <address> New York, </address> <month> March </month> <year> 1984. </year>
Reference-contexts: For a fuller description of the Warren Abstract Machine, the reader is referred to Warren's original specification [39]. A description of the PLM variation of the abstract machine is given by Dobry [41]. Additional 24 references are by David S. Warren <ref> [68] </ref> and by Hassan Ait-Kaci [69]. The first of these reports describes the abstract machine in stages building from a description of the implementation of a procedural language. The later tutorial builds the WAM by describing each design decision as a means to improve performance. 2.2.
Reference: [69] <author> H. Ait-Kaci, </author> <title> ``The WAM: A (Real) Tutorial,'' </title> <type> PRL Research Report 5, </type> <institution> Paris Research Laboratory, Digital Equiptment Corporation, Paris, France, </institution> <year> 1990. </year>
Reference-contexts: For a fuller description of the Warren Abstract Machine, the reader is referred to Warren's original specification [39]. A description of the PLM variation of the abstract machine is given by Dobry [41]. Additional 24 references are by David S. Warren [68] and by Hassan Ait-Kaci <ref> [69] </ref>. The first of these reports describes the abstract machine in stages building from a description of the implementation of a procedural language. The later tutorial builds the WAM by describing each design decision as a means to improve performance. 2.2.
Reference: [70] <author> H. Nishikawa, M. Yokota, A. Yamamoto, K. Taki, and S. Uchida, </author> <title> ``The Personal Sequential Inference Machine (PSI) : Its Design Philosophy and Machine Architecture,'' </title> <booktitle> Proceedings of the Logic Programming Workshop, </booktitle> <pages> pp. 53-72, </pages> <year> 1983. </year>
Reference-contexts: These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines <ref> [70, 66, 42] </ref>, the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. <p> The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. The Personal Sequential Inference Machine I (PSI-I) <ref> [70, 80, 66, 81, 82] </ref> is a single-user, self-contained, multiprocess machine with KL0 as its target language. The PSI-I is built from two thousand high-speed Schottky TTL MSI circuits and MOS RAM integrated circuits on twelve printed circuit boards running at a two-hundred-nanosecond clock speed.
Reference: [71] <author> S. Abe, T. Bandoh, S. Yamaguchi, K. Kurosawa, and K. Kiriyama, </author> <title> ``High Performance Integrated Prolog Processor IPP,'' </title> <booktitle> in Proceedings of the 14th International Symposium on Computer Architecture. </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pp. 100-107, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) <ref> [71, 72] </ref>, and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. Tick's Pipelined Prolog Processor Working with Warren, Evan Tick designed a machine to determine the maximum Prolog performance attainable by a sequential machine for a cost comparable to high-performance Lisp machines [65, 40].
Reference: [72] <author> M. Morioka, S. Yamaguchi, and T. Bandoh, </author> <title> ``Evaluation of Memory System for Integrated Prolog Processor IPP,'' </title> <booktitle> in Proceedings of the 16th International Symposium on Computer Architecture. Jerusalem, Israel, </booktitle> <pages> pp. 203-221, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) <ref> [71, 72] </ref>, and the Knowledge Crunching Machine (KCM) [73]. 2.2.1. Tick's Pipelined Prolog Processor Working with Warren, Evan Tick designed a machine to determine the maximum Prolog performance attainable by a sequential machine for a cost comparable to high-performance Lisp machines [65, 40].
Reference: [73] <author> H. Benker, J. M. Beacco, S. Bescos, M. Dorochevsky, T. Jeffre, A. Pohimann, J. Noye, B. Poterie, A. Sexton, J. C. Syre, O. Thibault, and G. Watzlawik, ``KCM: </author> <title> A Knowledge Crunching Machine,'' </title> <booktitle> in Proceedings of the 16th International Symposium on Computer Architecture. Jerusalem, Israel, </booktitle> <pages> pp. 186-194, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: These included Tick's pipelined Prolog processor [65, 40], the Berkeley Programmed Logic Machine (PLM) [41, 5], the Fifth Generation Computer System's Personal Sequential Inference (PSI) Machines [70, 66, 42], the Integrated Pro-log Processor (IPP) [71, 72], and the Knowledge Crunching Machine (KCM) <ref> [73] </ref>. 2.2.1. Tick's Pipelined Prolog Processor Working with Warren, Evan Tick designed a machine to determine the maximum Prolog performance attainable by a sequential machine for a cost comparable to high-performance Lisp machines [65, 40].
Reference: [74] <author> A. M. Despain and Y. N. Patt, </author> <title> ``Aquarius: A High Performance Computing System for Symbolic/Numeric Applications,'' </title> <booktitle> in Digest of Papers Compcon Spring 1985. </booktitle> <publisher> IEEE Press, </publisher> <pages> pp. 376-382, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: The Programmed Logic Machine (PLM) The Aquarius Project was founded in 1983 at the University of California at Berkeley by Alvin Despain and Yale Patt <ref> [74] </ref> as an attempt to achieve improvements in computer performance in applications that have substantial artificial intelligence and numerically computational components [75]. A large portion of the project involved the design of architectures and compilers for sequential Prolog execution.
Reference: [75] <author> A. M. Despain, Y. N. Patt, T. P. Dobry, J. H. Chang, and W. Citrin, </author> <title> ``High Performance Prolog, The Multiplicative Effect of Several Levels of Implementation.'' </title>
Reference-contexts: The Programmed Logic Machine (PLM) The Aquarius Project was founded in 1983 at the University of California at Berkeley by Alvin Despain and Yale Patt [74] as an attempt to achieve improvements in computer performance in applications that have substantial artificial intelligence and numerically computational components <ref> [75] </ref>. A large portion of the project involved the design of architectures and compilers for sequential Prolog execution. The first achievements were the Programmed Logic Machine (PLM) developed by Tep Dobry [41, 5] and the PLM compiler developed by Peter Van Roy [62].
Reference: [76] <author> V. P. Srini, J. V. Tam, T. M. Nguyen, Y. N. Patt, A. M. Despain, M. Moll, and D. Ellsworth, </author> <title> ``A CMOS Chip for Prolog,'' </title> <booktitle> in Proceedings fo the International Conference on Computer Design. </booktitle> <pages> pp. 605-610, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The PLM is a microprogrammed engine implementing the Warren Abstract Machine. Built in 1985, using five hundred TTL chips, it was the first hardware implementation of the WAM. This work led to the VLSI-PLM <ref> [76] </ref>, a single-chip WAM processor containing two hundred and fifty thousand transistors, and the Xenologic X-1 commercial Sun coprocessor. The PLM is a coprocessor that attaches to a host central processor that provides a memory system, input and output, and assistance in implementing built-in predicates.
Reference: [77] <author> K. Kawanobe, </author> <title> ``Current Status and Future Plans of the Fifth Generation Computer Systems Project,'' </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems. </booktitle> <pages> pp. 3-17, </pages> <year> 1984. </year>
Reference-contexts: The Personal Sequential Inference (PSI) Machine The Japanese Fifth Generation Computer Systems (FGCS) Project was announced in 1981 to develop a new generation of computers suited to knowledge information processing and centered around logic programming <ref> [77, 3, 4] </ref>. The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language [2, 78] with new hardware architectures to support these languages.
Reference: [78] <author> K. Furukawa, S. Kunifuji, A. Takeuchi, and K. Ueda, </author> <title> ``The Conceptual Specification of the Kernel Language Version 1,'' </title> <institution> ICOT TR-054, Institute for New Generation Computer Technology, </institution> <year> 1984. </year>
Reference-contexts: The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language <ref> [2, 78] </ref> with new hardware architectures to support these languages. The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support.
Reference: [79] <author> M. Yokota, T. Hattori, and T. Chikayama, </author> <title> in Fifth Generation Kernel Language. </title> <institution> Institute for New Generation Computer Technology, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: The project developed a hierarchy of languages starting with a logic programming machine language and building up to a knowledge representation language [2, 78] with new hardware architectures to support these languages. The Kernel Language Version 0 (KL0) <ref> [79] </ref>, designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support.
Reference: [80] <author> M. Yokota, A. Yamamoto, K. Taki, H. Nishikawa, and S. Uchida, </author> <title> ``The Design and Implementation of a Personal Sequential Inference Machine: PSI,'' </title> <journal> Journal of New Generation Computing, </journal> <volume> vol. 1, </volume> <pages> pp. 125-144, </pages> <year> 1983. </year> <month> 126 </month>
Reference-contexts: The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. The Personal Sequential Inference Machine I (PSI-I) <ref> [70, 80, 66, 81, 82] </ref> is a single-user, self-contained, multiprocess machine with KL0 as its target language. The PSI-I is built from two thousand high-speed Schottky TTL MSI circuits and MOS RAM integrated circuits on twelve printed circuit boards running at a two-hundred-nanosecond clock speed. <p> Alternative implementations for stack-overow checking can include a combination of software and hardware or hardware-exclusive solutions. An example of the former is the multi-stack management scheme of the PSI machines that eliminates the need for stack-overow checking by allowing stacks to grow to any size <ref> [80, 42] </ref>. The memory system of the PSI is similar to traditional virtual memory systems. Each stack area is implemented as a virtually-addressed memory segment built from a pool of fixed-size pages.
Reference: [81] <author> K. Nakajima, H. Nakashima, M. Yokota, K. Taki, S. Uchida, H. Nishikawa, A. Yamamoto, and M. Mitsui, </author> <title> ``Evaluation of PSI Micro-Interpreter,'' </title> <booktitle> in Digest of Papers Compcon Spring 1986. </booktitle> <address> San Francisco, Califor-nia, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. The Personal Sequential Inference Machine I (PSI-I) <ref> [70, 80, 66, 81, 82] </ref> is a single-user, self-contained, multiprocess machine with KL0 as its target language. The PSI-I is built from two thousand high-speed Schottky TTL MSI circuits and MOS RAM integrated circuits on twelve printed circuit boards running at a two-hundred-nanosecond clock speed.
Reference: [82] <author> K. Taki, K. Nakajima, H. Nakashima, and M. Ikeda, </author> <title> ``Performance and Architectural Evaluation of the PSI Machine,'' </title> <booktitle> in Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems. </booktitle> <address> Palo Alto, California, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: The Kernel Language Version 0 (KL0) [79], designed as a sequential machine language, is a low-level derivative of Prolog with extensions for built-in predicates, modularization, data-type checking, interfacing to relational databases, and operating system support. The Personal Sequential Inference Machine I (PSI-I) <ref> [70, 80, 66, 81, 82] </ref> is a single-user, self-contained, multiprocess machine with KL0 as its target language. The PSI-I is built from two thousand high-speed Schottky TTL MSI circuits and MOS RAM integrated circuits on twelve printed circuit boards running at a two-hundred-nanosecond clock speed.
Reference: [83] <author> C. S. Mellish, </author> <title> ``An Alternative to Structure Sharing in the Implementation of a Prolog Interpreter.'' </title>
Reference-contexts: While the shift from the PSI-I to the PSI-II increases complexity through the need of a KL0 to WAM translator, the instructions of the WAM may be executed at higher speed than the machine code of the PSI-I <ref> [83] </ref>. Improvements in speed are also due to specialized hardware components for multiple-stack management and tagged data manipulation. 32 The majority of the PSI-II's attributes are inherited from its predecessor including multiple concurrent processes, microprogramming, and the design of its memory organization, address translation mechanism, and cache.
Reference: [84] <author> M. Hill, et al., </author> <title> ``Design Decisions in SPUR,'' </title> <journal> IEEE Computer Magazine, </journal> <volume> vol. 19, </volume> <pages> pp. 8-22, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Attention turned to the design and tuning of RISC instruction sets for Prolog steered by the work of a group at the University of California at Berkeley that compared the performance of Prolog running on the PLM to Prolog executing on SPUR (Symbolic Programming Using RISC) <ref> [43, 84] </ref> and a Prolog RISC architecture proposed by the LOW RISC group at Arizona State University [85, 86, 6]. 2.3.1. <p> to Berkeley's PLM project led to a study by Borriello, Cherenson, Danzig, and Nelson comparing the expected performance of Prolog running on the PLM to Prolog executed on SPUR [43]. 37 SPUR is a high-performance, multiprocessor, personal workstation designed for general-purpose processing with support for Lisp and oating point computations <ref> [84] </ref>. SPUR extends the work of the RISC [44] and SOAR [87] architectures as a reduced-instruction-set computer with extensions for tagged data, a large mixed instruction and data cache, and a tightly-coupled coprocessor interface. The architecture uses forty-bit internal words that contain an eight-bit tag.
Reference: [85] <author> J. W. Mills, </author> <title> ``Coming to Grips with a RISC: A Report of the Progress of the LOW RISC Design Group,'' </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> vol. 15, </volume> <pages> pp. 53-62, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: steered by the work of a group at the University of California at Berkeley that compared the performance of Prolog running on the PLM to Prolog executing on SPUR (Symbolic Programming Using RISC) [43, 84] and a Prolog RISC architecture proposed by the LOW RISC group at Arizona State University <ref> [85, 86, 6] </ref>. 2.3.1.
Reference: [86] <author> B. </author> <title> Short, ``Use of Instruction Set Simulators to Evaluate the LOW RISC,'' </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> vol. 15, </volume> <pages> pp. 63-67, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: steered by the work of a group at the University of California at Berkeley that compared the performance of Prolog running on the PLM to Prolog executing on SPUR (Symbolic Programming Using RISC) [43, 84] and a Prolog RISC architecture proposed by the LOW RISC group at Arizona State University <ref> [85, 86, 6] </ref>. 2.3.1. <p> A switch instruction provides transfer of control using a single register 's tag field to select one of four offsets. A hook instruction serves to interface to the attached processor. A fair amount of simulation <ref> [86] </ref> prompted the group to enhance its architecture resulting in a doubling in the number of instructions. Additional integer arithmetic and logical instructions eliminate nearly all calls to the attached processor. Logical operations facilitate other methods of indexing clauses.
Reference: [87] <author> D. Ungar, </author> <title> ``Architecture of SOAR: Smalltalk on a RISC,'' </title> <booktitle> in Proceedings of the Eleventh International Symposium on Computer Architecture. </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: SPUR extends the work of the RISC [44] and SOAR <ref> [87] </ref> architectures as a reduced-instruction-set computer with extensions for tagged data, a large mixed instruction and data cache, and a tightly-coupled coprocessor interface. The architecture uses forty-bit internal words that contain an eight-bit tag.
Reference: [88] <author> P. Kursawe, </author> <title> ``How to Invent a Prolog Machine,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 134-148, </pages> <month> July </month> <year> 1986. </year>
Reference: [89] <author> M. Bruynooghe, G. Janssens, A. Callebaut, and B. Demoen, </author> <title> ``Abstract Interpretation Towards the Global Optimization of Prolog Programs,'' </title> <booktitle> in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <pages> pp. 192-204, </pages> <month> August </month> <year> 1987. </year>
Reference: [90] <author> A. K. Turk, </author> <title> ``Compiler Optimizations for the WAM,'' </title> <booktitle> in Proceedings of the Third International Conference on Logic Programming. </booktitle> <address> London, United Kingdom, </address> <pages> pp. 657-662, </pages> <month> July </month> <year> 1986. </year>
Reference: [91] <author> T. Hickey and S. Mudambi, </author> <title> ``Global Compilation of Prolog,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 7, </volume> <pages> pp. 193-230, </pages> <year> 1989. </year>
Reference: [92] <author> U. S. Reddy, </author> <title> ``Transformation of Logic Programs into Functional Program,'' </title> <booktitle> in Proceedings of the 1984 International Symposium on Logic Programming. </booktitle> <address> Atlantic City, New Jersey, </address> <pages> pp. 187-196, </pages> <month> February </month> <year> 1984. </year>
Reference: [93] <author> W. Drabent, </author> <title> ``Do Logic Programs Resemble Programs in Conventional Languages?,'' </title> <booktitle> in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <month> August </month> <year> 1987. </year>
Reference: [94] <author> H. Mannila and E. Ukkonen, </author> <title> ``Flow Analysis of Prolog Programs,'' </title> <booktitle> in Proceedings of the 1987 Symposium on Logic Programming. </booktitle> <address> San Francisco, California, </address> <month> August </month> <year> 1987. </year>
Reference: [95] <author> S. K. Debray, </author> <title> ``Flow Analysis of Dynamic Logic Programs,'' </title> <journal> Journal of Logic Programming, </journal> <volume> vol. 7, </volume> <pages> pp. 149-176, </pages> <year> 1989. </year>
Reference: [96] <author> G. Kildall, </author> <title> ``A Unified Approach to Global Program Optimization,'' </title> <booktitle> in Proceedings of the First Symposium on Principles of Programming Languages. </booktitle> <pages> pp. 194-206, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Results (characteristics of the program) are more feasibly computed in the abstract domain and reect properties in the program's standard operation. This method has been studied extensively and developed into a practical compilation tool <ref> [96, 97] </ref>. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1.
Reference: [97] <author> P. Cousot and R. Cousot, </author> <title> ``Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints,'' </title> <booktitle> in Proceedings of the Fourth Symposium on Principles of Programming Languages. </booktitle> <pages> pp. 238-252, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Results (characteristics of the program) are more feasibly computed in the abstract domain and reect properties in the program's standard operation. This method has been studied extensively and developed into a practical compilation tool <ref> [96, 97] </ref>. Representative of the abstract interpretation and global optimization work are efforts by Saumya Debry and David S. Warren [52, 51] and C. S. Mellish [49, 50] on detecting determinism and automatic mode determination and Peter Van Roy's development of the Aquarius compiler [54, 55]. 2.4.1.
Reference: [98] <author> M. A. Covington, D. Nute, and A. Vellino, </author> <title> in Prolog Programming in Depth. </title> <address> Glenview, Illinois: Scott, </address> <publisher> Foresman and Company, </publisher> <year> 1988. </year>
Reference-contexts: Cannibals is a variation of the missionaries and cannibals problem. It is an extension of the solution developed in Covington, Nute, and Vellino <ref> [98] </ref> using an incremental generate and test search strategy. The characters benchmark determines the frequency of use of characters within a text file.
Reference: [99] <author> I. Bratko, </author> <booktitle> in Prolog Programming for Artificial Intelligence. </booktitle> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: It is an extension of the solution developed in Covington, Nute, and Vellino [98] using an incremental generate and test search strategy. The characters benchmark determines the frequency of use of characters within a text file. It includes the creation and search of two binary trees <ref> [99] </ref> and a technique to implement updatable variables adapted from a suggestion by O'Keefe for implementing updatable arrays [100]. The compiler benchmark is expanded from the Algol-like compiler described in Warren's thesis [38] and run with compilations of twelve short procedures. <p> The primes benchmark uses the Sieve of Eratosthenes algorithm [102] to generate a table of prime numbers. It is adapted from the solution given by Clocksin and Mellish [13]. Queens generates all solutions to the eight-queens problem <ref> [99] </ref>. Its search strategy improves upon the generate and test technique through use of an agenda. The routes benchmark plans routes between different cities based on different criteria using a database of highway information, bus routes, and ight information.
Reference: [100] <author> P. M. Ross, </author> <title> in Advanced Prolog: Techniques and Examples. </title> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The characters benchmark determines the frequency of use of characters within a text file. It includes the creation and search of two binary trees [99] and a technique to implement updatable variables adapted from a suggestion by O'Keefe for implementing updatable arrays <ref> [100] </ref>. The compiler benchmark is expanded from the Algol-like compiler described in Warren's thesis [38] and run with compilations of twelve short procedures. Cryptoarithmetic solves cryptoarithmetic problems utilizing an incremental generate and test search strategy [101]. Grammar is a natural language parser based on a small context-free grammar [20].
Reference: [101] <author> A. Newell and H. A. Simon, </author> <title> in Human Problem Solving.. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: The compiler benchmark is expanded from the Algol-like compiler described in Warren's thesis [38] and run with compilations of twelve short procedures. Cryptoarithmetic solves cryptoarithmetic problems utilizing an incremental generate and test search strategy <ref> [101] </ref>. Grammar is a natural language parser based on a small context-free grammar [20]. Hexconversion reads a file of decimal numerals and produces a conversion table showing each decimal numeral and its equivalent hexadecimal representation.
Reference: [102] <author> D. Knuth, </author> <booktitle> in The Art of Computer Programming. p. 394, 1981, 2nd Edition. </booktitle> <pages> 127 </pages>
Reference-contexts: Grammar is a natural language parser based on a small context-free grammar [20]. Hexconversion reads a file of decimal numerals and produces a conversion table showing each decimal numeral and its equivalent hexadecimal representation. The primes benchmark uses the Sieve of Eratosthenes algorithm <ref> [102] </ref> to generate a table of prime numbers. It is adapted from the solution given by Clocksin and Mellish [13]. Queens generates all solutions to the eight-queens problem [99]. Its search strategy improves upon the generate and test technique through use of an agenda.
Reference: [103] <author> E. Tick, </author> <title> ``Prolog Memory-Referencing Behavior,'' </title> <type> Technical Report No. 85-281, </type> <institution> Standford University, Stan-ford, California, </institution> <month> September </month> <year> 1985. </year>
Reference-contexts: These larger loops suggest that Prolog has a lower temporal instruction locality than languages with many highly repetitive, small inner loops. Tick presents an extensive report on Prolog memory-referencing behavior predomi nantly concentrating on Prolog's performance with various cache and buffer configurations <ref> [103, 104] </ref>; however, we are aware of no report that compares more fundamental measures of memory-referencing behavior of Prolog to other languages. <p> Each benchmark needs atom space to hold its symbol table that on average contains two hundred and forty words. We limit our discussion of memory characteristics in this report. The reader is referred to Tick's s tudy for a more comprehensive report <ref> [103, 104] </ref>. 3.3. Quality of the Benchmarks An important and much needed contribution that we add to Prolog implementation research is a new benchmark suite. The benchmarks most often cited in the literature are the eight Warren benchmarks [38] and the six PLM benchmarks [5].
Reference: [104] <author> E. Tick, </author> <title> ``Studies in Prolog Architectures,'' </title> <type> Technical Report No. </type> <institution> CSL-TR-87-329, Standford University, Stanford, California, </institution> <month> June </month> <year> 1987, </year> <type> PhD Disertation. </type>
Reference-contexts: These larger loops suggest that Prolog has a lower temporal instruction locality than languages with many highly repetitive, small inner loops. Tick presents an extensive report on Prolog memory-referencing behavior predomi nantly concentrating on Prolog's performance with various cache and buffer configurations <ref> [103, 104] </ref>; however, we are aware of no report that compares more fundamental measures of memory-referencing behavior of Prolog to other languages. <p> Each benchmark needs atom space to hold its symbol table that on average contains two hundred and forty words. We limit our discussion of memory characteristics in this report. The reader is referred to Tick's s tudy for a more comprehensive report <ref> [103, 104] </ref>. 3.3. Quality of the Benchmarks An important and much needed contribution that we add to Prolog implementation research is a new benchmark suite. The benchmarks most often cited in the literature are the eight Warren benchmarks [38] and the six PLM benchmarks [5]. <p> The search spaces in our programs are broader showing larger frequencies of use of choice point operations. Recently, a new set of benchmarks have appeared in the literature <ref> [104, 7] </ref> which we refer to as the BAM benchmarks. Within the BAM benchmarks are a few programs more similar in size statically or dynamically to our programs. Table 3.13 compares the static and dynamic instruction counts, memory reference count, and memory usage of the different benchmark suites.
Reference: [105] <author> P. M. Kogge, </author> <title> in The Architecture of Pipelined Computers. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: The introduction of push and pop instructions decreases the benchmarks' code size by seven percent and the number of instructions executed by eight percent. CHAPTER 6 Instruction-Level Parallelism Pipelining is an effective technique to increase parallelism at the instruction level <ref> [105] </ref>. Ideally, pipelining offers a speedup equal to the number of pipeline stages introduced in an architecture's implementation. In practice, instruction streams include both control dependencies and data dependencies that limit actual performance because these dependencies reduce the amount of parallelism that can be realized [106].
Reference: [106] <author> S. R. Kunkel and J. E. Smith, </author> <title> ``Optimal Pipelining in Supercomputers,'' </title> <booktitle> in Proceedings of the 13th Annual International Symposium on Computer Architecture. </booktitle> <pages> pp. 404-411, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Ideally, pipelining offers a speedup equal to the number of pipeline stages introduced in an architecture's implementation. In practice, instruction streams include both control dependencies and data dependencies that limit actual performance because these dependencies reduce the amount of parallelism that can be realized <ref> [106] </ref>. A control dependency occurs in the case of every conditional branch or conditional jump because the instruction that will be executed following the branch or jump is determined by the outcome of the branch or jump decision.
Reference: [107] <author> T. Agerwala and J. Cocke, </author> <title> ``High Performance Reduced Instruction Set Processors,'' </title> <type> IBM Technical Report, </type> <month> March </month> <year> 1987. </year>
Reference-contexts: The empty cycles during a pipeline stall act to reduce pipelined performance from ideal levels. Superscalar and very long instruction word (VLIW) machines exploit instruction-level parallelism by initiating multiple operations simultaneously <ref> [107, 108] </ref>. Superscalar machines issue more than one instruction per clock cycle when their hardware can guarrentee the independence of a series of instructions. In a VLIW machine, the compiler creates a package of operations that can be simultaneously issued without additional hardware assistance.
Reference: [108] <author> J. A. Fischer, </author> <title> ``Very Long Instruction Word Architectures and ELI-512,'' </title> <booktitle> in Proceedings of the 10th Symposium on Computer Architecture. </booktitle> <pages> pp. 478-490, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The empty cycles during a pipeline stall act to reduce pipelined performance from ideal levels. Superscalar and very long instruction word (VLIW) machines exploit instruction-level parallelism by initiating multiple operations simultaneously <ref> [107, 108] </ref>. Superscalar machines issue more than one instruction per clock cycle when their hardware can guarrentee the independence of a series of instructions. In a VLIW machine, the compiler creates a package of operations that can be simultaneously issued without additional hardware assistance.

References-found: 106

