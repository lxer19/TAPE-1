URL: http://euler.mcs.utulsa.edu/~sandip/aaai94.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/DAI.html
Root-URL: 
Email: sandip@kolkata.mcs.utulsa.edu  
Title: Learning to coordinate without sharing information  
Author: Sandip Sen, Mahendra Sekaran, and John Hale 
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences University of Tulsa  
Abstract: Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A. B. Barto, R. S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <booktitle> In Proceedings of 1989 Conference on Neural Information Processing, </booktitle> <year> 1989. </year>
Reference: <editor> A. H. Bond and L. Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In a reinforcement learning scenario, an agent chooses actions based on its perceptions, receives scalar feedbacks based on past actions, and is expected to develop a mapping from perceptions to actions that will maximize feedbacks. Multiagent systems are a particular type of distributed AI system <ref> (Bond & Gasser 1988) </ref>, in which autonomous intelligent agents inhabit a world with no global control or globally consistent knowledge. These agents may still need to coordinate their activities with others to achieve their own local goals.
Reference: <editor> P. Brazdil, M. Gams, S. Sian, L. Torgo, and W. van de Velde. </editor> <booktitle> Learning in distributed systems and multi-agent environments. In European Working Session on Learning, Lecture Notes in AI, 482, </booktitle> <address> Berlin, March 1991. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Though the latter form of learning may be more time-consuming, it is generally more robust in the presence of noisy, uncertain, and incomplete information. Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge <ref> (Brazdil et al. 1991) </ref>, or on cooperative domains with unrestricted information sharing (Sian 1991). Even previous work on using reinforcement learning for coordinating multiple agents (Tan 1993; Wei1993) have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: <author> P. R. Cohen and C. R. Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results (Durfee & Lesser 1991), speech acts <ref> (Cohen & Perrault 1979) </ref>, resource availabilities (Smith 1980), etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents (Fox 1981; Genesereth, Ginsberg, & Rosenschein 1986) to aid local decision-making.
Reference: <author> E. H. Durfee and V. R. Lesser. </author> <title> Partial global planning: A coordination framework for distributed hypothesis formation. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5), </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results <ref> (Durfee & Lesser 1991) </ref>, speech acts (Cohen & Perrault 1979), resource availabilities (Smith 1980), etc. to other agents to facilitate the process of coordination.
Reference: <author> M. S. Fox. </author> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80, </pages> <month> Jan. </month> <year> 1981. </year>
Reference: <author> M. Genesereth, M. Ginsberg, and J. Rosenschein. </author> <title> Cooperation without communications. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 51-57, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1986. </year>
Reference: <author> J. H. Holland. </author> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. Michalski, J. Car-bonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, an artificial intelligence approach: Volume II. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> Los Alamos, CA, </address> <year> 1986. </year>
Reference: <author> J. W. Shavlik and T. G. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Cal-ifornia, </address> <year> 1990. </year>
Reference-contexts: This research opens up a new dimension of coordination strategies for multia-gent systems. Acquiring coordination knowledge Researchers in the field of machine learning have investigated a number of schemes for using past experience to improve problem solving behavior <ref> (Shavlik & Diet terich 1990) </ref>. A number of these schemes can be effec-tively used to aid the problem of coordinating multiple agents inhabiting a common environment.
Reference: <author> S. Sian. </author> <title> Adaptation based on cooperative learning in multi-agent systems. </title> <editor> In Y. Demazeau and J.-P. Muller, editors, </editor> <booktitle> Decentralize AI, </booktitle> <volume> volume 2, </volume> <pages> pages 257-272. </pages> <publisher> Elsevier Science Publications, </publisher> <year> 1991. </year>
Reference-contexts: Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge (Brazdil et al. 1991), or on cooperative domains with unrestricted information sharing <ref> (Sian 1991) </ref>. Even previous work on using reinforcement learning for coordinating multiple agents (Tan 1993; Wei1993) have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: <author> R. G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: Almost all of the coordination schemes developed to date assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results (Durfee & Lesser 1991), speech acts (Cohen & Perrault 1979), resource availabilities <ref> (Smith 1980) </ref>, etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents (Fox 1981; Genesereth, Ginsberg, & Rosenschein 1986) to aid local decision-making.
Reference: <author> R. S. Sutton. </author> <title> Integrated architecture for learning, planning, and reacting based on approximate dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-225, </pages> <year> 1990. </year>
Reference-contexts: We plan to use Boltzmann selection of action in place of deterministic action choice to remedy this problem, though this will lead to slower convergence. We also plan to develop mechanisms to incorporate world models to speed up reinforcement learning as proposed by Sutton <ref> (Sutton 1990) </ref>. We are currently investigating the application of reinforcement learning for resource-sharing problems involving non-benevolent agents.
Reference: <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337, </pages> <month> June </month> <year> 1993. </year>
Reference: <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Various reinforcement learning strategies have been proposed using which agents can can develop a policy to maximize rewards accumulated over time. For our experiments, we use the Q-learning <ref> (Watkins 1989) </ref> algorithm, which is designed to find a policy fl that maximizes V fl (s) for all states s 2 S. The decision policy is represented by a function, Q : S fi A 7! &lt;, which estimates long-term discounted rewards for each state-action pair.
Reference: <author> G. Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <month> August </month> <year> 1993. </year>
References-found: 15

