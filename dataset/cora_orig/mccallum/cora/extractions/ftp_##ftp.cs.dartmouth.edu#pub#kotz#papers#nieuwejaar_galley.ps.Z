URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/nieuwejaar:galley.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/galley.html
Root-URL: http://www.cs.dartmouth.edu
Email: fnils,dfkg@cs.dartmouth.edu  
Title: The Galley Parallel File System  
Author: Nils Nieuwejaar David Kotz 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Web: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/nieuwejaar:galley.ps.Z  
Note: Copyright 1996 by ACM. Appeared in International Conference on Supercomputing, May 1996, pp. 374-381. Available at  
Abstract: As the I/O needs of parallel scientific applications increase, file systems for multiprocessors are being designed to provide applications with parallel access to multiple disks. Many parallel file systems present applications with a conventional Unix-like interface that allows the application to access multiple disks transparently. This interface conceals the parallelism within the file system, which increases the ease of programmability, but makes it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs to exploit that parallelism. Furthermore, most current parallel file systems are optimized for a different workload than they are being asked to support. We introduce Galley, a new parallel file system that is intended to efficiently support realistic parallel workloads. We discuss Galley's file structure and application interface, as well as an application that has been implemented using that interface. 
Abstract-found: 1
Intro-found: 1
Reference: [Are91] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: The library could store the compressed data chunks in one fork and index information in another. Another example of the use of this type of file structure may be found in the problem of genome-sequence comparison, which requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs. <p> Another example of the use of this type of file structure may be found in the problem of genome-sequence comparison, which requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stan-fill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Allow easy and efficient implementations of libraries. * Scale to many compute and I/O processors. * Minimize memory and performance overhead. 3 File Structure Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which a file is seen as an addressable, linear sequence of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. The file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), allowing parallel access to the file. This parallel access reduces the effect of the bottleneck imposed by the relatively slow disk speed. <p> For convenience, there are gfs all open, gfs all close, and gfs all delete calls as well. 5.3 Data Operations Most parallel file systems present applications with an application interface similar to that of Unix <ref> [Pie89, RP95, BGST93] </ref>. While this interface is simple and familiar to programmers, it was not designed to allow parallel applications to access parallel disks. <p> CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other parallel file systems with this style of interface are SUNMOS and its successor, PUMA [WMR + 94], sfs [LIN + 93], and CMMD <ref> [BGST93] </ref>. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95]. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Other libraries currently being implemented provide Panda [SCJ + 95] and Vesta [CFP + 95] interfaces. We also plan to implement ViC*, a variant of C* designed for out-of-core computations, on top of Galley <ref> [CC94] </ref>. one active metadata request waiting for the NameServer, and three idle CP threads. 4.2 I/O Processors I/O servers in Galley are composed of several independent units (as shown in Figure 3). Each unit is implemented as a single thread.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: PPFS provides several mapping functions, which correspond to common data distributions, and allows an application to provide its own mapping function as well. One of the most interesting parallel file systems is the Vesta file system (and its commercial version, PIOFS) <ref> [CF94, CFP + 95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra Johnson Bay-lor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and An-thony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <pages> pages 222-248, </pages> <year> 1995. </year>
Reference-contexts: One such library will be one that supports a linear, Unix-like file model, which will reduce the effort required to port applications to Galley. Other libraries currently being implemented provide Panda [SCJ + 95] and Vesta <ref> [CFP + 95] </ref> interfaces. <p> PPFS provides several mapping functions, which correspond to common data distributions, and allows an application to provide its own mapping function as well. One of the most interesting parallel file systems is the Vesta file system (and its commercial version, PIOFS) <ref> [CF94, CFP + 95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: To address these problems, Galley does not automatically decluster an application's data. Instead, Galley provides applications with the ability to fully control this declus-tering according to their own needs. This control is particularly important when implementing I/O-optimal algorithms <ref> [CK93] </ref>. Applications are also able to explicitly indicate which disk they wish to access in each request. To allow this behavior, files are composed of one or more sub-files, each of which resides entirely on a single disk, and which may be directly addressed by the application.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blu-menthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Other parallel file systems with this style of interface are SUNMOS and its successor, PUMA [WMR + 94], sfs [LIN + 93], and CMMD [BGST93]. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface <ref> [HER + 95] </ref>. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte. PPFS maps the logical, linear stream of records onto an underlying two-dimensional model, indexed with a (disk, record) pair.
Reference: [KFG94] <author> John F. Karpovich, James C. French, and An--drew S. Grimshaw. </author> <title> High performance access to radio astronomy data: A case study. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 240-249, </pages> <month> September </month> <year> 1994. </year> <note> Also available as UVA TR CS-94-25. </note>
Reference-contexts: A library that was capable of handling many kinds of queries and FITS files is a perfect example of the type of domain-specific library we expect to be implemented on Galley. 6.1 FITS at NRAO One specific example of how FITS files are used in practice is described in <ref> [KGF93, KFG94] </ref>. This type of FITS file contains records with 6 keys, describing the frequency domain (U; V; W ), the baseline, and the time the data was collected. The baseline is a single number that indicates which antenna or combination of antennas generated that record. <p> For example, a user may want to examine all the records within a given time range, and sorted along the U axis. Previous work on these files has focused on increasing locality along several dimensions simultaneously. In <ref> [KFG94] </ref>, the authors examine studied the effectiveness of Piecewise Linear Order-Preserving Hashing (PLOP) files at reducing the amount of time required to perform common queries, by increasing certain kinds of locality within the files.
Reference: [KGF93] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Breaking the I/O bottleneck at the National Radio Astronomy Observatory (NRAO). </title> <type> Technical Report CS-94-37, </type> <institution> University of Virginia, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: A library that was capable of handling many kinds of queries and FITS files is a perfect example of the type of domain-specific library we expect to be implemented on Galley. 6.1 FITS at NRAO One specific example of how FITS files are used in practice is described in <ref> [KGF93, KFG94] </ref>. This type of FITS file contains records with 6 keys, describing the frequency domain (U; V; W ), the baseline, and the time the data was collected. The baseline is a single number that indicates which antenna or combination of antennas generated that record. <p> To evaluate the efficacy of their PLOP-file implementation, the authors performed several queries, which were intended to be representative of those that were most commonly used in practice at NRAO <ref> [KGF93] </ref>. Their tests were performed on a single-processor, single-disk system. We performed the same set of queries, using the same data set, on our implementation. Our tests were performed on a cluster of IBM RS/6000s connected by an FDDI network. <p> The specific queries performed in both cases are briefly described below. More detail about each query, and why it is commonly used at NRAO, may be found in <ref> [KGF93] </ref>. 1. Read the full data set. 2. Read the full data set, sorting records by time. 3. Read the full data set, sorting records by baseline. 4. Read a subvolume of the data including 10% of the time range. 5.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. the development of most parallel file systems were incorrect <ref> [KN94, NK95, PEK + 95] </ref>. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91].
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Indeed, if an interface were available that allowed an application to issue such highly regular requests, the number of I/O requests issued in one production file-system workload could have been reduced by over 90% [NK95]. Such structured operations can also lead to significant performance improvements <ref> [Kot94] </ref>. In addition to simple read ()/write () operations, Galley supports simple-strided, nested-strided, and nested-batched operations. Descriptions of these operations and the interfaces required to support them may be found in [NK95]. The tremendous performance improvements achieved using these interfaces in Galley are described in [NK96].
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Abstracting with credit is permitted. the development of most parallel file systems were incorrect [KN94, NK95, PEK + 95]. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. <p> Allow easy and efficient implementations of libraries. * Scale to many compute and I/O processors. * Minimize memory and performance overhead. 3 File Structure Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which a file is seen as an addressable, linear sequence of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. The file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), allowing parallel access to the file. This parallel access reduces the effect of the bottleneck imposed by the relatively slow disk speed. <p> CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other parallel file systems with this style of interface are SUNMOS and its successor, PUMA [WMR + 94], sfs <ref> [LIN + 93] </ref>, and CMMD [BGST93]. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95]. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [MHQ96] <author> Jason A. Moore, Phil Hatcher, and Michael J. Quinn. </author> <title> Efficient data-parallel files via automatic mode detection. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-14, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Under Galley, this index could be stored in one fork, while the database itself could be stored in a second fork. A final example of the use of forks is Stream*, a parallel file abstraction for the data-parallel language, C* <ref> [MHQ96] </ref>. Briefly, Stream* divides a file into three distinct segments, each of which corresponds to a particular set of access semantics. Although one could use a different fork for each segment, Stream* was actually designed to store them all in a single file.
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Abstracting with credit is permitted. the development of most parallel file systems were incorrect [KN94, NK95, PEK + 95]. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>.
Reference: [NAS94] <institution> NASA/Science Office of Standards and Technology, NASA Goddard Space Flight Center, </institution> <month> Greensbelt, </month> <title> MD 020771. A User's Guide for the Flexible Image Transport System (FITS), </title> <address> 3.1 edition, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Users may achieve similar results by submitting multiple requests asynchronously, one to each desired fork. 6 Example: FITS We present an example of how the features described above may be used in practice. The Flexible Image Transport System (FITS) data format is a standard format for astronomical data <ref> [NAS94] </ref>. A FITS file begins with an ASCII header that describes the contents of the file and structure of the records in the file. The remainder of the file is a series of records, stored in binary form.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: While many of these were similar to the traditional Unix-style file system, there have been also several more ambitious attempts. Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, and its successor, PFS, are examples of parallel file systems that provide a linear file model with a Unix-like interface. Support for parallel applications is limited to file pointers that may be shared by all the processes in the application.
Reference: [NK95] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In IPPS '95 Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. the development of most parallel file systems were incorrect <ref> [KN94, NK95, PEK + 95] </ref>. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91]. <p> Indeed, if an interface were available that allowed an application to issue such highly regular requests, the number of I/O requests issued in one production file-system workload could have been reduced by over 90% <ref> [NK95] </ref>. Such structured operations can also lead to significant performance improvements [Kot94]. In addition to simple read ()/write () operations, Galley supports simple-strided, nested-strided, and nested-batched operations. Descriptions of these operations and the interfaces required to support them may be found in [NK95]. <p> workload could have been reduced by over 90% <ref> [NK95] </ref>. Such structured operations can also lead to significant performance improvements [Kot94]. In addition to simple read ()/write () operations, Galley supports simple-strided, nested-strided, and nested-batched operations. Descriptions of these operations and the interfaces required to support them may be found in [NK95]. The tremendous performance improvements achieved using these interfaces in Galley are described in [NK96]. In addition to these structured operations, Galley provides a more general file interface, which we call a list interface. This interface accepts an array of (file offset, memory offset, size) triples from the application.
Reference: [NK96] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In this paper we describe the features and design of the system. The performance and scalability of the system are examined in greater detail in <ref> [NK96] </ref>. The remainder of this paper is organized as follows. In Section 2 we describe the specific goals Galley was designed to satisfy. In Section 3 we discuss a new, three-dimensional way to structure files in a parallel file system. Section 4 describes the design and current implementation of Galley. <p> This multi-threading makes it easy for an IOP to service requests from many clients simultaneously. While one potential concern is that this thread-per-CP design may limit the scalability of the system, we have not observed such a limitation in our performance tests <ref> [NK96] </ref>. One may reasonably assume that a thread that is idle (i.e., not actively handling a request) is not likely to noticeably affect the performance of an IOP. <p> In addition to simple read ()/write () operations, Galley supports simple-strided, nested-strided, and nested-batched operations. Descriptions of these operations and the interfaces required to support them may be found in [NK95]. The tremendous performance improvements achieved using these interfaces in Galley are described in <ref> [NK96] </ref>. In addition to these structured operations, Galley provides a more general file interface, which we call a list interface. This interface accepts an array of (file offset, memory offset, size) triples from the application. <p> This freedom allows Galley to transfer the data from the disk to the IOP's memory and from the IOP to the CP in the most efficient order rather than strictly sequentially. This ability to reorder data transfers can lead to remarkable performance gains <ref> [NK96] </ref>, and is a distinct advantage of these interfaces over any interface where the user must request one small piece of data at a time, forcing the file system to service requests in a particular order. <p> Galley provides several new forms of I/O request that reduce the aggregate latency of multiple small requests and allows the file system to optimize the disk accesses required to satisfy the request. The case studies contained in this paper, as well as performance evaluations described elsewhere <ref> [NK96] </ref>, suggest that Galley rectifies many of the shortcomings of existing parallel file systems. In particular, we demonstrated the usefulness of Galley's "fork" structure and higher-level interfaces. Galley has been completely implemented.
Reference: [NKP + 95] <author> Nils Nieuwejaar, David Kotz, Apratim Pu-rakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> Submitted to IEEE TPDS. </note>
Reference-contexts: Studies of two parallel file-system workloads, running a variety of applications in a variety of scientific domains, at two sites on two architectures, under both data-parallel and control-parallel programming models, show that many applications make many small, regular, but non-consecutive requests to the file system <ref> [NKP + 95] </ref>. These studies suggest that most parallel file systems have been optimized for a workload that is very different than that which actually exists. <p> However, the declustering unit size is frequently measured in kilobytes (e.g., 4KB in Intel's CFS [Pie89]), while our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes <ref> [NKP + 95] </ref>. This disparity means that most of the individual requests generated by parallel applications are not being executed in parallel. <p> While this interface is simple and familiar to programmers, it was not designed to allow parallel applications to access parallel disks. In particular, it does not allow programmers to issue the highly structured requests that we have observed to be common among parallel, scientific applications <ref> [NKP + 95] </ref>. Indeed, if an interface were available that allowed an application to issue such highly regular requests, the number of I/O requests issued in one production file-system workload could have been reduced by over 90% [NK95]. Such structured operations can also lead to significant performance improvements [Kot94].
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. the development of most parallel file systems were incorrect <ref> [KN94, NK95, PEK + 95] </ref>. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91].
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: Abstracting with credit is permitted. the development of most parallel file systems were incorrect [KN94, NK95, PEK + 95]. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Abstracting with credit is permitted. the development of most parallel file systems were incorrect [KN94, NK95, PEK + 95]. It was commonly believed that parallel scientific applications would behave like sequential and vector scientific applications: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. <p> Allow easy and efficient implementations of libraries. * Scale to many compute and I/O processors. * Minimize memory and performance overhead. 3 File Structure Most existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which a file is seen as an addressable, linear sequence of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. The file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), allowing parallel access to the file. This parallel access reduces the effect of the bottleneck imposed by the relatively slow disk speed. <p> However, the declustering unit size is frequently measured in kilobytes (e.g., 4KB in Intel's CFS <ref> [Pie89] </ref>), while our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes [NKP + 95]. This disparity means that most of the individual requests generated by parallel applications are not being executed in parallel. <p> For convenience, there are gfs all open, gfs all close, and gfs all delete calls as well. 5.3 Data Operations Most parallel file systems present applications with an application interface similar to that of Unix <ref> [Pie89, RP95, BGST93] </ref>. While this interface is simple and familiar to programmers, it was not designed to allow parallel applications to access parallel disks. <p> While many of these were similar to the traditional Unix-style file system, there have been also several more ambitious attempts. Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, and its successor, PFS, are examples of parallel file systems that provide a linear file model with a Unix-like interface. Support for parallel applications is limited to file pointers that may be shared by all the processes in the application.
Reference: [RP95] <author> Brad Rullman and David Payne. </author> <title> An efficient file I/O interface for parallel applications. DRAFT presented at the Workshop on Scalable I/O, </title> <booktitle> Frontiers '95, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: For convenience, there are gfs all open, gfs all close, and gfs all delete calls as well. 5.3 Data Operations Most parallel file systems present applications with an application interface similar to that of Unix <ref> [Pie89, RP95, BGST93] </ref>. While this interface is simple and familiar to programmers, it was not designed to allow parallel applications to access parallel disks.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We expect that most applications will use a higher-level library or language layered above the Galley run-time library. One such library will be one that supports a linear, Unix-like file model, which will reduce the effort required to port applications to Galley. Other libraries currently being implemented provide Panda <ref> [SCJ + 95] </ref> and Vesta [CFP + 95] interfaces.
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ouster-hout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the 1990 Winter USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: Galley uses 32 KB as its disk block size. As new requests arrive from the CacheManager, they are placed into the list according to the disk scheduling algorithm. The DiskManager currently uses a Cyclical Scan algorithm <ref> [SCO90] </ref>. When a block has been read from disk, the DiskManager updates the cache status of that block from `not ready' to `ready', and notifies any threads that may have been waiting for that block. For portability, Galley does not use a low-level driver to directly access the disk.
Reference: [SW95] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Forks are most likely to be useful when implementing libraries. In addition to data in the traditional sense, many libraries also need to store persistent, library-specific `meta-data' independently of the data proper. One example of such a library would be a compression library similar to that described in <ref> [SW95] </ref>, which compresses a data file in multiple independent chunks. The library could store the compressed data chunks in one fork and index information in another.
Reference: [TG96] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The portion of the file residing on disk 0 is shown in greater detail than the portions on the other two disks. out-of-core linear-algebra library <ref> [TG96] </ref>. This data layout is intended to efficiently support a wide variety of out-of-core algorithms. In particular, it allows blocks of rows and columns to be transferred efficiently, as well as square or nearly-square submatrices. To address these problems, Galley does not automatically decluster an application's data.
Reference: [WMR + 94] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year>
Reference-contexts: CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other parallel file systems with this style of interface are SUNMOS and its successor, PUMA <ref> [WMR + 94] </ref>, sfs [LIN + 93], and CMMD [BGST93]. PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95].
References-found: 28

