URL: http://www.cs.berkeley.edu/~ang/papers/pomdp.ps
Refering-URL: http://www.cs.berkeley.edu/~ang/
Root-URL: http://www.cs.berkeley.edu/~ang/
Email: E-mail: mkearns@research.att.com.  
Title: Approximate Planning in Large POMDPs via Reusable Trajectories  keywords: Reinforcement Learning, POMDPs, Planning, VC Dimension  
Author: Michael Kearns Yishay Mansour Andrew Y. Ng 
Address: Room A235, 180 Park Avenue, Florham Park, New Jersey, 07932.  
Note: Contact author. Address: AT&T Labs,  On sabbatical from  
Date: January 27, 1999  
Affiliation: AT&T Labs  AT&T Labs  UC Berkeley  Tel Aviv University.  
Abstract: We consider the problem of choosing a near-best strategy from a restricted class of strategies in a partially observable Markov decision process (POMDP). We assume we are given the ability to simulate the behavior of the POMDP, and we provide methods for generating simulated experience sufficient to accurately approximate the expected return of any strategy in the class . We prove upper bounds on the amount of simulated experience our methods must generate in order to achieve such uniform approximation. These bounds have no dependence on the size or complexity of the underlying POMDP, but depend only on the complexity of the restricted strategy class . The main challenge is in generating trajectories in the POMDP that can be reused , in the sense that they simultaneously provide estimates of the return of many strategies in the class. Our measure of strategy class complexity generalizes the classical notion of VC dimension, and our methods develop connections between problems of current interest in reinforcement learning and well-studied issues in the theory of supervised learning. We also discuss a number of practical planning algorithms for POMDPs that arise from our reusable trajectories. 
Abstract-found: 1
Intro-found: 1
Reference: [BDH99] <author> C. Boutilier, T. Dean, and S. Hanks. </author> <title> Decision theoretic planning: Structural assumptions and computational leverage. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> 1999. To appear. </note>
Reference-contexts: To intelligently discuss the problem of planning | that is, computing a good strategy in a given POMDP | compact or implicit representations of POMDPs (such as representations in which the next-state distributions can be factored <ref> [BDH99, BK98] </ref>) must be developed. Second, even a compact representation of a POMDP is no guarantee that a good strategy in that POMDP has a compact representation.
Reference: [BK98] <author> X. Boyen and D. Koller. </author> <title> Tractable inference for complex stochastic processes. </title> <booktitle> In Proc. UAI, </booktitle> <pages> pages 33-42, </pages> <year> 1998. </year>
Reference-contexts: To intelligently discuss the problem of planning | that is, computing a good strategy in a given POMDP | compact or implicit representations of POMDPs (such as representations in which the next-state distributions can be factored <ref> [BDH99, BK98] </ref>) must be developed. Second, even a compact representation of a POMDP is no guarantee that a good strategy in that POMDP has a compact representation.
Reference: [CB96] <author> R. Crites and A. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1017-1023, </pages> <year> 1996. </year>
Reference-contexts: We can really only use the generative model to find a strategy that maps from (histories of) observables to actions. As a concrete example, in designing an elevator control system <ref> [CB96] </ref>, we may have access to a simulator that generates random rider arrival times, and keeps track of the waiting time of each rider, the number of riders waiting at every floor at every time of day, and so on.
Reference: [Hau92] <author> David Haussler. </author> <title> Decision-theoretic generalizations of the PAC model for neural networks and other applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: For real-valued classes H, the notions of "shattering" or "exhaustive" behavior on a finite set of points must already be generalized in a way that is beyond the scope of this paper <ref> [Hau92] </ref>. But by analogy, in the full paper we define a notion of a set T 1 ; : : : ; T d of trajectory trees being "shattered" by the strategy class . <p> Generalizations to multiple actions or stochastic strategies require the machinery of combinatorial dimension or pseudo-dimension <ref> [Hau92] </ref>. Suppose is an infinite class of deterministic strategies in a two-action POMDP.
Reference: [KMN99] <author> M. Kearns, Y. Mansour, and A. Ng. </author> <title> A sparse sampling algorithm for near-optimal planning in large Markov decision processes. 1999. </title> <type> Unpublished manuscript. </type>
Reference-contexts: However, the key point is that algorithms pro-vided with this generative model must still find a strategy that performs well in the partially observable setting. For instance, although we could simply use the generative model to find a near-optimal policy (state-to-action mapping) for the underlying MDP <ref> [KMN99] </ref>, this policy will be useless in the POMDP, where the state is unknown. We can really only use the generative model to find a strategy that maps from (histories of) observables to actions.
Reference: [KYK95] <author> H. Kimura, M. Yamamura, and S. Kobayashi. </author> <title> Reinforcement learning by stochastic hill climbing on discounted reward. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pages 295-303, </pages> <year> 1995. </year>
Reference-contexts: unbiased estimate of the gradient (d=d) ^ V (s 0 ) evaluated at 0 . (Details in the full paper.) Having come this far, there is also a simple further modification to the subsampling algorithm that leads us close to a line of research pursued by Kimura, Yamamura and Kobayashi <ref> [KYK95] </ref>, and which gives a procedure also bearing some similarity to William's reinforce algorithm [Wil92]. Rather than doing stochastic gradient ascent on ^ V (s 0 ), it is possible to perform stochastic gradient ascent directly on the true value function V (s 0 ).
Reference: [SB98] <author> Richard S. Sutton and Andrew G. Barto. </author> <title> Reinforcement Learning. </title> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: Second, even a compact representation of a POMDP is no guarantee that a good strategy in that POMDP has a compact representation. Thus, we must also be prepared to consider compact representations for strategies (such as those typically considered when using function approximation in standard MDPs <ref> [SB98] </ref>). <p> We note that the generalization of this estimate to the case of stochastic strategies can be viewed as a form of importance sampling <ref> [SB98] </ref>.
Reference: [Wil92] <author> Ronald J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256, </pages> <year> 1992. </year> <month> 16 </month>
Reference-contexts: . (Details in the full paper.) Having come this far, there is also a simple further modification to the subsampling algorithm that leads us close to a line of research pursued by Kimura, Yamamura and Kobayashi [KYK95], and which gives a procedure also bearing some similarity to William's reinforce algorithm <ref> [Wil92] </ref>. Rather than doing stochastic gradient ascent on ^ V (s 0 ), it is possible to perform stochastic gradient ascent directly on the true value function V (s 0 ).
References-found: 8

