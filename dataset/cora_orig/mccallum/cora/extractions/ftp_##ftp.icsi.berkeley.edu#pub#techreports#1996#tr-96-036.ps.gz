URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1996/tr-96-036.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1996.html
Root-URL: http://www.icsi.berkeley.edu
Title: Adaptive load sharing based on a broker module  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: M. Avvenuti L. Rizzo, and L. Vicisano 
Note: Email: marco@iet.unipi.it  
Date: August 1996  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-96-036  
Abstract: This paper describes a dynamic, symmetrically-initiated load sharing scheme which adapts to changing load condition by varying the algorithm's dependency on system's status information. The scheme is hybrid in that it relies on a a fully distributed algorithm when the system is heavily loaded, but resorts to a centrally coordinated location policy when parts of the system become idle. The simplicity of the algorithms proposed makes it possible to use a centralized component without incurring in scalability problems and presenting instabilities. Both algorithms are very lightweight and do not need any tuning of parameters, so that they are extremely easy to implement to the point that an inexpensive hardware implementation of the centralized component is capable of handling millions of requests per second. Simulations show that the hybrid approach outperforms existing dynamic algorithms under all load conditions and task generation patterns, it is weakly sensitive to processing overhead and communication delays, and scales well (to hundreds of nodes) despite the use of a centralized component. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.Avvenuti, L.Rizzo, and L.Vicisano, </author> <title> "Hardware support for load sharing in parallel systems", </title> <journal> Journal of Systems Architecture, </journal> <note> (to appear). </note>
Reference-contexts: That can be achieved with the hardware implementation of the broker component briefly described in the following (for a more comprehensive discussion see <ref> [1] </ref>). In order to adapt the broker to a hardware implementation, the algorithm must be changed slightly from the description given in Section 3.1. The reason is twofold: First, our system model is not anymore a message-passing one, but rather a shared-memory one.
Reference: [2] <author> T.L.Casavant, and J.G.Kuhl, </author> <title> "Effects of Response and Stability on Scheduling in Distributed Computing Systems", </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol 14, No 11, pp.1578-1587, </volume> <month> Nov </month> <year> 1988. </year>
Reference-contexts: avoiding algorithm's instability, which may occur with particular load configurations because the algorithm enters a state where it spends all of the system's time in doing useless activities (such as looking for an idle node on a busy system, or looking for a busy node in an almost idle system) <ref> [2] </ref>. In this paper we describe a hybrid, adaptive load sharing scheme able to act quickly and efficiently in situations where some nodes are becoming idle, and to achieve a weaker but very cheap load balancing when the system is saturated.
Reference: [3] <author> R.B.Cooper, </author> <title> Introduction to Queuing Theory, 2 nd ed., London, </title> <editor> E. Arnold, </editor> <year> 1981. </year>
Reference-contexts: However, nodes compete to access the bus where the broker is connected. Thus the actual service time of the broker depends on the traffic on this communication channel. We can model this system, with good precision, as an M/D/1 queue. The expected waiting time is thus <ref> [3] </ref> E (w) = 1 2 where t bus is the bus service time, is the frequency of requests, and = t bus . The actual service time thus becomes E (w) + t bus . <p> In turn, in the case of zero processing overhead and communication latency, the broker component of HBT can be viewed as a single queue model with a non-biased queuing discipline <ref> [3] </ref>, so we obtained the same performance of the M/M/k system. 0, fl m = fl tp = fl tnp = latency.
Reference: [4] <institution> ES2 ECPD10 Library Databook, European Silicon Structures Inc., </institution> <year> 1994. </year>
Reference-contexts: To get some realistic numbers, we have to size the RAM, and determine its access speed using state-of-the art technology, such as the one made available by ES2 <ref> [4] </ref>. The queue field of the data structure is 2-bit wide. All the other dimensions depend on the number of nodes that must be supported by a single broker.
Reference: [5] <author> D.L. Eager, E.D. Lazowska, and J. Zahorjan, </author> <title> "A Comparison of Receiver-Initiated and Sender-Initiated Adaptive Load Sharing", </title> <journal> Performance Evaluation, </journal> <volume> Vol 6, No 1, pp.53-68, </volume> <month> March </month> <year> 1986. </year> <month> 21 </month>
Reference-contexts: The references quoted in this paper constitute a small but hopefully representative part of them. In this Section we briefly describe three of the most well known dynamic load sharing algorithms: the sender initiated [6], the receiver initiated <ref> [5, 14] </ref> and the stable symmetrically initiated [14], with the aim to provide terms of comparison to evaluate the behavior of our algorithm. The three algorithms under consideration work in a fully distributed manner.
Reference: [6] <author> D.L. Eager, E.D. Lazowska, and J. Zahorjan, </author> <title> "Adaptive Load Sharing in Homogeneous Dis--tributed Systems", </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol 12, No 5, pp.662-675, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: The references quoted in this paper constitute a small but hopefully representative part of them. In this Section we briefly describe three of the most well known dynamic load sharing algorithms: the sender initiated <ref> [6] </ref>, the receiver initiated [5, 14] and the stable symmetrically initiated [14], with the aim to provide terms of comparison to evaluate the behavior of our algorithm. The three algorithms under consideration work in a fully distributed manner. <p> Experimental studies have shown that simple load distributing policies such as random placement and cyclic splitting yield good performance improvement, and that using more informed algorithms with high load levels might even be detrimental to performance, because of the waste of resources for maintaining information on system's status <ref> [6, 15, 17] </ref>; * in a lightly loaded system, i.e. a system where the probability of having idle nodes is high, it can be worth spending additional resources for running more information-dependent, possibly centrally coordinated algorithms. <p> Also, it is well known by literature that complex policies collecting large amounts of information do not sensibly improve the performance yielded by simpler policies <ref> [6] </ref>. <p> M/M/k mod takes into account transfer overheads for those tasks that have to be necessarily moved. It is based on a similar model proposed by Eager et al. in <ref> [6] </ref>. M/M/k mod is obtained introducing in M/M/k the task transfer cost, as an increase in task processing time. To do that, we evaluate the minimum number of task transfers required to perform load sharing regardless the algorithm used.
Reference: [7] <author> W.Fischer and K.Meire-Hellstern, </author> <title> "The Markov-modulated Poisson process (MMPP) cookbook", </title> <journal> Performance Evaluation North-Holland, </journal> <volume> Vol 18, No 2, </volume> <pages> pp 149-171, </pages> <year> 1993. </year>
Reference-contexts: We have assumed exponentially distributed task service times with unit average value (S = 1). As far as task arrivals at nodes are concerned, we used both Poisson arrivals and IPP arrivals <ref> [7] </ref> (to model burst workload). In addition to processing tasks, each node also has to process messages related to the broker algorithm, and to do some small amount of work to handle tickets (no additional communication is required by the ticket algorithm). <p> An IPP is a Poisson process which is alternately turned on for an exponentially distributed length of time (with mean T on ) and then turned off an other exponentially distributed time (with mean T off ) <ref> [7] </ref>. <p> This way the average generation rate per node is still regardless the actual value of . offered workload = :8. The lower curve is the response time of the ideal MMPP/M/k system whose input process (MMPP) is obtained from the superposition of all the IPPs at nodes <ref> [7] </ref>. As we can see the response time of HBT algorithm is essentially that of the ideal case plus a (nearly constant) overhead. SSI algorithm presents a slowly increasing overhead as the duty-cycle decreases, while RI and SI quickly diverge from the ideal behavior.
Reference: [8] <author> P. Krueger, and M.Livny, </author> <title> "The Diverse Objectives of Distributed Scheduling Policies", </title> <booktitle> Proc. of the 7 th International Conference on Distributed Computing Systems, </booktitle> <address> pp.242-249, </address> <month> Sept </month> <year> 1987. </year>
Reference-contexts: When no a priori assumption can be made on the task service time and the system's behavior, load distributing has to be performed at run-time by some dynamic algorithm that arranges for task transfer between nodes based on information on the system's status <ref> [8] </ref>. A distributed system may experience highly variable load conditions, including periods during which a few nodes generate very heavy workload. This entails for dynamic load sharing algorithms to behave adaptively, i.e. to dynamically change their parameters and/or policies to suit the system's changing status [11].
Reference: [9] <author> T.Kunz, </author> <title> "The Influence of Different Workload Descriptions on a Heuristic Load Balancing Scheme", </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol 17, No 7, pp.725-730, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: The load index is the number of pending tasks at nodes; this can be correlated to the CPU queue length, which is generally considered a good workload descriptor <ref> [9, 18] </ref>, and is very cheap to evaluate. The performance index is the average response time for tasks. The scheme deals with tasks eligible for migration, provided that a task migration facility able to move tasks either non-preemptively and preemptively is available in the system.
Reference: [10] <author> M.Livny, and M.Melman, </author> <title> "Load Balancing in Homogeneous Broadcast Distributed Systems", </title> <journal> ACM Performance Evaluation Review, </journal> <volume> Vol 11, No 1, pp.47-55, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: Load sharing strategies are aimed at reducing the average response time of tasks by avoiding unshared states, i.e. situations in which some nodes lie idle while tasks contend for service at some other nodes <ref> [10] </ref>. When no a priori assumption can be made on the task service time and the system's behavior, load distributing has to be performed at run-time by some dynamic algorithm that arranges for task transfer between nodes based on information on the system's status [8]. <p> M/M/k mod is obtained introducing in M/M/k the task transfer cost, as an increase in task processing time. To do that, we evaluate the minimum number of task transfers required to perform load sharing regardless the algorithm used. As pointed out in <ref> [10] </ref>, a task must be necessarily transferred when a completion occurs in a system having more than k task (receiver-initiated transfer), and when a new task arrive in a system with less than k task (sender-initiated transfer).
Reference: [11] <author> R. Mirchandaney, D.Towsley, and J.A.Stankovic, </author> <title> "Adaptive load sharing in heterogeneous distributed systems", </title> <journal> J. Parallel & Distributed Computing, </journal> <volume> Vol 9, No 4, pp.331-346, </volume> <month> Aug </month> <year> 1990. </year>
Reference-contexts: A distributed system may experience highly variable load conditions, including periods during which a few nodes generate very heavy workload. This entails for dynamic load sharing algorithms to behave adaptively, i.e. to dynamically change their parameters and/or policies to suit the system's changing status <ref> [11] </ref>.
Reference: [12] <author> L. </author> <title> Rizzo, "Simulation and performance evaluation of parallel software on multiprocessor systems", </title> <journal> Microprocessors & Microsystems, </journal> <volume> Vol 13, No 1, pp.39-46, </volume> <month> Jan/Feb </month> <year> 1989. </year>
Reference-contexts: Simulation has been carried out with psim <ref> [12] </ref>, a software that allows us to simulate the execution of a parallel program on a distributed memory system. psim lets the user define system features such as the number of processors, the processing speed of individual processors, the execution time of sections of code, the communication time. psim allows writing
Reference: [13] <author> C.G. Rommel, </author> <title> "The Probability of Load Balancing Success in a Homogeneous Network", </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol 17, No 9, pp.922-933, </volume> <month> Sept </month> <year> 1991. </year>
Reference-contexts: This is because the performance of a lightly loaded system is more sensitive to the effects of inappropriate task transfers which may result from uncoordinated location policies <ref> [13] </ref>. The solution we propose consists in a symmetrically-initiated algorithm that adapts to changing load condition by varying the algorithm's dependency on system's status information. This means that, as the system load increases, the algorithm becomes more lightweight by degrading the accuracy with which it balances the load.
Reference: [14] <author> N.G. Shivaratri, and P. Krueger, </author> <title> "Two Adaptive Location Policies for Global Scheduling", </title> <booktitle> Proc. 10 th Int'l Conf. Distributed Computing Systems, </booktitle> <publisher> IEEE CS Press, </publisher> <address> pp.502-509, </address> <year> 1990. </year>
Reference-contexts: The references quoted in this paper constitute a small but hopefully representative part of them. In this Section we briefly describe three of the most well known dynamic load sharing algorithms: the sender initiated [6], the receiver initiated <ref> [5, 14] </ref> and the stable symmetrically initiated [14], with the aim to provide terms of comparison to evaluate the behavior of our algorithm. The three algorithms under consideration work in a fully distributed manner. <p> The references quoted in this paper constitute a small but hopefully representative part of them. In this Section we briefly describe three of the most well known dynamic load sharing algorithms: the sender initiated [6], the receiver initiated [5, 14] and the stable symmetrically initiated <ref> [14] </ref>, with the aim to provide terms of comparison to evaluate the behavior of our algorithm. The three algorithms under consideration work in a fully distributed manner.
Reference: [15] <author> N.G. Shivaratri, P. Krueger, M. Singhal, </author> <title> "Load Distributing for Locally Distributed Systems", </title> <journal> IEEE Computer, </journal> <volume> Vol 25, No 12, pp.33-45, </volume> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Finally, we show comparisons with other algorithms, including the behavior in response to bursty workload. 1.1 Dynamic Load Sharing A large amount of load sharing algorithms have been devised in the last years <ref> [15, 16, 17] </ref>. The references quoted in this paper constitute a small but hopefully representative part of them. <p> Experimental studies have shown that simple load distributing policies such as random placement and cyclic splitting yield good performance improvement, and that using more informed algorithms with high load levels might even be detrimental to performance, because of the waste of resources for maintaining information on system's status <ref> [6, 15, 17] </ref>; * in a lightly loaded system, i.e. a system where the probability of having idle nodes is high, it can be worth spending additional resources for running more information-dependent, possibly centrally coordinated algorithms.
Reference: [16] <author> M.Singhal, N.G. Shivaratri, </author> <title> Advanced Concepts in Operating Systems, </title> <publisher> McGraw-Hill, </publisher> <year> 1994, </year> <note> Chap. 11. </note>
Reference-contexts: Finally, we show comparisons with other algorithms, including the behavior in response to bursty workload. 1.1 Dynamic Load Sharing A large amount of load sharing algorithms have been devised in the last years <ref> [15, 16, 17] </ref>. The references quoted in this paper constitute a small but hopefully representative part of them.
Reference: [17] <author> Y.Wang, R.J.T.Morris, </author> <title> "Load Sharing in Distributed Systems", </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol C-34, No 3, pp.204-217, </volume> <month> March </month> <year> 1985. </year>
Reference-contexts: Finally, we show comparisons with other algorithms, including the behavior in response to bursty workload. 1.1 Dynamic Load Sharing A large amount of load sharing algorithms have been devised in the last years <ref> [15, 16, 17] </ref>. The references quoted in this paper constitute a small but hopefully representative part of them. <p> Experimental studies have shown that simple load distributing policies such as random placement and cyclic splitting yield good performance improvement, and that using more informed algorithms with high load levels might even be detrimental to performance, because of the waste of resources for maintaining information on system's status <ref> [6, 15, 17] </ref>; * in a lightly loaded system, i.e. a system where the probability of having idle nodes is high, it can be worth spending additional resources for running more information-dependent, possibly centrally coordinated algorithms.
Reference: [18] <author> S. Zhou, </author> <title> "An Experimental Assessment of Resource Queue Lengths as Load Indices", </title> <booktitle> Proc. of the 1987 Winter USENIX Conference, </booktitle> <address> Washington, D.C., pp.73-82, </address> <month> Jan </month> <year> 1987. </year> <pages> 22 23 24 </pages>
Reference-contexts: The load index is the number of pending tasks at nodes; this can be correlated to the CPU queue length, which is generally considered a good workload descriptor <ref> [9, 18] </ref>, and is very cheap to evaluate. The performance index is the average response time for tasks. The scheme deals with tasks eligible for migration, provided that a task migration facility able to move tasks either non-preemptively and preemptively is available in the system.
References-found: 18

