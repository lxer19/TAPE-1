URL: http://www.isi.edu/~crago/hidisc/tr-96-07.ps
Refering-URL: http://www.isi.edu/~crago/hidisc/papers.html
Root-URL: http://www.isi.edu
Email: crago@isi.edu  apoorv@isi.edu  obenland@isi.edu  despain@isi.edu  
Phone: (310) 822-1511 ext. 713  (310) 822-1511 ext. 102  (310) 822-1511 ext. 449  (310) 822-1511 ext. 377  
Title: A High-Performance, Hierarchical Decoupled Architecture  
Author: Stephen P. Crago Apoorv Srivastava Kevin Obenland Alvin M. Despain 
Date: November 1, 1996  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: University of Southern California Information Sciences Institute  
Abstract-found: 0
Intro-found: 1
Reference: [AgCo87] <author> T.Y. Agawala and J. Cocke. </author> <title> High Performance Reduced Instruction Set Processors. </title> <institution> IBM T.J. Watson Research Center, </institution> <type> Technical Report #55845, </type> <month> March </month> <year> 1987. </year>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar <ref> [AgCo87] </ref> paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [BaCh91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <year> 1991. </year>
Reference-contexts: Related Work Hardware and software schemes have attempted to bridge the gap between processor and memory speeds. Hardware schemes, such as hardware prefetching and multithreading, adapt dynamically to run-time behavior. However, hardware schemes are hardwired and cannot adapt to individual November 1, 1996 4 programs. Hardware prefetching <ref> [BaCh91] </ref> tries to reduce the miss ratio of the cache by prefetch-ing additional data when a miss occurs. Stride-based accesses can be accommodated by hardware prefetching because the memory access pattern is static over the course of a program.
Reference: [Brig74] <author> E.O. Brigham. </author> <title> The Fast Fourier Transform, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1974, </year> <note> p. 165. </note>
Reference-contexts: The simulator does not model cache or memory contention. If sufficient memory bandwidth is not provided, no processor architecture will provide significant speedup. The benchmarks we used are C versions of the first five Livermore Loops [McMa72], discrete convolution, and the bit-reverse access pattern used in the FFT <ref> [Brig74] </ref>. The benchmarks were compiled on a MIPS R5000-based SGI Indy running IRIX 5.3. We compiled using the cc compiler with the -O3 -sopt and -non_shared options. Loop unrolling was performed on all loops.
Reference: [Denn74] <author> R. Dennard et al. </author> <title> Design of Ion-Implanted MOSFETs with very small physical dimensions. </title> <journal> IEEE Journal of Solid-State Circuits, v. CS-9, </journal> <volume> No. 5, </volume> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster <ref> [Denn74] </ref> and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94].
Reference: [FaTP94] <author> M. Farrens, G. Tyson, and A.R. Pleszkun. </author> <title> A Study of Single-Chip Processor/ Cache Organizations for Large Number of Transistors. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 338-347. </pages>
Reference-contexts: Processors have used these additional transistors by adding multiple functional units and highly pipelined modules. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors <ref> [FaTP94] </ref>. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed.
Reference: [Fish83] <author> Fisher, J.A. </author> <title> Very long instruction word architectures and ELSI-512. </title> <booktitle> Proceedings of the Tenth Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 140-150. </pages>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW <ref> [Fish83] </ref> or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [GHLP85] <author> J.R. Goodman, J.T. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, and H.C. Young. </author> <title> PIPE: a VLSI decoupled architecture. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1985, </year> <pages> pp. 20-27. </pages>
Reference-contexts: Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The SMA [PlDa83], PIPE <ref> [GHLP85] </ref>, DEAP [KuHC94], and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [JRHC95] <author> L.K. John, V. Reddy, P.T. Hulina, and L.D. Coraor. </author> <title> Program balance and its impact on high performance RISC architectures. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The floating point computations only take one or two cycles each, while each floating point operand fetched from the cache takes two cycles to put into the load queue. Previous studies have shown that access instructions comprise more than half of the instructions of many floating point programs <ref> [JRHC95] </ref>. The high miss rates exasperates the problem. The uniprocessor with prefetching shows better performance because of the low prefetch instruction overhead of these benchmarks and the ability to hide much of the memory latency.
Reference: [Joup90] <author> N.P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <type> Technical Note TN-14, </type> <institution> Digital Western Research Laboratory, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The caches are 2-way set associative (except for LLL5 for which we used 4-way) data cache to reduce the effect of prefetches conflicting with useful data, which could also be reduced using a victim cache <ref> [Joup90] </ref>. The cache sizes are small because they allow us to simulate the effects of large data sets while keeping the simulation time low. We believe that similar results would be attained with larger caches and larger data sets.
Reference: [KuHC94] <author> L. Kurian, P.T. Hulina, and L.D. Caraor. </author> <title> Memory latency effects in decoupled architectures. </title> <journal> IEEE Transactions on Computers, v. </journal> <volume> 43, no. </volume> <month> 10 (October </month> <year> 1994), </year> <pages> pp. 1129-1139. </pages>
Reference-contexts: Many applications do not have enough locality to reap the benefit of a cache. Many signal processing applications, for example, have been shown to perform worse with a cache than without one because there is a penalty associated with cache misses <ref> [KuHC94] </ref>. When each datum is used only once, the penalty is often not offset by a sufficient benefit. <p> Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The SMA [PlDa83], PIPE [GHLP85], DEAP <ref> [KuHC94] </ref>, and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [Lam88] <author> M.S. Lam. </author> <title> Software pipelining: an effective scheduling technique for VLIW machines. </title> <booktitle> Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages> <month> November 1, </month> <year> 1996 </year> <month> 19 </month>
Reference-contexts: These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone. Software pipelining <ref> [Lam88] </ref> is a technique to expose more parallelism so that computation can be overlapped. Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining.
Reference: [McMa72] <author> F.H. McMahon. </author> <title> Fortran CPU Performance Analysis. </title> <institution> Lawrence Livermore Laboratories, </institution> <year> 1972. </year>
Reference-contexts: The simulator does not model cache or memory contention. If sufficient memory bandwidth is not provided, no processor architecture will provide significant speedup. The benchmarks we used are C versions of the first five Livermore Loops <ref> [McMa72] </ref>, discrete convolution, and the bit-reverse access pattern used in the FFT [Brig74]. The benchmarks were compiled on a MIPS R5000-based SGI Indy running IRIX 5.3. We compiled using the cc compiler with the -O3 -sopt and -non_shared options. Loop unrolling was performed on all loops.
Reference: [Mips95] <institution> MIPS Technologies, Inc. </institution> <note> MIPS R10000 Microprocessor Users Manual, </note> <year> 1995. </year>
Reference-contexts: It also models the data cache, the main memory, a prefetch buffer, and the queues. The simulator allows overlapped execution of integer and floating point instructions and models interlocks for integer multiply and divide and the latencies and issue rates of floating point operations of the MIPS R10000 <ref> [Mips95] </ref>. The simulator models infinite Load, Store Data and Address Queues, a Slip Control Queue with three to six tokens, and a prefetch buffer that can hold 20 entries. The simulator does not model cache or memory contention.
Reference: [MoLG92] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: Software schemes, such as software prefetching and software controlled caches, expose the memory hierarchy to the compiler. The compiler can adapt the memory hierarchy behavior to the individual program. However, software schemes incur an instruction overhead and cannot adapt to dynamic run-time behavior. Software prefetching <ref> [MoLG92] </ref> allows prefetch instructions to be inserted into the instruction stream and hides memory latency by loading data into the cache before it is needed. Prefetching initiates data accesses a fixed number of instructions before the data is needed and works well only when memory behavior is predictable. <p> It should also be noted that these benchmarks (except for bit-reverse) introduce very little prefetching instruction overhead. In all cases, only prefetch instructions were added. No additional address calculation was necessary, in contrast with many other benchmarks in which prefetching adds a significant (but not prohibitive) instruction overhead <ref> [MoLG92] </ref>. The Slip Control Queue length was set to either three or six depending on the benchmark, and was not changed as the miss latency changed.
Reference: [Papo80] <author> A. Papoulis. </author> <title> Circuits and Systems, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1980, </year> <note> p. 146. </note>
Reference-contexts: Table 3 shows the instruction extensions of the CMP. 3.4 Sample Program In this section, we use the inner loop of the discrete convolution algorithm <ref> [Papo80] </ref> to illustrate how a program runs on the HiDISC architecture. Figure 3 shows the inner loop of the discrete con volution algorithm in C and shows the three instruction streams that would execute on HiDISC in a C-based pseudocode.
Reference: [PlDa83] <author> A.R. Pleszkun and E.S. Davidson. </author> <title> Structured Memory Access Architecture. </title> <booktitle> Proceedings of the 1983 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1983, </year> <pages> pp. 461-471. </pages>
Reference-contexts: Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The SMA <ref> [PlDa83] </ref>, PIPE [GHLP85], DEAP [KuHC94], and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [Sano94] <author> B.J. Sano. </author> <title> Microparallel Processors. </title> <type> Ph.D. Thesis, </type> <institution> University of Southern Cali-fornia, </institution> <year> 1994. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years <ref> [Sano94] </ref>. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
Reference: [SoDa84] <author> G.S. Sohi and E.S. Davidson. </author> <title> Performance of the structured memory access architecture. </title> <booktitle> Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1984, </year> <pages> pp. 506-513. </pages>
Reference: [Smit89] <author> J. Smith. </author> <title> Dynamic Instruction Scheduling and the Astronautics ZS-1. </title> <journal> Computer, v.22, </journal> <volume> no. </volume> <month> 2 (July </month> <year> 1989), </year> <pages> pp. 21-35. </pages>
Reference-contexts: Our decoupled architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The SMA [PlDa83], PIPE [GHLP85], DEAP [KuHC94], and ZS-1 <ref> [Smit89] </ref> are similar to a 2-level HiDISC without a data cache. The WM [Wulf92] exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming.
Reference: [TyFP92] <author> G. Tyson, M. Farrens, and A.R. Pleszkun. MISC: </author> <title> a multiple instruction stream computer. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microar-chitecture, </booktitle> <month> December </month> <year> 1992, </year> <pages> pp. 193-196. </pages>
Reference-contexts: The absence of a data cache in these decoupled architectures prevents them from exploiting locality when it does exist. When dependencies prevent the access processor from running far enough ahead of the computation processor, these decoupled architectures will not be able to hide memory latency. The MISC <ref> [TyFP92] </ref> architecture includes a data cache, but, like the WM does not address the performance of the memory hierarchy. Each processor in the MISC architecture is the same and has the same access to other processors and the memory hierarchy, unlike the HiDISC, in which each processor is specialized.
Reference: [WeGu89] <author> W.D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1989, </year> <pages> pp. 273-280. </pages>
Reference-contexts: Hardware prefetching [BaCh91] tries to reduce the miss ratio of the cache by prefetch-ing additional data when a miss occurs. Stride-based accesses can be accommodated by hardware prefetching because the memory access pattern is static over the course of a program. Multithread-ing <ref> [WeGu89] </ref> has also been proposed to hide memory latency from the processor by switching threads when cache misses occur. Multithreading places a heavy burden on the compiler because the compiler has to find enough independent threads to hide the memory latency.
Reference: [Wolf92] <author> M.E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: These techniques do not extend well to programs with irregular data structures such as dynamically allocated structures in symbolic programs. November 1, 1996 5 Techniques such as tiling <ref> [Wolf92] </ref> try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone.
Reference: [WuMc94] <author> W.A. Wulf. </author> <title> Hitting the Memory Wall: Implications of the Obvious. Computer Architecture News, </title> <editor> v. </editor> <volume> 23, no. </volume> <month> 1 (December </month> <year> 1994), </year> <pages> pp. 20-24. </pages> <month> November 1, </month> <year> 1996 </year> <month> 20 </month>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate <ref> [WuMc94] </ref>. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
Reference: [Wulf92] <author> W.A. Wulf. </author> <title> Evaluation of the WM Architecture. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992, </year> <pages> pp. 382-390. </pages>
Reference-contexts: Many decoupled architectures have been proposed to exploit instruction-level parallelism and to hide memory latency. The SMA [PlDa83], PIPE [GHLP85], DEAP [KuHC94], and ZS-1 [Smit89] are similar to a 2-level HiDISC without a data cache. The WM <ref> [Wulf92] </ref> exploits microparallelism by decoupling the fetch unit, the integer processing unit, and the floating point processing and provides streaming. The absence of a data cache in these decoupled architectures prevents them from exploiting locality when it does exist.
References-found: 24

