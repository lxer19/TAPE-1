URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95706.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: bixby@rice.edu  martin@zib.de  
Title: Parallelizing the Dual Simplex Method  
Author: Robert E. Bixby Alexander Martin 
Address: Houston, Texas, USA  Berlin, Germany  
Affiliation: Rice University and CPLEX Optimization, Inc.  Konrad-Zuse-Zentrum  
Abstract: We study the parallelization of the steepest-edge version of the dual simplex algorithm. Three different parallel implementations are examined, each of which is derived from the CPLEX dual simplex implementation. One alternative uses PVM, one general-purpose System V shared-memory constructs, and one the PowerC extension of C on a Silicon Graphics multi-processor. These versions were tested on different parallel platforms, including heterogeneous workstation clusters, Sun S20-502, Silicon Graphics multi-processors, and an IBM SP2. We report on our computational experience. 
Abstract-found: 1
Intro-found: 1
Reference: [BaHi94] <author> R. S. Barr, B. L. Hickman, </author> <year> 1994. </year> <title> Parallel Simplex for Large Pure Network Problems: Computational Testing and Sources of Speedup. </title> <journal> Operations Research 42, </journal> <pages> 65-80. </pages>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books [DuErRe86], [KuGrGuKa94] and [GaHeNgOrPePlRoSaVo90] for surveys and further references. Finally, in <ref> [BaHi94] </ref>, [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see [Ch83] or [Pa95].
Reference: [BiGrLuMaSh92] <author> R. E. Bixby, J. W. Gregory, I. J. Lustig, R. E. Marsten, and D. F. Shanno, </author> <year> 1992. </year> <title> Very Large-Scale Linear Programming: A Case Study in Combining Interior Point and Simplex Methods. </title> <journal> Operations Research 40, </journal> <pages> 885-897. </pages>
Reference: [Ch83] <author> V. Chv atal, </author> <year> 1983. </year> <title> Linear Programming, </title> <editor> W. H. </editor> <publisher> Freeman and Company. </publisher>
Reference-contexts: Finally, in [BaHi94], [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see <ref> [Ch83] </ref> or [Pa95]. Consider a linear program (LP) in the following standard form: (1) s.t. Ax = b where c 2 R n , b 2 R m and A 2 R mfin .
Reference: [ChEnFiMe88] <author> M. D. Chang, M. Engquist, R. Finkel, R. R. Meyer, </author> <year> 1988. </year> <title> A Parallel Algorithm for Generalized Networks. </title> <journal> Annals of Operations Research 14, </journal> <pages> 125-145. </pages>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books [DuErRe86], [KuGrGuKa94] and [GaHeNgOrPePlRoSaVo90] for surveys and further references. Finally, in [BaHi94], <ref> [ChEnFiMe88] </ref>, [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see [Ch83] or [Pa95].
Reference: [DaYe90] <author> T. A. Davis, P. Yew, </author> <year> 1990. </year> <title> A Nondeterministic Parallel Algorithm for General Unsymmetric Sparse LU Factorization. </title> <journal> SIAM Journal on Matrix Analysis and Application 11, </journal> <pages> 383-402. </pages>
Reference-contexts: See the discussion in Section 5. Finally, the problem of parallelization of the LU-factorization is largely independent of the simplex method itself. We have chosen not to investigate it here. See [HeKeZa88], <ref> [DaYe90] </ref>, [DuErRe86], [KuGrGuKa94], and [GaHeNgOrPePlRoSaVo90] for a further discussion of this problem.
Reference: [DuErRe86] <author> I. S. Duff, A. M. Erisman, J. K. Reid, </author> <year> 1986. </year> <title> Direct Methods for Sparse Matrices, </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books <ref> [DuErRe86] </ref>, [KuGrGuKa94] and [GaHeNgOrPePlRoSaVo90] for surveys and further references. Finally, in [BaHi94], [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. <p> See the discussion in Section 5. Finally, the problem of parallelization of the LU-factorization is largely independent of the simplex method itself. We have chosen not to investigate it here. See [HeKeZa88], [DaYe90], <ref> [DuErRe86] </ref>, [KuGrGuKa94], and [GaHeNgOrPePlRoSaVo90] for a further discussion of this problem.
Reference: [EcBoPoGo95] <author> J. Eckstein, I. I. Boduroglu, L. C. Polymenakos, D. Goldfarb, </author> <year> 1995. </year> <title> Data-parallel Implementation of Dense Simplex Methods on the Connection Machine CM-2. </title> <journal> ORSA Journal on Computing 7, </journal> <pages> 402-416. </pages>
Reference-contexts: These results use an extensive set of test problems, statistic for which appear in the appendix. Other work on the parallelization of the simplex algorithm includes the following. [HeKeZa88] present a parallelization of the simplex method based on a quadrant interlocking factorization, but no computational results are given. In <ref> [EcBoPoGo95] </ref> an implementation of a more practical revised simplex method is investigated, but the assumption is made that the constraint matrices are dense, a rare occurrence in practice (see the tables at the end of this paper).
Reference: [FoGo92] <author> J. J. Forrest and D. Goldfarb, </author> <year> 1992. </year> <title> Steepest-Edge Simplex Algorithms for Linear Programming. </title> <booktitle> Mathematical Programming 57, </booktitle> <pages> 341-374. </pages>
Reference-contexts: These computations may be considered part of step 5. Steepest Edge There are three different dual algorithms implemented in CPLEX: The standard algorithm, described above, and two steepest-edge variants. The default algorithm is steepest-edge. Several steepest-edge alternatives are proposed in <ref> [FoGo92] </ref>. These algorithms replace the rule for selecting the index of the entering variable d B i by i = argminfx B k = k : k = 1; : : : ; mg; where the k are the steepest-edge norms. <p> Note that the implementation of this formula requires the solution of one extra linear system per iteration, the one used to compute B 1 z. As suggested in <ref> [FoGo92] </ref>, this second "FTRAN" can be solved simultaneously with the linear system in Step 4, thus requiring only a single traversal of the updated LU -factorization of B. The default dual in CPLEX uses the (SE) norms with the approximate starting values k = 1 for all k. <p> The default dual in CPLEX uses the (SE) norms with the approximate starting values k = 1 for all k. This choice corresponds to the assumption that most variables in the initial basis will be slacks or artificials. See <ref> [FoGo92] </ref> for a detailed discussion. 4 Summary In the sections that follow we discuss three different parallel implementations of the (SE) variant of the standard dual simplex method: One using PVM, one using general-purpose System V shared-memory constructs, and one using the PowerC extension of C on an Silicon Graphics multi-processor.
Reference: [FoTo72] <author> J. J. H. Forrest and J. A. Tomlin, </author> <year> 1972. </year> <title> Updating Triangular Factors of the Basis to Maintain Sparsity in the Product-Form Simplex Method. </title> <booktitle> Mathematical Programming 2, </booktitle> <pages> 263-278. </pages>
Reference-contexts: The details of how these shifts are removed have no effect on our implementation and are omitted.) 3. In order to solve the two linear systems in the above algorithm (see Steps 2 and 4), we keep an updated LU-factorization of B, using the so-called Forrest-Tomlin update <ref> [FoTo72] </ref>. For most models, a new factorization is computed once every 100 iterations. These computations may be considered part of step 5. Steepest Edge There are three different dual algorithms implemented in CPLEX: The standard algorithm, described above, and two steepest-edge variants. The default algorithm is steepest-edge.
Reference: [GiMuSaWr89] <author> P. E. Gill, W. Murray, M. A. Saunders and M. H. Wright, </author> <year> 1989. </year> <title> A Practical Anti-Cycling Procedure for Linearly Constrained Optimization, </title> <booktitle> Mathematical Programming 45, </booktitle> <pages> 437-474. </pages>
Reference-contexts: In that case, depending upon the magnitude of r j , we may shift c j to some value at least c j + jd j j, and then repeat the calculation of t and j employing the new r j . (See <ref> [GiMuSaWr89] </ref> for a discussion of the approach that suggested this shifting. The details of how these shifts are removed have no effect on our implementation and are omitted.) 3.
Reference: [GaHeNgOrPePlRoSaVo90] <author> K. A. Gallivan, M. T. Heath, E. Ng, J. M. Ortega, B. W. Peyton, R. J. Plemmons, C. H. Romine, A. H. Sameh, R. G. Voigt, </author> <year> 1990. </year> <title> Parallel Algorithms for Matrix Computations, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA. </address>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books [DuErRe86], [KuGrGuKa94] and <ref> [GaHeNgOrPePlRoSaVo90] </ref> for surveys and further references. Finally, in [BaHi94], [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see [Ch83] or [Pa95]. <p> See the discussion in Section 5. Finally, the problem of parallelization of the LU-factorization is largely independent of the simplex method itself. We have chosen not to investigate it here. See [HeKeZa88], [DaYe90], [DuErRe86], [KuGrGuKa94], and <ref> [GaHeNgOrPePlRoSaVo90] </ref> for a further discussion of this problem.
Reference: [Ha73] <author> P. M. J. Harris, </author> <year> 1973. </year> <title> Pivot Selection Methods of the Devex LP Code. </title> <booktitle> Mathematical Programming 5, </booktitle> <pages> 1-28. </pages>
Reference-contexts: This computation is implemented by storing A N row-wise so that zero elements in z need be examined only once. 2. To improve stability, the ratio test (Step 3) is applied in several passes, using an idea of Harris <ref> [Ha73] </ref>. First, the ratios r k = d k =ff k if ff k &gt; 0 and +1 otherwise; are computed for each k 2 N .
Reference: [HeKeZa88] <author> R. V. Helgason, J. L. Kennington, H. A. A. Zaki, </author> <year> 1988. </year> <title> Parallelization of the Simplex Method. </title> <journal> Annals of Operations Research 14, </journal> <pages> 17-40. </pages>
Reference-contexts: Finally we give computational results. These results use an extensive set of test problems, statistic for which appear in the appendix. Other work on the parallelization of the simplex algorithm includes the following. <ref> [HeKeZa88] </ref> present a parallelization of the simplex method based on a quadrant interlocking factorization, but no computational results are given. <p> See the discussion in Section 5. Finally, the problem of parallelization of the LU-factorization is largely independent of the simplex method itself. We have chosen not to investigate it here. See <ref> [HeKeZa88] </ref>, [DaYe90], [DuErRe86], [KuGrGuKa94], and [GaHeNgOrPePlRoSaVo90] for a further discussion of this problem.
Reference: [KuGrGuKa94] <author> V. Kumar, A. Grama, A. Gupta, G. Karypis, </author> <year> 1994. </year> <title> Introduction to Parallel Computing, </title> <publisher> The Benjamin/Cummings Publishing Company. </publisher> <pages> 23 </pages>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books [DuErRe86], <ref> [KuGrGuKa94] </ref> and [GaHeNgOrPePlRoSaVo90] for surveys and further references. Finally, in [BaHi94], [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. <p> See the discussion in Section 5. Finally, the problem of parallelization of the LU-factorization is largely independent of the simplex method itself. We have chosen not to investigate it here. See [HeKeZa88], [DaYe90], [DuErRe86], <ref> [KuGrGuKa94] </ref>, and [GaHeNgOrPePlRoSaVo90] for a further discussion of this problem.
Reference: [LuMaSh94] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno, </author> <year> 1994. </year> <title> Interior Point Meth--ods for Linear Programming: Computational State of the Art. </title> <journal> ORSA Journal on Computing 6, </journal> <pages> 1-14. </pages>
Reference: [LuRo96] <author> I. J. Lustig, E. Rothberg, </author> <year> 1996. </year> <title> Gigaflops in Linear Programming. </title> <journal> Operations Research Letters 18, </journal> <pages> 157-165. </pages>
Reference-contexts: Second, we envision the primary application of our work to "reoptimiza-tion" in integer programming applications. There the dual is the natural algorithm, even for many very large, difficult models where, say, barrier algorithms <ref> [LuRo96] </ref> potentially provide better performance when solving from scratch. In addition, integer programming applications, particularly those that employ "column-generation", sometimes offer the opportunity to improve the underlying formulation by increasing the number of variables, thus improving the potential for parallelism.
Reference: [Pa95] <author> M. Padberg, </author> <year> 1995. </year> <title> Linear Optimization and Extensions, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: Finally, in [BaHi94], [ChEnFiMe88], [Pe90] computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see [Ch83] or <ref> [Pa95] </ref>. Consider a linear program (LP) in the following standard form: (1) s.t. Ax = b where c 2 R n , b 2 R m and A 2 R mfin .
Reference: [Pe90] <author> J. Peters, </author> <year> 1990. </year> <title> The Network Simplex Method on a Multiprocessor. </title> <booktitle> Networks 20, </booktitle> <pages> 845-859. </pages>
Reference-contexts: Parallelizing the LU factorizations is a topic of its own and has highly been investigated in computational linear algebra, see the books [DuErRe86], [KuGrGuKa94] and [GaHeNgOrPePlRoSaVo90] for surveys and further references. Finally, in [BaHi94], [ChEnFiMe88], <ref> [Pe90] </ref> computational results for parallelizing the network simplex method are reported. 2. Dual Simplex Algorithms We suppose the reader to be familiar with the basic terms of linear programming. For a good introduction to linear programming see [Ch83] or [Pa95].
Reference: [SGI] <author> Power C User's Guide, </author> <title> Silicon Graphics, </title> <publisher> Inc. </publisher>
Reference-contexts: For the aa-problems we obtain ideal speed up beginning at a ratio of 80. 5. PowerC We describe a thread-based parallel implementation of the dual steepest-edge algorithm on an SGI Power Challenge using the SGI PowerC extension of the C programming language <ref> [SGI] </ref>. The work described in this section was carried out at a somewhat later date than that in previous sections. As a result, the base sequential version of CPLEX was somewhat different.
Reference: [St90] <author> W. R. Stevens, </author> <year> 1990. </year> <title> Unix Network Programming, </title> <publisher> PTR Prentice-Hall. </publisher>
Reference-contexts: We restricted our choice to System V mainly because it provides high portability. Possible candidates for inter-process communication (IPC) on a single computer system are pipes, FIFOs, message queues, and shared memory in conjunction with semaphores (for an excellent description of these methods see <ref> [St90] </ref>). We looked at the performance of these four types of IPC by sending data of different sizes between two processors. It turned out that the shared memory/semaphore version was the fastest (see also [St90], page 683). Shared Memory allows two or more processes to share a certain memory segment. <p> queues, and shared memory in conjunction with semaphores (for an excellent description of these methods see <ref> [St90] </ref>). We looked at the performance of these four types of IPC by sending data of different sizes between two processors. It turned out that the shared memory/semaphore version was the fastest (see also [St90], page 683). Shared Memory allows two or more processes to share a certain memory segment. The access to such a shared memory segment is controlled by semaphores. Semaphores are a synchronization primitive. <p> There are different system calls available that create, open, give access, modify or remove shared memory segments and semaphores. For a description of these functions, see the man pages of Unix System V or <ref> [St90] </ref>. We implemented our shared memory version in the following way: We have one shared memory segment for sending data from the boss to the workers. This segment can be viewed as a buffer of appropriate size.
Reference: [Sun] <author> Sun Microsystems, </author> <note> WWW Page: http://www.sun.com/smi/bang/ss20.spec.html. </note>
Reference-contexts: There is a serious memory bottleneck in the SUN S20-502 architecture. Because the data bus is rather small, processes running in parallel interfere with each other when accessing memory. Looking at the SPEC results for the single processor and 2-processor models (see <ref> [Sun] </ref>) we have SUN S20-50 SUN S20-502 SPECrate int92 1708 3029 SPECrate fp92 1879 3159 13 This means that up to about 19% is lost even under ideal circumstances. For memory intensive codes like CPLEX, the numbers are even worse.
Reference: [Wu96] <author> R. Wunderling, </author> <year> 1996. </year> <title> Paralleler und objektorientierter Simplex. </title> <type> Technical Report TR 96-09, </type> <institution> Konrad Zuse Zentrum fur Informationstechnik Berlin. </institution>
Reference-contexts: In [EcBoPoGo95] an implementation of a more practical revised simplex method is investigated, but the assumption is made that the constraint matrices are dense, a rare occurrence in practice (see the tables at the end of this paper). In <ref> [Wu96] </ref> a parallel implementation of the simplex algorithm for sparse linear systems is described where good speed ups could be obtained for problems with a high ratio of variables to constraints.
References-found: 22

