URL: ftp://ftp.eecs.umich.edu/people/huffman/papers/ml93-dist.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/laird/airesearch.html
Root-URL: 
Email: huffman@umich.edu  
Title: Learning procedures from interactive natural language instructions  
Author: Scott B. Huffman and John E. Laird 
Note: In P. Utgoff, ed., Machine Learning: Proceedings of the Tenth International Conference, Amhearst, Mass., June 1993.  
Date: April 19, 1993  
Address: 1101 Beal Ave. Ann Arbor, Michigan 48109-2110  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  
Abstract: Despite its ubiquity in human learning, very little work has been done in artificial intelligence on agents that learn from interactive natural language instructions. In this paper, we examine the problem of learning procedures from interactive, situated instruction, in which the student is attempting to perform tasks within the instructional domain, and asks for instruction when it is needed. We present Instructo-Soar, a system that behaves and learns in response to interactive natural language instructions. Instructo-Soar learns completely new procedures from sequences of instruction, and also learns how to extend its knowledge of previously known procedures to new situations. These learning tasks require both inductive and analytic learning. Instructo-Soar exhibits a multiple execution learning process in which initial learning has a rote, episodic flavor, and later executions allow the initially learned knowledge to be generalized properly. 
Abstract-found: 1
Intro-found: 1
Reference: [Alterman et al., 1991] <author> R. Alterman, R. Zito-Wolf, and T. Carpenter. </author> <title> Interaction, comprehension, and instruction usage. </title> <type> Tech. Report CS-91-161, </type> <institution> Computer Science Dept., Brandeis Univ., </institution> <year> 1991. </year>
Reference: [Carbonell et al., 1983] <author> J. G. Carbonell, R. S. Michalski, and T. M. Mitchell. </author> <title> An overview of machine learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: During later executions of the task, analytic techniques generalize the procedure. 2 Related Work This work is most closely tied to work on learning from external guidance and advice taking <ref> [McCarthy, 1968; Carbonell et al., 1983] </ref>. Prior research in these areas has usually emphasized one of the following: natural language instruction, situated instruction, or interactive instruction. SHRDLU [Winograd, 1972] learned new goal specifications by directly transforming sentences into state descriptions, but did not learn how to perform procedures.
Reference: [Chapman, 1990] <author> D. Chapman. </author> <title> Vision, Instruction, and Action. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Others have learned declarative knowledge bases from natural language (e.g., [Haas and Hendrix, 1983]). A number of recent systems perform in response to natural language input, but do no learning <ref> [Vere and Bickmore, 1990; Chapman, 1990; DiEugenio and White, 1992] </ref>. Lewis et al. [1989] present a system that learns operator sequences from natural language instructions taken in batch form (unsituated and non-interactive).
Reference: [DiEugenio and White, 1992] <author> B. DiEugenio and M. White. </author> <title> On the interpretation of natural language instructions. </title> <booktitle> In Proceedings COLING 92, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Others have learned declarative knowledge bases from natural language (e.g., [Haas and Hendrix, 1983]). A number of recent systems perform in response to natural language input, but do no learning <ref> [Vere and Bickmore, 1990; Chapman, 1990; DiEugenio and White, 1992] </ref>. Lewis et al. [1989] present a system that learns operator sequences from natural language instructions taken in batch form (unsituated and non-interactive).
Reference: [DiEugenio, 1992] <author> B. DiEugenio. </author> <title> Understanding natural language instructions: The case of purpose clauses. </title> <booktitle> In Proceedings of Annual Meeting of the ACL, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: In general, mapping from instructions to task operators and objects can be difficult, as it can require complex natural language comprehension, and possibly reasoning about the task itself <ref> [Huffman and Laird, 1992; DiEugenio, 1992] </ref>. To learn a general operator implementation from instructions, an agent must determine the proper scope of applicability of each instruction. Some features of the current task and situation are important conditions, while others may be ignored.
Reference: [Golding et al., 1987] <author> A. Golding, P. S. Rosenbloom, and J. E. Laird. </author> <title> Learning search control from outside guidance. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 334-337, </pages> <month> August </month> <year> 1987. </year>
Reference: [Grosz, 1977] <author> B. J. Grosz. </author> <title> The Representation and use of focus in dialogue understanding. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1977. </year>
Reference-contexts: While reading each sentence, the agent learns a set of rules that encode the sentence's semantic features. The rules help NL-Soar to resolve referents in later sentences (implementing a simple version of Grosz's focus space mechanism <ref> [Grosz, 1977] </ref>). The rules record each instruction, indexed by the goal it applies to and its place in the instruction sequence. This episodic "case" corresponds to a lock-step, overspecific sequencing of the instructions given to perform the new operator.
Reference: [Haas and Hendrix, 1983] <author> N. Haas and G. G. Hendrix. </author> <title> Learning by being told: Acquiring knowledge for information management. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: SHRDLU [Winograd, 1972] learned new goal specifications by directly transforming sentences into state descriptions, but did not learn how to perform procedures. Others have learned declarative knowledge bases from natural language (e.g., <ref> [Haas and Hendrix, 1983] </ref>). A number of recent systems perform in response to natural language input, but do no learning [Vere and Bickmore, 1990; Chapman, 1990; DiEugenio and White, 1992].
Reference: [Huffman and Laird, 1992] <author> S. B. Huffman and J. E. Laird. </author> <title> Dimensions of complexity in learning from interactive instruction. </title> <editor> In Jon Erikson, editor, </editor> <booktitle> Proceedings of Cooperative Intelligent Robotics in Space III, SPIE Volume 1829, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: In general, mapping from instructions to task operators and objects can be difficult, as it can require complex natural language comprehension, and possibly reasoning about the task itself <ref> [Huffman and Laird, 1992; DiEugenio, 1992] </ref>. To learn a general operator implementation from instructions, an agent must determine the proper scope of applicability of each instruction. Some features of the current task and situation are important conditions, while others may be ignored. <p> Instructo-Soar can be extended in a number of directions. In addition to positive imperative sentences, we are currently investigating learning from other types of instructions, such as positive and negative constraints, conditionals, and actions with monitoring conditions <ref> [Huffman and Laird, 1992] </ref>. The difference-of-states method used to induce operator termination conditions is being extended to allow instructor feedback about the induced conditions.
Reference: [Huffman, 1992] <author> S. B. Huffman. </author> <title> An analysis of instruction sequences. </title> <institution> Artificial Intelligence Laboratory, University of Michigan, </institution> <year> 1992. </year>
Reference-contexts: A simple mathematical analysis shows that for a sequence of only six primitive actions, with preconditions for each action, over 100 possible sequences of interactive instruction are possible <ref> [Huffman, 1992] </ref>. This contrasts with learning from observation, in which systems learn from observing only the sequence of primitive operations performed to carry out the task. After completing the "move above" action, the agent continues receiving and executing instructions for the new "pick up" operator.
Reference: [Kieras and Bovair, 1984] <author> D. E. Kieras and S. Bovair. </author> <title> The role of a mental model in learning to operate a device. </title> <journal> Cognitive Science, </journal> <volume> 8 </volume> <pages> 255-273, </pages> <year> 1984. </year>
Reference-contexts: If the agent does not have knowledge of the primitive operators' effects, learning degrades to rote learning. This appears to be consistent with psychological research showing that subjects given procedural instructions learn and perform better when they have a domain model <ref> [Kieras and Bovair, 1984] </ref>. 7 Conclusion We have described Instructo-Soar, an agent that learns and extends procedures by receiving interactive, situated natural language instructions.
Reference: [Kodratoff and Tecuci, 1987] <editor> Y. Kodratoff and G. Tecuci. DISCIPLE-1: </editor> <title> Interactive apprentice system in weak theory fields. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 271-273, </pages> <month> August </month> <year> 1987. </year>
Reference: [Laird et al., 1987] <author> J. E. Laird, A. Newell, and P. S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year> <month> 10 </month>
Reference-contexts: Supporting these capabilities is dependent in part upon the architecture in which the agent in constructed. We use Soar <ref> [Laird et al., 1987] </ref> as our underlying architecture. Soar's basic structure provides a framework in which these capabilities can be approached. In Soar, all activity occurs by applying operators to states within problem spaces, supporting general problem solving and planning.
Reference: [Laird et al., 1990] <author> J. E. Laird, M. Hucka, E. S. Yager, and C. M. Tuck. </author> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <year> 1990. </year>
Reference-contexts: Although not sophisticated here, this type of inductive learning has the advantage that the agent can alter the bias to reflect other available knowledge. This might include more instruction (e.g., simply asking which features are relevant <ref> [Laird et al., 1990] </ref>); analogy to other known operators (e.g., pick up actions in related domains), domain heuristics, etc.
Reference: [Lehman et al., 1991] <author> J. Lehman, R. L. Lewis, and A. Newell. </author> <title> Natural language comprehension in Soar: Spring 1991. </title> <type> Tech. Report CMU-CS-91-117, </type> <institution> School of Computer Science, Carnegie Mellon Univ., </institution> <year> 1991. </year>
Reference-contexts: Instructo-Soar's problem spaces implement an agent with three main categories of knowledge: natural language processing knowledge, originally developed for NL-Soar <ref> [Lehman et al., 1991] </ref>; knowledge about obtaining and using instruction; and knowledge of the task domain itself. This task knowledge is extended through learning from instruction. Assumed characteristics of the Instructo-Soar agent include: 1. Relevant relationships. <p> Thus, the agent must learn, in some form, all of the parts of the new operator, as described in the previous section. The first instruction given is "Pick up the red block." It is comprehended using Soar's natural language capability, NL-Soar <ref> [Lehman et al., 1991] </ref>, which produces a semantic structure and resolves "the red block" to a block in the agent's environment. However, the semantic structure produced does not correspond to any known operator, indicating that the agent must learn a new operator.
Reference: [Lewis et al., 1989] <author> R. L. Lewis, A. Newell, and T. A. Polk. </author> <title> Toward a Soar theory of taking instructions for immediate reasoning tasks. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society, </booktitle> <month> August </month> <year> 1989. </year>
Reference: [Martin and Firby, 1991] <author> C. E. Martin and R. J. Firby. </author> <title> Generating natural language expectations from a reactive execution system. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 811-815, </pages> <month> August </month> <year> 1991. </year>
Reference: [McCarthy, 1968] <author> J. McCarthy. </author> <title> The advice taker. </title> <editor> In M. Minsky, editor, </editor> <booktitle> Semantic Information Processing, </booktitle> <pages> pages 403-410. </pages> <publisher> MIT Press, </publisher> <year> 1968. </year>
Reference-contexts: During later executions of the task, analytic techniques generalize the procedure. 2 Related Work This work is most closely tied to work on learning from external guidance and advice taking <ref> [McCarthy, 1968; Carbonell et al., 1983] </ref>. Prior research in these areas has usually emphasized one of the following: natural language instruction, situated instruction, or interactive instruction. SHRDLU [Winograd, 1972] learned new goal specifications by directly transforming sentences into state descriptions, but did not learn how to perform procedures.
Reference: [Miller and Laird, 1991] <author> C. Miller and J. E. Laird. </author> <title> A constraint-motivated lexical acquisition model. </title> <booktitle> In Proceedings of the 13th Annual Conference on the Cognitive Science Society, </booktitle> <pages> pages 827-831, </pages> <address> Boston, </address> <year> 1991. </year>
Reference: [Mitchell et al., 1986] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: There have been a variety of systems that learn from observation [Redmond, 1989; Segre, 1987; Wilkins, 1988; VanLehn, 1987]. These systems take traces of expert behavior on a specific problem and learn general procedures using analytic techniques such as explanation-based learning <ref> [Mitchell et al., 1986] </ref>. However, these system do not support interaction with the instructor. <p> Since whole circuits are learned for each function, LEAP avoids the problem of an instructor "skipping steps" by specifying operations with unmet preconditions. In addition, learning apprentice systems require that either the termination conditions of the procedure being taught (the goal concept <ref> [Mitchell et al., 1986] </ref>) are already known, or that the instructor provide a complete description of termination conditions.
Reference: [Mitchell et al., 1990] <author> T. M. Mitchell, S. Mahadevan, and L. I. Steinberg. </author> <title> LEAP: A learning apprentice system for VLSI design. </title> <editor> In Yves Kodratoff and R. S. Michalski, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Vol. III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: However, the instructor can only select actions that are directly performable at the current problem state. LAS's cannot take instructions specifying new, unknown actions (that thus must be learned) or actions with unmet preconditions (which the agent may or may not know how to achieve). For example, LEAP's <ref> [Mitchell et al., 1990] </ref> instructor inputs a complete circuit implementing a desired function, but cannot instruct the system to perform some new, unknown circuit transformation. Since whole circuits are learned for each function, LEAP avoids the problem of an instructor "skipping steps" by specifying operations with unmet preconditions.
Reference: [Pazzani, 1991] <author> M. Pazzani. </author> <title> Learning to predict and explain: An integration of similarity-based, theory driven, and explanation-based learning. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1(2) </volume> <pages> 153-199, </pages> <year> 1991. </year>
Reference-contexts: The difference-of-states method used to induce operator termination conditions is being extended to allow instructor feedback about the induced conditions. Finally, allowing weaker forms of explanation, such as analogy and heuristic causality theories (e.g., <ref> [Pazzani, 1991; VanLehn et al., 1992; Schank and Leake, 1989] </ref>), would lead to more graded degradation of learning with domain theory incompleteness.
Reference: [Pearson et al., 1993] <author> D. J. Pearson, S. B. Huffman, M. B. Willis, J. E. Laird, and R. M. Jones. </author> <title> Intelligent multi-level control in a highly reactive domain. </title> <booktitle> In Proceedings of the International Conference on Intelligent Autonomous Systems, </booktitle> <address> Pgh., PA., </address> <year> 1993. </year>
Reference-contexts: This is the primary domain Instructo-Soar has been applied to; the techniques have also been applied in a more limited way to a flight domain, in which Soar controls a flight simulator and instructions are given for simple maneuvers like taking off <ref> [Pearson et al., 1993] </ref>. To explain Instructo-Soar's performance, we will use the example of teaching the agent in Figure 1 (a) to pick up blocks. Since picking up blocks is not a known operator, when told "Pick up the red block," the agent must learn a new operator.
Reference: [Redmond, 1989] <author> M. </author> <title> Redmond. Combining case-based reasoning, explanation-based learning and learning from instruction. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 20-22, </pages> <year> 1989. </year>
Reference-contexts: Alterman et al. [1991] and Martin and Firby [1991] describe situated systems that recover from execution errors by learning from instruction. There have been a variety of systems that learn from observation <ref> [Redmond, 1989; Segre, 1987; Wilkins, 1988; VanLehn, 1987] </ref>. These systems take traces of expert behavior on a specific problem and learn general procedures using analytic techniques such as explanation-based learning [Mitchell et al., 1986]. However, these system do not support interaction with the instructor.
Reference: [Rosenbloom and Aasman, 1990] <author> P. S. Rosenbloom and J. Aasman. </author> <title> Knowledge level and inductive uses of chunking (ebl). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference: [Rosenbloom and Newell, 1986] <author> P. S. Rosenbloom and A. Newell. </author> <title> The chunking of goal hierarchies: A generalized model of practice. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Generalized learning begins at the end of the implementation sequence and moves towards the beginning. As Figure 3 shows for learning "pick up", the resulting learning curve closely approximates the power law of practice <ref> [Rosenbloom and Newell, 1986] </ref> (r = 0:98). * Effectiveness of hierarchical instruction (single recall strategy). Due to the bottom-up effect, the system learns more quickly when taught hierarchical organizations than flat sequences. General learning for an N step operator takes N executions using a flat instruction sequence.
Reference: [Schank and Leake, 1989] <author> R. C. Schank and D. B. Leake. </author> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 353-385, </pages> <year> 1989. </year>
Reference-contexts: The difference-of-states method used to induce operator termination conditions is being extended to allow instructor feedback about the induced conditions. Finally, allowing weaker forms of explanation, such as analogy and heuristic causality theories (e.g., <ref> [Pazzani, 1991; VanLehn et al., 1992; Schank and Leake, 1989] </ref>), would lead to more graded degradation of learning with domain theory incompleteness.
Reference: [Segre, 1987] <author> A. M. Segre. </author> <title> A learning apprentice system for mechanical assembly. </title> <booktitle> In Third IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 112-117, </pages> <year> 1987. </year> <month> 11 </month>
Reference-contexts: Alterman et al. [1991] and Martin and Firby [1991] describe situated systems that recover from execution errors by learning from instruction. There have been a variety of systems that learn from observation <ref> [Redmond, 1989; Segre, 1987; Wilkins, 1988; VanLehn, 1987] </ref>. These systems take traces of expert behavior on a specific problem and learn general procedures using analytic techniques such as explanation-based learning [Mitchell et al., 1986]. However, these system do not support interaction with the instructor.
Reference: [VanLehn et al., 1990] <author> K. VanLehn, W. Ball, and B. Kowalski. </author> <title> Explanation-based learning of correctness: Towards a model of the self-explanation effect. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 717-724, </pages> <year> 1990. </year>
Reference: [VanLehn et al., 1992] <author> K. VanLehn, R. M. Jones, and M. T. H. Chi. </author> <title> A model of the self-explanation effect. </title> <journal> Journal of the learning sciences, </journal> <volume> 2(1) </volume> <pages> 1-59, </pages> <year> 1992. </year>
Reference-contexts: The difference-of-states method used to induce operator termination conditions is being extended to allow instructor feedback about the induced conditions. Finally, allowing weaker forms of explanation, such as analogy and heuristic causality theories (e.g., <ref> [Pazzani, 1991; VanLehn et al., 1992; Schank and Leake, 1989] </ref>), would lead to more graded degradation of learning with domain theory incompleteness.
Reference: [VanLehn, 1987] <author> K. VanLehn. </author> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31(1) </volume> <pages> 1-40, </pages> <year> 1987. </year>
Reference-contexts: Alterman et al. [1991] and Martin and Firby [1991] describe situated systems that recover from execution errors by learning from instruction. There have been a variety of systems that learn from observation <ref> [Redmond, 1989; Segre, 1987; Wilkins, 1988; VanLehn, 1987] </ref>. These systems take traces of expert behavior on a specific problem and learn general procedures using analytic techniques such as explanation-based learning [Mitchell et al., 1986]. However, these system do not support interaction with the instructor.
Reference: [Vere and Bickmore, 1990] <author> S. Vere and T. Bickmore. </author> <title> A basic agent. </title> <journal> Computational Intelligence, </journal> <volume> 6 </volume> <pages> 41-60, </pages> <year> 1990. </year>
Reference-contexts: Others have learned declarative knowledge bases from natural language (e.g., [Haas and Hendrix, 1983]). A number of recent systems perform in response to natural language input, but do no learning <ref> [Vere and Bickmore, 1990; Chapman, 1990; DiEugenio and White, 1992] </ref>. Lewis et al. [1989] present a system that learns operator sequences from natural language instructions taken in batch form (unsituated and non-interactive).
Reference: [Wilkins, 1988] <author> D. C. Wilkins. </author> <title> Knowledge base refinement using apprenticeship learning techniques. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 646-651, </pages> <year> 1988. </year>
Reference-contexts: Alterman et al. [1991] and Martin and Firby [1991] describe situated systems that recover from execution errors by learning from instruction. There have been a variety of systems that learn from observation <ref> [Redmond, 1989; Segre, 1987; Wilkins, 1988; VanLehn, 1987] </ref>. These systems take traces of expert behavior on a specific problem and learn general procedures using analytic techniques such as explanation-based learning [Mitchell et al., 1986]. However, these system do not support interaction with the instructor.
Reference: [Winograd, 1972] <author> T. Winograd. </author> <title> Understanding Natural Language. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Prior research in these areas has usually emphasized one of the following: natural language instruction, situated instruction, or interactive instruction. SHRDLU <ref> [Winograd, 1972] </ref> learned new goal specifications by directly transforming sentences into state descriptions, but did not learn how to perform procedures. Others have learned declarative knowledge bases from natural language (e.g., [Haas and Hendrix, 1983]).
References-found: 34

