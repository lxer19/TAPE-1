URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-04.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: A Characterization of Scalable Shared Memories  
Author: Prince Kohli Gil Neiger Mustaque Ahamad 
Keyword: Scalable shared memories, memory consistency, synchronization, formal models.  
Note: This work was supported in part by the National Science Foundation under grant CCR-9106627.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: January 19, 1993  
Pubnum: GIT-CC-93/04  
Abstract: The traditional consistency requirements of shared memory are expensive to provide both in large scale multiprocessor systems and also in distributed systems that implement a shared memory abstraction in software. As a result, several memory systems have been proposed that enhance performance and scalability of shared memories by providing weaker consistency guarantees. Often, different models are used to describe such memories which makes it difficult to relate and compare them. We develop a simple non-operational model and identify parameters that can be varied to describe not only the existing memories but also to identify new ones. We show how such a uniform framework makes it easy to compare and relate the various memories. We also use the model to show that a well known software solution to the critical section problem can be used to distinguish the RC sc and RC pc memories explored in the DASH architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering | a new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <year> 1990. </year>
Reference-contexts: It was later shown [2] that the two definitions were distinct and incomparable: neither is stronger than the other. 3.4 Release Consistency In the DASH architecture, processor consistency is provided only for memory operations that implement "synchronization" between processors (other memories that provide selective synchronization were defined weak ordering <ref> [1] </ref> and hybrid consistency [4]). Such operations are called labeled and others are ordinary. Release consistency (RC) is designed to be used with programs that are properly labeled.
Reference: [2] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency. </title> <type> Technical Report 92/34, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: However, r cannot then return 1 when it reads y and 0 (the initial value) for x in the read operation that follows r r (y)1. Before PC was implemented in the DASH architecture, it was first defined by Good-man [9]. It was later shown <ref> [2] </ref> that the two definitions were distinct and incomparable: neither is stronger than the other. 3.4 Release Consistency In the DASH architecture, processor consistency is provided only for memory operations that implement "synchronization" between processors (other memories that provide selective synchronization were defined weak ordering [1] and hybrid consistency [4]). <p> SC is the strongest and hence the set representing it is contained in the sets of all other memories. PRAM is the weakest memory: we have already argued that it is weaker than causal memory; it is not hard to show <ref> [2] </ref> that it is also weaker than PC. TSO is weaker than SC but is strictly stronger than both PC and causal memory. Causal memory and PC are not comparable. This is because, while causal memory's ordering condition requires the stronger causal order, it lacks PC's mutual consistency requirement.
Reference: [3] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <editor> In S. Toueg, P. G. Spirakis, and L. Kirousis, editors, </editor> <booktitle> Proceedings of the Fifth International Workshop on Distributed Algorithms, volume 579 of Lecture Notes on Computer Science, </booktitle> <pages> pages 9-30. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: We then consider other weak memories, including pipelined RAM (PRAM) [15] and Causal Memory <ref> [3] </ref>. 3.2 TSO In TSO, processors have local first-in-first-out (FIFO) buffers and a logically shared memory that is single-ported. A write operation 3 by a processor simply adds the newly written value to the buffer. <p> Causal memory <ref> [3] </ref> is similar to PRAM in the sense that processor views include local operations and write operations of other processors. Also similar to PRAM, causal memory has no mutual consistency requirement. However, the ordering requirement is stronger than that of PRAM.
Reference: [4] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high performance multiprocessors. </title> <booktitle> In Proceedings of the Twenty-Fourth ACM Symposium on Theory of Computing, </booktitle> <pages> pages 679-690. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: There exist models that further distinguish memory operations. Examples of these include labeled operations in release consistency [6] and strong and weak operations in hybrid consistency <ref> [4] </ref>. In such models, ffi p might include only a certain type of read or write operations of other processors. 2. Mutual Consistency: Although processors can define their own views of memory, there may need to be mutual consistency requirements as the views result from accessing a shared memory. <p> shown [2] that the two definitions were distinct and incomparable: neither is stronger than the other. 3.4 Release Consistency In the DASH architecture, processor consistency is provided only for memory operations that implement "synchronization" between processors (other memories that provide selective synchronization were defined weak ordering [1] and hybrid consistency <ref> [4] </ref>). Such operations are called labeled and others are ordinary. Release consistency (RC) is designed to be used with programs that are properly labeled.
Reference: [5] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-22, </pages> <month> February </month> <year> 1988. </year>
Reference: [6] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: When ffi p consists of write operations of processors other than p, we will denote it by w and hence we will use S p+w instead of S p+ffi p . There exist models that further distinguish memory operations. Examples of these include labeled operations in release consistency <ref> [6] </ref> and strong and weak operations in hybrid consistency [4]. In such models, ffi p might include only a certain type of read or write operations of other processors. 2. <p> Thus, this single history can be the view of each processor and hence the mutual consistency requirement is trivially satisfied. To see how memories with weaker consistency require all three kinds of requirements, we consider the total store ordering (TSO) [17] and the processor consistency (PC) <ref> [6] </ref> memory models that have been implemented in the DASH and SPARC architectures, respectively. We then consider other weak memories, including pipelined RAM (PRAM) [15] and Causal Memory [3]. 3.2 TSO In TSO, processors have local first-in-first-out (FIFO) buffers and a logically shared memory that is single-ported. <p> In a later section, we provide further comparison between our definition of TSO and the axiomatic specification given in [17]. 3.3 Processor Consistency We consider processor consistency as defined by Gharachorloo et al. <ref> [6] </ref>. They give an operational definition that describes the implementation of PC in the DASH architecture. This definition explicitly requires coherence; that is, for each memory location, there is a unique ordering of the writes to that location. <p> This provides added efficiency for ordinary operations. For a program to execute correctly, stronger consistency needs to be provided for synchronization operations. In <ref> [6] </ref>, two consistency requirements are identified: RC sc guarantees that labeled operations are sequentially consistent, and RC pc guarantees that they are processor consistent. Our goal is to characterize the behavior of shared memory when it provides RC. <p> Thus, TSO executions are also executions allowed by PC. 5 An Algorithm that Distinguishes RC sc and RC pc It has been claimed that the RC sc and RC pc memory models should be equivalent for most practical applications. Gharachorloo et al. <ref> [6] </ref> state that "[f]or all applications that we have encountered, sequential consistency and processor consistency (for special [labeled] accesses) give the same results." In other words, programs that execute correctly on RC sc memory should also execute correctly when they are run on a system that provides RC pc memory.
Reference: [7] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories (extended abstract). </title> <booktitle> In Proceedings of the Fourth Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306-315. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: In this case, we say o p;i is ordered 4 before o p;j by the program order. This defines program order to be total on any processor execution history; it orders all operations of a given processor. Some memory definitions consider non-blocking operations <ref> [7] </ref>; after invoking a non-blocking operation. When considering such operations, an operation o p;i+1 that follows o p;i may bypass it. In other words, o p;i+1 may complete before o p;i in some processor view. In this case, all orderings defined by ! po may not be maintained.
Reference: [8] <author> Phillip B. Gibbons, Michael Merritt, and Kourosh Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories (extended abstract). </title> <booktitle> In Proceedings of the Third Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, all memory operations that are executed to gain access to the critical section or to release it are labeled. It is not hard to see that the program will now be properly labeled (see above). Gibbons, Merritt, and Gharachor-loo <ref> [8] </ref> showed that any program proved correct with SC will remain correct if properly 14 shared choosing [n] : boolean /* Initially false */ number [n] : integer /* Initially 0 */ local j : integer while true do w (choosing [i])true mine = 1 + maxfnumber [j] j j 6=
References-found: 8

