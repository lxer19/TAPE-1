URL: http://www.stat.washington.edu:80/tech.reports/tr295.ps
Refering-URL: http://www.stat.washington.edu:80/tech.reports/
Root-URL: 
Title: Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering  
Author: Abhijit Dasgupta and Adrian E. Raftery 
Abstract: Technical Report No. 295 Department of Statistics, University of Washington October, 1995 1 Abhijit Dasgupta is a graduate student at the Department of Biostatistics, University of Washington, Box 357232, Seattle, WA 98195-7232, and his e-mail address is dasgupta@biostat.washington.edu. Adrian E. Raftery is Professor of Statistics and Sociology, Department of Statistics, University of Washington, Box 354322, Seattle, WA 98195-4322, and his e-mail address is raftery@stat.washington.edu. This research was supported by Office of Naval Research Grant no. N-00014-91-J-1074. The authors are grateful to Peter Guttorp, Girardeau Henderson and Robert Muise for helpful discussions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allard, D. </author> <year> (1995). </year> <title> Non-Parametric Maximum Likelihood Estimation of Features in Spatial Point Processes using Voronoi Tesselation. </title> <type> Technical Report, </type> <institution> Department of Statistics, University of Washington. </institution>
Reference: <author> Anderson, T.W. </author> <year> (1984). </year> <title> An Introduction to Multivariate Statistical Analysis, 2nd edition. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: It is easily adapted to the mixture likelihood setting by replacing n k by P j z kj in equations (4) and (5). When there is one (non clutter) group, i.e. G = 1, we have ^ff = (d 1)! 1 which is the maximum likelihood estimate of ff <ref> (Anderson, 1984) </ref>. If we have two normal clusters, i.e.
Reference: <author> Banfield, J. D. </author> <year> (1988). </year> <title> Constrained Cluster Analysis and Image Understanding. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <address> Seattle, WA 98195. </address>
Reference: <author> Banfield, J. D. and Raftery, A. E. </author> <year> (1993). </year> <title> Model-based Gaussian and Non-Gaussian Clustering. </title> <journal> Biometrics, </journal> <volume> 49 </volume> <pages> 803-821. </pages>
Reference: <author> Bensmail, H., Celeux, G., Raftery, A., and Robert, C. </author> <year> (1994). </year> <title> Inference for model based cluster analysis. </title> <type> Technical Report 285, </type> <institution> Department of Statistics, University of Wash-ington. </institution>
Reference-contexts: A more ambitious approach would be to do a fully Bayesian analysis of the mixture model directly using Markov chain Monte Carlo (Diebolt and Robert, 1994; 15 Lavine and West, 1992). This has been implemented for the mclust models of BR without clutter <ref> (Bensmail et al., 1994) </ref>, and could be extended to the case where there is clutter without great difficulty, at least in principle. One way of improving the results would be to use more information.
Reference: <author> Celeux, G. and Govaert, G. </author> <year> (1992). </year> <title> A Classification EM Algorithm for Clustering and Two Stochastic Versions. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 14 </volume> <pages> 315-332. </pages>
Reference-contexts: likelihood estimates of and are obtained by maximizing the estimated complete data log-likelihood given below (M-step). ` fl (y; ) = i=1 k=1 ^z ik flog k + log f k (x i ; )g This process is iterated to convergence. 2.3 The CEM algorithm The CEM (Classification EM) algorithm <ref> (Celeux and Govaert, 1992) </ref> is a modification of the EM algorithm developed specifically for classification models. It is based on the classification log-likelihood `(x; ) = k=1 x i 2E k = i=1 k=1 where, now, z ik = I (x i 2 E k ).
Reference: <author> Cressie, N. A. </author> <year> (1991). </year> <title> Statistics for Spatial Data. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum Likelihood from Incomplete Data via the EM Algorithm (c/r: </title> <journal> P22-37). Journal of the Royal Statistical Society, Series B, Methodological, </journal> <volume> 39 </volume> <pages> 1-22. </pages>
Reference-contexts: a way of estimating the shape parameter ff, posterior probabilities of belonging to a feature or to the clutter for each point, and an approximation for the posterior probabilities of the number of clusters that works well in examples. 2.2 The EM Algorithm for Mixture Models The EM (Expectation-Maximization) algorithm <ref> (Dempster et al., 1977) </ref> was originally proposed as a general method for obtaining maximum likelihood estimates in the presence of missing data. It can be used for the estimation of mixture models (McLachlan and Basford, 1988; Redner and Walker, 1984).
Reference: <author> Diebolt, J. and Robert, C.P.(1994). </author> <title> Estimation of Finite Mixture Distributions through Bayesian Sampling. </title> <journal> Journal of the Royal Statistical Society, Series B, Methodological, </journal> <volume> 56 </volume> <pages> 363-375. </pages>
Reference: <author> Diggle, P. J. </author> <year> (1984). </year> <title> Statistical Analysis of Spatial Point Patterns. </title> <publisher> Academic Press. </publisher>
Reference: <author> Kass, R. E. and Raftery, A. E. </author> <year> (1995). </year> <title> Bayes Factors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90 </volume> <pages> 773-795. </pages>
Reference: <author> Lavine, M. and West, M. </author> <year> (1992). </year> <title> A Bayesian method for classification and discrimination. </title> <journal> The Canadian Journal of Statistics, </journal> <volume> 20 </volume> <pages> 451-461. </pages>
Reference: <author> McKenzie, M., Miller, R., and Uhrhammer, R. </author> <year> (1982). </year> <note> Bulletin of the Seismographic Stations. </note> <institution> University of California, Berkeley. </institution> <note> Vol. 53, No. 1-2. 17 McLachlan, </note> <author> G. J. and Basford, K. E. </author> <year> (1988). </year> <title> Mixture models: Inference and applications to clustering. </title> <publisher> Marcel Dekker. </publisher>
Reference-contexts: So, over time, observed earthquake epicenters should be clustered along such faults. We considered an earthquake catalog recorded over a 40,000 km 2 region of the 11 value shown, for the data set in Figure 1, based on mclust-em 12 central coast ranges in California from 1962 to 1981 <ref> (McKenzie et al., 1982) </ref>. An advantage to looking at this region is that the known fault structure is well documented. structures are clearly visible in the plot. Figure 8 (b) shows the mclust-em solution. Figure 9 shows the BIC values for the different numbers of clusters and their successive differences.
Reference: <author> Muise, R. and Smith, C. </author> <year> (1992). </year> <title> Nonparametric minefield detection and localization. </title> <type> Technical Report CSS-TM-591-91, </type> <institution> Naval Surface Warfare Center, Coastal Systems Station. </institution>
Reference-contexts: methodology has been used successfully in a variety of situations and is the 1 While actual minefield data were not available to us, the data in Figure 1 were simulated according to specifications developed at the Naval Coastal Systems Station, Panama City, Florida, to represent minefield data encountered in practice <ref> (Muise and Smith, 1992) </ref>. 1 basis for the mclust software in S-PLUS. 2 For the present problem it works reasonably well even when the amount of clutter is very large, but there is room for improvement.
Reference: <author> Murtagh, F. and Raftery, A. E. </author> <year> (1984). </year> <title> Fitting Straight Lines to Point Patterns. </title> <journal> Pattern Recognition, </journal> <volume> 17 </volume> <pages> 479-483. </pages>
Reference: <author> Ogata, Y. and Katsura, K. </author> <year> (1988). </year> <title> Likelihood analysis of spatial inhomogeneity for marked point patterns. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 40 </volume> <pages> 29-39. </pages>
Reference: <author> Ogata, Y. and Tanemura, M. </author> <year> (1985). </year> <title> Estimation of interaction potentials of marked spatial point patterns through the maximum likelihood method. </title> <journal> Biometrics, </journal> <volume> 41 </volume> <pages> 421-433. </pages>
Reference: <author> Redner, R. A. and Walker, H. F. </author> <year> (1984). </year> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239. </pages>
Reference: <author> Ripley, B. D. </author> <year> (1991). </year> <title> Statistical Inference for Spatial Processes. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464. </pages>
Reference-contexts: Now, thanks to the EM algorithm, we can find the maximized mixture likelihood and use the more firmly-based BIC approximation <ref> (Schwarz, 1978) </ref>, namely 2 log p (xjG) 2`(x; ^ ; G) m G log n = BIC; where p (xjG) is the (integrated) likelihood of the data given that there are G clusters, `(x; ^ ; G) is the maximized mixture log-likelihood with G clusters, and m G is the number
Reference: <author> Tierney, L. and Kadane, J. B. </author> <year> (1986). </year> <title> Accurate approximations for posterior moments and marginal densities. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 81 </volume> <pages> 82-86. </pages>
Reference-contexts: Standard errors for the parameters could be found using the Supplemented EM algorithm (Meng and Rubin 1991) to solve the first problem, and then these could be used to correct the posterior probabilities of the classification via the Laplace method <ref> (Tierney and Kadane, 1986) </ref>, to overcome the second one. A more direct approach might be via the weighted likelihood bootstrap (WLB) (Newton and Raftery 1994), which can use the EM code for maximizing the likelihood directly to compute a full Bayesian posterior distribution.
Reference: <author> Titterington, D. M., Smith, A. F. M., and Makov, U. E. </author> <year> (1985). </year> <title> Statistical analysis of finite mixture distributions. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Wolfe, J. </author> <year> (1971). </year> <title> A Monte Carlo study of the sampling distribution of the likelihood ratio for mixtures of multinormal distributions. </title> <type> Technical Report STB 72-2, </type> <institution> San Diego: U.S. Naval Personnel and Training Research Laboratory. </institution> <month> 18 </month>
References-found: 23

