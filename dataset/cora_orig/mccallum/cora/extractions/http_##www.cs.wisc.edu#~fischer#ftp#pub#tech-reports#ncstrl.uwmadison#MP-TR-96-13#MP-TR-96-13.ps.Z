URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-13/MP-TR-96-13.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-13/
Root-URL: http://www.cs.wisc.edu
Title: SYNCHRONOUS AND ASYNCHRONOUS MULTI-COORDINATION METHODS FOR THE SOLUTION OF BLOCK-ANGULAR PROGRAMS  
Author: R.R. MEYER AND G. ZAKERI 
Abstract: Several types of multi-coordination methods for block-angular programs are considered. We present a computational comparison of synchronous multi-coordination methods. The most efficient of these approaches is shown to involve an intermediate number of blocks in the coordination phase. We also develop a new stabilization algorithm and present asynchronous multi-coordination schemes, which are particularly useful when the number of blocks exceeds the number of available processors or when the block sizes vary significantly. 1. Introduction. In this paper we present multi-coordinator synchronous and 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Ali and J. Kennington, </author> <title> MNETGEN program documentation, </title> <type> Tech. Report IEOR 77003, </type> <institution> Department of Industrial Engineering and Operations Research, Southern Methodist University, Dallas, Texas 75275, </institution> <year> 1977. </year>
Reference-contexts: The single-variable, group and block-plus-group multi-coordination methods are discussed extensively in [4]. We will briefly review them now. 2 2.1. Single-Variable Multi-Coordination (SVMC). Here, once each pro-cessor has obtained the search direction for its corresponding block from (2.5), it solves the following single-variable coordinator problem: minimize w k 2&lt; <ref> [1] </ref> ; : : : ; x t [k] + (y t [k] )w k ; x t [K] )(2.7) subject to 0 x t [k] + (y t [k] )w k u [k] k be the optimal solution to the above, then define x 2 [k] + (y t [k] <p> Suppose p f1; : : :; Kg is the set of blocks coordinator p is responsible for, then the group coordinator problem for processor p is given by: minimize w2&lt; K <ref> [1] </ref> + (y t [1] )w 1 ; : : : ; x t [K] x t subject to 0 x t [k] + (y t [k] )w k u [k] k 2 p Here again we look for the coordinator with the least objective (coordinator c (t)). <p> Suppose p f1; : : :; Kg is the set of blocks coordinator p is responsible for, then the group coordinator problem for processor p is given by: minimize w2&lt; K <ref> [1] </ref> + (y t [1] )w 1 ; : : : ; x t [K] x t subject to 0 x t [k] + (y t [k] )w k u [k] k 2 p Here again we look for the coordinator with the least objective (coordinator c (t)). <p> McBride and Mamer [5] implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5. 3.4.4. The MNETGEN Problems. Another set of problems we considered were those produced by MNETGEN <ref> [1] </ref>, a multicommodity network flow generator which is a derivative of NETGEN [3]. We discovered that the problems produced by this generator contain some mutual constraints that held as equations for any feasible point. Therefore there is no interior for the mutual constraints. <p> Note that [ p;q = fqP; qP + 1; ; (q + 1)P 1g which is the same as the batch of blocks processed at time t where t mod n = q. Define Y t as: Y t = B y t <ref> [1] </ref> : : : 0 . . . 0 [K] x t 1 A (5.2) Let p be the index of our processor and ^w t p be the solution to the following: minrf (x t )Y t w + w 0 H t w (5.3) subject to w [k] =
Reference: [2] <author> M. Grigoriadis and L. Khachiyan, </author> <title> An exponential-function reduction method for block-angular convex programs, </title> <type> Tech. Report 211, </type> <institution> Laboratory for Computer Sciences Research, Rutgers, Computer Sciences Dept., </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1. Introduction. In this paper we present multi-coordinator synchronous and asynchronous solution methods for the block-angular problem BAP: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u We assume that functions c and D are convex and at least once continuously differentiable. <p> 1. Introduction. In this paper we present multi-coordinator synchronous and asynchronous solution methods for the block-angular problem BAP: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u We assume that functions c and D are convex and at least once continuously differentiable. <p> 1. Introduction. In this paper we present multi-coordinator synchronous and asynchronous solution methods for the block-angular problem BAP: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u We assume that functions c and D are convex and at least once continuously differentiable. <p> In the last part of this section we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [4], Zenios and Pinar [8], McBride and Mamer [5], Schultz and Meyer [10] and Grigoriadis and Khachiyan <ref> [2] </ref>. 3.1. Parallel Implementation. Our algorithm follows the basic three-phase method of Schultz and Meyer. Figure (3.1) presents a sketch of the three-phase method. Although we use the same structure as the three-phase method, we generate approximate solutions of the shifted barrier problem using single-variable or 3 group multi-coordination schemes. <p> Results obtained by Zenios and Pinar (in column labeled ZP) have been implemented on the Cray-YMP with 8 processors using the vector units hence the processors are 2-4 times faster than the nodes of CM5. Grigoriadis and Khachiyan <ref> [2] </ref> implemented their algorithm on the IBM RS 6000-550 which is 3 times faster than a node on the CM-5. McBride and Mamer [5] implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5. 3.4.4. The MNETGEN Problems.
Reference: [3] <author> D. Klingman, A. Napier, and J. Stutz, </author> <title> NETGEN|A program for generation of large-scale (un)capacitated assignment, transportation and minimum cost network problems, </title> <booktitle> Management Science, 20 (1974), </booktitle> <pages> pp. 814-822. </pages>
Reference-contexts: The MNETGEN Problems. Another set of problems we considered were those produced by MNETGEN [1], a multicommodity network flow generator which is a derivative of NETGEN <ref> [3] </ref>. We discovered that the problems produced by this generator contain some mutual constraints that held as equations for any feasible point. Therefore there is no interior for the mutual constraints.
Reference: [4] <author> R. D. Leone, R. Meyer, S. Kontogiorgis, A. Zakarian, and G. Zakeri, </author> <title> Coordination in coarse grained decomposition, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 (1994), </volume> <pages> pp. 777-793. </pages>
Reference-contexts: We assume the upper bounds u [i] are finite. The blocks x [i] are only coupled together through the J mutual constraints D (x) d: Previously in <ref> [4] </ref> we presented multi-coordination schemes for the solution of the BAP using barrier decomposition. Here we remind the reader of those methods and present new results as well as a new stabilization algorithm. We also present asynchronous multi-coordination methods for the BAP. 2. Review of Synchronous Multi-Coordination Methods. <p> This was originally done by Schultz and Meyer (in [10]) using a complex coordinator. We, however, perform this step using multiple simpler coordinators hence taking advantage of parallelism. The single-variable, group and block-plus-group multi-coordination methods are discussed extensively in <ref> [4] </ref>. We will briefly review them now. 2 2.1. Single-Variable Multi-Coordination (SVMC). <p> The computational experience below indicates that intermediate values lead to the most efficient implementation. The convergence proofs for the multi-coordination methods given above (as well as a third method called block-plus-group multi-coordination) can be found in <ref> [4] </ref>. 3. Computational Results. In this section we present computational results from the implementation of single-variable and group multi-coordination schemes for linear BAP's. Two sets of test problems are discussed: 1) the well-known PDS problems that arise from a logistic application, and 2) some randomly generated problems. <p> In the last part of this section we present our computational results for each of the above mentioned schemes and compare them to those of De Leone <ref> [4] </ref>, Zenios and Pinar [8], McBride and Mamer [5], Schultz and Meyer [10] and Grigoriadis and Khachiyan [2]. 3.1. Parallel Implementation. Our algorithm follows the basic three-phase method of Schultz and Meyer. Figure (3.1) presents a sketch of the three-phase method.
Reference: [5] <author> R. D. McBride and J. W. Mamer, </author> <title> Solving multicommodity flow problems with a primal embedded network simplex algorithm. </title> <note> submitted. </note>
Reference-contexts: In the last part of this section we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [4], Zenios and Pinar [8], McBride and Mamer <ref> [5] </ref>, Schultz and Meyer [10] and Grigoriadis and Khachiyan [2]. 3.1. Parallel Implementation. Our algorithm follows the basic three-phase method of Schultz and Meyer. Figure (3.1) presents a sketch of the three-phase method. <p> Grigoriadis and Khachiyan [2] implemented their algorithm on the IBM RS 6000-550 which is 3 times faster than a node on the CM-5. McBride and Mamer <ref> [5] </ref> implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5. 3.4.4. The MNETGEN Problems. Another set of problems we considered were those produced by MNETGEN [1], a multicommodity network flow generator which is a derivative of NETGEN [3].
Reference: [6] <author> R. Meyer, </author> <type> Tech. Report 373, </type> <institution> Computer Sciences Department, University of Wisconsin. </institution>
Reference-contexts: Therefore we can invoke theorem (1) of <ref> [6] </ref> which states that the optimal value of the above LP is a continuous function of x t , therefore: lim jrf (x g (t) ) [k] :(y [k] x [k] ) rf (x (t) ) [k] :(y [k] x [k] )j = 0 which contradicts (5.8), therefore lim inf rf
Reference: [7] <author> B. Murtagh and M. Saunders, </author> <title> MINOS 5.4 release notes, appendix to MINOS 5.1 user's guide, </title> <type> technical report, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Since the upper bounds on the subproblems are changed from each iteration to the next (we adjust the decoupled resource allocation) we can not use "hot starting". That is, we need to start with an all-artificial basis at every iteration. We used the optimization package MINOS <ref> [7] </ref> in the form of a subroutine (MI-NOS 5.4) in order to solve the coordinator problems for both single-variable and group coordination. MINOS solves the above using a reduced-gradient algorithm in 5 To generate an approximate solution of the SBP using single-variable multi- coordination: 1.
Reference: [8] <author> M. Pnar and S. Zenios, </author> <title> Parallel decomposition of multicommodity network flows using a linear-quadratic penalty algorithm, </title> <journal> ORSA Journal on Computing, </journal> <volume> 4 (1992), </volume> <pages> pp. 235-249. </pages>
Reference-contexts: In the last part of this section we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [4], Zenios and Pinar <ref> [8] </ref>, McBride and Mamer [5], Schultz and Meyer [10] and Grigoriadis and Khachiyan [2]. 3.1. Parallel Implementation. Our algorithm follows the basic three-phase method of Schultz and Meyer. Figure (3.1) presents a sketch of the three-phase method.
Reference: [9] <author> R. Rockafellar, </author> <title> Convex Analysis, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1970. </year>
Reference-contexts: Therefore, it is clear that f is a proper convex function for a feasible BAP. In addition, we require f to be essentially smooth. A proper, convex function is said to be essentially smooth (see <ref> [9] </ref>) if it satisfies the following three conditions for S = int (domf): * S is non-empty. * f is differentiable throughout S. * lim i!1 jrf (x i )j = +1 if fx i g is a sequence in S converging to the boundary of S.
Reference: [10] <author> G. Schultz, </author> <title> Barrier Decomposition for the parallel optimization of block-angular programs, </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: Here we remind the reader of those methods and present new results as well as a new stabilization algorithm. We also present asynchronous multi-coordination methods for the BAP. 2. Review of Synchronous Multi-Coordination Methods. Schultz and Meyer (in <ref> [10] </ref>) developed specialized barrier methods for the solution of the BAP. <p> Office of Scientific Research grant F 49620-94-1-0036. y Center for Parallel Optimization, Computer Sciences Department, University of Wisconsin Madison, 1210 West Dayton Street, Madison, WI 53706 z Operations Research Group, Department of Engineering Science, Auckland University, Private Bag 92019, Auckland, New Zealand 1 where is a convex barrier function (see <ref> [10] </ref>). <p> This condition ensures that we can always take a step in any feasible search direction. For more details on the decoupled resource allocation see <ref> [10] </ref>. <p> Once the search directions y t [k] are determined we need to assign stepsizes to be taken in each direction. This was originally done by Schultz and Meyer (in <ref> [10] </ref>) using a complex coordinator. We, however, perform this step using multiple simpler coordinators hence taking advantage of parallelism. The single-variable, group and block-plus-group multi-coordination methods are discussed extensively in [4]. We will briefly review them now. 2 2.1. Single-Variable Multi-Coordination (SVMC). <p> In the last part of this section we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [4], Zenios and Pinar [8], McBride and Mamer [5], Schultz and Meyer <ref> [10] </ref> and Grigoriadis and Khachiyan [2]. 3.1. Parallel Implementation. Our algorithm follows the basic three-phase method of Schultz and Meyer. Figure (3.1) presents a sketch of the three-phase method.
Reference: [11] <author> A. Zakarian. </author> <title> Private communication. </title> <type> 20 </type>
Reference-contexts: The three-phase method the number of mutual constraints. The column labeled total var. contains the total number of variables. The block constraint matrices for these block-angular problems are node-arc incidence matrices. We take advantage of this fact in our code and use a very efficient network flow solver NSM <ref> [11] </ref> to solve the subproblems. NSM uses the network simplex method to solve the subproblems. Since the upper bounds on the subproblems are changed from each iteration to the next (we adjust the decoupled resource allocation) we can not use "hot starting".
References-found: 11

