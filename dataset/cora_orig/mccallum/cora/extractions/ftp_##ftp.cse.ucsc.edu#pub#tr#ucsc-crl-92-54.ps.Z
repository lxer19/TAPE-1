URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-92-54.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: On Weak Learning  
Author: David P. Helmbold and Manfred K. Warmuth 
Note: Manfred K. Warmuth was supported by ONR grant N00014-91-J-1162 and part of this work was done while he was visiting the  
Address: Santa Cruz, CA 95064  Japan,  
Affiliation: Board of Studies in Computer and Information Sciences University of California, Santa Cruz  IIAS-SIS Institute of Fujitsu Laboratories, Numazu,  
Pubnum: UCSC-CRL-92-54  
Email: email: manfred@cis.ucsc.edu.  
Date: December 16, 1992  
Abstract: An algorithm is a weak learning algorithm if with some small probability it outputs a hypothesis with error slightly below 50%. This paper presents relationships between weak learning, weak prediction (where the probability of being correct is slightly larger than 50%), and consistency oracles (which decide whether or not a given set of examples is consistent with a concept in the class). Our main result is a simple polynomial prediction algorithm which makes only a single query to a consistency oracle and whose predictions have a polynomial edge over random guessing. We compare this prediction algorithm with several of the standard prediction techniques, deriving an improved worst case bound on Gibbs Algorithm in the process. We use our algorithm to show that a concept class is polynomially learnable if and only if there is a polynomial probabilistic consistency oracle for the class. Since strong learning algorithms can be built from weak learning algorithms, our results also characterizes strong learnability. fl David P. Helmbold was supported by NSF grant CCR-9102635, email: dph@cis.ucsc.edu.
Abstract-found: 1
Intro-found: 1
Reference: [AHU74] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: = S S n X n is learnable and the oracle O for F on X is a polynomial consistency oracle then e Q is a polynomial weak prediction algorithm. 12 The main resource we are interested in is running time in some standard computational model such as the RAM <ref> [AHU74] </ref>. All of our algorithms can be implemented so that the space used and number of random bits required is bounded by the running time. 8.
Reference: [AHW87] <author> N. Alon, D. Haussler, and E. Welzl. </author> <title> Partitioning and geometric embedding of range spaces of finite Vapnik-Chervonenkis dimension. </title> <booktitle> In Proceedings of Third Symposium on Computational Geometry, </booktitle> <pages> pages 331-340, </pages> <month> June </month> <year> 1987. </year>
Reference: [BEHW87] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Previously it has been shown that if the cardinality of H is small (bounded by p (n; s) m 1 , where is a constant less than one), then the Occam algorithm is a strong learning algorithm <ref> [BEHW87] </ref>. We show that even if the size of the hypotheses class grows exponentially, Occam algorithms may already be weak learners. <p> We define weak prediction algorithms in Section 3 and relate them to weak learning algorithms. Section 4 shows that one kind of weak learning algorithm is a "weak Occam algorithm" (similar to the "strong" Occam algorithms studied by Blumer et al. <ref> [BEHW87] </ref>, Board and Pitt [BP92]) and Haussler 4 et al. [HKLW91]. The next part of the paper concentrates on prediction algorithms. In Section 5 we define lookahead prediction algorithms which get the entire set of instances where predictions will be required before making any predictions. <p> An "Occam algorithm" is a learning algorithm that outputs consistent hypothesis from a "small" hypothesis class <ref> [BEHW87] </ref>. <p> It has been shown <ref> [BEHW87] </ref> that for each strong Occam algorithm for S S there is a sample size polynomial in n, s, 1=*, and 1=ffi for which this algorithm is a strong learning algorithm. <p> We require only that the hypothesis class be polynomially evaluatable. 8 4. Weak Occam Algorithms Here we define "weak Occam algorithms" whose hypothesis classes grow exponentially in m and show using the methods of Blumer et al. <ref> [BEHW87] </ref> that weak Occam algorithms lead to weak learning algorithms. Thus they can be used iteratively to build strong learning algorithms [Sch90, Fre90]. <p> Note that a strong Occam algorithm produces hypotheses with smaller error when the sample size m is increased <ref> [BEHW87] </ref> and for some polynomial choice of m the strong Occam algorithm becomes a strong learning algorithm. This is not necessarily true for a weak Occam algorithm as the error (1=21=p 2 (m)) approaches 1=2 as m increases.
Reference: [BEHW89] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. War-muth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: show that if a polynomial time probabilistic consistency oracle is available then it can be used to construct a polynomial weak learning algorithm for F whenever F is learnable at all with respect to an arbitrary distribution (i.e. the Vapnik-Chervonenkis dimension [VC71] of F grows polynomially in n and s <ref> [BEHW89] </ref>). Previously a direct construction of a strong learning algorithm using consistency oracles was given by Haussler, Littlestone, and Warmuth [HLW]. However that algorithm is only polynomial if the VC dimension of F is a constant independent of n and s. <p> If sam F (x) = sam fl (x) then x is shattered by F . The Vapnik-Chervonenkis dimension, or VC dimension, of a concept class F on X is the largest k such that there exists an x 2 X k that is shattered by F <ref> [VC71, BEHW89] </ref>. 5 If x is the empty sequence of instances, then sam f (x) is the empty sequence of examples. 4 3. <p> The two definitions are polynomially equivalent (see Lemma 3.4 of [HKLW91]). For a strong learning algorithm <ref> [Val84, BEHW89] </ref>, Inequality (3.1) is replaced by the inequality Pr x2D m ;r2U [0;1] [Err D (f; h) &gt; *] ffi; where * and ffi are additional paramenters in [0; 1]. <p> Sample Complexity of Weak Learning 17 Proof: If F = S S n X n is learnable then the VC dimension of F s on X n is upper-bounded by some polynomial p (n; s) <ref> [BEHW89] </ref>. By Sauer's Lemma [Sau72], for any m and x 2 (X n ) m : jsam F s (x)j P p (n;s) m m p (n;s) + 1. <p> In Haussler, Littlestone, and Warmuth [HLW] it is shown that maxdens F (x) is upper bounded by the Vapnik-Chervonenkis dimension of the class F <ref> [VC71, BEHW89] </ref>. The main drawback of the 1-Inclusion Graph Prediction Algorithm is that it is not generally efficient as it solves flow problems on graphs containing jsam F (x)j vertices.
Reference: [Bon72] <author> J. A. </author> <title> Bondy. Induced subsets. </title> <journal> J. Comb. Theory, </journal> <volume> 12 </volume> <pages> 201-202, </pages> <year> 1972. </year>
Reference: [BP92] <author> R. Board and L. Pitt. </author> <title> On the necessity of Occam algorithms. </title> <journal> Theoretical Computer Science, </journal> <volume> 100 </volume> <pages> 157-184, </pages> <year> 1992. </year>
Reference-contexts: We define weak prediction algorithms in Section 3 and relate them to weak learning algorithms. Section 4 shows that one kind of weak learning algorithm is a "weak Occam algorithm" (similar to the "strong" Occam algorithms studied by Blumer et al. [BEHW87], Board and Pitt <ref> [BP92] </ref>) and Haussler 4 et al. [HKLW91]. The next part of the paper concentrates on prediction algorithms. In Section 5 we define lookahead prediction algorithms which get the entire set of instances where predictions will be required before making any predictions. <p> The above definition of (strong) Occam algorithm is less restrictive than previous definitions as they require that the hypotheses produced by the Occam algorithm be in the concept class <ref> [BP92] </ref>, or in a specified hypotheses class [HKLW91]. We require only that the hypothesis class be polynomially evaluatable. 8 4.
Reference: [Fre90] <author> Y. Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-231, </pages> <address> San Mateo, CA, </address> <month> August </month> <year> 1990. </year> <note> Morgan Kaufmann. 34 References </note>
Reference-contexts: Surprisingly, it has been shown that any polynomial weak learning algorithm can be used to build a polynomial strong learning algorithm <ref> [Sch90, Fre90] </ref>. These constructions create many copies of the weak learning algorithm and each copy generates a hypothesis based on a filtered sequence of examples. These hypotheses are then combined to form a master hypothesis. <p> It has been shown that any weak learning algorithm A for S S n X n can be used iteratively to build a strong learning algorithm for S S n X n <ref> [Sch90, Fre90] </ref>. Moreover if the weak learning algorithm is polynomial then the resulting strong learning algorithm is also polynomial. Note that one can not convert weak learning algorithms into strong learning algorithms by simply increasing the sample size m. <p> Note that one can not convert weak learning algorithms into strong learning algorithms by simply increasing the sample size m. In fact, the error bound of 1 2 1 hypotheses produced by a weak learning algorithm can approach 1 2 as m increases. The conversion algorithms of [Sch90] and <ref> [Fre90] </ref> repeatedly use the weak learning algorithm on different "small" samples of size m = p 1 (n; s) (where p 1 (n; s) is the first polynomial in the weak learning algorithm definition). <p> Weak Occam Algorithms Here we define "weak Occam algorithms" whose hypothesis classes grow exponentially in m and show using the methods of Blumer et al. [BEHW87] that weak Occam algorithms lead to weak learning algorithms. Thus they can be used iteratively to build strong learning algorithms <ref> [Sch90, Fre90] </ref>. <p> This is not necessarily true for a weak Occam algorithm as the error (1=21=p 2 (m)) approaches 1=2 as m increases. Instead, the conversion algorithms of <ref> [Sch90, Fre90] </ref> use the weak Occam algorithm repeatedly for a number of different samples of size p 1 (n; s), where p 1 (n; s) is the size of the sample expected by the Occam algorithm when the parameters are n and s. <p> The results of Freund <ref> [Fre90] </ref> imply that there is a randomized Algorithm B which repeatedly uses the weak learning algorithm A to compress a large sample.
Reference: [GKS90] <author> Sally A. Goldman, Michael J. Kerns, and Robert E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 217-231, </pages> <address> San Mateo, CA, August 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In Section 8 we show using our algorithm that sample size 2d ( p dlogd) suffices for weak learning (not necessarily polynomial weak learning) of concept classes of VC dimension d. In Goldman et al. <ref> [GKS90] </ref> it was shown that no algorithm can weakly learn some concept classes of VC dimension d from d O (log (d)) examples. Section 9 compares our prediction algorithm with several of the standard prediction algorithms, as well as the weak prediction algorithm of [HW92b]. <p> Since here we are not interested in computational resources we omit the parameterization during this section. Even though the results in Goldman et al. <ref> [GKS90] </ref> suggest the sample complexity for weak learning is not well correlated with the VC dimension, the following theorem and corollary gives the lowest sample size known to us (2d O ( p d log d)) for which there is a general weak learning algorithm. <p> We show in Section 8 that the sample complexity of weak learning an arbitrary concept classes of VC dimension d (disreguarding computational considerations) is at most 2d ( d lg d). It is shown in Goldman et al. <ref> [GKS90] </ref> that there are classes of VC dimension d where every weak learning algorithm requires at least d O (log d) examples. We would like to see these bounds tightened.
Reference: [GT90] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In K. Thuemann and R. Koeberle, editors, </editor> <title> Neural Networks and Spin Glasses. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: Although the function g used by a volume prediction algorithm may be simple, computing the volume of a sample may not be computationally feasible. Here we consider three volume prediction algorithms. Algorithm Gibbs P (Gibbs Algorithm) is well known <ref> [HKS91, HO91, GT90, HS90, STS90] </ref> and can be viewed as predicting with a randomly chosen consistent concept from the class where the consistent concepts are weighted according to the prior P.
Reference: [HKLW91] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: They differ only when n measures some aspect of the instances other than their bit length. 2 In the original definition of weak learning the paramenter 1 ffi is used in place of 1=p 3 (m). The two definitions are polynomially equivalent (see Lemma 3.4 of <ref> [HKLW91] </ref>). 2 1. Introduction viewed as an oracle which returns a concept consistent with the set of examples. <p> Section 4 shows that one kind of weak learning algorithm is a "weak Occam algorithm" (similar to the "strong" Occam algorithms studied by Blumer et al. [BEHW87], Board and Pitt [BP92]) and Haussler 4 et al. <ref> [HKLW91] </ref>. The next part of the paper concentrates on prediction algorithms. In Section 5 we define lookahead prediction algorithms which get the entire set of instances where predictions will be required before making any predictions. <p> In this case (see <ref> [HKLW91] </ref>), with x &lt;m and f fixed, E x m 2D fi fl E r2 [0;1] [Err D (f; h)]. The same relationship holds when learning algorithm A 0 uses the prediction algorithm A as its hypothesis evaluator. <p> Weak Learning Models Pr x2D m ;r2U [0;1] Err D (f; h) &gt; 2 1 1 (3.1) In the original definition of weak learning the paramenter ffi is used is place of 1 1=p 3 (m). The two definitions are polynomially equivalent (see Lemma 3.4 of <ref> [HKLW91] </ref>). For a strong learning algorithm [Val84, BEHW89], Inequality (3.1) is replaced by the inequality Pr x2D m ;r2U [0;1] [Err D (f; h) &gt; *] ffi; where * and ffi are additional paramenters in [0; 1]. <p> The above definition of (strong) Occam algorithm is less restrictive than previous definitions as they require that the hypotheses produced by the Occam algorithm be in the concept class [BP92], or in a specified hypotheses class <ref> [HKLW91] </ref>. We require only that the hypothesis class be polynomially evaluatable. 8 4. Weak Occam Algorithms Here we define "weak Occam algorithms" whose hypothesis classes grow exponentially in m and show using the methods of Blumer et al. [BEHW87] that weak Occam algorithms lead to weak learning algorithms. <p> Such an oracle is called polynomial if its answers are computed in time polynomial in n, s, and the total bit length of S. We will exploit the "one-sidedness" of probabilistic consistency oracles in the same way that "random polynomial time hypothesis finders" were exploited by Haussler et al. <ref> [HKLW91] </ref>. Theorem 10.4: A polynomial weak learning algorithm for F = S S be used to construct a polynomial probabilistic consistency oracle. <p> Proof: Let A be a polynomial weak learning algorithm for F = S S assume that Algorithm A is deterministic as polynomially many random bits can be obtained from additional examples (see <ref> [HKLW91] </ref>, Lemma 3.5).
Reference: [HKS91] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the 1991 Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA, </address> <month> August </month> <year> 1991. </year> <note> Morgan Kaufmann. To appear in Machine Learning. </note>
Reference-contexts: Section 9 compares our prediction algorithm with several of the standard prediction algorithms, as well as the weak prediction algorithm of [HW92b]. This comparison includes an improved bound on the expected total number of mistakes made by the Gibbs prediction algorithm <ref> [HKS91] </ref> when learning a worst-case concept. In Section 10 we define one-sided and probabilistic consistency oracles, and prove that a concept class is polynomially weakly learnable if and only if there is a polynomial probabilistic consistency oracle for the class. <p> Although the function g used by a volume prediction algorithm may be simple, computing the volume of a sample may not be computationally feasible. Here we consider three volume prediction algorithms. Algorithm Gibbs P (Gibbs Algorithm) is well known <ref> [HKS91, HO91, GT90, HS90, STS90] </ref> and can be viewed as predicting with a randomly chosen consistent concept from the class where the consistent concepts are weighted according to the prior P. <p> Bounds on Gibbs and Bayesian prediction 23 jxj X M (Gibbs P ; f; x) (ln 2) lg V P (sam f (x)) (9.3) The bounds on Bayes P and Gibbs P were shown in <ref> [HKS91] </ref> and the bound on G P appears in [Vov90] and is presented in Appendix A. It is easy to show that the constants in these bounds cannot be improved unless the form of the bounds are changed. <p> Better bounds can be shown in the average case setting, where the target is chosen at random using the same distribution as the prior. For this average case setting, the constant of 1 2 has also been obtained for the Gibbs and Bayes Algorithms <ref> [HKS91] </ref>.
Reference: [HLW] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0,1g functions on randomly drawn points. </title> <note> To appear in Information and Computation. An extended abstract appeared in COLT 88. </note>
Reference-contexts: Previously a direct construction of a strong learning algorithm using consistency oracles was given by Haussler, Littlestone, and Warmuth <ref> [HLW] </ref>. However that algorithm is only polynomial if the VC dimension of F is a constant independent of n and s. Thus a class is polynomially learnable if and only if it has a polynomial probabilistic consistency oracle. The following section contains an introduction to our notation. <p> Thus it suffices to bound the probability of a mistake on the last instance of a random permutation of x (this was used extensively in <ref> [HLW] </ref>). 12 5. Lookahead Prediction Lemma 5.1: Let X be any domain, D be any distribution on X, m 2 N, and R be a random variable on X m . <p> The optimal algorithm for minimizing the worst case (over targets f 2 F ) probability of predicting wrong on the last instance of a random permutation of x is the 1-Inclusion Graph Algorithm <ref> [HLW] </ref>, here denoted by "1-Inc." For prediction algorithm A and any x 2 X + , 26 9. <p> In Haussler, Littlestone, and Warmuth <ref> [HLW] </ref> it is shown that maxdens F (x) is upper bounded by the Vapnik-Chervonenkis dimension of the class F [VC71, BEHW89]. The main drawback of the 1-Inclusion Graph Prediction Algorithm is that it is not generally efficient as it solves flow problems on graphs containing jsam F (x)j vertices. <p> When computational considerations are ignored the algorithm that minimizes sup E y2U (x) [M (A; f; y)] is the 1-inclusion graph algorithm of Haussler, Littlestone, and Warmuth <ref> [HLW] </ref> (as discussed in the Section 9). For that algorithm the supremum equals maxdens F (x) jxj where maxdens F (x) is the maximum density of any subgraph of the 1-inclusion graph with respect to F and x.
Reference: [HO91] <author> D. Haussler and M. Opper. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Proceedings of the 1991 Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Although the function g used by a volume prediction algorithm may be simple, computing the volume of a sample may not be computationally feasible. Here we consider three volume prediction algorithms. Algorithm Gibbs P (Gibbs Algorithm) is well known <ref> [HKS91, HO91, GT90, HS90, STS90] </ref> and can be viewed as predicting with a randomly chosen consistent concept from the class where the consistent concepts are weighted according to the prior P.
Reference: [HS90] <author> D. Hansel and H. Sompolinsky. </author> <title> Learning from examples in a single-layer neural network. </title> <journal> Europhys. Lett., </journal> <volume> 11(7) </volume> <pages> 687-692, </pages> <year> 1990. </year>
Reference-contexts: Although the function g used by a volume prediction algorithm may be simple, computing the volume of a sample may not be computationally feasible. Here we consider three volume prediction algorithms. Algorithm Gibbs P (Gibbs Algorithm) is well known <ref> [HKS91, HO91, GT90, HS90, STS90] </ref> and can be viewed as predicting with a randomly chosen consistent concept from the class where the consistent concepts are weighted according to the prior P.
Reference: [HW92a] <author> D. Helmbold and M. K. Warmuth. </author> <title> On weak learning. </title> <booktitle> In Proceedings of the Third NEC Research Symposium on Computational Learning and Cognition, </booktitle> <institution> 3600 University City Science Center, </institution> <address> Philadelphia, PA 19104-2688, </address> <month> May </month> <year> 1992. </year> <note> SIAM. </note>
Reference-contexts: In Section 11 we introduce polynomial "data interpolators" and discuss how they generalize Weak Occam algorithms. We conclude in Section 12 by discussing a number of open problems raised by this research. Preliminary versions of several results presented here have appeared in conference papers <ref> [HW92b, HW92a] </ref>. 2 Notation Throughout, lg and ln denote the binary and natural logarithms, respectively. When logarithms appear in asymptotic notation we use log, as the base is not relevant. We use N to denote the positive numbers, and adopt the convention that 0 = 1 and 1 = 0.
Reference: [HW92b] <author> D. Helmbold and M. K. Warmuth. </author> <title> Some weak learning results. </title> <booktitle> In Proceedings of the 1992 Workshop on Computational Learning Theory. ACM, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Notation 3 single query to the consistency oracle. In contrast, our earlier prediction algorithm <ref> [HW92b] </ref> requires a large number of consistency oracle queries to make each prediction. In Section 8 we show using our algorithm that sample size 2d ( p dlogd) suffices for weak learning (not necessarily polynomial weak learning) of concept classes of VC dimension d. <p> In Goldman et al. [GKS90] it was shown that no algorithm can weakly learn some concept classes of VC dimension d from d O (log (d)) examples. Section 9 compares our prediction algorithm with several of the standard prediction algorithms, as well as the weak prediction algorithm of <ref> [HW92b] </ref>. This comparison includes an improved bound on the expected total number of mistakes made by the Gibbs prediction algorithm [HKS91] when learning a worst-case concept. <p> In Section 11 we introduce polynomial "data interpolators" and discuss how they generalize Weak Occam algorithms. We conclude in Section 12 by discussing a number of open problems raised by this research. Preliminary versions of several results presented here have appeared in conference papers <ref> [HW92b, HW92a] </ref>. 2 Notation Throughout, lg and ln denote the binary and natural logarithms, respectively. When logarithms appear in asymptotic notation we use log, as the base is not relevant. We use N to denote the positive numbers, and adopt the convention that 0 = 1 and 1 = 0. <p> Calls to O (n; s; S) need answer correctly only when jSj = 2p (n; s) for some polynomial p (n; s) which is always at least the VC dimension of F s on X n . Even weaker oracles with "one-sided error" were considered in <ref> [HW92b] </ref> and are discussed in Section 10. We now consider the Lookahead Transform, e Q, of the parameterized Query Lookahead Prediction Algorithm. <p> Algorithm G P is a special case of the aggregating strategy introduced by Vovk [Vov90], and was used as the basis for a polynomial weak prediction algorithm <ref> [HW92b] </ref>. The classical Bayes Prediction Algorithm, Bayes P , is known to be optimal when the target is drawn according to the prior. <p> However, in our previous paper <ref> [HW92b] </ref> a polynomial approximation to e G was given. The number of consistency oracle queries used by this approximation to e G is ( 2 ff 0 1 ff 0 ) where ff 0 is an underestimate of ff which must be supplied to the algorithm. <p> Although Algorithms g Gibbs, g Bayes, and e G can not be implemented efficiently, they can be approximated by making many calls to a consistency oracle. We presented an approximation to e G using this approach <ref> [HW92b] </ref>. However, this approximation to e G uses ( 2 ff (x) 1 ff (x) ) calls 14 to a consistency oracle. As ff (x) goes to jxj, the number of queries used grows exponentially in ff (x).
Reference: [Kha92] <author> Michael Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <type> unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: A more specific conjecture is given in the final section of this paper. Note these results have certain negative implications. Since DFAs over a binary alphabet are not polynomially learnable under certain cryptographic assumptions <ref> [KV89, Kha92] </ref>, there can't exist a polynomial weak Occam algorithm for this class. Thus, given the same cryptographic assumptions, if there exists a polynomial algorithm that, on inputs 10 The m + 1 bitvectors of f0; 1g m with at most one 1 are shattered by F m . 5.
Reference: [KV89] <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <address> New York, </address> <month> May </month> <year> 1989. </year> <note> ACM. To appear in Journal of the ACM. </note>
Reference-contexts: Not too many concept classes have been shown to be polynomially strongly learnable and a less stringent definition of learning was given by Kearns and Valiant <ref> [KV89] </ref>. A weak learning algorithm must, after seeing m = p 1 (n; s) many examples, output a hypothesis having error at most 1 2 1=p 2 (m) with probability at least 2 1=p 3 (m), where p 1 , p 2 and p 3 are polynomials. <p> We extend the M (A; f; x) notation to handle these parameterized learning problems by defining M n;s (A; f; x) = E r2U [0;1] A (sam f (x &lt;m ); x m ; r; n; s) 6= f (x m ) : Algorithm A is a weak learning algorithm <ref> [KV89] </ref> for S S n X n if there exist three polynomials p 1 , p 2 , and p 3 such that if A is given the parameters n and s then for all f 2 F s and probability distributions D on X n the following holds: upon receiving <p> A more specific conjecture is given in the final section of this paper. Note these results have certain negative implications. Since DFAs over a binary alphabet are not polynomially learnable under certain cryptographic assumptions <ref> [KV89, Kha92] </ref>, there can't exist a polynomial weak Occam algorithm for this class. Thus, given the same cryptographic assumptions, if there exists a polynomial algorithm that, on inputs 10 The m + 1 bitvectors of f0; 1g m with at most one 1 are shattered by F m . 5. <p> Conclusions and Directions for Further Research Second, the interaction of our results with cryptography could be a promising direction. For example, there is no polynomial weak learning algorithm for DFAs given standard cryptographic assumptions <ref> [KV89] </ref>.
Reference: [MS77] <author> F.J. MacWilliams and N.J.A. Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: From Sauer's Lemma [Sau72], for any m and x 2 (X n ) m : jsam F (x)j d X i = 2 m i=d+1 m ! m , so that 1 2 1. We use the following approximation to the binomial coefficient m (see <ref> [MS77] </ref>, Lemma 7, page 309): m X i m ! 2 mH 2 () 8m (1 ) where H 2 () = lg 1 + (1 ) lg 1 1 .
Reference: [Sau72] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (Series A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Sample Complexity of Weak Learning 17 Proof: If F = S S n X n is learnable then the VC dimension of F s on X n is upper-bounded by some polynomial p (n; s) [BEHW89]. By Sauer's Lemma <ref> [Sau72] </ref>, for any m and x 2 (X n ) m : jsam F s (x)j P p (n;s) m m p (n;s) + 1. Let m = 2p (n; s) so that P p (n;s) m = 2 m1 , and jsam F s (x)j 2 m1 . <p> The first inequality is Inequality 8.1, and the last inequality in the sequence will follow from the conditions of the theorem. By Fact 2.1 jsam F (x)j 2 m (1 ln 2 ) implies Inequality 8.1. From Sauer's Lemma <ref> [Sau72] </ref>, for any m and x 2 (X n ) m : jsam F (x)j d X i = 2 m i=d+1 m ! m , so that 1 2 1. <p> Our results show that, under the same cryptographic assumptions, a polynomial probabilistic consistency oracle for F on X can not exist. For any fixed sequence x of m instances, jsam F s (x)j is at most m O (s log s) <ref> [Sau72] </ref>, since the VC dimension of F s is O (s log s). When m is a polynomial in s of degree 2 or greater, then jsam F s (x)j is much smaller than the 2 m samples in sam fl (x).
Reference: [Sch90] <author> R. E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Surprisingly, it has been shown that any polynomial weak learning algorithm can be used to build a polynomial strong learning algorithm <ref> [Sch90, Fre90] </ref>. These constructions create many copies of the weak learning algorithm and each copy generates a hypothesis based on a filtered sequence of examples. These hypotheses are then combined to form a master hypothesis. <p> It has been shown that any weak learning algorithm A for S S n X n can be used iteratively to build a strong learning algorithm for S S n X n <ref> [Sch90, Fre90] </ref>. Moreover if the weak learning algorithm is polynomial then the resulting strong learning algorithm is also polynomial. Note that one can not convert weak learning algorithms into strong learning algorithms by simply increasing the sample size m. <p> Note that one can not convert weak learning algorithms into strong learning algorithms by simply increasing the sample size m. In fact, the error bound of 1 2 1 hypotheses produced by a weak learning algorithm can approach 1 2 as m increases. The conversion algorithms of <ref> [Sch90] </ref> and [Fre90] repeatedly use the weak learning algorithm on different "small" samples of size m = p 1 (n; s) (where p 1 (n; s) is the first polynomial in the weak learning algorithm definition). <p> Weak Occam Algorithms Here we define "weak Occam algorithms" whose hypothesis classes grow exponentially in m and show using the methods of Blumer et al. [BEHW87] that weak Occam algorithms lead to weak learning algorithms. Thus they can be used iteratively to build strong learning algorithms <ref> [Sch90, Fre90] </ref>. <p> This is not necessarily true for a weak Occam algorithm as the error (1=21=p 2 (m)) approaches 1=2 as m increases. Instead, the conversion algorithms of <ref> [Sch90, Fre90] </ref> use the weak Occam algorithm repeatedly for a number of different samples of size p 1 (n; s), where p 1 (n; s) is the size of the sample expected by the Occam algorithm when the parameters are n and s.
Reference: [STS90] <author> H. Sompolinsky, N. Tishby, and H.S. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys. Rev. Lett., </journal> <volume> 65 </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: Although the function g used by a volume prediction algorithm may be simple, computing the volume of a sample may not be computationally feasible. Here we consider three volume prediction algorithms. Algorithm Gibbs P (Gibbs Algorithm) is well known <ref> [HKS91, HO91, GT90, HS90, STS90] </ref> and can be viewed as predicting with a randomly chosen consistent concept from the class where the consistent concepts are weighted according to the prior P.
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <title> A. Bounds on the Information Gain Prediction Algorithm 35 </title>
Reference-contexts: Learning algorithms are called polynomial if both their running time and the running time for evaluating the output hypotheses on instances of length at most n is bounded by a polynomial in all four parameters. 1 This notion of learning was introduced by Valiant <ref> [Val84] </ref>. Not too many concept classes have been shown to be polynomially strongly learnable and a less stringent definition of learning was given by Kearns and Valiant [KV89]. <p> The two definitions are polynomially equivalent (see Lemma 3.4 of [HKLW91]). For a strong learning algorithm <ref> [Val84, BEHW89] </ref>, Inequality (3.1) is replaced by the inequality Pr x2D m ;r2U [0;1] [Err D (f; h) &gt; *] ffi; where * and ffi are additional paramenters in [0; 1].
Reference: [VC71] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Application, </title> <booktitle> 16(2) </booktitle> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: We also show that if a polynomial time probabilistic consistency oracle is available then it can be used to construct a polynomial weak learning algorithm for F whenever F is learnable at all with respect to an arbitrary distribution (i.e. the Vapnik-Chervonenkis dimension <ref> [VC71] </ref> of F grows polynomially in n and s [BEHW89]). Previously a direct construction of a strong learning algorithm using consistency oracles was given by Haussler, Littlestone, and Warmuth [HLW]. However that algorithm is only polynomial if the VC dimension of F is a constant independent of n and s. <p> If sam F (x) = sam fl (x) then x is shattered by F . The Vapnik-Chervonenkis dimension, or VC dimension, of a concept class F on X is the largest k such that there exists an x 2 X k that is shattered by F <ref> [VC71, BEHW89] </ref>. 5 If x is the empty sequence of instances, then sam f (x) is the empty sequence of examples. 4 3. <p> In Haussler, Littlestone, and Warmuth [HLW] it is shown that maxdens F (x) is upper bounded by the Vapnik-Chervonenkis dimension of the class F <ref> [VC71, BEHW89] </ref>. The main drawback of the 1-Inclusion Graph Prediction Algorithm is that it is not generally efficient as it solves flow problems on graphs containing jsam F (x)j vertices.
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <address> San Mateo, CA, August 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Algorithm G P is a special case of the aggregating strategy introduced by Vovk <ref> [Vov90] </ref>, and was used as the basis for a polynomial weak prediction algorithm [HW92b]. The classical Bayes Prediction Algorithm, Bayes P , is known to be optimal when the target is drawn according to the prior. <p> Bounds on Gibbs and Bayesian prediction 23 jxj X M (Gibbs P ; f; x) (ln 2) lg V P (sam f (x)) (9.3) The bounds on Bayes P and Gibbs P were shown in [HKS91] and the bound on G P appears in <ref> [Vov90] </ref> and is presented in Appendix A. It is easy to show that the constants in these bounds cannot be improved unless the form of the bounds are changed.
References-found: 25

