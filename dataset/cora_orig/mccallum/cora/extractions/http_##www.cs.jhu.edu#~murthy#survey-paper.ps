URL: http://www.cs.jhu.edu/~murthy/survey-paper.ps
Refering-URL: http://www.cs.jhu.edu/~murthy/
Root-URL: 
Email: Email: murthy@scr.siemens.com  
Phone: Phone: 609-734-3649 Fax: 609-734-6565  
Title: Automatic Construction of Decision Trees from Data: A MultiDisciplinary Survey  
Author: Author: Sreerama K. Murthy 
Address: 755 College Road East Princeton, NJ 08540 USA  
Affiliation: Siemens Corporate Research  
Note: For submission to the Data Mining and Knowledge Discovery journal Title:  Contact address:  Work  
Abstract-found: 0
Intro-found: 1
Reference: 1. <editor> AAAI. </editor> <booktitle> AAAI-92: Proc. of the Tenth National Conf. on Artificial Intelligence, </booktitle> <address> San Jose, CA, 12-16th, July 1992. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: Most of the above results consider only univariate decision tree construction. Intuitively, linear or multivariate tree construction should be more difficult than univariate tree construction, as there is a much larger space of splits to be searched. Heath <ref> (1992) </ref> proved that the problem of finding the split that minimizes the number of misclassified points, given two sets of mutually exclusive points, is NP-complete.
Reference: 2. <editor> AAAI. </editor> <booktitle> AAAI-93: Proc. of the Eleventh National Conf. on Artificial Intelligence, </booktitle> <address> Washing-ton, DC, 11-15th, July 1993. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: 3. <author> J. Aczel and J. Daroczy. </author> <title> On measures of information and their characterizations. </title> <publisher> Academic Pub., </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: We restrict almost entirely to classifi cation trees in this paper. 5. The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy [245] possesses all of these properties <ref> [3] </ref>. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [278]. 6.
Reference: 4. <author> David W. Aha and Richard L. Bankert. </author> <title> A comparitive evaluation of sequential feature selection algorithms. </title> <booktitle> In AI&Statistics-95 [6], </booktitle> <pages> pages 1-7. </pages>
Reference-contexts: There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [141, 158, 36, 58, 190, 4] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 5. <editor> AI&Stats-93: </editor> <booktitle> Preliminary Papers of the Fourth Int. Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 3rd-6th, </address> <month> January </month> <year> 1993. </year> <institution> Society for AI and Statistics. </institution>
Reference: 6. <editor> AI&Stats-95: </editor> <booktitle> Preliminary Papers of the Fifth Int. Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 4-7th, </address> <month> January </month> <year> 1995. </year> <institution> Society for AI and Statistics. </institution>
Reference: 7. <author> C. Aldrich, D. W. Moolman, F. S. Gouws, and G. P. J. Schmitz. </author> <title> Machine learning strategies for control of flotation plants. </title> <journal> Control Eng. Practice, </journal> <volume> 5(2) </volume> <pages> 263-269, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: De Merckt [269] suggested an attribute selection measure that combined geometric distance with information gain, and argued that such measures are more appropriate for numeric attribute spaces. Recently, Pal et al. <ref> (1997) </ref> proposed a variant of the ID3 algorithm for real data, in which tests at an internal node are found using genetic algorithms. Rules derived from distance measures: "Distance" here refers to the distance between class probability distributions.
Reference: 8. <author> Hussein Almuallim and Thomas G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 279-305, </pages> <year> 1994. </year>
Reference-contexts: 13 * The ratio of the sample size to dimensionality should vary inversely proportional to the amount of available knowledge about the class conditional densities. 16 In tasks where more features than the "optimal" are available, decision tree quality is known to be affected by the redundant and irrelevant attributes <ref> [8, 234] </ref>. To avoid this problem, either a feature subset selection method (Section 5.1.1) or a method to form a small set of composite features (Section 5.1.2) can be used as a preprocessing step to tree induction.
Reference: 9. <author> Peter Argentiero, Roland Chin, and Paul Beaudet. </author> <title> An automated approach to the design of decision tree classifiers. </title> <journal> IEEE Trans.on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-4(1):51-57, </volume> <month> January </month> <year> 1982. </year>
Reference-contexts: The problem of constructing identification keys is not the same as the problem of constructing decision trees from data, but many common concerns exist (e.g: optimal key construction, choosing good tests at tree nodes etc.). Moret <ref> (1982) </ref> provided a tutorial overview of the work on representing Boolean functions as decision trees and diagrams. He summarized results on constructing decision trees in discrete variable domains. Although Moret mentions some pattern recognition work on constructing decision trees from data, this was not his primary emphasis. <p> Methods used for selecting a good subset of features are typically quite different. We will postpone the discussion of feature subset selection methods to Section 5.1.1. A taxonomy, proposed by Ben-Bassat <ref> (1982) </ref>, is helpful in understanding the large number of existing feature evaluation criteria. Ben-Basset divides feature evaluation rules into three, not necessarily distinct, categories: rules derived from information theory, rules derived from distance measures and rules derived from dependence measures. <p> We concentrate in this paper on decision trees that are constructed from labelled examples. The problem of learning trees from decision rules instead of examples is addressed in [122]. The problem of learning trees solely from prior probability distributions is considered in <ref> [9] </ref>. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [215]. Several attempts at generalizing the decision tree representation exist. Chou (1988) considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes.
Reference: 10. <author> Haldun Aytug, Siddhartha Bhattacharya, Gary J. Koehler, and Jane L. Snowdon. </author> <title> A review of machine learning in scheduling. </title> <journal> IEEE Trans. on Eng. Management, </journal> <volume> 41(2) </volume> <pages> 165-171, </pages> <month> May </month> <year> 1994. </year>
Reference: 11. <author> L. Bahl, P.F.Brown, P.V. de Souza, and R. L. Mercer. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Trans. on Accoustics, Speech and Signal Processing, </journal> <volume> 37(7) </volume> <pages> 1001-1008, </pages> <year> 1989. </year> <month> 27 </month>
Reference-contexts: Use of linear regression to find good feature combinations has been explored recently in [23]. Discovery of good combinations of Boolean features to be used as tests at tree nodes is explored in the machine learning literature in [210] as well as in signal processing <ref> [11] </ref>. Ragavan and Rendell (1993) describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Lookahead for construction of Boolean feature combinations is also considered in [289]. Linear threshold unit trees for Boolean functions are described in [232]. <p> Quinlan (1990b) discussed methods of extracting probabilities from decision trees. Buntine (1992b) described Bayesian methods for building, smoothing and averaging class probability trees. 15 Smoothing in the context of tree structured vector quantizers is described in <ref> [11] </ref>. An approach, which refines the class probability estimates in a greedily induced decision tree using local kernel density estimates has been suggested in [252]. Assignment of probabilistic goodness to splits in a decision tree is described in [99].
Reference: 12. <author> F. A. Baker, David L. Verbyla, C. S. Hodges Jr., and E. W. Ross. </author> <title> Classification and regression tree analysis for assessing hazard of pine mortality caused by hetero basidion annosum. Plant Disease, </title> <address> 77(2):136, </address> <month> February </month> <year> 1993. </year>
Reference: 13. <author> W. A. Belson. </author> <title> Matching and prediction on the principle of biological classification. </title> <journal> Applied Statistics, </journal> <volume> 8 </volume> <pages> 65-75, </pages> <year> 1959. </year>
Reference-contexts: Taylor and Silverman (1993) pointed out that the Gini index emphasizes equal sized offspring and purity of both children. They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. Bhattacharya distance [166], Kolmogorov-Smirnoff distance [82, 228, 107] and the 2 statistic <ref> [13, 105, 181, 289, 281] </ref> are some other distance-based measures that have been used for tree induction. Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems [82, 228], it was subsequently extended to multiclass domains [107].
Reference: 14. <author> Moshe Ben-Bassat. </author> <title> Myopic policies in sequential classification. </title> <journal> IEEE Trans. on Computing, </journal> <volume> 27(2) </volume> <pages> 170-174, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: Baker and Jain (1976) reported experiments comparing eleven feature evaluation criteria and concluded that the feature rankings induced by various rules are very similar. Several feature evaluation criteria, including Shannon's entropy and divergence measures, are compared using simulated data in <ref> [14] </ref>, on a sequential, multi-class classification problem. The conclusions are that no feature selection rule is consistently superior to the others, and that no specific strategy for alternating different rules seems to be significantly more effective.
Reference: 15. <author> Moshe Ben-Bassat. </author> <title> Use of distance measures, information measures and error bounds on feature evaluation. </title> <booktitle> In Krishnaiah and Kanal [148], </booktitle> <pages> pages 773-791. </pages>
Reference-contexts: Rules derived from dependence measures: These measure the statistical dependence between two random variables. All dependence-bassed measures can be interpreted as belonging to one of the above two categories <ref> [15] </ref>. There exist several attribute selection criteria that do not clearly belong to any category in Ben-Basset's taxonomy. Gleser and Collen (1972) and Talmon (1986) used a combination of mutual information and 2 measures.
Reference: 16. <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [175, 18, 16, 174, 17] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [165]. Brown and Pittard [29] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 17. <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Multicategory discrimination via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 29-39, </pages> <year> 1994. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [175, 18, 16, 174, 17] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [165]. Brown and Pittard [29] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 18. <author> Kristin P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <booktitle> In Proc. of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conf., </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [175, 18, 16, 174, 17] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [165]. Brown and Pittard [29] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: 19. <author> Kristin P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <booktitle> In Proc. of Interface 94: The 26th Symposium on the Interface, </booktitle> <institution> Research Triangle, North Carolina, </institution> <year> 1994. </year>
Reference-contexts: Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary. In that sense, these methods are more similar to perceptron training methods [183], than to decision tree splitting criteria. 12 Mangasarian <ref> (1994) </ref> described a linear programming formulation to minimize the number of misclassified points instead of the geometric distance. Neural Trees: In the neural networks community, many researchers have considered hybrid structures between decision trees and neural nets. <p> An extension of entropy nets, that converts linear decision trees into neural nets was described in [211]. Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in [97]. Jordan and Jacobs <ref> (1994) </ref> described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [236]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory [241]. <p> Kim and Koehler <ref> (1994) </ref> analytically investigate the conditions under which pruning is beneficial for accuracy. Their main result states pruning is more beneficial with increasing skewness in class distribution and/or increasing sample size. <p> Several other pruning methods exist. Quinlan and Rivest (1989) used minimum description length [226] for tree construction as well as for pruning. An error in their coding method (which did not have an effect on their main conclusions) was pointed out in [273]. Forsyth et al. <ref> (1994) </ref> recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data. Use of dynamic programming to prune trees optimally and efficiently has been explored in [20]. <p> In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming [178], fuzzy logic search [275] and multi-linear programming <ref> [19] </ref>. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. <p> C4.5 [220] uses a simple form of soft splitting. Use of fuzzy splits in pattern recognition literature can be found in [241, 275]. Jordan and Jacobs <ref> (1994) </ref> describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [79]. Induction of fuzzy decision trees has also been considered in [159, 287]. 5.6. Estimating probabilities Decision trees have crisp decisions at leaf nodes. <p> He concluded that, on most real world data sets commonly used by the machine learning community [196], decision trees do not perform significantly better than one level rules. These conclusions, however, were refuted by Elomaa 24 <ref> (1994) </ref> on several grounds. Elomaa argued that Holte's observations may have been the peculiarities of the data he used, and that the slight differences in accuracy that Holte observed were still significant. Bias: Smaller consistent decision trees have higher generalization accuracy than larger consistent trees (Occam's Razor). <p> Elomaa argued that Holte's observations may have been the peculiarities of the data he used, and that the slight differences in accuracy that Holte observed were still significant. Bias: Smaller consistent decision trees have higher generalization accuracy than larger consistent trees (Occam's Razor). Analysis: Murphy and Pazzani <ref> (1994) </ref> empirically investigated the truth of this bias. Their experiments indicate that this conjecture seems to be true. However, their experiments indicate that the smallest decision trees typically have lesser generalization accuracy than trees that are slightly larger. In an extension of this study, Murphy (1994) evaluated the size bias as <p> Analysis: Murphy and Pazzani <ref> (1994) </ref> empirically investigated the truth of this bias. Their experiments indicate that this conjecture seems to be true. However, their experiments indicate that the smallest decision trees typically have lesser generalization accuracy than trees that are slightly larger. In an extension of this study, Murphy (1994) evaluated the size bias as a function of concept size.
Reference: 20. <author> Marko Bohanec and Ivan Bratko. </author> <title> Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: Forsyth et al. (1994) recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data. Use of dynamic programming to prune trees optimally and efficiently has been explored in <ref> [20] </ref>. A few studies have been done to study the relative effectiveness of pruning methods [182, 45, 65]. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others.
Reference: 21. <author> David Bowser-Chao and Debra L. Dzialo. </author> <title> Comparison of the use of binary decision trees and neural networks in top quark detection. Physical Review D: Particles and Fields, </title> <address> 47(5):1900, </address> <month> March </month> <year> 1993. </year>
Reference: 22. <author> D. Boyce, A. Farhi, and R. Weishedel. </author> <title> Optimal Subset Selection. </title> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: On the other hand, if the training sample has too many objects, a subsample selection method (Section 5.1.3) can be employed to filter out the unnecessary observations. 5.1.1. Feature subset selection There is a large body of work on choosing relevant subsets of features (for example, see the textbooks <ref> [60, 22, 180] </ref>). Most of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: 23. <author> Anna Bramanti-Gregor and Henry W. Davis. </author> <title> The statistical learning of accurate heuristics. </title> <booktitle> In IJCAI-93 [120], </booktitle> <pages> pages 1079-1085. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Friedman's (1977) tree induction 17 method could consider with equal ease atomic and composite features. Techniques to search for multivariate splits (Section 3.2) can be seen as ways for constructing composite features. Use of linear regression to find good feature combinations has been explored recently in <ref> [23] </ref>. Discovery of good combinations of Boolean features to be used as tests at tree nodes is explored in the machine learning literature in [210] as well as in signal processing [11].
Reference: 24. <author> Leo Breiman. </author> <title> Bagging predictors. </title> <type> Technical report, </type> <institution> Department of Statistics, Univ. of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [155, 247, 248, 34, 110, 24] </ref>.
Reference: 25. <author> Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Int. Group, </publisher> <year> 1984. </year>
Reference-contexts: A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [40]. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees <ref> [25] </ref>. For a review of earlier statistical work on hierarchical classification, see [75]. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s work and Quinlan's ID3 algorithm [219]. <p> The feature evaluation criteria in this class measure separability, divergence or discrimination between classes. A popular distance measure is the Gini index of diversity 7 , which has been used for tree construction in statistics <ref> [25] </ref>, pattern recognition [86] and sequential fault diagnosis [212]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [25] as a remedy. <p> the Gini index of diversity 7 , which has been used for tree construction in statistics <ref> [25] </ref>, pattern recognition [86] and sequential fault diagnosis [212]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [25] as a remedy. Taylor and Silverman (1993) pointed out that the Gini index emphasizes equal sized offspring and purity of both children. They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. <p> He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani (1992b) showed that their measure C-SEP, performs better than Gini index <ref> [25] </ref> and information gain [219] for specific types of problems. Several researchers [105, 219] pointed out that information gain is biased towards attributes with a large number of possible values. Mingers (1987) compared information gain and the 2 statistic for growing the tree as well as for stop-splitting. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [206, 207, 259, 263], pattern recognition: [56, 194], statistics: [140]) and incorporating misclassification costs <ref> [25, 48, 59, 39, 263] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 5.3. <p> They suggested an information based metric to evaluate a classifier, as a remedy to the above problems. Martin (1995a) argued that information theoretic measures of classifier complexity are not practically computable except within severely restricted families of classifiers, and suggested a generalized version of CART's <ref> [25] </ref> 1-standard error rule as a means of achieving a tradeoff between classifier complexity and accuracy. 21 Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier [221, <p> A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal in terms of average tree depth. Trees of maximal size generated by the CART algorithm <ref> [25] </ref> have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal [96]. <p> This conjecture is substantiated empirically in [199], where it is shown that the expected depth of trees greedily induced using information gain [219] and Gini index <ref> [25] </ref> is very close to that of the optimal, under a variety of experimental conditions. Relationship between feature evaluation by Shannon's entropy and the probability of error is investigated in [146, 224]. 7.
Reference: 26. <author> Richard P. Brent. </author> <title> Fast training algorithms for multilayer neural nets. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 2(3) </volume> <pages> 346-354, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [93, 250, 26, 43, 112, 235, 51] </ref>. Sethi (1990) described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits.
Reference: 27. <author> Carla E. Brodley. </author> <title> Recursive Automatic Algorithm Selection for Inductive Learning. </title> <type> PhD thesis, </type> <institution> Univ. of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1994. </year>
Reference-contexts: A lot of work exists in the neural networks literature on using committees or ensembles of networks to improve classification performance. See [104] for example. An alternative to multiple trees is a hybrid classifier that uses several small classifiers as parts of a larger classifier. Brodley <ref> [27] </ref> describes a system that automatically selects the most suitable among a uni-variate decision tree, a linear discriminant and an instance based classifier at each node of a hierarchical, recursive classifier. 17.
Reference: 28. <author> Carla E. Brodley and Paul E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: John [128] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [204], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [28, 59] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [276, 251] suggested a procedure to train a linear split to minimize the error probability.
Reference: 29. <author> Donald E. Brown and Clarence Louis Pittard. </author> <title> Classification trees with optimal multivariate splits. </title> <booktitle> In Proc. of the Int. Conf. on Systems, Man and Cybernetics, </booktitle> <volume> volume 3, </volume> <pages> pages 475-477, </pages> <address> Le Touquet, France, 17-20th, </address> <month> October </month> <year> 1993. </year> <booktitle> IEEE, </booktitle> <address> New York. </address>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [175, 18, 16, 174, 17]. Use of zero-one integer programming for designing vector quantizers can be found in [165]. Brown and Pittard <ref> [29] </ref> also employed linear programming for finding optimal multivariate splits at classification tree nodes. Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary.
Reference: 30. <author> R.S. Bucy and R.S. Diesposti. </author> <title> Decision tree design by simulated annealing. </title> <journal> Mathematical Modieling and Numerical Analysis, </journal> <volume> 27(5) </volume> <pages> 515-534, </pages> <year> 1993. </year> <note> A RAIRO J. </note>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques, such as genetic programming [147] and simulated annealing <ref> [30, 173] </ref>, have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [255].
Reference: 31. <author> M. E. Bullock, D. L. Wang, Fairchild S. R., and T. J. Patterson. </author> <title> Automated training of 3-D morphology algorithm for object recognition. </title> <booktitle> Proc. of SPIE The Int. Society for Optical Eng., </booktitle> <volume> 2234 </volume> <pages> 238-251, </pages> <year> 1994. </year> <title> Issue title: Automatic Object Recognition IV. </title>
Reference: 32. <author> Shashi D. Buluswer and Bruce A. Draper. </author> <title> Non-parametric classification of pixels under varying illumination. SPIE: </title> <journal> The Int. Society for Optical Eng., </journal> <volume> 2353 </volume> <pages> 529-536, </pages> <month> November </month> <year> 1994. </year>
Reference: 33. <author> Wray Buntine. </author> <title> A theory of learning classification rules. </title> <type> PhD thesis, </type> <institution> Univ. of Technology, </institution> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Chou (1988) considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes. Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [33, 34] </ref>. Oliver (1993) suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [272]. Rymon (1993) suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: 34. <author> Wray Buntine. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73, </pages> <year> 1992. </year>
Reference-contexts: For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [192]. Tree construction using partial or exhaustive lookahead has been considered in statistics [75, 63], in pattern recognition [106], for tree structured vector quantizers [227], for Bayesian class probability trees <ref> [34] </ref>, for neural trees [51] and in machine learning [206, 222, 200]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. <p> A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [155, 247, 248, 34, 110, 24] </ref>. <p> Chou (1988) considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes. Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [33, 34] </ref>. Oliver (1993) suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [272]. Rymon (1993) suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: 35. <author> Jan M. Van Campenhout. </author> <title> Topics in measurement selection. </title> <booktitle> In Krishnaiah and Kanal [148], </booktitle> <pages> pages 793-803. </pages>
Reference-contexts: Efron [62] showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples. However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [144]. 13. Van Campenhout <ref> [35] </ref> argues that increasing the amount of information in a measurement subset through enlarging its size or complexity never worsens the error probability of a truly Bayesian classifier.
Reference: 36. <author> Rich Caruana and Dayne Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In ML-94 [187], </booktitle> <pages> pages 28-36. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: In stepwise backward elimination, we start with the full feature set and remove, at each step, the worst feature. When more than one feature is greedily added or removed, beam search is said to have been performed <ref> [249, 36] </ref>. A combination of forward selection and backward elimination, a bidirectional search, was attempted in [249]. <p> There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [141, 158, 36, 58, 190, 4] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 37. <author> Richard G. Casey and George Nagy. </author> <title> Decision tree design using a probabilistic model. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-30(1):93-99, </volume> <month> January </month> <year> 1984. </year> <month> 28 </month>
Reference-contexts: i.e., by expanding tree nodes that contribute to the largest gain in average mutual information of the whole tree, is explored in pattern recognition [92, 243, 257]. 6 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [106, 274, 37, 103] </ref>, in sequential fault diagnosis [270] and in machine learning [219]. Mingers [181] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop.
Reference: 38. <author> Jason Catlett. </author> <title> Megainduction. </title> <type> PhD thesis, </type> <institution> Basser Department of Computer Science, Univ. of Sydney, Australia, </institution> <year> 1991. </year>
Reference-contexts: For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) <ref> [274, 38] </ref>. Breiman et al. (1984) pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [203, 142]. Overfitting avoidance as a specific bias is studied in [284, 237].
Reference: 39. <author> Jason Catlett. </author> <title> Tailoring rulesets to misclassification costs. </title> <booktitle> In AI&Statistics-95 [6], </booktitle> <pages> pages 88-94. </pages>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [206, 207, 259, 263], pattern recognition: [56, 194], statistics: [140]) and incorporating misclassification costs <ref> [25, 48, 59, 39, 263] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 5.3.
Reference: 40. <author> B. Chandrasekaran. </author> <title> From numbers to symbols to knowledge structures: </title> <booktitle> Pattern Recognition and Artificial Intelligence perspectives on the classification task. </booktitle> <volume> volume 2, </volume> <pages> pages 547-559. </pages> <publisher> Elsevier Science, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference-contexts: Treatises and surveys An overview of work on decision trees in the patter recognition literature can be found in [55]. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in <ref> [40] </ref>. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees [25]. For a review of earlier statistical work on hierarchical classification, see [75].
Reference: 41. <author> B. Chandrasekaran and A. K. Jain. </author> <title> Quantization complexity and independent measurements. </title> <journal> IEEE Trans. on Comp., </journal> <volume> C-23(1):102-106, </volume> <month> January </month> <year> 1974. </year>
Reference-contexts: In this section, we address several such issues. 5.1. Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature <ref> [113, 134, 78, 41, 132, 151, 126, 83] </ref>. Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: 42. <author> Philip A. Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4) </volume> <pages> 340-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This seems natural considering application domains such as spectral analysis and remote sensing [256]. In these fields, special techniques [242] were developed to accommodate discrete attributes into what are primarily algorithms for ordered attributes. Fast methods for splitting multiple valued categorical variables are described in <ref> [42] </ref>. In machine learning, a subfield of Artificial Intelligence, which in turn has been dominated by symbolic processing, many tree induction methods (e.g., [217] were originally developed for categorical attributes. The problem of incorporating continuous attributes into these algorithms is considered subsequently.
Reference: 43. <author> Krzysztof J. Cios and Ning Liu. </author> <title> A machine learning method for generation of a neural network architecture: A continuous ID3 algorithm. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 280-291, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [93, 250, 26, 43, 112, 235, 51] </ref>. Sethi (1990) described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits.
Reference: 44. <author> I. Cleote and H. Theron. CID3: </author> <title> An extension of ID3 for attributes with ordered domains. </title> <journal> South African Computer J., </journal> <volume> 4 </volume> <pages> 10-16, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [73, 84]. 10 An extension to ID3 [219] 13 that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in <ref> [44] </ref>. Quinlan (1996) recently discussed improved ways of using continuous attributes with C4.5. 4. Obtaining the right sized trees See Breslow and Aha's recent survey (1996) on simplifying decision trees for a detailed account of the motivation for tree simplification and existing solution approaches.
Reference: 45. <author> W.W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In IJCAI-93 [120], </booktitle> <pages> pages 988-994. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored in [20]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [182, 45, 65] </ref>. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others. The choice of a pruning method depends on the size of the training set, availability of extra data for pruning etc. 5.
Reference: 46. <author> T.M. Cover and J.M. Van Campenhout. </author> <title> On the possible orderings in the measurement selection problems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> SMC-7(9), </volume> <year> 1977. </year>
Reference-contexts: A combination of forward selection and backward elimination, a bidirectional search, was attempted in [249]. Comparisons of heuristic feature subset selection methods resound the conclusions of studies comparing feature evaluation criteria and studies comparing pruning methods | no feature subset selection heuristic is far superior to the others. <ref> [46, 268] </ref> showed that heuristic sequential feature selection methods can do arbitrarily worse than the optimal strategy. Mucciardi and Gose (1971) compared seven feature subset selection techniques empirically and concluded that no technique was uniformly superior to the others.
Reference: 47. <author> Louis Anthony Cox. </author> <title> Using causal knowledge to learn more useful decision rules from data. </title> <booktitle> In AI&Statistics-95 [6], </booktitle> <pages> pages 151-160. </pages>
Reference-contexts: Oliver (1993) suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [272]. Rymon (1993) suggested SE-trees, set enumeration structures each of which can embed several decision trees. Cox <ref> [47] </ref> argues that classification tree technology, as implemented in commercially available systems, is often more useful for pattern recognition than it is for decision support. He suggests several ways of modifying existing methods to be prescriptive rather than descriptive.
Reference: 48. <author> Louis Anthony Cox and Yuping Qiu. </author> <title> Minimizing the expected costs of classifying patterns by sequential costly inspections. </title> <booktitle> In AI&Statistics-93 [5]. </booktitle>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [206, 207, 259, 263], pattern recognition: [56, 194], statistics: [140]) and incorporating misclassification costs <ref> [25, 48, 59, 39, 263] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 5.3.
Reference: 49. <author> Stuart L. Crawford. </author> <title> Extensions to the CART algorithm. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 31(2) </volume> <pages> 197-217, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: This occurs because the best split at a node vacillates widely while the sample at the node is still small. An incremental version of CART that uses significance thresholds to avoid the above problem is described in <ref> [49] </ref>. 5.9. Tree quality measures The fact that several trees can correctly represent the same data raises the question of how to decide that one tree is better than another. Several measures have been suggested to quantify tree quality.
Reference: 50. <author> K.T. Dago, R. Luthringer, R. Lengelle, G. Rinaudo, and J. P. </author> <title> Matcher. Statistical decision tree: A tool for studying pharmaco-EEG effects of CNS-active drugs. </title> <journal> Neuropsychobiology, </journal> <volume> 29(2) </volume> <pages> 91-96, </pages> <year> 1994. </year>
Reference: 51. <author> Florence DAlche-Buc, Didier Zwierski, and Jean-Pierre Nadal. </author> <title> Trio learning: A new strategy for building hybrid neural trees. </title> <journal> Int. J. of Neural Systems, </journal> <volume> 5(4) </volume> <pages> 259-274, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [93, 250, 26, 43, 112, 235, 51] </ref>. Sethi (1990) described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits. <p> Tree construction using partial or exhaustive lookahead has been considered in statistics [75, 63], in pattern recognition [106], for tree structured vector quantizers [227], for Bayesian class probability trees [34], for neural trees <ref> [51] </ref> and in machine learning [206, 222, 200]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: 52. <author> S.K. Das and S. Bhambri. </author> <title> A decision tree approach for selecting between demand based, reorder and JIT/kanban methods for material procurement. Production Planning and Control, </title> <address> 5(4):342, </address> <year> 1994. </year>
Reference: 53. <author> Belur V. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Quinlan suggested "windowing", a random training set sampling method, for his programs ID3 and C4.5 [220, 283]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [53, 54] </ref>, for examples). Some of these techniques may be helpful for inducing decision trees on large samples, provided they are efficient. 5.2. Incorporating costs In most real-world domains, attributes can have costs of measurement, and objects can have misclassification costs.
Reference: 54. <author> Belur V. Dasarathy. </author> <title> Minimal consistent set (MCS) identification for optimal nearest neighbor systems design. </title> <journal> IEEE Trans. on systems, man and cybernetics, </journal> <volume> 24(3) </volume> <pages> 511-517, </pages> <year> 1994. </year>
Reference-contexts: Quinlan suggested "windowing", a random training set sampling method, for his programs ID3 and C4.5 [220, 283]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [53, 54] </ref>, for examples). Some of these techniques may be helpful for inducing decision trees on large samples, provided they are efficient. 5.2. Incorporating costs In most real-world domains, attributes can have costs of measurement, and objects can have misclassification costs.
Reference: 55. <editor> G. R. Dattatreya and Laveen N. Kanal. </editor> <title> Decision trees in pattern recognition. </title> <editor> In Kanal and Rosenfeld, editors, </editor> <booktitle> Progress in Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 189-239. </pages> <publisher> Elsevier Science, </publisher> <year> 1985. </year>
Reference-contexts: The testing algorithms normally take the form of decision trees or AND/OR trees [270, 212]. Many heuristics used to construct decision trees are used for test sequencing also. 2.2. Treatises and surveys An overview of work on decision trees in the patter recognition literature can be found in <ref> [55] </ref>. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [40]. Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees [25].
Reference: 56. <author> G. R. Dattatreya and V. V. S. Sarma. </author> <title> Bayesian and decision tree approaches to pattern recognition including feature measurement costs. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-3(3):293-298, </volume> <year> 1981. </year>
Reference-contexts: If the measurement (misclassification) costs are not identical between different attributes (classes), decision tree algorithms may need to be designed explicitly to prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [206, 207, 259, 263], pattern recognition: <ref> [56, 194] </ref>, statistics: [140]) and incorporating misclassification costs [25, 48, 59, 39, 263]. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 5.3.
Reference: 57. <author> Thomas G. Dietterich and Eun Bae Kong. </author> <title> Machine learning bias, statistical bias and statistical variance of decision tree algorithms. </title> <note> In ML-95 [188]. to appear. </note>
Reference-contexts: Multiple trees A known peril of decision tree construction is its variance, especially when the samples are small and the features are many <ref> [57] </ref>. Variance can be caused by random choice of training and pruning samples, by many equally good attributes only one of which can be chosen at a node, due to cross validation or because of other reasons. <p> A c-regular tree is a tree in which all nodes have c children, and if one child of an internal node is a leaf, then so are all other children. A tree is regular is it is c-regular for any c. 18. It is argued empirically <ref> [57] </ref> that the variance in decision tree methods is more a reason than bias for their poor performance on some domains.
Reference: 58. <author> Justin Doak. </author> <title> An evaluation of search algorithms for feature selection. </title> <type> Technical report, </type> <institution> Graduate Group in Computer Science, Univ. of California at Davis; and Safeguards Systems Group, Los Alamos National Lab., </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations. These studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets <ref> [141, 158, 36, 58, 190, 4] </ref>. 5.1.2. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: 59. <author> B. A. Draper, Carla E. Brodley, and Paul E. Utgoff. </author> <title> Goal-directed classification using linear machine decision trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(9):888, </volume> <year> 1994. </year>
Reference-contexts: John [128] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [204], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [28, 59] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [276, 251] suggested a procedure to train a linear split to minimize the error probability. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (machine learning: [206, 207, 259, 263], pattern recognition: [56, 194], statistics: [140]) and incorporating misclassification costs <ref> [25, 48, 59, 39, 263] </ref>. Methods to incorporate attribute measurement costs typically include a cost term into the feature evaluation criterion, whereas variable misclassification costs are accounted for by using prior probabilities or cost matrices. 5.3.
Reference: 60. <author> N. R. Draper and H. Smith. </author> <title> Applied Regression Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1966. </year> <note> 2nd edition in 1981. </note>
Reference-contexts: On the other hand, if the training sample has too many objects, a subsample selection method (Section 5.1.3) can be employed to filter out the unnecessary observations. 5.1.1. Feature subset selection There is a large body of work on choosing relevant subsets of features (for example, see the textbooks <ref> [60, 22, 180] </ref>). Most of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: 61. <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Methods used in the literature for finding good linear tests include linear discriminant analysis, hill climbing search, linear programming, perceptron training and others. Linear Discriminant Trees: Several authors have considered the problem of constructing tree-structured classifiers that have linear discriminants <ref> [61] </ref> at each node. You and Fu (1976) used a linear discriminant at each node in the decision tree, computing the hyperplane coefficients using the Fletcher-Powell descent method (1963). Their method requires that the best set of features at each node be prespecified by a human.
Reference: 62. <author> Bradley Efron. </author> <title> Estimating the error rate of a prediction rule: improvements on cross-validation. </title> <journal> J. of American Statistical Association, </journal> <volume> 78(382) </volume> <pages> 316-331, </pages> <month> June </month> <year> 1983. </year> <month> 29 </month>
Reference-contexts: In bootstrapping, B independent learning samples, each of size N are created by random sampling with replacement from the original learning sample L. In cross validation, L is divided randomly into B mutually exclusive, equal sized partitions. Efron <ref> [62] </ref> showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples. However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [144]. 13.
Reference: 63. <author> John F. Elder, </author> <title> IV. Heuristic search for model structure. </title> <booktitle> In AI&Statistics-95 [6], </booktitle> <pages> pages 199-210. </pages>
Reference-contexts: The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [192]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [75, 63] </ref>, in pattern recognition [106], for tree structured vector quantizers [227], for Bayesian class probability trees [34], for neural trees [51] and in machine learning [206, 222, 200]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: 64. <author> A. Ercil. </author> <title> Classification trees prove useful in nondestructive testing of spotweld quality. </title> <editor> Welding J., 72(9):59, </editor> <month> September </month> <year> 1993. </year> <note> Issue Title: Special emphasis: Rebuilding America's roads, railways and bridges. </note>
Reference: 65. <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> A further study of pruning methods in decision tree induction. </title> <booktitle> In AI&Statistics-95 [6], </booktitle> <pages> pages 211-218. </pages>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored in [20]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [182, 45, 65] </ref>. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others. The choice of a pruning method depends on the size of the training set, availability of extra data for pruning etc. 5.
Reference: 66. <author> Bob Evans and Doug Fisher. </author> <title> Overcoming process delays with decision tree induction. </title> <journal> IEEE Expert, </journal> <pages> pages 60-66, </pages> <month> February </month> <year> 1994. </year>
Reference: 67. <author> Brian Everitt. </author> <title> Cluster Analysis 3rd Edition. </title> <editor> E. </editor> <publisher> Arnold Press, </publisher> <address> London., </address> <year> 1993. </year>
Reference-contexts: confer ences. * Our coverage of decision tree applications falls far short of being comprehensive; it is merely illustrative. * Most of the work on automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis <ref> [67] </ref>, machine learning (e.g., [77, 88]) and vector quantization [89] is not covered. Theoretical work on the learnability of decision trees, and work on hand-constructed decision trees (common in medicine) are also not considered, due to the author's limited knowledge in these topics. 1.1.
Reference: 68. <author> Judith A. Falconer, Bruce J. Naughton, Dorothy D. Dunlop, Elliot J. Roth, and Dale C. Strasser. </author> <title> Predicting stroke inpatient rehabilitation outcome using a classification tree approach. </title> <journal> Archives of Physical Medicine and Rehabilitation, </journal> <volume> 75(6):619, </volume> <month> June </month> <year> 1994. </year>
Reference: 69. <author> A. Famili. </author> <title> Use of decision tree induction for process optimization and knowledge refinement of an industrial process. </title> <journal> Artificial Intelligence for Eng. Design, Analysis and Manufacturing (AI EDAM), </journal> <volume> 8(1) </volume> <pages> 63-75, </pages> <month> Winter </month> <year> 1994. </year>
Reference: 70. <author> R. M. Fano. </author> <title> Transmission of Information. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: Goodrich [95] proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 23 6.2. Theoretical Insights Goodman and Smyth (1988) showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding <ref> [70] </ref>. A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal in terms of average tree depth.
Reference: 71. <author> Usama M. Fayyad and Keki B. Irani. </author> <title> The attribute specification problem in decision tree generation. </title> <booktitle> In AAAI-92 [1], </booktitle> <pages> pages 104-110. </pages>
Reference-contexts: Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems [82, 228], it was subsequently extended to multiclass domains [107]. Class separation-based metrics developed in the machine learning literature <ref> [71, 288] </ref> are also distance measures. A relatively simplistic method for estimating class separation, which assumes that 8 the values of each feature follow a Gaussian distribution in each class, was used for tree construction in [172].
Reference: 72. <author> Usama M. Fayyad and Keki B. Irani. </author> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8(2) </volume> <pages> 87-102, </pages> <year> 1992. </year>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in <ref> [72, 138, 269, 195] </ref>. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [73, 84]. 10 An extension to ID3 [219] 13 that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in [44].
Reference: 73. <author> Usama M. Fayyad and Keki B. Irani. </author> <title> Multi-interval discretization of continuous valued attributes for classification learning. </title> <booktitle> In IJCAI-93 [120], </booktitle> <pages> pages 1022-1027. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in [72, 138, 269, 195]. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature <ref> [73, 84] </ref>. 10 An extension to ID3 [219] 13 that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in [44]. Quinlan (1996) recently discussed improved ways of using continuous attributes with C4.5. 4.
Reference: 74. <author> Edward A. Feigenbaum. </author> <title> Expert systems in the 1980s. </title> <editor> In A. Bond, editor, </editor> <booktitle> State of the Art in Machine Intelligence. </booktitle> <address> Pergamon-Infotech, Maidenhead, </address> <year> 1981. </year>
Reference-contexts: Pattern recognition work on decision trees was motivated by the need to interpret images from remote sensing satellites such as LANDSAT in the 1970s [256]. Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck <ref> [74] </ref> for expert systems. In sequential fault diagnosis, the inputs are a set of possible tests with associated costs and a set of system states with associated prior probabilities. One of the states is a "fault-free" state and the other states represent distinct faults.
Reference: 75. <author> A. Fielding. </author> <title> Binary segmentation: the automatic interaction detector and related techniques for exploring data structure. </title> <booktitle> In O'Muircheartaigh and Payne [209], </booktitle> <pages> pages 221-257. </pages>
Reference-contexts: High level pointers A decision tree performs mutistage hierarchical decision making. For a general rationale for multistage classification schemes and a categorization of such schemes, see [133]. 2.1. Origins Work on decision tree induction in statistics began due to the need for exploring survey data <ref> [75] </ref>. Statistical programs such as AID [253], MAID [91], THAID [193] and CHAID [135] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables. <p> Tree induction from a statistical perspective, as it is popularly used today, is reviewed in Breiman et al.'s excellent book Classification and Regression Trees [25]. For a review of earlier statistical work on hierarchical classification, see <ref> [75] </ref>. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s work and Quinlan's ID3 algorithm [219]. Quinlan's book on C4.5 [220], although specific to 6 his tree building program, provides an outline of tree induction methodology from a machine learning perspective. <p> Another measure suggested by Heath, called the sum of impurities, assigns an integer to each class and measures the variance between class numbers in each partition [111, 198]. An almost identical measure was used earlier in the Automatic Interaction Detection (AID) program <ref> [75] </ref>. Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. <p> The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [192]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [75, 63] </ref>, in pattern recognition [106], for tree structured vector quantizers [227], for Bayesian class probability trees [34], for neural trees [51] and in machine learning [206, 222, 200]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: 76. <author> P. E. File, P. I. Dugard, and A. S. Houston. </author> <title> Evaluation of the use of induction in the develeopment of a medical expert system. </title> <journal> Comp. and Biomedical Research, </journal> <volume> 27(5) </volume> <pages> 383-395, </pages> <month> October </month> <year> 1994. </year>
Reference: 77. <author> Douglas Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 130-172, </pages> <year> 1987. </year>
Reference-contexts: coverage of decision tree applications falls far short of being comprehensive; it is merely illustrative. * Most of the work on automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis [67], machine learning (e.g., <ref> [77, 88] </ref>) and vector quantization [89] is not covered. Theoretical work on the learnability of decision trees, and work on hand-constructed decision trees (common in medicine) are also not considered, due to the author's limited knowledge in these topics. 1.1.
Reference: 78. <author> D. H. Foley. </author> <title> Considerations of sample and feature size. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-18:618-626, </volume> <year> 1972. </year>
Reference-contexts: In this section, we address several such issues. 5.1. Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature <ref> [113, 134, 78, 41, 132, 151, 126, 83] </ref>. Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: 79. <author> F. Forouraghi, L. W. Schmerr, and G. M. Prabhu. </author> <title> Induction of multivariate regression trees for design optimization. </title> <booktitle> volume 1, </booktitle> <pages> pages 607-612, </pages> <address> Seattle, WA, </address> <booktitle> 31st July 4th August 1994. </booktitle> <publisher> AAAI, AAAI Press / The MIT Press. </publisher>
Reference-contexts: C4.5 [220] uses a simple form of soft splitting. Use of fuzzy splits in pattern recognition literature can be found in [241, 275]. Jordan and Jacobs (1994) describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered <ref> [79] </ref>. Induction of fuzzy decision trees has also been considered in [159, 287]. 5.6. Estimating probabilities Decision trees have crisp decisions at leaf nodes. On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes.
Reference: 80. <author> Iman Foroutan and Jack Sklansky. </author> <title> Feature selection for automatic classification of non-Gaussian data. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 17(2) </volume> <pages> 187-198, </pages> <month> March/April </month> <year> 1987. </year>
Reference-contexts: There are two components to any method that attempts to choose the best subset of features. The first is a metric using which two feature subsets can be compared to determine which is better. Feature subsets have been compared in the literature using direct error estimation <ref> [80, 129] </ref> and using feature evaluation criteria discussed in Section 3.1 (e.g. Bhattacharya distance was used for comparing subsets of features in [201]). The second component of feature subset selection methods is a search algorithm through the space of possible feature subsets.
Reference: 81. <author> Richard S. Forsyth, David D. Clarke, and Richard L. Wright. </author> <title> Overfitting revisited: an information-theoretic approach to simplifying discrimination trees. </title> <journal> J. of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6(3) </volume> <pages> 289-302, </pages> <month> July-September </month> <year> 1994. </year>
Reference-contexts: [25] 1-standard error rule as a means of achieving a tradeoff between classifier complexity and accuracy. 21 Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier <ref> [221, 81] </ref> . 5.10. Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion. Bottom up induction of trees is considered in [157]. Bottom up tree induction is also common [212] in problems such as building identification keys and optimal test sequences.
Reference: 82. <author> Jerome H. Friedman. </author> <title> A recursive partitioning decision rule for nonparametric classifiers. </title> <journal> IEEE Trans. on Comp., </journal> <volume> C-26:404-408, </volume> <month> April </month> <year> 1977. </year>
Reference-contexts: Taylor and Silverman (1993) pointed out that the Gini index emphasizes equal sized offspring and purity of both children. They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. Bhattacharya distance [166], Kolmogorov-Smirnoff distance <ref> [82, 228, 107] </ref> and the 2 statistic [13, 105, 181, 289, 281] are some other distance-based measures that have been used for tree induction. Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems [82, 228], it was subsequently extended to multiclass domains [107]. <p> Bhattacharya distance [166], Kolmogorov-Smirnoff distance [82, 228, 107] and the 2 statistic [13, 105, 181, 289, 281] are some other distance-based measures that have been used for tree induction. Though the Kolmogorov-Smirnoff distance was originally proposed for tree induction in two-class problems <ref> [82, 228] </ref>, it was subsequently extended to multiclass domains [107]. Class separation-based metrics developed in the machine learning literature [71, 288] are also distance measures. <p> This strategy, which is known to be not robust, is used in some early methods <ref> [82] </ref>. * Two stage search: In this variant, tree induction is divided into two subtasks: first, a good structure for the tree is determined; then splits are found at all the nodes. 11 The optimization method in the first stage may or may not be related to that used in the
Reference: 83. <author> Keinosuke Fukanaga and R. A. Hayes. </author> <title> Effect of sample size in classifier design. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 873-885, </pages> <year> 1989. </year>
Reference-contexts: In this section, we address several such issues. 5.1. Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature <ref> [113, 134, 78, 41, 132, 151, 126, 83] </ref>. Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: 84. <author> Truxton K. Fulton, Simon Kasif, and Steven Salzberg. </author> <title> An efficient algorithm for for finding multi-way splits for decision trees. </title> <note> In ML-95 [188]. to appear. </note>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in [72, 138, 269, 195]. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature <ref> [73, 84] </ref>. 10 An extension to ID3 [219] 13 that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in [44]. Quinlan (1996) recently discussed improved ways of using continuous attributes with C4.5. 4.
Reference: 85. <author> Michael R. Garey and Ronald L. Graham. </author> <title> Performance bounds on the splitting algorithm for binary testing. </title> <journal> Acta Informatica, 3(Fasc. </journal> 4):347-355, 1974. 
Reference-contexts: Breiman et al.'s CART system (1984) more or less implemented Friedman's suggestions. Quinlan (1989) also considered the problem of missing attribute values. 5.4. Improving on greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. Several authors (e.g., <ref> [85, 223] </ref>) pointed out the inadequacy of greedy induction for difficult concepts. The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [192].
Reference: 86. <author> Saul B. Gelfand, C. S. Ravishankar, and Edward J. Delp. </author> <title> An iterative growing and pruning algorithm for classification tree design. </title> <journal> IEEE Transaction on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(2) </volume> <pages> 163-174, </pages> <month> February </month> <year> 1991. </year> <month> 30 </month>
Reference-contexts: The feature evaluation criteria in this class measure separability, divergence or discrimination between classes. A popular distance measure is the Gini index of diversity 7 , which has been used for tree construction in statistics [25], pattern recognition <ref> [86] </ref> and sequential fault diagnosis [212]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [25] as a remedy. <p> One of the main difficulties of inducing a recursive partitioning structure is knowing when to stop. Obtaining the "right" sized trees is important for several reasons, which depend on the size of the classification problem <ref> [86] </ref>. For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) [274, 38].
Reference: 87. <editor> Edzard S. Gelsema and Laveen S. Kanal, editors. </editor> <title> Pattern Recognition in Practice IV: Multiple paradigms, Comparative studies and hybrid systems, </title> <booktitle> volume 16 of Machine Intelligence and Pattern Recognition. </booktitle> <editor> Series editors: Kanal, L. S. and Rozenfeld, A. </editor> <publisher> Elsevier, </publisher> <year> 1994. </year>
Reference: 88. <author> G. H. Gennari, Pat Langley, and Douglas Fisher. </author> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):11-62, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: coverage of decision tree applications falls far short of being comprehensive; it is merely illustrative. * Most of the work on automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis [67], machine learning (e.g., <ref> [77, 88] </ref>) and vector quantization [89] is not covered. Theoretical work on the learnability of decision trees, and work on hand-constructed decision trees (common in medicine) are also not considered, due to the author's limited knowledge in these topics. 1.1.
Reference: 89. <author> Allen Gersho and Robert M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Academic Pub., </publisher> <year> 1991. </year>
Reference-contexts: falls far short of being comprehensive; it is merely illustrative. * Most of the work on automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis [67], machine learning (e.g., [77, 88]) and vector quantization <ref> [89] </ref> is not covered. Theoretical work on the learnability of decision trees, and work on hand-constructed decision trees (common in medicine) are also not considered, due to the author's limited knowledge in these topics. 1.1.
Reference: 90. <author> W. J. Gibb, D. M. Auslander, and J. C. Griffin. </author> <title> Selection of myocardial electrogram features for use by implantable devices. </title> <journal> IEEE Trans. on Biomedical Eng., </journal> <volume> 40(8) </volume> <pages> 727-735, </pages> <month> August </month> <year> 1993. </year>
Reference: 91. <author> M. W. Gillo. MAID: </author> <title> A Honeywell 600 program for an automatised survey analysis. </title> <booktitle> Behavioral Science, </booktitle> <volume> 17 </volume> <pages> 251-252, </pages> <year> 1972. </year>
Reference-contexts: For a general rationale for multistage classification schemes and a categorization of such schemes, see [133]. 2.1. Origins Work on decision tree induction in statistics began due to the need for exploring survey data [75]. Statistical programs such as AID [253], MAID <ref> [91] </ref>, THAID [193] and CHAID [135] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables. Pattern recognition work on decision trees was motivated by the need to interpret images from remote sensing satellites such as LANDSAT in the 1970s [256].
Reference: 92. <author> Malcolm A. Gleser and Morris F. Collen. </author> <title> Towards automated medical decisions. </title> <journal> Comp. and Biomedical Research, </journal> <volume> 5(2) </volume> <pages> 180-189, </pages> <month> April </month> <year> 1972. </year>
Reference-contexts: Rules derived from information theory: Examples of this variety are rules based on Shannon's entropy. 5 Tree construction by maximizing global mutual information, i.e., by expanding tree nodes that contribute to the largest gain in average mutual information of the whole tree, is explored in pattern recognition <ref> [92, 243, 257] </ref>. 6 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [106, 274, 37, 103], in sequential fault diagnosis [270] and in machine learning [219]. <p> Thresholds can be imposed on local (i.e., individual node) goodness measures or on global (i.e., entire tree) goodness. The former alternative is used in <ref> [92, 228, 218, 176] </ref> and the latter in [243]. A problem with the former method is that the value of most splitting criteria (Section 3.1) varies with the size of the training sample.
Reference: 93. <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> Euro-Physics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. Examples of this work include <ref> [93, 250, 26, 43, 112, 235, 51] </ref>. Sethi (1990) described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits.
Reference: 94. <author> Rodney M. Goodman and Padhraic J. Smyth. </author> <title> Decision tree design from a communication theory standpoint. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 34(5) </volume> <pages> 979-994, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy [245] possesses all of these properties [3]. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [278]. 6. Goodman and Smyth <ref> [94] </ref> report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [161]. 7. named after the Italian economist Corrado Gini (1884-1965) 8.
Reference: 95. <author> Michael T. Goodrich, Vincent Mirelli, Mark Orletsky, and Jeffery Salowe. </author> <title> Decision tree conctruction in fixed dimensions: Being global is hard but local greed is good. </title> <type> Technical Report TR-95-1, </type> <institution> Johns Hopkins Univ., Department of Computer Science, </institution> <address> Baltimore, MD 21218, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Shang and Breiman (1996) argue that trees built from probability distributions (which in turn are inferred from attribute values) are more accurate than trees built di 9 rectly from attribute values. Grewe and Kak <ref> (1995) </ref> recently proposed a method for building multi-attribute hash tables using decision trees for object localization and detection in 3D. Their decision trees are also built from probability distributions of attributes rather than the attribute values themselves. 3.1.1. <p> A hypergeometric distribution is proposed as a means to avoid the biases of information gain, gain ratio and 2 met-rics in [176]. Kononenko <ref> (1995) </ref> recently pointed out that Minimum Description Length-based feature evaluation criteria have the least bias towards multi-valued attributes. 3.2. Multivariate splits Decision trees have been popularly univariate, i.e., they use splits based on a single attribute at each internal node. <p> Heath (1992) proved that the problem of finding the split that minimizes the number of misclassified points, given two sets of mutually exclusive points, is NP-complete. Hoeffgen et al. <ref> (1995) </ref> proved that a more general problem is NP-hard | they proved that, for any C 1, the problem of finding a hyperplane that misclassifies no more than C fl opt examples, where opt is the minimum number of misclassifications possible using a hyperplane, is also NP-hard. <p> This does not seem to be the case either . Blum and Rivest (1988) showed that the problem of constructing an optimal 3-node neural network is NP-complete. Goodrich <ref> [95] </ref> proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 23 6.2. Theoretical Insights Goodman and Smyth (1988) showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding [70].
Reference: 96. <author> L. Gordon and R. A. Olshen. </author> <title> Asymptotically efficient solutions to the classification problem. </title> <journal> Annals of Statistics, </journal> <volume> 6(3) </volume> <pages> 515-533, </pages> <year> 1978. </year>
Reference-contexts: Trees of maximal size generated by the CART algorithm [25] have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal <ref> [96] </ref>. Miyakawa (1985) considered the problem of coverting decision tables to optimal trees, and studied the properties of optimal variables, the class of attributes only members of which can be used at the root of an optimal tree.
Reference: 97. <author> Heng Guo and Saul B. Gelfand. </author> <title> Classification trees with neural network feature extraction. </title> <journal> IEEE Trans. on Neural Networks., </journal> <volume> 3(6) </volume> <pages> 923-933, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: An extension of entropy nets, that converts linear decision trees into neural nets was described in [211]. Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in <ref> [97] </ref>. Jordan and Jacobs (1994) described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [236]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory [241].
Reference: 98. <author> Y. Guo and K.J. Dooley. </author> <title> Distinguishing between mean, variance and autocorrelation changes in statistical quality control. </title> <journal> Int. J. of Production Research, </journal> <volume> 33(2) </volume> <pages> 497-510, </pages> <month> February </month> <year> 1995. </year>

References-found: 98

