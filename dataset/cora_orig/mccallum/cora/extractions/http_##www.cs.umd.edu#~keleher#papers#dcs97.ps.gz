URL: http://www.cs.umd.edu/~keleher/papers/dcs97.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/papers.html
Root-URL: 
Email: keleher)@cs.umd.edu  
Title: Multi-threading and Remote Latency in Software DSMs  
Author: Kritchalach Thitikamol and Pete Keleher 
Address: (kritchal  
Affiliation: University of Maryland  
Abstract: This paper evaluates the use of per-node multi-threading to hide remote memory and synchronization latencies in a software DSM. As with hardware systems, multi-threading in software systems can be used to reduce the costs of remote requests by switching threads when the current thread blocks. We added multi-threading to the C VM software DSM and evaluated its impact on performance for a suite of common shared memory programs. Multi-threading resulted in s peed improvements of at least 17 % in three of the seven applications in our suit e, and lesser improvements in the other appl i-cations. However, we found that i) good performance is not always achievable transparently for nontrivial applications , ii) multi-threading can negatively interact with DSM oper a-tions, iii) multi-threading decreases cache and TLB locality , and iv) any multi-threading speedup is dependent on available work. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Keleher, P. </author> <title> The Relative Importance of Concurrent Writers and Weak Consistency Models in Proceedings of the 16 th International Conference on Distributed Co m-puting Systems. </title> <year> 1996. </year>
Reference-contexts: 1. Introduction This paper presents an empirical evaluation of the use of per-node multi-threading to hide remote latencies in CVM <ref> [1] </ref>, a software distributed shared memory (DSM) system. DSMs are software systems that emulate shared memory semantics in software over hardware that provides support only for me s-sage-passing. Multi-threading for latency-hiding is a well-known technique for hi ding cache miss latencies in the har d-ware environment [2, 3].
Reference: 2. <author> Agarwal, </author> <title> e.a. The MIT Alewife Machine: </title> <booktitle> Architecture and Performance in Proceedings of the 22th Intern a-tional Conference on Computer Architecture. </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: DSMs are software systems that emulate shared memory semantics in software over hardware that provides support only for me s-sage-passing. Multi-threading for latency-hiding is a well-known technique for hi ding cache miss latencies in the har d-ware environment <ref> [2, 3] </ref>. However, the software environment presents special challenges. The paradigm usually assumed in DSM-related literature is that of a distributed system containing a single thread on each processor. This arrangement is simple, and yet allows reasonably high processor efficiency.
Reference: 3. <author> Mowry, T. and A. Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Mu l-tiprocessors in Journal of Parallel and Distributed Co m-puting. </title> <month> June </month> <year> 1991. </year>
Reference-contexts: DSMs are software systems that emulate shared memory semantics in software over hardware that provides support only for me s-sage-passing. Multi-threading for latency-hiding is a well-known technique for hi ding cache miss latencies in the har d-ware environment <ref> [2, 3] </ref>. However, the software environment presents special challenges. The paradigm usually assumed in DSM-related literature is that of a distributed system containing a single thread on each processor. This arrangement is simple, and yet allows reasonably high processor efficiency.
Reference: 4. <author> Wilson, R.P., et al., </author> <title> SUIF: An Infrastructure for research on parallelizing and optimizing compilers , ACM SI G-PLAN Notices, </title> <month> December </month> <year> 1994. </year> <note> 29 (12): </note> <author> p. </author> <month> 31-37. </month>
Reference-contexts: Socalled fine-grain programs have several advantages, including arch i-tecture independence, clarity of expression, implicit load ba lancing, and ease of code generation for parallelizing compi lers <ref> [4] </ref>. Three of the seven applications sped up by at least 17 %, and the others by lesser amount s. However, our performance does not come close to the potential speedup implied by pro c-essor utilizations [5]. <p> Ocean (contiguous ocean from Splash-2) simulates large scale ocean movements based on eddy and boundary currents. SOR implements successive over-relaxation uses nearest neighbor communication. SWM750 performs a two dimensional stencil computation that applies finite-difference methods to solve shallow-water equations. This application was automatically parallelized by the SUIF compiler <ref> [4] </ref>. Finally, Water-Nsq and WaterSp are molecular dynamics simulations from Splash-2 . While the Water- Nsq uses O (N 2 ) algorithms, the WaterSp uses a more efficient algorithm by imposing a uniform 3-D grid of cells on the problem domain.
Reference: 5. <author> Lim, B.-H. and R. </author> <title> Bianchini. </title> <booktitle> Limits on the Performance Benefits of Multithreading and Prefetching in Proceedings of the International Conference on the Measurement and Modeling of Computer Systems. </booktitle> <year> 1996. </year>
Reference-contexts: Three of the seven applications sped up by at least 17 %, and the others by lesser amount s. However, our performance does not come close to the potential speedup implied by pro c-essor utilizations <ref> [5] </ref>. We have identified five contributors to this shortfall, and present strategies for dealing with them, if applicable: 1. Local contention for resources - The threads on a single node often contend or block and wait for the same resource.
Reference: 6. <author> Freeh, V.W., D.K. Lowenthal, and G.R. Andrews. </author> <title> Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations in Proc. </title> <booktitle> of the First Sy m-posium on Operating Systems Design and Implement a-tion. </booktitle> <month> November </month> <year> 1994. </year> <title> Monterey, </title> <address> CA: </address> <publisher> USENIX Assoc. </publisher>
Reference-contexts: There are essentially two strategies for dealing with local contention. First, one could create a large enough number of threads that all of them are never blocked at the same time . This is too expensive in our environment, but is practical in systems where threads are very lightweight <ref> [6] </ref>. The second approach is source modification. We used this approach in some of our applications. 2. Reduction operations - As the singlethread-per- node model is nearly universal, programmers tend to accumulate results locally before communicating with other threads whenever possible. These operations are essentially reductions. <p> Thread switch cost - Efficient thread switching is crucial to getting good coverage of remote latency. One obvious approach to solving the majority of the above problems is to combine a lightweight, fine-grained threading package with adaptive load-balancing [11]. Ligh t-weight thread packages <ref> [6] </ref> are fine-grained enough that it is possible to load-balance through thread migration, and to minimize unhealthy interactions with the underlying DSM by bin scheduling threads [12].
Reference: 7. <author> Weimin Yu Cristiana Amza, A.L.C., Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony and W. Zwaenepoel. TreadMarks: </author> <booktitle> Shared Memory Compu t-ing on Networks of Workstations in IEEE Computer . February 1996. </booktitle>
Reference-contexts: Global locks can be held by only a single thread at a time. 3. Implementation 3.1 CVM The DSM target used in this work is CVM, a software DSM that supports multiple protocols and consistency mo dels. Like commercially available systems such as TreadMarks <ref> [7] </ref>, CVM is written entirely as a user-level library and runs on most UNIX-like systems. Unlike TreadMarks, CVM was cr e-ated specifically as a platform for protocol experimentation. The system is written in C++, and opaque interfaces are strictly enforced between different functional units of the sy stem whenever possible.
Reference: 8. <author> Gharachorloo, K., et al. </author> <booktitle> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors in Proceedings of the 17 th Annual International Symp o-sium on Computer Architecture. </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: We therefore expect the pe r-formance of the fully functional system to improve over the existing base. Memory Consistency - CVM's primary protocol impl e-ments a multiple-writer version of lazy release consistency, which is a derivation of release consistency <ref> [8] </ref>. In release consistency, a processor delay s making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of modific a-tions can thus be postponed until the next synchronization o p-eration takes effect.
Reference: 9. <author> Keleher, P., A.L. Cox, and W. Zwaenepoel. </author> <booktitle> Lazy R e-lease Consistency for Software Distributed Shared Me m-ory in Proceedings of the 19 th Annual International Symposium on Computer Architecture. </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: In practice, most shared-memory programs require little or no modifications to meet these requirements. Lazy release consistency (LRC) <ref> [9] </ref> allows the propag a-tion of modifications to be further postponed until the time of the next subsequent acquire of a released synchronization var i-able. At this time, the acquiring processor determines which modifications it needs to see according to the definition of LRC.
Reference: 10. <author> Woo, </author> <title> S.C., </title> <editor> et al. </editor> <title> The SPLASH-2 Programs: </title> <booktitle> Character i-zation and Methodological Considerations in Proceedings of the 22 nd Annual International Symposium on Computer Architecture. </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Minimal 8-processor barriers cost 2470 m secs. Thread switches cost approximately 8 m secs. 4.2 Application Suite Our application suite consists of seven applications: Bar-nes, FFT, Ocean, WaterSp, and Water- Nsq from the Splash suite <ref> [10] </ref>, SOR, a simple nearest-neighbor application, and SWM750 from the SPEC92 benchmark suite. Table 1 lists specifics for the applications in our study. Sync Type indicates the synchronization operations used by the applications .
Reference: 11. <author> Itzkovitz, A., A. Schuster, and L. Wolfovich, </author> <title> Thread M i-gration and its Applications in Distributed Shared Me m-ory Systems, . 1997, </title> <institution> Technion IIT. </institution>
Reference-contexts: Thread switch cost - Efficient thread switching is crucial to getting good coverage of remote latency. One obvious approach to solving the majority of the above problems is to combine a lightweight, fine-grained threading package with adaptive load-balancing <ref> [11] </ref>. Ligh t-weight thread packages [6] are fine-grained enough that it is possible to load-balance through thread migration, and to minimize unhealthy interactions with the underlying DSM by bin scheduling threads [12].
Reference: 12. <author> Philbin, J., et al. </author> <title> Thread Scheduling for Cache Locality in Proceedings of the 7 th International Conference on A r-chitectural Supports for Programming Languages and Operating Systems. </title> <year> 1996. </year>
Reference-contexts: Ligh t-weight thread packages [6] are fine-grained enough that it is possible to load-balance through thread migration, and to minimize unhealthy interactions with the underlying DSM by bin scheduling threads <ref> [12] </ref>. The challenge is to build a ligh t-weight threading system without changing the programming model, i.e. without constraining the threads to be runto completion. We are continuing our research in this direction.
References-found: 12

