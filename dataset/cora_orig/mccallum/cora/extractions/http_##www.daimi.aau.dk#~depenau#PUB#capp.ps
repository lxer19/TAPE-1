URL: http://www.daimi.aau.dk/~depenau/PUB/capp.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Email: Email:  Email: depenau@daimi.aau.dk  
Phone: Phone: +45 8622 2200,  Phone: +45 89423371,  
Title: Aspects of Generalization and Pruning  
Author: Jan Depenau and Martin Moller and 
Affiliation: DAIMI, Computer Science Department, Aarhus University,  
Web: jdterma.login.dkuug.dk  
Address: Denmark  Ny Munkegade, Bldg. 540, DK-8000 Aarhus C, Denmark  
Note: TERMA Elektronik AS, Hovmarken 4 DK-8520 Lystrup,  
Abstract: It is well known that the generalization ability of a neural network can be improved by reducing the network's complexity. Pruning methods based on the idea of eliminating weights of a well working feed-forward network have proved to be successful. The theoretical background of pruning is established through a mathematical model of generalization. A short description of three of the most popular pruning methods, Optimal Brain Surgeon (OBS) [Hassibi et al. 93], Optimal Brain Damage (OBS) [LeCun et al. 90] and Magnitude Based Pruning (MAG) is given. These three methods have experimentally been tested on standard benchmark problems known as the MONK's problems [Thrun et al. 91]. It is shown that there is no theoretical evidence for choosing one of the methods compared to the other. This was confirmed by the experiments showing that all methods were capable of reducing the number of weights of a well working network, but none of the methods was the best every time. However, OBS was the most robust, stable and fastest method although it could be caught in a local minimum. Both MAG and OBD showed a fluctuating performance according to number of weights they could remove when the initialized conditions or learning parameters were changed. 
Abstract-found: 1
Intro-found: 1
Reference: [Baum et al. 89] <author> Baum, E.B. and D. </author> <title> Haussler (1989). What Size Net Gives Valid Generalization? Neural Computation 1 151-160. </title>
Reference: [Denker et al. 87] <author> Denker, J., D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield (1987). Large Automatic Learning, Rule Extraction, and Generalization Complex Systems 1 877-922. </title>
Reference: [Hassibi et al. 93] <author> Hassibi, B, Stork, D. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal Brain Surgeone. </title> <booktitle> Advances in Neural Information Processing Systems V (Denver 1993). </booktitle> <editor> ed. S.J. Hanson et al., </editor> <address> 164-171. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: in the following way: H 1 p p X (p+1) X (p+1)T H 1 P + X (p+1)T H 1 with H 1 0 = ff 1 I and H 1 where P is the number of patterns and X is a vector containing information of the second derivative, see <ref> [Hassibi et al. 93] </ref> for a detailed description. The algorithm is: delete the weight with smallest saliency and adjust the other weights according to ffiw, repeat until a certain stop criterion is satisfied. 4 Test and experiments The three methods were tested on the MONK's problems (Thrun et al. 1991).
Reference: [Hertz et al. 91] <author> Hertz, J. Krogh, A. and Palmer, R. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley: </publisher> <pages> 115 - 162. </pages>
Reference: [Le Cun et al. 90] <author> Le Cun., J.S. Denker, and S.A. </author> <month> Sollar </month> <year> (1990). </year> <title> Optimal Brain Damage. </title> <booktitle> Advances in Neural Information Pr ocessing Systems II (Denver 1989). </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 598-605. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Thrun et al. 91] <author> Thrun, </author> <title> S.B. and 23 co-authors (1991). The MONK's Problems A performance comparison of different learning algorithms, </title> <institution> CMU-CS-91-197 Carnegie-Mellon U. Department of Computer Science Tech Report 278 </institution>
Reference: [Vapnik 82] <author> Vapnik, V.N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [Vapnik 92] <author> Vapnik, V.N. </author> <year> (1992). </year> <title> Principles of Risk Minimization for Learning Theory Advances in Neural Information Processing Systems IV (Denver 1992). </title> <editor> ed. J.E.Moody et al., </editor> <address> 831-838. San Mateo: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 279 </pages>
References-found: 8

