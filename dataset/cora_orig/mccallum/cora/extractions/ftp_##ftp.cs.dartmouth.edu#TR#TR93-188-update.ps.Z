URL: ftp://ftp.cs.dartmouth.edu/TR/TR93-188-update.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR93-188/
Root-URL: http://www.cs.dartmouth.edu
Author: Thomas H. Cormen David Kotz 
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Abstract: Available at URL ftp://ftp.cs.dartmouth.edu/pub/CS-techreports/TR93-188-update.ps.Z Earlier version appeared in DAGS 1993 Symposium on Parallel I/O and Databases Integrating Theory and Practice in Parallel File Systems Abstract Several algorithms for parallel disk systems have appeared in the literature recently, and they are asymptotically optimal in terms of the number of disk accesses. Scalable systems with parallel disks must be able to run these algorithms. We present a list of capabilities that must be provided by the system to support these optimal algorithms: control over declustering, querying about the configuration, independent I/O, turning off file caching and prefetching, and bypassing parity. We summarize recent theoretical and empirical work that justifies the need for these capabilities.
Abstract-found: 1
Intro-found: 1
Reference: [Bar94] <author> Eric Barton. </author> <title> Private communication, </title> <month> August </month> <year> 1994. </year>
Reference-contexts: A program may query to find out a file's declustering information [CF94]. All I/O is independent, and there is no support for parity (they depend on checkpoints for reliability). The Parallel File System (PFS) for the Meiko CS-2 [Mei93, Mei94] apparently supports all of our required capabilities <ref> [Bar94] </ref>. In Meiko's PFS, separate file systems run under UFS (the Unix file system) on multiple server nodes. One server node stores directory information, and all 2 These systems use RAID 3, which serializes what look to the programmer like independent writes. 4 others store data.
Reference: [BCR93] <author> Rajesh Bordawekar, Alok Choudhary, and Juan Miguel Del Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <year> 1993. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR93, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50.
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: This paper has not proposed any specific file-system interface for out-of-core algorithms using parallel I/O. There are several proposed interfaces that support the specific operations of reading and writing matrices <ref> [BdC93, GGL93, Mas92] </ref>. The authors are aware of two projects that define more general interfaces providing the required capabilities listed in this paper.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Reads and writes in the Thinking Machines Corporation's DataVault [TMC91] are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array <ref> [TMC92, LIN + 93, BGST93] </ref> nor the file system for the MasPar MP-1 and MP-2 [Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system [CFPB93] for the SP-1 and SP-2 multiprocessors supports many of the capabilities we require.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Vesta File System Programmer's Reference. </title> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> February 15 </month> <year> 1994. </year> <note> Version 0.93. </note>
Reference-contexts: Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size. A program may query to find out a file's declustering information <ref> [CF94] </ref>. All I/O is independent, and there is no support for parity (they depend on checkpoints for reliability). The Parallel File System (PFS) for the Meiko CS-2 [Mei93, Mei94] apparently supports all of our required capabilities [Bar94].
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: Striping unit sizes are often either one bit (as in RAID level 3 [PGK88]) or equal to the block size (as in RAID levels 4 and 5). Some systems, such as Vesta <ref> [CFPB93] </ref>, allow the user to define the striping unit size. The optimal algorithms assume striping with a block-sized striping unit. <p> Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array [TMC92, LIN + 93, BGST93] nor the file system for the MasPar MP-1 and MP-2 [Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system <ref> [CFPB93] </ref> for the SP-1 and SP-2 multiprocessors supports many of the capabilities we require. Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size. A program may query to find out a file's declustering information [CF94].
Reference: [CGG + 94] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Dar-ren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms (extended abstract). </title> <note> Submitted to SODA '95, </note> <month> July </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Computational geometry: Goodrich et al. [GTVV93] present algorithms for several computational-geometry problems, including convex hull in 2 and 3 dimensions, planar point location, all nearest neighbors in the plane, rectangle intersection and union, and line segment visibility from a point. Graph algorithms: Chiang et al. <ref> [CGG + 94] </ref> present algorithms for several graph problems.
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> De--partment of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: General permutations: Vitter and Shriver [VS94] use their sorting algorithm to perform general permutations by sorting on target addresses. Mesh and torus permutations: Cormen <ref> [Cor92] </ref> presents algorithms for mesh and torus permutations, in which each element moves a fixed amount in each dimension of a multidimensional grid. In fact, these permutations are just special cases of monotonic and k-monotonic routes. In a monotonic route, all elements stay in the same relative order. <p> In a monotonic route, all elements stay in the same relative order. A k-monotonic route is the superposition of k monotonic routes. All of the above permutations can be performed with fewer parallel I/Os than general permutations. Bit-defined permutations: Cormen, Sundquist, and Wisniewski <ref> [Cor92, Cor93, CSW94] </ref> present algorithms to perform BMMC permutations often with fewer parallel I/O operations than general permutations. <p> Among the useful BPC permutations are matrix transpose 3 with 3 Vitter and Shriver earlier gave an algorithm for matrix transpose. 5 dimensions that are powers of 2, bit-reversal permutations, vector-reversal permutations, hypercube permutations, and matrix reblocking. General matrix transpose: Cormen <ref> [Cor92] </ref> gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver [VS94] give an asymptotically optimal algorithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS94] cover matrix multiplication as well.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: In a monotonic route, all elements stay in the same relative order. A k-monotonic route is the superposition of k monotonic routes. All of the above permutations can be performed with fewer parallel I/Os than general permutations. Bit-defined permutations: Cormen, Sundquist, and Wisniewski <ref> [Cor92, Cor93, CSW94] </ref> present algorithms to perform BMMC permutations often with fewer parallel I/O operations than general permutations. <p> The left-symmetric RAID 5 interpretation uses the number of physical disks. There are two sources of trouble here. One is that some of the optimal algorithms (e.g., those in <ref> [Cor93, CSW94] </ref>) compute disk numbers and relative locations on disks mathematically, based in part on the number of logical|not physical|disks. The algorithms use these disk numbers to generate an independent I/O that accesses exactly one block on each logical disk.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Submitted to IEEE Transactions on Parallel and Distributed Systems. Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. </note>
Reference-contexts: In a monotonic route, all elements stay in the same relative order. A k-monotonic route is the superposition of k monotonic routes. All of the above permutations can be performed with fewer parallel I/Os than general permutations. Bit-defined permutations: Cormen, Sundquist, and Wisniewski <ref> [Cor92, Cor93, CSW94] </ref> present algorithms to perform BMMC permutations often with fewer parallel I/O operations than general permutations. <p> The left-symmetric RAID 5 interpretation uses the number of physical disks. There are two sources of trouble here. One is that some of the optimal algorithms (e.g., those in <ref> [Cor93, CSW94] </ref>) compute disk numbers and relative locations on disks mathematically, based in part on the number of logical|not physical|disks. The algorithms use these disk numbers to generate an independent I/O that accesses exactly one block on each logical disk.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: In fact, the operating system treats each disk as a separate file system and does not decluster individual files across disks. Thus, the nCUBE provides the low-level access one needs, but no higher-level access. The current nCUBE file system <ref> [Dd93, dBC93] </ref> supports declustering and does allow applications to manipulate the striping unit size and distribution pattern. The file system for the Kendall Square Research KSR-1 [KSR92] shared-memory multiprocessor declusters file data across disk arrays attached to different processors. <p> The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR93, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests [dBC93, BCR93, Nit92, GP91, GL91]. del Rosario et al. <ref> [dBC93] </ref> find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. <p> del Rosario et al. <ref> [dBC93] </ref> find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. Nitzberg [Nit92] shows that some layouts experience poor performance on CFS because of thrashing in the file system cache. His solution to this problem carefully schedules the processors' accesses to the disks by reducing concurrency.
Reference: [Dd93] <author> Erik P. DeBenedictis and Juan Miguel del Rosario. </author> <title> Modular scalable I/O. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):122-128, January and February 1993. </note>
Reference-contexts: In fact, the operating system treats each disk as a separate file system and does not decluster individual files across disks. Thus, the nCUBE provides the low-level access one needs, but no higher-level access. The current nCUBE file system <ref> [Dd93, dBC93] </ref> supports declustering and does allow applications to manipulate the striping unit size and distribution pattern. The file system for the Kendall Square Research KSR-1 [KSR92] shared-memory multiprocessor declusters file data across disk arrays attached to different processors.
Reference: [For94] <author> Rich Fortier, </author> <month> September </month> <year> 1994. </year> <title> Private communication. </title>
Reference-contexts: The file system for the Kendall Square Research KSR-1 [KSR92] shared-memory multiprocessor declusters file data across disk arrays attached to different processors. Like the Intel Paragon, each disk array is a RAID 3 <ref> [For94] </ref>. The KSR-1, however, has an unalterable striping unit size. The memory-mapped interface uses virtual memory techniques to page data to and from the file, which does not provide sufficient control to an application trying to optimize disk I/O.
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: This paper has not proposed any specific file-system interface for out-of-core algorithms using parallel I/O. There are several proposed interfaces that support the specific operations of reading and writing matrices <ref> [BdC93, GGL93, Mas92] </ref>. The authors are aware of two projects that define more general interfaces providing the required capabilities listed in this paper.
Reference: [Gib93] <author> Garth A. Gibson. </author> <title> Private communication, </title> <month> June </month> <year> 1993. </year>
Reference-contexts: In this organization, the mapping from where one expects a block to be to where it actually goes appears to be relatively straightforward. We expect the ith logical block to reside on disk i, modulo the number of disks, but possibly at a different location on the disk <ref> [Gib93] </ref>. The phrase "modulo the number of disks" is open to interpretation, however, since there is one more physical disk than logical disks. The left-symmetric RAID 5 interpretation uses the number of physical disks. There are two sources of trouble here. <p> Perhaps the best solution, therefore, is to store temporary files in a separate RAID 0 partition, i.e., a partition of the storage system that has been configured without parity <ref> [Gib93] </ref>. It would need D + 1 physical disks if we wish to use D logical disks with occasional paritypointing. Another solution is to use RAID 4 with the capability to turn off parity on a per-file basis.
Reference: [GL91] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR93, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must have file-system primitives to discover and control the I/O system configuration. The ELFS file system is based on this principle <ref> [GP91, GL91] </ref>. ELFS is an extensible file system, building object-oriented, application-specific classes on top of a simple set of file-access primitives.
Reference: [GP91] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR93, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must have file-system primitives to discover and control the I/O system configuration. The ELFS file system is based on this principle <ref> [GP91, GL91] </ref>. ELFS is an extensible file system, building object-oriented, application-specific classes on top of a simple set of file-access primitives.
Reference: [GTVV93] <author> Michael T. Goodrich, Jyh-Jong Tsay, Darren E. Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Fast Fourier Transform: Vitter and Shriver [VS94] give an asymptotically optimal algorithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS94] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. Computational geometry: Goodrich et al. <ref> [GTVV93] </ref> present algorithms for several computational-geometry problems, including convex hull in 2 and 3 dimensions, planar point location, all nearest neighbors in the plane, rectangle intersection and union, and line segment visibility from a point. Graph algorithms: Chiang et al. [CGG + 94] present algorithms for several graph problems.
Reference: [HG92] <author> Mark Holland and Garth A. Gibson. </author> <title> Parity declustering for continuous operation in redundant disk arrays. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 23-35, </pages> <month> October </month> <year> 1992. </year> <title> [Int88] iPSC/2 I/O facilities. </title> <publisher> Intel Corporation, </publisher> <year> 1988. </year> <title> Order number 280120-001. </title>
Reference-contexts: In a left-symmetric RAID 5 organization <ref> [HG92] </ref>, for example, there are D logical disks to hold data but D + 1 physical disks, with parity blocks spread among them. In this organization, the mapping from where one expects a block to be to where it actually goes appears to be relatively straightforward.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. 11 </note>
Reference-contexts: with a technique known as disk-directed I/O, in which the high-level I/O request is passed through to the I/O nodes, which then arrange for blocks of data to be transferred between disks and memory in a way that is efficient in terms of the number and order of disk accesses <ref> [Kot94] </ref>. Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must have file-system primitives to discover and control the I/O system configuration. The ELFS file system is based on this principle [GP91, GL91].
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Meiko's PFS was designed to support Parallel Oracle, which has its own cache management mechanism; therefore, PFS allows client nodes to turn off file caching. Server nodes, however, must use the file caching of UFS. The Hurricane File System (HFS) <ref> [Kri94] </ref> for the Hector multiprocessor at the University of Toronto supplies all of our requirements. HFS uses an object-oriented building-block approach to provide flexible, scalable high performance.
Reference: [KSR92] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Thus, the nCUBE provides the low-level access one needs, but no higher-level access. The current nCUBE file system [Dd93, dBC93] supports declustering and does allow applications to manipulate the striping unit size and distribution pattern. The file system for the Kendall Square Research KSR-1 <ref> [KSR92] </ref> shared-memory multiprocessor declusters file data across disk arrays attached to different processors. Like the Intel Paragon, each disk array is a RAID 3 [For94]. The KSR-1, however, has an unalterable striping unit size.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Reads and writes in the Thinking Machines Corporation's DataVault [TMC91] are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array <ref> [TMC92, LIN + 93, BGST93] </ref> nor the file system for the MasPar MP-1 and MP-2 [Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system [CFPB93] for the SP-1 and SP-2 multiprocessors supports many of the capabilities we require.
Reference: [Mas92] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array [TMC92, LIN + 93, BGST93] nor the file system for the MasPar MP-1 and MP-2 <ref> [Mas92] </ref> support independent I/O as we have defined it. 2 IBM's Vesta file system [CFPB93] for the SP-1 and SP-2 multiprocessors supports many of the capabilities we require. Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size. <p> This paper has not proposed any specific file-system interface for out-of-core algorithms using parallel I/O. There are several proposed interfaces that support the specific operations of reading and writing matrices <ref> [BdC93, GGL93, Mas92] </ref>. The authors are aware of two projects that define more general interfaces providing the required capabilities listed in this paper.
Reference: [Mei93] <institution> Meiko. </institution> <address> Elan Widget Library, </address> <year> 1993. </year>
Reference-contexts: A program may query to find out a file's declustering information [CF94]. All I/O is independent, and there is no support for parity (they depend on checkpoints for reliability). The Parallel File System (PFS) for the Meiko CS-2 <ref> [Mei93, Mei94] </ref> apparently supports all of our required capabilities [Bar94]. In Meiko's PFS, separate file systems run under UFS (the Unix file system) on multiple server nodes.
Reference: [Mei94] <author> Meiko. </author> <title> CS-2 System Administration Guide, </title> <year> 1994. </year>
Reference-contexts: A program may query to find out a file's declustering information [CF94]. All I/O is independent, and there is no support for parity (they depend on checkpoints for reliability). The Parallel File System (PFS) for the Meiko CS-2 <ref> [Mei93, Mei94] </ref> apparently supports all of our required capabilities [Bar94]. In Meiko's PFS, separate file systems run under UFS (the Unix file system) on multiple server nodes.
Reference: [MMRW94] <author> Arthur B. Maccabe, Kevin S. McCurley, Rolf Riesen, and Stephen R. Wheat. </author> <title> SUNMOS for the Intel Paragon: A brief user's guide. </title> <booktitle> In Proceedings of the Intel Supercomputer Users Group Conference, </booktitle> <pages> pages 245-251, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The authors are aware of two projects that define more general interfaces providing the required capabilities listed in this paper. Researchers at Sandia National Laboratories [Shr94] are developing a library of C-callable, low-level, synchronous, parallel-I/O functions to run on the Intel Paragon under the SUNMOS operating system <ref> [MMRW94] </ref> on compute nodes and under OSF on I/O nodes. These functions perform synchronous reads and writes on blocks, full stripes, or entire memory loads, with the declustering method specified in the call.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR93, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. Nitzberg <ref> [Nit92] </ref> shows that some layouts experience poor performance on CFS because of thrashing in the file system cache. His solution to this problem carefully schedules the processors' accesses to the disks by reducing concurrency.
Reference: [NV91] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Large-scale sorting in parallel memories. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver [VS94] give a randomized sorting algorithm, and Nodine and Vitter <ref> [NV91, NV92] </ref> present two deterministic sorting algorithms. General permutations: Vitter and Shriver [VS94] use their sorting algorithm to perform general permutations by sorting on target addresses.
Reference: [NV92] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Optimal deterministic sorting on parallel disks. </title> <type> Technical Report CS-92-08, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver [VS94] give a randomized sorting algorithm, and Nodine and Vitter <ref> [NV91, NV92] </ref> present two deterministic sorting algorithms. General permutations: Vitter and Shriver [VS94] use their sorting algorithm to perform general permutations by sorting on target addresses.
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer. <p> The apparent number of independent disks, therefore, is only the number of I/O nodes, rather than the larger number of physical disks. The first file system for the nCUBE multiprocessor <ref> [PFDJ89] </ref> gives plenty of control to the user. In fact, the operating system treats each disk as a separate file system and does not decluster individual files across disks. Thus, the nCUBE provides the low-level access one needs, but no higher-level access.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A common distribution pattern is striping, in which striping units are distributed in round-robin order among the disks; a stripe consists of the data distributed in one round. Striping unit sizes are often either one bit (as in RAID level 3 <ref> [PGK88] </ref>) or equal to the block size (as in RAID levels 4 and 5). Some systems, such as Vesta [CFPB93], allow the user to define the striping unit size. The optimal algorithms assume striping with a block-sized striping unit. <p> This section shows why we want to do so. Because we maintain parity to improve data reliability, this section also describes typical situations in which we can bypass consistent parity maintenance without compromising data reliability. The cost of maintaining parity Patterson, Gibson, and Katz <ref> [PGK88] </ref> outline various RAID (Redundant Arrays of Inexpensive Disks) organizations. RAID levels 4 and 5 support independent I/Os. Both use check disks to store parity information. In RAID 4, the parity information is stored on a single dedicated check disk.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer.
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, caching and prefetching are completely out of the control of the user, and the pattern for declustering the file across disks is not predictable and mostly out of the user's control. The Parallel File System (PFS) for the Intel Paragon <ref> [Roy93] </ref> supports our list of capabilities [Rul93], with a few restrictions. While it appears that caching can be disabled on the compute node, I/O-node caching is always active. Declustering parameters are determined on a per-filesystem basis. Finally, the Paragon does not maintain parity across I/O nodes.
Reference: [Rul93] <author> Brad Rullman. </author> <title> Private communication, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Unfortunately, caching and prefetching are completely out of the control of the user, and the pattern for declustering the file across disks is not predictable and mostly out of the user's control. The Parallel File System (PFS) for the Intel Paragon [Roy93] supports our list of capabilities <ref> [Rul93] </ref>, with a few restrictions. While it appears that caching can be disabled on the compute node, I/O-node caching is always active. Declustering parameters are determined on a per-filesystem basis. Finally, the Paragon does not maintain parity across I/O nodes.
Reference: [SGH93] <author> Daniel Stodolsky, Garth Gibson, and Mark Holland. </author> <title> Parity logging: Overcoming the small write problem in redundant disk arrays. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 64-75, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Even if an algorithm were to try to compensate for this mapping, it would need to know the block's exact location. This information might not be available to the algorithm when file system block-allocation policies hide physical locations from the application. One might be tempted to use parity logging <ref> [SGH93] </ref> to alleviate RAID 5 parity-mapping problems. Parity logging, however, further complicates the mapping between logical and physical locations. Moreover, it requires us to dedicate some physical memory to hold several track images of parity log information prior to writing it out to disks.
Reference: [Shr94] <author> Elizabeth A. M. Shriver. </author> <title> Private communication, </title> <month> July </month> <year> 1994. </year> <note> Paper in progress. 12 </note>
Reference-contexts: There are several proposed interfaces that support the specific operations of reading and writing matrices [BdC93, GGL93, Mas92]. The authors are aware of two projects that define more general interfaces providing the required capabilities listed in this paper. Researchers at Sandia National Laboratories <ref> [Shr94] </ref> are developing a library of C-callable, low-level, synchronous, parallel-I/O functions to run on the Intel Paragon under the SUNMOS operating system [MMRW94] on compute nodes and under OSF on I/O nodes.
Reference: [TMC91] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> Connection Machine I/O System Programming Guide, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: To bypass parity within a RAID 3, one can deliberately "fail" the RAID parity disk, but then parity is lost for all files, not just large temporary files. Reads and writes in the Thinking Machines Corporation's DataVault <ref> [TMC91] </ref> are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms.
Reference: [TMC92] <institution> CM-5 scalable disk array. Thinking Machines Corporation glossy, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Reads and writes in the Thinking Machines Corporation's DataVault [TMC91] are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array <ref> [TMC92, LIN + 93, BGST93] </ref> nor the file system for the MasPar MP-1 and MP-2 [Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system [CFPB93] for the SP-1 and SP-2 multiprocessors supports many of the capabilities we require.
Reference: [Ven94] <author> Darren Erik Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proceedings of the DAGS '94 Symposium, </booktitle> <pages> pages 117-134, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These functions perform synchronous reads and writes on blocks, full stripes, or entire memory loads, with the declustering method specified in the call. The other project is Vengroff's TPIE (Transparent Parallel I/O Environment) <ref> [Ven94] </ref>, which is a C++ interface for synchronous parallel I/O. TPIE includes low-level functions to access disk blocks and manage memory, and it also includes higher-level functions that operate on streams of records by filtering, distributing, merging, permuting, and sorting.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The parallel disk model used by these algorithms was originally proposed in 1990 by Vitter and Shriver <ref> [VS90] </ref>. (See [VS94] for a more recent version.) The cost measure is the number of parallel I/O operations performed over the course of a computation.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: Theoretical grounds Several algorithms for parallel disk systems have been developed recently. These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver <ref> [VS94] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present two deterministic sorting algorithms. General permutations: Vitter and Shriver [VS94] use their sorting algorithm to perform general permutations by sorting on target addresses. <p> They solve the following problems: Sorting: Vitter and Shriver <ref> [VS94] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present two deterministic sorting algorithms. General permutations: Vitter and Shriver [VS94] use their sorting algorithm to perform general permutations by sorting on target addresses. Mesh and torus permutations: Cormen [Cor92] presents algorithms for mesh and torus permutations, in which each element moves a fixed amount in each dimension of a multidimensional grid. <p> General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS94] </ref> give an asymptotically optimal algorithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS94] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS94] </ref> give an asymptotically optimal algorithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS94] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> The parallel disk model used by these algorithms was originally proposed in 1990 by Vitter and Shriver [VS90]. (See <ref> [VS94] </ref> for a more recent version.) The cost measure is the number of parallel I/O operations performed over the course of a computation. The model does not specify the memory's organization, connection to the disks, or relation to the processors, and so it is independent of any particular machine architecture. <p> It turns out that the constraint of fully striped I/O increases the number of disk accesses by more than a constant factor compared to independent I/O unless there are very few disks <ref> [VS94] </ref>. Disk accesses are expensive enough; to increase their number by more than a constant factor for large amounts of data can be prohibitively expensive. 6 The algorithms treat all physical memory uniformly; there is no distinct file cache.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In DAGS '93, </booktitle> <month> June </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: Fast Fourier Transform: Vitter and Shriver [VS94] give an asymptotically optimal algorithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS94] cover matrix multiplication as well. LU decomposition: Womble et al. <ref> [WGWR93] </ref> sketch an LU-decomposition algorithm. Computational geometry: Goodrich et al. [GTVV93] present algorithms for several computational-geometry problems, including convex hull in 2 and 3 dimensions, planar point location, all nearest neighbors in the plane, rectangle intersection and union, and line segment visibility from a point.
References-found: 44

