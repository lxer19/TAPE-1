URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P425.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Email: fhenderson,nickless, stevensg@mcs.anl.gov  
Title: A Scalable High-Performance I/O System  
Author: Mark Henderson, Bill Nickless, Rick Stevens 
Address: Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: A significant weakness of many existing parallel supercomputers is their lack of high-performance parallel I/O. This weakness has prevented, in many cases, the full exploitation of the true potential of MPP systems. As part of a joint project with IBM, we have designed a parallel I/O system for an IBM SP system that can provide sustained I/O rates of greater than 160 MB/s from collections of compute nodes to archival disk and peak transfer rates that should exceed 400 MB/s from compute nodes to I/O servers. This testbed system will be used for a number of projects. First, it will provide a high-performance experimental I/O system for traditional computational science applications; second, it will be used as an I/O software and development environment for new parallel I/O algorithms and operating systems support; and third, it will be used as the foundation for a number of new projects designed to develop enabling technology for the National Information Infrastructure. This report describes the system under development at Argonne National Laboratory, provides some preliminary performance results, and outlines future experiments and directions. 
Abstract-found: 1
Intro-found: 1
Reference: [Bers 93] <author> B. Bershad et al. </author> <title> The Scalable I/O Initiative, </title> <note> white paper available from Argonne National Laboratory, </note> <year> 1993. </year>
Reference-contexts: ANL is collaborating with the National Storage Laboratory (NSL) on the development of and testing of the successor to Unitree |the High-Performance Storage System (HPSS)| and with the Scalable I/O Initiative <ref> [Bers 93] </ref> to develop new parallel I/O concepts and implementations. 2 Applications Requirements As the processing power of parallel supercomputers has increased over time, there has not been a corresponding increase in the I/O capability of those systems [Fren 91], [Redd 90]. <p> Our collaborators in the Scalable I/O Initiative <ref> [Bers 93] </ref>, [Rosa 93] will be using the Argonne system as one testbed for their research. We hope the flexibility of the system will allow these and other researchers the opportunity to test techniques for efficient, parallel I/O and data storage in a production-scale facility.
Reference: [Dura 93] <author> Dannie Durand and Ravi Jain. </author> <title> Distributed Scheduling Algorithms to Improve the Performance of Parallel Data Transfers, </title> <type> Bell-core Technical Report, </type> <year> 1993. </year>
Reference-contexts: As the compute/IO nodes are complete computers in their own right, they are capable of running arbitrarily complex heuristics for scheduling I/O transactions efficiently. <ref> [Dura 93] </ref> There is also support for TCP/IP over Fibre Channel and the high-performance switch, so data access can be transparent, yet relatively high-performance, for applications not taking full advantage of the capabilities of the system. 16 17 18 19 20 21 22 23 24 25 10 1 10 3 16 <p> They provide another opportunity for arbitrarily complex scheduling heuristics to be tested <ref> [Dura 93] </ref>, [Rosa 93]. 4 Scalability Issues system of the Argonne I/O system showing the relationship of the various components. From left to right there are fewer instances of each component, the data rates increase, and the optimal size of each I/O operation increases as well.
Reference: [Fost 91] <author> Ian Foster, Mark Henderson, Rick Stevens. </author> <title> Proceedings of the Workshop on Data Systems for Parallel Climate Models. </title> <institution> Ar-gonne National Laboratory, Mathematics and Computer Science Division Technical Memorandum ANL/MCS-TM-169, </institution> <year> 1991. </year>
Reference: [Fren 91] <author> J. C. French, T. W. Pratt, and M. Das. </author> <title> Performance of a Parallel Input/Output System for the Intel iPSC/860 Hypercube, </title> <booktitle> in SIG-METRICS Conf. Measurement and Modeling of Comput. Syst., </booktitle> <pages> pp. 178-187, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Unitree |the High-Performance Storage System (HPSS)| and with the Scalable I/O Initiative [Bers 93] to develop new parallel I/O concepts and implementations. 2 Applications Requirements As the processing power of parallel supercomputers has increased over time, there has not been a corresponding increase in the I/O capability of those systems <ref> [Fren 91] </ref>, [Redd 90]. Many existing parallel applications such as QCD and Monte Carlo problems do not have a large I/O requirements. However, many problems in computational chemistry, computational fluid dynamics, seismology, and emerging applications under development for the NII and other areas require high-performance I/O [Work 93].
Reference: [Katz 89] <author> R. H. Katz, G. A. Gibson, and D. A. Patter-son. </author> <title> Disk System Architectures for High Performance Computing, </title> <booktitle> Proc. IEEE 77(12), </booktitle> <pages> pp. 1842-1858, </pages> <month> Dec. </month> <year> 1989. </year> <month> 7 </month>
Reference-contexts: Server processors are IBM RS/6000 970Bs with 256 MB RAM and 6 GB of local disk. A secondary networking layer connects the eight I/O servers to four high-performance IBM 9570 RAID arrays (220 GB total) <ref> [Katz 89] </ref> and an automated tape library (DST-800 with three AMPEX DD-2 19-mm helical scan tape drives) with 6.4 TB of tape. The secondary networking layer is built on a NSC HIPPI crossbar enabling any I/O server to access any RAID drive or tape unit.
Reference: [Lesl 93] <author> Ian M. Leslie, Derek R. McAuley, and David L. Tennenhouse. </author> <title> ATM everywhere?, </title> <journal> IEEE Network, </journal> <pages> pp. 40-46, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: servers? * How much and what kind of I/O scheduling information or hints must be communicated from the I/O nodes to the I/O servers to ensure high performance? Other experiments are under way to evaluate the use of FCS as the secondary network layer and to compare it with ATM <ref> [Lesl 93] </ref>.
Reference: [Mich 93] <author> John G. </author> <title> Michalakes. </title> <type> private communication, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference: [Mill 93] <author> Ethan L. Miller. </author> <title> Input/Output Behavior of Supercomputing Applications, </title> <type> UCB report 91/616, </type> <month> January </month> <year> 1991, </year> <institution> University of Califor-nia, Berkeley, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: This I/O is used for checkpointing large runs, visualization archival storage of run histories, and support of digital libraries. On current parallel supercomputing systems, applications require from hundreds of megabytes to hundreds of gigabytes per run and need I/O bandwidths in excess of 100 MB/s <ref> [Mill 93] </ref>. 2.1 Global Climate Models Global climate modeling is a good example to illustrate some of the issues. Estimates of performance and data requirements for parallel climate models indicate the scale of the I/O problem [Work 93].
Reference: [Redd 90] <author> A. L. Narasimha Reddy and Prithviraj Banerjee, </author> <title> A Study of I/O Behavior of Perfect Benchmarks on a Multiprocessor, </title> <booktitle> Proc. IEEE 17th Annual Intl. Symp. on Comp. Arch., </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: High-Performance Storage System (HPSS)| and with the Scalable I/O Initiative [Bers 93] to develop new parallel I/O concepts and implementations. 2 Applications Requirements As the processing power of parallel supercomputers has increased over time, there has not been a corresponding increase in the I/O capability of those systems [Fren 91], <ref> [Redd 90] </ref>. Many existing parallel applications such as QCD and Monte Carlo problems do not have a large I/O requirements. However, many problems in computational chemistry, computational fluid dynamics, seismology, and emerging applications under development for the NII and other areas require high-performance I/O [Work 93].
Reference: [Rosa 93] <author> Juan Miguel del Rosario and Alok Choud-hary, </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects, </title> <institution> preprint Syracuse University, Northeast Parallel Architectures Center, Syracuse University, </institution> <address> NY 13244-4100, </address> <year> 1993. </year>
Reference-contexts: They provide another opportunity for arbitrarily complex scheduling heuristics to be tested [Dura 93], <ref> [Rosa 93] </ref>. 4 Scalability Issues system of the Argonne I/O system showing the relationship of the various components. From left to right there are fewer instances of each component, the data rates increase, and the optimal size of each I/O operation increases as well. <p> Our collaborators in the Scalable I/O Initiative [Bers 93], <ref> [Rosa 93] </ref> will be using the Argonne system as one testbed for their research. We hope the flexibility of the system will allow these and other researchers the opportunity to test techniques for efficient, parallel I/O and data storage in a production-scale facility.
Reference: [Sale 86] <author> K. Salem and H. Garcia-Molina, </author> <title> Disk Striping, </title> <booktitle> in Proc. Intl. Conf. Data Engineering, </booktitle> <pages> pp. 336-342, </pages> <year> 1986. </year>
Reference-contexts: The Ampex DST-800 system has three drives installed and accessible from the 970. Current versions of Uni-tree do not stripe data across the multiple drives and an important I/O issue under investigation is to explore the feasibility of tape striping <ref> [Sale 86] </ref>. We are concerned with the raw performance of writing to all three drives simultaneously using both direct access and under Unitree control.
Reference: [Vest 93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta Parallel File System, </title> <type> Research Report, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> P.O. Box 218, Yorktown Heights, NY 10598. </address>
Reference-contexts: It appears that one of the most important feature of any intermediate I/O network is the degree of host CPU load required to sustain peak transfer rates. 6 Future Plans We will be working with NSL-Unitree, HPSS, and the Vesta <ref> [Vest 93] </ref> (IBM PFS) parallel filesystem to perform early testing and to provide feedback into the development process. Our collaborators in the Scalable I/O Initiative [Bers 93], [Rosa 93] will be using the Argonne system as one testbed for their research.
Reference: [Work 93] <editor> R. Stevens (Editor). </editor> <booktitle> Workshop on Grand Challenges Applications and Software Technology, </booktitle> <address> Pittsburgh, </address> <month> May </month> <year> 1993, </year> <note> available on http://www.mcs.anl.gov/workshop.html. 8 </note>
Reference-contexts: Many existing parallel applications such as QCD and Monte Carlo problems do not have a large I/O requirements. However, many problems in computational chemistry, computational fluid dynamics, seismology, and emerging applications under development for the NII and other areas require high-performance I/O <ref> [Work 93] </ref>. This I/O is used for checkpointing large runs, visualization archival storage of run histories, and support of digital libraries. <p> Estimates of performance and data requirements for parallel climate models indicate the scale of the I/O problem <ref> [Work 93] </ref>.
References-found: 13

