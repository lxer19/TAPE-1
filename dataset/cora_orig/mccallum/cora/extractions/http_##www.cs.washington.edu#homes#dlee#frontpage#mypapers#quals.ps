URL: http://www.cs.washington.edu/homes/dlee/frontpage/mypapers/quals.ps
Refering-URL: http://www.cs.washington.edu/homes/dlee/frontpage/mypapers/recent_papers.htm
Root-URL: http://www.cs.washington.edu
Title: Instruction Cache Effects of Different Code Reordering Algorithms  
Author: Dennis Lee 
Date: October 13, 1994  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Abstract: While scientific programs written in procedural languages like C and Fortran tend to have good instruction cache behavior, improving instruction cache performance continues to be an important issue for commonly used applications, such as compilers and document previewers, and for applications written using object oriented languages. This paper explores several code reordering algorithms that aim to improve instruction cache hit rates. We show the effects of different levels of aggressiveness in applying the standard depth-first algorithm and the effects of different enhancements to this standard. We also show that code reordering becomes more important as cache line size increases. 
Abstract-found: 1
Intro-found: 1
Reference: [Aho et al. 88] <author> Aho, A. V., Sethi, R., and Ullman, J. D. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Finally, Section 6 contains a summary and the conclusion of this paper. 2 Motivation and Design Choices Compilers typically flatten out a program's control flow graph (CFG) in a breadth-first manner. This lays out both branches of a conditional statement into consecutive linear addresses <ref> [Aho et al. 88] </ref>. This algorithm unnecessarily increases the amount of cache space used by statements which process exceptional conditions.
Reference: [Ball & Larus 93] <author> Ball, T. and Larus, J. R. </author> <title> Branch Prediction for Free. </title> <booktitle> In Proc. ACM SIGPLAN '93 Conference On Programming Language Design and Implementation, </booktitle> <pages> pages 300-313, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Usually this measurement is based on profile information (as will be the case for this study) although compile time heuristics may also be used <ref> [Ball & Larus 93] </ref>. For this study, we explored four code layout algorithms: orig, depth-first, skew, and pack. Orig is just the default compiler algorithm. Depth-first explores the CFG in depth-first order. Skew considers the relative weights of the edges in order to decide between depth-first or breadth-first traversal.
Reference: [Calder & Grunwald 94] <author> Calder, B. and Grunwald, D. </author> <title> Reducing Branch Costs via Branch Alignment. </title> <booktitle> In Proc. 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The Illinois IMPACT compiler uses trace scheduling to schedule code for superscalar machines [Hwu & Chang 89]. The effect of the algorithm used in the IMPACT compiler on a number of scientific benchmarks was studied in [Chen et al. 93]. More recently, <ref> [Calder & Grunwald 94] </ref> looked into the effect that code reordering has on the branch penalties. They showed that the main impact of code reordering was in reducing the instruction cache miss rate, and that there were secondary but significant effects coming from improving the branch behavior for their benchmarks.
Reference: [Calder et al. 94] <author> Calder, B., Grunwald, D., and Zorn, B. </author> <title> QuantifyingBehavioral Differences Between C and C++ Programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, unlike the SPEC benchmark, many commonly used applications (e.g. compilers, document previewers, database applications, etc.) suffer a significant number of instruction cache misses even for caches as large as 64 KB <ref> [Chen 94, Calder et al. 94] </ref>. These misses have a significant impact on application performance as it increases the overall CPI for the machine [Chen 94, Chen & Bershad 93]. Moreover, more and more applications are being written in C++ and other object oriented languages. <p> Applications written using these languages also tend to suffer more instruction cache misses than similar programs written in procedural languages, primarily due to the tendency of programmers to use many smaller functions rather than a few big functions when coding in an object oriented style <ref> [Calder et al. 94] </ref>. Instruction cache misses are also more difficult to hide than data cache misses. <p> However, for a two-way associative cache, the miss rate actually goes down after rearranging procedures, as shown in misses. This confirms the observation in <ref> [Calder et al. 94] </ref> that the large number of smaller procedures in programs written in an object oriented style tend to conflict more in the cache. The biggest win comes from rearranging the basic blocks within the procedures.
Reference: [Chambers 93] <author> Chambers, C. </author> <title> The Cecil Language: Specification and Rationale. </title> <type> Technical Report 93-03-05, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The results will be for caches with a 32-byte line size unless otherwise noted. 3 Cecil is an efficient object-oriented language being developed at the University of Washington <ref> [Chambers 93] </ref> 4 Application Description Code Size (KB) Miss rate Static Dynamic 8 KB 32 KB cc1 gcc compiling a 69 KB program into Sun-3 assem bly code. (C) 670 268 4.51% 1.33% cecil a 6000-line unoptimized Cecil program that calls several toy benchmarks including the 20 queens problem, Towers of
Reference: [Chen & Bershad 93] <author> Chen, J. B. and Bershad, B. N. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proc. 14th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: These misses have a significant impact on application performance as it increases the overall CPI for the machine <ref> [Chen 94, Chen & Bershad 93] </ref>. Moreover, more and more applications are being written in C++ and other object oriented languages.
Reference: [Chen 94] <author> Chen, J. B. </author> <title> Memory Behavior For An X11 Window System. </title> <booktitle> In Proc. Winter 1994 USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: However, unlike the SPEC benchmark, many commonly used applications (e.g. compilers, document previewers, database applications, etc.) suffer a significant number of instruction cache misses even for caches as large as 64 KB <ref> [Chen 94, Calder et al. 94] </ref>. These misses have a significant impact on application performance as it increases the overall CPI for the machine [Chen 94, Chen & Bershad 93]. Moreover, more and more applications are being written in C++ and other object oriented languages. <p> These misses have a significant impact on application performance as it increases the overall CPI for the machine <ref> [Chen 94, Chen & Bershad 93] </ref>. Moreover, more and more applications are being written in C++ and other object oriented languages.
Reference: [Chen et al. 93] <author> Chen, W., Chung, P., Conte, T., and Hwu, W. </author> <title> The Effect of Code Expanding Optimizations on Instructio Cache Design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(9) </volume> <pages> 1045-1057, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The Illinois IMPACT compiler uses trace scheduling to schedule code for superscalar machines [Hwu & Chang 89]. The effect of the algorithm used in the IMPACT compiler on a number of scientific benchmarks was studied in <ref> [Chen et al. 93] </ref>. More recently, [Calder & Grunwald 94] looked into the effect that code reordering has on the branch penalties.
Reference: [Digital Equipment Corporation 91] <institution> Digital Equipment Corporation. Cord. Ultrix manual page, </institution> <year> 1991. </year>
Reference-contexts: Cord is a tool used in MIPS machines and in DEC Alpha workstations that uses profile information to reorder basic blocks for better instruction cache usage <ref> [Digital Equipment Corporation 91] </ref>. McFarling uses a labeling algorithm to label program basic blocks so that frequently used basic blocks do not interfere with each other [McFarling 89].
Reference: [Ellis 84] <author> Ellis, J. </author> <title> A Compiler for VLIW Architectures. </title> <type> PhD dissertation, </type> <institution> Yale, </institution> <address> 1984. </address> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: it has a very long inner loop. 1 Profile based code reordering is a technique that uses basic block execution frequencies and the frequency of control transfer between basic blocks to arrange code in the executable so that frequently co-executed code is placed closer together to improve overall application performance <ref> [Ellis 84, Hwu & Chang 89] </ref>. <p> Fisher and Ellis used the technique to schedule code for VLIW machines <ref> [Fisher 81, Ellis 84] </ref>. The Illinois IMPACT compiler uses trace scheduling to schedule code for superscalar machines [Hwu & Chang 89]. The effect of the algorithm used in the IMPACT compiler on a number of scientific benchmarks was studied in [Chen et al. 93].
Reference: [Farkas & Jouppi 94] <author> Farkas, K. I. and Jouppi, N. P. </author> <title> Complexity/Performance Tradeoffs with Non-Blocking Loads. </title> <booktitle> In Proc. 21st Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 211-222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Instruction cache misses are also more difficult to hide than data cache misses. Next generation processors will use non-blocking loads and lock-up free caches that allow the processor to continue execution while the data cache waits for data to come back from memory, effectively hiding data cache misses <ref> [Farkas & Jouppi 94] </ref>. However, the processor is forced to remain idle when a miss occurs in the instruction cache.
Reference: [Fisher 81] <author> Fisher, J. </author> <title> Trace Scheduling: A Technique for Global Microcode Compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-30(7), </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Fisher and Ellis used the technique to schedule code for VLIW machines <ref> [Fisher 81, Ellis 84] </ref>. The Illinois IMPACT compiler uses trace scheduling to schedule code for superscalar machines [Hwu & Chang 89]. The effect of the algorithm used in the IMPACT compiler on a number of scientific benchmarks was studied in [Chen et al. 93].
Reference: [Gee et al. 91] <author> Gee, J. D., Hill, M. D., Pnevmatikatos, D. N., and Smith, A. J. </author> <title> Cache Performance Of The SPEC Benchmark Suite. </title> <type> Technical report, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: In fact, for the industry standard SPEC benchmark suite [SPE 89] (mostly scientific and engineering applications written in Fortran and C), only three benchmarks 1 suffer a significant number of instruction cache misses for instruction cache sizes beyond 8 KB <ref> [Gee et al. 91] </ref>. However, unlike the SPEC benchmark, many commonly used applications (e.g. compilers, document previewers, database applications, etc.) suffer a significant number of instruction cache misses even for caches as large as 64 KB [Chen 94, Calder et al. 94].
Reference: [Graham et al. 83] <author> Graham, S. L., Kessler, P. B., and McKusick, M. K. </author> <title> An Execution Profiler for Modular Programs. </title> <journal> Software Practice and Experience, </journal> <volume> 13 </volume> <pages> 671-685, </pages> <year> 1983. </year>
Reference-contexts: It is an approximation of the information available in the edges since most frequently used blocks would mostly have control transfers to other frequently used blocks. This algorithm utilizes the information readily available from profiling programs like pixie and gprof <ref> [Smith 91, Graham et al. 83] </ref>. So far we've assumed that sequentiality is measured using the frequency of control transfer from one node to another. Another way to measure sequentiality is to use the relative time between transfers [Wu 92].
Reference: [Hwu & Chang 89] <author> Hwu, W. and Chang, P. </author> <title> Achieving High Instruction Cache Performance With An Optimizing Compiler. </title> <booktitle> In Proc. 16th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 183-191, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: On the hardware side, sequential prefetching has been shown to successfully alleviate the effects of instruction cache misses [Jouppi 90]. On the software side, static code reordering based on profile and static code analysis can decrease the miss rates of instruction caches <ref> [Hwu & Chang 89] </ref>. 1 These are fppp, doduc, and gcc. fppp behaves badly because it has a very long inner loop. 1 Profile based code reordering is a technique that uses basic block execution frequencies and the frequency of control transfer between basic blocks to arrange code in the executable <p> it has a very long inner loop. 1 Profile based code reordering is a technique that uses basic block execution frequencies and the frequency of control transfer between basic blocks to arrange code in the executable so that frequently co-executed code is placed closer together to improve overall application performance <ref> [Ellis 84, Hwu & Chang 89] </ref>. <p> This improvement can be attributed to 1) better instruction cache performance because of the smaller working set size, 2) better utilization of functional units by minimizing control transfers, and 3) smaller number of dynamic instructions by eliminating control flow instructions in frequently executed code paths <ref> [Pettis & Hansen 90, Hwu & Chang 89] </ref>. In this paper, we evaluate the instruction cache performance of different code reordering algorithms on the MIPS R2000 [Kane 88] using trace based simulations. <p> Fisher and Ellis used the technique to schedule code for VLIW machines [Fisher 81, Ellis 84]. The Illinois IMPACT compiler uses trace scheduling to schedule code for superscalar machines <ref> [Hwu & Chang 89] </ref>. The effect of the algorithm used in the IMPACT compiler on a number of scientific benchmarks was studied in [Chen et al. 93]. More recently, [Calder & Grunwald 94] looked into the effect that code reordering has on the branch penalties.
Reference: [Jouppi 90] <author> Jouppi, N. P. </author> <title> Improving Direct-Mapped Cache Performance By The Addition Of A Small Fully-Associative Cache And Prefetch Buffers. </title> <booktitle> In Proc. 17th Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The only way to deal with instruction cache misses is to either hide the latency of the miss before it happens or to avoid the miss altogether. On the hardware side, sequential prefetching has been shown to successfully alleviate the effects of instruction cache misses <ref> [Jouppi 90] </ref>.
Reference: [Kane 88] <author> Kane, G. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: In this paper, we evaluate the instruction cache performance of different code reordering algorithms on the MIPS R2000 <ref> [Kane 88] </ref> using trace based simulations.
Reference: [McFarling 89] <author> McFarling, S. </author> <title> Program Optimization For Instruction Caches. </title> <booktitle> In Proc. 3rd International Conference On Architectural Support For Programming Languages And Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: McFarling uses a labeling algorithm to label program basic blocks so that frequently used basic blocks do not interfere with each other <ref> [McFarling 89] </ref>.
Reference: [Pettis & Hansen 90] <author> Pettis, K. and Hansen, R. </author> <title> Profile Guided Code Positioning. </title> <booktitle> In Proc. Conference On Programming Language Design And Implementation, </booktitle> <pages> pages 16-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This improvement can be attributed to 1) better instruction cache performance because of the smaller working set size, 2) better utilization of functional units by minimizing control transfers, and 3) smaller number of dynamic instructions by eliminating control flow instructions in frequently executed code paths <ref> [Pettis & Hansen 90, Hwu & Chang 89] </ref>. In this paper, we evaluate the instruction cache performance of different code reordering algorithms on the MIPS R2000 [Kane 88] using trace based simulations.
Reference: [Smith 91] <author> Smith, M. D. </author> <title> Tracing with pixie. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: It is an approximation of the information available in the edges since most frequently used blocks would mostly have control transfers to other frequently used blocks. This algorithm utilizes the information readily available from profiling programs like pixie and gprof <ref> [Smith 91, Graham et al. 83] </ref>. So far we've assumed that sequentiality is measured using the frequency of control transfer from one node to another. Another way to measure sequentiality is to use the relative time between transfers [Wu 92].
Reference: [SPE 89] <institution> System Performance Evaluation Cooperative. SPEC Benchmark Suite Release 1.0, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Typically, on-chip caches are split: one cache is for instructions and another one is for data. The instruction cache tends to have a better hit rate than the data cache because of better locality. In fact, for the industry standard SPEC benchmark suite <ref> [SPE 89] </ref> (mostly scientific and engineering applications written in Fortran and C), only three benchmarks 1 suffer a significant number of instruction cache misses for instruction cache sizes beyond 8 KB [Gee et al. 91].
Reference: [Thekkath & Eggers 94] <author> Thekkath, R. and Eggers, S. J. </author> <title> Impact of Sharing-Based Thread Placement on Multithreaded Architectures. </title> <booktitle> In Proc. 21st Annual International Symposium On Computer Architecture, </booktitle> <pages> pages 176-186, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The situation becomes more serious as processors are built to exploit instruction level parallelism: 1) multi-threaded processors place a greater demand on the instruction cache as code from multiple working sets need to coexist in the cache <ref> [Thekkath & Eggers 94] </ref>, and 2) aggressive superscalar processors will increase the performance penalty associated with an instruction cache miss as more functional units idle while the instruction is fetched from memory.
Reference: [Wall 92] <author> Wall, D. W. </author> <title> Systems for Late Code Modification, </title> <address> pages 275-293. </address> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In Section 4.2 we look at the merits of using one measure over the other. 3 Methodology We use traces from code running on DECStation 5000/200 machines under Ultrix 4.2 using the Epoxie tracer from DEC WRL and CMU <ref> [Wall 92] </ref>. The traces contain both user and system references. The benchmark suite for this study is shown in Table 1. These programs are representative of a range of applications written in a diverse set of languages (i.e., Fortran, C, C++, and Cecil 3 ).

References-found: 23

