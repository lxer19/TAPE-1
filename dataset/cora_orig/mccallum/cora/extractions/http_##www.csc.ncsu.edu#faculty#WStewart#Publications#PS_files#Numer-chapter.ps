URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/PS_files/Numer-chapter.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/Publications.html
Root-URL: http://www.csc.ncsu.edu
Title: Numerical Methods for Computing Stationary Distributions of Finite Irreducible Markov Chains  
Author: William J. Stewart 
Address: Raleigh, N.C. 27695-8206, USA.  
Affiliation: Department of Computer Science, North Carolina State University,  
Date: July 21, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A.V. Aho, J.E. Hopcroft and J.D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address> <year> 1974. </year>
Reference-contexts: We have been successful in using a depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in <ref> [1] </ref>. Coding details for DFS are given in [26]. The complexity of this algorithm is O (jV j + jEj), where jV j is the number of vertices and jEj is the number of edges in the graph.
Reference: [2] <author> W.E. </author> <title> Arnoldi. The principle of minimized iteration in the solution of the matrix eigenvalue problem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9, </volume> <pages> pp. 17-29, </pages> <year> 1951. </year>
Reference-contexts: Performance can be dramatically improved. Details on implementation and some experiments are described in [43]. 14 5.3 Arnoldi's Method A second technique discussed in the literature is Arnoldi's method <ref> [2, 45] </ref> which is an orthogonal projection process onto K m = spanfv 1 ; Av 1 ; : : : ; A m1 v 1 g.
Reference: [3] <author> O. Alexsson. </author> <title> A survey of preconditioned iterative methods for linear systems of algebraic equations. </title> <journal> BIT, </journal> <volume> 25, </volume> <pages> pp. 166-187, </pages> <year> 1985. </year>
Reference-contexts: In a general context, a preconditioning technique consists of replacing a system Ax = b by a modified system such as M 1 Ax = M 1 b. Here M is a preconditioning matrix for which the solution of M x = y is inexpensive, <ref> [3] </ref>. When the coefficient matrix is singular and the right hand side is zero, the method turns out to be equivalent to the power method applied to the matrix (I M 1 A).
Reference: [4] <author> S. Balsamo and B. Pandolfi. </author> <title> Bounded aggregation in Markovian networks. Computer Performance and Reliability, </title> <editor> Eds. G. Iazeolla, P.J. Courtois and O.J. Boxma, </editor> <publisher> North Holland, Amsterdam, </publisher> <pages> pp. 73-92, </pages> <year> 1988. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, <ref> [4] </ref>, [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [5] <author> G.P. Barker and R.J. Plemmons. </author> <title> Convergent iterations for computing stationary distributions of Markov chains. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7, </volume> <pages> pp. 390-398, </pages> <year> 1986. </year>
Reference-contexts: In these examples the magnitude of the nonzero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best. For further information on convergence and convergence properties, see <ref> [5] </ref>, [6], [13], [34] and [58]. 8 4.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: [6] <author> A. Berman and R.J. Plemmons. </author> <title> Nonnegative Matrices in the Mathematical Sciences. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: In these examples the magnitude of the nonzero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best. For further information on convergence and convergence properties, see [5], <ref> [6] </ref>, [13], [34] and [58]. 8 4.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: [7] <author> W.L. Cao and W.J. Stewart. </author> <title> Iterative aggregation/disaggregation techniques for nearly uncoupled Markov chains. </title> <journal> J. Assoc. Comp. Mach., </journal> <volume> 32, 3, </volume> <pages> pp. 702-719, </pages> <year> 1985. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], <ref> [7] </ref>, [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [8] <author> F. Chatelin. </author> <title> Spectral Approximation of Linear Operators. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], <ref> [8] </ref>, [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [9] <author> F. Chatelin. </author> <title> Iterative aggregation/disaggregation methods. Mathematical Computer Performance and Reliability, </title> <editor> Eds. G. Iazeolla, P.J. Courtois and A. Hordijk, </editor> <publisher> North Holland, Amsterdam, </publisher> <pages> pp. 199-207, </pages> <year> 1984. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], <ref> [9] </ref>, [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [10] <author> F. Chatelin and W.L. Miranker. </author> <title> Acceleration by aggregation of successive approximation methods. </title> <journal> Linear Algebra Appl., </journal> <volume> 43, </volume> <pages> pp. 17-47, </pages> <year> 1982. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], <ref> [10] </ref>, [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [11] <author> P.J. Courtois. </author> <title> Decomposability; Queueing and Computer System Applications. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1977. </year>
Reference-contexts: The pioneering work on NCD systems was performed by Simon and Ando, [51], in investigating the dynamic behavior of linear systems as they apply to economic models. The concept was later extended to Markov chains and the performance analysis of computer systems by Courtois, <ref> [11] </ref>.
Reference: [12] <author> P.J. Courtois and P. Semal. </author> <title> Bounds for the positive eigenvectors of nonnegative matrices and their approximation by decomposition. </title> <journal> J. Assoc. Comp. Mach. </journal> <volume> 31, </volume> <pages> pp. 804-825, </pages> <year> 1984. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], <ref> [12] </ref>, [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [13] <author> P.J. Courtois and P. Semal. </author> <title> Block iterative algorithms for stochastic matrices. </title> <journal> Linear Algebra Appl., </journal> <volume> 76, </volume> <pages> pp. 59-70, </pages> <year> 1986. </year>
Reference-contexts: In these examples the magnitude of the nonzero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best. For further information on convergence and convergence properties, see [5], [6], <ref> [13] </ref>, [34] and [58]. 8 4.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: [14] <author> T. Dayar and W.J. Stewart. </author> <title> On the Effects of Using the Grassmann-Taksar-Heyman method in Iterative Aggregation-Disaggregation. </title> <journal> SIAM Journal on Scientific Computing. </journal> <volume> Vol. 17, </volume> <pages> pp 1 - 17, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately, difficulties arise in implementing GTH when computer memory is at a premium and sparse compact storage schemes, such as those described in Section 3.2, must be used <ref> [14] </ref>. Suppose first that Q is stored by rows and an LU decomposition of Q is sought.
Reference: [15] <author> T. Dayar and W.J. Stewart. Quasi-Lumpability, </author> <title> Lower Bounding Coupling Matrices and nearly Completely Decomposable Markov Chains. </title> <note> To appear in: SIAM Journal on Matrix Analysis and Applications , (with T. Dayar). </note>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], <ref> [15] </ref>, [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [16] <author> B.N. Feinberg and S.S. Chiu. </author> <title> A method to calculate steady state distributions of large Markov chains by aggregating states. </title> <journal> Operations Research, </journal> <volume> 35, </volume> <pages> pp. 282-290, </pages> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], <ref> [16] </ref>, [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [17] <author> R.E. Funderlic and C.D. Meyer. </author> <title> Sensitivity of the stationary distribution vector for an ergodic Markov chain. </title> <journal> Linear Algebra Appl., </journal> <volume> 76, </volume> <pages> pp. 1-17, </pages> <year> 1986. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, <ref> [17] </ref>, [19], [20], [21], [22], [25], [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [18] <author> R.E. Funderlic and R.J. Plemmons. </author> <title> A combined direct-iterative method for certain M-matrix linear systems. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> Vol. 5, No. 1, </volume> <pages> pp. 32-42, </pages> <year> 1984. </year>
Reference-contexts: As before, the diagonal elements are kept regardless of their magnitude. This incomplete factorization is referred to as ILUK. Although the above three ILU factorizations are the only ones we cite, there are other possibilities (see <ref> [18] </ref>, [41] for example). However, we believe that, for the moment, ILU0, ILUTH and ILUK are the most effective for Markov chain problems. 4.7 Block Iterative Methods In Markov chain problems it is frequently the case that the state space can be meaningfully partitioned into subsets.
Reference: [19] <author> R.E. Funderlic and R.J. Plemmons. </author> <title> Updating LU factorizations for computing stationary distributions. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7, </volume> <pages> pp. 30-42, </pages> <year> 1986. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], <ref> [19] </ref>, [20], [21], [22], [25], [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [20] <author> G.H. Golub and C.D. Meyer. </author> <title> Using the QR factorization and group inversion to compute, differentiate and estimate the sensitivity of stationary distributions for Markov chains. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7, </volume> <pages> pp. 273-281, </pages> <year> 1986. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], <ref> [20] </ref>, [21], [22], [25], [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [21] <author> W.K. Grassmann, M.J. </author> <title> Taksar and D.P. Heyman. Regenerative analysis and steady state distributions for Markov chains. </title> <journal> Operations Research, </journal> <volume> 33, </volume> <pages> pp. 1107-1116, </pages> <year> 1985. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], [20], <ref> [21] </ref>, [22], [25], [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory. <p> This procedure is commonly referred to as the GTH (Grassmann-Taksar-Heyman) algorithm <ref> [21, 50] </ref>. In GTH the diagonal elements are obtained by summing the off-diagonal elements rather than performing a subtraction; it is known that subtractions can sometimes lead to loss of significance in the representation of real numbers.
Reference: [22] <author> W.J. Harrod and R.J. Plemmons. </author> <title> Comparisons of some direct methods for computing stationary distributions of Markov chains. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 5, </volume> <pages> pp. 453-469, </pages> <year> 1984. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], [20], [21], <ref> [22] </ref>, [25], [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [23] <author> M. Haviv. </author> <title> Aggregation/disagregation methods for computing the stationary distribution of a Markov chain. </title> <journal> SIAM J. Numer. Anal. </journal> , <volume> 24, </volume> <pages> pp. 952-966, </pages> <year> 1987. </year>
Reference-contexts: Some of these methods have been applied to large economic models and most recently to the analysis of computer systems. Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], <ref> [23] </ref>, [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [24] <author> M. Haviv. </author> <title> More on a Rayleigh-Ritz refinement technique for nearly uncoupled stochastic matrices. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 10, </volume> <pages> pp. 287-293, </pages> <year> 1989. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], <ref> [24] </ref>, [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [25] <author> D.P. Heyman. </author> <title> Further comparisons of direct methods for computing stationary distributions of Markov chains. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 8, </volume> <pages> pp. 226-232, </pages> <year> 1987. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], [20], [21], [22], <ref> [25] </ref>, [56] and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [26] <author> J.E. Hopcroft and R.J. Tarjan. </author> <title> Efficient algorithms for graph manipulation. </title> <journal> CACM, </journal> <volume> 16, 6, </volume> <pages> pp. 372-378, </pages> <year> 1973. </year>
Reference-contexts: We have been successful in using a depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in [1]. Coding details for DFS are given in <ref> [26] </ref>. The complexity of this algorithm is O (jV j + jEj), where jV j is the number of vertices and jEj is the number of edges in the graph.
Reference: [27] <author> A. Jennings and W.J. Stewart. </author> <title> Simultaneous iteration for partial eigensolution of real matrices. </title> <journal> J. IMA, </journal> <volume> 15, </volume> <pages> pp. 351-361, </pages> <year> 1975. </year>
Reference-contexts: This is then referred to as an orthogonal projection process. 5.2 Subspace Iteration One method for computing invariant subspaces is called subspace iteration. In its simplest form, subspace iteration can be described as follows; see <ref> [27] </ref>, [28], [54], [59] for details. Algorithm: Subspace Iteration 1. Choose an initial orthonormal system V 0 j [v 1 ; v 2 ; : : :; v m ] and an integer k; 2. Compute X = A k V 0 and orthonormalize X to get V . 3.
Reference: [28] <author> A. Jennings and W.J. Stewart. </author> <title> A simultaneous iteration algorithm for real matrices. </title> <journal> ACM Trans. of Math. Software, </journal> <volume> 7, </volume> <pages> pp. 184-198, </pages> <year> 1981. </year>
Reference-contexts: This is then referred to as an orthogonal projection process. 5.2 Subspace Iteration One method for computing invariant subspaces is called subspace iteration. In its simplest form, subspace iteration can be described as follows; see [27], <ref> [28] </ref>, [54], [59] for details. Algorithm: Subspace Iteration 1. Choose an initial orthonormal system V 0 j [v 1 ; v 2 ; : : :; v m ] and an integer k; 2. Compute X = A k V 0 and orthonormalize X to get V . 3.
Reference: [29] <author> L. Kaufman. </author> <title> Matrix methods for queueing problems. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 4, </volume> <pages> pp. 525-552, </pages> <year> 1983. </year>
Reference-contexts: We point out that some specialized counter examples exist which makes the above recommendations only rules of thumb, <ref> [29] </ref>. Little information is available on the effect of the ordering of the state space on the convergence of these iterative methods. Examples are available in which Gauss-Seidel works well for one ordering but not at all for an opposing ordering, [37].
Reference: [30] <author> D.S. Kim and R.L. Smith. </author> <title> An exact aggregation algorithm for mandatory set decomposable Markov chains. First International Workshop on the Numerical Solution of Markov Chains. </title> <type> N. </type> <institution> Carolina State University, </institution> <address> Raleigh, </address> <month> January </month> <year> 1990. </year> <note> Published by Marcel Dekker, </note> <institution> Inc. </institution> <year> 1990. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], <ref> [30] </ref>, [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [31] <author> R. Koury, D.F. McAllister and W.J. Stewart. </author> <title> Iterative methods for computing stationary distributions of nearly completely decomposable Markov chains. </title> <journal> SIAM J. Alg. Disc. Math., </journal> <volume> 5, 2, </volume> <pages> pp. 164-186, </pages> <year> 1984. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], <ref> [31] </ref>, [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [32] <author> U.R. Krieger. </author> <title> Numerical solution of large finite Markov chains by algebraic multigrid techniques. Computations with Markov Chains, </title> <editor> Edited by William J. Stewart, </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], <ref> [32] </ref>, [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [33] <author> S.T. Leutenegger and G. Horton. </author> <title> On the utility of the multi-level algorithm for the solution of nearly compleely decomposable Markov chains. Computations with Markov Chains, </title> <editor> Edited by William J. Stewart, </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], <ref> [33] </ref>, [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [34] <author> B.D. Lubachevsky and D. Mitra. </author> <title> A chaotic asynchronous algorithm for computing the fixed point of a non-negative matrix of unit spectral radius. </title> <journal> J. Asoc. Comput. Mach., </journal> <volume> 33, </volume> <pages> pp. 130-150, </pages> <year> 1986. </year>
Reference-contexts: In these examples the magnitude of the nonzero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best. For further information on convergence and convergence properties, see [5], [6], [13], <ref> [34] </ref> and [58]. 8 4.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: [35] <author> D.F. McAllister, G.W. Stewart and W.J. Stewart. </author> <title> On a Raleigh-Ritz refinement technique for nearly uncoupled stochastic matrices. Linear Alg. </title> <journal> Applications, </journal> <volume> 60, </volume> <pages> pp. 1-25, </pages> <year> 1984. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], <ref> [35] </ref>, [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious.
Reference: [36] <author> C.D. Meyer. </author> <title> Stochastic complementation, uncoupling Markov chains and the theory of nearly reducible systems. </title> <journal> SIAM Rev., </journal> <volume> 31, </volume> <pages> pp. 240-272, </pages> <year> 1989. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], <ref> [36] </ref>, [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions. <p> In particular, it may be noted that there exists a distribution of this probability mass that results in the exact answer i being obtained up to a multiplicative constant, <ref> [36] </ref>. Unfortunately it is not known how to determine this distribution without either a knowledge of the stationary probability vector itself, or performing extensive calculations possibly in excess of that required to compute the exact solution.
Reference: [37] <author> D. Mitra and P. Tsoucas. </author> <title> Relaxations for the numerical solutions of some stochastic problems. Stochastic Models, </title> <type> 4, 3, </type> <year> 1988. </year>
Reference-contexts: Little information is available on the effect of the ordering of the state space on the convergence of these iterative methods. Examples are available in which Gauss-Seidel works well for one ordering but not at all for an opposing ordering, <ref> [37] </ref>. In these examples the magnitude of the nonzero elements appears to have little effect on the speed of convergence. It appears that an ordering that in some sense preserves the direction of probability flow works best.
Reference: [38] <author> B. Philippe, Y.Saad and W.J. Stewart. </author> <title> Numerical methods in Markov chain modelling, </title> <journal> Operations Research, </journal> <volume> Vol. 40, No. 6, </volume> <pages> pp. 1156-1179, </pages> <year> 1992, </year>
Reference-contexts: Such processes have begun to be applied successfully to Markov chain problems <ref> [38] </ref>. Whereas iterative methods begin with an approximate solution vector that is modified at each iteration and which (supposedly) converges to a solution, projection methods create vector subspaces and search for the best possible approximation to the solution that can be obtained from that subspace.
Reference: [39] <author> M. Rieders. </author> <title> State space decomposition for large Markov chains. Computations with Markov Chains, </title> <editor> Edited by William J. Stewart, </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], <ref> [39] </ref>, [47], [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [40] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: Similarly the approximate Schur vectors are the vector columns of V U , where U = [u 1 ; u 2 ; : : : ; u m ] are the Schur vectors of C, i.e., U H CU is quasi-upper triangular, (see, for example, <ref> [40] </ref>, pp. 17-18). A common particular case is when K = L and V = W is an orthogonal basis of K. This is then referred to as an orthogonal projection process. 5.2 Subspace Iteration One method for computing invariant subspaces is called subspace iteration.
Reference: [41] <author> Y. Saad. </author> <title> Preconditioned Krylov subspace methods for the numerical solution of Markov chains. Computations with Markov Chains, </title> <editor> Edited by William J. Stewart, </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year>
Reference-contexts: As before, the diagonal elements are kept regardless of their magnitude. This incomplete factorization is referred to as ILUK. Although the above three ILU factorizations are the only ones we cite, there are other possibilities (see [18], <ref> [41] </ref> for example). However, we believe that, for the moment, ILU0, ILUTH and ILUK are the most effective for Markov chain problems. 4.7 Block Iterative Methods In Markov chain problems it is frequently the case that the state space can be meaningfully partitioned into subsets.
Reference: [42] <author> Y. Saad. </author> <title> Krylov subspace methods for solving large unsymmetric linear systems. </title> <journal> Math. Comp., </journal> <volume> 37, </volume> <pages> pp. 105-126, </pages> <year> 1981. </year>
Reference-contexts: Then the solution is clearly non-unique and one may wonder whether or not this can cause the corresponding iterative schemes to fail. The answer is usually no and we will illustrate in this 15 section how standard Krylov subspace methods can be used to solve (10), <ref> [42] </ref>. We start by describing the GMRES (generalized minimum residual) algorithm, [46], for solving the more common linear system Ax = b: (11) in which A is nonsingular. GMRES is a least squares procedure for solving (11) on the Krylov subspace K m .
Reference: [43] <author> Y. Saad. </author> <title> Chebyshev acceleration techniques for solving non-symmetric eigenvalue problems. </title> <journal> Mathematics of Computation, </journal> <volume> 42, </volume> <pages> pp. 567-588, </pages> <year> 1984. </year>
Reference-contexts: The three-term recurrence of Chebyshev polynomials allows us to compute a vector w = t k (A)v at almost the same cost as A k v. Performance can be dramatically improved. Details on implementation and some experiments are described in <ref> [43] </ref>. 14 5.3 Arnoldi's Method A second technique discussed in the literature is Arnoldi's method [2, 45] which is an orthogonal projection process onto K m = spanfv 1 ; Av 1 ; : : : ; A m1 v 1 g.
Reference: [44] <author> Y. Saad. </author> <title> Projection methods for solving large sparse eigenvalue problems. </title> <editor> In B. Kagstrom and A. Ruhe, editors, </editor> <title> Matrix Pencils, </title> <booktitle> proceedings, Pitea Havsbad, </booktitle> <pages> pp. 121-144, </pages> <institution> University of Umea, </institution> <address> Sweden, </address> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: In this case the reduction in the number of iterations makes the block methods very attractive indeed. 5 Projection Techniques 5.1 General Projection Processes An idea that is basic to sparse linear systems and eigenvalue problems is that of projection processes <ref> [44] </ref>. Such processes have begun to be applied successfully to Markov chain problems [38].
Reference: [45] <author> Y. Saad. </author> <title> Variations on Arnoldi's method for computing eigenelements of large unsymmetric matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 34, </volume> <pages> pp. 269-295, </pages> <year> 1980. </year>
Reference-contexts: Performance can be dramatically improved. Details on implementation and some experiments are described in [43]. 14 5.3 Arnoldi's Method A second technique discussed in the literature is Arnoldi's method <ref> [2, 45] </ref> which is an orthogonal projection process onto K m = spanfv 1 ; Av 1 ; : : : ; A m1 v 1 g.
Reference: [46] <author> Y. Saad and M.H. Schultz. </author> <title> GMRES: A generalized minimal residual algorithm for solving non-symmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7, </volume> <pages> pp. 856-869, </pages> <year> 1986. </year>
Reference-contexts: The answer is usually no and we will illustrate in this 15 section how standard Krylov subspace methods can be used to solve (10), [42]. We start by describing the GMRES (generalized minimum residual) algorithm, <ref> [46] </ref>, for solving the more common linear system Ax = b: (11) in which A is nonsingular. GMRES is a least squares procedure for solving (11) on the Krylov subspace K m . <p> Details of this algorithm can be found in <ref> [46] </ref>. 5.5 Preconditioned Arnoldi and GMRES Algorithms Preconditioning techniques can also be used to improve the convergence rates of Arnoldi's method and GMRES.
Reference: [47] <author> P.J. Schweitzer. </author> <title> Aggregation methods for large Markov chains. </title> <booktitle> International Workshop on Applied Mathematics and Performance Reliability Models of Computer Communication Systems. </booktitle> <institution> University of Pisa, </institution> <address> Italy, </address> <pages> pp. 225-234, </pages> <year> 1983. </year> <month> 24 </month>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], <ref> [47] </ref>, [48], [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [48] <author> P.J. Schweitzer. </author> <title> Perturbation series expansions for nearly completely decomposable Markov chains. Teletraffic Analysis and Computer Performance Evaluation, </title> <editor> Eds. O.J. Boxma, J.W. Cohen and H.C. Tijms, </editor> <publisher> Elsevier North-Holland, Amsterdam, </publisher> <pages> pp. 319-328, </pages> <year> 1986. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], <ref> [48] </ref>, [49], [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [49] <author> P.J. Schweitzer and K.W. Kindle. </author> <title> An iterative aggregation-disaggregation algorithm for solving linear systems. </title> <journal> Applied Math. and Comp., </journal> <volume> 18, </volume> <pages> pp. 313-353, </pages> <year> 1986. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], <ref> [49] </ref>, [52], [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [50] <author> T.J. Sheskin. </author> <title> A Markov chain partitioning algorithm for computing steady state probabilities. </title> <journal> Operations Research, </journal> <volume> 33(1) </volume> <pages> 228-235, </pages> <year> 1985. </year>
Reference-contexts: This procedure is commonly referred to as the GTH (Grassmann-Taksar-Heyman) algorithm <ref> [21, 50] </ref>. In GTH the diagonal elements are obtained by summing the off-diagonal elements rather than performing a subtraction; it is known that subtractions can sometimes lead to loss of significance in the representation of real numbers.
Reference: [51] <author> H.A. Simon and A. Ando. </author> <title> Aggregation of variables in dynamic systems. </title> <type> Econometrica 29, </type> <pages> pp. 111-138, </pages> <year> 1961. </year>
Reference-contexts: It is apparent that the assumption that the subsystems are independent and can therefore be solved separately does not hold. Consequently an error arises. This error will be small if the assumption is approximately true. The pioneering work on NCD systems was performed by Simon and Ando, <ref> [51] </ref>, in investigating the dynamic behavior of linear systems as they apply to economic models. The concept was later extended to Markov chains and the performance analysis of computer systems by Courtois, [11].
Reference: [52] <author> G.W. Stewart. </author> <title> Computable error bounds for aggregated Markov chains. </title> <journal> J. Assoc. Comp. Mach. </journal> <volume> 30, </volume> <pages> pp. 271-285, </pages> <year> 1983. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], <ref> [52] </ref>, [53], [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [53] <author> G.W. Stewart, W.J. Stewart and D.F. McAllister. </author> <title> A two stage iteration for solving nearly uncoupled Markov chains. </title> <journal> IMA Volumes in Mathematics and its Applications, </journal> <volume> Vol. 60, </volume> <booktitle> Recent Advances in Iterative Methods, </booktitle> <pages> pp. 201-216, </pages> <year> 1993. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], <ref> [53] </ref>, [55], [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [54] <author> G.W. Stewart. </author> <title> Simultaneous iteration for computing invariant subspaces of non-Hermitian matrices. </title> <journal> Numer. Mat., </journal> <volume> 25, </volume> <pages> pp. 123-136, </pages> <year> 1976. </year>
Reference-contexts: This is then referred to as an orthogonal projection process. 5.2 Subspace Iteration One method for computing invariant subspaces is called subspace iteration. In its simplest form, subspace iteration can be described as follows; see [27], [28], <ref> [54] </ref>, [59] for details. Algorithm: Subspace Iteration 1. Choose an initial orthonormal system V 0 j [v 1 ; v 2 ; : : :; v m ] and an integer k; 2. Compute X = A k V 0 and orthonormalize X to get V . 3.
Reference: [55] <author> G.W. Stewart. </author> <title> On the sensitivity of nearly uncoupled Markov chains. First International Workshop on the Numerical Solution of Markov Chains. </title> <type> N. </type> <institution> Carolina State University, </institution> <address> Raleigh, </address> <month> January </month> <year> 1990. </year> <note> Published by Marcel Dekker, </note> <institution> Inc. </institution> <year> 1990. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], <ref> [55] </ref>, [61], [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [56] <author> W.J. Stewart. </author> <title> A comparison of numerical techniques in Markov modelling. </title> <journal> Comm. ACM, </journal> <volume> 21, </volume> <pages> pp. 144-151, </pages> <year> 1978. </year>
Reference-contexts: For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], [20], [21], [22], [25], <ref> [56] </ref> and [58]. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory.
Reference: [57] <author> W.J. Stewart. MARCA: </author> <title> Markov Chain Analyzer. A software package for Markov modelling. </title> <note> IEEE Computer Repository No R76 232, 1976. Also IRISA Publication Interne No. 45, </note> <institution> Universite de Rennes, France. </institution>
Reference: [58] <author> W.J. Stewart. </author> <title> An Introduction to the Numerical Solution of Markov Chains, </title> <publisher> Princeton University Press, </publisher> <address> New Jersey, </address> <year> 1994. </year>
Reference-contexts: To make up for this, 1 we have provided an extensive bibliography at the end of the chapter. In particular, we cite the recent text of the author of this chapter, <ref> [58] </ref>. <p> For more information on these and other direct methods, including sensitivity analyses, the reader should consult the following references, [17], [19], [20], [21], [22], [25], [56] and <ref> [58] </ref>. 3.2 Compact Storage Schemes for Direct Methods Frequently the matrices generated from Markov models are too large to permit regular two-dimensional arrays to be used to store them in computer memory. <p> It appears that an ordering that in some sense preserves the direction of probability flow works best. For further information on convergence and convergence properties, see [5], [6], [13], [34] and <ref> [58] </ref>. 8 4.3 SSOR Iteration The Symmetric Successive Overrelaxation method (SSOR) consists of following a relaxation sweep from top down by a relaxation sweep from bottom up.
Reference: [59] <author> W.J. Stewart and A. Jennings. </author> <title> A simultaneous iteration algorithm for real matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7, </volume> <pages> pp. 184-198, </pages> <year> 1981. </year>
Reference-contexts: This is then referred to as an orthogonal projection process. 5.2 Subspace Iteration One method for computing invariant subspaces is called subspace iteration. In its simplest form, subspace iteration can be described as follows; see [27], [28], [54], <ref> [59] </ref> for details. Algorithm: Subspace Iteration 1. Choose an initial orthonormal system V 0 j [v 1 ; v 2 ; : : :; v m ] and an integer k; 2. Compute X = A k V 0 and orthonormalize X to get V . 3.
Reference: [60] <author> W.J. Stewart and W. Wu. </author> <title> Numerical Experiments with Iteration and Aggregation for Markov Chains. </title> <journal> ORSA Journal on Computing, </journal> <volume> Vol 4, No. 3, </volume> <pages> pp. 336-350, </pages> <year> 1992. </year>
Reference-contexts: For more information on Iteration/Aggregation algorithms, including implementation details and the results of many test problems, the interested reader is referred to <ref> [60] </ref>. 21
Reference: [61] <author> U. Sumita and M. Reiders. </author> <title> A comparison of the replacement process with aggregation-disaggregation. First International Workshop on the Numerical Solution of Markov Chains. </title> <type> N. </type> <institution> Carolina State University, </institution> <address> Raleigh, </address> <month> January </month> <year> 1990. </year> <note> Published by Marcel Dekker, </note> <institution> Inc. </institution> <year> 1990. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], <ref> [61] </ref>, [62], [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [62] <author> Y. Takahashi. </author> <title> A lumping method for numerical calculation of stationary distributions of Markov chains. </title> <institution> Research Report B-18, Department of Information Sciences, Tokyo Institute of Technology, </institution> <address> Tokyo, Japan, </address> <month> June </month> <year> 1975. </year> <month> 25 </month>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], <ref> [62] </ref>, [64] and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [63] <author> R.J. Tarjan. </author> <title> Depth first search and linear graph algorithms. </title> <journal> SIAM J. Comput. </journal> <volume> 1, 2, </volume> <pages> pp. 146-160, </pages> <year> 1972. </year>
Reference-contexts: Only after reordering the states, can we guarantee that the resulting transition matrix will have the property that directly reflects the structural characteristics of the NCD system. This may be accomplished by treating the Markov chain as a directed graph and utilizing some of the graph algorithms of Tarjan <ref> [63] </ref>. We have been successful in using a depth-first search (DFS) algorithm which searches in the forward (deeper) direction as long as possible. Details of the non-recursive algorithm of DFS are given in [1]. Coding details for DFS are given in [26].
Reference: [64] <author> H. Vantilborgh. </author> <title> Agreggation with an error of O(* 2 ). JACM, </title> <booktitle> 32, </booktitle> <pages> pp. 161-190, </pages> <year> 1985. </year>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], <ref> [64] </ref> and [70]. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
Reference: [65] <author> R.S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: This has been shown to be true under fairly general assumptions on the coefficient matrix for general systems of equations (see <ref> [65] </ref>).
Reference: [66] <author> V.L. Wallace and R.S. Rosenberg. </author> <title> Markovian models and numerical analysis of computer system behavior. </title> <booktitle> Proc. AFIPS Spring Joint Computer Conference, 28, </booktitle> <address> New Jersey: </address> <publisher> AFIPS Press, </publisher> <year> 1966. </year>
Reference: [67] <author> V.L. Wallace and R.S. Rosenberg. RQA-1: </author> <title> The recursive queue analyzer. </title> <type> Technical Report No. 2, </type> <institution> Systems Engineering Laboratory, university of Michigan, Ann Arbor, Michigan, </institution> <year> 1966. </year>
Reference-contexts: It is obviously impossible in a single chapter to cover the entire field of numerical procedures for computing stationary distributions of Markov chains, a field that has been in constant evolution from the initial papers of Wallace and his colleagues, ([66], <ref> [67] </ref>, [68]). To make up for this, 1 we have provided an extensive bibliography at the end of the chapter. In particular, we cite the recent text of the author of this chapter, [58].
Reference: [68] <author> V.L. Wallace. </author> <title> Towards an algebraic theory of Markovian networks. </title> <booktitle> Proc. Symp. Computer-Communication Networks and Teletraffic, </booktitle> <address> New York: </address> <publisher> Polytechnic Press, </publisher> <year> 1973. </year>
Reference-contexts: It is obviously impossible in a single chapter to cover the entire field of numerical procedures for computing stationary distributions of Markov chains, a field that has been in constant evolution from the initial papers of Wallace and his colleagues, ([66], [67], <ref> [68] </ref>). To make up for this, 1 we have provided an extensive bibliography at the end of the chapter. In particular, we cite the recent text of the author of this chapter, [58].
Reference: [69] <author> D.M. Young. </author> <title> Iterative solution of large linear systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The resulting optimal convergence rate can be a considerably improvement over Gauss-Seidel. The choice of an optimal, or even a reasonable, value for ! has been the subject of much study, especially for problems arising in the numerical solution of partial differential equations <ref> [69] </ref>. Some results have been obtained for certain classes of matrices. Unfortunately, very little is known at present for arbitrary non-symmetric linear systems.
Reference: [70] <author> R.L. Zarling. </author> <title> Numerical solutions of nearly completely decomposable queueing networks. </title> <type> Ph.D. </type> <institution> Dessertation, Department of Computer Science, University of North Carolina, </institution> <year> 1976. </year> <month> 26 </month>
Reference-contexts: Currently, there is a very large research effort being devoted to these methods, by many different research groups. (See, for example, [4], [7], [8], [9], [10], [12], [15], [16], [23], [24], [30], [31], [32], [33], [35], [36], [39], [47], [48], [49], [52], [53], [55], [61], [62], [64] and <ref> [70] </ref>. With the advent of parallel and distributed computing systems, their advantages are immediately obvious. Ideally the problem is broken into subproblems that can be solved independently and the global solution obtained by "pasting" together the subproblem solutions.
References-found: 70

