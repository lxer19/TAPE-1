URL: ftp://ftp.cs.washington.edu/tr/1997/11/UW-CSE-97-11-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-name.html
Root-URL: http://www.cs.washington.edu
Title: Wrapper Induction for Information Extraction  
Author: by Nicholas Kushmerick 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1997  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [Adali et al. 96] <author> Adali, S., Candan, K., Papakonstantinou, Y., and Subrahmanian, V. </author> <title> Query caching and optimization in distributed mediator systems. </title> <booktitle> In Procceedings of SIGMOD-96, </booktitle> <year> 1996. </year>
Reference: [Aiken 95] <author> Aiken, P. </author> <title> Data Reverse Engineering: Staying the Legacy Dragon. </title> <publisher> McGraw Hill, </publisher> <year> 1995. </year>
Reference: [Andreoli et al. 96] <author> Andreoli, J., Borghoff, U., and Pareschi, R. </author> <title> The Constraint-Based Knowledge Broker Mode: Semantics, Implementation and Analysis. </title> <journal> J. Symbolic Computation, </journal> <volume> 21(4) </volume> <pages> 635-67, </pages> <year> 1996. </year>
Reference: [Angluin & Laird 88] <author> Angluin, D. and Laird, P. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-70, </pages> <year> 1988. </year>
Reference-contexts: Nevertheless, might be close to zero, if the information resource under consideration is structured so that incorrect labels can be discovered by the Generalize hlrt algorithm. 191 This approach to handling noise appears to be novel. The PAC literature (e.g. <ref> [Angluin & Laird 88, Kearns 93, Decatur & Gennaro 95] </ref>) covers a wide variety of noise models and strategies for coping with this noise.
Reference: [Angluin & Smith 83] <author> Angluin, D. and Smith, C. </author> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-69, </pages> <year> 1983. </year>
Reference: [Angluin 82] <author> Angluin, D. </author> <title> Inference of reversible languages. </title> <journal> J. ACM, </journal> <volume> 29(3) </volume> <pages> 741-65, </pages> <year> 1982. </year>
Reference-contexts: Therefore, we require that the finite-state automaton to which the learned grammar corresponds have a specific state topology. Efficient induction algorithms have been developed for several classes of regular grammars (e.g., reversible <ref> [Angluin 82] </ref> and strictly regular [Tanida & Yokomori 92] grammars). The difficulty is simply that we do not know of any such results that deliver the particular state topology we require.
Reference: [Angluin 87] <author> Angluin, D. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference: [Angluin 92] <author> Angluin, D. </author> <title> Computational learning theory: survey and selected bibliography. </title> <booktitle> In Proc. 24th ACM Symp. Theory Comp., </booktitle> <pages> pages 351-69, </pages> <year> 1992. </year>
Reference: [ANSI 92] <author> ANSI. </author> <title> Database Language SQL, 1992. Standard X3.135-1992. </title>
Reference: [Arens et al. 96] <author> Arens, Y., Knoblock, C., Chee, C., and Hsu, C. SIMS: </author> <title> Single interface to multiple sources. </title> <type> TR RL-TR-96-118, </type> <institution> USC Rome Labs, </institution> <year> 1996. </year> <month> 237 </month>
Reference: [ARPA 95] <editor> ARPA. </editor> <booktitle> Proc. 6th Message Understanding Conf. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Central to these techniques is a library of recognizers, domain-specific procedures for identifying instances of particular attributes. Recognizers for specific attributes have received much attention in the text processing communities. For example, the Sixth Message Understanding Conference's "Named Entity" task <ref> [ARPA 95, ftp.muc.saic.com/pub/MUC/MUC6-guidelines/ne-task-def.v2.1.ps.Z] </ref> involves identifying particular kinds of information such as people and company names, dates, locations, and so forth. Certain highly valuable attributes such as company and people's names have received substantial attention [Rau 91, Borgman & Siegfried 92, Paik et al. 93, Hayes 94].
Reference: [Ashish & Knoblock 97a] <author> Ashish, N. and Knoblock, C. </author> <title> Semi-automatic wrapper generation for Internet information sources. </title> <booktitle> In Proc. Cooperative Information Systems, </booktitle> <year> 1997. </year>
Reference: [Ashish & Knoblock 97b] <author> Ashish, N. and Knoblock, C. </author> <title> Wrapper Generation for Semi-structured Information Sources. </title> <booktitle> In Proc. ACM SIGMOD Workshop on Management of Semi-structured Data, </booktitle> <year> 1997. </year>
Reference: [Bartlett & Williamson 91] <author> Bartlett, P. and Williamson, R. </author> <title> Investigating the distributional assumptions of the PAC learning model. </title> <booktitle> In Proc. 4th Workshop Computational Learning Theory, </booktitle> <pages> pages 24-32, </pages> <year> 1991. </year>
Reference-contexts: Note that the PAC model makes worst-case assumptions about the learning task. Specifically, it assumes that the distribution D 202 over examples is arbitrary. A standard technique for tightening a PAC model is to assume that D has certain properties <ref> [Benedek & Itai 88, Bartlett & Williamson 91] </ref>. [Schuurmans & Greiner 95] suggests another strategy: by replacing the "batch" model on inductive learning with a "sequential" model in which the PAC-theoretic analysis is repeated as each example is observed, many fewer examples are predicted.
Reference: [Benedek & Itai 88] <author> Benedek, G. and Itai, A. </author> <title> Learnability by fixed distributions. </title> <booktitle> In Proc. 1st Workshop Computational Learning Theory, </booktitle> <pages> pages 80-90, </pages> <year> 1988. </year>
Reference-contexts: Note that the PAC model makes worst-case assumptions about the learning task. Specifically, it assumes that the distribution D 202 over examples is arbitrary. A standard technique for tightening a PAC model is to assume that D has certain properties <ref> [Benedek & Itai 88, Bartlett & Williamson 91] </ref>. [Schuurmans & Greiner 95] suggests another strategy: by replacing the "batch" model on inductive learning with a "sequential" model in which the PAC-theoretic analysis is repeated as each example is observed, many fewer examples are predicted.
Reference: [Biermann & Feldman 72] <author> Biermann, A. and Feldman, J. </author> <title> On the synthesis of finite state machines from samples of their behavior. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-21:592-7, </volume> <year> 1972. </year>
Reference: [Blumer et al. 89] <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-65, </pages> <year> 1989. </year>
Reference-contexts: Therefore, the VC dimension of the real interval hypothesis space is exactly two. The VC dimension is useful because PAC bounds for hypothesis classes with finite VC dimension have been developed <ref> [Haussler 88, Blumer et al. 89] </ref>. 190 Analyzing the VC dimension of the hlrt wrapper class is somewhat complicated. We can easily analyze the VC dimension of the r k and ` k (k &gt; 1) delimiters in isolation. <p> If this conjecture holds, then the bounds reported in <ref> [Haussler 88, Blumer et al. 89] </ref> are inapplicable. In summary, we speculate that the PAC bounds we report for hlrt are tighter than obtainable using VC-theoretic analysis. Label noise.
Reference: [Borgman & Siegfried 92] <author> Borgman, C. and Siegfried, S. </author> <title> Getty's Synoname and its cousin: A survey of applications of personal name-matching algorithms. </title> <journal> J. Amer. Soc. of Information Science, </journal> <volume> 43 </volume> <pages> 459-76, </pages> <year> 1992. </year>
Reference: [Bowman et al. 94] <author> Bowman, M., Danzig, P., Manber, U., and Schwartz, F. </author> <title> Scalable Internet discovery: Research problems and approaches. </title> <journal> C. ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <year> 1994. </year>
Reference-contexts: However, automatically constructing such wrappers is a challenging direction for future work. A second important long-term research direction involves closing information integration loop. We have examined information extraction in isola 200 tion, but our techniques must be integrated with work on resource discovery <ref> [Bowman et al. 94, Zaiane & Jiawei 95] </ref>, learning to query information resources [Cohen & Singer 96, Doorenbos et al. 97], and learning semantic models of information resources [Perkowitz & Etzioni 95, Tejada et al. 96]. Third, our work has focused on resources whose content is formatted by html tags.
Reference: [Bradshaw 97] <author> Bradshaw, J., </author> <title> editor. Intelligent Agents. </title> <publisher> MIT Press, </publisher> <year> 1997. </year> <month> 238 </month>
Reference: [Brodie & Stonebraker 95] <author> Brodie, M. and Stonebraker, M. </author> <title> Migrating Legacy Systems: Gateways, Interfaces, & the Incremental Approach. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1995. </year>
Reference: [Carey et al. 95] <author> Carey, M., Haas, L., Schwarz, P., Arya, M., Cody, W., Fagin, R., Flickner, M., Luniewski, A., Niblack, W., Petkovic, D., Thomas, J., Williams, J., and Wimmers, E. </author> <title> Towards heterogeneous multimedia information systems: The Garlic approach. </title> <booktitle> In Proc. 5th Int. Workshop of Research Issues in Data Engineering: Distributed Object Management, </booktitle> <pages> pages 124-31, </pages> <year> 1995. </year>
Reference: [Chawathe et al. 94] <author> Chawathe, S., Garcia-Molina, H., Hammer, J., Ireland, K., Pa-pakonstantinou, Y., Ullman, J., and Widom, J. </author> <title> The TSIMMIS project: Integration of heterogeneous information sources. </title> <booktitle> In Proc. 10th Meeting of the Information Processing Soc. of Japan, </booktitle> <pages> pages 7-18, </pages> <year> 1994. </year>
Reference: [Chidlovskii et al. 97] <author> Chidlovskii, B., Borghoff, U., and Chevalier, P. </author> <title> Towards Sophisticated Wrapping of Web-based Information Repositories. </title> <booktitle> In Proc. Conf. Computer-Assisted Information Retrieval, </booktitle> <pages> pages 123-35, </pages> <year> 1997. </year>
Reference: [Cohen & Singer 96] <author> Cohen, W. and Singer, W. </author> <title> Learning to Query the Web. </title> <booktitle> In Proc. Workshop Internet-based Information Systems, 13th Nat. Conf. Artificial Intelligence, </booktitle> <pages> pages 16-25, </pages> <year> 1996. </year>
Reference-contexts: A second important long-term research direction involves closing information integration loop. We have examined information extraction in isola 200 tion, but our techniques must be integrated with work on resource discovery [Bowman et al. 94, Zaiane & Jiawei 95], learning to query information resources <ref> [Cohen & Singer 96, Doorenbos et al. 97] </ref>, and learning semantic models of information resources [Perkowitz & Etzioni 95, Tejada et al. 96]. Third, our work has focused on resources whose content is formatted by html tags.
Reference: [Collet et al. 91] <author> Collet, C., Huhns, M., and Shen, W. </author> <title> Resource integration using a large knowledge base in CARNOT. </title> <booktitle> IEEE Computer, </booktitle> <year> 1991. </year>
Reference: [Cowie & Lehnert 96] <author> Cowie, J. and Lehnert, W. </author> <title> Information extraction. </title> <journal> C. ACM, </journal> <volume> 39(1) </volume> <pages> 80-91, </pages> <year> 1996. </year>
Reference-contexts: This field has a rich literature; see <ref> [Hobbs 92, Cowie & Lehnert 96] </ref> for surveys. Although similar in spirit|traditional IE is the task of identifying literal fragments of an input text that instantiate some relation or concept|our use of the phrase "information extraction" differs in five ways.
Reference: [Decatur & Gennaro 95] <author> Decatur, S. and Gennaro, R. </author> <title> On Learning from Noisy and Incomplete Examples. </title> <booktitle> In Proc. 8th Annual ACM Conf. Computational Learning Theory, </booktitle> <pages> pages 353-60, </pages> <year> 1995. </year> <month> 239 </month>
Reference-contexts: Nevertheless, might be close to zero, if the information resource under consideration is structured so that incorrect labels can be discovered by the Generalize hlrt algorithm. 191 This approach to handling noise appears to be novel. The PAC literature (e.g. <ref> [Angluin & Laird 88, Kearns 93, Decatur & Gennaro 95] </ref>) covers a wide variety of noise models and strategies for coping with this noise.
Reference: [Decker et al. 97] <author> Decker, K., Sycara, K., and Williamson, M. </author> <title> Middle-agents for the internet. </title> <booktitle> In Proc. 15th Int. Joint Conf. AI, </booktitle> <pages> pages 578-83, </pages> <year> 1997. </year>
Reference: [Dietterich & Michalski 83] <author> Dietterich, T. and Michalski, R. </author> <title> A comparative review of selected methods for learning from examples. </title> <editor> In [Michalski et al. </editor> <volume> 83], chapter 3, </volume> <pages> pages 41-82. </pages>
Reference: [Doorenbos 97] <author> Doorenbos, R., </author> <month> October </month> <year> 1997. </year> <type> Personal communication. </type>
Reference-contexts: As a concrete example, the information from a query to the country/code resource is contained in a single page, and there is just one table of data on this page (namely, the country/code table). But industrial-strength wrapper applications require more sophisticated functionality <ref> [Doorenbos 97] </ref>, such as * extracting and merging data from multiple documents|e.g., clicking on a table of hyper-linked product names in order to get their prices; and * extracting information from a single page that contains more than one collection of data|e.g., a page might contain one table listing products and
Reference: [Doorenbos et al. 97] <author> Doorenbos, R., Etzioni, O., and Weld, D. </author> <title> A scalable comparison-shopping agent for the World-Wide Web. </title> <booktitle> In Proc. Autonomous Agents, </booktitle> <pages> pages 39-48, </pages> <year> 1997. </year>
Reference-contexts: A second important long-term research direction involves closing information integration loop. We have examined information extraction in isola 200 tion, but our techniques must be integrated with work on resource discovery [Bowman et al. 94, Zaiane & Jiawei 95], learning to query information resources <ref> [Cohen & Singer 96, Doorenbos et al. 97] </ref>, and learning semantic models of information resources [Perkowitz & Etzioni 95, Tejada et al. 96]. Third, our work has focused on resources whose content is formatted by html tags.
Reference: [Douglas & Hurst 96] <author> Douglas, S. and Hurst, M. </author> <title> Layout and language: Lists and tables in tehcnical documents. </title> <booktitle> In Proc. SIGPARSE, </booktitle> <pages> pages 19-24, </pages> <year> 1996. </year>
Reference-contexts: There is a wide variety of research concerned with recovering a document's structure. Of particular relevance to our work is the recovery of structured information such as tables of tables-of-contents. <ref> [Douglas et al. 95, Douglas & Hurst 96] </ref> discuss techniques for identifying the tabular structured in plain text documents. [Green & Krishnamoorthy 95] solve a similar problem, except that their system takes as input scanned images of documents.
Reference: [Douglas et al. 95] <author> Douglas, S., Hurst, M., and Quinn, D. </author> <title> Using Natural Language Processing for Identifying and Intepreting Tables in Plain Text. </title> <booktitle> In Proc. 4th Symp. Document Analysis and Information Retrieval, </booktitle> <pages> pages 535-46, </pages> <year> 1995. </year>
Reference-contexts: There is a wide variety of research concerned with recovering a document's structure. Of particular relevance to our work is the recovery of structured information such as tables of tables-of-contents. <ref> [Douglas et al. 95, Douglas & Hurst 96] </ref> discuss techniques for identifying the tabular structured in plain text documents. [Green & Krishnamoorthy 95] solve a similar problem, except that their system takes as input scanned images of documents.
Reference: [Draper 97] <author> Draper, D., </author> <month> October </month> <year> 1997. </year> <type> Personal communication. </type>
Reference: [Etzioni & Weld 94] <author> Etzioni, O. and Weld, D. </author> <title> A softbot-based interface to the Internet. </title> <journal> C. ACM, </journal> <volume> 37(7) </volume> <pages> 72-6, </pages> <year> 1994. </year>
Reference: [Etzioni 93] <author> Etzioni, O. </author> <title> Intelligence without robots: A reply to Brooks. </title> <journal> AI Magazine, </journal> <volume> 14(4) </volume> <pages> 7-13, </pages> <year> 1993. </year>
Reference: [Etzioni 96a] <author> Etzioni, O. </author> <title> Moving up the information food chain: softbots as information carnivores. </title> <booktitle> In Proc. 13th Nat. Conf. AI, </booktitle> <year> 1996. </year>
Reference: [Etzioni 96b] <author> Etzioni, O. </author> <title> The World Wide Web: quagmire or gold mine? C. </title> <journal> ACM, </journal> <volume> 37(7) </volume> <pages> 65-8, </pages> <year> 1996. </year> <month> 240 </month>
Reference: [Etzioni et al. 93] <author> Etzioni, O., Lesh, N., and Segal, R. </author> <title> Building softbots for UNIX (preliminary report). </title> <type> Technical Report 93-09-01, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference: [Etzioni et al. 94] <author> Etzioni, O., Maes, P., Mitchell, T., and Shoham, Y., </author> <title> editors. </title> <booktitle> Working Notes of the AAAI Spring Symposium on Software Agents, </booktitle> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference: [Finin et al. 94] <author> Finin, T., Fritzson, R., McKay, D., and McEntire, R. </author> <title> KQML: A language and protocol for knowledge and information exchange. In Knowledge Building and Knowledge Sharing. </title> <publisher> Ohmsha and IOS Press, </publisher> <year> 1994. </year>
Reference: [Florescu et al. 95] <author> Florescu, D., Rashid, L., and Valduriez, P. </author> <title> Using heterogeneous equivalences for query rewriting in multi-database systems. </title> <booktitle> In Proc. Cooperative Information Systems, </booktitle> <year> 1995. </year>
Reference: [Freitag 96] <author> Freitag, D. </author> <title> Machine Learning for Information Extraction from Online Documents: A Preliminary Experiment. </title> <type> Unpublished manuscript, </type> <year> 1996. </year> <note> Available at www.cs.cmu.edu/afs/cs.cmu.edu/user/dayne/www/prelim.ps. </note>
Reference-contexts: Similar ideas are explored in <ref> [Freitag 96, Freitag 97] </ref>. Using as a test domain relatively unstructured departmental talk announcements, Freitag demonstrates that different machine learning techniques can be combined to improve the precision with which various information extraction tasks can be performed. These three systems|autoslog, crystal/webfoot, and Freitag's work| suggest promising directions for future research. <p> The wil system [Goan et al. 96] uses novel grammar induction techniques to learn regular expressions from examples of the attribute's values; they demonstrate that their techniques are effective for learning attributes such as telephone numbers and US Library of Congress Call Numbers. <ref> [Freitag 96] </ref> presents similar results for the domain of academic talk announcements. Finally, the "field matching" problem [Monge & Elkan 96] is relevant to building recognizers. Field matching involves determining whether two character strings, such as "Dept. Comput.
Reference: [Freitag 97] <author> Freitag, D. </author> <title> Using grammatical inference to improve precision in information extraction. In Working Papers of the Workshop on Automata Induction, Grammatical Inference, and Language Acquisition, </title> <booktitle> 14th Int. Conf. Machine Learning, </booktitle> <year> 1997. </year> <note> Available at http://www.cs.cmu.edu/~pdupont/- ml97p/ml97 GI wkshp.tar. </note>
Reference-contexts: Similar ideas are explored in <ref> [Freitag 96, Freitag 97] </ref>. Using as a test domain relatively unstructured departmental talk announcements, Freitag demonstrates that different machine learning techniques can be combined to improve the precision with which various information extraction tasks can be performed. These three systems|autoslog, crystal/webfoot, and Freitag's work| suggest promising directions for future research.
Reference: [Friedman & Weld 97] <author> Friedman, M. and Weld, D. </author> <title> Efficiently executing information-gathering plans. </title> <booktitle> In Proc. 15th Int. Joint Conf. AI, </booktitle> <pages> pages 785-91, </pages> <year> 1997. </year>
Reference: [Goan et al. 96] <author> Goan, T., Benson, N., and Etzioni, O. </author> <title> A grammar inference algorithm for the world wide web. </title> <booktitle> In Proc. AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <year> 1996. </year>
Reference-contexts: Our recogniz-ers are also similar to Apple Computer's Data Detectors [applescript.apple.com/data detectors]. Others have taken machine learning approaches to constructing recognizers for 186 particular kinds of attributes. The wil system <ref> [Goan et al. 96] </ref> uses novel grammar induction techniques to learn regular expressions from examples of the attribute's values; they demonstrate that their techniques are effective for learning attributes such as telephone numbers and US Library of Congress Call Numbers. [Freitag 96] presents similar results for the domain of academic talk
Reference: [Gold 78] <author> Gold, E. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37(3) </volume> <pages> 302-320, </pages> <year> 1978. </year> <month> 241 </month>
Reference: [Green & Krishnamoorthy 95] <author> Green, E. and Krishnamoorthy, M. </author> <title> Model-Based Analysis of Printed Tables. </title> <booktitle> In Proc. 3rd Int. Conf. Document Analysis and Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: There is a wide variety of research concerned with recovering a document's structure. Of particular relevance to our work is the recovery of structured information such as tables of tables-of-contents. [Douglas et al. 95, Douglas & Hurst 96] discuss techniques for identifying the tabular structured in plain text documents. <ref> [Green & Krishnamoorthy 95] </ref> solve a similar problem, except that their system takes as input scanned images of documents. More ambitiously, [Rus & Subramanian 97] provide a theoretical characterization of information capture and access, a novel approach to the development of systems 187 that integrate heterogeneous information sources.
Reference: [Gupta 89] <author> Gupta, A., </author> <title> editor. Integration of Information Systems: Bridging Heterogeneous Databases. </title> <publisher> IEEE Press, </publisher> <year> 1989. </year>
Reference: [Haussler 88] <author> Haussler, D. </author> <title> Quantifying inductive bias. </title> <journal> J. Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: Therefore, the VC dimension of the real interval hypothesis space is exactly two. The VC dimension is useful because PAC bounds for hypothesis classes with finite VC dimension have been developed <ref> [Haussler 88, Blumer et al. 89] </ref>. 190 Analyzing the VC dimension of the hlrt wrapper class is somewhat complicated. We can easily analyze the VC dimension of the r k and ` k (k &gt; 1) delimiters in isolation. <p> If this conjecture holds, then the bounds reported in <ref> [Haussler 88, Blumer et al. 89] </ref> are inapplicable. In summary, we speculate that the PAC bounds we report for hlrt are tighter than obtainable using VC-theoretic analysis. Label noise.
Reference: [Hayes 94] <author> Hayes, P. NameFinder: </author> <title> Softwre that finds names in text. </title> <booktitle> In Proc. Conf. Computer-Assisted Information Retrieval, </booktitle> <pages> pages 762-74, </pages> <year> 1994. </year>
Reference: [Hobbs 92] <author> Hobbs, J. </author> <title> The generic information extraction system. </title> <booktitle> In Proc. 4th Message Understanding Conf., </booktitle> <year> 1992. </year>
Reference-contexts: This field has a rich literature; see <ref> [Hobbs 92, Cowie & Lehnert 96] </ref> for surveys. Although similar in spirit|traditional IE is the task of identifying literal fragments of an input text that instantiate some relation or concept|our use of the phrase "information extraction" differs in five ways.
Reference: [Kearns & Vazirani 94] <author> Kearns, M. and Vazirani, U. </author> <title> An introduction to computational learning theory. </title> <publisher> MIT, </publisher> <year> 1994. </year>
Reference-contexts: However, once these constructions are in place, we make use of well-known proof techniques. Our proof of Lemma B.1's parts (1-2) is a simplified version of the well-known proof that axis-aligned rectangles are PAC-learnable <ref> [Kearns & Vazirani 94, pp 1-6] </ref>, and our proof of part Lemma B.1 part (3) is so basic as to be published in AI textbooks (e.g., [Russell & Norvig 95, pp 553-5]).
Reference: [Kearns 93] <author> Kearns, M. </author> <title> Efficient Noise-Tolerant Learning from Statistical Queries. </title> <booktitle> In Proc. 25th Annual ACM Symp. Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: Nevertheless, might be close to zero, if the information resource under consideration is structured so that incorrect labels can be discovered by the Generalize hlrt algorithm. 191 This approach to handling noise appears to be novel. The PAC literature (e.g. <ref> [Angluin & Laird 88, Kearns 93, Decatur & Gennaro 95] </ref>) covers a wide variety of noise models and strategies for coping with this noise.
Reference: [Kirk et al. 95] <author> Kirk, T., Levy, A., Sagiv, Y., and Srivastava, D. </author> <title> The Information Manifold. </title> <booktitle> In AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <pages> pages 85-91, </pages> <year> 1995. </year>
Reference: [Knuth et al. 77] <author> Knuth, D., Morris, J., and Pratt, V. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM J. Computing, </journal> <volume> 6(2) </volume> <pages> 323-50, </pages> <year> 1977. </year>
Reference: [Krulwich 96] <author> Krulwich, B. </author> <title> The BargainFinder agent: Comparison price shopping on the Internet. </title> <editor> In Williams, J., editor, Bots and Other Internet Beasties, </editor> <booktitle> chapter 13. </booktitle> <address> SAMS.NET, </address> <year> 1996. </year> <month> 242 </month>
Reference: [Kwok & Weld 96] <author> Kwok, C. and Weld, D. </author> <title> Planning to gather information. </title> <booktitle> In Proc. 13th Nat. Conf. AI, </booktitle> <year> 1996. </year>
Reference: [Levy et al. 96] <author> Levy, A., Rajaraman, A., and Ordille, J. </author> <title> Query-answering algorithms for information agents. </title> <booktitle> In Proc. 13th Nat. Conf. AI, </booktitle> <year> 1996. </year>
Reference: [Luke et al. 97] <author> Luke, S., Spector, L., Rager, D., and Hendler, J. </author> <title> Ontology-based web agents. </title> <booktitle> In Proc. First Int. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference: [Martin & Biggs 92] <author> Martin, A. and Biggs, N. </author> <title> Computational learning theory: An introduction. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference: [Michalski 83] <author> Michalski, R. </author> <title> A theory and methodology of inductive learning. </title> <editor> In [Michalski et al. </editor> <volume> 83], chapter 4, </volume> <pages> pages 83-135. </pages>
Reference: [Michalski et al. 83] <editor> Michalski, R., Carbonell, J., and Mitchell, T., editors. </editor> <booktitle> Machine Learning | An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1983. </year>
Reference: [Mitchell 80] <author> Mitchell, T. </author> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Dept. of Computer Science, Rutgers Univ., </institution> <year> 1980. </year>
Reference: [Mitchell 97] <author> Mitchell, T. </author> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <year> 1997. </year>
Reference: [Monge & Elkan 96] <author> Monge, A. and Elkan, C. </author> <title> The field matching problem: </title> <booktitle> Algorithms and applications. In Proc. 2nd Int. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1996. </year>
Reference-contexts: Finally, the "field matching" problem <ref> [Monge & Elkan 96] </ref> is relevant to building recognizers. Field matching involves determining whether two character strings, such as "Dept. Comput. Sci. & Eng." and "Department of Computer Science and Engineering ", do in fact designate the same entity.
Reference: [Paik et al. 93] <author> Paik, W., Liddy, E., Yu, E., and McKenna, M. </author> <title> Categorizing and standardizing proper nouns for efficient retrieval. </title> <booktitle> In Proc. Assoc. for Computational Linguistics Workshop on the Aquisition of Lexical Knowledge from Text, </booktitle> <year> 1993. </year>
Reference: [Papakonstantinou et al. 95] <author> Papakonstantinou, Y., Garcia-Monlina, H., and Widom, J. </author> <title> Object exchange across heterogeneous information sources. </title> <booktitle> In Proc. 11th Int. Conf. Data Engineering, </booktitle> <pages> pages 251-60, </pages> <year> 1995. </year> <month> 243 </month>
Reference: [Perkowitz & Etzioni 95] <author> Perkowitz, M. and Etzioni, O. </author> <title> Category translation: Learning to understand information on the Internet. </title> <booktitle> In Proc. 14th Int. Joint Conf. AI, </booktitle> <pages> pages 930-6, </pages> <year> 1995. </year>
Reference-contexts: For example, they have developed an agent that scans various sources of stock price listings to generate graphs of performance over time. Unfortunately, this system relies on hand-coded heuristics to help it determine the semantics of the extracted information. Integrating these techniques with ila <ref> [Perkowitz & Etzioni 95, Perkowitz et al. 97] </ref> would be an interesting direction for future work. 8.4 Formal issues So far, we have described the related projects at a relatively shallow level of detail. <p> We have examined information extraction in isola 200 tion, but our techniques must be integrated with work on resource discovery [Bowman et al. 94, Zaiane & Jiawei 95], learning to query information resources [Cohen & Singer 96, Doorenbos et al. 97], and learning semantic models of information resources <ref> [Perkowitz & Etzioni 95, Tejada et al. 96] </ref>. Third, our work has focused on resources whose content is formatted by html tags. Let us emphasize that our techniques do not depend on html or any other particular formatting convention.
Reference: [Perkowitz et al. 97] <author> Perkowitz, M., Doorenbos, R., Etzioni, O., and Weld, D. </author> <title> Learning to understand information on the Internet: An example-based approach. </title> <journal> J. Intelligent Information Systems, </journal> <volume> 8(2) </volume> <pages> 133-153, </pages> <year> 1997. </year>
Reference-contexts: For example, they have developed an agent that scans various sources of stock price listings to generate graphs of performance over time. Unfortunately, this system relies on hand-coded heuristics to help it determine the semantics of the extracted information. Integrating these techniques with ila <ref> [Perkowitz & Etzioni 95, Perkowitz et al. 97] </ref> would be an interesting direction for future work. 8.4 Formal issues So far, we have described the related projects at a relatively shallow level of detail.
Reference: [Rau 91] <author> Rau, L. </author> <title> Extracting company names from text. </title> <booktitle> In Proc. 9th Nat. Conf. AI, </booktitle> <year> 1991. </year>
Reference: [Riloff 93] <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proc. 11th Nat. Conf. AI, </booktitle> <pages> pages 811-6, </pages> <year> 1993. </year>
Reference-contexts: Finally, given our goal of eliminating the engineering bottleneck caused by constructing wrappers by hand, we are interested in automatic learning techniques. In contrast, many IE systems are hand-crafted. The two notable exceptions are au-toslog and crystal. autoslog <ref> [Riloff 93] </ref> learns information extraction rules. The system uses heuristics to identify specific words that trigger extraction. <p> Ideally, inductive learning techniques can be used to discover such regularities automatically. This thesis demonstrates that such an approach is feasible for the kinds of semi-structured documents found on the Internet. At the other end of the spectrum, autoslog <ref> [Riloff 93] </ref> and crystal/webfoot [Soderland et al. 95, Soderland 97b, Soderland 97c] have demonstrated the feasibility of this approach for natural language text.
Reference: [Roth & Schwartz 97] <author> Roth, M. and Schwartz, P. </author> <title> Don't scrap it, wrap it! A wrapper architecture for legacy data sources. </title> <booktitle> In Proc. 22nd VLDB Conf., </booktitle> <pages> pages 266-75, </pages> <year> 1997. </year>
Reference: [Rus & Subramanian 97] <author> Rus, D. and Subramanian, D. </author> <title> Customizing information capture and access. </title> <journal> ACM Trans. Information Systems, </journal> <volume> 15(1) </volume> <pages> 67-101, </pages> <year> 1997. </year>
Reference-contexts: More ambitiously, <ref> [Rus & Subramanian 97] </ref> provide a theoretical characterization of information capture and access, a novel approach to the development of systems 187 that integrate heterogeneous information sources. The idea is to formalize the notion of a document segmenter , which identifies possibly-relevant fragments of the document.
Reference: [Russell & Norvig 95] <author> Russell, S. and Norvig, P. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Our proof of Lemma B.1's parts (1-2) is a simplified version of the well-known proof that axis-aligned rectangles are PAC-learnable [Kearns & Vazirani 94, pp 1-6], and our proof of part Lemma B.1 part (3) is so basic as to be published in AI textbooks (e.g., <ref> [Russell & Norvig 95, pp 553-5] </ref>). However, there are two areas of related work which must be addressed: a comparison of our results with the use of the Vapnik-Chervonenkis (VC) dimension, and our model of label noise. The VC dimension. <p> PAC bounds for finite hypothesis spaces are well known in the literature (e.g., <ref> [Russell & Norvig 95, pp 553-5] </ref>).
Reference: [Schuurmans & Greiner 95] <author> Schuurmans, D. and Greiner, R. </author> <title> Practical PAC Learning. </title> <booktitle> In Proc. 14th Int. Joint Conf. AI, </booktitle> <pages> pages 1169-75, </pages> <year> 1995. </year>
Reference-contexts: Note that the PAC model makes worst-case assumptions about the learning task. Specifically, it assumes that the distribution D 202 over examples is arbitrary. A standard technique for tightening a PAC model is to assume that D has certain properties [Benedek & Itai 88, Bartlett & Williamson 91]. <ref> [Schuurmans & Greiner 95] </ref> suggests another strategy: by replacing the "batch" model on inductive learning with a "sequential" model in which the PAC-theoretic analysis is repeated as each example is observed, many fewer examples are predicted.
Reference: [Selberg & Etzioni 95] <author> Selberg, E. and Etzioni, O. </author> <title> Multi-service search and comparison using the metacrawler. </title> <booktitle> In Proc. 4th World Wide Web Conf., </booktitle> <pages> pages 195-208, </pages> <address> Boston, MA USA, </address> <year> 1995. </year>
Reference: [Selberg & Etzioni 97] <author> Selberg, E. and Etzioni, O. </author> <title> The metacrawler architecture for resource aggregation on the web. </title> <journal> IEEE Expert, </journal> <volume> 12(1) </volume> <pages> 8-14, </pages> <month> January </month> <year> 1997. </year> <month> 244 </month>
Reference: [Shakes et al. 97] <author> Shakes, J., Langheinrich, M., and Etzioni, O. </author> <title> Dynamic reference sifting: a case study in the homepage domain. </title> <booktitle> In Proc. 6th World Wide Web Conf., </booktitle> <year> 1997. </year> <note> See http://www.cs.washington.edu/research/ahoy. </note>
Reference: [Shklar et al. 94] <author> Shklar, L., Thatte, S., Marcus, H., and Sheth, A. </author> <title> The InfoHarness Information Integration Platform. </title> <booktitle> In Proc. 2nd Int. WWW Conf., </booktitle> <year> 1994. </year>
Reference: [Shklar et al. 95] <author> Shklar, L., Shah, K., and Basu, C. </author> <title> Putting Legacy Data on the Web: A Repository Definition Language. </title> <booktitle> In Proc. 3rd Int. WWW Conf., </booktitle> <year> 1995. </year>
Reference: [Smeaton & Crimmins 97] <author> Smeaton, A. and Crimmins, F. </author> <title> Relevance Feedback and Query Expansion for Searching the Web: A Model for Searching a Digital Library. </title> <booktitle> In Proc. 1st European Conf. Digital Libraries, </booktitle> <pages> pages 99-112, </pages> <year> 1997. </year>
Reference: [Soderland 97a] <author> Soderland, S., </author> <month> October </month> <year> 1997. </year> <type> Personal communication. </type>
Reference-contexts: At the other end of the spectrum, autoslog [Riloff 93] and crystal/webfoot [Soderland et al. 95, Soderland 97b, Soderland 97c] have demonstrated the feasibility of this approach for natural language text. More recently, Soderland has developed whisk <ref> [Soderland 97a] </ref>, which learns information extraction rules in highly structured but non-grammatical domains such as apartment listings. whisk's rules are essentially regular expressions, specifying specific delimiters, quite similar to our delimiter-based wrappers.
Reference: [Soderland 97b] <author> Soderland, S. </author> <title> Learning Text Analysis Rules for Domain-Specific Natural Language Processing. </title> <type> PhD dissertation, </type> <institution> University of Massachusetts, </institution> <year> 1997. </year>
Reference-contexts: Like autoslog, crystal <ref> [Soderland et al. 95, Soderland 97b] </ref> and its descendant webfoot [Soderland 97c] learn information extraction rules. crystal takes as input a set of labeled example documents and a set of features describing these documents. <p> Ideally, inductive learning techniques can be used to discover such regularities automatically. This thesis demonstrates that such an approach is feasible for the kinds of semi-structured documents found on the Internet. At the other end of the spectrum, autoslog [Riloff 93] and crystal/webfoot <ref> [Soderland et al. 95, Soderland 97b, Soderland 97c] </ref> have demonstrated the feasibility of this approach for natural language text.
Reference: [Soderland 97c] <author> Soderland, S. </author> <title> Learning to Extract Text-based Information from the World Web. </title> <booktitle> In Proc. 3rd Int. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: Like autoslog, crystal [Soderland et al. 95, Soderland 97b] and its descendant webfoot <ref> [Soderland 97c] </ref> learn information extraction rules. crystal takes as input a set of labeled example documents and a set of features describing these documents. <p> Ideally, inductive learning techniques can be used to discover such regularities automatically. This thesis demonstrates that such an approach is feasible for the kinds of semi-structured documents found on the Internet. At the other end of the spectrum, autoslog [Riloff 93] and crystal/webfoot <ref> [Soderland et al. 95, Soderland 97b, Soderland 97c] </ref> have demonstrated the feasibility of this approach for natural language text.
Reference: [Soderland et al. 95] <author> Soderland, S., Fisher, D., Aseltine, J., and Lehnert, W. </author> <title> CRYSTAL: Inducing a Conceptual Dictionary. </title> <booktitle> In Proc. 14th Int. Joint Conf. AI, </booktitle> <pages> pages 1314-21, </pages> <year> 1995. </year>
Reference-contexts: Like autoslog, crystal <ref> [Soderland et al. 95, Soderland 97b] </ref> and its descendant webfoot [Soderland 97c] learn information extraction rules. crystal takes as input a set of labeled example documents and a set of features describing these documents. <p> Ideally, inductive learning techniques can be used to discover such regularities automatically. This thesis demonstrates that such an approach is feasible for the kinds of semi-structured documents found on the Internet. At the other end of the spectrum, autoslog [Riloff 93] and crystal/webfoot <ref> [Soderland et al. 95, Soderland 97b, Soderland 97c] </ref> have demonstrated the feasibility of this approach for natural language text.
Reference: [Tanida & Yokomori 92] <author> Tanida, N. and Yokomori, T. </author> <title> Polynomial-time identification of strictly regular languages in the limit. </title> <journal> IEICE Tran. Information and Systems, </journal> <volume> E75-D(1):125-32, </volume> <year> 1992. </year> <month> 245 </month>
Reference-contexts: Therefore, we require that the finite-state automaton to which the learned grammar corresponds have a specific state topology. Efficient induction algorithms have been developed for several classes of regular grammars (e.g., reversible [Angluin 82] and strictly regular <ref> [Tanida & Yokomori 92] </ref> grammars). The difficulty is simply that we do not know of any such results that deliver the particular state topology we require.
Reference: [Tejada et al. 96] <author> Tejada, S., Knoblock, C., and Minton, S. </author> <title> Learning models for multi-source integration. </title> <booktitle> In Proc. AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <year> 1996. </year>
Reference-contexts: We have examined information extraction in isola 200 tion, but our techniques must be integrated with work on resource discovery [Bowman et al. 94, Zaiane & Jiawei 95], learning to query information resources [Cohen & Singer 96, Doorenbos et al. 97], and learning semantic models of information resources <ref> [Perkowitz & Etzioni 95, Tejada et al. 96] </ref>. Third, our work has focused on resources whose content is formatted by html tags. Let us emphasize that our techniques do not depend on html or any other particular formatting convention.
Reference: [Valiant 84] <author> Valiant, L. </author> <title> A theory of the learnable. </title> <journal> C. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference: [Vapnik & Chervonenkis 71] <author> Vapnik, V. and Chervonenkis, A. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-80, </pages> <year> 1971. </year>
Reference-contexts: The VC dimension. The VC dimension of a hypothesis class is a combinatorial measure of the inherent difficulty of learning hypotheses in the class <ref> [Vapnik & Chervonenkis 71] </ref>. The VC dimension of hypothesis class H is defined as the cardinality of the largest set I of instances for which there exists hypotheses in H that can classify I's members in all 2 jIj possible ways.
Reference: [Wooldridge & Jennings 95] <editor> Wooldridge, M. and Jennings, N., editors. </editor> <booktitle> Intelligent Agents. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>

References-found: 92

