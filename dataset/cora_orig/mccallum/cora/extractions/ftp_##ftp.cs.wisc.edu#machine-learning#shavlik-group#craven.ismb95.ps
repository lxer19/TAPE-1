URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.ismb95.ps
Refering-URL: http://www.lehigh.edu/~ob00/integrated/references-new.html
Root-URL: 
Email: craven@cs.wisc.edu  m9l@ornl.gov  hauserlj@bioax1.bio.ornl.gov  ube@ornl.gov  
Phone: 1995.  
Title: on Intelligent Systems for Molecular Biology,  Predicting Protein Folding Classes without Overly Relying on Homology  
Author: C. Rawlings, D. Clark, R. Altman, L. Hunter, T. Lengauer S. Mark W. Craven Richard J. Mural Loren J. Hauser Edward C. Uberbacher 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  Biology Division Oak Ridge National Laboratory  Computer Science Mathematics Division Oak Ridge National Laboratory  Computer Science Mathematics Division Oak Ridge National Laboratory  
Address: Menlo Park, CA,  1210 W. Dayton St. Madison WI 53706  P.O. Box 2008, Bldg. 9211 Oak Ridge TN 37831-8077  P.O. Box 2008, Bldg. 9211 Oak Ridge TN 37831-8077  P.O. Box 2008, Bldg. 6010 Oak Ridge TN 37831-6364  
Note: Appears in Proc. of the 3rd International Conference  Wodak (eds.), AAAI Press,  
Abstract: An important open problem in molecular biology is how to use computational methods to understand the structure and function of proteins given only their primary sequences. We describe and evaluate an original machine-learning approach to classifying protein sequences according to their structural folding class. Our work is novel in several respects: we use a set of protein classes that previously have not been used for classifying primary sequences, and we use a unique set of attributes to represent protein sequences to the learners. We evaluate our approach by measuring its ability to correctly classify proteins that were not in its training set. We compare our input representation to a commonly used input representation amino acid composition and show that our approach more accurately classifies proteins that have very limited homology to the sequences on which the systems are trained. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Altschul, S. F.; Gish, W.; Miller, W.; Myers, E. W.; and Lipman, D. J. </author> <year> 1990. </year> <title> Basic local alignment search tool. </title> <journal> Journal of Molecular Biology 215(3) </journal> <pages> 403-410. </pages>
Reference-contexts: Note that this set contains sequences with only very limited homology. We then use each of these proteins as a query sequence to search the Swiss-Prot database (Bairoch & Boeckman 1992) for similar sequences. We use both Blast <ref> (Altschul et al. 1990) </ref> and Fasta (Pearson & Lipman 1988) for sequence comparisons. As many as nine examples are extracted from each search and added to our data set to increase the number of examples for each fold.
Reference: <author> Bairoch, A., and Boeckman, B. </author> <year> 1992. </year> <title> The Swiss-Prot protein sequence data bank. </title> <journal> Nucleic Acids Research 20 </journal> <pages> 2019-2022. </pages>
Reference-contexts: Note that this set contains sequences with only very limited homology. We then use each of these proteins as a query sequence to search the Swiss-Prot database <ref> (Bairoch & Boeckman 1992) </ref> for similar sequences. We use both Blast (Altschul et al. 1990) and Fasta (Pearson & Lipman 1988) for sequence comparisons. As many as nine examples are extracted from each search and added to our data set to increase the number of examples for each fold.
Reference: <author> Bernstein, F.; Koeyzle, T.; Williams, G.; Meyer, J.; Brice, M.; Rodgers, J.; Kennard, O.; Shimanouchi, T.; and Tatsumi, M. </author> <year> 1977. </year> <title> The protein data bank: A computer-based archival file for macromolecular structures. </title> <journal> Journal of Molecular Biology 112 </journal> <pages> 535-542. </pages>
Reference-contexts: The method that Orengo et al. used to define their fold groups involved four primary steps. 1. A set of proteins with known folds was assembled from the Brookhaven Protein Data Bank <ref> (Bernstein et al. 1977) </ref>. 2. The proteins in this set were clustered according to sequence similarity. Using the Needleman-Wunsch Table 1: Protein class representation. The middle column lists the classes we use in our classification method.
Reference: <author> Bridle, J. </author> <year> 1989. </year> <title> Probabilistic interpretation of feed-forward classification network outputs, with relationships to pattern recognition. </title> <editor> In Fogelman-Soulie, F., and Herault, J., eds., Neurocomputing: </editor> <booktitle> Algorithms, Architectures, and Applications. </booktitle> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The neural networks that we use in our experiments are fully connected between layers, and have 3, 5, 10, 20 or no hidden units. We use the logistic activation function for hidden units, and the "softmax" activation function <ref> (Bridle 1989) </ref> for output units. The softmax 1 Generalization refers to how accurately a system classifies examples that are not in its training set. 2 In leave-one-out cross-validation, classifiers are trained on n 1 of the n available examples and then tested on the example left out.
Reference: <author> Cover, T. M., and Hart, P. E. </author> <year> 1967. </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory 13(1) </journal> <pages> 21-27. </pages>
Reference-contexts: We use several different learning algorithms to evaluate these two representations, since we do not know a priori which algorithm has the most appropriate inductive bias for each representation. We evaluate three inductive learning algorithms: C4.5 (Quinlan 1993), feed-forward neural networks (Rumelhart, Hin-ton, & Williams 1986), and k-nearest-neighbor classifiers <ref> (Cover & Hart 1967) </ref>. We evaluate the suitability of these algorithms for the protein-classification task by estimating their generalization ability. In order to estimate generalization for each learning method, we conduct leave-one-out cross-validation runs. 2 C4.5 is an algorithm for learning decision trees.
Reference: <author> Devereux, J.; Haeberli, P.; and Smithies, O. </author> <year> 1984. </year> <title> A comprehensive set of sequence analysis programs for the VAX. </title> <journal> Nucleic Acids Research 12 </journal> <pages> 387-395. </pages>
Reference-contexts: The trained network is scanned along the protein sequence, generating a prediction of ff, fi or coil for each residue. The number of ff and fi predictions are summed and then divided by the sequence length. * Isoelectric point: Using the Wisconsin Sequence Analysis Package (version 6.0) <ref> (Devereux, Haeberli, & Smithies 1984) </ref>, we calculate the isoelectric point of the given sequence. * Fourier transform of hydrophobicity function: Using hydrophobicity values for each amino acid, we convert a given sequence into a one-dimensional hydrophobicity function, H.
Reference: <author> Dickerson, R. E., and Geis, I. </author> <year> 1969. </year> <title> The structure and action of proteins. </title> <address> Menlo Park, CA: </address> <publisher> W. A. Benjamin. </publisher>
Reference-contexts: The attributes that we use are the following: * Average residue volume: Using values that represent the volume of each amino acid's side group <ref> (Dickerson & Geis 1969) </ref>, we calculate the average residue volume for a given sequence. * Charge composition: We use three attributes to represent the fraction of residues in a given sequence that have positive charge, negative charge, and neu tral charge (Lehninger, Nelson, & Cox 1993). * Polarity composition: We use
Reference: <author> Dietterich, T. G., and Bakiri, G. </author> <year> 1995. </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research 2 </journal> <pages> 263-286. </pages>
Reference-contexts: Finally, we plan to investigate the utility of using a distributed output representation during learning. In a distributed representation, each of the problem classes is represented using a bit-string in which more than one bit is "on". A carefully engineered encoding scheme can result in significantly better generalization <ref> (Dietterich & Bakiri 1995) </ref>. We have presented a novel machine-learning approach to classifying proteins into folding groups. Our method uses attributes that can be easily computed from the primary sequence of a given protein.
Reference: <author> Dubchak, I.; Holbrook, S. R.; and Kim, S.-H. </author> <year> 1993. </year> <title> Prediction of protein folding class from amino acid composition. Proteins: Structure, Function, </title> <booktitle> and Genetics 16 </booktitle> <pages> 79-91. </pages>
Reference: <author> Eisenberg, D.; Weiss, R. M.; and Terwilliger, T. C. </author> <year> 1984. </year> <title> The hydrophobic moment detects periodicity in protein hydrophobicity. </title> <booktitle> Proceedings of the National Academy of Sciences, USA 81 </booktitle> <pages> 140-144. </pages>
Reference-contexts: We calculate the modulus of the Fourier transform of this function as follows <ref> (Eisenberg, Weiss, & Terwilliger 1984) </ref>: (ffi) = &lt; " N X H n sin (ffin) # 2 " N X H n cos (ffin) # 2 = 1=2 where (ffi) is the value for the periodicity with frequency ffi, and n ranges over the residues in the sequence.
Reference: <author> Ferran, E. A.; Ferrara, P.; and Pflugfelder, B. </author> <year> 1993. </year> <title> Protein classification using neural networks. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> 127-135. </pages> <address> Bethesda, MD: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Fields, C.; Adams, M. D.; White, O.; and Venter, J. C. </author> <year> 1994. </year> <title> How many genes in the human genome? Nature Genetics 7(3) </title> <type> 345-346. </type>
Reference-contexts: DE-AC05-84OR21400 with Martin Marietta Energy Systems, Inc. the structure and function of a protein is to identify a homologous protein that has already been characterized. However, from current genome-sequencing efforts it appears that as many as half of the newly discovered proteins do not have corresponding, well-understood homologs <ref> (Fields et al. 1994) </ref>. The goal of our research program, therefore, is to develop protein-classification methods that are not overly reliant on sequence homology, but instead represent the essential properties of analogous proteins that have similar folds.
Reference: <author> Hinton, G. </author> <year> 1989. </year> <title> Connectionist learning procedures. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 185-234. </pages>
Reference-contexts: The networks are trained using the cross-entropy error function <ref> (Hinton 1989) </ref>, and a conjugate-gradient learning method (Kramer & Sangiovanni-Vincentelli 1989), which obviates the need for learning-rate and momentum parameters. Networks are trained until either (1) they correctly classify all of the training-set examples, (2) they converge to a minimum, or (3) 1000 search directions have been tried.
Reference: <author> Klein, P., and Delisi, C. </author> <year> 1986. </year> <title> Prediction of protein structural class from the amino acid sequence. </title> <type> Biopolymers 25 </type> <pages> 1659-1672. </pages>
Reference: <author> Kolinski, A., and Skolnick, J. </author> <year> 1992. </year> <title> Discretized model of proteins. I. Monte Carlo study of cooperativity in homopolypeptides. </title> <journal> Journal of Chemical Physics 97(12) </journal> <pages> 9412-9426. </pages>
Reference-contexts: Our experiments indicate that our approach provides a promising alternative to homology-based methods for protein classification. There is a wide variety of existing approaches for predicting structural or functional aspects of proteins given their primary (i.e., amino-acid) sequences. These approaches include tertiary structure prediction <ref> (e.g., Kolinski & Skolnick 1992) </ref>, secondary structure prediction (e.g., Rost & Sander 1993), sequence homology searching (e.g., Pearson & Lipman 1988; Altschul et al. 1990), and classification according to folding class (Dubchak, Holbrook, & Kim 1993; Ferran, Ferrara, & Pflugfelder 1993; Metfessel et al. 1993; Wu et al. 1993; Nakashima &
Reference: <author> Kramer, A. H., and Sangiovanni-Vincentelli, A. </author> <year> 1989. </year> <title> Efficient parallel learning algorithms for neural networks. </title> <editor> In Touretzky, D., ed., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 1. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 40-48. </pages>
Reference-contexts: The networks are trained using the cross-entropy error function (Hinton 1989), and a conjugate-gradient learning method <ref> (Kramer & Sangiovanni-Vincentelli 1989) </ref>, which obviates the need for learning-rate and momentum parameters. Networks are trained until either (1) they correctly classify all of the training-set examples, (2) they converge to a minimum, or (3) 1000 search directions have been tried.
Reference: <author> Le Cun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. D. </author> <year> 1989. </year> <title> Handwritten digit recognition with a backpropagation network. </title> <editor> In Touretzky, D., ed., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The neural networks that we use in our experiments are fully connected between layers, and have 3, 5, 10, 20 or no hidden units. We use the logistic activation function for hidden units, and the "softmax" activation function <ref> (Bridle 1989) </ref> for output units. The softmax 1 Generalization refers to how accurately a system classifies examples that are not in its training set. 2 In leave-one-out cross-validation, classifiers are trained on n 1 of the n available examples and then tested on the example left out. <p> In this section, we describe an experiment in which we employ a strategy that is commonly used in the domain of handwritten character recognition: classifiers "reject" (i.e., do not classify) examples for which they cannot confidently predict a class <ref> (Le Cun et al. 1989) </ref>. In our fourth experiment, we evaluate neural net works and nearest neighbor classifiers using the same training and test sets as in the previous experiment (i.e., no test-set example has a homolog in the training set).
Reference: <author> Lehninger, A. L.; Nelson, D. L.; and Cox, M. M. </author> <year> 1993. </year> <title> Principles of Biochemistry. </title> <address> New York, NY: </address> <publisher> Worth Publishers. </publisher>
Reference-contexts: represent the volume of each amino acid's side group (Dickerson & Geis 1969), we calculate the average residue volume for a given sequence. * Charge composition: We use three attributes to represent the fraction of residues in a given sequence that have positive charge, negative charge, and neu tral charge <ref> (Lehninger, Nelson, & Cox 1993) </ref>. * Polarity composition: We use three attributes to represent the fraction of residues in a given sequence that are polar, apolar, and neutral (Lehninger, Nel son, & Cox 1993). * Predicted ff-helix/fi-sheet composition: One of these attributes represents the fraction of the protein's residues that are <p> three attributes to represent the fraction of residues in a given sequence that have positive charge, negative charge, and neu tral charge (Lehninger, Nelson, & Cox 1993). * Polarity composition: We use three attributes to represent the fraction of residues in a given sequence that are polar, apolar, and neutral <ref> (Lehninger, Nel son, & Cox 1993) </ref>. * Predicted ff-helix/fi-sheet composition: One of these attributes represents the fraction of the protein's residues that are predicted to occur in ff-helices, the other represents the fraction that are predicted to occur in fi-sheets.
Reference: <author> Levitt, M., and Chothia, C. </author> <year> 1976. </year> <title> Structural patterns in globular proteins. </title> <booktitle> Nature (London) 261 </booktitle> <pages> 552-557. </pages>
Reference-contexts: that we use as our classes, as well as the number of examples in each class that we use in our experiments, and whether each class falls into the ff (primarily alpha), fi ( primarily beta), ff=fi (alternating ff and fi), or ff + fi (non-alternating ff and fi) family <ref> (Levitt & Chothia 1976) </ref>. The method that Orengo et al. used to define their fold groups involved four primary steps. 1. A set of proteins with known folds was assembled from the Brookhaven Protein Data Bank (Bernstein et al. 1977). 2.
Reference: <author> Metfessel, B. A.; Saurugger, P. N.; Connelly, D. P.; and Rich, S. S. </author> <year> 1993. </year> <title> Cross-validation of protein structural class prediction using statistical clustering and neural networks. </title> <booktitle> Protein Science 2 </booktitle> <pages> 1171-1182. </pages>
Reference: <author> Nakashima, H., and Nishikawa, K. </author> <year> 1994. </year> <title> Discrimination of intracellular and extracellular proteins using amino acid composition and residue-pair frequencies. </title> <journal> Journal of Molecular Biology 238 </journal> <pages> 54-61. </pages>
Reference: <author> Nakashima, H.; Nishikawa, K.; and Ooi, T. </author> <year> 1986. </year> <title> The folding type of a protein is relevant to its amino acid composition. </title> <journal> Journal of Biochemistry (Tokyo) 99 </journal> <pages> 153-162. </pages>
Reference: <author> Needleman, S. B., and Wunsch, C. D. </author> <year> 1970. </year> <title> A general method applicable to the search for similarities in the amino acid sequence of two proteins. </title> <journal> Journal of Molecular Biology 48 </journal> <pages> 443-453. </pages>
Reference-contexts: 14 EF Hand 5 Up/Down 7 Metal Rich 16 fi Orthogonal Barrel 5 Greek Key 24 Jelly Roll 5 Complex Sandwich 7 Trefoil 7 Disulphide Rich 11 ff=fi TIM Barrel 15 Doubly Wound 26 ff + fi Mainly Alpha 9 Sandwich 20 Beta Open Sheet 14 total examples 212 algorithm <ref> (Needleman & Wunsch 1970) </ref>, they performed pairwise comparisons on 1410 protein sequences selected in the previous step. They then used single-linkage cluster analysis to form clusters of related sequences.
Reference: <author> Orengo, C. A.; Flores, T. P.; Taylor, W. R.; and Thornton, J. M. </author> <year> 1993. </year> <title> Identification and classification of protein fold families. </title> <booktitle> Protein Engineering 6(5) </booktitle> <pages> 485-500. </pages>
Reference: <author> Pearson, W. R., and Lipman, D. J. </author> <year> 1988. </year> <title> Improved tools for biological sequence comparison. </title> <booktitle> Proceedings of the National Academy of Sciences, USA 85 </booktitle> <pages> 2444-2448. </pages>
Reference-contexts: Note that this set contains sequences with only very limited homology. We then use each of these proteins as a query sequence to search the Swiss-Prot database (Bairoch & Boeckman 1992) for similar sequences. We use both Blast (Altschul et al. 1990) and Fasta <ref> (Pearson & Lipman 1988) </ref> for sequence comparisons. As many as nine examples are extracted from each search and added to our data set to increase the number of examples for each fold.
Reference: <author> Qian, N., and Sejnowski, T. </author> <year> 1988. </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology 202 </journal> <pages> 865-884. </pages>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We use several different learning algorithms to evaluate these two representations, since we do not know a priori which algorithm has the most appropriate inductive bias for each representation. We evaluate three inductive learning algorithms: C4.5 <ref> (Quinlan 1993) </ref>, feed-forward neural networks (Rumelhart, Hin-ton, & Williams 1986), and k-nearest-neighbor classifiers (Cover & Hart 1967). We evaluate the suitability of these algorithms for the protein-classification task by estimating their generalization ability.
Reference: <author> Reczko, M., and Bohr, H. </author> <year> 1994. </year> <title> The DEF data base of sequence based protein fold class predictions. </title> <journal> Nucleic Acids Research 22(17) </journal> <pages> 3616-3619. </pages>
Reference: <author> Rost, B., and Sander, C. </author> <year> 1993. </year> <title> Prediction of pro-tein secondary structure at better than 70% accuracy. </title> <journal> Journal of Molecular Biology 232 </journal> <pages> 584-599. </pages>
Reference-contexts: There is a wide variety of existing approaches for predicting structural or functional aspects of proteins given their primary (i.e., amino-acid) sequences. These approaches include tertiary structure prediction (e.g., Kolinski & Skolnick 1992), secondary structure prediction <ref> (e.g., Rost & Sander 1993) </ref>, sequence homology searching (e.g., Pearson & Lipman 1988; Altschul et al. 1990), and classification according to folding class (Dubchak, Holbrook, & Kim 1993; Ferran, Ferrara, & Pflugfelder 1993; Metfessel et al. 1993; Wu et al. 1993; Nakashima & Nishikawa 1994; Reczko & Bohr 1994).
Reference: <author> Rumelhart, D.; Hinton, G.; and Williams, R. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D., and McClelland, J., eds., </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 318-363. </pages>
Reference-contexts: We use several different learning algorithms to evaluate these two representations, since we do not know a priori which algorithm has the most appropriate inductive bias for each representation. We evaluate three inductive learning algorithms: C4.5 (Quinlan 1993), feed-forward neural networks <ref> (Rumelhart, Hin-ton, & Williams 1986) </ref>, and k-nearest-neighbor classifiers (Cover & Hart 1967). We evaluate the suitability of these algorithms for the protein-classification task by estimating their generalization ability.
Reference: <author> Sander, C., and Schneider, R. </author> <year> 1991. </year> <title> Database of homology-derived protein structures and the structural meaning of sequence alignment. Proteins: Structure, Function and Genetics 9 </title> <type> 56-68. </type>
Reference-contexts: Two proteins were deemed related, in this case, if their sequence identity was 35%. For small proteins, this threshold was adjusted using the equation of Sander and Schneider <ref> (Sander & Schneider 1991) </ref>. Also, for proteins with 25-35% sequence identity, a significance test was used to determine if the proteins were to be considered related. 3.
Reference: <author> Wu, C.; Berry, M.; Fung, Y.-S.; and McLarty, J. </author> <year> 1993. </year> <title> Neural networks for molecular sequence classification. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> 429-437. </pages> <address> Bethesda, MD: </address> <publisher> AAAI Press. </publisher>
References-found: 32

