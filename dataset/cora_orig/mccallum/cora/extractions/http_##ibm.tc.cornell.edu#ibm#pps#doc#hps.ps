URL: http://ibm.tc.cornell.edu/ibm/pps/doc/hps.ps
Refering-URL: http://ibm.tc.cornell.edu/ibm/pps/doc/
Root-URL: http://www.tc.cornell.edu
Title: in Proceedings of the Scalable High Performance  The SP1 High-Performance Switch  
Author: Craig B. Stunkel Dennis G. Shea Don G. Grice Peter H. Hochschild Michael Tsao 
Address: P.O. Box 218 Neighborhood Road Yorktown Heights, NY 10598 Kingston, NY 12401  
Affiliation: IBM Thomas J. Watson Research Center IBM Highly Parallel Supercomputing Systems Laboratory  
Date: May 1994  
Note: Computing Conference, Knoxville, TN,  
Abstract: The IBM Scalable POWERparallel Systems 1 9076 SP1 1 connects RISC System/6000 1 processors via a communication network called the High-Performance Switch. This Switch|based upon the Vulcan parallel processor [1]|incorporates a number of unusual features to enhance reliability, diagnose faults, and simplify cabling. This paper examines the SP1 Switch architecture and implementation and overviews the Switch support software. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. B. Stunkel, D. G. Shea, B. Abali, M. M. Denneau, P. H. Hochschild, D. J. Joseph, B. J. Nathanson, M. Tsao, and P. R. Varker, </author> <booktitle> "Architecture and implementation of Vulcan," in Proc. 8th Int. Parallel Processing Symp., </booktitle> <year> 1994. </year> <note> (An extended version of this paper is also available as an IBM Technical Report from the IBM T. J. Wat-son Research Center, 1994.). </note>
Reference-contexts: The Switch network is capable of scaling its aggregate bandwidth linearly with respect to the number of nodes, and provides low latency, fault-tolerant communication. The network can leverage the latest processor technology. The Switch|based upon the Vulcan architecture and Vulcan prototype <ref> [1] </ref> developed in the Parallel Systems department at IBM Research|incorporates a number of unusual features to scale aggregate bandwidth, enhance reliability, diagnose faults, and simplify cabling. It is a bidirectional multistage network, and is a unified data and service network driven by a single oscillator. <p> The Switch is built upon two custom ASIC chips: the Vulcan switch chip, and the Vul-can MSMU (Memory & Switch Management Unit) <ref> [1] </ref>. The 8 fi 8 switch chip is the basic switching element of the network, while the MSMU functions as the interface between a switch chip and the RISC System/6000 processor.
Reference: [2] <author> W. J. Dally, </author> <title> "Virtual-channel flow control," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Each packet contains route information examined by switching elements to forward the packet correctly to its destination. The smallest unit on which flow control is performed is called a flow-control digit, or flit <ref> [2] </ref>, which is one byte in the SP1 Switch. The width of the data transmitted by an output port is also one byte. 2.1 Network topology SP1 networks are bidirectional multistage interconnection networks (MIN's). <p> For example, a global event trace from a parallel application can be correctly synthesized from the individual event traces. 2.3 Packet flow control The SP1 method of packet transfer is related to wormhole routing <ref> [2] </ref>. In wormhole routing, each flit of a packet is advanced to the appropriate output port as soon is it arrives at a switching element input port. When the head of a packet is blocked, the flits are buffered in place.
Reference: [3] <author> G. J. Lipovski and M. Malek, </author> <title> Parallel Computing: Theory and Comparisons. </title> <address> New York, NY: </address> <publisher> Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: The width of the data transmitted by an output port is also one byte. 2.1 Network topology SP1 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN <ref> [3, 4] </ref> each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth [5] linearly with the number of nodes while maintaining a fixed number of communication ports per switching element.
Reference: [4] <author> I. D. Scherson and C.-H. Chien, </author> <title> "Least common ancestor networks," </title> <booktitle> in Proc. 7th Int. Parallel Processing Symp., </booktitle> <pages> pp. 507-513, </pages> <year> 1993. </year>
Reference-contexts: The width of the data transmitted by an output port is also one byte. 2.1 Network topology SP1 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN <ref> [3, 4] </ref> each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth [5] linearly with the number of nodes while maintaining a fixed number of communication ports per switching element.
Reference: [5] <author> W. J. Dally, </author> <title> "Performance analysis of k-ary n-cube interconnection networks," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In a bidirectional MIN [3, 4] each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth <ref> [5] </ref> linearly with the number of nodes while maintaining a fixed number of communication ports per switching element. MIN's with hundreds or thousands of nodes are richly connected and require some long communication links to support large numbers of nodes.
Reference: [6] <author> L. R. Goke and G. J. Lipovski, </author> <title> "Banyan networks for partitioning multiprocessor systems," </title> <booktitle> in Proc. 1st Ann. Symp. on Computer Architecture, </booktitle> <pages> pp. 21-28, </pages> <year> 1973. </year>
Reference-contexts: Any 151 shortest path route selection algorithm is deadlock--free. Thus, we can take full advantage of the redundant paths in the Switch. SP releases supporting 128 nodes or more will scale Switch bandwidth by cascading switch assemblies analogous to an SW Banyan <ref> [6] </ref>. These larger SP1 Switch topologies are also related to practical implementations of fat-tree networks [7] derived from Leiserson's idealized fat-trees [8]. 2.2 Single global oscillator The Switch operates synchronously|all network devices are driven by an image of a single oscillator.
Reference: [7] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Dou-glas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S.-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5," </title> <booktitle> in Proc. 1992 Symp. Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Thus, we can take full advantage of the redundant paths in the Switch. SP releases supporting 128 nodes or more will scale Switch bandwidth by cascading switch assemblies analogous to an SW Banyan [6]. These larger SP1 Switch topologies are also related to practical implementations of fat-tree networks <ref> [7] </ref> derived from Leiserson's idealized fat-trees [8]. 2.2 Single global oscillator The Switch operates synchronously|all network devices are driven by an image of a single oscillator. It is not necessary to adjust the phase of each device's oscillator image.
Reference: [8] <author> C. E. Leiserson, "Fat-trees: </author> <title> Universal networks for hardware-efficient supercomputing," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-34, </volume> <pages> pp. 892-901, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: SP releases supporting 128 nodes or more will scale Switch bandwidth by cascading switch assemblies analogous to an SW Banyan [6]. These larger SP1 Switch topologies are also related to practical implementations of fat-tree networks [7] derived from Leiserson's idealized fat-trees <ref> [8] </ref>. 2.2 Single global oscillator The Switch operates synchronously|all network devices are driven by an image of a single oscillator. It is not necessary to adjust the phase of each device's oscillator image.
Reference: [9] <author> L. Lamport, </author> <title> "Time, clocks, and the ordering of events in a distributed system," </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, </volume> <pages> pp. 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Synchronous operation enables the use of chip replication for the purpose of fault detection. Synchronous operation, in conjunction with appropriate hardware mechanisms, facilitates the achievement of a closely synchronized and non-drifting global time, trivializing the well-known clock synchronization problem <ref> [9] </ref>. Each device in the Switch network accesses this time via its local copy: a loadable incre-menter called the Absolute Time Counter (ATC). During the initialization of the network, the ATC within each device is synchronized to within one cycle of one neighbor device's ATC.
Reference: [10] <author> M. M. Denneau, P. H. Hochschild, and G. Schich-man, </author> <title> "The switching network of the TF-1 parallel supercomputer," </title> <booktitle> Supercomputing Magazine, </booktitle> <pages> pp. 7-10, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: All ports are one flit (one byte) wide. In the absence of contention, packet flits incur 5 cycles of latency cutting through the chip via the crossbar path (x3.1.2). The design of this chip was originally targeted for the TF-1 machine <ref> [10] </ref>. 3.1.1 Receivers The switch chip contains eight identical receiver modules, one associated with each of the eight input ports.
Reference: [11] <author> Y. Tamir and G. L. Frazier, </author> <title> "Hardware support for high-priority traffic in VLSI communication switches," </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 402-416, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: It accepts packet chunks from the receivers, stores them, and eventually passes them to the appropriate transmitters. The Vulcan switch chip central buffering scheme is similar to the centrally-buffered, dynamically-allocated switch described by Tamir and Frazier <ref> [11] </ref>, but the Vulcan chip allocates central queue space on a chunk basis instead of a packet basis. The central queue stores packets until they can be transmitted. The storage, a 128 by 64-bit dual-port RAM, holds up to 128 eight-flit packet chunks.
Reference: [12] <author> W. Gropp, </author> <title> "Early experiences with the IBM SP1 and the high-performance switch," </title> <type> tech. rep. </type> <institution> ANL-93/41, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: This communication library maps the MSMU registers into user memory space. SP1 node to node performance has been benchmarked and compared to several commercial machines by researchers at Ar-gonne <ref> [12] </ref>. 4 Support Software The Switch network function is supported by service software known as the "Worm." This service software is coupled with the Route Table Generator, which examines the state of the Switch as determined by the Worm execution, and generates valid routes from every node to any other node.
Reference: [13] <author> C. B. Stunkel, </author> <title> "The Vulcan Worm," </title> <type> tech. rep., </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1994. </year>
Reference-contexts: This section overviews the Worm and the Route Table Generator. 4.1 The Worm The Worm package is the SP1 Switch network service software <ref> [13] </ref>. It executes on an arbitrarily chosen "service" or "primary" node and controls the network during service mode. The Worm package provides initialization, tuning, global time synchronization, fault determination, and diagnostic services to all devices within or attached to the Switch.
Reference: [14] <author> R. D. Rettberg, W. R. Crowther, P. P. Car-vey, and R. S. Tomlinson, </author> <title> "The Monarch parallel processor hardware design," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 18-30, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Tuning A particularly crucial Worm function is tuning of link data arrival. In the SP1 Switch, all tuning is software-controlled, and utilizes delay logic at each network output port. As a contrasting example, the Monarch parallel processor incorporated a hardwired method for tuning a serial link <ref> [14] </ref>. Two service commands enable the service software to synchronize data arrival for a channel. One command (tuning-control) adds an identical amount of delay to each of the Data and Tag outputs, and selects a separate delay adder for the output port's Token input.
Reference: [15] <author> E. M. Reingold, J. Nievergelt, and N. Deo, </author> <title> Algorithms and Complexity. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1977. </year> <month> 157 </month>
Reference-contexts: The Route Table Generator, or Router, produces this route table. Just as for the Worm, no topology is assumed. Instead, the Router uses the set of links and devices found acceptable by the Worm, and builds breadth-first spanning trees <ref> [15] </ref> using the resulting network graph to find shortest-path routes.
References-found: 15

