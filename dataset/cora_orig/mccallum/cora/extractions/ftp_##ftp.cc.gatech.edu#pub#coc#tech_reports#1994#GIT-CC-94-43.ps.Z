URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1994/GIT-CC-94-43.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.94.html
Root-URL: 
Title: Performance Evaluation of A Seismic Data Analysis Kernel on The KSR Multiprocessors 1  
Author: Weiming Gu 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: September 25, 1994  
Pubnum: GIT-CC-94-43  
Abstract: The paper investigates the effective performance attainable for a specific class of application programs on shared memory supercomputers. Specifically, we are to investigate how seismic data analysis applications behave on the Kendall Square Research Inc.'s KSR multiprocessors. The computational kernel of seismic computation algorithms is parallelized and its performance is analyzed. Three approaches for parallelizing the g5 kernel are analyzed: column-based, row-based, and grid-based parallelizations. All three approaches result in well balanced decompositions, but differ significantly in data locality. In general, the column-based approach has the best data locality, while the small grid-based approach has the worst. These results clearly indicate that data locality is one of the critical factors for attaining high performance for the g5 kernel. The best parallelized g5 kernel code achieves about 44% of both the KSR-1 and KSR-2 machines' peak computational performance. 
Abstract-found: 1
Intro-found: 1
Reference: [BBDS92] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> Nas parallel benchmark results. </title> <booktitle> In Proceedings of Supercomputing'92, </booktitle> <pages> pages 386-393, </pages> <address> Minneapolis, Minnesota, November 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: However, these numbers often capture peak machine performance, and can not be achieved by most application programs. In addition, an application may perform very differently on different MPPs depending on their memory architectures and processor/memory interconnects <ref> [BBDS92] </ref>. The topic of this work is the investigation of the effective performance attainable for a specific class of application programs on shared memory supercomputers. Specifically, we are to investigate how seismic data analysis applications behave on the Kendall Square Research Inc.'s KSR multiprocessors.
Reference: [GEK + 95] <author> Weiming Gu, Greg Eisenhauer, Eileen Kraemer, Karsten Schwan, John Stasko, Jef frey Vetter, and Nirupama Mallavarupu. </author> <title> Falcon: On-line monitoring and steering of large-scale parallel programs. </title> <booktitle> In Proceedings of FRONTIERS'95, </booktitle> <month> February </month> <year> 1995. </year> <note> To appear. Also available as Technical Report GIT-CC-94-21, </note> <institution> College of Computing, Georgia Institute of Technology. </institution>
Reference-contexts: User level threads avoid the overheads of kernel calls experienced by pthreads programs. A program and performance monitoring system, called Falcon <ref> [GEK + 95] </ref>, also developed at Georgia Tech, assists programmers in the process of performance and correctness debugging of their Cthreads programs. 4 3 The g5 Kernel and Its Parallelization on KSR Seismic data analysis is an important step in oil exploration. Data is first acquired from surface survey.
Reference: [Hal90] <author> D. Hale. </author> <title> Stable explicit depth extrapolation of seismic wavefields. </title> <booktitle> In 60th Annual International Meeting and Exposition of the Society of Exploration Geophysicists, </booktitle> <pages> pages 1301-1304, </pages> <year> 1990. </year>
Reference-contexts: Data is first acquired from surface survey. It is then processed and interpreted in order to understand the subsurface structure of a region, and to determine whether there are oil deposits in the surveyed area <ref> [Hal90, HP91] </ref>. Such seismic data processing typically takes months on the fastest computing machines available today, and it involves terabytes of data.
Reference: [HP91] <author> P. T. Highnam and A. Pieprzak. </author> <title> Implementation of a fast, accurate 3-d migration on a massively parallel computer. </title> <booktitle> In 61st Annual International Meeting and Exposition of the Society of Exploration Geophysicists, </booktitle> <pages> pages 338-340, </pages> <year> 1991. </year>
Reference-contexts: Data is first acquired from surface survey. It is then processed and interpreted in order to understand the subsurface structure of a region, and to determine whether there are oil deposits in the surveyed area <ref> [Hal90, HP91] </ref>. Such seismic data processing typically takes months on the fastest computing machines available today, and it involves terabytes of data.
Reference: [Ken93a] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA 02154-1379. </address> <note> KSR FORTRAN Programming, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: Parallel programming support on the KSR machines. Parallel programming on the KSR machines is supported at the low level by the pthreads library and at the high level by three types of parallel constructs <ref> [Ken93b, Ken93a] </ref>: parallel regions, parallel sections, and tile families. The pthreads library, supported at the KSR operating system's kernel level, is the basic lowest 3 level parallel construct, upon which higher level parallel constructs are eventually implemented.
Reference: [Ken93b] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA 02154-1379. </address> <booktitle> KSR Parallel Programming, </booktitle> <month> December </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: Parallel programming support on the KSR machines. Parallel programming on the KSR machines is supported at the low level by the pthreads library and at the high level by three types of parallel constructs <ref> [Ken93b, Ken93a] </ref>: parallel regions, parallel sections, and tile families. The pthreads library, supported at the KSR operating system's kernel level, is the basic lowest 3 level parallel construct, upon which higher level parallel constructs are eventually implemented.
Reference: [Ken93c] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA 02154-1379. </address> <note> KSR Principles of Operations, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: The KSR multiprocessors (both KSR-1 and KSR-2) are NUMA (non-uniform memory access) shared memory cache-only architectures with an interconnection network that consists of hierarchically interconnected rings <ref> [Ken93c] </ref>. Each first level ring can support up to 32 nodes, and up to 32 first level rings can be connected by a second level ring. <p> All 32 nodes are linked by a single ring. Additional results are obtained on a later version of the machine, a 64-node KSR-2 with 2 rings, each connecting 32 processors. KSR's ALLCACHE memory model. The KSR's memory architecture, called ALLCACHE <ref> [Ken93c] </ref>, is a three-level hierarchy. At the top is each node's high performance cache, called sub-cache in ALLCACHE's terminology. At the second level is each node's main memory, termed local cache. The lowest level is the disk storage providing conventional virtual memory. ALLCACHE implements a sequentially consistent shared memory model.
Reference: [Muk91] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: By default, variables are shared if not otherwise specified. If the inner variable i is not specified as a private variable, sharing it among processors will produce incorrect computations. In addition to these parallel constructs, a user level threads package, called Cthreads library (developed at Georgia Institute of Technology <ref> [Muk91] </ref>), is also available for programming on the KSR machines. User level threads avoid the overheads of kernel calls experienced by pthreads programs.
References-found: 8

