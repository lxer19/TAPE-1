URL: http://www.ai.mit.edu/people/cpapa/publications/gen-obj-det-iccv98.ps.gz
Refering-URL: http://www.ai.mit.edu/people/cpapa/publications.html
Root-URL: 
Email: fcpapa,oren,tpg@ai.mit.edu  
Title: A General Framework for Object Detection  
Author: Constantine P. Papageorgiou Michael Oren Tomaso Poggio 
Address: Cambridge, MA 02139  
Affiliation: Center for Biological and Computational Learning Artificial Intelligence Laboratory MIT  
Date: January 1998  
Note: Appears in Proceedings of International Conference on Computer Vision, Bombay, India,  
Abstract: This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Betke and N. Makris. </author> <title> Fast object recognition in motion discontinuities, full motion regions, and improved detection results. noisy images using simulated annealing. </title> <booktitle> In Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <pages> pages 523-20, </pages> <year> 1995. </year>
Reference-contexts: This motion module is a general one that can be used with many detection algorithms and does not compromise the ability of the system to detect non-moving objects. Initial work on the detection of rigid objects in static images, such as street signs or faces, Betke & Makris <ref> [1] </ref>, Yuille, et. al.[21], used template matching approaches with a set of rigid templates or hand-crafted parameterized curves. These approaches are difficult to extend to more complex objects such as people, since they involve a significant amount of prior information and domain knowledge.
Reference: [2] <author> B. Boser, I. Guyon, and V. Vapnik. </author> <title> A training algorithm for optim margin classifier. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-52. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference: [3] <author> B. Heisele, U. Kressel, and W. Ritter. </author> <title> Tracking non-rigid, moving objects based on color cluster flow. </title> <note> In CVPR '97, 1997. to appear. </note>
Reference: [4] <author> D. Hogg. </author> <title> Model-based vision: a program to see a walking person. </title> <journal> Image and Vision Computing, </journal> <volume> 1(1) </volume> <pages> 5-20, </pages> <year> 1983. </year>
Reference-contexts: Pentland [9], Rowley, et. al.[14], and Osuna et al.[11]. Most previous systems that detect objects in video sequences focused on using motion and 3D models or constraints to find people: Tsukiyama & Shirai [17], Leung & Yang [6], Hogg <ref> [4] </ref>, Rohr [13], Wren, et al.[20], Heisele, et. al.[3], McKenna & Gong [8]. These systems suffer from restrictive assumptions on the scene structure, for instance, a single object in the scene or a stationary camera and a sequence of frames.
Reference: [5] <author> M. Leung and Y.-H. Yang. </author> <title> Human body motion segmentation in a complex scene. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 55-64, </pages> <year> 1987. </year>
Reference: [6] <author> M. Leung and Y.-H. Yang. </author> <title> A region based approach for human body analysis. </title> <journal> Pattern Recognition, </journal> <volume> 20(3) </volume> <pages> 321-39, </pages> <year> 1987. </year>
Reference-contexts: Pentland [9], Rowley, et. al.[14], and Osuna et al.[11]. Most previous systems that detect objects in video sequences focused on using motion and 3D models or constraints to find people: Tsukiyama & Shirai [17], Leung & Yang <ref> [6] </ref>, Hogg [4], Rohr [13], Wren, et al.[20], Heisele, et. al.[3], McKenna & Gong [8]. These systems suffer from restrictive assumptions on the scene structure, for instance, a single object in the scene or a stationary camera and a sequence of frames.
Reference: [7] <author> S. Mallat. </author> <title> A theory for multiresolution signal decomposition: The wavelet representation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(7) </volume> <pages> 674-93, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Here, we expand on the original system and apply it to another domain, faces, with promising results. 2 The Wavelet Representation The Haar wavelets are a natural set basis functions which encode differences in average intensities between different regions; for an in depth description of wavelets, see <ref> [7] </ref>. To achieve the spatial resolution necessary for detection and to increase the expressive power of the model, we introduce the quadruple density transform in [10], an extension of the 2D Haar wavelet (Figure 2-1), that yields an overcomplete set of basis functions.
Reference: [8] <author> S. McKenna and S. Gong. </author> <title> Non-intrusive person authentication for access control by visual tracking and face recognition. </title> <editor> In J. Bigun, G. Chollet, and G. Borgefors, editors, </editor> <booktitle> Audio- and Video-based Biometric Person Authentication, </booktitle> <pages> pages 177-183. </pages> <address> IAPR, </address> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: Most previous systems that detect objects in video sequences focused on using motion and 3D models or constraints to find people: Tsukiyama & Shirai [17], Leung & Yang [6], Hogg [4], Rohr [13], Wren, et al.[20], Heisele, et. al.[3], McKenna & Gong <ref> [8] </ref>. These systems suffer from restrictive assumptions on the scene structure, for instance, a single object in the scene or a stationary camera and a sequence of frames. In some of these motion-based systems, the focus is on model fitting, tracking and motion interpretation.
Reference: [9] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <type> Technical Report 326, </type> <institution> Media Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: This approach was used by Sung &Poggio [16] and Vaillant, et al.[18] for the detection of frontal faces in cluttered scenes, with similar architectures presented by Moghaddam and A. Pentland <ref> [9] </ref>, Rowley, et. al.[14], and Osuna et al.[11].
Reference: [10] <author> M. Oren, C. Papageorgiou, P. Sinha, E. Osuna, and T. Poggio. </author> <title> Pedestrian detection using wavelet templates. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 193-99, </pages> <year> 1997. </year>
Reference-contexts: We use a Haar wavelet representation to capture the structural similarities between instances of an object class. This idea of an overcomplete, or redundant, representation was introduced in <ref> [10] </ref> and we showed how this model is learnable from examples, using pedestrian detection as a testbed. <p> To achieve the spatial resolution necessary for detection and to increase the expressive power of the model, we introduce the quadruple density transform in <ref> [10] </ref>, an extension of the 2D Haar wavelet (Figure 2-1), that yields an overcomplete set of basis functions. <p> The use of this quadruple density transform results in an overcomplete dictionary of basis functions that facilitates the definition of complex constraints on the object patterns. In <ref> [10] </ref>, we also show that we do not lose computational efficiency with respect to the standard wavelet transform. 3 Learning the Class Model Given an object class, the central problem is how to learn which are the relevant coefficients that express structure common to the entire object class and which are <p> A similar analysis of the average values of the coefficients was done for the pedestrian class and Figure 6 shows the grey-scale coding similar to Figure 4. We refer the interested reader to <ref> [10] </ref> for the details. It is interesting to observe that for the pedestrian class, there are no strong internal patterns as in the face class; rather, the significant basis functions are along the exterior boundary of the class, indicating a different type of significant visual information.
Reference: [11] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Support vector machines: Training and applications. A.I. </title> <type> Memo 1602, </type> <institution> MIT A. I. Lab., </institution> <year> 1997. </year>
Reference-contexts: Databases of this size and composition have been used extensively in face detection [15] [14] <ref> [11] </ref> [12] and we keep this data format for comparison purposes. For the coefficient analysis, we use the wavelets at scales of 4 fi 4 pixels and 2 fi 2 pixels since their dimensions correspond to typical facial features for this size of face image. <p> The classification technique we use is the support vector machine (SVM) developed by Vapnik et al.[2][19]. This recently developed technique has the appealing features of having very few tunable parameters and using structural risk minimization which minimizes a bound on the generalization error (see <ref> [11] </ref> [12]). We train our systems using databases of positive examples gathered from outdoor and indoor scenes. The initial negative examples in the training database are patterns from natural scenes not containing people or faces. While the target class is well-defined, there are no typical examples of the negative class. <p> To understand the effect of different penalties in the Support Vector training (see <ref> [11] </ref> [12]), we train several systems using different penalties for misclassification. The systems undergo the bootstrapping cycle detailed in Section 3, and end up with between 4500 and 9500 negative examples.
Reference: [12] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Training support vector machines: An application to face detection. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 130-36, </pages> <year> 1997. </year>
Reference-contexts: Databases of this size and composition have been used extensively in face detection [15] [14] [11] <ref> [12] </ref> and we keep this data format for comparison purposes. For the coefficient analysis, we use the wavelets at scales of 4 fi 4 pixels and 2 fi 2 pixels since their dimensions correspond to typical facial features for this size of face image. <p> The classification technique we use is the support vector machine (SVM) developed by Vapnik et al.[2][19]. This recently developed technique has the appealing features of having very few tunable parameters and using structural risk minimization which minimizes a bound on the generalization error (see [11] <ref> [12] </ref>). We train our systems using databases of positive examples gathered from outdoor and indoor scenes. The initial negative examples in the training database are patterns from natural scenes not containing people or faces. While the target class is well-defined, there are no typical examples of the negative class. <p> To understand the effect of different penalties in the Support Vector training (see [11] <ref> [12] </ref>), we train several systems using different penalties for misclassification. The systems undergo the bootstrapping cycle detailed in Section 3, and end up with between 4500 and 9500 negative examples.
Reference: [13] <author> K. Rohr. </author> <title> Incremental recognition of pedestrians from image sequences. </title> <journal> Computer Vision and Pattern Recognition, </journal> <pages> pages 8-13, </pages> <year> 1993. </year>
Reference-contexts: Pentland [9], Rowley, et. al.[14], and Osuna et al.[11]. Most previous systems that detect objects in video sequences focused on using motion and 3D models or constraints to find people: Tsukiyama & Shirai [17], Leung & Yang [6], Hogg [4], Rohr <ref> [13] </ref>, Wren, et al.[20], Heisele, et. al.[3], McKenna & Gong [8]. These systems suffer from restrictive assumptions on the scene structure, for instance, a single object in the scene or a stationary camera and a sequence of frames.
Reference: [14] <author> H. Rowley, S. Baluja, and T. Kanade. </author> <title> Human face detection in visual scenes. </title> <type> Technical Report CMU-CS-95-158, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> July/November </month> <year> 1995. </year>
Reference-contexts: Databases of this size and composition have been used extensively in face detection [15] <ref> [14] </ref> [11] [12] and we keep this data format for comparison purposes. For the coefficient analysis, we use the wavelets at scales of 4 fi 4 pixels and 2 fi 2 pixels since their dimensions correspond to typical facial features for this size of face image.
Reference: [15] <author> K.-K. Sung. </author> <title> Learning and Example Selection for Object and Pattern Detection. </title> <type> PhD thesis, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: Databases of this size and composition have been used extensively in face detection <ref> [15] </ref> [14] [11] [12] and we keep this data format for comparison purposes. For the coefficient analysis, we use the wavelets at scales of 4 fi 4 pixels and 2 fi 2 pixels since their dimensions correspond to typical facial features for this size of face image.
Reference: [16] <author> K.-K. Sung and T. Poggio. </author> <title> Example-based learning for view-based human face detection. A.I. </title> <type> Memo 1521, </type> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: In recent research, more closely related to our system, the detection problem is solved using learning-based techniques that are data driven. This approach was used by Sung &Poggio <ref> [16] </ref> and Vaillant, et al.[18] for the detection of frontal faces in cluttered scenes, with similar architectures presented by Moghaddam and A. Pentland [9], Rowley, et. al.[14], and Osuna et al.[11]. <p> While the target class is well-defined, there are no typical examples of the negative class. To overcome this problem of defining this extremely large negative class, we use the idea of "bootstrapping" training <ref> [16] </ref>. In the context of the pedestrian detection system, after the initial training, we run the system over arbitrary images that do not contain any people, adding false detections into the training set as examples of the negative class, and retraining the classifier (Figure 8).
Reference: [17] <author> T. Tsukiyama and Y. Shirai. </author> <title> Detection of the movements of persons from a sparse sequence of tv images. </title> <journal> Pattern Recognition, </journal> 18(3/4):207-13, 1985. 
Reference-contexts: Pentland [9], Rowley, et. al.[14], and Osuna et al.[11]. Most previous systems that detect objects in video sequences focused on using motion and 3D models or constraints to find people: Tsukiyama & Shirai <ref> [17] </ref>, Leung & Yang [6], Hogg [4], Rohr [13], Wren, et al.[20], Heisele, et. al.[3], McKenna & Gong [8]. These systems suffer from restrictive assumptions on the scene structure, for instance, a single object in the scene or a stationary camera and a sequence of frames.
Reference: [18] <author> R. Vaillant, C. Monrocq, and Y. L. Cun. </author> <title> Original approach for the localisation of objects in images. </title> <booktitle> IEE Proc.-Vis. Image Signal Processing, </booktitle> <volume> 141(4), </volume> <month> August </month> <year> 1994. </year>
Reference: [19] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference: [20] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pent-land. Pfinder: </author> <title> Real-time tracking of the human body. </title> <type> Technical Report 353, </type> <institution> Media Laboratory, Mas-sachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference: [21] <author> A. Yuille, P. Hallinan, and D. Cohen. </author> <title> Feature Extraction from Faces using Deformable Templates. </title> <journal> International Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 99-111, </pages> <year> 1992. </year>
References-found: 21

