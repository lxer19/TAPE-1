URL: ftp://ftp.cs.cornell.edu/pub/chandra/failure.detectors.algorithms.ps.Z
Refering-URL: http://www.cs.cornell.edu/Info/People/chandra/UnreliableFD.html
Root-URL: 
Email: tushar@watson.ibm.com, sam@cs.cornell.edu  
Title: Unreliable Failure Detectors for Reliable Distributed Systems  
Author: Tushar Deepak Chandra Sam Toueg 
Address: Ithaca, New York 14853  
Affiliation: Department of Computer Science Cornell University  
Abstract: We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties | completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [CHT92]. To appear in the Journal of the ACM fl Research supported by an IBM graduate fellowship, NSF grants CCR-8901780 and CCR-9102231, DARPA/NASA Ames Grant NAG-2-593, and in part by Grants from IBM and Siemens Corp. A preliminary version of this paper appeared in Proceedings of the Tenth ACM Symposium on Principles of Distributed Computing, pages 325-340. ACM press, August 1991. 
Abstract-found: 1
Intro-found: 1
Reference: [ABD + 87] <author> Hagit Attiya, Amotz Bar-Noy, Danny Dolev, Daphne Koller, David Peleg, and Rudiger Reischuk. </author> <title> Achievable cases in an asynchronous environment. </title> <booktitle> In Proceedings of the Twenty-Eighth Symposium on Foundations of Computer Science, </booktitle> <pages> pages 337-346. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions <ref> [DLP + 86, ABD + 87, BW87, BMZ88] </ref>, or the study of several models of partial synchrony [DDS87, DLS88].
Reference: [ADKM91] <author> Yair Amir, Danny Dolev, Shlomo Kramer, and Dalia Malki. Transis: </author> <title> A communication sub-system for high availability. </title> <type> Technical Report CS91-13, </type> <institution> Computer Science Department, The Hebrew University of Jerusalem, </institution> <month> November </month> <year> 1991. </year> <title> 25 For example, the time-out period in the current version of Isis is greater than 10 seconds. </title> <type> 41 </type>
Reference-contexts: Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis <ref> [ADKM91] </ref>, HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. In this paper, we focus on solutions to Consensus and Atomic Broadcast in the asynchronous model of distributed computing.
Reference: [ADLS91] <author> Hagit Attiya, Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer. </author> <title> Bounds on the time to reach agreement in the presence of timing uncertainity. </title> <booktitle> In Proceedings of the Twenty third ACM Symposium on Theory of Computing, </booktitle> <pages> pages 359-369. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: An example of this approach is given by an algorithm in <ref> [ADLS91] </ref>, which, as pointed out by the authors, "can be viewed as an asynchronous algorithm that uses a fault detection (e.g., timeout) mechanism." Acknowledgements We are deeply grateful to Vassos Hadzilacos for his crucial help in revising this paper.
Reference: [BCJ + 90] <author> Kenneth P. Birman, Robert Cooper, Thomas A. Joseph, Kenneth P. Kane, and Frank Bernhard Schmuck. </author> <title> ISIS A Distributed Programming Environment, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis <ref> [BJ87, BCJ + 90] </ref>, Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. <p> This makes the presentation of applications and their proof of correctness more modular. Our approach is well-suited to model many existing systems that decouple the design of fault-tolerant applications from the underlying failure detection mechanisms, such as the Isis Toolkit <ref> [BCJ + 90] </ref> for asynchronous fault-tolerant distributed computing. We characterise a class of failure detectors by specifying the completeness and accuracy properties that failure detectors in this class must satisfy.
Reference: [Ben83] <author> Michael Ben-Or. </author> <title> Another advantage of free choice: Completely asynchronous agreement protocols. </title> <booktitle> In Proceedings of the Second ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 27-30. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1983. </year>
Reference-contexts: It turns out that this requirement is necessary: Using 3P to solve Consensus requires a majority of correct processes. Since 3P - 3S, the algorithm in Figure 6 is optimal with respect to fault-tolerance. The proof of this result (Theorem 24) uses standard "partitioning" techniques (e.g., <ref> [Ben83, BT85] </ref>). It is also a corollary of Theorem 4.3 in [DLS88] together with Theorem 38 in Section 9.1.
Reference: [BGP89] <author> Piotr Berman, Juan A. Garay, and Kenneth J. Perry. </author> <title> Towards optimal distributed consensus. </title> <booktitle> In Proceedings of the Thirtieth Symposium on Foundations of Computer Science, </booktitle> <pages> pages 410-415. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1989. </year>
Reference-contexts: In such systems, the algorithm in Figure 6 solves Consensus using any Eventual Strong failure detector D 2 3S. In other words, it works with any failure detector D that satisfies strong completeness and eventual weak accuracy. This algorithm uses the rotating coordinator paradigm <ref> [Rei82, CM84, DLS88, BGP89, CT90] </ref>, and it proceeds in asynchronous "rounds". We assume that all processes have a priori knowledge that during round r, the coordinator is process c = (r mod n) + 1. All messages are either to or from the "current" coordinator.
Reference: [BGT90] <author> Navin Budhiraja, Ajei Gopal, and Sam Toueg. </author> <title> Early-stopping distributed bidding and applications. </title> <booktitle> In Proceedings of the Fourth International Workshop on Distributed Algorithms, </booktitle> <pages> pages 301-320. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [BJ87] <author> Kenneth P. Birman and Thomas A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis <ref> [BJ87, BCJ + 90] </ref>, Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. <p> The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [BMZ88] <author> Ofer Biran, Shlomo Moran, and Shmuel Zaks. </author> <title> A combinatorial characterization of the distributed tasks that are solvable in the presence of one faulty processor. </title> <booktitle> In Proceedings of the Seventh ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 263-275. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions <ref> [DLP + 86, ABD + 87, BW87, BMZ88] </ref>, or the study of several models of partial synchrony [DDS87, DLS88].
Reference: [BT85] <author> Gabriel Bracha and Sam Toueg. </author> <title> Asynchronous consensus and broadcast protocols. </title> <journal> Journal of the ACM, </journal> <volume> 32(4) </volume> <pages> 824-840, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In Section 3, we use the concept of reduction to show that we can focus on four classes of failure detectors rather than eight. In Section 4, we present Reliable Broadcast <ref> [BT85] </ref>, a communication primitive for asynchronous systems used by several of our algorithms. In Section 5, we 7 They are actually equivalent even in asynchronous systems with arbitrary, i.e., "Byzantine", failures. <p> This algorithm satisfies validity, agreement and uniform integrity in asynchronous systems with up to n 1 crash failures. The proof is obvious and therefore omitted. 13 This is a crash-failure version of the asynchronous broadcast primitive defined in <ref> [BT85] </ref> for "Byzantine" failures. 16 5 The Consensus problem In the Consensus problem, all correct processes propose a value and must reach a unanimous and irrevocable decision on some value that is related to the proposed values [Fis83]. <p> It turns out that this requirement is necessary: Using 3P to solve Consensus requires a majority of correct processes. Since 3P - 3S, the algorithm in Figure 6 is optimal with respect to fault-tolerance. The proof of this result (Theorem 24) uses standard "partitioning" techniques (e.g., <ref> [Ben83, BT85] </ref>). It is also a corollary of Theorem 4.3 in [DLS88] together with Theorem 38 in Section 9.1.
Reference: [BW87] <author> Michael Bridgland and Ronald Watro. </author> <title> Fault-tolerant decision making in totally asynchronous distributed systems. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 52-63. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions <ref> [DLP + 86, ABD + 87, BW87, BMZ88] </ref>, or the study of several models of partial synchrony [DDS87, DLS88].
Reference: [CASD85] <author> Flaviu Cristian, Houtan Aghili, Raymond Strong, and Danny Dolev. </author> <title> Atomic broadcast: From simple message diffusion to Byzantine agreement. </title> <booktitle> In Proceedings of the Fifteenth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 200-206, </pages> <month> June </month> <year> 1985. </year> <note> A revised version appears as IBM Research Laboratory Technical Report RJ5244 (April 1989). </note>
Reference-contexts: The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [CD89] <author> Benny Chor and Cynthia Dwork. </author> <title> Randomization in byzantine agreement. </title> <booktitle> Advances in Computer Research, </booktitle> <volume> 5 </volume> <pages> 443-497, </pages> <year> 1989. </year> <month> 42 </month>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques <ref> [CD89] </ref>, the definition of some weaker problems and their solutions [DLP + 86, ABD + 87, BW87, BMZ88], or the study of several models of partial synchrony [DDS87, DLS88]. <p> This is because Consensus has no deterministic solution in such systems [FLP85]. 2. Atomic Broadcast can be solved using randomisation or unreliable failure detectors in asynchronous systems. This is because Consensus is solvable with these techniques in such systems (for a survey of randomised Consensus algorithms, see <ref> [CD89] </ref>). Consensus can be easily reduced to Atomic Broadcast as follows [DDS87]. To propose a value, a process atomically broadcasts it. <p> Corollary 34: Atomic Broadcast cannot be solved using 3P in asynchronous systems with f d n 2 e. Furthermore, Theorem 30 shows that by "plugging in" any randomised Consensus algorithm (such as the ones in <ref> [CD89] </ref>) into the algorithm of Figure 7, we automatically get a randomised algorithm for Atomic Broadcast in asynchronous systems.
Reference: [CDD90] <author> Flaviu Cristian, Robert D. Dancey, and Jon Dehn. </author> <title> Fault-tolerance in the advanced automation system. </title> <type> Technical Report RJ 7424, </type> <institution> IBM Research Laboratory, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA <ref> [CDD90] </ref>, and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. In this paper, we focus on solutions to Consensus and Atomic Broadcast in the asynchronous model of distributed computing.
Reference: [CHT92] <author> Tushar D. Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <type> Technical Report 92-1293, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> July </month> <year> 1992. </year> <title> Available from ftp://ftp.cs.cornell.edu/pub/chandra/failure.detectors.weakest.dvi.Z. </title> <booktitle> A preliminary version appeared in the Proceedings of the Eleventh ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 147-158. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Any failure detector that satisfies the completeness and accuracy properties of 3W provides sufficient information about failures to solve Consensus. But is this information necessary? Indeed, what it is the "weakest" failure detector for solving Consensus? <ref> [CHT92] </ref> answers this question by considering 3W 0 , the weakest failure detector in 3W. Roughly speaking, 3W 0 satisfies the properties of 3W, and no other properties. [CHT92] shows that 3W 0 is the weakest failure detector that can be used to solve Consensus in asynchronous systems (with a majority <p> But is this information necessary? Indeed, what it is the "weakest" failure detector for solving Consensus? <ref> [CHT92] </ref> answers this question by considering 3W 0 , the weakest failure detector in 3W. Roughly speaking, 3W 0 satisfies the properties of 3W, and no other properties. [CHT92] shows that 3W 0 is the weakest failure detector that can be used to solve Consensus in asynchronous systems (with a majority of correct processes). More precisely, [CHT92] shows that 6 Indeed, no algorithm can implement such a failure detector in an asynchronous system: as we show in Section 6.2, <p> Roughly speaking, 3W 0 satisfies the properties of 3W, and no other properties. <ref> [CHT92] </ref> shows that 3W 0 is the weakest failure detector that can be used to solve Consensus in asynchronous systems (with a majority of correct processes). More precisely, [CHT92] shows that 6 Indeed, no algorithm can implement such a failure detector in an asynchronous system: as we show in Section 6.2, this implementation could be used to solve Consensus in such a system, contradicting the impossibility result of [FLP85]. 6 if a failure detector D can be used to <p> Informally, a failure detector D provides (possibly incorrect) information about the failure pattern F that occurs in an execution. Formally, failure detector D is a function that maps each failure pattern F to a set of failure detector histories D (F ). This is the set 9 In <ref> [CHT92] </ref> failure detectors can output values from an arbitrary range. 8 of all failure detector histories that could occur in executions with failure pattern F and failure detector D. 10 In this paper, we do not define failure detectors in terms of specific implementations. <p> The resulting definitions and corresponding notation are given in Figure 1. 2.5 Algorithms and runs In this paper, we focus on algorithms that use unreliable failure detectors. To describe such algorithms, we only need informal definitions of algorithms and runs, based on the formal definitions given in <ref> [CHT92] </ref>. 11 An algorithm A is a collection of n deterministic automata, one for each process in the system. Computation proceeds in steps of A. <p> step and queries its failure detector module, it gets the current value output by its local failure detector module, and (3) every process that is correct in F takes an infinite number of steps in S and eventually receives every message sent to it. 11 Formal definitions are necessary in <ref> [CHT92] </ref> to prove a subtle lower bound. 12 [CHT92] assumes that each step is atomic, i.e., indivisible with respect to failures. Furthermore, each process can send a message to all processes during such a step. These assumptions were made to strengthen the lower bound result of [CHT92]. 11 Informally, a problem <p> gets the current value output by its local failure detector module, and (3) every process that is correct in F takes an infinite number of steps in S and eventually receives every message sent to it. 11 Formal definitions are necessary in <ref> [CHT92] </ref> to prove a subtle lower bound. 12 [CHT92] assumes that each step is atomic, i.e., indivisible with respect to failures. Furthermore, each process can send a message to all processes during such a step. These assumptions were made to strengthen the lower bound result of [CHT92]. 11 Informally, a problem P is a set of runs (usually defined <p> definitions are necessary in <ref> [CHT92] </ref> to prove a subtle lower bound. 12 [CHT92] assumes that each step is atomic, i.e., indivisible with respect to failures. Furthermore, each process can send a message to all processes during such a step. These assumptions were made to strengthen the lower bound result of [CHT92]. 11 Informally, a problem P is a set of runs (usually defined by a set of properties that these runs must satisfy). <p> Thus, Consensus can be solved in asynchronous systems using any failure detector in 3W, the weakest class of failure detectors considered in this paper. This leads to the following question: What is the weakest failure detector for solving Consensus? The answer to this question, given in a companion paper <ref> [CHT92] </ref>, is summarised below. Let 3W 0 be the "weakest" failure detector in 3W. Roughly speaking, 3W 0 is the failure detector that exhibits all the failure detector behaviours allowed by the properties that define 3W. <p> Roughly speaking, 3W 0 is the failure detector that exhibits all the failure detector behaviours allowed by the properties that define 3W. More precisely, 3W 0 consists of all the failure detector histories that satisfy weak completeness and eventual weak accuracy (for a formal definition see <ref> [CHT92] </ref>). 25 Together with Hadzilacos, in [CHT92] we show that 3W 0 is the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes. More precisely, we show: Theorem 22: [CHT92] If a failure detector D can be used to solve Consensus in an asynchronous system, <p> More precisely, 3W 0 consists of all the failure detector histories that satisfy weak completeness and eventual weak accuracy (for a formal definition see <ref> [CHT92] </ref>). 25 Together with Hadzilacos, in [CHT92] we show that 3W 0 is the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes. More precisely, we show: Theorem 22: [CHT92] If a failure detector D can be used to solve Consensus in an asynchronous system, then D - 3W 0 in <p> detector histories that satisfy weak completeness and eventual weak accuracy (for a formal definition see <ref> [CHT92] </ref>). 25 Together with Hadzilacos, in [CHT92] we show that 3W 0 is the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes. More precisely, we show: Theorem 22: [CHT92] If a failure detector D can be used to solve Consensus in an asynchronous system, then D - 3W 0 in that system. <p> In contrast to [DDS87], which identified a set of minimal models of partial synchrony in which Consensus is solvable, in <ref> [CHT92] </ref> together with Hadzi-lacos we are able to exhibit a single minimum failure detector, 3W 0 , that can be used to solve Consensus.
Reference: [CL94] <author> Tushar Chandra and Mikel Larrea. </author> <title> E-mail correspondence. Showed that 3W cannot be used to solve non-blocking atomic commit, </title> <month> November </month> <year> 1994. </year>
Reference-contexts: TRB is not the only "natural" problem that can be solved using P but cannot be solved using 3W. Other examples include the non-blocking atomic commitment problem <ref> [CL94, Gue95] </ref>, and a form of leader election [SM95]. Figure 9 summarises these results. 9 Related work 9.1 Partial synchrony Fischer, Lynch and Paterson showed that Consensus cannot be solved in an asynchronous system subject to crash failures [FLP85].
Reference: [CM84] <author> J. Chang and N. Maxemchuk. </author> <title> Reliable broadcast protocols. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 251-273, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: In such systems, the algorithm in Figure 6 solves Consensus using any Eventual Strong failure detector D 2 3S. In other words, it works with any failure detector D that satisfies strong completeness and eventual weak accuracy. This algorithm uses the rotating coordinator paradigm <ref> [Rei82, CM84, DLS88, BGP89, CT90] </ref>, and it proceeds in asynchronous "rounds". We assume that all processes have a priori knowledge that during round r, the coordinator is process c = (r mod n) + 1. All messages are either to or from the "current" coordinator. <p> The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [Cri87] <author> Flaviu Cristian. </author> <title> Issues in the design of highly available computing services. </title> <booktitle> In Annual Symposium of the Canadian Information Processing Society, </booktitle> <pages> pages 9-16, </pages> <month> July </month> <year> 1987. </year> <note> Also IBM Research Report RJ5856, </note> <month> July </month> <year> 1987. </year>
Reference-contexts: Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS <ref> [Cri87] </ref>, FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. In this paper, we focus on solutions to Consensus and Atomic Broadcast in the asynchronous model of distributed computing.
Reference: [CT90] <author> Tushar D. Chandra and Sam Toueg. </author> <title> Time and message efficient reliable broadcasts. </title> <booktitle> In Proceedings of the Fourth International Workshop on Distributed Algorithms, </booktitle> <pages> pages 289-300. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: In such systems, the algorithm in Figure 6 solves Consensus using any Eventual Strong failure detector D 2 3S. In other words, it works with any failure detector D that satisfies strong completeness and eventual weak accuracy. This algorithm uses the rotating coordinator paradigm <ref> [Rei82, CM84, DLS88, BGP89, CT90] </ref>, and it proceeds in asynchronous "rounds". We assume that all processes have a priori knowledge that during round r, the coordinator is process c = (r mod n) + 1. All messages are either to or from the "current" coordinator.
Reference: [DDS87] <author> Danny Dolev, Cynthia Dwork, and Larry Stockmeyer. </author> <title> On the minimal synchronism needed for distributed consensus. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 77-97, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Although the asynchronous model of computation is attractive for the reasons outlined above, it is well known that Consensus and Atomic Broadcast cannot be solved deterministically in an asynchronous system that is subject to even a single crash failure <ref> [FLP85, DDS87] </ref>. 1 Essentially, the impossibility results for Consensus and Atomic Broadcast stem from the inherent difficulty of determining whether a process has actually crashed or is only "very slow". <p> To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions [DLP + 86, ABD + 87, BW87, BMZ88], or the study of several models of partial synchrony <ref> [DDS87, DLS88] </ref>. Nevertheless, the impossibility of deterministic solutions to many agreement problems (such as Consensus and Atomic Broadcast) remains a major obstacle to the use of the asynchronous model of computation for fault-tolerant distributed computing. <p> Thus, Atomic Broadcast can be solved using the unreliable failure detectors described in this paper. Furthermore, 3W 0 is the weakest failure detector that can be used to solve Atomic Broadcast. A different tack on circumventing the unsolvability of Consensus is pursued in <ref> [DDS87] </ref> and [DLS88]. The approach of those papers is based on the observation that between the completely synchronous and completely asynchronous models of distributed systems there lie a variety of intermediate partially synchronous models. <p> Agreement: No two correct processes decide differently. Uniform validity: If a process decides v, then v was proposed by some process. 14 It is well-known that Consensus cannot be solved in asynchronous systems that are subject to even a single crash failure <ref> [FLP85, DDS87] </ref>. 6 Solving Consensus using unreliable failure detec tors We now show how to solve Consensus using each one of the eight classes of failure detectors defined in Figure 1. <p> Atomic Broadcast can be solved using randomisation or unreliable failure detectors in asynchronous systems. This is because Consensus is solvable with these techniques in such systems (for a survey of randomised Consensus algorithms, see [CD89]). Consensus can be easily reduced to Atomic Broadcast as follows <ref> [DDS87] </ref>. To propose a value, a process atomically broadcasts it. To decide a value, a process picks the value of the first message that it atomically delivers. 18 By total order of Atomic Broadcast, all correct processes deliver the same first message. <p> This real-isation led us to augment the asynchronous model of computation with unreliable failure detectors as described in this paper. A different tack on circumventing the unsolvability of Consensus is pursued in <ref> [DDS87] </ref> and [DLS88]. <p> In particular, <ref> [DDS87] </ref> defines a space of 32 models by considering five key parameters, each of which admits a "favourable" and an "unfavourable" setting. For instance, one of the parameters is whether the maximum message delay is bounded and known (favourable setting) or unbounded (unfavourable setting). <p> For instance, one of the parameters is whether the maximum message delay is bounded and known (favourable setting) or unbounded (unfavourable setting). Each of the 32 models corresponds to a particular setting of the 5 parameters. <ref> [DDS87] </ref> identifies four "minimal" models in which Consensus is solvable. These are minimal in the sense that the weakening of any parameter from favourable to unfavourable would yield a model of partial synchrony where Consensus is unsolvable. Thus, within the space of the models considered, [DDS87] delineates precisely the boundary between <p> setting of the 5 parameters. <ref> [DDS87] </ref> identifies four "minimal" models in which Consensus is solvable. These are minimal in the sense that the weakening of any parameter from favourable to unfavourable would yield a model of partial synchrony where Consensus is unsolvable. Thus, within the space of the models considered, [DDS87] delineates precisely the boundary between solvability and unsolvability of Consensus, and provides an answer to the question "What is the least amount of synchrony sufficient to solve Consensus?". [DLS88] considers two models of partial synchrony. <p> Instead of focusing on the operational features of partial synchrony (such as the parameters that define M 1 , M 2 , and M 3 , or the five parameters considered in <ref> [DDS87] </ref>), we can consider the axiomatic properties that failure detectors must have in order to solve Consensus. The problem of implementing a certain type of failure detector in a specific model of partial synchrony becomes a separate issue; this separation affords greater modularity. <p> In contrast to <ref> [DDS87] </ref>, which identified a set of minimal models of partial synchrony in which Consensus is solvable, in [CHT92] together with Hadzi-lacos we are able to exhibit a single minimum failure detector, 3W 0 , that can be used to solve Consensus. <p> Essentially, the Isis failure detector forces the system to conform 24 The proof in [LA87] is similar to the proof that Consensus is impossible in message-passing systems when send and receive are not part of the same atomic step <ref> [DDS87] </ref>. 40 to its view. From the application's point of view, this failure detector looks "perfect": it never makes visible mistakes.
Reference: [DLP + 86] <author> Danny Dolev, Nancy A. Lynch, Shlomit S. Pinter, Eugene W. Stark, and William E. Weihl. </author> <title> Reaching approximate agreement in the presence of faults. </title> <journal> Journal of the ACM, </journal> <volume> 33(3) </volume> <pages> 499-516, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions <ref> [DLP + 86, ABD + 87, BW87, BMZ88] </ref>, or the study of several models of partial synchrony [DDS87, DLS88].
Reference: [DLS88] <author> Cynthia Dwork, Nancy A. Lynch, and Larry Stockmeyer. </author> <title> Consensus in the presence of partial synchrony. </title> <journal> Journal of the ACM, </journal> <volume> 35(2) </volume> <pages> 288-323, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: To circumvent these impossibility results, previous research focused on the use of ran-domisation techniques [CD89], the definition of some weaker problems and their solutions [DLP + 86, ABD + 87, BW87, BMZ88], or the study of several models of partial synchrony <ref> [DDS87, DLS88] </ref>. Nevertheless, the impossibility of deterministic solutions to many agreement problems (such as Consensus and Atomic Broadcast) remains a major obstacle to the use of the asynchronous model of computation for fault-tolerant distributed computing. <p> example, if a Consensus algorithm assumes the properties of 3W, but the failure detector that it actually uses misbehaves continuously, processes may be prevented from deciding, but they never decide different 5 Solving a problem with the assumption that certain properties hold for sufficiently long has been done previously, see <ref> [DLS88] </ref>. 5 values (or a value that is not allowed). Similarly, with an Atomic Broadcast algorithm, processes may stop delivering messages, but they never deliver messages out-of-order. <p> For instance, one could envision specialised hardware to support this abstraction. However, most implementations of failure detectors are based on timeout mechanisms. For the purpose of illustration, we now outline one such implementation based on an idea in <ref> [DLS88] </ref> (a more detailed description of this implementation and of its properties is given in Section 9.1). Every process q periodically sends a "q-is-alive" message to all. If a process p times-out on some process q, it adds q to its list of suspects. <p> Thus, Atomic Broadcast can be solved using the unreliable failure detectors described in this paper. Furthermore, 3W 0 is the weakest failure detector that can be used to solve Atomic Broadcast. A different tack on circumventing the unsolvability of Consensus is pursued in [DDS87] and <ref> [DLS88] </ref>. The approach of those papers is based on the observation that between the completely synchronous and completely asynchronous models of distributed systems there lie a variety of intermediate partially synchronous models. <p> In this paper, we argue that partial synchrony assumptions can be encapsulated in the unreliability of failure detectors. For example, in the models of partial synchrony considered in <ref> [DLS88] </ref> it is easy to implement a failure detector that satisfies the properties of 3W. This immediately implies that Consensus and Atomic Broadcast can be solved in these models. <p> In such systems, the algorithm in Figure 6 solves Consensus using any Eventual Strong failure detector D 2 3S. In other words, it works with any failure detector D that satisfies strong completeness and eventual weak accuracy. This algorithm uses the rotating coordinator paradigm <ref> [Rei82, CM84, DLS88, BGP89, CT90] </ref>, and it proceeds in asynchronous "rounds". We assume that all processes have a priori knowledge that during round r, the coordinator is process c = (r mod n) + 1. All messages are either to or from the "current" coordinator. <p> In 15 In the literature, t is often used instead of f, the notation adopted here. In this paper, we reserve t to denote real-time. 16 Many Consensus algorithms in the literature have the property that a value gets locked before processes decide, e.g. <ref> [Rei82, DLS88] </ref>. 21 Every process p executes the following: procedure propose (v p ) estimate p v p festimate p is p's estimate of the decision valueg state p undecided r p 0 fr p is p's current round numberg ts p 0 fts p is the last round in which <p> Since 3P - 3S, the algorithm in Figure 6 is optimal with respect to fault-tolerance. The proof of this result (Theorem 24) uses standard "partitioning" techniques (e.g., [Ben83, BT85]). It is also a corollary of Theorem 4.3 in <ref> [DLS88] </ref> together with Theorem 38 in Section 9.1. Theorem 24: Consensus cannot be solved using 3P in asynchronous systems with f d n Proof: We give a failure detector D 2 3P such that no algorithm can solve Consensus using D in asynchronous systems with f d n 2 e. <p> This real-isation led us to augment the asynchronous model of computation with unreliable failure detectors as described in this paper. A different tack on circumventing the unsolvability of Consensus is pursued in [DDS87] and <ref> [DLS88] </ref>. <p> Thus, within the space of the models considered, [DDS87] delineates precisely the boundary between solvability and unsolvability of Consensus, and provides an answer to the question "What is the least amount of synchrony sufficient to solve Consensus?". <ref> [DLS88] </ref> considers two models of partial synchrony. Roughly speaking, the first model (denoted M 1 here) stipulates that in every execution there are bounds on relative process speeds and on message transmission times, but these bounds are not known. <p> The implementation of D 2 3P for M 3 , which uses an idea found in <ref> [DLS88] </ref>, works as follows (see Figure 10). To measure elapsed time, each process p maintains a local clock, say by counting the number of steps that it takes. Each process p periodically sends a "p-is-alive" message to all the processes. <p> After this point, p cannot time-out on q any more|a contradiction to our assumption that p times-out on q infinitely often. Thus Case 2 cannot occur. 2 In this paper we have not considered communication failures. In the second model of partial synchrony of <ref> [DLS88] </ref>, where bounds are known but hold only after GST, messages sent before GST can be lost. <p> On the other hand, these initial message losses invalidate the Consensus algorithm in Figure 6. It is easy to modify this algorithm, however, so that it does work in M 3 : One can adopt the techniques used in <ref> [DLS88] </ref> to mask the loss of messages that are sent before GST. Failure detectors can be viewed as a more abstract and modular way of incorporating partial synchrony assumptions into the model of computation.
Reference: [Fis83] <author> Michael J. Fischer. </author> <title> The consensus problem in unreliable distributed systems (a brief survey). </title> <type> Technical Report 273, </type> <institution> Department of Computer Science, Yale University, </institution> <month> June </month> <year> 1983. </year>
Reference-contexts: 13 This is a crash-failure version of the asynchronous broadcast primitive defined in [BT85] for "Byzantine" failures. 16 5 The Consensus problem In the Consensus problem, all correct processes propose a value and must reach a unanimous and irrevocable decision on some value that is related to the proposed values <ref> [Fis83] </ref>. We define the Consensus problem in terms of two primitives, propose (v) and decide (v), where v is a value drawn from a set of possible proposed values. <p> Thus, our algorithm for solving Consensus using 3S (or 3P) is optimal with respect to the number of failures that it tolerates. 14 The validity property captures the relation between the decision value and the proposed values. Changing this property results in other types of Consensus <ref> [Fis83] </ref>. 17 6.1 Solving Consensus using S The algorithm in Figure 5 solves Consensus using any Strong failure detector D 2 S. In other words, it works with any failure detector D that satisfies strong completeness and weak accuracy.
Reference: [FLP85] <author> Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. </author> <title> Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Although the asynchronous model of computation is attractive for the reasons outlined above, it is well known that Consensus and Atomic Broadcast cannot be solved deterministically in an asynchronous system that is subject to even a single crash failure <ref> [FLP85, DDS87] </ref>. 1 Essentially, the impossibility results for Consensus and Atomic Broadcast stem from the inherent difficulty of determining whether a process has actually crashed or is only "very slow". <p> More precisely, [CHT92] shows that 6 Indeed, no algorithm can implement such a failure detector in an asynchronous system: as we show in Section 6.2, this implementation could be used to solve Consensus in such a system, contradicting the impossibility result of <ref> [FLP85] </ref>. 6 if a failure detector D can be used to solve Consensus, then there is a distributed algorithm that transforms D into 3W 0 . Thus, in a precise sense, 3W 0 is necessary and sufficient for solving Consensus in asynchronous systems (with a majority of correct processes). <p> Our model of asynchronous computation with failure detection is patterned after the one in <ref> [FLP85] </ref>. The system consists of a set of n processes, = fp 1 ; p 2 ; : : : ; p n g. Every pair of processes is connected by a reliable communication channel. <p> Agreement: No two correct processes decide differently. Uniform validity: If a process decides v, then v was proposed by some process. 14 It is well-known that Consensus cannot be solved in asynchronous systems that are subject to even a single crash failure <ref> [FLP85, DDS87] </ref>. 6 Solving Consensus using unreliable failure detec tors We now show how to solve Consensus using each one of the eight classes of failure detectors defined in Figure 1. <p> Atomic Broadcast cannot be solved with a deterministic algorithm in asynchronous systems, even if we assume that at most one process may fail, and it can only fail by crashing. This is because Consensus has no deterministic solution in such systems <ref> [FLP85] </ref>. 2. Atomic Broadcast can be solved using randomisation or unreliable failure detectors in asynchronous systems. This is because Consensus is solvable with these techniques in such systems (for a survey of randomised Consensus algorithms, see [CD89]). Consensus can be easily reduced to Atomic Broadcast as follows [DDS87]. <p> Other examples include the non-blocking atomic commitment problem [CL94, Gue95], and a form of leader election [SM95]. Figure 9 summarises these results. 9 Related work 9.1 Partial synchrony Fischer, Lynch and Paterson showed that Consensus cannot be solved in an asynchronous system subject to crash failures <ref> [FLP85] </ref>. The fundamental reason why Consensus cannot be solved in completely asynchronous systems is the fact that, in such systems, it is impossible to reliably distinguish a process that has crashed from one that is merely very slow. In other words, Consensus is unsolvable because accurate failure detection is impossible.
Reference: [GSTC90] <author> Ajei Gopal, Ray Strong, Sam Toueg, and Flaviu Cristian. </author> <title> Early-delivery atomic broadcast. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 297-310. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1990. </year> <month> 43 </month>
Reference-contexts: The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [Gue95] <author> Rachid Guerraoui. </author> <title> Revisiting the relationship between non blocking atomic commitment and consensus. </title> <booktitle> In Proceedings of the Ninth International Workshop on Distributed Algorithms. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: TRB is not the only "natural" problem that can be solved using P but cannot be solved using 3W. Other examples include the non-blocking atomic commitment problem <ref> [CL94, Gue95] </ref>, and a form of leader election [SM95]. Figure 9 summarises these results. 9 Related work 9.1 Partial synchrony Fischer, Lynch and Paterson showed that Consensus cannot be solved in an asynchronous system subject to crash failures [FLP85].
Reference: [HM90] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <month> July </month> <year> 1990. </year>
Reference: [HT93] <author> Vassos Hadzilacos and Sam Toueg. </author> <title> Fault-tolerant broadcasts and related problems. </title> <editor> In Sape J. Mullender, editor, </editor> <booktitle> Distributed Systems, chapter 5, </booktitle> <pages> pages 97-145. </pages> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference: [HT94] <author> Vassos Hadzilacos and Sam Toueg. </author> <title> A modular approach to fault-tolerant broadcasts and related problems. </title> <type> Technical Report 94-1425, </type> <institution> Computer Science Department, Cornell University, </institution> <address> Ithaca, New York 14853, </address> <month> May </month> <year> 1994. </year> <note> Available by anonymous ftp from ftp://ftp.db.toronto.edu/pub/vassos/fault.tolerant.broadcasts.dvi.Z. An earlier version is also available in [HT93]. </note>
Reference-contexts: We assume that every message m includes a field denoted sender (m) that contains the identity of the sender, and a field with a sequence number; these two fields make every message unique. Reliable Broadcast satisfies the following properties <ref> [HT94] </ref>: Validity: If a correct process R-broadcasts a message m, then it eventually R-delivers m. Agreement: If a correct process R-delivers a message m, then all correct processes even tually R-deliver m. <p> This raises an interesting question: Are there any "natural" problems that require classes of failure detectors that are stronger than 3S? To answer this question, consider the problem of Terminating Reliable Broadcast, abbreviated here as TRB <ref> [HT94] </ref>. With TRB there is a distinguished process, the sender s, that is supposed to broadcast a single message from a set M of possible messages.
Reference: [LA87] <author> M.C. Loui and Abu-Amara. </author> <title> Memory requirements for agreement among unreliable asynchronous processes. </title> <booktitle> Advances in computing research, </booktitle> <volume> 4 </volume> <pages> 163-183, </pages> <year> 1987. </year>
Reference-contexts: The technical device that made this possible is the notion of reduction between failure detectors. 9.2 Unreliable failure detection in shared memory systems Loui and Abu-Amara showed that in asynchronous shared memory systems with atomic read/write registers, Consensus cannot be solved even if at most one process may crash <ref> [LA87] </ref>. 24 This raises the following question: can we use unreliable failure detectors to circumvent this impossibility result? Lo and Hadzilacos [LH94] showed that this is indeed possible: They gave an algorithm that solves Consensus using 3W (in shared memory systems with registers). <p> An application using such a failure detector cannot distinguish between a faulty process that really crashed, and a correct one that was forced to do so. Essentially, the Isis failure detector forces the system to conform 24 The proof in <ref> [LA87] </ref> is similar to the proof that Consensus is impossible in message-passing systems when send and receive are not part of the same atomic step [DDS87]. 40 to its view. From the application's point of view, this failure detector looks "perfect": it never makes visible mistakes.
Reference: [Lam78] <author> Leslie Lamport. </author> <title> The implementation of reliable distributed multiprocess systems. </title> <journal> Computer Networks, </journal> <volume> 2 </volume> <pages> 95-114, </pages> <year> 1978. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines <ref> [Lam78, Sch90] </ref>, Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade.
Reference: [LH94] <author> Wai Kau Lo and Vassos Hadzilacos. </author> <title> Using failure detectors to solve consensus in asynchronous shared-memory systems. </title> <booktitle> In Proceedings of the Eighth International Workshop on Distributed Algorithms, </booktitle> <pages> pages 280-295. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1994. </year> <note> Available from ftp://ftp.db.toronto.edu/pub/vassos/failure.detectors.shared.memory.ps.Z. </note>
Reference-contexts: shared memory systems Loui and Abu-Amara showed that in asynchronous shared memory systems with atomic read/write registers, Consensus cannot be solved even if at most one process may crash [LA87]. 24 This raises the following question: can we use unreliable failure detectors to circumvent this impossibility result? Lo and Hadzilacos <ref> [LH94] </ref> showed that this is indeed possible: They gave an algorithm that solves Consensus using 3W (in shared memory systems with registers).
Reference: [LSP82] <author> Leslie Lamport, Robert Shostak, and Marshall Pease. </author> <title> The Byzantine generals problem. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 382-401, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: TRB is a well-known and studied problem, usually known under the name of the Byzan-tine Generals' Problem <ref> [PSL80, LSP82] </ref>. 21 It turns out that in order to solve TRB in asynchronous systems one needs to use the strongest class of failure detectors that we defined in this paper. Specifically: Theorem 37: 1. TRB can be solved using P in asynchronous systems with any number of crashes. 2.
Reference: [MDH86] <author> Yoram Moses, Danny Dolev, and Joseph Y. Halpern. </author> <title> Cheating husbands and other stories: a case study of knowledge, action, </title> <journal> and communication. Distributed Computing, </journal> <volume> 1(3) </volume> <pages> 167-176, </pages> <year> 1986. </year>
Reference: [Mul87] <author> Sape J. Mullender, </author> <title> editor. The Amoeba distributed operating system: </title> <booktitle> Selected papers 1984 - 1987. </booktitle> <institution> Centre for Mathematics and Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba <ref> [Mul87] </ref>, Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade.
Reference: [Nei95] <author> Gil Neiger. </author> <title> Failure detectors and the wait-free hierarchy. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Principles of Distributed Computing. </booktitle> <publisher> ACM Press, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: Recently, Neiger extended the work of Lo and Hadzilacos by studying the conditions under which unreliable failure detectors boost the Consensus power of shared objects <ref> [Nei95] </ref>. 9.3 The Isis toolkit With our approach, even if a correct process p is repeatedly suspected to have crashed by the other processes, it is still required to behave like every other correct process in the system.
Reference: [NT90] <author> Gil Neiger and Sam Toueg. </author> <title> Automatically increasing the fault-tolerance of distributed algorithms. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 374-419, </pages> <month> September </month> <year> 1990. </year> <month> 44 </month>
Reference-contexts: Since 3P - 3S, this algorithm also solves Consensus using 3P. Our Consensus algorithms actually solve a stronger form of Consensus than the one specified in Section 5: They ensure that no two processes, whether correct or faulty, decide differently | a property called Uniform Agreement <ref> [NT90] </ref>. The Consensus algorithm that uses S tolerates any number of failures. In contrast, the one that uses 3S requires a majority of correct processes. We show that to solve Consensus this requirement is necessary even if one uses 3P, a class of failure detectors that is stronger than 3S.
Reference: [PBS89] <author> Larry L. Peterson, Nick C. Bucholz, and Richard D. Schlichting. </author> <title> Preserving and using context information in interprocess communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(3) </volume> <pages> 217-246, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync <ref> [PBS89] </ref>, Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade.
Reference: [PGM89] <author> Frank Pittelli and Hector Garcia-Molina. </author> <title> Reliable scheduling in a tmr database system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 25-60, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [Pow91] <editor> David Powell, editor. Delta-4: </editor> <title> A Generic Architecture for Dependable Distributed Computing. </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 <ref> [Pow91] </ref>, Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade.
Reference: [PSL80] <author> M. Pease, R. Shostak, and Leslie Lamport. </author> <title> Reaching agreement in the presence of faults. </title> <journal> Journal of the ACM, </journal> <volume> 27(2) </volume> <pages> 228-234, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: TRB is a well-known and studied problem, usually known under the name of the Byzan-tine Generals' Problem <ref> [PSL80, LSP82] </ref>. 21 It turns out that in order to solve TRB in asynchronous systems one needs to use the strongest class of failure detectors that we defined in this paper. Specifically: Theorem 37: 1. TRB can be solved using P in asynchronous systems with any number of crashes. 2.
Reference: [RB91] <author> Aleta Ricciardi and Kenneth P. Birman. </author> <title> Using process groups to implement failure detection in asynchronous environments. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 341-351. </pages> <publisher> ACM Press, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Informally, a failure detector D 0 is reducible to failure detector D if there is a distributed algorithm 2 A different approach was taken by the Isis system <ref> [RB91] </ref>: a correct process that is wrongly suspected to have crashed, is forced to leave the system. In other words, the Isis failure detector forces the system to conform to its view. To applications such a failure detector makes no mistakes. <p> Furthermore, processes are never "discriminated against" if they are falsely suspected to have crashed. Isis takes an alternative approach based on the assumption that failure detectors rarely make mistakes <ref> [RB91] </ref>. In those cases in which a correct process p is falsely suspected by the failure detector, p is effectively forced "to crash" (via a group membership protocol that removes p from all the groups that it belongs to).
Reference: [Rei82] <author> Rudiger Reischuk. </author> <title> A new solution for the Byzantine general's problem. </title> <type> Technical Report RJ 3673, </type> <institution> IBM Research Laboratory, </institution> <month> November </month> <year> 1982. </year>
Reference-contexts: In such systems, the algorithm in Figure 6 solves Consensus using any Eventual Strong failure detector D 2 3S. In other words, it works with any failure detector D that satisfies strong completeness and eventual weak accuracy. This algorithm uses the rotating coordinator paradigm <ref> [Rei82, CM84, DLS88, BGP89, CT90] </ref>, and it proceeds in asynchronous "rounds". We assume that all processes have a priori knowledge that during round r, the coordinator is process c = (r mod n) + 1. All messages are either to or from the "current" coordinator. <p> In 15 In the literature, t is often used instead of f, the notation adopted here. In this paper, we reserve t to denote real-time. 16 Many Consensus algorithms in the literature have the property that a value gets locked before processes decide, e.g. <ref> [Rei82, DLS88] </ref>. 21 Every process p executes the following: procedure propose (v p ) estimate p v p festimate p is p's estimate of the decision valueg state p undecided r p 0 fr p is p's current round numberg ts p 0 fts p is the last round in which
Reference: [Sch90] <author> Fred B. Schneider. </author> <title> Implementing fault-tolerant services using the state machine approach: A tutorial. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(4) </volume> <pages> 299-319, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT [WLG + 78], State Machines <ref> [Lam78, Sch90] </ref>, Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade. <p> The total order and agreement properties of Atomic Broadcast ensure that all correct processes deliver the same sequence of messages. Atomic Broadcast is a powerful communication paradigm for fault-tolerant distributed computing <ref> [CM84, CASD85, BJ87, PGM89, BGT90, GSTC90, Sch90] </ref>. 27 We now show that Consensus and Atomic Broadcast are equivalent in asynchronous systems with crash failures. This is shown by reducing each to the other. 17 In other words, a solution for one automatically yields a solution for the other.
Reference: [SM95] <author> Laura Sabel and Keith Marzullo. </author> <title> Election vs. consensus in asynchronous systems. </title> <type> Technical Report TR95-411, </type> <institution> University of California at San Diego, </institution> <month> February </month> <year> 1995. </year> <note> Available at ftp://ftp.cs.cornell.edu/pub/sabel/tr94-1413.ps. </note>
Reference-contexts: TRB is not the only "natural" problem that can be solved using P but cannot be solved using 3W. Other examples include the non-blocking atomic commitment problem [CL94, Gue95], and a form of leader election <ref> [SM95] </ref>. Figure 9 summarises these results. 9 Related work 9.1 Partial synchrony Fischer, Lynch and Paterson showed that Consensus cannot be solved in an asynchronous system subject to crash failures [FLP85].
Reference: [WLG + 78] <author> John H. Wensley, Leslie Lamport, Jack Goldberg, Milton W. Green, Karl N. Levitt, P.M. Melliar-Smith, Robert E. Shostak, and Charles B. Weinstock. SIFT: </author> <title> Design and analysis of a fault-tolerant computer for aircraft control. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(10) </volume> <pages> 1240-1255, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: Atomic Broadcast allows processes to reliably broadcast messages, so that they agree on the set of messages they deliver and the order of message deliveries. Applications based on these paradigms include SIFT <ref> [WLG + 78] </ref>, State Machines [Lam78, Sch90], Isis [BJ87, BCJ + 90], Psync [PBS89], Amoeba [Mul87], Delta-4 [Pow91], Transis [ADKM91], HAS [Cri87], FAA [CDD90], and Atomic Commitment. Given their wide applicability, Consensus and Atomic Broadcast have been extensively studied by both theoretical and experimental researchers for over a decade.
References-found: 46

