URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P545.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Email: bischof@mcs.anl.gov  xiaobai@cs.duke.edu  
Title: On Tridiagonalizing and Diagonalizing Symmetric Matrices with Repeated Eigenvalues  
Author: Christian H. Bischof Xiaobai Sun 
Date: #25  
Note: Argonne Preprint MCS-P545-1095 PRISM Working Note  
Address: Argonne, IL 60439-4801  Durham, NC 27708-0129  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  Department of Computer Science Duke University  
Abstract: We describe a divide-and-conquer tridiagonalization approach for matrices with repeated eigenvalues. Our algorithm hinges on the fact that, under easily constructively verifiable conditions, a symmetric matrix with bandwidth b and k distinct eigenvalues must be block diagonal with diagonal blocks of size at most bk. A slight modification of the usual orthogonal band-reduction algorithm allows us to reveal this structure, which then leads to potential parallelism in the form of independent diagonal blocks. Compared with the usual Householder reduction algorithm, the new approach exhibits improved data locality, significantly more scope for parallelism, and the potential to reduce arithmetic complexity by close to 50% for matrices that have only two numerically distinct eigenvalues. The actual improvement depends to a large extent on the number of distinct eigenvalues and a good estimate thereof. However, at worst the algorithm behaves like a successive bandreduction approach to tridiagonalization. Moreover, we provide a numerically reliable and effective algorithm for computing the eigenvalue decomposition of a symmetric matrix with two numerically distinct eigenvalues. Such matrices arise, for example, in invariant subspace decomposition approaches to the symmetric eigenvalue problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: As a result, this algorithm exhibits improved data locality and hence is likely to be preferable on cache-based architectures. This block algorithm has been incorporated into the LAPACK library of portable linear algebra codes for high-performance architectures <ref> [1, 2] </ref>. Parallel versions for distributed-memory machines of the standard algorithm and of the block algorithm are described in [12] and in [13], respectively. A different approach to tridiagonalization is the so-called successive band reduction (SBR) method, which completes the tridiagonal reduction through a sequence of band reductions [10,7]. <p> As we show below, the fill-ins generated by these sweeps are of the same order as the perturbation in the eigenvalues and hence can be considered negligible. Lemma 4 Let T be a symmetric tridiagonal matrix with (T ) ae [; ] [ <ref> [1; 1+] </ref>; , where def = max 2 (T) fmin (j1j; jj)g o 1. Then kT 2 T k 2 where = + 2 : (5) Proof.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide Release 2.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: As a result, this algorithm exhibits improved data locality and hence is likely to be preferable on cache-based architectures. This block algorithm has been incorporated into the LAPACK library of portable linear algebra codes for high-performance architectures <ref> [1, 2] </ref>. Parallel versions for distributed-memory machines of the standard algorithm and of the block algorithm are described in [12] and in [13], respectively. A different approach to tridiagonalization is the so-called successive band reduction (SBR) method, which completes the tridiagonal reduction through a sequence of band reductions [10,7].
Reference: [3] <author> Louis Auslander and Anna Tsao. </author> <title> A divide-and-conquer algorithm for the eigenproblem via complementary invariant subspace decomposition. </title> <type> Technical Report SRC-TR-89-003, </type> <institution> Supercomputing Research Center, Institute for Defense Analysis, Bowie, Maryland, </institution> <year> 1989. </year>
Reference-contexts: One particularly intriguing case is matrices that have only two eigenvalues, which arise in eigensolvers based on variant subspace decompositions <ref> [3, 19, 4] </ref>. We may assume without loss of generality that the eigenvalues are at 1 and 0 (any other two eigenvalues can be mapped to 0 and 1 by shifting and scaling). The following corollary is a special case of Theorem 1.
Reference: [4] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric toolbox, part i. </title> <type> Technical Report 92-09, </type> <institution> Department of Mathematics, University of Kentucky, </institution> <year> 1992. </year>
Reference-contexts: One particular situation where repeated eigenvalues arise is in the context of invariant-subspace methods for eigenvalue problems [3,19,6,4], where a matrix with only two distinct, predetermined, eigenvalues is generated either by repeated application of incomplete beta functions [19] or the matrix sign function <ref> [4] </ref>. In exact arithmetic, our tridiagonalization procedure would result in a block diagonal matrix with diagonal blocks of order no larger than 2. Hence the eigenvalue decomposition could be computed easily by independently diagonalizing the 2 fi2 blocks on the diagonal. <p> One particularly intriguing case is matrices that have only two eigenvalues, which arise in eigensolvers based on variant subspace decompositions <ref> [3, 19, 4] </ref>. We may assume without loss of generality that the eigenvalues are at 1 and 0 (any other two eigenvalues can be mapped to 0 and 1 by shifting and scaling). The following corollary is a special case of Theorem 1.
Reference: [5] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Structured second- and higher-order derivatives through univariate Taylor series. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 211-232, </pages> <year> 1993. </year>
Reference-contexts: This algorithm employs mainly matrix-vector multiplications and symmetric rank-one updates, which require more memory references than the matrix-matrix operations [9,8,14]. The block tridiagonalization algorithm in <ref> [5, 15] </ref> combines sets of p successive symmetric rank-1 updates into one symmetric rank-p update, at the cost of O (2pn 2 ) extra flops. As a result, this algorithm exhibits improved data locality and hence is likely to be preferable on cache-based architectures.
Reference: [6] <author> Christian Bischof, Steven Huss-Lederman, Xiaobai Sun, and Anna Tsao. </author> <title> The PRISM project: Infrastructure and algorithms for parallel eigensolvers. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 123-131, </pages> <address> Washington, D.C., 1994. </address> <publisher> IEEE Computer Society. </publisher>
Reference: [7] <author> Christian Bischof, Xiaobai Sun, and Bruno Lang. </author> <title> Parallel tridiagonalization through two-step band reduction. </title> <booktitle> In Proceedings of Scalable High Performance Computing Conference, </booktitle> <pages> pages 23-27. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference: [8] <author> Christian H. Bischof. </author> <title> Fundamental Linear Algebra Computations on High-Performance Computers, </title> <booktitle> volume 250 of Informatik Fachberichte, </booktitle> <pages> pages 167-182. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference: [9] <author> Christian H. Bischof and Jack J. Dongarra. </author> <title> A project for developing a linear algebra library for high-performance computers. </title> <editor> In Graham Carey, editor, </editor> <booktitle> Parallel and Vector Supercomputing: Methods and Algorithms, </booktitle> <pages> pages 45-56. </pages> <publisher> John Wiley & Sons, </publisher> <address> Somerset, N.J., </address> <year> 1989. </year>
Reference: [10] <author> Christian H. Bischof and Xiaobai Sun. </author> <title> A framework for band reduction and tridiagonalization of symmetric matrices. </title> <type> Preprint MCS-P298-0392, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In addition, we can work in parallel on independent problems. If the estimate k of the number of distinct eigenvalues is inaccurate, the algorithm becomes either the standard eigenvalue algorithm (for k &gt; n=2) or the SBR tridiagonalization procedure suggested in <ref> [10] </ref>, but in either case, it will return numerically accurate results. 4 Invariant Subspace Splitting The computational cost and the degree of parallelism in the algorithm depend on k, the number of distinct eigenvalues.
Reference: [11] <author> Christian H. Bischof and Charles F. Van Loan. </author> <title> The WY representation for products of Householder matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8:s2-s13, </volume> <year> 1987. </year>
Reference-contexts: Since our algorithm (at least in the early stages) reduces matrices to banded form with a relatively wide band, it is easy to block the Householder transformations by using the WY representation <ref> [11] </ref> or the compact WY representation [20], as has, for example, been described in [17]. In this fashion, one can easily capitalize on the favorable memory transfer characteristics of block algorithms. 19
Reference: [12] <author> H. Chang, S. Utku, M Salama, and D. Rapp. </author> <title> A parallel Householder tridiagonalization stratagem using scattered square decomposition. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 297-311, </pages> <year> 1988. </year>
Reference-contexts: This block algorithm has been incorporated into the LAPACK library of portable linear algebra codes for high-performance architectures [1, 2]. Parallel versions for distributed-memory machines of the standard algorithm and of the block algorithm are described in <ref> [12] </ref> and in [13], respectively. A different approach to tridiagonalization is the so-called successive band reduction (SBR) method, which completes the tridiagonal reduction through a sequence of band reductions [10,7]. This approach leads to algorithms that exhibit an even greater degree of memory locality, among other desirable features.
Reference: [13] <author> J. Dongarra and R. van de Geijn. </author> <title> Reduction to condensed form for the eigenvalue problem on distributed-memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 973-982, </pages> <year> 1992. </year> <month> 20 </month>
Reference-contexts: This block algorithm has been incorporated into the LAPACK library of portable linear algebra codes for high-performance architectures [1, 2]. Parallel versions for distributed-memory machines of the standard algorithm and of the block algorithm are described in [12] and in <ref> [13] </ref>, respectively. A different approach to tridiagonalization is the so-called successive band reduction (SBR) method, which completes the tridiagonal reduction through a sequence of band reductions [10,7]. This approach leads to algorithms that exhibit an even greater degree of memory locality, among other desirable features.
Reference: [14] <author> Jack Dongarra and Sven Hammarling. </author> <title> Evolution of Numerical Software for Dense Linear Algebra, </title> <address> pages 297-327. </address> <publisher> Oxford University Press, Oxford, </publisher> <address> UK, </address> <year> 1989. </year>
Reference: [15] <author> Jack J. Dongarra, Sven J. Hammarling, and Danny C. Sorensen. </author> <title> Block reduction of matrices to condensed form for eigenvalue computations. </title> <type> Technical Report ANL/MCS-TM-99, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: This algorithm employs mainly matrix-vector multiplications and symmetric rank-one updates, which require more memory references than the matrix-matrix operations [9,8,14]. The block tridiagonalization algorithm in <ref> [5, 15] </ref> combines sets of p successive symmetric rank-1 updates into one symmetric rank-p update, at the cost of O (2pn 2 ) extra flops. As a result, this algorithm exhibits improved data locality and hence is likely to be preferable on cache-based architectures.
Reference: [16] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. 2nd ed. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Our goal is to compute an orthogonal-tridiagonal decomposition of A, AQ = QT , where Q is orthogonal and T is tridiagonal. Reduction to tridiagonal form is a standard preprocessing step in dense eigensolvers based on QR iteration, bisection, or Cuppen's method <ref> [16] </ref>. <p> Our goal is to compute an orthogonal-tridiagonal decomposition of A, AQ = QT , where Q is orthogonal and T is tridiagonal. Reduction to tridiagonal form is a standard preprocessing step in dense eigensolvers based on QR iteration, bisection, or Cuppen's method [16]. The conventional tridiagonalization procedure <ref> [16, p. 419] </ref> reduces A one column at a time through a Householder transformation at a cost of O (4n 3 =3) flops for the reduction of A, and an fl This work was supported by the Advanced Research Projects Agency, under contract DM28E04120 and P-95006, and the Mathematical, Information, and
Reference: [17] <author> Roger G. Grimes and Horst D. Simon. </author> <title> Solution of large, dense symmetric generalized eigenvalue problems using secondary storage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(3) </volume> <pages> 241-256, </pages> <year> 1988. </year>
Reference-contexts: Since our algorithm (at least in the early stages) reduces matrices to banded form with a relatively wide band, it is easy to block the Householder transformations by using the WY representation [11] or the compact WY representation [20], as has, for example, been described in <ref> [17] </ref>. In this fashion, one can easily capitalize on the favorable memory transfer characteristics of block algorithms. 19
Reference: [18] <author> Alston S. </author> <title> Householder. The Theory of Matrices in Numerical Analysis. </title> <publisher> Dover Publications, Inc., </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: Numerical experiments with a Matlab implementation are reported in Section 5. Lastly, we summarize our results. 2 The Structure of Band Matrices with Repeated Eigenvalues A tridiagonal matrix whose off-diagonal entries are all nonzero is called unreduced. It is well known <ref> [18, p. 66] </ref> that an unreduced tridiagonal matrix does not have multiple eigenvalues. Consequently, if an nfin tridiagonal matrix has only k o n distinct eigenvalues, it must be block diagonal, and the largest block cannot be larger than kfik.
Reference: [19] <author> Steven Lederman, Anna Tsao, and Thomas Turnbull. </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues. </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center, Institute for Defense Analysis, Bowie, Maryland, </institution> <year> 1991. </year>
Reference-contexts: In addition, the need for data movement is reduced. One particular situation where repeated eigenvalues arise is in the context of invariant-subspace methods for eigenvalue problems [3,19,6,4], where a matrix with only two distinct, predetermined, eigenvalues is generated either by repeated application of incomplete beta functions <ref> [19] </ref> or the matrix sign function [4]. In exact arithmetic, our tridiagonalization procedure would result in a block diagonal matrix with diagonal blocks of order no larger than 2. Hence the eigenvalue decomposition could be computed easily by independently diagonalizing the 2 fi2 blocks on the diagonal. <p> One particularly intriguing case is matrices that have only two eigenvalues, which arise in eigensolvers based on variant subspace decompositions <ref> [3, 19, 4] </ref>. We may assume without loss of generality that the eigenvalues are at 1 and 0 (any other two eigenvalues can be mapped to 0 and 1 by shifting and scaling). The following corollary is a special case of Theorem 1.
Reference: [20] <author> Robert Schreiber and Charles Van Loan. </author> <title> A storage efficient WY representation for products of Householder transformations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(1) </volume> <pages> 53-57, </pages> <year> 1989. </year>
Reference-contexts: Since our algorithm (at least in the early stages) reduces matrices to banded form with a relatively wide band, it is easy to block the Householder transformations by using the WY representation [11] or the compact WY representation <ref> [20] </ref>, as has, for example, been described in [17]. In this fashion, one can easily capitalize on the favorable memory transfer characteristics of block algorithms. 19
References-found: 20

