URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1995/4.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1995/SCSE_publications.html
Root-URL: 
Title: w w Schematic layouts: above Elman net right: Elman tower state vector hidden layer s
Keyword: w  
Date: 142-145  
Note: To appear in Proceedings of the Sixth Australian Conference on Neural Networks (ACNN95)  w 1  
Abstract: Stability of Learning in Classes of The relationship to work by Mozer, [5], on induction of temporal structure, is briefly described in [13]. Recurrent and Feedforward Networks In the research reported here, the task is similar to Elmans: predicting the next letter in a word (or the end of William H. Wilson the word) from the current letter and the representation of past letters held in the state vector. While the original billw@cse.unsw.edu.au motivation for this task was linguistic [12], the current paper focuses on the efficacy of a range of network School of Computer Science and Engineering architectures, and learning regimes, applied to the task. University of New South Wales Sydney 2052 Australia This paper concerns a class of recurrent neural networks related to Elman networks (simple recurrent networks) and Jordan networks and a class of feedforward networks architecturally similar to Waibels TDNNs. The recurrent nets used herein, unlike standard Elman/Jordan networks, may have more than one state vector. It is known that such multi-state Elman networks have better learning performance on certain tasks than standard Elman networks of similar weight complexity. The task used involves learning the graphotactic structure of a sample of about 400 English words. Learning performance was tested using regimes in which the state vectors are, or are not, zeroed between words: the former results in larger minimum total error, but without the large oscillations in total error observed when the state vectors are not periodically zeroed. Learning performance comparisons of the three classes of network favour the feedforward nets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Elman, Jeffrey L., </author> <title> Representation and structure in connectionist models, </title> <type> TRL Technical Report 8903, </type> <institution> Centre for Research in Language, Univ. of California, Figure 7: Global Minima for 10 runs of a range of San Diego, La Jolla, </institution> <address> CA 92093 (1989) 26 pages. </address> <note> Elman, and Elman tower architectures, from [13]. </note>
Reference: [2] <author> Elman, Jeffrey L., </author> <title> Finding structure in time, State vectors were not zeroed between words. Cognitive Science 14 (1990) 179-211. though the number of weights is held constant), while for [3] Jordan, M.I. (1986) Attractor dynamics and parallelism in a connectionist sequential machine, Proceedings of zeroed state vectors/past inputs, the absolute minimum the Eighth Annual Meeting of the Cognitive Science error occurs for 2 or 3 states/past inputs. </title> <publisher> Society, </publisher> <address> Hillsdale, NJ: </address> <note> Erlbaum. Figure 5 shows that TDNNs found somewhat better </note>
Reference: [4] <author> Lang, K.J., Waibel, A.H., and Hinton, G.E., </author> <title> A time-minima than Elman towers (in fact, they also did better delay neural network architecture for isolated word than Jordan towers). recognition, Neural Networks 3 (1990) 23-43. Figures 5 and 7 also permit the observation that while </title>
Reference-contexts: Further details are in [1,2] and a summary erratic, as illustrated in Figure 2. from the point of view of the current work is in [13]. Waibel [10], and Lang and Waibel <ref> [4] </ref>, devised nonrecurrent timedelay neural networks (TDNNs) which learnt speech recognition tasks by considering past as well as present inputs. 142 ACNN95 10009008007006005004003002001000 850 950 1050 1150 Epoch T S S 1 state vector 2 state vectors 4 state vectors 7 state vectors 2 Experiments with Tower-Recurrent Networ ks Let us

Reference: [7] <author> Rumelhart, D.E.& McClelland, </author> <title> J.L Explorations in vectors: there is still a little instability for Jordan towers, Parallel Distributed Processing, book with software, but not for Elman towers. However, the learning is poorer Cambridge, </title> <address> MA: </address> <publisher> MIT Press, </publisher> <year> 1989. </year> <title> than with nonzeroed state vectors/past inputs. </title>
Reference-contexts: Compare Figure 7. word given an initial string of letters in the word [13]. 10009008007006005004003002001000 600 1000 not zeroed zeroed Epoch T S Two training regimes were used (cf. <ref> [7] </ref>): in training a net to predict next letters in, say, the English word cat, one presents the pattern for c as input and the pattern for a as output (c fi a ), next, (a fi t) and finally (t fi &lt;end-of-word&gt;).
Reference: [8] <author> Sejnowski, T.J. and Rosenberg, C.R., </author> <title> Parallel 10009008007006005004003002001000 700 900 1100 TDNN Elman Jordan Epoch E r r o r networks that learn to pronounce English text, </title> <booktitle> Complex Systems 1 (1987) 145-168. </booktitle>
Reference-contexts: and are trained by a backpropagation-through-time type algorithm ([11]).) Such nets are analogous to Elman and Jordan nets and towers, with a feedback connection from the input layer to the state vectors, which are thus better termed past inputs. (These nets seem to remind many people of the NETtalk architecture <ref> [8] </ref>.
Reference: [9] <author> Servan-Shreiber, D., Cleeremans, A. & McClelland, J.L., </author> <title> Learning sequential structure in simple recurrent networks, </title> <booktitle> in Advances in Neural Information Processing 1, edited by D.S. Touretzky, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989 </year>
Reference: [10] <author> Waibel, Alex, </author> <title> Modular construction of timedelay neural networks for speech recognition, N e u r a l Computation 1 (1989) 39-46. </title>
Reference-contexts: Further details are in [1,2] and a summary erratic, as illustrated in Figure 2. from the point of view of the current work is in [13]. Waibel <ref> [10] </ref>, and Lang and Waibel [4], devised nonrecurrent timedelay neural networks (TDNNs) which learnt speech recognition tasks by considering past as well as present inputs. 142 ACNN95 10009008007006005004003002001000 850 950 1050 1150 Epoch T S S 1 state vector 2 state vectors 4 state vectors 7 state vectors 2 Experiments with
Reference: [11] <author> Werbos, P.J., </author> <title> Backpropagation through time: What it is and how to do it, </title> <booktitle> IEEE Proceedings 78 (1990) 1550-1560. </booktitle>
Reference: [12] <author> Wilson, William H., </author> <title> Dealing with unknown words: classifying unknown letterstrings using trigram with 6 state vectors/past inputs, zeroed state vectors. </title> <address> ations 14(1) (1992) 981-988. </address>
Reference-contexts: The minimum error averaged over 10 runs with zeroed state type, with 6 state vectors/past inputs, and nonzeroed average minimum was 549 (153 less). state vectors. 1 the measure of learning speed used in <ref> [12] </ref>, namely epoch It can be seen that for nonzeroed state vectors, the error number of the first local minimum, is of course inapprop steadily decreases as more state vectors are added (even riate with smoothly descending error curves. 144 ACNN95 76543210 810 830 850 870 890 910 930 State Vectors
Reference: [13] <author> Wilson, William H., </author> <title> A Comparison of Architectural 5 Conc lusi ons Alternatives for Recurrent Networks, Proceedings of The results obtained might be different with a different the Fourth Australian Conference on Neural Networks (ACNN93) (1993) 189-192. task. However, on this graphotactic prediction task, </title> <booktitle> in ACNN95 145 </booktitle>
Reference-contexts: Elman [1,2], and at sequential tasks. We term such networks Elman tower of nets, now termed simple recurrent nets, or Elman nets, networks. This possibility was confirmed in <ref> [13] </ref>, where it which differed from Jordan nets in that the state vector is a was shown that among networks with (about) the same copy of the hidden layer in the previous time step, as number of weights, but different numbers of state vectors, illustrated in Figure 1, left side. <p> His specific task hidden units and 7 state vectors (1925 weights) found was to predict the next word in the sentence from the weight values which gave total error of around 815 units. current word and the representation of past words held in However, the learning performance reported in <ref> [13] </ref>, was the state vector. Further details are in [1,2] and a summary erratic, as illustrated in Figure 2. from the point of view of the current work is in [13]. <p> of around 815 units. current word and the representation of past words held in However, the learning performance reported in <ref> [13] </ref>, was the state vector. Further details are in [1,2] and a summary erratic, as illustrated in Figure 2. from the point of view of the current work is in [13]. <p> In NETtalk, however, the output was a phoneme to pronounce, and the inputs were a window of letters on either side of the current letter.) Figure 2: Typical Error Plots for a range of Elman, The aim of the experiments was to compare networks and Elman tower architectures, from <ref> [13] </ref>. differing in numbers of state vectors, Jordan-type vs This paper describes three types of advance on the work Elman-type vs TDNNs, and training regime, while holding reported in [13]: (i) a learning regime which largely network complexity otherwise constant. <p> Error Plots for a range of Elman, The aim of the experiments was to compare networks and Elman tower architectures, from <ref> [13] </ref>. differing in numbers of state vectors, Jordan-type vs This paper describes three types of advance on the work Elman-type vs TDNNs, and training regime, while holding reported in [13]: (i) a learning regime which largely network complexity otherwise constant. Many removes the erratic learning performance; (ii) results with architectural parameters were used as factors in the Jordan networks and towers, and with TDNNs - see experiments, but it was still possible to hold constant the capacity. <p> The number of weights and biases in an Elman tower net with n inputs and outputs, h hidden units and s state vectors was computed in <ref> [13] </ref> to be w Elman (n, h, s) = 2nh + h + n + sh 2 . For Jordan towers and TDNNs, the formulae are: w Jordan (n, h, s) = 2nh + h + n + shn w TDNN (n, h, s) =w Jordan (n, h, s). <p> The task using zeroed state vectors for Elman towers, and using used was that of predicting the next letter in an English zeroed past inputs for TDNNs. Compare Figure 7. word given an initial string of letters in the word <ref> [13] </ref>. 10009008007006005004003002001000 600 1000 not zeroed zeroed Epoch T S Two training regimes were used (cf. [7]): in training a net to predict next letters in, say, the English word cat, one presents the pattern for c as input and the pattern for a as output (c fi a ), next, <p> There are arguments for both policies: briefly, the leftover activations are irrelevant to next-letter prediction in the next word, but it is hard to postulate a biological process which inactivates selected neurons which happen to have recurrent connections to them. In <ref> [13] </ref>, state vectors were not zeroed at the end of words: in some of the simulations reported here, they were. regime, for a 6-state Elman tower (27 hidden units). <p> In <ref> [13] </ref> and Figure 2, greatest oscillation occurred with large numbers of states, hence the choice of 6-state-vector architectures: similar effects, perhaps less pronounced, occur for fewer state vectors and for Jordan towers.
References-found: 10

