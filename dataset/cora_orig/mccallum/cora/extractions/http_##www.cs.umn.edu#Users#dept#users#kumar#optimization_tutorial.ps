URL: http://www.cs.umn.edu/Users/dept/users/kumar/optimization_tutorial.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: ananth@cs.umn.edu and kumar@cs.umn.edu  pardalos@math.ufl.edu  
Title: Parallel Processing of Discrete Optimization Problems  
Author: Grama Y. Ananth and Vipin Kumar and Panos Pardalos 
Address: Minneapolis, MN 55455  Gainesville, FL 32611  
Affiliation: Department of Computer Science, University of Minnesota  Department of Industrial and Systems Engineering University of Florida  
Abstract: Discrete optimization problems (DOPs) arise in various applications such as planning, scheduling, computer aided design, robotics, game playing and constraint directed reasoning. Often, a DOP is formulated in terms of finding a (minimum cost) solution path in a graph from an initial node to a goal node and solved by graph/tree search methods such as branch-and-bound and dynamic programming. Availability of parallel computers has created substantial interest in exploring the use of parallel processing for solving discrete optimization problems. This article provides an overview of parallel search algorithms for solving discrete optimization problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> AAAI-88 Workshop on Parallel Algorithms for Machine Intelligence, </institution> <address> St. Paul, Minneapo-lis, </address> <month> August </month> <year> 1988. </year> <editor> Organizers: V. Kumar, L. Kanal, P.S. </editor> <booktitle> Gopalakrishnan; Sponsored by American Association for Artificial Intelligence. </booktitle>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [2] <institution> IJCAI-89 Workshop on Parallel Algorithms for Machine Intelligence, </institution> <address> Detroit, Michigan, </address> <month> 20 August </month> <year> 1989. </year> <editor> Organizers: V. Kumar, L. Kanal, P.S. </editor> <booktitle> Gopalakrishnan; Sponsored by American Association for Artificial Intelligence. </booktitle>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [3] <author> T.S. Abdelrahman and T.N. Mudge. </author> <title> Parallel branch and bound algorithms on hypercube multiprocessors. </title> <booktitle> In Proceedings of the 3rd Conference on Hypercube Concurrent Computers and Applications Volume II, </booktitle> <publisher> ACM Press, </publisher> <pages> pages 1492-1499, </pages> <year> 1988. </year>
Reference-contexts: Integer linear programming problems appear in many applications and are solved, in general, by some branch and bound type algorithm [58], [65]. 12 In this section, we describe some of the first attempts to parallelize these branch and bound algorithms for integer linear programs. In <ref> [3] </ref> the 0-1 integer linear program min f (x) = c T x is considered. We may assume that all cost coefficients c i are nonnegative (if some c i &lt; 0 then we may replace x i by 1 x i ). <p> The processors continue to iterate until the list of active subproblems becomes empty. The algorithm then terminates and the solution is stored at the incumbent. Abdelrahman and Mudge <ref> [3] </ref> propose two parallelization methods on a distributed memory multiprocessor. The first method maintains a centralized list of subproblems (using N slave processes) and a manager (master process). The master process maintains the global data, and the slave processes perform the operations necessary for the expansion of subproblems. <p> The second method outperforms the first by distributing the list of subproblems and balancing the load among neighboring processors. When all neighboring processors are idle, the algorithm "guesses" to terminate (for details see <ref> [3] </ref>). The two methods are also capable of incorporating multiple search strategies. Computational results on an NCUBE/six multiprocessor are reported. In [11] a methodology is proposed which uses a collection of workstations connected by an Ethernet network as a parallel processor for solving 0-1 linear problems.
Reference: [4] <author> A. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: The essence of many DP algorithms lies in computing solutions to the smallest subproblems and storing the results and using them to compute solution to bigger problems. Thus the solution to the original problem is constructed in a bottom-up fashion <ref> [4] </ref>. We will illustrate this algorithm using the classical example of finding the multiplication sequence of a sequence of matrices so that the total number of operations (individual multiply / add operations) is minimized [4]. <p> Thus the solution to the original problem is constructed in a bottom-up fashion <ref> [4] </ref>. We will illustrate this algorithm using the classical example of finding the multiplication sequence of a sequence of matrices so that the total number of operations (individual multiply / add operations) is minimized [4]. Let the given matrices be represented as M 1 ; M 2 ; ::::; M n . Also let m ij denote the number of operations required to multiply matrices M i through M j in sequence.
Reference: [5] <author> S. Arvindam, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Floorplan optimization on multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Computer Design (ICCD-89), </booktitle> <year> 1989. </year> <note> Also published as MCC Tech Report ACT-OODS-241-89. </note>
Reference-contexts: For these applications, parallel processing is perhaps the only way to obtain acceptable performance. For some problems, optimal solutions are highly desirable, and can be obtained for moderate sized instances in a reasonable amount of time using parallel search techniques (e.g. VLSI floor-plan optimization <ref> [5] </ref>). Parallel computers containing thousands of processing elements are now commercially available. The cost of these machines is similar to that of large mainframes, but they offer significantly more raw computing power. <p> Note that if a processor's current best solution path is worse than the global best solution path, then it only affects the efficiency of the search but not its correctness. Parallel formulations of DFBB have been shown to yield linearly increasing speedups for many problems and architectures <ref> [5, 6] </ref>. 9 Parallel Formulations of IDA*. There are two approaches to parallelizing IDA*. In one approach, different processors work on different iterations of IDA* independently [31, 35]. This approach is not very suitable for finding optimal solutions. <p> For example, Kumar et. al. present parallel formulations of depth-first search, depth-first branch-and-bound, IDA*, and A* algorithms. They experimentally evaluate them in the context of problems such as optimizing floorplan of a VLSI chip <ref> [5] </ref>, generating test patterns for combinatorial circuits [7] and tautology verification [34, 21, 6] on various machines including a 1024 processor nCUBE2, 128 processor Intel Hypercube, Symult 2010, a 128 processor BBN Butterfly T M , and a network of 16 SUN T M workstations.
Reference: [6] <author> S. Arvindam, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Efficient parallel algorithms for search problems: Applications in vlsi cad. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Note that if a processor's current best solution path is worse than the global best solution path, then it only affects the efficiency of the search but not its correctness. Parallel formulations of DFBB have been shown to yield linearly increasing speedups for many problems and architectures <ref> [5, 6] </ref>. 9 Parallel Formulations of IDA*. There are two approaches to parallelizing IDA*. In one approach, different processors work on different iterations of IDA* independently [31, 35]. This approach is not very suitable for finding optimal solutions. <p> For example, Kumar et. al. present parallel formulations of depth-first search, depth-first branch-and-bound, IDA*, and A* algorithms. They experimentally evaluate them in the context of problems such as optimizing floorplan of a VLSI chip [5], generating test patterns for combinatorial circuits [7] and tautology verification <ref> [34, 21, 6] </ref> on various machines including a 1024 processor nCUBE2, 128 processor Intel Hypercube, Symult 2010, a 128 processor BBN Butterfly T M , and a network of 16 SUN T M workstations. Bixby [9] presents a parallel branch and cut algorithm for solving the symmetric traveling salesman problem.
Reference: [7] <author> S. Arvindam, Vipin Kumar, V. Nageshwara Rao, and Vineet Singh. </author> <title> Automatic test pattern generation on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 17, number 12 </volume> <pages> 1323-1342, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree <ref> [7] </ref>. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms [35, 37, 43, 48, 64, 70, 77, 80]. <p> For example, Kumar et. al. present parallel formulations of depth-first search, depth-first branch-and-bound, IDA*, and A* algorithms. They experimentally evaluate them in the context of problems such as optimizing floorplan of a VLSI chip [5], generating test patterns for combinatorial circuits <ref> [7] </ref> and tautology verification [34, 21, 6] on various machines including a 1024 processor nCUBE2, 128 processor Intel Hypercube, Symult 2010, a 128 processor BBN Butterfly T M , and a network of 16 SUN T M workstations.
Reference: [8] <author> R. Bellman and S. Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: Problems which can be solved efficiently using DP are characterized by the Principle of Optimality defined by Bellman <ref> [8] </ref>. This principle states that an optimal sequence of decisions has the property that irrespective of the initial state and decision, the remaining decisions must constitute an optimal decision sequence with respect to the state resulting from the first decision.
Reference: [9] <author> Robert Bixby. </author> <title> Two applications of linear programming. </title> <booktitle> In Proceedings of the Workshop On Parallel Computing of Discrete Optimization Problems, </booktitle> <year> 1991. </year>
Reference-contexts: Bixby <ref> [9] </ref> presents a parallel branch and cut algorithm for solving the symmetric traveling salesman problem. He also presents solutions of the LP relaxations of airline crew-scheduling models.
Reference: [10] <author> R.E. Burkard and U.Derigs. </author> <title> Assignment and matching problems: Solution methods with fortran programs. </title> <booktitle> Lecture Notes in Economics and Mathematical Systems, </booktitle> <volume> Vol 184, </volume> <year> 1980. </year> <month> 21 </month>
Reference-contexts: The algorithm consists of three steps. In the first step, the algorithm computes an initial best known upper bound (BKUB) and sets up an initial branch-and-bound search tree, The initial best known upper bound is computed by using the heuristic algorithm described in <ref> [10] </ref>. Then the initial search tree consists of n fi (n 1) nodes storing partial permutations p defined by p (1) = i; p (2) = j; for i; j = 1; 2; : : :; n and i 6= j; i.e., those permutations whose first two assignments are fixed.
Reference: [11] <author> T.L. Cannon and K.L. Hoffman. </author> <title> Large scale 0-1 linear programming on distributed work-stations. </title> <type> Working Paper, </type> <institution> Dept. of Operations Research, George Mason University, Febru-ary, </institution> <year> 1989. </year>
Reference-contexts: When all neighboring processors are idle, the algorithm "guesses" to terminate (for details see [3]). The two methods are also capable of incorporating multiple search strategies. Computational results on an NCUBE/six multiprocessor are reported. In <ref> [11] </ref> a methodology is proposed which uses a collection of workstations connected by an Ethernet network as a parallel processor for solving 0-1 linear problems.
Reference: [12] <author> C.Roucairol. </author> <title> A parallel branch and bound algorithm for the quadratic assignment problem. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 18 </volume> <pages> 211-225, </pages> <year> 1987. </year>
Reference-contexts: Besides applications in facility allocation, this model can be applied in many other applications, including backboard wiring, machine scheduling, ordering interrelated data on a computer storage device, scheduling of economic lot sizes, and designing typewriter keyboard [45]. Next, we discuss a parallel exact algorithms for the QAP. Roucairol <ref> [12] </ref> first proposed a parallel branch-and-bound algorithm for solving the QAP and implemented the algorithm on a CRAY XMP/48. However her computational results were not very satisfactory and only problems of sizes 12 were solved. As a comparison, Pardalos and Crouse [57] proposed a more efficient branch-and-bound algorithm.
Reference: [13] <author> H. Crowder, E.L. Johnson, and M. Padberg. </author> <title> Solving large-scale zero-one linear programming problem. </title> <journal> Operations Research, </journal> <volume> 2 </volume> <pages> 803-834, </pages> <year> 1983. </year>
Reference-contexts: In [11] a methodology is proposed which uses a collection of workstations connected by an Ethernet network as a parallel processor for solving 0-1 linear problems. The algorithm is a based on the branch and cut approach and has been used to solve a set of test problems from <ref> [13] </ref>. 5 A Parallel Algorithm for the Quadratic Assignment Prob lem In this section we consider one of the most difficult discrete problems, the quadratic assignment problem, and discuss some details on solution techniques using parallel machines [45].
Reference: [14] <author> R. Dehne, A. Ferreira, and A. Rau-Chaplin. </author> <title> A massively parallel knowledge-base server using a hypercube multiprocessor. </title> <type> Technical report, </type> <institution> Carleton University, SCS-TR-170, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [15] <author> J. D. DeMello, J. L. Calvet, and J. M. Garcia. </author> <title> Vectorization and multitasking of dynamic programming in control: Experiments on a CRAY-2. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 261-269, </pages> <year> 1990. </year>
Reference-contexts: Lee et. al. [42] demonstrate experimentally that it is possible to obtain linear speedup for large instances of the 0/1 knapsack problem, using a divide-and-conquer DP algorithm, provided enough memory is available. Dantas et. al. <ref> [15] </ref> use vectorized formulations of DP 20 for the Cray to solve optimal control problems. A technique for parallelizing DP algorithms in VLSI is presented in [22]. In summary, multiprocessors offer the potential for effectively speeding up many computation ally intensive applications in discrete optimization.
Reference: [16] <author> A. Gottlieb et al. </author> <title> The NYU ultracomputer designing a MIMD, shared memory parallel computer. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Consequently, when the number of processors increases, its performance degrades. On the other hand, random polling does not suffer from such a drawback. However, on machines that have hardware support for concurrent access to a global pointer (e.g., the hardware fetch and add <ref> [16] </ref>), the performance of the global round robin scheme would be better than random polling. When a work transfer is made, work in the donor's stack is split into two stacks one of which is given to the requester.
Reference: [17] <author> Chris Ferguson and Richard Korf. </author> <title> Distributed tree search and its application to alpha-beta pruning. </title> <booktitle> In Proceedings of the 1988 National Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: An alternate technique is to distribute parts of the search space as they are being generated. For example, every time the successors of a node are generated, they could be sent to selected processors. A number of schemes have been proposed which use such work distribution strategies <ref> [67, 18, 17] </ref>. A major drawback of these schemes is that an interprocessor communication is required for each node generation. For a detailed discussion on the relative merits of these schemes, see [34]. Parallel Formulations of DFBB. Parallel formulations of DFBB are similar to those of DFS.
Reference: [18] <author> M. Furuichi, K. Taki, and N. Ichiyoshi. </author> <title> A multi-level load balancing scheme for or-parallel exhaustive search programs on the multi-psi. </title> <booktitle> In Proceedings of the 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1990. </year> <month> pp.50-59. </month>
Reference-contexts: An alternate technique is to distribute parts of the search space as they are being generated. For example, every time the successors of a node are generated, they could be sent to selected processors. A number of schemes have been proposed which use such work distribution strategies <ref> [67, 18, 17] </ref>. A major drawback of these schemes is that an interprocessor communication is required for each node generation. For a detailed discussion on the relative merits of these schemes, see [34]. Parallel Formulations of DFBB. Parallel formulations of DFBB are similar to those of DFS.
Reference: [19] <author> M. Garey and D.S. Johnson. </author> <title> Computers and Intractability. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Branch and bound and dynamic programming methods use the structure of these graphs to solve DOPs without searching the set X exhaustively [36]. Given that DOP is an NP-hard problem <ref> [19] </ref>, one may argue that there is no point in applying parallel processing to these problems, as the worst-case run time can never be reduced to a polynomial unless we have an exponential number of processors.
Reference: [20] <author> P.C. Gilmore. </author> <title> Optimal and suboptimal algorithms for the quadratic assignment problem. </title> <journal> J. SIAM, </journal> <volume> 10 </volume> <pages> 305-313, </pages> <year> 1962. </year>
Reference-contexts: The matrices F; D are shared data. Since they are accessed only by reading, they are not locked in a critical section. The heaps of the processors are not shared among them. The Gilmore-Lawler lower bound <ref> [20, 41] </ref> is used here as the lower bound of a partial permutation. If the given problem has multiple optimal solutions, the algorithm finds all the optimal solutions. 18 The algorithm is evaluated with two classes of test problems.
Reference: [21] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing 91 Conference, </booktitle> <year> 1991. </year>
Reference-contexts: For example, Kumar et. al. present parallel formulations of depth-first search, depth-first branch-and-bound, IDA*, and A* algorithms. They experimentally evaluate them in the context of problems such as optimizing floorplan of a VLSI chip [5], generating test patterns for combinatorial circuits [7] and tautology verification <ref> [34, 21, 6] </ref> on various machines including a 1024 processor nCUBE2, 128 processor Intel Hypercube, Symult 2010, a 128 processor BBN Butterfly T M , and a network of 16 SUN T M workstations. Bixby [9] presents a parallel branch and cut algorithm for solving the symmetric traveling salesman problem.
Reference: [22] <author> L. Guibas, H. T. Kung, and C. Thompson. </author> <title> Direct VLSI implementation of combinatorial algorithms. </title> <booktitle> In Proc. Caltech Conf. VLSI: Architecture, Design, Fabrication, </booktitle> <pages> pages 509-525, </pages> <year> 1979. </year>
Reference-contexts: Instead, it waits only for the nodes on which its own processing depends. Kung and Leiserson use this technique and present a parallel formulation of the DP algorithm for the matrix multiplication sequence problem which can use O (n 2 ) processors and run in O (n) time <ref> [22] </ref>. Not all DP algorithms can, however, be formulated as a multilevel bottom-up tree structure (e.g., see DP algorithms for solving the 0/1 Knapsack problem and the single source shortest path problem). Parallel formulations of these algorithms have to be specially designed. <p> Dantas et. al. [15] use vectorized formulations of DP 20 for the Cray to solve optimal control problems. A technique for parallelizing DP algorithms in VLSI is presented in <ref> [22] </ref>. In summary, multiprocessors offer the potential for effectively speeding up many computation ally intensive applications in discrete optimization.
Reference: [23] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1978. </year> <month> 22 </month>
Reference-contexts: This model is validated by experiments on synthetic state-space trees modeling the hackers problem [75], the 15-puzzle problem and the N-Queens problem <ref> [23] </ref>. (In these experiments, serial and parallel DFS do not use any heuristic ordering, and select successors arbitrarily.) The basic reason for this phenomenon is that parallel search can invest resources into multiple regions of the search frontier concurrently.
Reference: [24] <author> Kai Hwang. </author> <title> Advanced Computer Architecture with Parallel Programming. </title> <publisher> McGraw Hill Inc., </publisher> <year> 1992. </year>
Reference-contexts: Each level of the stack keeps track of the untried alternatives at that level. Each processor maintains its own local stack on which it executes DFS. When the local stack is empty, it takes some of the untried alternatives of another processor's stack. For shared memory architectures <ref> [24] </ref>, this operation can be performed by simply locking the other processor's stack and picking up a part of its search space. For distributed memory machines [24], this has to be accomplished using messages. The detailed schematic of this process is shown in Figure 2. <p> When the local stack is empty, it takes some of the untried alternatives of another processor's stack. For shared memory architectures <ref> [24] </ref>, this operation can be performed by simply locking the other processor's stack and picking up a part of its search space. For distributed memory machines [24], this has to be accomplished using messages. The detailed schematic of this process is shown in Figure 2. A processor on running out of work selects a target processor for addressing a work request.
Reference: [25] <author> Guo jie Li and Benjamin W. Wah. </author> <title> Parallel processing of serial dynamic programming problems. </title> <booktitle> In Proc. COMPSAC 85, </booktitle> <pages> pages 81-89, </pages> <year> 1985. </year>
Reference-contexts: This principle states that an optimal sequence of decisions has the property that irrespective of the initial state and decision, the remaining decisions must constitute an optimal decision sequence with respect to the state resulting from the first decision. Many different formulations of DP have been presented <ref> [36, 27, 25] </ref>. The essence of many DP algorithms lies in computing solutions to the smallest subproblems and storing the results and using them to compute solution to bigger problems. Thus the solution to the original problem is constructed in a bottom-up fashion [4]. <p> This processor can use no more processors than the maximum number of nodes at any level. In many applications, it is possible to extract a greater degree of parallelism than the above formulation. The first technique uses multiple processors to compute the cost of a node (e.g., <ref> [25] </ref>). In the second technique, processing of nodes at different levels is pipelined. Thus, processing of a node at level i does not wait for all nodes at levels below i to be completed. Instead, it waits only for the nodes on which its own processing depends.
Reference: [26] <editor> Laveen Kanal and Vipin Kumar. </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Section 5 discusses parallel formulations and applications of the quadratic assignment problem. Section 6 contains concluding remarks. 2 2 Sequential Algorithms for Solving Discrete Optimization Problems. Here we provide a brief overview of sequential search algorithms. For detailed descriptions, see <ref> [54, 62, 26] </ref>. 2.1 Depth-First Search Algorithms. Depth-first search is a name commonly used for various search techniques for solving DOPs that perform search as follows. The search begins by expanding the initial node, i.e., by generating its successors.
Reference: [27] <author> R. M. Karp and M. H. </author> <title> Held. Finite state processes and dynamic programming. </title> <journal> SIAM J. Appl. Math, </journal> <volume> 15:693 - 718, </volume> <year> 1967. </year>
Reference-contexts: This principle states that an optimal sequence of decisions has the property that irrespective of the initial state and decision, the remaining decisions must constitute an optimal decision sequence with respect to the state resulting from the first decision. Many different formulations of DP have been presented <ref> [36, 27, 25] </ref>. The essence of many DP algorithms lies in computing solutions to the smallest subproblems and storing the results and using them to compute solution to bigger problems. Thus the solution to the original problem is constructed in a bottom-up fashion [4].
Reference: [28] <author> George Karypis and Vipin Kumar. </author> <title> Unstructured Tree Search on SIMD Parallel Computers. </title> <type> Technical Report 92-21, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1992. </year> <note> A short version of this paper appears in the Proceedings of Supercomputing 1992 Conference, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion.
Reference: [29] <author> T.C. Koopmans and M.J. Beckmann. </author> <title> Assignment problems and the location of economic activities. </title> <journal> Econometrica, </journal> <volume> 25 </volume> <pages> 53-76, </pages> <year> 1957. </year>
Reference-contexts: This model, first proposed by Koopmans and Beckmann in 1957 <ref> [29] </ref>, can be stated as the following minimization problem min i=1 j=1 s.t. p 2 ; where n is a positive integer, F = (f ij ) 2 R nfin , D = (d ij ) 2 R nfin , and is the set of permutations of the set N =
Reference: [30] <author> R.E. Korf. </author> <title> Depth-first iterative-deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: The algorithm continues until a goal node is selected for expansion. It might appear that IDA* performs a lot of redundant work in successive iterations. But for many problems 3 of interest, the redundant work is minimal and the algorithm finds an optimal solution <ref> [30] </ref>. 2.2 Best-First Search. Best-first search techniques use heuristics to direct search to spaces which are more likely to yield solutions. A*/Best-first branch-and-bound search is a commonly used best-first search technique. A* makes use of a heuristic evaluation function, f , defined over the nodes of the search space.
Reference: [31] <author> Richard Korf. </author> <title> Optimal path finding algorithms. </title> <editor> In Laveen Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Parallel formulations of DFBB have been shown to yield linearly increasing speedups for many problems and architectures [5, 6]. 9 Parallel Formulations of IDA*. There are two approaches to parallelizing IDA*. In one approach, different processors work on different iterations of IDA* independently <ref> [31, 35] </ref>. This approach is not very suitable for finding optimal solutions. This is because a solution found by a node on a particular iteration is not provably optimal until all the other processors have also exhausted search space until that iteration and have not found a better solution.
Reference: [32] <author> W. Kornfeld. </author> <title> The use of parallelism to implement a heuristic search. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 575-580, </pages> <year> 1981. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming. <p> It may appear that on the average the speedup would be either linear or sublinear. This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers <ref> [32, 40, 44, 49, 53] </ref> for a variety of problems and is referred to by the term speedup anomaly. The average speedup in parallel DFS for two different types of models has been analyzed in [52, 69].
Reference: [33] <editor> V. Kumar, P.S. Gopalkrishnan, and L. Kanal (editors). </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [34] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical report, Tech Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. <p> A number of load distribution techniques have been developed for parallel DFS [72, 73, 34, 39, 48, 67, 28]. A general method for parallelizing DFS is presented in <ref> [34, 39] </ref>. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. When a processor finishes searching its part of the search space, it tries to get an unsearched part of the search space from other processors. <p> Select a processor and Processor idle. Processor active. Got a reject. Issued a request. Got work. work. Finished available Service any pending messages. Do a fixed amount of work. Service any pending messages. architectures. 7 8 example, two techniques discussed in <ref> [34] </ref> are random polling and global round robin. In random polling, a processor is selected at random and the work request is targeted to this processor. In global round robin, a global pointer is maintained at a designated processor. This pointer determines the target of a work request. <p> Every time a work request is made, the pointer is read and incremented (modulo the number of processors). Though the global round robin scheme minimizes the total number of work requests for a wide class of problems, accessing the global pointer forms a bottleneck <ref> [34] </ref>. Consequently, when the number of processors increases, its performance degrades. On the other hand, random polling does not suffer from such a drawback. <p> A number of schemes have been proposed which use such work distribution strategies [67, 18, 17]. A major drawback of these schemes is that an interprocessor communication is required for each node generation. For a detailed discussion on the relative merits of these schemes, see <ref> [34] </ref>. Parallel Formulations of DFBB. Parallel formulations of DFBB are similar to those of DFS. The parallel formulation of DFS described above can be applied to DFBB with one minor modification. Now we need to keep all the processors informed of the current best solution path. <p> For example, Kumar et. al. present parallel formulations of depth-first search, depth-first branch-and-bound, IDA*, and A* algorithms. They experimentally evaluate them in the context of problems such as optimizing floorplan of a VLSI chip [5], generating test patterns for combinatorial circuits [7] and tautology verification <ref> [34, 21, 6] </ref> on various machines including a 1024 processor nCUBE2, 128 processor Intel Hypercube, Symult 2010, a 128 processor BBN Butterfly T M , and a network of 16 SUN T M workstations. Bixby [9] presents a parallel branch and cut algorithm for solving the symmetric traveling salesman problem.
Reference: [35] <editor> Vipin Kumar and Laveen Kanal. </editor> <title> Parallel branch-and-bound formulations for and/or tree search. </title> <journal> IEEE Trans. Pattern. Anal. and Machine Intell., </journal> <volume> PAMI-6:768-778, </volume> <year> 1984. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming. <p> Parallel formulations of DFBB have been shown to yield linearly increasing speedups for many problems and architectures [5, 6]. 9 Parallel Formulations of IDA*. There are two approaches to parallelizing IDA*. In one approach, different processors work on different iterations of IDA* independently <ref> [31, 35] </ref>. This approach is not very suitable for finding optimal solutions. This is because a solution found by a node on a particular iteration is not provably optimal until all the other processors have also exhausted search space until that iteration and have not found a better solution. <p> Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [36] <editor> Vipin Kumar and Laveen Kanal. </editor> <title> The cdp: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound. </title> <editor> In Laveen Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Branch and bound and dynamic programming methods use the structure of these graphs to solve DOPs without searching the set X exhaustively <ref> [36] </ref>. Given that DOP is an NP-hard problem [19], one may argue that there is no point in applying parallel processing to these problems, as the worst-case run time can never be reduced to a polynomial unless we have an exponential number of processors. <p> This principle states that an optimal sequence of decisions has the property that irrespective of the initial state and decision, the remaining decisions must constitute an optimal decision sequence with respect to the state resulting from the first decision. Many different formulations of DP have been presented <ref> [36, 27, 25] </ref>. The essence of many DP algorithms lies in computing solutions to the smallest subproblems and storing the results and using them to compute solution to bigger problems. Thus the solution to the original problem is constructed in a bottom-up fashion [4].
Reference: [37] <author> Vipin Kumar, K. Ramesh, and V. Nageshwara Rao. </author> <title> Parallel best-first search of state-space graphs: A summary of results. </title> <booktitle> In Proceedings of the 1988 National Conference on Artificial Intelligence, </booktitle> <pages> pages 122-126, </pages> <month> August </month> <year> 1988. </year> <month> 23 </month>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [38] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: This is because a solution found by a node on a particular iteration is not provably optimal until all the other processors have also exhausted search space until that iteration and have not found a better solution. Another approach is to execute each iteration of IDA* via parallel DFS <ref> [38, 39, 51] </ref>. Since all processors work with the same cost bound, each processor stores this value locally and performs DFS on its own search space. 3.1.1 Speedup Anomalies in Backtracking / DFS.
Reference: [39] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. <p> A number of load distribution techniques have been developed for parallel DFS [72, 73, 34, 39, 48, 67, 28]. A general method for parallelizing DFS is presented in <ref> [34, 39] </ref>. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. When a processor finishes searching its part of the search space, it tries to get an unsearched part of the search space from other processors. <p> This is because a solution found by a node on a particular iteration is not provably optimal until all the other processors have also exhausted search space until that iteration and have not found a better solution. Another approach is to execute each iteration of IDA* via parallel DFS <ref> [38, 39, 51] </ref>. Since all processors work with the same cost bound, each processor stores this value locally and performs DFS on its own search space. 3.1.1 Speedup Anomalies in Backtracking / DFS.
Reference: [40] <author> T. H. Lai and Sartaj Sahni. </author> <title> Anomalies in parallel branch and bound algorithms. </title> <journal> Communications of the ACM, </journal> <pages> pages 594-602, </pages> <year> 1984. </year>
Reference-contexts: It may appear that on the average the speedup would be either linear or sublinear. This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers <ref> [32, 40, 44, 49, 53] </ref> for a variety of problems and is referred to by the term speedup anomaly. The average speedup in parallel DFS for two different types of models has been analyzed in [52, 69].
Reference: [41] <author> E.L. Lawler. </author> <title> The quadratic assignment problem. </title> <journal> Management Science, </journal> <volume> 9 </volume> <pages> 586-599, </pages> <year> 1963. </year>
Reference-contexts: The matrices F; D are shared data. Since they are accessed only by reading, they are not locked in a critical section. The heaps of the processors are not shared among them. The Gilmore-Lawler lower bound <ref> [20, 41] </ref> is used here as the lower bound of a partial permutation. If the given problem has multiple optimal solutions, the algorithm finds all the optimal solutions. 18 The algorithm is evaluated with two classes of test problems.
Reference: [42] <author> J. Lee, E. Shragowitz, and S. Sahni. </author> <title> A hypercube algorithm for the 0/1 knapsack problem. </title> <booktitle> In Proc. Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 699 - 706, </pages> <year> 1987. </year>
Reference-contexts: Not all DP algorithms can, however, be formulated as a multilevel bottom-up tree structure (e.g., see DP algorithms for solving the 0/1 Knapsack problem and the single source shortest path problem). Parallel formulations of these algorithms have to be specially designed. For example, Lee et. al. <ref> [42] </ref> use the divide-and-conquer strategy for parallelizing the DP algorithm for the 0/1 knapsack problem on a MIMD distributed memory computer. 4 Parallel Integer Linear Programming Algorithms In the next two sections we discuss parallel algorithms for solving the general linear zero-one and the quadratic assignment problems. <p> Roucairol [71] presents parallel branch and bound formulations for shared memory computers and uses these to solve the Multiknapsack and Quadratic assignment problems. Lee et. al. <ref> [42] </ref> demonstrate experimentally that it is possible to obtain linear speedup for large instances of the 0/1 knapsack problem, using a divide-and-conquer DP algorithm, provided enough memory is available. Dantas et. al. [15] use vectorized formulations of DP 20 for the Cray to solve optimal control problems.
Reference: [43] <editor> D.B. Leifker and L.N. Kanal. </editor> <title> A hybrid sss*/alpha-beta algorithm for parallel search of game trees. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 1044-1046, </pages> <year> 1985. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [44] <author> Guo-Jie Li and Benjamin W. Wah. </author> <title> Coping with anomalies in parallel branch-and-bound algorithms. </title> <journal> IEEE Trans on Computers, </journal> <volume> C-35, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: It may appear that on the average the speedup would be either linear or sublinear. This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers <ref> [32, 40, 44, 49, 53] </ref> for a variety of problems and is referred to by the term speedup anomaly. The average speedup in parallel DFS for two different types of models has been analyzed in [52, 69].
Reference: [45] <author> Y. Li and P.M. Pardalos. </author> <title> Parallel algorithms for the quadratic assignment problem. In P.M. </title> <editor> Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 177-189. </pages> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: approach and has been used to solve a set of test problems from [13]. 5 A Parallel Algorithm for the Quadratic Assignment Prob lem In this section we consider one of the most difficult discrete problems, the quadratic assignment problem, and discuss some details on solution techniques using parallel machines <ref> [45] </ref>. The quadratic assignment problem (QAP) is a mathematical model arising from many location problems in which the cost associated with allocating a facility at a certain location depends, not only on the distances from other facilities, but also on the interaction with other facilities. <p> Besides applications in facility allocation, this model can be applied in many other applications, including backboard wiring, machine scheduling, ordering interrelated data on a computer storage device, scheduling of economic lot sizes, and designing typewriter keyboard <ref> [45] </ref>. Next, we discuss a parallel exact algorithms for the QAP. Roucairol [12] first proposed a parallel branch-and-bound algorithm for solving the QAP and implemented the algorithm on a CRAY XMP/48. However her computational results were not very satisfactory and only problems of sizes 12 were solved. <p> For smaller size problems, the parallel algorithm is not as efficient. Details and additional computational results can be found in [57], <ref> [45] </ref>. Regarding computational results for solving more general problems such as the quadratic zero-one programming problem (using distributed and shared memory machines) can be found in [59] and [60]. 6 Concluding Remarks. Extensive work has been done on development of parallel formulations of search algorithms for solving DOPs.
Reference: [46] <author> G. Manzini and M. Somalvico. </author> <title> Probabilistic performance analysis of heuristic search using parallel hash tables. </title> <booktitle> In Proceedings of the International Symposium on Artificial Intelligence and MAthematics, </booktitle> <address> Ft. Lauderdale, FL, </address> <month> JAnuary, </month> <year> 1990. </year>
Reference-contexts: This would lead to redundant node expansions and poor speedups. Various schemes can be developed to trade off redundant node expansions, communication overheads and contention to optimize performance. A commonly used technique for implementing distributed queues is to hash every node generated to a unique processor. As shown in <ref> [46] </ref> this method ensures a good distribution of promising nodes among all processors. This technique also allows checking for duplication of nodes which is required for graph search.
Reference: [47] <author> Donald Miller. </author> <title> Exact distributed algorithms for travelling salesman problem. </title> <booktitle> In Proceedings of the Workshop On Parallel Computing of Discre te Optimization Problems, </booktitle> <year> 1991. </year>
Reference-contexts: Bixby [9] presents a parallel branch and cut algorithm for solving the symmetric traveling salesman problem. He also presents solutions of the LP relaxations of airline crew-scheduling models. Miller et. al. <ref> [47] </ref> present parallel formulations of the branch and bound technique for solving the asymmetric traveling salesman problem on heterogeneous network computer architectures. Roucairol [71] presents parallel branch and bound formulations for shared memory computers and uses these to solve the Multiknapsack and Quadratic assignment problems.
Reference: [48] <author> B. Monien and O. Vornberger. </author> <title> Parallel processing of combinatorial search trees. </title> <booktitle> In Proceedings of International Workshop on Parallel Algorithms and Architectures, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. <p> Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [49] <author> B. Monien, O. Vornberger, and E. Spekenmeyer. </author> <title> Superlinear speedup for parallel backtracking. </title> <type> Technical Report 30, </type> <institution> Univ. of Paderborn, </institution> <address> FRG, </address> <year> 1986. </year>
Reference-contexts: It may appear that on the average the speedup would be either linear or sublinear. This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers <ref> [32, 40, 44, 49, 53] </ref> for a variety of problems and is referred to by the term speedup anomaly. The average speedup in parallel DFS for two different types of models has been analyzed in [52, 69].
Reference: [50] <author> K.A. Murthy and P.M. Pardalos. </author> <title> A polynomial-time approximation algorithm for the quadratic assignment problem. </title> <type> Technical report, Technical Report CS-33-90, </type> <institution> The Penn-sylvania State University, </institution> <year> 1990. </year>
Reference-contexts: The first set of problems include the NUGENT collection of the QAP [55]. The other set of test problems, denoted PALU-BETSKES, are generated by the algorithm described in <ref> [50, 56] </ref>. The NUGENT set of test problems is one of the most widely used in the literature and can be used to test both heuristic and exact algorithms for the QAP. <p> The PALUBETSKES set of test problems are generated according to the test problem generator, which outputs test problems with known optimal solutions, as reported in <ref> [50, 56] </ref>. The test problem generator contains two positive integer parameters, z and w. A random variable with a uniform distribution in (0; 1) is used also in the generator.
Reference: [51] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 (6):479-499, 1987. 24 </volume>
Reference-contexts: This is because a solution found by a node on a particular iteration is not provably optimal until all the other processors have also exhausted search space until that iteration and have not found a better solution. Another approach is to execute each iteration of IDA* via parallel DFS <ref> [38, 39, 51] </ref>. Since all processors work with the same cost bound, each processor stores this value locally and performs DFS on its own search space. 3.1.1 Speedup Anomalies in Backtracking / DFS.
Reference: [52] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Superlinear speedup in state-space search. </title> <booktitle> In Pro--ceedings of the 1988 Foundation of Software Technology and Theoretcal Computer Science, </booktitle> <month> December </month> <year> 1988. </year> <booktitle> Lecture Notes in Computer Science number 338, </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: The average speedup in parallel DFS for two different types of models has been analyzed in <ref> [52, 69] </ref>. In the first model, no heuristic information is available to order the successors of a node. For this model, analysis shows that on the average, the speedup obtained is (i) linear when distribution of solutions is uniform, and (ii) superlinear when distribution of solutions is non-uniform.
Reference: [53] <author> V. Nageshwara Rao, Vipin Kumar, and K. Ramesh. </author> <title> A parallel implementation of iterative-deepening-a*. </title> <booktitle> In Proceedings of the National Conf. on Artificial Intelligence (AAAI-87), </booktitle> <pages> pages 878-882, </pages> <year> 1987. </year>
Reference-contexts: It may appear that on the average the speedup would be either linear or sublinear. This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers <ref> [32, 40, 44, 49, 53] </ref> for a variety of problems and is referred to by the term speedup anomaly. The average speedup in parallel DFS for two different types of models has been analyzed in [52, 69].
Reference: [54] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Press, </publisher> <year> 1980. </year>
Reference-contexts: Section 5 discusses parallel formulations and applications of the quadratic assignment problem. Section 6 contains concluding remarks. 2 2 Sequential Algorithms for Solving Discrete Optimization Problems. Here we provide a brief overview of sequential search algorithms. For detailed descriptions, see <ref> [54, 62, 26] </ref>. 2.1 Depth-First Search Algorithms. Depth-first search is a name commonly used for various search techniques for solving DOPs that perform search as follows. The search begins by expanding the initial node, i.e., by generating its successors.
Reference: [55] <author> C.E. Nugent, T.E. Vollmann, and J. Ruml. </author> <title> An experimental comparison of techniques for the assignment of facilities to locations. </title> <journal> Journal of Operations Research, </journal> <volume> 16 </volume> <pages> 150-173, </pages> <year> 1969. </year>
Reference-contexts: If the given problem has multiple optimal solutions, the algorithm finds all the optimal solutions. 18 The algorithm is evaluated with two classes of test problems. The first set of problems include the NUGENT collection of the QAP <ref> [55] </ref>. The other set of test problems, denoted PALU-BETSKES, are generated by the algorithm described in [50, 56]. The NUGENT set of test problems is one of the most widely used in the literature and can be used to test both heuristic and exact algorithms for the QAP.
Reference: [56] <author> G.S. Palubetskes. </author> <title> Generation of quadratic assignment test problems with known optimal solutions (in Russian). </title> <journal> Zh. Vychisl. Mat. Mat. Fiz., </journal> <volume> 28(11) </volume> <pages> 1740-1743, </pages> <year> 1988. </year>
Reference-contexts: The first set of problems include the NUGENT collection of the QAP [55]. The other set of test problems, denoted PALU-BETSKES, are generated by the algorithm described in <ref> [50, 56] </ref>. The NUGENT set of test problems is one of the most widely used in the literature and can be used to test both heuristic and exact algorithms for the QAP. <p> The PALUBETSKES set of test problems are generated according to the test problem generator, which outputs test problems with known optimal solutions, as reported in <ref> [50, 56] </ref>. The test problem generator contains two positive integer parameters, z and w. A random variable with a uniform distribution in (0; 1) is used also in the generator.
Reference: [57] <author> P.M. Pardalos and J.Crouse. </author> <title> A parallel algorithm for the quadratic assignment problem. </title> <booktitle> In Proceedings of the Supercomputing 1989 Conference, </booktitle> <pages> pages 351-360, </pages> <publisher> ACM Press, </publisher> <year> (1989). </year>
Reference-contexts: Roucairol [12] first proposed a parallel branch-and-bound algorithm for solving the QAP and implemented the algorithm on a CRAY XMP/48. However her computational results were not very satisfactory and only problems of sizes 12 were solved. As a comparison, Pardalos and Crouse <ref> [57] </ref> proposed a more efficient branch-and-bound algorithm. The discussion in the rest of this section is based on [57]. The exact algorithm presented below is of the branch-and-bound type [57], [61]. The term "solution" and "permutation" are used interchangeably in our discussion. The algorithm consists of three steps. <p> However her computational results were not very satisfactory and only problems of sizes 12 were solved. As a comparison, Pardalos and Crouse <ref> [57] </ref> proposed a more efficient branch-and-bound algorithm. The discussion in the rest of this section is based on [57]. The exact algorithm presented below is of the branch-and-bound type [57], [61]. The term "solution" and "permutation" are used interchangeably in our discussion. The algorithm consists of three steps. <p> As a comparison, Pardalos and Crouse <ref> [57] </ref> proposed a more efficient branch-and-bound algorithm. The discussion in the rest of this section is based on [57]. The exact algorithm presented below is of the branch-and-bound type [57], [61]. The term "solution" and "permutation" are used interchangeably in our discussion. The algorithm consists of three steps. <p> For smaller size problems, the parallel algorithm is not as efficient. Details and additional computational results can be found in <ref> [57] </ref>, [45]. Regarding computational results for solving more general problems such as the quadratic zero-one programming problem (using distributed and shared memory machines) can be found in [59] and [60]. 6 Concluding Remarks. Extensive work has been done on development of parallel formulations of search algorithms for solving DOPs.
Reference: [58] <author> P.M. Pardalos, A.T. Phillips, and J.B. Rosen. </author> <booktitle> Topics in Parallel Computing in Mathematical Programming. </booktitle> <publisher> Science Press, </publisher> <year> 1992. </year>
Reference-contexts: Integer linear programming problems appear in many applications and are solved, in general, by some branch and bound type algorithm <ref> [58] </ref>, [65]. 12 In this section, we describe some of the first attempts to parallelize these branch and bound algorithms for integer linear programs. In [3] the 0-1 integer linear program min f (x) = c T x is considered.
Reference: [59] <author> P.M. Pardalos and G.P. Rodgers. </author> <title> Parallel branch and bound algorithms for unconstrainted quadratic zero-one programming. </title> <editor> In et al. R. Sharda, editor, </editor> <booktitle> Impacts of Recent Computer Advances on Operations Research, </booktitle> <pages> pages 131-143. </pages> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: For smaller size problems, the parallel algorithm is not as efficient. Details and additional computational results can be found in [57], [45]. Regarding computational results for solving more general problems such as the quadratic zero-one programming problem (using distributed and shared memory machines) can be found in <ref> [59] </ref> and [60]. 6 Concluding Remarks. Extensive work has been done on development of parallel formulations of search algorithms for solving DOPs. Many of these formulations have been implemented for solving abstract and practical problems on commercially available parallel computers.
Reference: [60] <author> P.M. Pardalos and G.P. Rodgers. </author> <title> Parallel branch and bound algorithms for quadratic zero-one programming on a hypercube architecture. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 271-292, </pages> <year> 1990. </year>
Reference-contexts: For smaller size problems, the parallel algorithm is not as efficient. Details and additional computational results can be found in [57], [45]. Regarding computational results for solving more general problems such as the quadratic zero-one programming problem (using distributed and shared memory machines) can be found in [59] and <ref> [60] </ref>. 6 Concluding Remarks. Extensive work has been done on development of parallel formulations of search algorithms for solving DOPs. Many of these formulations have been implemented for solving abstract and practical problems on commercially available parallel computers.
Reference: [61] <editor> P.M. Pardalos and X.Li. </editor> <title> Parallel branch and bound algorithms for combinatorial optimization. </title> <journal> Supercomputer, </journal> <volume> 39 </volume> <pages> 23-30, </pages> <year> 1990. </year>
Reference-contexts: As a comparison, Pardalos and Crouse [57] proposed a more efficient branch-and-bound algorithm. The discussion in the rest of this section is based on [57]. The exact algorithm presented below is of the branch-and-bound type [57], <ref> [61] </ref>. The term "solution" and "permutation" are used interchangeably in our discussion. The algorithm consists of three steps.
Reference: [62] <author> Judea Pearl. </author> <title> Heuristics Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Section 5 discusses parallel formulations and applications of the quadratic assignment problem. Section 6 contains concluding remarks. 2 2 Sequential Algorithms for Solving Discrete Optimization Problems. Here we provide a brief overview of sequential search algorithms. For detailed descriptions, see <ref> [54, 62, 26] </ref>. 2.1 Depth-First Search Algorithms. Depth-first search is a name commonly used for various search techniques for solving DOPs that perform search as follows. The search begins by expanding the initial node, i.e., by generating its successors.
Reference: [63] <author> C. Powley and R. Korf. </author> <title> Single agent parallel window search: A summary of results. </title> <booktitle> In Proceedings of the eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 36-41, </pages> <year> 1989. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [64] <author> Michael J. Quinn. </author> <title> Analysis and implementation of branch-and-bound algorithms on a Hypercube multicomputer. </title> <journal> IEEE Transactions on Computers, </journal> ?, <year> 1989. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [65] <author> M.J. Quinn. </author> <title> Designing Efficient Algorithms For Parallel Computers. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1987. </year> <month> 25 </month>
Reference-contexts: Integer linear programming problems appear in many applications and are solved, in general, by some branch and bound type algorithm [58], <ref> [65] </ref>. 12 In this section, we describe some of the first attempts to parallelize these branch and bound algorithms for integer linear programs. In [3] the 0-1 integer linear program min f (x) = c T x is considered.
Reference: [66] <author> Finkel R.A. and J.P. Fishburn. </author> <title> Parallelism in alpha-beta search. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 89-106, </pages> <year> 1982. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [67] <author> Abhiram Ranade. </author> <title> Optimal speedup for backtrack search on a butterfly network. </title> <booktitle> In Proceedings of the Third ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1991. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion. <p> An alternate technique is to distribute parts of the search space as they are being generated. For example, every time the successors of a node are generated, they could be sent to selected processors. A number of schemes have been proposed which use such work distribution strategies <ref> [67, 18, 17] </ref>. A major drawback of these schemes is that an interprocessor communication is required for each node generation. For a detailed discussion on the relative merits of these schemes, see [34]. Parallel Formulations of DFBB. Parallel formulations of DFBB are similar to those of DFS.
Reference: [68] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [69] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> On the efficicency of parallel backtracking. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> (to appear), 1992. available as a technical report TR 90-55, </note> <institution> Computer Science Department, University of Minnesota. </institution>
Reference-contexts: The average speedup in parallel DFS for two different types of models has been analyzed in <ref> [52, 69] </ref>. In the first model, no heuristic information is available to order the successors of a node. For this model, analysis shows that on the average, the speedup obtained is (i) linear when distribution of solutions is uniform, and (ii) superlinear when distribution of solutions is non-uniform.
Reference: [70] <author> Shie rei Huang and Larry S. Davis. </author> <title> Parallel iterative a* search: An admissible distributed heuristic search algorithm. </title> <booktitle> In Proceedings of the eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 23-29, </pages> <year> 1989. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [71] <author> C. Roucairol. </author> <title> Parallel branch-and-bound on shared memory multiprocessors. </title> <booktitle> In Proceedings of the Workshop On Parallel Computing of Discre te Optimization Problems, </booktitle> <year> 1991. </year>
Reference-contexts: He also presents solutions of the LP relaxations of airline crew-scheduling models. Miller et. al. [47] present parallel formulations of the branch and bound technique for solving the asymmetric traveling salesman problem on heterogeneous network computer architectures. Roucairol <ref> [71] </ref> presents parallel branch and bound formulations for shared memory computers and uses these to solve the Multiknapsack and Quadratic assignment problems.
Reference: [72] <author> Vikram Saletore and L. V. Kale. </author> <title> Consistent linear speedup to a first solution in parallel state-space search. </title> <booktitle> In Proceedings of the 1990 National Conference on Artificial Intelligence, </booktitle> <pages> pages 227-233, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion.
Reference: [73] <author> Wei Shu and L. V. Kale. </author> <title> A dynamic scheduling strategy for the chare-kernel system. </title> <booktitle> In Proceedings of Supercomputing 89, </booktitle> <pages> pages 389-398, </pages> <year> 1989. </year>
Reference-contexts: The core of any parallel formulation of DFS algorithms is thus a dynamic load balancing technique which minimizes communication and processor idling. A number of load distribution techniques have been developed for parallel DFS <ref> [72, 73, 34, 39, 48, 67, 28] </ref>. A general method for parallelizing DFS is presented in [34, 39]. In this formulation, each processor searches a disjoint part of the search space in a depth-first fashion.
Reference: [74] <author> Douglas R. Smith. </author> <title> Random trees and the analysis of branch and bound proceedures. </title> <journal> Journal of the ACM, </journal> <volume> 31 No. 1, </volume> <year> 1984. </year>
Reference-contexts: However, the average time complexity of heuristic search algorithms for many problems is polynomial <ref> [74, 82] </ref>. Also, there are some heuristic search algorithms which find suboptimal solutions in polynomial time (e.g., for certain problems, approximate branch-and-bound algorithms are known to run in polynomial time [81]). In these cases, parallel processing can significantly increase the size of solvable problems.
Reference: [75] <author> H. Stone and P. Sipala. </author> <title> The average complexity of depth-first search with backtracking and cutoff. </title> <journal> IBM Journal of Research and Development, </journal> <month> May </month> <year> 1986. </year>
Reference-contexts: For this model, analysis shows that on the average, the speedup obtained is (i) linear when distribution of solutions is uniform, and (ii) superlinear when distribution of solutions is non-uniform. This model is validated by experiments on synthetic state-space trees modeling the hackers problem <ref> [75] </ref>, the 15-puzzle problem and the N-Queens problem [23]. (In these experiments, serial and parallel DFS do not use any heuristic ordering, and select successors arbitrarily.) The basic reason for this phenomenon is that parallel search can invest resources into multiple regions of the search frontier concurrently.
Reference: [76] <author> T.Ibaraki. </author> <title> Theoretical comparisons of search strategies in branch-and-bound algorithms. </title> <journal> International Journal of Computer and Information Sciences, </journal> <volume> 5(4) </volume> <pages> 315-344, </pages> <year> 1976. </year>
Reference-contexts: The root of the tree has the maximum key value. For each partial permutation (solution) in the heap, the corresponding subproblem is the QAP with part of the facilities being allocated. In the second step, the four procedures of the branch-and-bound (selection, branching, elimination, and termination <ref> [76] </ref>) are used as follows: 1. The selection procedure simply selects the partial permutation stored in the root of the heap. 2. The branching procedure splits the selected partial permutation into two new partial permutations.
Reference: [77] <author> Olivier Vornberger. </author> <title> Implementing branch-and-bound in a ring of processors. </title> <type> Technical Report 29, </type> <institution> Univ. of Paderborn, </institution> <address> FRG, </address> <year> 1986. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [78] <author> Benjamin W. Wah, Guo jie Li, and Chee Fen Yu. </author> <title> Multiprocessing of combinatorial search problems. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 93-108, </pages> <month> June, </month> <year> 1985. </year>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [79] <author> Benjamin W. Wah, G.J. Li, and C. F. Yu. </author> <title> Multiprocessing of combinatorial search problems. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year> <month> 26 </month>
Reference-contexts: It may be possible to construct computers comprising of thousands to millions of processing elements at costs ranging from those of high-end workstations to large mainframes. This technology has created substantial interest in exploring the use of parallel processing for search based applications <ref> [1, 2, 14, 32, 33, 35, 63, 66, 68, 78, 79] </ref>. This article provides a survey of parallel algorithms for solving DOPs. Section 2 reviews serial algorithms for solving DOPs. Section 3 discusses parallel formulations of depth-first and best-first search algorithms and dynamic programming.
Reference: [80] <author> Benjamin W. Wah and Y. W. Eva Ma. </author> <title> Manip amulticomputer architecture for solving combinatorial extremum-search problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: Results from this model have been verified on the parallel formulation of a DFS algorithm, called PODEM, which uses very powerful heuristics to order the search tree [7]. 3.2 Parallel Best-First Search. A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [35, 37, 43, 48, 64, 70, 77, 80] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [81] <author> Benjamin W. Wah and C. F. Yu. </author> <title> Stochastic modelling of branch-and-bound algorithms with best-first search. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11, </volume> <month> September </month> <year> 1985. </year>
Reference-contexts: However, the average time complexity of heuristic search algorithms for many problems is polynomial [74, 82]. Also, there are some heuristic search algorithms which find suboptimal solutions in polynomial time (e.g., for certain problems, approximate branch-and-bound algorithms are known to run in polynomial time <ref> [81] </ref>). In these cases, parallel processing can significantly increase the size of solvable problems. Some applications using search algorithms (e.g. robot motion planning, task scheduling) require real time solutions. For these applications, parallel processing is perhaps the only way to obtain acceptable performance.
Reference: [82] <author> Herbert S. Wilf. </author> <title> Algorithms and Complexity. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year> <month> 27 </month>
Reference-contexts: However, the average time complexity of heuristic search algorithms for many problems is polynomial <ref> [74, 82] </ref>. Also, there are some heuristic search algorithms which find suboptimal solutions in polynomial time (e.g., for certain problems, approximate branch-and-bound algorithms are known to run in polynomial time [81]). In these cases, parallel processing can significantly increase the size of solvable problems.
References-found: 82

