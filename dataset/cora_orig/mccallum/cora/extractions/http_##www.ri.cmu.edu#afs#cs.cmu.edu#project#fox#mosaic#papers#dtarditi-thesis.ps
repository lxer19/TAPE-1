URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/fox/mosaic/papers/dtarditi-thesis.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/fox/mosaic/papers.html
Root-URL: 
Title: Design and Implementation of Code Optimizations for a Type-Directed Compiler for Standard ML  
Author: David Tarditi Steven Lucco Nevin Heintze 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee: Peter Lee, Chair  
Note: Copyright c fl1996 David Tarditi This research was sponsored by the Advanced Research Projects Agency CSTO under the title "The Fox Project: Advanced Languages for Systems Software", ARPA Order No. C533, issued by ESC/ENS under Contract No. F19628-95-C-0050. The views and conclusions contained in this document are those of the author and should not be interpreted as representing official policies, either expressed or implied, of the Advanced Research Projects Agency or the U.S. Government.  
Address: Pittsburgh, PA 15213  
Date: December, 1996  
Affiliation: Carnegie Mellon University  School of Computer Science Carnegie Mellon University  Simon Peyton-Jones, Glasgow University  
Pubnum: CMU-CS-97-108  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Proceedings of the 18th Annual ACM Symposium on Principles of Programming Languages, </institution> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year> <note> ACM. </note>
Reference: [2] <institution> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </institution> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference: [3] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Because my algorithms work on programs in a -calculus based intermediate language, the techniques that I use in my algorithms are quite different from conventional optimization techniques <ref> [3] </ref>. Conventional techniques, for example, use control-flow graphs. In contrast, because there is only recursion in SML I use call graphs instead of control-flow graphs. Con- ventional techniques also use dataflow analysis. Because SML emphasizes binding instead of assignment, I use a simple side-effects analysis instead of dataflow analysis. <p> Similarly, an expression that assigns to a variable implicitly updates the store. This implicit use of the store makes careful tracking of side-effects to the store a central component of conventional data flow analysis <ref> [3] </ref>. Indeed, it has led to the development of the static single assignment (SSA) intermediate representation, where variables can be assigned to only once. The fact that variables can be assigned to only once simplifes many analyses. In SML, however, variables are not updateable. Variables may only be bound. <p> It is important to emphasize that the algorithms are new, even though most of the optimizations are well-known. I designed new algorithms because I apply these optimizations to programs in a -calculus-based intermediate language (B-form). This makes it impossible to apply the textbook versions of optimizations <ref> [3] </ref>: the languages used in the textbooks differ too much from from the -calculus. Those languages are first-order, have a flat name space, are imperative and have looping constructs. In contrast, my -calculus-based intermediate language is higher-order, has lexical scoping, emphasizes variable binding instead of assignment, and uses only recursion.
Reference: [4] <author> Andrew W. Appel. </author> <title> Garbage collection can be faster than stack allocation. </title> <journal> Information Processing Letters, </journal> <volume> 25(4) </volume> <pages> 275-279, </pages> <year> 1987. </year>
Reference-contexts: Optimizations done by the compiler include inlining, passing function arguments in registers, register targeting, constant folding, code hoisting, uncurrying, and instruction scheduling. The most controversial design decision in the compiler is to allocate procedure activation records on the heap instead of the stack <ref> [4, 10] </ref>. In principle, the presence of higher-order functions means that procedure activation records must be allocated on the heap. With a suitable analysis, a stack can be used to store most activation records [52].
Reference: [5] <author> Andrew W. Appel. </author> <title> Simple generational garbage collection and fast allocation. </title> <journal> Software | Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 171-184, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: the pointer to the next list cell, alloc is the address of the next free word in the allocation area, and top contains the end of the allocation area. 2.1.4 Garbage collection in SML/NJ I use version 0.91 of the SML/NJ compiler, which uses a simple generational copying garbage collector <ref> [5] </ref>. Memory is divided into an old generation and an allocation area. New objects are created in the allocation area; garbage collection copies the live objects in the allocation area to the old generation, freeing the allocation area. <p> Used default compilation settings for SML/NJ. Default compilation settings enable extensive optimization (Section 2.1.6). Evaluating the impact of these optimizations on cache behavior is beyond the scope of this thesis. 6. Used default garbage-collection settings. The preferred ratio of heap size to live data is set to 5 <ref> [5] </ref>. The softmax, which is the desired upper limit on the heap size, is set to 20MB; the benchmark programs never reach this limit. The initial heap size is 1MB. I do not investigate the interaction of the sizing strategy and cache size. <p> The SML/NJ compiler uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap instead of a call stack. The heap is managed automatically using generational copying garbage collection <ref> [5, 6, 58] </ref>. In copying garbage collection [20, 32], an area of memory is reclaimed by copying the live (non-garbage) data to another area of memory. <p> The area from which the data is copied can then be reused. 46 Version 0.91 of the SML/NJ compiler uses a simple variant of generational copying garbage collection <ref> [5] </ref>. It divides memory into an old generation and an allocation area. The mutator creates new objects in the allocation area. When the allocation area becomes full, the garbage collector copies the live data in the allocation area to the old generation in a minor collection. <p> All numbers are fractions of total execution time 3.2.4 Garbage collection sizing parameters I use the default strategy for sizing the allocation area and the old generation <ref> [5] </ref>. The heap is sized as r times the size of the old generation after the old generation is collected, where r is the desired ratio of heap size to live data. I use the default system value (r=5).
Reference: [6] <author> Andrew W. Appel. </author> <title> A Runtime System. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 3(4) </volume> <pages> 343-380, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In principle, the presence of higher-order functions means that procedure activation records must be allocated on the heap. With a suitable analysis, a stack can be used to store most activation records [52]. However, using only a heap simplifies the compiler, the run-time system <ref> [6] </ref>, and the implementation of first- class continuations [40]. The decision to use only a heap is controversial because it greatly increases the amount of heap allocation, which is believed to cause poor memory system performance. 2.2 Methodology I use trace-driven simulation to evaluate the memory-system performance of programs. <p> The SML/NJ compiler uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap instead of a call stack. The heap is managed automatically using generational copying garbage collection <ref> [5, 6, 58] </ref>. In copying garbage collection [20, 32], an area of memory is reclaimed by copying the live (non-garbage) data to another area of memory.
Reference: [7] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: This thesis is divided into three parts. In the first part of the thesis, I study the performance of SML programs. I focus on two widely-held conjectures about where SML programs spend their time. First, I study the memory-system performance of SML programs, which is believed to be poor <ref> [7] </ref>. I also study the cost of automatic storage management. I find that the problem is neither memory-system performance nor the cost of automatic storage management. <p> In the TIL compiler, I take an approach to compiling languages with higher-order functions such as SML that is quite different from the approach suggested in the literature. Many researchers who have implemented languages with higher-order functions such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions. <p> In the third part of the thesis, I provide evidence to support my claims about how to compile SML. First, I establish that a "pay-as-you-go" compilation strategy is a practical strategy for compiling SML by comparing TIL to the SML/NJ compiler <ref> [7] </ref>, a widely used reference compiler for SML. I find that TIL produces good code compared to the SML/NJ compiler. Indeed, TIL often produces code that is much better than that produced by the SML/NJ compiler. <p> To study memory-system performance, I use trace-driven simulation of programs compiled by the SML/NJ compiler <ref> [7] </ref>. I instrument programs to produce a trace of the memory references made by programs, and use those traces to simulate various memory systems. This allows me to study memory-system performance across a variety of memory system architectures. benchmark program, a program that does scheduling for a VLIW machine. <p> Inlining suffices to eliminate many higher-order functions and make most of the control-flow graph known at compile time. Furthermore, I show that simple local transformations suffice to produce good code for recursive functions. Appel <ref> [7] </ref> shows that optimizations, especially inlining and uncurrying, are important to compiling SML programs. However, he does not argue for the importance of applying optimizations known to improve loops in conventional language to recursive functions in SML. Leroy [56] shows that using untagged integers significantly increases performance. <p> The first conjecture is that processors executing SML programs spend much of their time waiting for the memory system. There are two reasons to believe that this may be true. First, Appel conjectures in his book on SML compilation <ref> [7] </ref> that SML programs spend up to two-thirds of their time waiting for the memory system. Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. <p> This is joint work with Amer Diwan and Eliot Moss: Amer, Eliot, and I designed the experiment. I conducted many of the measurements and Amer modified the SML/NJ compiler <ref> [7] </ref>, QPT [12, 53, 54], and Tycho [42] so that we could instrument SML programs and simulate memory-system performance. Amer also conducted some of the measurements. <p> New objects are created in the allocation area; garbage collection copies the live objects in the allocation area to the old generation, freeing the allocation area. Generational garbage collection relies on the fact that most allocated objects die young. Thus, most objects (about 99% <ref> [7, p. 206] </ref>) are not copied from the allocation area. This makes the garbage collector efficient because it works mostly on an area of memory where it is very effective at reclaiming space. <p> The only kinds of assignable data structures are ref cells and arrays, which must be declared explicitly. The implications of this non-imperative programming style for compilation are clear: SML programs tend to do more allocation and copying than programs written in imperative languages. 2.1.6 SML/NJ compiler The SML/NJ compiler <ref> [7] </ref> is a publicly available compiler for SML. As I mention earlier, I use version 0.91 of the compiler. The compilation strategy focuses on making memory allocation inexpensive and making function calls fast. Allocation is done in-line, except for the allocation of arrays. <p> For example, a 128K cache may take longer to access than an 8K cache. 2.2.3 Benchmarks Table 2.1 describes the benchmark programs. Knuth-Bendix, Lexgen, Life, Simple, VLIW, and YACC are identical to the benchmarks that Appel <ref> [7] </ref> measures. The description of these benchmarks is taken from [7]. Table 2.2 gives the following for each benchmark: lines of SML code excluding comments and empty lines, maximum heap size, compiled code size, and user-mode CPU time on a DECStation 5000/200. <p> For example, a 128K cache may take longer to access than an 8K cache. 2.2.3 Benchmarks Table 2.1 describes the benchmark programs. Knuth-Bendix, Lexgen, Life, Simple, VLIW, and YACC are identical to the benchmarks that Appel <ref> [7] </ref> measures. The description of these benchmarks is taken from [7]. Table 2.2 gives the following for each benchmark: lines of SML code excluding comments and empty lines, maximum heap size, compiled code size, and user-mode CPU time on a DECStation 5000/200. <p> Memory-system features such as write buffers and page-mode writes interact with the costs of hits and misses in the cache and should be simulated to give a correct picture of memory-system behavior. I simulate the entire memory system. Appel <ref> [7] </ref> estimates CPI for the SML/NJ system on a single machine using elapsed time and instruction counts. His CPI differs substantially from mine. However, Appel confirms my measurements by personal communication and later work [8]. The reason for the difference is that instructions were undercounted in his measurements. <p> In Sec- tion 3.2.4, I describe the criteria that the system uses to decide when to collect the whole heap. Generational garbage collection is efficient because most allocated objects die young (about 99% <ref> [7, p. 206] </ref>) and few objects are copied from the allocation area. Before a mutator can allocate an object, it must check that there is sufficient space on the heap to allocate the object. If not, it must invoke the garbage collector. <p> example, the SML type datatype d = NONE | SOME of int * int is represented as Enumorrec 1 (Cons (Int; Cons (Int; N il))) The special sum types Enum, Enumorrec, and Enumorsum allow LMLI to directly ex <br>- press the various strategies for representing SML datatypes given by Appel <ref> [7] </ref>. The Rectype constructor defines recursive types. It takes a list of variables and constructors and a constructor , binds the variables mutually recursively to the constructors, and places in their scope. <p> The operators enum enumorsum and sum enumorsum provide similar operations for enumorsum sums. The operators tostring and fromstring coerce values of string types to the actual representation of strings. LMLI uses a string representation similar to that used by Appel <ref> [7] </ref>. Strings are represented as either a character or a pair consisting of the length of the string (in bytes) and an integer array. <p> The types and values corresponding to these free variables are placed in records. These records are paired with the code to form an abstract closure. TIL uses a flat environment representation for type and value environments <ref> [7] </ref>. For known functions, TIL generates closed code but avoids creating environments or a closure. Following Kranz [52], we modify the call sites of known functions to pass free variables as additional arguments. TIL closes over only variables which are function arguments or are bound within functions. <p> I also have described each phase of the TIL compiler and given an example that illustrates how TIL compiles SML programs to machine code. 86 Chapter 5 The TIL optimizer In this chapter, I describe TIL's optimizer. The optimizer is based on the SML/NJ optimizer <ref> [7, 10] </ref>. It includes all the optimizations that Appel found to be important for SML [7]. However, it uses improved algorithms for inlining and uncurrying. I improved these algorithms because it is particularly important to eliminate higher-order functions before doing loop optimizations [83]. I have organized this chapter as follows. <p> The optimizer is based on the SML/NJ optimizer [7, 10]. It includes all the optimizations that Appel found to be important for SML <ref> [7] </ref>. However, it uses improved algorithms for inlining and uncurrying. I improved these algorithms because it is particularly important to eliminate higher-order functions before doing loop optimizations [83]. I have organized this chapter as follows. First, I discuss the intermediate language used by the optimizer, B-form. <p> the application using the same procedure used by Algorithm 2. 5.4.2 Related work My basic inlining strategy: * inline a function if it is called only once * or inline a function if it is suitably small and non-recursive is a simpler and more conservative version of Appel's inlining strategy <ref> [7] </ref>. Appel's strategy inlines recursive functions, but my strategy does not (directly) inline recursive functions. From examining code produced by the SML/NJ compiler, I believe that inlining recursive functions using a simple local syntactic criteria often leads to code expansion without any significant improvement in execution time. <p> Thus, the running time of the entire algorithm becomes O (N 2 ). However, empirical evidence demonstrates that c = 5 is a reasonable assumption. 5.5.3 Related work This uncurrying strategy was inspired by the uncurrying strategy suggested by Appel <ref> [7] </ref>. His uncurrying strategy is to apply a simple syntactic transformation and then do inline expansion. His transformation is phrased in terms of CPS programs, but it can be used for B-form programs easily. <p> I believe that my algorithm, even though it is more complex than this algorithm, is better because it reliably uncurries functions with many arguments. 5.6 Other optimizations The optimizer also implements the following optimizations, which were originally described for SML by Appel and Jim <ref> [7, 10, 9] </ref>: * dead-code elimination: eliminate unreferenced, pure expressions and functions. * constant folding: reduce arithmetic operations, switches, and typecases on constant values, as well as projections from known records. * sinking: push pure expressions used in only one branch of a switch into that branch. <p> For uncurrying, I have presented an algorithm which does a better job of uncurrying functions with a large number of arguments than the algorithm presented by Appel <ref> [7] </ref>. In Chapter 10, I will show that the algorithms for uncurrying and inlining eliminate nearly all higher-order or polymorphic functions when whole programs are compiled. <p> case tf of dfl (t 1 : : : t n ):de : dfl (t 1 : : : t n ):c d (d)e c ta (a) = case a of dc 0 : de : dc 0 : c d (d)e that used by Appel in the SML/NJ compiler <ref> [7] </ref>. First, a round of reduction optimizations are done. These optimization include constant-folding, common-subexpression elimination, redundant switch elimination, inlining of functions only called once, invariant removal, hoisting of constant expressions, and dead-code elimination. <p> Kranz et al. [48, 52, 51] also use source-to-source transformations in the ORBIT compiler for SCHEME. Their set of optimizations is similar to Steele's set, with one notable difference: they propagate values, such as small function definitions, across compilation units. Appel and Jim <ref> [10, 7] </ref> continue in this vein by using source-to-source transformations in compiling SML. Their optimizer does inlining, uncurrying, constant-folding, and dead-code elimination. In addition, they also do hoisting (moving variable bindings up or down in scope) and a form of CSE. <p> However, they do not hoist expressions of out of function bodies (that is, they do not do a form of hoisting analogous to invariant removal). Appel <ref> [7] </ref> finds that hoisting provides small improvements in execution time (5-10%) and that CSE provides no improvement in execution time. It is unclear why his form of CSE provides no improvement, because it is quite similar to my form of CSE. <p> My approach to compiling B-form programs to machine code is quite different from the approach suggested in the literature. Many researchers who have implemented higher-order languages such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions. <p> If the function is escaping, closure conversion builds a closure for the function, which consists of a constructor environment, a value environment, and a closed version of the function. The environments respectively bind free constructor variables and value variables; closure conversion represents them as flat records <ref> [7] </ref>. The closed version of the function takes the environments as two additional arguments; closure conversion changes all references to free variables to be projections from the environments. Closure conversion closes only over variables bound within functions; it does not close over top-level variables that are not bound within functions. <p> Instead, it changes all applications of the function to use the call construct, and passes the free variables as additional arguments. Closure conversion uses a flat environment representation for the constructor and value environments <ref> [7] </ref>; it builds them using the Tuple constructors and record expressions respectively. Closure conversion closes over only variables that are function arguments or are bound within functions. <p> The benchmarks cover a range of application areas including scientific computing, list-processing, systems programming, and compilers. Appel used some of these programs to measure ML performance <ref> [7] </ref>. The set of benchmark programs that I use in this part of the thesis includes only some of the programs that I used in the first part of the thesis. <p> For Lexgen and Simple, they increase the amount of data copied by 1 and 4%, respectively. This is presumably because the loop optimizations are not safe for space <ref> [7] </ref>. For other programs, such as Checksum, Life, and FFT, they nearly eliminate all copying of data. They have no effect on Matmult, because the generational collector does not copy any data when Matmult runs.
Reference: [8] <author> Andrew W. Appel. </author> <type> Personal communication. </type> <month> March 22 </month> <year> 1993. </year>
Reference-contexts: I simulate the entire memory system. Appel [7] estimates CPI for the SML/NJ system on a single machine using elapsed time and instruction counts. His CPI differs substantially from mine. However, Appel confirms my measurements by personal communication and later work <ref> [8] </ref>. The reason for the difference is that instructions were undercounted in his measurements. Jouppi [45] studies the effect of cache write policies on the performance of C and Fortran programs.
Reference: [9] <author> Andrew W. Appel and Trevor Jim. </author> <title> Making lambda calculus smaller, faster. </title> <journal> Journal of Functional Programming, </journal> <note> 1995. accepted for publication. </note>
Reference-contexts: I believe that my algorithm, even though it is more complex than this algorithm, is better because it reliably uncurries functions with many arguments. 5.6 Other optimizations The optimizer also implements the following optimizations, which were originally described for SML by Appel and Jim <ref> [7, 10, 9] </ref>: * dead-code elimination: eliminate unreferenced, pure expressions and functions. * constant folding: reduce arithmetic operations, switches, and typecases on constant values, as well as projections from known records. * sinking: push pure expressions used in only one branch of a switch into that branch. <p> It is O (N 4 ) if transitive closure is used during redundant comparision elimination. Iterating optimizations during the "shrink" phase is useful because these optimizations interact, particularly with inlining of functions only called once <ref> [9] </ref>. For example, inlining of a function only called once may expose the fact that a variable passed to a function is in fact unused, thus presenting an opportunity for dead-code elimination.
Reference: [10] <author> Andrew W. Appel and Trevor Y. Jim. </author> <title> Continuation-Passing, Closure-Passing Style. </title> <booktitle> In Proceedings of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 293-302, </pages> <address> Austin, Texas, </address> <month> January </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: Optimizations done by the compiler include inlining, passing function arguments in registers, register targeting, constant folding, code hoisting, uncurrying, and instruction scheduling. The most controversial design decision in the compiler is to allocate procedure activation records on the heap instead of the stack <ref> [4, 10] </ref>. In principle, the presence of higher-order functions means that procedure activation records must be allocated on the heap. With a suitable analysis, a stack can be used to store most activation records [52]. <p> I also have described each phase of the TIL compiler and given an example that illustrates how TIL compiles SML programs to machine code. 86 Chapter 5 The TIL optimizer In this chapter, I describe TIL's optimizer. The optimizer is based on the SML/NJ optimizer <ref> [7, 10] </ref>. It includes all the optimizations that Appel found to be important for SML [7]. However, it uses improved algorithms for inlining and uncurrying. I improved these algorithms because it is particularly important to eliminate higher-order functions before doing loop optimizations [83]. I have organized this chapter as follows. <p> I believe that my algorithm, even though it is more complex than this algorithm, is better because it reliably uncurries functions with many arguments. 5.6 Other optimizations The optimizer also implements the following optimizations, which were originally described for SML by Appel and Jim <ref> [7, 10, 9] </ref>: * dead-code elimination: eliminate unreferenced, pure expressions and functions. * constant folding: reduce arithmetic operations, switches, and typecases on constant values, as well as projections from known records. * sinking: push pure expressions used in only one branch of a switch into that branch. <p> Kranz et al. [48, 52, 51] also use source-to-source transformations in the ORBIT compiler for SCHEME. Their set of optimizations is similar to Steele's set, with one notable difference: they propagate values, such as small function definitions, across compilation units. Appel and Jim <ref> [10, 7] </ref> continue in this vein by using source-to-source transformations in compiling SML. Their optimizer does inlining, uncurrying, constant-folding, and dead-code elimination. In addition, they also do hoisting (moving variable bindings up or down in scope) and a form of CSE.
Reference: [11] <author> Andrew W. Appel, James S. Mattson, and David Tarditi. </author> <title> A lexical analyzer generator for Standard ML. Distributed with Standard ML of New Jersey, </title> <year> 1989. </year>
Reference-contexts: The input is the sample session from Section 7.5 of [22]. Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple <br>- mented by Gerard Huet, processing some axioms of geometry. Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi <ref> [11] </ref>, processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade [72], running 50 generations of a glider gun. It is implemented using lists. PIA The Perspective Inversion Algorithm [94] decides the location of an object in a perspective video image. <p> FFT 246 Fast fourier transform, multiplying polynomials up to degree 65,536 Knuth-Bendix 618 An implementation of the Knuth-Bendix completion algorithm. Lexgen 1123 A lexical-analyzer generator <ref> [11] </ref>, processing the lexical descrip <p>- tion of Standard ML. Life 146 The game of Life implemented using lists [72]. Matmult 62 Integer matrix multiply, on 200x200 integer arrays. PIA 2065 The Perspective Inversion Algorithm [94] deciding the location of an object in a perspective video image.
Reference: [12] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In 19th Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: This is joint work with Amer Diwan and Eliot Moss: Amer, Eliot, and I designed the experiment. I conducted many of the measurements and Amer modified the SML/NJ compiler [7], QPT <ref> [12, 53, 54] </ref>, and Tycho [42] so that we could instrument SML programs and simulate memory-system performance. Amer also conducted some of the measurements. <p> Section 2.2.2 states my assumptions and argues that they are reasonable. Section 2.2.3 describes and characterizes 21 the benchmark programs that I use. Section 2.2.4 describes the metrics that I use to measure memory-system performance. 2.2.1 Tools Amer Diwan extended QPT (Quick Program Profiler and Tracer) <ref> [54, 12, 53] </ref> to produce memory traces for SML/NJ programs. QPT rewrites an executable program to produce compressed trace information; QPT also produces a program-specific regeneration program that expands the compressed trace into a full trace. <p> This is joint work with Amer Diwan. Amer and I designed the experiment and did the measurements. Amer also made additional changes to the SML/NJ compiler and QPT <ref> [12, 53, 54] </ref> beyond those described in Chapter 2 so that we could make the measurements. I measure the cost of storage management for eight programs on a DECstation 5000/200 [28]. I chose the DECStation 5000/200 because its memory system is favorable to programs that heap allocate intensively.
Reference: [13] <author> J.M. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year> <month> 259 </month>
Reference-contexts: CSE can be done for applications of functions even when the functions may not terminate or may raise exceptions; the only requirement is that functions not modify or use the store. Barth's work <ref> [13] </ref> and related work on computing interprocedural summary information could be extended to compute effects that occur when a function is applied. Interprocedural summary information tells what variables are used or modified by functions, or what variables are aliased to other variables.
Reference: [14] <author> Edoardo Biagioni, Robert Harper, Peter Lee, and Brian Milnes. </author> <title> Signatures for a network protocol stack: A systems application of Standard ML. </title> <booktitle> In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 55-64, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: The fact that the TIL compiler does not yet compile the entire SML module system is not a fundamental limitation of the TIL approach. Harper and Stone [38] show how to extend the TIL framework to support modules. 187 Program lines Description Checksum 241 Checksum fragment from the Foxnet <ref> [14] </ref>, doing 5000 checksums on a 4096-byte array. FFT 246 Fast fourier transform, multiplying polynomials up to degree 65,536 Knuth-Bendix 618 An implementation of the Knuth-Bendix completion algorithm. Lexgen 1123 A lexical-analyzer generator [11], processing the lexical descrip <p>- tion of Standard ML.
Reference: [15] <author> Lars Birkedal, Nick Rothwell, Mads Tofte, and David N. </author> <title> Tur ner. The ML Kit, </title> <type> Version 1. Technical Report 93/14, </type> <institution> DIKU, </institution> <year> 1993. </year>
Reference-contexts: The phase after closure conversion use an untyped language where variables are annotated with garbage collection information. The low-level phases of the compiler use languages where registers are annotated with garbage collection information. 4.2.1 Front end The first phase of TIL uses the front-end of the ML Kit Compiler <ref> [15] </ref> to parse and elaborate (type check) SML source code. The ML Kit Compiler produces annotated abstract syntax for all of SML, which it typechecks using the Hindley-Milner typechecking algorithm. It then compiles a subset of this abstract syntax to an explicitly-typed language called Lambda.
Reference: [16] <author> Hans-Juergen Boehm. </author> <title> Space-efficient conservative garbage collection. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation [2], </booktitle> <pages> pages 197-206. </pages>
Reference-contexts: Recording the information at compile time makes it possible for code to use untagged representations. Unlike so-called conservative collectors (see for example <ref> [16, 26] </ref>), the information recorded by TIL is sufficient to collect all unreachable objects. Collection is "nearly" tag-free because tags are placed only on heap-allocated data structures (records and arrays); values in registers, on the stack, and within data structures remain tagless.
Reference: [17] <author> Dianne Ellen Britton. </author> <title> Heap storage management for the programming language Pascal. </title> <type> Master's thesis, </type> <institution> University of Arizona, </institution> <year> 1975. </year>
Reference-contexts: The collector looks up the layout of each stack-frame to determine which stack locations to trace. TIL records additional liveness information for each variable so that the garbage collector can avoid tracing pointers that are no longer needed. This approach is well-understood for monomorphic languages requiring garbage collection <ref> [17] </ref>. Following Tolmach [90], we extended it to a polymorphic language as follows: when a variable whose type is unknown is saved in a stack frame, the type of the variable is also saved in the stack frame.
Reference: [18] <author> David R. Chase. </author> <title> Safety considerations for storage allocation optimizations. </title> <booktitle> Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <volume> 23(7) </volume> <pages> 1-10, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The first three entries are the cost of garbage collection. The remaining rows are the storage management costs in the mutator. The one instruction-level cost of storage management that I do not measure is the effect of storage management on program optimization <ref> [18] </ref>. Diwan et al. [30] present techniques that allow extensive optimization even using copying collection with unambiguous roots. Storage management also affects the memory-system cost that a mutator incurs. I am unable to measure this effect directly.
Reference: [19] <author> J. Bradley Chen and Brian N. Bershad. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Fourteenth Symposium on Operating System Principles. ACM, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Specifically, for the memory-system configuration corresponding to the DECStation 5000/200, the memory-system performance of the SML benchmarks that I measure is comparable to that of C and Fortran programs <ref> [19] </ref>: programs run only 3 to 13% slower due to data-cache misses than they would run with a zero-latency memory. <p> First, surprisingly, the memory-system performance of SML programs is good for some memory system configurations corresponding to actual machines. For example, for the memory-system configuration corresponding to the DECStation 5000/200, the memory-system performance of SML programs is comparable to that of C and Fortran programs <ref> [19] </ref>: programs run only 3 to 13% slower due to data-cache misses than they would run with a zero-latency memory. Second, heap allocation has a dominant effect on the memory-system performance of SML programs. <p> Recall that the 64K direct-mapped configuration corresponds to the DECStation 5000/200 memory system. The memory system overhead of SML/NJ programs on the DECStation 5000/200 is similar to that of C and Fortran programs <ref> [19] </ref>. <p> To my surprise, I find the memory-system performance of SML programs is quite good on some memory systems. In particular, on an actual machine (the DECStation 5000/200), I find that the memory-system performance of SML programs is comparable to that of C and Fortran programs <ref> [19] </ref>: programs run only 3 to 13% slower due to data-cache misses than they would run with a zero-latency memory. I also find that heap allocation has a dominant effect on the memory-system performance of SML programs.
Reference: [20] <author> C.J. </author> <title> Cheney. A nonrecursive list compacting algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year>
Reference-contexts: CPI is machine dependent because it is calculated using actual penalties. 2.1.3 Copying garbage collection A copying garbage collector <ref> [32, 20] </ref> reclaims an area of memory by copying all the live (non- garbage) data to another area of memory. The area can then be re-used. Because copying garbage collection reclaims memory in large contiguous areas, programs can allocate objects sequentially from such areas in a few instructions. <p> The SML/NJ compiler uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap instead of a call stack. The heap is managed automatically using generational copying garbage collection [5, 6, 58]. In copying garbage collection <ref> [20, 32] </ref>, an area of memory is reclaimed by copying the live (non-garbage) data to another area of memory. The area from which the data is copied can then be reused. 46 Version 0.91 of the SML/NJ compiler uses a simple variant of generational copying garbage collection [5]. <p> When the size of the old generation becomes sufficiently large, the garbage collector collects the entire heap in a major collection. The garbage collector copies live objects using a Cheney scan <ref> [20] </ref>, which copies objects in a breadth-first order. In Sec- tion 3.2.4, I describe the criteria that the system uses to decide when to collect the whole heap.
Reference: [21] <author> Frederick C. Chow. </author> <title> A Portable Machine-Independent Global Optimizer | Design and Measurements. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> December </month> <year> 1983. </year> <type> Technical Report No. </type> <pages> 83-254. </pages>
Reference-contexts: I chose the code motion optimizations because I believe they are likely to demonstrate the importance of applying "loop" optimizations: these kinds of optimizations are well- known to be important for more conventional languages <ref> [21] </ref>. The array-bounds checking optimizations are intended to support the code motion optimizations: Chow shows that array bounds checking can interfere with code motion optimizations [21]. It is important to emphasize that the algorithms are new, even though most of the optimizations are well-known. <p> they are likely to demonstrate the importance of applying "loop" optimizations: these kinds of optimizations are well- known to be important for more conventional languages <ref> [21] </ref>. The array-bounds checking optimizations are intended to support the code motion optimizations: Chow shows that array bounds checking can interfere with code motion optimizations [21]. It is important to emphasize that the algorithms are new, even though most of the optimizations are well-known. I designed new algorithms because I apply these optimizations to programs in a -calculus-based intermediate language (B-form).
Reference: [22] <author> Rance Cleaveland, Joachim Parrow, and Bernhard Steffen. </author> <title> The Concurrency Workbench: A semantics-based tool for the verification of concurrent systems. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 15(1) </volume> <pages> 36-72, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: All the benchmarks have long traces; most related works (described in Section 2.4) use traces that are an order of magnitude smaller. Also, the benchmark programs do few assignments; the majority of the writes are initializing writes. 23 Program Description CW The Concurrency Workbench <ref> [22] </ref> is a tool for analyzing networks of finite state processes expressed in Milner's Calculus of Communicating Systems. The input is the sample session from Section 7.5 of [22]. Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple <br>- mented by Gerard Huet, processing some axioms of geometry. <p> Also, the benchmark programs do few assignments; the majority of the writes are initializing writes. 23 Program Description CW The Concurrency Workbench <ref> [22] </ref> is a tool for analyzing networks of finite state processes expressed in Milner's Calculus of Communicating Systems. The input is the sample session from Section 7.5 of [22]. Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple <br>- mented by Gerard Huet, processing some axioms of geometry. Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi [11], processing the lexical description of Standard ML.
Reference: [23] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, Livermore, </institution> <address> CA, </address> <month> February </month> <year> 1978. </year>
Reference-contexts: It is implemented using lists. PIA The Perspective Inversion Algorithm [94] decides the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark <ref> [23] </ref>, translated into ID [31], and then translated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan <br>- skin. YACC A LALR (1) parser generator, implemented by David R. Tarditi [89], pro <br>- cessing the grammar of Standard ML.
Reference: [24] <author> Cypress Semiconductor, Ross Technology Subsidiary. </author> <title> SPARC RISC User's Guide, </title> <note> second edition, </note> <month> February </month> <year> 1990. </year>
Reference-contexts: There are many variables involved and the dependencies between the variables are complex. Therefore I can study only a subset of the memory-system design space. I restrict this study to features found in RISC workstations of the late 1980's <ref> [29, 28, 84, 24] </ref>. Table 2.6 summarizes the cache organizations that I simulated. Table 2.7 lists the memory-system organizations of some popular machines. I simulate only separate instruction and data caches (i.e., no unified caches). <p> In Chapter 2, I show that halving the cache size for the DECStation 5000/200 organization affects performance little. Some other memory-system organizations, such as the SPARCStationII <ref> [24] </ref>, are more sensitive to cache size. Increasing the allocation area size can increase the memory-system cost greatly.
Reference: [25] <author> Olivier Danvy and Julia L. Lawall. </author> <title> Back to direct style II: First-class continuations. </title> <booktitle> In LFP '92 [57], </booktitle> <pages> pages 299-310. </pages>
Reference-contexts: Most compilers use a DS intermediate language. From a theoretical point of view, the two languages are essentially equivalent: programs can be converted from one representation to the other in asymptotically-efficient time <ref> [25, 77, 78] </ref>. The choice of which representation to use is a matter of engineering. I chose to use a DS intermediate language. This has two advantages. First, a DS language is more amenable to the kinds of interprocedural program analyses that I want to do than a CPS language.
Reference: [26] <author> A. Demers, M. Weiser, B. Hayes, H. Boehm, D. Bobrow, and S. Shenker. </author> <title> Combining generational and conservative garbage collection: Framework and implementations. </title> <booktitle> In Conference Record of the 17th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year> <journal> ACM. </journal> <volume> 260 </volume>
Reference-contexts: Recording the information at compile time makes it possible for code to use untagged representations. Unlike so-called conservative collectors (see for example <ref> [16, 26] </ref>), the information recorded by TIL is sufficient to collect all unreachable objects. Collection is "nearly" tag-free because tags are placed only on heap-allocated data structures (records and arrays); values in registers, on the stack, and within data structures remain tagless.
Reference: [27] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <type> Technical Report CU-CS-665-93, </type> <institution> University of Colorado, </institution> <year> 1993. </year>
Reference-contexts: He finds that tag insertion and removal 59 costs about 4.5% with the best software scheme. There have been several studies of the cost of storage management in languages with explicitly-managed heap storage and stack allocation of procedure activation records. Detlefs <ref> [27] </ref> measures time spent in allocation and deallocation routines, but does not measure the cost of managing the stack.
Reference: [28] <author> Digital Equipment Corporation. </author> <title> DS5000/200 KN02 System Module Functional Specification. </title>
Reference-contexts: Cycles per useful instruction is a measure of how well the memory system is being utilized. A lower number is better. The memory system for the 64K point corresponds closely to the memory system for an actual machine, the DECStation 5000/200 <ref> [28] </ref>. At this point, the memory system performance is quite good (the cycles per useful instruction is about 1.4). Most cache misses are due to the instruction cache. This is not surprising, because VLIW is substantially larger than the cache (its executable size is about 500K). <p> There are many variables involved and the dependencies between the variables are complex. Therefore I can study only a subset of the memory-system design space. I restrict this study to features found in RISC workstations of the late 1980's <ref> [29, 28, 84, 24] </ref>. Table 2.6 summarizes the cache organizations that I simulated. Table 2.7 lists the memory-system organizations of some popular machines. I simulate only separate instruction and data caches (i.e., no unified caches). <p> Amer also made additional changes to the SML/NJ compiler and QPT [12, 53, 54] beyond those described in Chapter 2 so that we could make the measurements. I measure the cost of storage management for eight programs on a DECstation 5000/200 <ref> [28] </ref>. I chose the DECStation 5000/200 because its memory system is favorable to programs that heap allocate intensively. Chapter 2 shows that a less favorable memory-system organization would increase the cost of storage management by increasing the cost of allocation.
Reference: [29] <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA. </address> <booktitle> DECStation 3100 Desktop Workstation Function Specification, </booktitle> <address> 1.3 edition, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: There are many variables involved and the dependencies between the variables are complex. Therefore I can study only a subset of the memory-system design space. I restrict this study to features found in RISC workstations of the late 1980's <ref> [29, 28, 84, 24] </ref>. Table 2.6 summarizes the cache organizations that I simulated. Table 2.7 lists the memory-system organizations of some popular machines. I simulate only separate instruction and data caches (i.e., no unified caches).
Reference: [30] <author> Amer Diwan, J. Eliot B. Moss, and Richard L. Hudson. </author> <title> Compiler support for garbage collection in a statically typed language. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 273-282, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year> <title> SIGPLAN, </title> <publisher> ACM Press. </publisher>
Reference-contexts: The first three entries are the cost of garbage collection. The remaining rows are the storage management costs in the mutator. The one instruction-level cost of storage management that I do not measure is the effect of storage management on program optimization [18]. Diwan et al. <ref> [30] </ref> present techniques that allow extensive optimization even using copying collection with unambiguous roots. Storage management also affects the memory-system cost that a mutator incurs. I am unable to measure this effect directly. I defer discussing this effect in detail and how I measure it to Section 3.3.3.
Reference: [31] <author> K. Ekanadham and Arvind. </author> <title> SIMPLE: An exercise in future scientific programming. Technical Report Computation Structures Group Memo 273, </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <month> July </month> <year> 1987. </year> <note> Simultaneously published as IBM/T. J. </note> <institution> Watson Research Center Research Report 12686, Yorktown Heights, NY. </institution>
Reference-contexts: It is implemented using lists. PIA The Perspective Inversion Algorithm [94] decides the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark [23], translated into ID <ref> [31] </ref>, and then translated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan <br>- skin. YACC A LALR (1) parser generator, implemented by David R. Tarditi [89], pro <br>- cessing the grammar of Standard ML. <p> Life 146 The game of Life implemented using lists [72]. Matmult 62 Integer matrix multiply, on 200x200 integer arrays. PIA 2065 The Perspective Inversion Algorithm [94] deciding the location of an object in a perspective video image. Simple 870 A spherical fluid-dynamics program <ref> [31] </ref>, run for 4 iterations with grid size of 100. Table 8.1: Benchmark Programs I compiled whole programs during benchmarking (the programs were single closed modules). I extended the built-in ML types with safe 2-dimensional arrays.
Reference: [32] <author> Robert R. Fenichel and Jerome C. Yochelson. </author> <title> A LISP garbage-collector for virtualmemory computer systems. </title> <journal> Communications of the ACM, </journal> <volume> 12(11) </volume> <pages> 611-612, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: CPI is machine dependent because it is calculated using actual penalties. 2.1.3 Copying garbage collection A copying garbage collector <ref> [32, 20] </ref> reclaims an area of memory by copying all the live (non- garbage) data to another area of memory. The area can then be re-used. Because copying garbage collection reclaims memory in large contiguous areas, programs can allocate objects sequentially from such areas in a few instructions. <p> The SML/NJ compiler uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap instead of a call stack. The heap is managed automatically using generational copying garbage collection [5, 6, 58]. In copying garbage collection <ref> [20, 32] </ref>, an area of memory is reclaimed by copying the live (non-garbage) data to another area of memory. The area from which the data is copied can then be reused. 46 Version 0.91 of the SML/NJ compiler uses a simple variant of generational copying garbage collection [5].
Reference: [33] <author> Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias Felleisen. </author> <title> The essence of compiling with continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation [2], </booktitle> <pages> pages 237-247. </pages>
Reference-contexts: Before optimization, TIL translates Lmli to a subset of Lmli called B-form. B-form, based on A-Normal-Form <ref> [33] </ref>, is a more regular intermediate language than Lmli that facilitates optimization. The translation from Lmli names all intermediate computations and binds them to variables by a let-construct. It also names all potentially heap-allocated values, including strings, records and functions. <p> B-form is a subset of LMLI that is a typed, direct-style intermediate language similar to A-Normal- Form <ref> [33] </ref>. I use a subset of LMLI instead of LMLI to simplify the writing of the optimizer. As I will explain, the subset is more easily manipulated by the optimizer. It is important to design the intermediate language for an optimizer carefully. <p> For example, it makes inlining a function easy: simply apply the fi v rule of the call-by-value -calculus. 5.1.4 B-form B-form is a conceptual subset of LMLI similar to A-Normal-Form <ref> [33] </ref>. Figure 5.1 shows the syntax for B-form constructors and types. B-form has the same kinds as LMLI and the same arity-0 and arity-1 primitive constructors as LMLI. The language of constructors is divided into three levels: constructor values, constructor expressions, and constructor declarations.
Reference: [34] <author> David K. Gifford, Pierre Jouvelot, John M. Lucassen, and Mark A. Sheldon. </author> <title> Fx87 reference manual. </title> <type> Technical Report MIT/LCS/TR-407, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The complexity of her analysis appears to be at least exponential. Lucassen and Gifford <ref> [59, 34] </ref> describe a type-and-effect system where expressions are explicitly annotated with types and descriptions of effects. Jouvelot and Gifford [46] show how to reconstruct types-and-effects in a language with explicit polymorphism, where quantification occurs over types and effects.
Reference: [35] <author> Dirk Grunwald, Benjamin Zorn, and Robert Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 177-186, </pages> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: There have been several studies of the cost of storage management in languages with explicitly-managed heap storage and stack allocation of procedure activation records. Detlefs [27] measures time spent in allocation and deallocation routines, but does not measure the cost of managing the stack. Grunwald et al. <ref> [35] </ref> finds that the implementation of explicit heap management can affect the performance of allocation-intensive C programs significantly. 3.5 Conclusion In this chapter, I study the cost of automatic storage management for SML programs.
Reference: [36] <author> Mary W. Hall and Ken Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(3) </volume> <pages> 227-242, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [37] <author> Robert Harper and Greg Morrisett. </author> <title> Compiling polymorphism using intensional type analysis. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: LMLI is the actual intermediate language used by the TIL compiler, and it is the language that I use in the next two chapters on optimization. Morrisett and Harper based their design on their intermediate language ML i <ref> [37] </ref>, which they used to explore the theory of intensional polymorphism. In this section, I describe the syntax of ML i and informally describe its semantics. I then present the syntax of LMLI and informally describe LMLI's semantics. <p> It then applies arrow to the resulting types. Typerec is useful for specifying the types of functions that operate inductively over the structure of types. Harper and Morrisett <ref> [37] </ref> present several examples of such functions, including a function for flattening nested tuples and functions for marshalling and unmar- shalling data that is transmitted over a network. <p> It then applies the expression e arrow to 1 , 2 , v 1 and v 2 . Morrisett and Harper <ref> [37] </ref> show that typechecking for ML i is decidable, despite its rich type system. 4.1.2 Overview of LMLI In this section, I describe LMLI, the intermediate language of the TIL compiler. Kinds, constructors, and types In Figure 4.2, I describe the syntax of kinds, constructors, and types of LMLI. <p> There are at least three different techniques that should be compared: using a conventional universal representation, where data is represented as a tagged machine word, Leroy's approach [56], where polymorphic values are placed in a universal representation but monomorphic values are kept in a native machine representation, and intensional polymorphism <ref> [37] </ref>. * comparing different inlining techniques. It would be interesting to compare SML/NJ's inlining strategy to TIL's inlining strategy. For example, TIL does not inline recursive functions, while SML/NJ does. * comparing different techniques for automatic storage management.
Reference: [38] <author> Robert Harper and Chris Stone. </author> <title> A type-theoretic account of Standard ML 1996 (ver-sion 2). </title> <type> Technical Report CMU-CS-96-136R, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: I was able to eliminate these language constructs from Lexgen and Simple, however. The fact that the TIL compiler does not yet compile the entire SML module system is not a fundamental limitation of the TIL approach. Harper and Stone <ref> [38] </ref> show how to extend the TIL framework to support modules. 187 Program lines Description Checksum 241 Checksum fragment from the Foxnet [14], doing 5000 checksums on a 4096-byte array. FFT 246 Fast fourier transform, multiplying polynomials up to degree 65,536 Knuth-Bendix 618 An implementation of the Knuth-Bendix completion algorithm.
Reference: [39] <author> Nevin Heintze. </author> <title> Set Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> October </month> <year> 1992. </year> <month> 261 </month>
Reference-contexts: The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [40] <author> Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing Control in the Presence of First-Class Continuations. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 66-77. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: With a suitable analysis, a stack can be used to store most activation records [52]. However, using only a heap simplifies the compiler, the run-time system [6], and the implementation of first- class continuations <ref> [40] </ref>. The decision to use only a heap is controversial because it greatly increases the amount of heap allocation, which is believed to cause poor memory system performance. 2.2 Methodology I use trace-driven simulation to evaluate the memory-system performance of programs.
Reference: [41] <author> Mark D. Hill. </author> <title> A case for direct mapped caches. </title> <journal> Computer, </journal> <volume> 21(12) </volume> <pages> 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: A miss is a capacity miss if it results from the cache not being large enough to hold all the memory blocks used by a program. It is a conflict miss if it results from two memory blocks mapping to the same set <ref> [41] </ref>. A read miss is handled by copying the missing block from main memory to the cache. A write hit is always written to the cache. There are several policies for handling a write miss, which differ in their performance penalties. <p> The maximum benefit from higher associativity is obtained for small cache sizes less than 16K. However, increasing associativity may increase CPU cycle time and thus the improvements may not be realized in practice <ref> [41] </ref>. From Figures 2.3, 2.4, and 2.5 we see that higher associativity improves the instruction- cache performance but has little or no effect on data-cache performance. Surprisingly, for direct-mapped caches (Figures 2.3 (a), 2.4 (a), and 2.5 (a)) the instruction-cache penalty is substantial for 128K or smaller caches.
Reference: [42] <author> M.D. Hill and A.J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This is joint work with Amer Diwan and Eliot Moss: Amer, Eliot, and I designed the experiment. I conducted many of the measurements and Amer modified the SML/NJ compiler [7], QPT [12, 53, 54], and Tycho <ref> [42] </ref> so that we could instrument SML programs and simulate memory-system performance. Amer also conducted some of the measurements. <p> Because QPT operates on the executable program, it can trace the SML code and the garbage collector (which is written in C). Amer Diwan also extended Tycho <ref> [42] </ref> for the memory system simulations. His extensions to Tycho include a write-buffer simulator. I obtain allocation statistics by using an allocation profiler built into SML/NJ. The profiler instruments intermediate code to increment appropriate elements of a count array on every allocation. <p> The tracing mechanism is non-intrusive: the traces produced by QPT correspond to addresses in the original programs rather than those in the instrumented programs. 3.2.2 Memory system simulation I simulate the DECstation 5000/200 memory system using the extended version of Tycho <ref> [42] </ref> described in Section 2.2.1. Table 3.2 summarizes the memory system of the DECstation 5000/200. The DECstation 5000/200 has a split instruction and data cache. The instruction cache is direct mapped and is composed of blocks of 16 bytes each.
Reference: [43] <author> F. Honsell, I. A. Mason, S. F. Smith, and C. L. Talcott. </author> <title> A variable typed logic of effects. </title> <journal> Information and Computation, </journal> <volume> 119(1) </volume> <pages> 55-90, </pages> <year> 1995. </year>
Reference-contexts: The fi rule, however, does not support this form of contextual reasoning. 120 To demonstrate the correctness of CSE of expressions that may raise exceptions or not terminate, we can use contextual assertions <ref> [43, 60] </ref> or conditional lambda-theories [93]. These variants of the call-by-value -calculus justify the correctness of the transformations that are done by Algorithm 6. <p> We can show the correctness of the transformations done by Algorithm 7, like the correctness of the transformations done by Algorithm 6, using variants of the call-by-value -calculus. Specifically, we need to use contextual assertions <ref> [43, 60] </ref> or conditional lambda- theories [93] to show the correctness of the transformations done by Algorithm 7. Algorithm 7 has an O (N log N ) asymptotic complexity, like Algorithm 6. The algorithm traverses each node in the program syntax tree.
Reference: [44] <author> Neil D. Jones. </author> <title> Flow analysis of lambda expressions. </title>
Reference-contexts: The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [45] <author> Norman P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Write allocate/subblock placement has a lower write-miss penalty than write-allocate/no subblock placement because it avoids fetching a memory block from main memory. In addition, it has a lower penalty than write no-allocate if the written word is read before being evicted from the cache. See Jouppi <ref> [45] </ref> for more information on write-miss policies. A write buffer reduces the cost of writes to main memory. A write buffer is a queue containing writes that are to be sent to main memory. <p> Second, they have different costs for write hits and write misses. Write hits and write misses for a write-back cache may cost one cycle more than they do for a write-through cache. A write-back cache must probe the tag before writing to the cache <ref> [45] </ref>, unlike a write-through cache. The different write-buffer costs can be ignored. The write-buffer costs for a write-through cache are usually an upper bound on the costs for a comparable write-back cache because write-through caches write to main memory more often than write-back caches do. <p> Because the programs allocate 0.4-0.9 bytes per instruction, my results suggest that a cache block is read within 9,000-20,000 instructions after it is written. The benefit of subblock placement is not limited to functional languages such as Standard ML. Jouppi <ref> [45] </ref> reports that subblock placement combined with an 8K data cache and a 16 byte cache block size eliminates 31% of the cost of cache misses for C programs. Reinhold [73] finds that the memory performance of Scheme is good with subblock placement. <p> His CPI differs substantially from mine. However, Appel confirms my measurements by personal communication and later work [8]. The reason for the difference is that instructions were undercounted in his measurements. Jouppi <ref> [45] </ref> studies the effect of cache write policies on the performance of C and Fortran programs. My class of programs is different from his, but his conclusions support mine: that a write-allocate policy with subblock placement is a desirable architectural feature.
Reference: [46] <author> Pierre Jouvelot. </author> <title> Algebraic reconstruction of types and effects. </title> <booktitle> In Proceedings of the 18th Annual ACM Symposium on Principles of Programming Languages [1], </booktitle> <pages> pages 303-310. </pages>
Reference-contexts: The complexity of her analysis appears to be at least exponential. Lucassen and Gifford [59, 34] describe a type-and-effect system where expressions are explicitly annotated with types and descriptions of effects. Jouvelot and Gifford <ref> [46] </ref> show how to reconstruct types-and-effects in a language with explicit polymorphism, where quantification occurs over types and effects. Talpin and Jouvelot [88] show how to infer types- and-effects in a manner similar to Hindley-Milner type reconstruction for ML.
Reference: [47] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: To understand the memory-system performance of SML programs in general, I fix the architecture to be a prototypical RISC | the MIPS R3000 <ref> [47] </ref> | and vary the memory-system configurations to cover the design space typical of workstations of the late 1980's, such as DECStations, SPARCStations, and HP 9000 series 700. I study eight substantial programs. <p> The initial heap size is 1MB. I do not investigate the interaction of the sizing strategy and cache size. Understanding these tradeoffs is beyond the scope of this thesis. 7. All the traces are for the DECStation 5000/200, which uses a MIPS R3000 CPU <ref> [47] </ref>. 8. All instructions take one cycle with a perfect memory system. This affects only write buffer costs because multi-cycle instructions give the write buffer more time to retire writes. The effect of this assumption is negligible; Section 2.3.4 shows that write-buffer costs are already small. 9.
Reference: [48] <author> Richard A. Kelsey. </author> <title> Compilation By Program Transformation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: His description is complicated by the fact that he uses a subset of SCHEME for his intermediate language. SCHEME differs from the call-by-value -calculus in that it allows variables to be assigned. Kranz et al. <ref> [48, 52, 51] </ref> also use source-to-source transformations in the ORBIT compiler for SCHEME. Their set of optimizations is similar to Steele's set, with one notable difference: they propagate values, such as small function definitions, across compilation units.
Reference: [49] <author> R. E. Kessler and Mark D. Hill. </author> <title> Page placement algorithms for large real-indexed caches. </title> <journal> Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: First, the simulations ignore the effects of context switches and system calls. Second, the simulations assume a virtual address=physical address mapping that can have many fewer conflict misses than the random mapping used in Mach 2.6 <ref> [49] </ref>. Third, the simulations assume that all instructions take exactly one cycle (plus memory-system overhead).
Reference: [50] <author> Philip J. Koopman, Jr., Peter Lee, and Daniel P. Siewiorek. </author> <title> Cache behavior of combinator graph reduction. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 14(2):265277, </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. Many researchers have claimed that heap allocation leads to poor memory-system performance <ref> [50, 68, 96, 97, 99] </ref>. Together, these results indicate that memory-system performance of SML programs may be problem. The second conjecture is that SML programs spend a lot of time doing automatic storage management. <p> Second, on a write miss, the memory system must avoid reading a cache block from memory if it will be written before being read. Of course, this requirement only holds for caches with a write-allocate policy. Subblock placement <ref> [50] </ref>, a block size of 1 word, and the ALLOCATE instruction [68] can all achieve this. Because the effects on cache performance of these features are similar, I discuss only subblock placement. <p> For the write allocate organizations, doubling the block size can halve the write-miss rate. Thus, larger block sizes improve performance when there is a penalty for a write miss <ref> [50] </ref>. In contrast, for caches with write allocate/subblock placement, where there is no write miss penalty, increasing the block size improves performance only a little. <p> Wilson et al. [96, 97] argue that cache performance of programs with generational garbage collection will improve substantially when the youngest generation fits in the cache. Koopman et al. <ref> [50] </ref> study the effect of cache organization on combinator graph reduction, an implementation technique for lazy functional programming languages. They observe the importance of a write-allocate policy with subblock placement for improving heap allocation.
Reference: [51] <author> David Kranz. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: I describe the benchmark programs and details of the measurements in Chapter 8. The 100% mark on the graph represents the execution time of programs compiled by SML/NJ. The bars show the relative execution time of programs compiled by TIL. 1.4 Related work Kranz et al. <ref> [52, 51] </ref> show that Scheme versions of Pascal programs can be compiled to be as efficient as the original Pascal programs. <p> His description is complicated by the fact that he uses a subset of SCHEME for his intermediate language. SCHEME differs from the call-by-value -calculus in that it allows variables to be assigned. Kranz et al. <ref> [48, 52, 51] </ref> also use source-to-source transformations in the ORBIT compiler for SCHEME. Their set of optimizations is similar to Steele's set, with one notable difference: they propagate values, such as small function definitions, across compilation units.
Reference: [52] <author> David Kranz, Richard Kelsey, Jonathan Rees, Paul Hudak, James Philbin, and Norman Adams. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <booktitle> In Proceedings of the SIGPLAN '86 Conference Symposium on Compiler Construction, </booktitle> <pages> pages 219-233, </pages> <address> Palo Alto, California, </address> <month> June </month> <year> 1986. </year> <note> ACM. </note>
Reference-contexts: In the TIL compiler, I take an approach to compiling languages with higher-order functions such as SML that is quite different from the approach suggested in the literature. Many researchers who have implemented languages with higher-order functions such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions. <p> I describe the benchmark programs and details of the measurements in Chapter 8. The 100% mark on the graph represents the execution time of programs compiled by SML/NJ. The bars show the relative execution time of programs compiled by TIL. 1.4 Related work Kranz et al. <ref> [52, 51] </ref> show that Scheme versions of Pascal programs can be compiled to be as efficient as the original Pascal programs. <p> In principle, the presence of higher-order functions means that procedure activation records must be allocated on the heap. With a suitable analysis, a stack can be used to store most activation records <ref> [52] </ref>. However, using only a heap simplifies the compiler, the run-time system [6], and the implementation of first- class continuations [40]. <p> Because the read and write miss penalties are the same, C is the same for write-allocate and write-no-allocate organizations. This is an upper bound on the expected memory-system cost of heap allocation with a stack because it may be possible to stack-allocate additional objects <ref> [52] </ref>. We see that even 41 Program Allocation rate Allocation rate including callee-save conts. excluding callee-save conts. <p> These records are paired with the code to form an abstract closure. TIL uses a flat environment representation for type and value environments [7]. For known functions, TIL generates closed code but avoids creating environments or a closure. Following Kranz <ref> [52] </ref>, we modify the call sites of known functions to pass free variables as additional arguments. TIL closes over only variables which are function arguments or are bound within functions. <p> His description is complicated by the fact that he uses a subset of SCHEME for his intermediate language. SCHEME differs from the call-by-value -calculus in that it allows variables to be assigned. Kranz et al. <ref> [48, 52, 51] </ref> also use source-to-source transformations in the ORBIT compiler for SCHEME. Their set of optimizations is similar to Steele's set, with one notable difference: they propagate values, such as small function definitions, across compilation units. <p> My approach to compiling B-form programs to machine code is quite different from the approach suggested in the literature. Many researchers who have implemented higher-order languages such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions. <p> It uses a simple strategy for choosing closure representations that depends on whether the function is known or is escaping. If the function is known, closure conversion adds the free constructor variables and value variables to the arguments for the function, following Kranz's approach <ref> [52] </ref>. If the function is escaping, closure conversion builds a closure for the function, which consists of a constructor environment, a value environment, and a closed version of the function. The environments respectively bind free constructor variables and value variables; closure conversion represents them as flat records [7].
Reference: [53] <author> James R. Larus. </author> <title> Abstract Execution: A technique for efficiently tracing programs. </title> <journal> Software Practice and Experience, </journal> 20(12) 1241-1258, December 1990. 
Reference-contexts: This is joint work with Amer Diwan and Eliot Moss: Amer, Eliot, and I designed the experiment. I conducted many of the measurements and Amer modified the SML/NJ compiler [7], QPT <ref> [12, 53, 54] </ref>, and Tycho [42] so that we could instrument SML programs and simulate memory-system performance. Amer also conducted some of the measurements. <p> Section 2.2.2 states my assumptions and argues that they are reasonable. Section 2.2.3 describes and characterizes 21 the benchmark programs that I use. Section 2.2.4 describes the metrics that I use to measure memory-system performance. 2.2.1 Tools Amer Diwan extended QPT (Quick Program Profiler and Tracer) <ref> [54, 12, 53] </ref> to produce memory traces for SML/NJ programs. QPT rewrites an executable program to produce compressed trace information; QPT also produces a program-specific regeneration program that expands the compressed trace into a full trace. <p> This is joint work with Amer Diwan. Amer and I designed the experiment and did the measurements. Amer also made additional changes to the SML/NJ compiler and QPT <ref> [12, 53, 54] </ref> beyond those described in Chapter 2 so that we could make the measurements. I measure the cost of storage management for eight programs on a DECstation 5000/200 [28]. I chose the DECStation 5000/200 because its memory system is favorable to programs that heap allocate intensively.
Reference: [54] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting executable files to measure program behavior. </title> <type> Technical Report Wis 1083, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> March </month> <year> 1992. </year> <month> 262 </month>
Reference-contexts: This is joint work with Amer Diwan and Eliot Moss: Amer, Eliot, and I designed the experiment. I conducted many of the measurements and Amer modified the SML/NJ compiler [7], QPT <ref> [12, 53, 54] </ref>, and Tycho [42] so that we could instrument SML programs and simulate memory-system performance. Amer also conducted some of the measurements. <p> Section 2.2.2 states my assumptions and argues that they are reasonable. Section 2.2.3 describes and characterizes 21 the benchmark programs that I use. Section 2.2.4 describes the metrics that I use to measure memory-system performance. 2.2.1 Tools Amer Diwan extended QPT (Quick Program Profiler and Tracer) <ref> [54, 12, 53] </ref> to produce memory traces for SML/NJ programs. QPT rewrites an executable program to produce compressed trace information; QPT also produces a program-specific regeneration program that expands the compressed trace into a full trace. <p> This is joint work with Amer Diwan. Amer and I designed the experiment and did the measurements. Amer also made additional changes to the SML/NJ compiler and QPT <ref> [12, 53, 54] </ref> beyond those described in Chapter 2 so that we could make the measurements. I measure the cost of storage management for eight programs on a DECstation 5000/200 [28]. I chose the DECStation 5000/200 because its memory system is favorable to programs that heap allocate intensively.
Reference: [55] <author> Xavier Leroy. </author> <title> Polymorphic type inference and assignment. </title> <booktitle> In Proceedings of the 18th Annual ACM Symposium on Principles of Programming Languages [1], </booktitle> <pages> pages 291-302. </pages>
Reference-contexts: Jouvelot and Gifford [46] show how to reconstruct types-and-effects in a language with explicit polymorphism, where quantification occurs over types and effects. Talpin and Jouvelot [88] show how to infer types- and-effects in a manner similar to Hindley-Milner type reconstruction for ML. Leroy and Weis <ref> [55] </ref> study another extension to the ML type system. 5.4 Inlining Inlining selectively replaces function calls with copies of the bodies of the called functions. It is an important optimization for eliminating higher-order functions.
Reference: [56] <author> Xavier Leroy. </author> <title> Unboxed objects and polymorphic typing. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-188, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Appel [7] shows that optimizations, especially inlining and uncurrying, are important to compiling SML programs. However, he does not argue for the importance of applying optimizations known to improve loops in conventional language to recursive functions in SML. Leroy <ref> [56] </ref> shows that using untagged integers significantly increases performance. How- ever, in his approach integers must still be boxed when stored in arrays or recursive data structures. With the TIL approach, integers are always unboxed and untagged. His approach also slows down programs that use polymorphic functions. <p> Integers are tagged with 1 and pointers are tagged with 0 in the least significant bit. By tagging integers with 0 and pointers with 1 and using displacement addressing, which is available on many architectures, many tagging operations can be removed. Representation analysis <ref> [56] </ref> may also eliminate some tagging operations. Third, I speculate that using a hash-based scheme would improve the write barrier implementation. <p> The following studies may be worthwhile for languages like SML: * comparing different data representation techniques for polymorphic garbage-collected languages. There are at least three different techniques that should be compared: using a conventional universal representation, where data is represented as a tagged machine word, Leroy's approach <ref> [56] </ref>, where polymorphic values are placed in a universal representation but monomorphic values are kept in a native machine representation, and intensional polymorphism [37]. * comparing different inlining techniques. It would be interesting to compare SML/NJ's inlining strategy to TIL's inlining strategy.
Reference: [57] <institution> Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </institution> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference: [58] <author> H. Lieberman and C. Hewitt. </author> <title> A real-time garbage collector based on the lifetimes of objects. </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <year> 1983. </year>
Reference-contexts: The SML/NJ compiler uses heap-only allocation: all allocation is done on the heap. In particular, all activation records are allocated on the heap instead of a call stack. The heap is managed automatically using generational copying garbage collection <ref> [5, 6, 58] </ref>. In copying garbage collection [20, 32], an area of memory is reclaimed by copying the live (non-garbage) data to another area of memory.
Reference: [59] <author> John M. Lucassen. </author> <title> Types and Effects: Towards the Integration of Functional and Imperative Programming. </title> <type> PhD thesis, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: The complexity of her analysis appears to be at least exponential. Lucassen and Gifford <ref> [59, 34] </ref> describe a type-and-effect system where expressions are explicitly annotated with types and descriptions of effects. Jouvelot and Gifford [46] show how to reconstruct types-and-effects in a language with explicit polymorphism, where quantification occurs over types and effects.
Reference: [60] <author> I. A. Mason and C. L. Talcott. </author> <title> Program transformation via contextual assertions. </title> <editor> In N. D. Jones, M. Hagiya, and M. Sato, editors, </editor> <booktitle> Logic, Language, and Computation: Festschrift in Honor of Satoru Takasu, number 792 in Lecture Notes in Computer Science, </booktitle> <pages> pages 225-254. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The fi rule, however, does not support this form of contextual reasoning. 120 To demonstrate the correctness of CSE of expressions that may raise exceptions or not terminate, we can use contextual assertions <ref> [43, 60] </ref> or conditional lambda-theories [93]. These variants of the call-by-value -calculus justify the correctness of the transformations that are done by Algorithm 6. <p> We can show the correctness of the transformations done by Algorithm 7, like the correctness of the transformations done by Algorithm 6, using variants of the call-by-value -calculus. Specifically, we need to use contextual assertions <ref> [43, 60] </ref> or conditional lambda- theories [93] to show the correctness of the transformations done by Algorithm 7. Algorithm 7 has an O (N log N ) asymptotic complexity, like Algorithm 6. The algorithm traverses each node in the program syntax tree.
Reference: [61] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: becoming more important than ever before, programmers have traditionally faced a dilemma: programs written in these languages traditionally have had lower performance than programs written in more conventional, but error-prone languages. 1 In this thesis, I study this problem in the context of one particular modern programming language, Standard ML <ref> [61] </ref> (SML). SML contains all the language features mentioned previously and more. <p> Thus, although allocation is cheap in terms of instruction counts, it may be expensive in terms of machine cycle counts. 2.1.5 Standard ML Standard ML (SML) <ref> [61] </ref> is a call-by-value, lexically scoped language with higher-order functions. SML encourages a non-imperative programming style. Variables cannot be altered once they are bound and by default data structures cannot be altered once they are created. <p> It then compiles a subset of this abstract syntax to an explicitly-typed language called Lambda. The compilation to Lambda converts pattern matching to decision trees and expands various derived forms according to <ref> [61] </ref>. 74 75 4.2.2 Translation to LMLI The next phase of TIL translates Lambda programs to LMLI programs and uses the intensionally polymorphic language constructs of LMLI to provide specialized arrays, multi- argument functions, efficient data representations for user-defined datatypes, and tag-free polymorphic equality.
Reference: [62] <author> Y. Minamide, G. Morrisett, and R. Harper. </author> <title> Typed closure conversion. </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Finally, it allows nested let expressions only within switches (branch expressions). Hence, the translation from Lmli to B-form linearizes and names nested computations and values. 4.2.4 Closure conversion TIL uses a type-directed, abstract closure conversion in the style suggested by Minamide, Morrisett, and Harper <ref> [62] </ref> to convert B-form programs to Lmli-Closure programs. Lmli- Closure is an extension of B-form that provides constructs for explicitly constructing closures and their environments. For each escaping B-form function, TIL generates a closed piece of code, a type environment, and a value environment.
Reference: [63] <author> J. Gregory Morrisett. </author> <title> Compiling with Types. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> December </month> <year> 1995. </year> <note> Published as Technical Report CMU-CS95-226. </note>
Reference-contexts: The TIL compiler is organized around the idea of typed intermediate languages: we propagate type information through all phases of the compiler to machine code generation, and if necessary pass type information around at run time. The TIL framework, which is the subject of Morrisett's PhD thesis <ref> [63] </ref>, allows us to implement two key parts of a "pay-as-you-go" compilation strategy: * tag-free garbage collection, * and compiling polymorphic functions to support native machine representations for data. Conventional implementation approachs for languages such as SML have used a universal representation for data. <p> It also solves the problem of how to allow the garbage collector to find data that is live: the garbage collector can use type information to determine how to traverse data structures. I have organized this chapter in the following manner. First, I introduce LMLI <ref> [63] </ref>, the primary intermediate language of TIL. Next, I give an overview of the phases of TIL and the garbage collection technique used by TIL. <p> TIL uses an intensionally polymorphic language called LMLI, which was designed by Morrisett and Harper <ref> [63] </ref>. LMLI is the actual intermediate language used by the TIL compiler, and it is the language that I use in the next two chapters on optimization. Morrisett and Harper based their design on their intermediate language ML i [37], which they used to explore the theory of intensional polymorphism. <p> In this section, I describe the syntax of ML i and informally describe its semantics. I then present the syntax of LMLI and informally describe LMLI's semantics. Finally, I give an example of an LMLI program. 4.1.1 Overview of ML i My description of ML i follows Morrisett's presentation <ref> [63] </ref>. The syntax for ML i is presented in Figure 4.1. There are four syntactic classes: kinds, constructors, types, and terms. Kinds describe constructors, while types describe terms. Constructors can be thought of as the representation of types at run time. <p> The expression extern (s,) denotes an external identifier of type . The expressions make vararg and call vararg are used to implement functions with variable numbers of arguments <ref> [63] </ref>. The expressions eq and neq implement structural polymorphic equality and inequality. They each take a constructor as an argument, and return an equality and inequality function, respectively. These operators can actually be coded in LMLI, but they are easier to optimize if they are kept as primitive operators. <p> Straight-line floating point code still runs fast: the optimizer later eliminates unecessary box/unbox operations during the constant-folding phase. In all, the combination of type-directed optimizations reduces running times by roughly 40% and allocation by 50% <ref> [63, Chapter 8] </ref>. However, much of this improvement can be realized by other techniques; For example, SML/NJ uses Leroy's unboxing technique to achieve comparable improvements for calling conventions [81]. <p> However, unlike Tolmach, we evaluate substitutions of ground types for type variables eagerly instead of lazily. This is due in part for technical reasons (see <ref> [63, Chapter 7] </ref>), and in part to avoid a class of space leaks that might result with lazy substitution. 4.3 An example This section shows an SML function as it passes through the various stages of TIL.
Reference: [64] <author> Anne Neirynck. </author> <title> Analysis of Side Effects in Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Cornell University, Cornell, </institution> <address> New York, </address> <year> 1988. </year>
Reference-contexts: In contrast, I have ignored the problem of higher-order functions by assuming that a term-level function application may have any effect, but my analysis is asymptotically efficient. As I demonstrate in Chapter 10, it is quite reasonable to ignore higher-order functions in practice. Neirynck and others <ref> [64, 65] </ref> present an analysis based on abstract interpretation for determining whether an expression uses or updates the store in a simply-typed -calculus.
Reference: [65] <author> Anne Neirynck, Prakash Panangaden, and Alan Demers. </author> <title> Effect analysis of higher-order languages. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(119), </volume> <pages> 119. </pages>
Reference-contexts: In contrast, I have ignored the problem of higher-order functions by assuming that a term-level function application may have any effect, but my analysis is asymptotically efficient. As I demonstrate in Chapter 10, it is quite reasonable to ignore higher-order functions in practice. Neirynck and others <ref> [64, 65] </ref> present an analysis based on abstract interpretation for determining whether an expression uses or updates the store in a simply-typed -calculus.
Reference: [66] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Main memory is divided into DRAM pages. Page-mode writes reduce the latency of writes to the same DRAM page when there are no intervening memory accesses to another DRAM page <ref> [66] </ref>. For example, on a DECStation 5000/200, a non-page-mode write takes 5 cycles and a page-mode write takes 1 cycle.
Reference: [67] <author> Laurence C. Paulson. </author> <title> ML for the Working Programmer. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1991. </year>
Reference-contexts: In SML, however, variables are not updateable. Variables may only be bound. In this aspect, SML differs from Scheme and Lisp, where variables are updateable just like C and FORTRAN. If you are unfamiliar with the distinction between binding versus assignment of variables, you should consult references on SML <ref> [67, 91] </ref> for a further explanation of this point. This means that tracking side-effects to the store is less important than it is for more conventional languages, because side-effects occur less frequently.
Reference: [68] <author> Chih-Jui Peng and Gurindar S. Sohi. </author> <title> Cache memory design considerations to support languages with dynamic heap allocation. </title> <type> Technical Report 860, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1989. </year> <month> 263 </month>
Reference-contexts: Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. Many researchers have claimed that heap allocation leads to poor memory-system performance <ref> [50, 68, 96, 97, 99] </ref>. Together, these results indicate that memory-system performance of SML programs may be problem. The second conjecture is that SML programs spend a lot of time doing automatic storage management. <p> Second, on a write miss, the memory system must avoid reading a cache block from memory if it will be written before being read. Of course, this requirement only holds for caches with a write-allocate policy. Subblock placement [50], a block size of 1 word, and the ALLOCATE instruction <ref> [68] </ref> can all achieve this. Because the effects on cache performance of these features are similar, I discuss only subblock placement. <p> Peng and Sohi <ref> [68] </ref> examine the data-cache behavior of small Lisp programs. They use trace-driven simulation, and propose an ALLOCATE instruction for improving cache behavior that allocates a block in the cache without fetching it from memory. <p> In contrast, I measure an actual implementation. He measures the memory-system cost using the cache-miss ratio, which is an inaccurate indicator of performance because it does not separate the cost of read and write misses. Wilson et al.[96] and Peng and Sohi <ref> [68] </ref> also measure the memory-system cost of garbage collection using the cache-miss ratio. They do not measure the instruction-level cost of garbage collection or costs incurred during mutation.
Reference: [69] <author> Simon Peyton-Jones. </author> <title> Compilation by transformation: a report from the trenches. </title> <booktitle> In European Symposium on Programming (ESOP '96), Lecture Notes in Computer Science 1058. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: He shows to use this information to do induction-variable elimination (IVE) and to potentially improve inlining. The optimizations that I describe in this chapter extend Shivers work. I have focused on the simpler problem of applying optimizations to programs that use first- order, recursive functions. Peyton-Jones <ref> [69] </ref> and Santos [79] describe an optimizer for the Glasgow Haskell compiler that is similar in spirit to my optimizer. Like my optimizer, their optimizer uses many simple transformations that are iterated repeatedly.
Reference: [70] <author> Gordon D. Plotkin. </author> <title> Call-by-name, call-by-value, and the -calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1 </volume> <pages> 125-159, </pages> <year> 1975. </year>
Reference-contexts: It uses a call-by-value parameter passing strategy, instead of a call-by-name parameter passing strategy. For SML, we can only use the weaker fi v rule <ref> [70] </ref>: (x:M )V = M [V =x] where V is a value. A value is a constant, such as 0, a variable, or a -expression. This rule does not allow us to prove correctness for any interesting cases of CSE, where computation is being done.
Reference: [71] <author> Steven A. Przybylski. </author> <title> Cache and Memory Hierarchy Design: A Performance-Directed Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: In this thesis, subblock placement implies a subblock of one word, i.e., each word has a valid bit. Moreover, on a read miss, the whole block is brought into the cache, not just the missing word. Przybylski <ref> [71] </ref> notes that this is a good choice. A memory access to a location that is resident in the cache is a hit. Otherwise, the memory access is a miss. A miss is a compulsory miss if it is due to a memory block being accessed for the first time.
Reference: [72] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Knuth-Bendix An implementation of the Knuth-Bendix completion algorithm, imple <br>- mented by Gerard Huet, processing some axioms of geometry. Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi [11], processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade <ref> [72] </ref>, running 50 generations of a glider gun. It is implemented using lists. PIA The Perspective Inversion Algorithm [94] decides the location of an object in a perspective video image. <p> FFT 246 Fast fourier transform, multiplying polynomials up to degree 65,536 Knuth-Bendix 618 An implementation of the Knuth-Bendix completion algorithm. Lexgen 1123 A lexical-analyzer generator [11], processing the lexical descrip <p>- tion of Standard ML. Life 146 The game of Life implemented using lists <ref> [72] </ref>. Matmult 62 Integer matrix multiply, on 200x200 integer arrays. PIA 2065 The Perspective Inversion Algorithm [94] deciding the location of an object in a perspective video image. Simple 870 A spherical fluid-dynamics program [31], run for 4 iterations with grid size of 100.
Reference: [73] <author> Mark B. Reinhold. </author> <title> Cache Performance of Garbage-Collected Programming Languages. </title> <type> PhD thesis, </type> <institution> Laboratory for Computer Science, MIT, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: The benefit of subblock placement is not limited to functional languages such as Standard ML. Jouppi [45] reports that subblock placement combined with an 8K data cache and a 16 byte cache block size eliminates 31% of the cost of cache misses for C programs. Reinhold <ref> [73] </ref> finds that the memory performance of Scheme is good with subblock placement. Changing Associativity From Figure 2.2 we see that increasing associativity improves all organizations. The improvement in going from one-way to two-way set associativity is much smaller than the improvement obtained from subblock placement. <p> Wilson et al.[96] and Peng and Sohi [68] also measure the memory-system cost of garbage collection using the cache-miss ratio. They do not measure the instruction-level cost of garbage collection or costs incurred during mutation. Reinhold <ref> [73] </ref> measures the cost of garbage collection for a Scheme system, including the change in memory-system performance of entire programs, but does not measure costs incurred during mutation. Steenkiste [86] studies ways to reduce the cost of tagging in Lisp. He also studies instructions used for stack allocation.
Reference: [74] <author> John H. Reppy. </author> <title> Asynchronous Signals in Standard ML. </title> <type> Technical Report 90-1144, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: The SML/NJ compiler also places checks on many extended basic blocks that do no allocation because these checks are also used to implement asynchronous signals <ref> [74] </ref>. The SML/NJ compiler generates in-line machine code to allocate everything but arrays and strings. The in-line machine code is only two instructions long. Recall from Section 2.1.4 that only two instructions are needed because the garbage collector always reclaims the entire allocation area. <p> This is despite the fact that the check and allocating an object both take two instructions on the MIPS, and that a check is sometimes for multiple allocations. I speculate that this is because the SML/NJ compiler overloads checks to implement asynchronous signals <ref> [74] </ref>.
Reference: [75] <author> Stephen Richardson and Mahadevan Ganapathi. </author> <title> Interprocedural optimization: exper-imental results. </title> <journal> Software | Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 149-168, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: O (N ) time, where N is the size of the program if the lattice of Section 5.3.1 is used, functions are traversed in depth-first search using the call graph, and mutually-recursive functions are coalesced as single nodes in the call graph, A cautionary note is that Richardson and Ganapathi <ref> [75] </ref> found that for Pascal programs interprocedural summary information did not improve optimization. However, their optimizer did not move function applications. 222 11.2.3 Additional optimizations It would be interesting to implement a much broader range of optimizations that improve loops in the context of SML.
Reference: [76] <author> Barbara G. Ryder. </author> <title> Constructing the call graph of a program. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(3) </volume> <pages> 216-226, </pages> <month> May </month> <year> 1979. </year>
Reference-contexts: In informal measurements, which I do not present in this thesis, I have found that profiles constructed from recursive-function nesting depths are very good predictors of where programs actually spend their time. To compute recursive-function nesting depth, we analyze the call graph <ref> [76] </ref> of a program. A call graph tells for each function in a program what functions it may call. A call graph is a directed graph G whose nodes are functions. <p> The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [77] <author> Amr Sabry and Matthias Felleisen. </author> <title> Reasoning about programs in continuations-passing style. </title> <booktitle> In LFP '92 [57], </booktitle> <pages> pages 288-298. </pages>
Reference-contexts: Most compilers use a DS intermediate language. From a theoretical point of view, the two languages are essentially equivalent: programs can be converted from one representation to the other in asymptotically-efficient time <ref> [25, 77, 78] </ref>. The choice of which representation to use is a matter of engineering. I chose to use a DS intermediate language. This has two advantages. First, a DS language is more amenable to the kinds of interprocedural program analyses that I want to do than a CPS language.
Reference: [78] <author> Amr Sabry and Matthias Felleisen. </author> <title> Reasoning about programs in continuation-passing style. </title> <journal> Lisp and Symbolic Computation, </journal> 6(3/4):289-360, November 1993. 
Reference-contexts: Most compilers use a DS intermediate language. From a theoretical point of view, the two languages are essentially equivalent: programs can be converted from one representation to the other in asymptotically-efficient time <ref> [25, 77, 78] </ref>. The choice of which representation to use is a matter of engineering. I chose to use a DS intermediate language. This has two advantages. First, a DS language is more amenable to the kinds of interprocedural program analyses that I want to do than a CPS language. <p> It is 88 unclear how to make this distinction when analyzing CPS programs, because functions never return. Second, a DS language makes it possible to re-use existing compilation techniques, such as interprocedural register allocation. On the other hand, using a DS language has a disadvantage. Sabry and Felleisen <ref> [78] </ref> showed that the fi v rule (inlining) is less powerful for DS programs than for CPS programs. They showed that you need several additional rules (that is, optimizations) for DS programs. These optimizations, however, are not onerous to implement.
Reference: [79] <author> Andre Santos. </author> <title> Compilation by transformation in non-strict functional languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, Glasgow University, </institution> <address> Glasgow, Scotland, </address> <year> 1995. </year>
Reference-contexts: He shows to use this information to do induction-variable elimination (IVE) and to potentially improve inlining. The optimizations that I describe in this chapter extend Shivers work. I have focused on the simpler problem of applying optimizations to programs that use first- order, recursive functions. Peyton-Jones [69] and Santos <ref> [79] </ref> describe an optimizer for the Glasgow Haskell compiler that is similar in spirit to my optimizer. Like my optimizer, their optimizer uses many simple transformations that are iterated repeatedly.
Reference: [80] <author> Robert Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: In other words, for components with two or more nodes, there is a a cyclic path (loop) connecting all nodes in the component. Algorithms for identifying maximal strongly-connected components are well-known <ref> [80] </ref>. It takes O (M ) time to find strongly-connected components, where M is the number of vertices and edges in the graph. The following definition uses strongly-connected components to identify recursive functions.
Reference: [81] <author> Zhong Shao. </author> <title> Compiling Standard ML for Efficient Execution on Modern Machines. </title> <type> PhD thesis, </type> <institution> Princeton University, Princeton, </institution> <address> New Jersey, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In the TIL compiler, I take an approach to compiling languages with higher-order functions such as SML that is quite different from the approach suggested in the literature. Many researchers who have implemented languages with higher-order functions such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions. <p> In all, the combination of type-directed optimizations reduces running times by roughly 40% and allocation by 50% [63, Chapter 8]. However, much of this improvement can be realized by other techniques; For example, SML/NJ uses Leroy's unboxing technique to achieve comparable improvements for calling conventions <ref> [81] </ref>. <p> My approach to compiling B-form programs to machine code is quite different from the approach suggested in the literature. Many researchers who have implemented higher-order languages such as SML have stated the importance of compiling functions well <ref> [52, 7, 81] </ref> | they have focused on strategies for representing environments for first-class functions. I believe that this is not the central problem for compiling languages such as SML, because optimization is so effective at eliminating higher-order functions.
Reference: [82] <author> Olin Shivers. </author> <title> Control Flow Analysis in Scheme. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: $9, $2, $9 ldl $9, ($9) mullv $8, $9, $8 addlv $7, $8, $7 addlv $6, 1, $6 trapb br $31, L1 mov $1, $0 ldq $8, 8 ($sp) lda $sp, 32 ($sp) .end Lv2851 dot 205955 10 25% 75% 125% cksum FFT KB lexgen life Mmult PIA SIMPLE Shivers <ref> [82, 83] </ref> argues for the importance of loop optimizations when compiling languages with higher-order functions, but proposes an approach based on control-flow analysis of higher-order functions. I show that control-flow analysis of higher-order functions is not needed to apply these optimizations to -calculus based programs. <p> The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [83] <author> Olin Shivers. </author> <title> Control-Flow Analysis of Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: $9, $2, $9 ldl $9, ($9) mullv $8, $9, $8 addlv $7, $8, $7 addlv $6, 1, $6 trapb br $31, L1 mov $1, $0 ldq $8, 8 ($sp) lda $sp, 32 ($sp) .end Lv2851 dot 205955 10 25% 75% 125% cksum FFT KB lexgen life Mmult PIA SIMPLE Shivers <ref> [82, 83] </ref> argues for the importance of loop optimizations when compiling languages with higher-order functions, but proposes an approach based on control-flow analysis of higher-order functions. I show that control-flow analysis of higher-order functions is not needed to apply these optimizations to -calculus based programs. <p> It includes all the optimizations that Appel found to be important for SML [7]. However, it uses improved algorithms for inlining and uncurrying. I improved these algorithms because it is particularly important to eliminate higher-order functions before doing loop optimizations <ref> [83] </ref>. I have organized this chapter as follows. First, I discuss the intermediate language used by the optimizer, B-form. B-form is a subset of LMLI, the intermediate language that I described in Chapter 4. In contrast to the intermediate language that Appel uses, B-form is a direct-style, typed intermediate language. <p> Passing each argument except the last one results in a function call, construction of a closure, and a function return. Also, currying introduces first-class functions, which means that the control-flow graph is no longer statically apparent <ref> [83] </ref>. For these two reasons, an important optimization for SML programs is uncurrying: taking a curried function and transforming it to a multi-argument function where all arguments are passed at once. <p> The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice. <p> If the variable is written to memory, it is written to the stack only once because TIL uses Chaitin's graph coloring register allocation, which allocates a variable to memory or a register for its entire live range. Shivers <ref> [83] </ref> advocates the importance of adapting optimizations used in compilers for C and FORTRAN so that the optimizations can be applied to languages like SML. He shows how to approximate the control-flow graph for programs that use higher-order functions.
Reference: [84] <author> Michael Slater. </author> <title> PA workstations set price/performance records. </title> <type> Microprocessor Report, </type> <institution> 5(6):1, </institution> <month> April </month> <year> 1991. </year> <month> 264 </month>
Reference-contexts: There are many variables involved and the dependencies between the variables are complex. Therefore I can study only a subset of the memory-system design space. I restrict this study to features found in RISC workstations of the late 1980's <ref> [29, 28, 84, 24] </ref>. Table 2.6 summarizes the cache organizations that I simulated. Table 2.7 lists the memory-system organizations of some popular machines. I simulate only separate instruction and data caches (i.e., no unified caches).
Reference: [85] <author> Guy L. Steele Jr. RABBIT: </author> <title> A Compiler for Scheme (A Study in Compiler Optimiza-tion). </title> <type> Master's thesis, </type> <institution> AI Laboratory Technical Report AI-TR-474, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: Third, the additional optimizations done during iteration may improve performance only a small amount. 6.7 Related work In this section, I describe related work on optimization of functional languages. That is, this section also covers some related work for the previous chapter. Steele <ref> [85] </ref> proposes the use of source-to-source transformations in compiling SCHEME, using a -calculus based intermediate language. His optimizer does some simple forms of inlining, eliminates unused arguments to functions, does short-circuiting of if-statements, and does constant-folding of primitive operations.
Reference: [86] <author> Peter Steenkiste. </author> <title> LISP on a Reduced-Instruction-Set Processor: Characterization and Optimization. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <address> Stanford,CA 94305, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: They do not measure the instruction-level cost of garbage collection or costs incurred during mutation. Reinhold [73] measures the cost of garbage collection for a Scheme system, including the change in memory-system performance of entire programs, but does not measure costs incurred during mutation. Steenkiste <ref> [86] </ref> studies ways to reduce the cost of tagging in Lisp. He also studies instructions used for stack allocation. He is primarily concerned with hardware support to improve tag checking required for dynamic typing. He finds that tag insertion and removal 59 costs about 4.5% with the best software scheme.
Reference: [87] <author> Darko Stefanovic and Eliot Moss. </author> <title> Characterisation of object behavior in Standard ML of New Jersey. </title> <booktitle> In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, </booktitle> <year> 1994. </year>
Reference-contexts: Chapter 2 shows that this memory system is favorable to allocation-intensive programs. 3.2.3 Benchmark Programs I use the same benchmarks that I use to study memory-system performance. I describe the benchmarks in Section 2.2.3. Stefanovic and Moss <ref> [87] </ref> find that the allocation of callee-save continuation closures on the heap has a profound impact on the young-object dynamics of ML programs. In the programs they measure 2 , most objects are short-lived. They attribute the high mortality rate to the allocation of callee-save continuation closures on the heap.
Reference: [88] <author> Jean-Pierre Talpin and Pierre Jouvelot. </author> <title> Polymorphic type, region,and effect inference. </title> <journal> Journal of Functional Programming, </journal> <volume> 2(3) </volume> <pages> 245-272, </pages> <year> 1992. </year>
Reference-contexts: Lucassen and Gifford [59, 34] describe a type-and-effect system where expressions are explicitly annotated with types and descriptions of effects. Jouvelot and Gifford [46] show how to reconstruct types-and-effects in a language with explicit polymorphism, where quantification occurs over types and effects. Talpin and Jouvelot <ref> [88] </ref> show how to infer types- and-effects in a manner similar to Hindley-Milner type reconstruction for ML. Leroy and Weis [55] study another extension to the ML type system. 5.4 Inlining Inlining selectively replaces function calls with copies of the bodies of the called functions.
Reference: [89] <author> David Tarditi and Andrew W. Appel. ML-YACC, </author> <title> version 2.0. Distributed with Standard ML of New Jersey, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark [23], translated into ID [31], and then translated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan <br>- skin. YACC A LALR (1) parser generator, implemented by David R. Tarditi <ref> [89] </ref>, pro <br>- cessing the grammar of Standard ML.
Reference: [90] <author> Andrew Tolmach. </author> <title> Tag-free garbage collection using explicit type parameters. </title> <booktitle> In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 1-11, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: TIL records additional liveness information for each variable so that the garbage collector can avoid tracing pointers that are no longer needed. This approach is well-understood for monomorphic languages requiring garbage collection [17]. Following Tolmach <ref> [90] </ref>, we extended it to a polymorphic language as follows: when a variable whose type is unknown is saved in a stack frame, the type of the variable is also saved in the stack frame. <p> It would be interesting to compare SML/NJ's inlining strategy to TIL's inlining strategy. For example, TIL does not inline recursive functions, while SML/NJ does. * comparing different techniques for automatic storage management. It would be interesting to compare fully tag-free automatic storage management <ref> [90] </ref> to the mostly tag-free approach used in TIL. 221 11.2 Optimization Several directions worth pursuing in the area of optimization are proving the correctness of the optimizations, studying how to improve the current optimizations, and implementing a broader range of loop optimizations. 11.2.1 Correctness of programs produced by the optimizer
Reference: [91] <author> Jeffrey D. Ullman. </author> <title> Elements of ML Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1994. </year>
Reference-contexts: In SML, however, variables are not updateable. Variables may only be bound. In this aspect, SML differs from Scheme and Lisp, where variables are updateable just like C and FORTRAN. If you are unfamiliar with the distinction between binding versus assignment of variables, you should consult references on SML <ref> [67, 91] </ref> for a further explanation of this point. This means that tracking side-effects to the store is less important than it is for more conventional languages, because side-effects occur less frequently.
Reference: [92] <author> David Ungar. </author> <title> The design and evaluation of a high performance Smalltalk system. </title> <publisher> ACM Distinguished Dissertation. MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Ungar <ref> [92] </ref> measures the time spent garbage collecting and the cost of integer tagging in a Smalltalk system, but does not measure other costs incurred during mutation. Zorn [100] compares the cost of two simulated garbage-collection algorithms. In contrast, I measure an actual implementation.
Reference: [93] <author> Mitchell Wand and Zheng-Yu Wang. </author> <title> Conditional lambda-theories and the verification of static properties of programs. </title> <journal> Information and Computation, </journal> <volume> 113(2) </volume> <pages> 253-277, </pages> <year> 1994. </year>
Reference-contexts: The fi rule, however, does not support this form of contextual reasoning. 120 To demonstrate the correctness of CSE of expressions that may raise exceptions or not terminate, we can use contextual assertions [43, 60] or conditional lambda-theories <ref> [93] </ref>. These variants of the call-by-value -calculus justify the correctness of the transformations that are done by Algorithm 6. <p> We can show the correctness of the transformations done by Algorithm 7, like the correctness of the transformations done by Algorithm 6, using variants of the call-by-value -calculus. Specifically, we need to use contextual assertions [43, 60] or conditional lambda- theories <ref> [93] </ref> to show the correctness of the transformations done by Algorithm 7. Algorithm 7 has an O (N log N ) asymptotic complexity, like Algorithm 6. The algorithm traverses each node in the program syntax tree.
Reference: [94] <author> Kevin G. Waugh, Patrick McAndrew, and Greg Michaelson. </author> <title> Parallel implementations from function prototypes: a case study. </title> <institution> Technical Report Computer Science 90/4, Heriot-Watt University, Edinburgh, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Lexgen A lexical-analyzer generator, implemented by James S. Mattson and David R. Tarditi [11], processing the lexical description of Standard ML. Life The game of Life, written by Chris Reade [72], running 50 generations of a glider gun. It is implemented using lists. PIA The Perspective Inversion Algorithm <ref> [94] </ref> decides the location of an object in a perspective video image. Simple A spherical fluid-dynamics program, developed as a "realistic" FORTRAN benchmark [23], translated into ID [31], and then translated into Standard ML by Lal George. VLIW A Very-Long-Instruction-Word instruction scheduler written by John Dan <br>- skin. <p> Lexgen 1123 A lexical-analyzer generator [11], processing the lexical descrip <p>- tion of Standard ML. Life 146 The game of Life implemented using lists [72]. Matmult 62 Integer matrix multiply, on 200x200 integer arrays. PIA 2065 The Perspective Inversion Algorithm <ref> [94] </ref> deciding the location of an object in a perspective video image. Simple 870 A spherical fluid-dynamics program [31], run for 4 iterations with grid size of 100. Table 8.1: Benchmark Programs I compiled whole programs during benchmarking (the programs were single closed modules).
Reference: [95] <author> William E. Weihl. </author> <title> Interprocedural data flow analysis in the presence of pointers, procedure variables, and label variables. </title> <booktitle> In Conference Record of the Ninth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 83-94, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: The functions called by map are not syntactically apparent and the set of possible functions that may be bound to f must be approximated. This problem has been studied extensively and numerous analyses have been proposed for constructing approximations of call graphs and control-flow graphs <ref> [36, 39, 44, 76, 82, 83, 95] </ref>. It is not clear, however, that these analyses are useful or needed. First, none of the previous works demonstrated that their analyses are useful in an actual optimizing compiler, or that the approximations they chose improve existing optimizations in practice.
Reference: [96] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection: a case for large and set-associative caches. </title> <type> Technical Report EECS-90-5, </type> <institution> University of Illinios at Chicago, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. Many researchers have claimed that heap allocation leads to poor memory-system performance <ref> [50, 68, 96, 97, 99] </ref>. Together, these results indicate that memory-system performance of SML programs may be problem. The second conjecture is that SML programs spend a lot of time doing automatic storage management. <p> Peng and Sohi [68] examine the data-cache behavior of small Lisp programs. They use trace-driven simulation, and propose an ALLOCATE instruction for improving cache behavior that allocates a block in the cache without fetching it from memory. Wilson et al. <ref> [96, 97] </ref> argue that cache performance of programs with generational garbage collection will improve substantially when the youngest generation fits in the cache. Koopman et al. [50] study the effect of cache organization on combinator graph reduction, an implementation technique for lazy functional programming languages.
Reference: [97] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection. </title> <booktitle> In 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year> <month> 265 </month>
Reference-contexts: Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. Many researchers have claimed that heap allocation leads to poor memory-system performance <ref> [50, 68, 96, 97, 99] </ref>. Together, these results indicate that memory-system performance of SML programs may be problem. The second conjecture is that SML programs spend a lot of time doing automatic storage management. <p> Peng and Sohi [68] examine the data-cache behavior of small Lisp programs. They use trace-driven simulation, and propose an ALLOCATE instruction for improving cache behavior that allocates a block in the cache without fetching it from memory. Wilson et al. <ref> [96, 97] </ref> argue that cache performance of programs with generational garbage collection will improve substantially when the youngest generation fits in the cache. Koopman et al. [50] study the effect of cache organization on combinator graph reduction, an implementation technique for lazy functional programming languages.
Reference: [98] <author> Andrew K. Wright. </author> <title> simple imperative polymorphism. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 8(4) </volume> <pages> 343-356, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Note that all functions must be named in B-form. I assume that all B-form programs satisfy the "value" restriction: the bodies of all polymorphic functions must be operationally equivalent to values. Wright <ref> [98] </ref> originally proposed the value restriction for SML programs as a simple way of remedying problems that arose in typing expressions which are polymorphic and use side-effects.
Reference: [99] <author> Benjamin Zorn. </author> <title> The effect of garbage collection on cache performance. </title> <type> Technical Report CU-CS-528-91, </type> <institution> University of Colorado at Boulder, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Second, Appel also shows that SML programs make intensive use of heap allocation | allocating roughly one word every five instructions. Many researchers have claimed that heap allocation leads to poor memory-system performance <ref> [50, 68, 96, 97, 99] </ref>. Together, these results indicate that memory-system performance of SML programs may be problem. The second conjecture is that SML programs spend a lot of time doing automatic storage management. <p> Koopman et al. [50] study the effect of cache organization on combinator graph reduction, an implementation technique for lazy functional programming languages. They observe the importance of a write-allocate policy with subblock placement for improving heap allocation. Zorn <ref> [99] </ref> studies the effect of cache behavior on the performance of a Common Lisp system when stop-and-copy and mark-and-sweep garbage collection algorithms are used. He concludes that when programs are run with mark-and-sweep garbage collection they have substantially better cache locality than when run with stop-and-copy garbage collection.
Reference: [100] <author> Benjamin G. Zorn. </author> <title> Comparative Performance evaluation of garbage collection algo-rithms. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> December </month> <year> 1989. </year> <month> 266 </month>
Reference-contexts: Ungar [92] measures the time spent garbage collecting and the cost of integer tagging in a Smalltalk system, but does not measure other costs incurred during mutation. Zorn <ref> [100] </ref> compares the cost of two simulated garbage-collection algorithms. In contrast, I measure an actual implementation. He measures the memory-system cost using the cache-miss ratio, which is an inaccurate indicator of performance because it does not separate the cost of read and write misses.
References-found: 100

