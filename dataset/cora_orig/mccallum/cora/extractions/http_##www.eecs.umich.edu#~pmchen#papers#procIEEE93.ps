URL: http://www.eecs.umich.edu/~pmchen/papers/procIEEE93.ps
Refering-URL: http://www.eecs.umich.edu/~pmchen/otherPapers.html
Root-URL: http://www.eecs.umich.edu
Email: pmchen@cs.Berkeley.EDU, pattrsn@cs.Berkeley.EDU  
Title: -1- Storage PerformanceMetrics and Benchmarks  
Author: Peter M. Chen David A. Patterson 
Address: Berkeley  
Affiliation: Computer Science Division, Dept. of EECS University of California,  
Abstract: An ever-widening mismatch between storage and processor performance is causing storage performance evaluation to become increasingly more important. In this paper, we discuss the metrics and benchmarks used in storage performance evaluation. We first highlight the technology trends taking place in storage systems, such as disk and tape evolution, disk arrays, and solid state disks. We then describe, review, and run today's popular I/O benchmarks on three systems: a DECstation 5000/200 running the Sprite Operating System, a SPARCstation 1+ running SunOS, and an HP Series 700 (Model 730) running HP_UX. We also describe two new approaches to storage benchmarksLADDIS and A Self-Scaling Benchmark with Predicted Performance. Keywords. I/O, storage, benchmark, workload, self-scaling benchmark, predicted performance, disk, performance evaluation. 
Abstract-found: 1
Intro-found: 1
Reference: [Amdahl67] <author> G. M. </author> <title> Amdahl, ``Validity of the single processor approach to achieving large scale computing capabilities'', </title> <booktitle> Proceedings AFIPS 1967 Spring Joint Computer Conference 30 (April 1967), </booktitle> <pages> 483-485. </pages>
Reference-contexts: If CPU performance continues to improve at its current pace and disk performance continues to obtain more moderate improvements, eventually the performance of all applications that do any input or output (I/O) will be limited by that I/O componentfurther CPU performance improvements will be wasted <ref> [Amdahl67] </ref>. In light of this developing trend toward I/O limited applications, storage performance and storage architecture become increasingly more crucial to overall system performance. In this paper, we use the -2- terms I/O performance and storage performance interchangeably.
Reference: [Anon85] <editor> Anon and et al., </editor> <title> ``A Measure of Transaction Processing Power'', </title> <journal> Datamation, </journal> <volume> 31, </volume> <month> 7 (April </month> <year> 1985), </year> <pages> 112-118. </pages>
Reference-contexts: Throughput is measured in two ways: I/O rate, measured in accesses/second, and data rate, measured in bytes/second or megabytes/second (MB/s). I/O rate is generally used for applications where the size of each request is small, such as transaction processing <ref> [Anon85] </ref>; data rate is generally used for applications where the size of each request is large, such as scientific applications [Miller91a]. Response time is the second basic performance metric for storage systems. Response time measures how long a storage system takes to access data. <p> More users generate higher system utilization and increase throughput. On the other hand, higher utilization leads to slower response times. Because a single performance number is easier to use than a full graph, many evaluators combine throughput and response time by reporting throughput at a given response time <ref> [Anon85, Chen90b] </ref>. For example, the TPC-B benchmark reports maximum throughput with 90% of all requests completed within 2 seconds [TPCB90]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh slower faster higherlower Response Time vs. Throughput ) m e i e n p e Throughput (MB/s) 500 300 100 1.00.80.60.40.20.0 to higher throughput but slower response time. <p> This supports our contention that systems with faster and faster CPUs will become more and more I/O lim ited. 7.1.2. TPC-B TPC-B measures transaction processing performance for a simple database update [TPCB90]. The first version, TP1, first appeared in 1985 <ref> [Anon85] </ref> and quickly became the de facto standard in bench marking transaction processing systems. TPC-A 1 [TPCA89] and TPC-B [TPCB90] are more tightly specified versions of TP1 and have replaced TP1 as the standard transaction processing benchmark.
Reference: [Bodega89] <institution> National Science Foundation Workshop on Next Generation Secondary Storage Architecture, National Science Foundation, Bodega Bay, </institution> <address> CA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Thus, reliability is a metric of great importance to storage systems. Cost, of course, applies to all components in computer systems. Disk subsystems are often the most expensive component in a large computer installation <ref> [Bodega89] </ref>. Cost is usually expressed as a compo -4- site metric, such as capacity cost, or throughput cost. Various combinations of these five metrics in storage system evaluation, throughput, response time, capacity, reliability, and cost, are common. One popular combination is a response time versus throughput graph (Figure 2).
Reference: [Cassidy89] <author> C. Cassidy, </author> <title> ``DEC's ESE20 Boosts Performance'', DEC Professional, </title> <month> May </month> <year> 1989, </year> <pages> 102-110. </pages>
Reference-contexts: Solid state disk is much more expensive than magnetic disk for equal capacity but is dramatically faster. -8- Response times for solid state disks are commonly less than 3 ms <ref> [Cassidy89, Jones89] </ref>, while response times for magnetic disks are approximately 10-30 ms. On the other hand, solid state disks cost 50-100 times more than magnetic disks for the same capacity [Gibson91]. Two storage metrics have been addressedthroughput and response time.
Reference: [Chen90a] <author> P. M. Chen and D. A. Patterson, </author> <title> ``Maximizing Performance in a Striped Disk Array'', </title> <booktitle> Proceedings of the 1990 International Symposium on Computer Architecture, </booktitle> <address> Seattle WA, </address> <month> May </month> <year> 1990, </year> <pages> 322-331. </pages>
Reference-contexts: This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. <p> Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance <ref> [Chen90a, Lee91a, Lee91b] </ref>, how to spread redundant information across disks to increase reliability and minimize the effect of disk failures [Holland92, Gibson91, Muntz90], and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service <p> 32 MB Operating System SunOS 4.1 Sprite LFS HP/UX 8.07 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c a three disk RAID disk array [Patterson88] with a 16 KB striping unit <ref> [Chen90a] </ref> and is configured without redundancy. The SPECint rating is a measure of the integer speed of the processor. Ratings are relative to the speed of a VAX 11/780. The full name of the HP 730 is the HP Series 700 Model 730. -12- 7.
Reference: [Chen90b] <author> P. M. Chen, G. Gibson, R. H. Katz and D. A. Patterson, </author> <title> ``An Evaluation of Redundant Arrays of Disks Using an Amdahl 5890'', </title> <booktitle> Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: More users generate higher system utilization and increase throughput. On the other hand, higher utilization leads to slower response times. Because a single performance number is easier to use than a full graph, many evaluators combine throughput and response time by reporting throughput at a given response time <ref> [Anon85, Chen90b] </ref>. For example, the TPC-B benchmark reports maximum throughput with 90% of all requests completed within 2 seconds [TPCB90]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh slower faster higherlower Response Time vs. Throughput ) m e i e n p e Throughput (MB/s) 500 300 100 1.00.80.60.40.20.0 to higher throughput but slower response time. <p> Throughput ) m e i e n p e Throughput (MB/s) 500 300 100 1.00.80.60.40.20.0 to higher throughput but slower response time. This figure was adapted from <ref> [Chen90b] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh -5- Another composite metric is data temperature, defined as I/O rate divided by capacity [Katz90]. Data temperature measures how many I/Os per second a storage system can support for a fixed amount of storage. <p> This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information.
Reference: [Chen93] <author> P. M. Chen and D. A. Patterson, </author> <title> ``A New Approach to I/O Performance Evaluation Self-Scaling I/O Benchmarks, Predicted I/O Performance (conference version)'', </title> <booktitle> Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Clara, California, </address> <month> May </month> <year> 1993, </year> <pages> 1-12. </pages>
Reference-contexts: A Self-Scaling Benchmark with Predicted Performance In this section, we describe two new ideas in I/O benchmarks, proposed in more detail by this paper's authors in <ref> [Chen93] </ref>. First, we describe a Self-Scaling Benchmark that automatically scales its workload depending on the performance of the system being measured. During evaluation, the benchmark automatically explores the workload space, searching for a relevant workload on which to base performance graphs. <p> Our five parameter workload model attempts to make a reasonable compromise between these two conflicting goals of simplicity and generality. <ref> [Chen93] </ref> gives a first-order verification that this synthetic workload model is general enough to capture the performance of interesting applications. 9.2.1.1. Single Parameter Graphs Most current benchmarks report the performance for only a single workload. The better benchmarks report performance for multiple workloads, usually in the form of a graph. <p> First, the benchmark should display a range of relevant work-loads. Relevancy in turn involves two factors: the workloads must perform reasonably well, and the work-loads must be practical. Second, the benchmark should adequately characterize the entire workload space with only a few graphs. <ref> [Chen93] </ref> gives more details as to how the benchmark chooses the focal points and the ranges of the graphs. 9.2.1.2.
Reference: [Ferrari84] <author> D. Ferrari, </author> <booktitle> ``On the Foundations of Artificial Workload Design'', Proceedings of the 1984 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1984, </year> <pages> 8-14. </pages>
Reference-contexts: In this paper, a workload refers to a user-level program with parameter values for each of the above five parameters. This program spawns and controls several processes if necessary. The most important question in developing a synthetic workload is the question of representativeness <ref> [Ferrari84] </ref>.
Reference: [Fine92] <author> J. A. Fine, T. E. Anderson, M. D. Dahlin, J. Frew, M. Olson and D. A. Patterson, </author> <title> ``Abstracts: A Latency-Hiding Technique for High-Capacity Mass Storage Systems'', </title> <type> Sequoia Technical Report 92/11, </type> <institution> University of California at Berkeley, </institution> <month> March </month> <year> 1992. </year> <month> -31- </month>
Reference-contexts: Current research related to tape devices addresses how to migrate data from tape to faster storage [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89], how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching <ref> [Gibson92, Fine92] </ref>. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly. The most common metric for reliability, mean-time-to-failure, has increased from 30,000 hours to 150,000-200,000 hours.
Reference: [Gaede81] <author> S. </author> <title> Gaede, ``Tools for Research in Computer Workload Characterization'', Experimental Computer Performance and Evaluation, 1981. </title> <editor> D. Ferrari, M. Spadoni, </editor> <publisher> eds.. </publisher>
Reference-contexts: Their first set of benchmarks, SPEC Release 1, primarily measures CPU performance. Their second set of benchmarks, System Development Multi-tasking (SDM) Suite, measures overall system performance for software development and research environments. SDM consists of two benchmarks, Sdet <ref> [Gaede81, Gaede82] </ref> and Kenbus1 [McDonell87]. Sdet and Kenbus1 are quite similar in benchmarking methodology; their main difference is the specific mix of user commands. We limit our discussion to Sdet, which does more I/O than Kenbus1. Sdet's workload consists of a number of concurrently running scripts.
Reference: [Gaede82] <author> S. </author> <title> Gaede, ``A Scaling Technique for Comparing Interactive System Capacities'', </title> <booktitle> 13th International Conference on Management and Performance Evaluation of Computer Systems, </booktitle> <year> 1982, </year> <pages> 62-67. </pages> <note> CMG 1982. </note>
Reference-contexts: Their first set of benchmarks, SPEC Release 1, primarily measures CPU performance. Their second set of benchmarks, System Development Multi-tasking (SDM) Suite, measures overall system performance for software development and research environments. SDM consists of two benchmarks, Sdet <ref> [Gaede81, Gaede82] </ref> and Kenbus1 [McDonell87]. Sdet and Kenbus1 are quite similar in benchmarking methodology; their main difference is the specific mix of user commands. We limit our discussion to Sdet, which does more I/O than Kenbus1. Sdet's workload consists of a number of concurrently running scripts.
Reference: [Gelsinger89] <author> P. P. Gelsinger, P. A. Gargini, G. H. Parker and A. Y. C. Yu, </author> <title> ``Microprocessors Circa 2000'', </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1989, </year> <pages> 43-47. </pages>
Reference-contexts: As can readily be seen, over the past two decades, IBM mainframe CPU performance has increased more than 30-fold, while IBM disk performance has barely doubled. Microprocessor performance has increased even faster than mainframe performance <ref> [Myers86, Gelsinger89] </ref>. If CPU performance continues to improve at its current pace and disk performance continues to obtain more moderate improvements, eventually the performance of all applications that do any input or output (I/O) will be limited by that I/O componentfurther CPU performance improvements will be wasted [Amdahl67].
Reference: [Gibson91] <author> G. A. Gibson, </author> <title> ``Redundant Disk Arrays: Reliable, Parallel Secondary Storage'', </title> <institution> UCB/Computer Science Dpt. 91/613, University of California at Berkeley, </institution> <month> December </month> <year> 1991. </year> <note> also available from MIT Press, </note> <year> 1992. </year>
Reference-contexts: The average yearly improvement in performance has inched forward at a few percent a year. Cost per capacity, on the other hand, has improved at a much faster pace, averaging a 23% reduction per year from 1977 to 1986 <ref> [Gibson91] </ref>. Disk size has also been gradually decreasing. The most common disk diameter of the 1970's and 1980's was 14". <p> This table was adapted from <ref> [Gibson91] </ref>. -6- replaced with 5.25" and 3.5" diameter disks. These smaller disks have somewhat better performance than their larger, more expensive predecessors. The trend toward smaller, less expensive disks creates an opportunity to combine many of these disk into a parallel storage system known as a disk array. <p> This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. <p> Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance [Chen90a, Lee91a, Lee91b], how to spread redundant information across disks to increase reliability and minimize the effect of disk failures <ref> [Holland92, Gibson91, Muntz90] </ref>, and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service requests. Requests which are serviced by a single disk, however, see the same response time. <p> On the other hand, solid state disks cost 50-100 times more than magnetic disks for the same capacity <ref> [Gibson91] </ref>. Two storage metrics have been addressedthroughput and response time. Dramatic improvements to capacity per cost have occurred in magnetic tapes (Figure 6). A new method of reading and writing tapes, helical scan, has increased the capacity of a single tape from .1-.2 GB to 5-20 GB [Katz91, Tan89, Vermeulen89]. <p> The most common metric for reliability, mean-time-to-failure, has increased from 30,000 hours to 150,000-200,000 hours. This jump in apparent reliability comes mostly from changing the method of computing mean-time-to-failure and is not expected to continue improving as quickly <ref> [Gibson91] </ref>. Innovation is also taking place in the file system. A good example of how file systems have improved I/O system performance is the Log-Structured File System (LFS) [Rosenblum91, Ousterhout89]. LFS writes data on the disk in the same order that it is written.
Reference: [Gibson92] <author> G. A. Gibson, R. H. Patterson and M. Satyanarayanan, </author> <title> ``Disk Reads with DRAM Latency'', </title> <booktitle> Third Workshop on Workstaion Operating Systems, </booktitle> <address> Key Biscayne, Florida, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Current research related to tape devices addresses how to migrate data from tape to faster storage [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89], how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching <ref> [Gibson92, Fine92] </ref>. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly. The most common metric for reliability, mean-time-to-failure, has increased from 30,000 hours to 150,000-200,000 hours.
Reference: [Hac89] <author> A. Hac, </author> <title> ``A Distributed Algorithm for Performance Improvement Through File Replication, File Migration, and Process Migration'', </title> <journal> IEEE Transactions on Software Engineering 15, </journal> <month> 11 (November </month> <year> 1989), </year> <pages> 1459-1470. </pages>
Reference-contexts: Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage <ref> [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89] </ref>, how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly.
Reference: [Harker81] <author> J. M. Harker, D. W. Brede, R. E. Pattison, G. R. Santana and L. G. Taft, </author> <title> ``A Quarter Century of Disk File Innovation'', </title> <journal> IBM Journal of Research and Development 25, </journal> <month> 5 (September </month> <year> 1981), </year> <pages> 677-689. </pages>
Reference-contexts: Average Seek Time 30 ms 12.5 ms 5% Average Rotational Delay 8.3 ms 7 ms 1% Transfer Rate 806 KB/s 1700 KB/s 4% iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c ter; the IBM 0661 was introduced in 1989 and has a 3.5 inch diameter <ref> [Harker81, IBM0661] </ref>. -7- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh IBM 3990 disk cache [Menon87], and main memory, as in the Sprite operating system's file cache. [Ousterhout88, Nelson88]. Response times for writes is decreased by writing the data to RAM, acknowledging the request, then transferring the data to disk asynchronously.
Reference: [Henderson89] <author> R. L. Henderson and A. Poston, ``MSS II and RASH: </author> <title> A Mainframe UNIX Based Mass Storage System with a Rapid Access Storage Hierarchy File Management System'', </title> <booktitle> Winter USENIX 1989, </booktitle> <month> January </month> <year> 1989, </year> <pages> 65-83. </pages>
Reference-contexts: Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage <ref> [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89] </ref>, how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly.
Reference: [Hennessy90] <author> J. L. Hennessy and D. A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: One way to demonstrate this declining validity is illustrated in Figure 1, where IBM disk performance, represented by the throughput of accessing a random 8 KB block of data, is contrasted with IBM mainframe CPU performance <ref> [Hennessy90] </ref>. For the sake of comparison, both CPU and disk performance are normalized to their 1971 levels. As can readily be seen, over the past two decades, IBM mainframe CPU performance has increased more than 30-fold, while IBM disk performance has barely doubled. <p> Both are normalized to their 1971 level. The IBM mainframe performance comes from Figure 1.1 on page 4 of <ref> [Hennessy90] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh -3- 2. Metrics More than other areas of computer performance evaluation, storage evaluation involves many varied types of metrics. In this section, we present an overview of some of the metrics commonly used today in choosing and evaluating storage systems. <p> The last column in the table lists the inherent measurement error, which was measured by running the same set of random workloads twice and using one run to ``predict'' performance of the other run. 10. Summary I/O performance, long neglected by computer architects and evaluators <ref> [Hennessy90] </ref>, is rapidly becoming an area of important research activity. As CPU and memory speed increases continue to outstrip I/O speed increases, this trend will continue and accelerate.
Reference: [Hill86] <author> M. D. Hill, S. J. Eggers, J. R. Larus, G. S. Taylor, G. Adams, B. K. Bose, G. A. Gibson, P. M. Hansen, J. Keller, S. I. Kong, C. G. Lee, D. Lee, J. M. Pendleton, S. A. Ritchie, D. A. Wood, B. G. Zorn, P. N. Hilfinger, D. Hodges, R. H. Katz, J. K. Ousterhout and D. A. Patterson, </author> <title> ``Design Decisions in SPUR'', </title> <booktitle> IEEE Computer 19, </booktitle> <month> 11 (November </month> <year> 1986). </year>
Reference-contexts: To make results directly comparable between machines for benchmarks which used the compiler, we took the same step as Ousterhout [Ousterhout90] in having the GNU C compiler generate code for an experimental CPU called SPUR <ref> [Hill86] </ref>. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii System Name SPARCstation 1+ DECstation 5000/200 HP 730 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Year Released 1989 1990 1991 CPU SPARC MIPS R3000 PA-RISC SPECint rating 8.3 19.9 76.8 Disk System CDC Wren IV 3 disk (Wren) RAID 0 HP 1350SX I/O Bus SCSI-I SCSI-I Fast SCSI-II Mem.
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> ``Parity Declustering for Continuous Operation in Redundant Disk Arrays'', </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <month> October </month> <year> 1992, </year> <pages> 23-35. </pages>
Reference-contexts: Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance [Chen90a, Lee91a, Lee91b], how to spread redundant information across disks to increase reliability and minimize the effect of disk failures <ref> [Holland92, Gibson91, Muntz90] </ref>, and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service requests. Requests which are serviced by a single disk, however, see the same response time.
Reference: [Howard88] <author> J. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham and M. J. West, </author> <title> ``Scale and Performance in a Distributed File System'', </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: But, as we shall see, they are often not I/O limited. 7.1.1. Andrew The Andrew benchmark was designed at Carnegie Mellon University to be a file system benchmark for comparatively evaluating the Andrew File System against other file systems <ref> [Howard88] </ref>. It was originally meant to be only a convenient yardstick for measuring file systems, not necessarily as a representative workload for benchmarking. Despite this intent, it has become a widely used de facto benchmarking standard [Ousterhout90].
Reference: [Hu86] <author> I. Hu, </author> <title> ``Measuring File Access Patterns in UNIX'', </title> <booktitle> Proceedings of the 1986 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1986, </year> <pages> 15-20. </pages>
Reference-contexts: We show results for the three systems in Figure 11. Most of Bonnie's workloads are I/O limited, however, the character reads and writes are CPU limited. 7.2.2. IOStone IOStone is a synthetic I/O benchmark [Park90] based on system traces of Unix minicomputers and workstations <ref> [Ousterhout85, Hu86] </ref> and IBM mainframes [Smith78, Smith81a]. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in [Ousterhout85]. One process performs all the accessesno I/O parallelism is present. IOStone reports a single throughput result, IOStones per second (Figure 12).
Reference: [IBM0661] <institution> IBM 0661 Disk Drive Product Description--Model 371, IBM, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: years 753 years Cost (estimated) $156,000 - $260,000 $67,000 - ? iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c for an IBM 3390 disk system and a redundant disk array made of IBM 0661 3.5" drives <ref> [IBM0661] </ref>. This table was adapted from [Gibson91]. -6- replaced with 5.25" and 3.5" diameter disks. These smaller disks have somewhat better performance than their larger, more expensive predecessors. <p> Average Seek Time 30 ms 12.5 ms 5% Average Rotational Delay 8.3 ms 7 ms 1% Transfer Rate 806 KB/s 1700 KB/s 4% iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c ter; the IBM 0661 was introduced in 1989 and has a 3.5 inch diameter <ref> [Harker81, IBM0661] </ref>. -7- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh IBM 3990 disk cache [Menon87], and main memory, as in the Sprite operating system's file cache. [Ousterhout88, Nelson88]. Response times for writes is decreased by writing the data to RAM, acknowledging the request, then transferring the data to disk asynchronously.
Reference: [Johnson84] <author> O. G. Johnson, </author> <title> ``Three-Dimensional Wave Equation Computations on Vector Computers'', </title> <booktitle> Proceedings of the IEEE 72, </booktitle> <month> 1 (January </month> <year> 1984). </year>
Reference-contexts: The trend toward smaller, less expensive disks creates an opportunity to combine many of these disk into a parallel storage system known as a disk array. The concept of constructing an array of multiple disks has been used for many years for special purposes <ref> [Johnson84] </ref> but is only now becoming popular for general use. The list of companies developing or marketing disk arrays is quite long: Array Technology, Auspex, Ciprico, Compaq, Cray, Datamax, Hewlett-Packard, IBM, Imprimis, Intel Scientific, Intellistor, Maximum Strategy, Pacstor, SF2, Storage Concepts, Storage Technology, and Thinking Machines.
Reference: [Jones89] <author> A. L. Jones, </author> <title> SSD is Cheaper than DASD, Storage Technology Corporation, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: Solid state disk is much more expensive than magnetic disk for equal capacity but is dramatically faster. -8- Response times for solid state disks are commonly less than 3 ms <ref> [Cassidy89, Jones89] </ref>, while response times for magnetic disks are approximately 10-30 ms. On the other hand, solid state disks cost 50-100 times more than magnetic disks for the same capacity [Gibson91]. Two storage metrics have been addressedthroughput and response time.
Reference: [Katz90] <author> R. H. Katz, D. W. Gordon and J. A. Tuttle, </author> <title> ``Storage System Metrics for Evaluating Disk Array Organizations'', </title> <institution> UCB/Computer Science Dpt. 90/611, University of California at Berkeley, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Throughput ) m e i e n p e Throughput (MB/s) 500 300 100 1.00.80.60.40.20.0 to higher throughput but slower response time. This figure was adapted from [Chen90b]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh -5- Another composite metric is data temperature, defined as I/O rate divided by capacity <ref> [Katz90] </ref>. Data temperature measures how many I/Os per second a storage system can support for a fixed amount of storage. Users who are limited by I/O rate rather than capacity should buy systems with high data tempera ture.
Reference: [Katz91] <author> R. H. Katz, T. E. Anderson, J. K. Ousterhout and D. A. Patterson, </author> <title> ``Robo-line Storage: Low Latency, High Capacity Storage Systems over Geographcially Distributed Networks'', </title> <institution> UCB/Computer Science Dpt. 91/651, University of California at Berkeley, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Two storage metrics have been addressedthroughput and response time. Dramatic improvements to capacity per cost have occurred in magnetic tapes (Figure 6). A new method of reading and writing tapes, helical scan, has increased the capacity of a single tape from .1-.2 GB to 5-20 GB <ref> [Katz91, Tan89, Vermeulen89] </ref>. Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. <p> Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89], how to increase tape throughput using striping <ref> [Katz91] </ref>, and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly. The most common metric for reliability, mean-time-to-failure, has increased from 30,000 hours to 150,000-200,000 hours.
Reference: [Kim86] <author> M. Y. Kim, </author> <title> ``Synchronized Disk Interleaving'', </title> <journal> IEEE Transactions on Computers C-35, </journal> <month> 11 (November </month> <year> 1986), </year> <pages> 978-988. </pages> <month> -32- </month>
Reference-contexts: This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information.
Reference: [Lee91a] <author> E. K. Lee and R. H. Katz, </author> <title> ``An Analytic Performance Model of Disk Arrays and its Applications'', </title> <institution> UCB/Computer Science Dpt. 91/660, University of California at Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance <ref> [Chen90a, Lee91a, Lee91b] </ref>, how to spread redundant information across disks to increase reliability and minimize the effect of disk failures [Holland92, Gibson91, Muntz90], and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service
Reference: [Lee91b] <author> E. K. Lee and R. H. Katz, </author> <title> ``Performance Consequences of Parity Placement in Disk Arrays'', </title> <booktitle> Proceedings of the 4rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <month> April </month> <year> 1991, </year> <pages> 190-199. </pages>
Reference-contexts: Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance <ref> [Chen90a, Lee91a, Lee91b] </ref>, how to spread redundant information across disks to increase reliability and minimize the effect of disk failures [Holland92, Gibson91, Muntz90], and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service
Reference: [Levitt92] <author> J. Levitt, </author> <title> ``Better Benchmarks are Brewing'', Unix Today!, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: The result was LADDIS, named after the seven companies (Legato, Auspex, Digital Equipment Corporation, Data General, Interphase, and Sun). LADDIS is based on NHFSStone but, unlike NHFSStone, runs on multiple, possibly heterogeneous, clients and networks <ref> [Nelson92, Levitt92] </ref>. Like NHFSStone, LADDIS is a synthetic benchmark with a certain mix of operations.
Reference: [Livny87] <author> M. Livny, S. Khoshafian and H. Boral, </author> <title> ``Multi-Disk Management Algorithms'', </title> <booktitle> Proceedings of the 1987 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1987, </year> <pages> 69-77. </pages>
Reference-contexts: This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information.
Reference: [McDonell87] <author> K. J. McDonell, </author> <title> ``Taking performance evaluation out of the stone age'', </title> <booktitle> Proceedings of the Summer Usenix Technical Conference, </booktitle> <address> Phoenix, Arizona, </address> <month> June </month> <year> 1987, </year> <pages> 407-417. </pages>
Reference-contexts: Their first set of benchmarks, SPEC Release 1, primarily measures CPU performance. Their second set of benchmarks, System Development Multi-tasking (SDM) Suite, measures overall system performance for software development and research environments. SDM consists of two benchmarks, Sdet [Gaede81, Gaede82] and Kenbus1 <ref> [McDonell87] </ref>. Sdet and Kenbus1 are quite similar in benchmarking methodology; their main difference is the specific mix of user commands. We limit our discussion to Sdet, which does more I/O than Kenbus1. Sdet's workload consists of a number of concurrently running scripts.
Reference: [Menon87] <author> J. Menon and M. Hartung, </author> <title> ``The IBM 3990 Model 3 Disk Cache'', </title> <type> RJ 5994 (59593), </type> <institution> IBM, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Rotational Delay 8.3 ms 7 ms 1% Transfer Rate 806 KB/s 1700 KB/s 4% iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c ter; the IBM 0661 was introduced in 1989 and has a 3.5 inch diameter [Harker81, IBM0661]. -7- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh IBM 3990 disk cache <ref> [Menon87] </ref>, and main memory, as in the Sprite operating system's file cache. [Ousterhout88, Nelson88]. Response times for writes is decreased by writing the data to RAM, acknowledging the request, then transferring the data to disk asynchronously. <p> This technique, called write-behind, leaves the data in RAM more vulnerable to system failures until written to disk. Some systems, such as the IBM 3990, mitigate this reliability problem by storing the cached data in non-volatile memory, which is immune to power failures <ref> [Menon87] </ref>. As with any cache, read response time is decreased if the requested data is found in cache RAM. Solid state disks are similar to caches in that they improve response time by storing requests in RAM rather than on magnetic disks.
Reference: [Menon89] <author> J. Menon and J. Kasson, </author> <title> ``Methods for Improved Update Performance of Disk Arrays'', </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <month> July </month> <year> 1989. </year>
Reference-contexts: how to distribute (stripe) data across disks to get optimal performance [Chen90a, Lee91a, Lee91b], how to spread redundant information across disks to increase reliability and minimize the effect of disk failures [Holland92, Gibson91, Muntz90], and how to reduce the penalties associated with small writes in certain types of disk arrays <ref> [Stodolsky93, Menon89] </ref> Disk arrays improve throughput by using more disks to service requests. Requests which are serviced by a single disk, however, see the same response time. File caches, disk caches, and solid state disks use dynamic RAM (random access memory) to decrease response time.
Reference: [Miller91a] <author> E. L. Miller and R. H. Katz, </author> <booktitle> ``Input/Output Behavior of Supercomputing Applications'', Proceedings of Supercomputing '91, </booktitle> <month> November </month> <year> 1991, </year> <pages> 567-576. </pages>
Reference-contexts: I/O rate is generally used for applications where the size of each request is small, such as transaction processing [Anon85]; data rate is generally used for applications where the size of each request is large, such as scientific applications <ref> [Miller91a] </ref>. Response time is the second basic performance metric for storage systems. Response time measures how long a storage system takes to access data. This time can be measured in several ways. <p> Scientific Andrew, SDM, IOStone, and Bonnie all target system development or workstation environments. Other application areas, such as scientific or supercomputing code, have substantially different workload characteristics <ref> [Miller91a] </ref>. Typical scientific applications generally touch much more data and use much larger request sizes than workstation applications.
Reference: [Miller91b] <author> E. L. Miller, </author> <title> ``File Migration on the Cray Y-MP at the National Center for Atmospheric Research'', </title> <institution> UCB/Computer Science Dpt. 91/638, University of California at Berkeley, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage <ref> [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89] </ref>, how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly.
Reference: [Montgomery91] <author> J. B. J. T. Liu, </author> <title> editor. ``RAID: A Technology Poised for Explosive Growth'', </title> <type> Report DJIA: 2902, Montgomery Securities, </type> <month> December </month> <year> 1991. </year>
Reference-contexts: Some analysts have projected the disk array market to expand to $8 billion market by 1994 <ref> [Montgomery91] </ref>. The basic concept behind disk arrays is straightforwardcombine many small disks and distribute data among them (Figure 5). This increases the aggregate throughput available to an application.
Reference: [Muntz90] <author> R. R. Muntz and J. C. S. Lui, </author> <title> ``Performance Analysis of Disk Arrays under Failure'', </title> <booktitle> Proceedings of the 16th Conference on Very Large Data Bases, 1990. VLDB XVI. </booktitle>
Reference-contexts: Current disk array research is focusing on how to distribute (stripe) data across disks to get optimal performance [Chen90a, Lee91a, Lee91b], how to spread redundant information across disks to increase reliability and minimize the effect of disk failures <ref> [Holland92, Gibson91, Muntz90] </ref>, and how to reduce the penalties associated with small writes in certain types of disk arrays [Stodolsky93, Menon89] Disk arrays improve throughput by using more disks to service requests. Requests which are serviced by a single disk, however, see the same response time.
Reference: [Myers86] <author> G. J. Myers, A. Y. C. Yu and D. L. House, </author> <title> ``Microprocessor Technology Trends'', </title> <booktitle> Proceedings of the IEEE 74, </booktitle> <month> 12 (December </month> <year> 1986), </year> <pages> 1605-1622. </pages>
Reference-contexts: As can readily be seen, over the past two decades, IBM mainframe CPU performance has increased more than 30-fold, while IBM disk performance has barely doubled. Microprocessor performance has increased even faster than mainframe performance <ref> [Myers86, Gelsinger89] </ref>. If CPU performance continues to improve at its current pace and disk performance continues to obtain more moderate improvements, eventually the performance of all applications that do any input or output (I/O) will be limited by that I/O componentfurther CPU performance improvements will be wasted [Amdahl67].
Reference: [Nelson88] <author> M. N. Nelson, B. B. Welch and J. K. Ousterhout, </author> <title> ``Caching in the Sprite Network File System'', </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 134-154. </pages>
Reference-contexts: KB/s 4% iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c ter; the IBM 0661 was introduced in 1989 and has a 3.5 inch diameter [Harker81, IBM0661]. -7- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh IBM 3990 disk cache [Menon87], and main memory, as in the Sprite operating system's file cache. <ref> [Ousterhout88, Nelson88] </ref>. Response times for writes is decreased by writing the data to RAM, acknowledging the request, then transferring the data to disk asynchronously. This technique, called write-behind, leaves the data in RAM more vulnerable to system failures until written to disk.
Reference: [Nelson92] <author> B. Nelson, B. Lyon, M. Wittle and B. Keith, </author> <title> ``LADDIS--A Multi-Vendor & Vendor-Neutral NFS Benchmark'', </title> <booktitle> UniForum Conference, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: The result was LADDIS, named after the seven companies (Legato, Auspex, Digital Equipment Corporation, Data General, Interphase, and Sun). LADDIS is based on NHFSStone but, unlike NHFSStone, runs on multiple, possibly heterogeneous, clients and networks <ref> [Nelson92, Levitt92] </ref>. Like NHFSStone, LADDIS is a synthetic benchmark with a certain mix of operations.
Reference: [Ousterhout85] <author> J. K. Ousterhout, H. Da Costa and et al., </author> <title> ``A TraceDriven Analysis of the UNIX 4.2 BSD File System'', </title> <booktitle> Operating Systems Review 19, </booktitle> <month> 5 (December </month> <year> 1985), </year> <pages> 15-24. </pages> <booktitle> Proceedings of the 10th Symp. on Operating System Principles. </booktitle>
Reference-contexts: We show results for the three systems in Figure 11. Most of Bonnie's workloads are I/O limited, however, the character reads and writes are CPU limited. 7.2.2. IOStone IOStone is a synthetic I/O benchmark [Park90] based on system traces of Unix minicomputers and workstations <ref> [Ousterhout85, Hu86] </ref> and IBM mainframes [Smith78, Smith81a]. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in [Ousterhout85]. One process performs all the accessesno I/O parallelism is present. IOStone reports a single throughput result, IOStones per second (Figure 12). <p> IOStone IOStone is a synthetic I/O benchmark [Park90] based on system traces of Unix minicomputers and workstations [Ousterhout85, Hu86] and IBM mainframes [Smith78, Smith81a]. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in <ref> [Ousterhout85] </ref>. One process performs all the accessesno I/O parallelism is present. IOStone reports a single throughput result, IOStones per second (Figure 12). IOStone runs much faster when the system's file cache is large enough to contain the small data space of the benchmark. <p> These operations included reads, writes, and various other file operations such as examining a file. The exact mix of operations was patterned after a study done by Sun [Sandberg85]; the file sizes were patterned after the study done in <ref> [Ousterhout85] </ref>. Later, Legato Systems refined NFSStone, dubbing it NHFSStone. NFSStone and NHFSStone had several problems: one client could not always fully stress a file server; different versions -20- of the benchmarks abounded; file and block sizes were not realistic; and only SunOS clients could run them.
Reference: [Ousterhout88] <author> J. K. Ousterhout, A. Cherenson, F. Douglis and M. Nelson, </author> <title> ``The Sprite Network Operating System'', </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: KB/s 4% iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c ter; the IBM 0661 was introduced in 1989 and has a 3.5 inch diameter [Harker81, IBM0661]. -7- hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh IBM 3990 disk cache [Menon87], and main memory, as in the Sprite operating system's file cache. <ref> [Ousterhout88, Nelson88] </ref>. Response times for writes is decreased by writing the data to RAM, acknowledging the request, then transferring the data to disk asynchronously. This technique, called write-behind, leaves the data in RAM more vulnerable to system failures until written to disk.
Reference: [Ousterhout89] <author> J. K. Ousterhout and F. Douglis, </author> <title> ``Beating the I/O Bottleneck: A Case for Log-Structured File Systems'', </title> <type> SIGOPS 23, </type> <month> 1 (January </month> <year> 1989), </year> <pages> 11-28. </pages>
Reference-contexts: Innovation is also taking place in the file system. A good example of how file systems have improved I/O system performance is the Log-Structured File System (LFS) <ref> [Rosenblum91, Ousterhout89] </ref>. LFS writes data on the disk in the same order that it is written. This leads to highly sequentialized disk writes and so improves the sustainable disk write throughput.
Reference: [Ousterhout90] <author> J. K. Ousterhout, </author> <title> ``Why aren't operating systems getting faster as fast as hardware?'', </title> <booktitle> Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990, </year> <pages> 247-256. </pages>
Reference-contexts: Hence, we used publicly available code for as many programs as possible. In general, we used GNU (Gnu's Not Unix) code developed by the Free Software Foundation. To make results directly comparable between machines for benchmarks which used the compiler, we took the same step as Ousterhout <ref> [Ousterhout90] </ref> in having the GNU C compiler generate code for an experimental CPU called SPUR [Hill86]. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii System Name SPARCstation 1+ DECstation 5000/200 HP 730 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Year Released 1989 1990 1991 CPU SPARC MIPS R3000 PA-RISC SPECint rating 8.3 19.9 76.8 Disk System CDC Wren IV 3 disk (Wren) RAID <p> It was originally meant to be only a convenient yardstick for measuring file systems, not necessarily as a representative workload for benchmarking. Despite this intent, it has become a widely used de facto benchmarking standard <ref> [Ousterhout90] </ref>. Andrew is meant to represent the workload generated by a typical set of software system developers. It copies a file directory hierarchy, examines and reads the new copy, then compiles the copy. The file directory contains 70 files totaling .2 MB. <p> Each of these numbers represents the average of three runs; each run starts with an empty file cache. Note the small percentage of time spent in I/O. -13- In Figure 8, we list results from Andrew on our three system platforms. As in <ref> [Ousterhout90] </ref>, we divide Andrew into two sections: the copy phase, consisting of the copy, examination, and reading stages; and the compile phase. Note that on all machines Andrew spends only 6%-13% actually doing data reads and writes.
Reference: [Park90] <author> A. Park and J. C. Becker, ``IOStone: </author> <title> A synthetic file system benchmark'', Computer Architecture News 18, </title> <month> 2 (June </month> <year> 1990), </year> <pages> 45-52. </pages>
Reference-contexts: We show results for the three systems in Figure 11. Most of Bonnie's workloads are I/O limited, however, the character reads and writes are CPU limited. 7.2.2. IOStone IOStone is a synthetic I/O benchmark <ref> [Park90] </ref> based on system traces of Unix minicomputers and workstations [Ousterhout85, Hu86] and IBM mainframes [Smith78, Smith81a]. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in [Ousterhout85]. One process performs all the accessesno I/O parallelism is present.
Reference: [Patterson88] <author> D. A. Patterson, G. Gibson and R. H. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID)'', </title> <booktitle> International Conference on Management of Data (SIGMOD), </booktitle> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information. <p> MB/s Memory Size 28 MB 32 MB 32 MB Operating System SunOS 4.1 Sprite LFS HP/UX 8.07 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c a three disk RAID disk array <ref> [Patterson88] </ref> with a 16 KB striping unit [Chen90a] and is configured without redundancy. The SPECint rating is a measure of the integer speed of the processor. Ratings are relative to the speed of a VAX 11/780. <p> LADDIS is an evolutionary step beyond current benchmarks; A Self-Scaling Benchmark with Predicted Performance is a research idea being developed by this paper's authors at the University of California at Berkeley in the context of the RAID (Redundant Arrays of Inexpensive Disks) project <ref> [Patterson88] </ref>. 9.1. LADDIS Network file systems provide file service to a set of client computers, connected by a network. The computer providing this file service is called the server. One popular protocol for network file service is Sun Microsystem's NFS [Sandberg85].
Reference: [Rosenblum91] <author> M. Rosenblum and J. K. Ousterhout, </author> <title> ``The Design and Implementation of a Log-Structured File System'', </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year> <month> -33- </month>
Reference-contexts: Innovation is also taking place in the file system. A good example of how file systems have improved I/O system performance is the Log-Structured File System (LFS) <ref> [Rosenblum91, Ousterhout89] </ref>. LFS writes data on the disk in the same order that it is written. This leads to highly sequentialized disk writes and so improves the sustainable disk write throughput.
Reference: [Rosenblum92] <author> M. Rosenblum, </author> <title> Sprite LFS Write Cache Size, </title> <type> personal communication, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: The write cache of LFS is smaller because LFS limits the number of dirty cache blocks to avoid deadlock during cleaning. The effective file cache size for writes is only 5-8 MB, while for reads it is 20 MB <ref> [Rosenblum92] </ref>. 5 In contrast, when uniqueBytes is large enough to exercise the disk for both reads and writes, writes are faster than reads. This phenomenon is due to Sprite's LFS, which improves write performance by grouping multiple small writes into fewer large writes. 9.2.2.
Reference: [SPEC91] <editor> SPEC SDM Release 1.0 Manual, </editor> <title> System Performance Evaluation Cooperative, </title> <note> 1991. </note> <author> [Saavedra-Barrera89] R. H. Saavedra-Barrera, A. J. Smith and E. Miya, </author> <title> ``Machine Characterization Based on an Abstract High-Level Language Machine'', </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1659-1679. </pages>
Reference-contexts: Sdet's workload consists of a number of concurrently running scripts. Each script contains a list of user commands in random order. These commands are taken from a typical software development environment and include editing, text formatting, compiling, file creating and deleting, as well as miscellaneous other UNIX utilities <ref> [SPEC91] </ref>.
Reference: [Salem86] <author> K. Salem and H. Garcia-Molina, </author> <title> ``Disk Striping'', </title> <booktitle> Proceedings of the Second International Conference on Data Engineering, </booktitle> <year> 1986, </year> <pages> 336-342. </pages>
Reference-contexts: This increases the aggregate throughput available to an application. The array of disks can either service many small accesses in parallel or cooperate to deliver a higher data rate to a single large access <ref> [Patterson88, Gibson91, Chen90a, Chen90b, Livny87, Kim86, Salem86] </ref>. Disk arrays compensate for the lower reliability inherent in using more disks by storing redundant, error-correcting information.
Reference: [Sandberg85] <author> R. Sandberg, D. Goldbert, S. Kleiman, D. Walsh and B. Lyon, </author> <title> ``Design and Implementation of the Sun Network Filesystem'', </title> <booktitle> Summer 1985 Usenix Conference, </booktitle> <year> 1985. </year>
Reference-contexts: LADDIS Network file systems provide file service to a set of client computers, connected by a network. The computer providing this file service is called the server. One popular protocol for network file service is Sun Microsystem's NFS <ref> [Sandberg85] </ref>. In 1989, Shein, Callahan, and Woodbury created NFSStone, a synthetic benchmark to measure NFS performance [Shein89]. NFSStone generated a series of NFS file requests from a single client to stress and measure server performance. These operations included reads, writes, and various other file operations such as examining a file. <p> NFSStone generated a series of NFS file requests from a single client to stress and measure server performance. These operations included reads, writes, and various other file operations such as examining a file. The exact mix of operations was patterned after a study done by Sun <ref> [Sandberg85] </ref>; the file sizes were patterned after the study done in [Ousterhout85]. Later, Legato Systems refined NFSStone, dubbing it NHFSStone.
Reference: [Scott90] <author> V. Scott, </author> <title> ``Is Standardization of Benchmarks Feasible?'', </title> <booktitle> Proceedings of the BUSCON Conference, </booktitle> <address> Long Beach, CA, </address> <month> February </month> <year> 1990, </year> <pages> 139-147. </pages>
Reference-contexts: In Figure 9, we show the TPC-B response time characteristic on our three systems, using Seltzer's simple transaction supporting package LIBTP [Seltzer92]. 7.1.3. Sdet The System Performance Evaluation Cooperative (SPEC) was founded in 1988 to establish independent standard benchmarks <ref> [Scott90] </ref>. Their first set of benchmarks, SPEC Release 1, primarily measures CPU performance. Their second set of benchmarks, System Development Multi-tasking (SDM) Suite, measures overall system performance for software development and research environments. SDM consists of two benchmarks, Sdet [Gaede81, Gaede82] and Kenbus1 [McDonell87].
Reference: [Seltzer92] <author> M. Seltzer and M. Olson, ``LIBTP: </author> <title> Portable, Modular Transactions for UNIX'', </title> <booktitle> USENIX 1992?, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: in I/O 90% response time 200 ms average response time 126 ms (concurrency of 2) peak throughput 11.0 transactions/second HP 730 n i c s a T o Response Time (seconds) 100 80 60 40 20 0 95% time in I/O gram, we used Seltzer's simple transaction processing library LIBTP <ref> [Seltzer92] </ref>. Due to software limitations, we were unable to run at concurrencies higher than 2, reflected by response times much faster than those required by TPC-B. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 The main difference between TPC-A and TPC-B is the presence of real terminals. <p> Using this database size, TPC-B reports a graph of throughput versus the average number of outstanding requests and a histogram of response times for the maximum throughput. In Figure 9, we show the TPC-B response time characteristic on our three systems, using Seltzer's simple transaction supporting package LIBTP <ref> [Seltzer92] </ref>. 7.1.3. Sdet The System Performance Evaluation Cooperative (SPEC) was founded in 1988 to establish independent standard benchmarks [Scott90]. Their first set of benchmarks, SPEC Release 1, primarily measures CPU performance.
Reference: [Shein89] <author> B. Shein, M. Callahan and P. Woodbuy, </author> <title> ``NFSStone--A Network File Server Performance Benchmark'', </title> <booktitle> Proceedings of the USENIX Summer Technical Conference 1989, </booktitle> , <pages> 269-275. </pages>
Reference-contexts: The computer providing this file service is called the server. One popular protocol for network file service is Sun Microsystem's NFS [Sandberg85]. In 1989, Shein, Callahan, and Woodbury created NFSStone, a synthetic benchmark to measure NFS performance <ref> [Shein89] </ref>. NFSStone generated a series of NFS file requests from a single client to stress and measure server performance. These operations included reads, writes, and various other file operations such as examining a file.
Reference: [Smith78] <author> A. J. Smith, </author> <title> ``Sequentiality and Prefetching in Database Systems'', </title> <journal> ACM Transactions on Database Systems 3, </journal> <volume> 3 (1978), </volume> <pages> 223-247. </pages>
Reference-contexts: We show results for the three systems in Figure 11. Most of Bonnie's workloads are I/O limited, however, the character reads and writes are CPU limited. 7.2.2. IOStone IOStone is a synthetic I/O benchmark [Park90] based on system traces of Unix minicomputers and workstations [Ousterhout85, Hu86] and IBM mainframes <ref> [Smith78, Smith81a] </ref>. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in [Ousterhout85]. One process performs all the accessesno I/O parallelism is present. IOStone reports a single throughput result, IOStones per second (Figure 12).
Reference: [Smith81a] <author> A. J. Smith, </author> <title> ``Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms'', </title> <journal> IEEE Transactions on Software Engineering SE-7, </journal> <volume> No. 4 (1981), </volume> <pages> 403-417. </pages>
Reference-contexts: We show results for the three systems in Figure 11. Most of Bonnie's workloads are I/O limited, however, the character reads and writes are CPU limited. 7.2.2. IOStone IOStone is a synthetic I/O benchmark [Park90] based on system traces of Unix minicomputers and workstations [Ousterhout85, Hu86] and IBM mainframes <ref> [Smith78, Smith81a] </ref>. Using 400 files totaling 1 MB, IOStone reads and writes data in patterns which approximate the locality found in [Ousterhout85]. One process performs all the accessesno I/O parallelism is present. IOStone reports a single throughput result, IOStones per second (Figure 12).
Reference: [Smith81b] <author> A. J. Smith, </author> <title> ``Optimization of I/O Systems by Cache Disk and File Migration: A Summary'', Performance Evaluation 1, </title> <month> 3 (November </month> <year> 1981), </year> <pages> 249-262. </pages>
Reference-contexts: Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage <ref> [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89] </ref>, how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly.
Reference: [Smith85] <author> A. J. Smith, </author> <title> ``Disk Cache-Miss Ratio Analysis and Design Considerations'', </title> <journal> ACM Transactions on Computer Systems 3, </journal> <month> 3 (August </month> <year> 1985), </year> <pages> 161-203. </pages>
Reference-contexts: Requests which are serviced by a single disk, however, see the same response time. File caches, disk caches, and solid state disks use dynamic RAM (random access memory) to decrease response time. Caches can be placed in a variety of places in the system memory hierarchy <ref> [Smith85] </ref>.
Reference: [Stodolsky93] <author> D. Stodolsky and G. A. Gibson, </author> <title> ``Parity Logging: Overcoming the Small Write Problem in Redundant Disk Arrays'', </title> <booktitle> Proceedings of the 1993 International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: how to distribute (stripe) data across disks to get optimal performance [Chen90a, Lee91a, Lee91b], how to spread redundant information across disks to increase reliability and minimize the effect of disk failures [Holland92, Gibson91, Muntz90], and how to reduce the penalties associated with small writes in certain types of disk arrays <ref> [Stodolsky93, Menon89] </ref> Disk arrays improve throughput by using more disks to service requests. Requests which are serviced by a single disk, however, see the same response time. File caches, disk caches, and solid state disks use dynamic RAM (random access memory) to decrease response time.
Reference: [TPCA89] <editor> TPC Benchmark A Standard Specification, </editor> <booktitle> Transaction Processing Performance Council, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: TPC-B TPC-B measures transaction processing performance for a simple database update [TPCB90]. The first version, TP1, first appeared in 1985 [Anon85] and quickly became the de facto standard in bench marking transaction processing systems. TPC-A 1 <ref> [TPCA89] </ref> and TPC-B [TPCB90] are more tightly specified versions of TP1 and have replaced TP1 as the standard transaction processing benchmark.
Reference: [TPCB90] <editor> TPC Benchmark B Standard Specification, </editor> <booktitle> Transaction Processing Performance Council, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Because a single performance number is easier to use than a full graph, many evaluators combine throughput and response time by reporting throughput at a given response time [Anon85, Chen90b]. For example, the TPC-B benchmark reports maximum throughput with 90% of all requests completed within 2 seconds <ref> [TPCB90] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh slower faster higherlower Response Time vs. Throughput ) m e i e n p e Throughput (MB/s) 500 300 100 1.00.80.60.40.20.0 to higher throughput but slower response time. <p> This supports our contention that systems with faster and faster CPUs will become more and more I/O lim ited. 7.1.2. TPC-B TPC-B measures transaction processing performance for a simple database update <ref> [TPCB90] </ref>. The first version, TP1, first appeared in 1985 [Anon85] and quickly became the de facto standard in bench marking transaction processing systems. TPC-A 1 [TPCA89] and TPC-B [TPCB90] are more tightly specified versions of TP1 and have replaced TP1 as the standard transaction processing benchmark. <p> TPC-B TPC-B measures transaction processing performance for a simple database update <ref> [TPCB90] </ref>. The first version, TP1, first appeared in 1985 [Anon85] and quickly became the de facto standard in bench marking transaction processing systems. TPC-A 1 [TPCA89] and TPC-B [TPCB90] are more tightly specified versions of TP1 and have replaced TP1 as the standard transaction processing benchmark.
Reference: [Tan89] <author> E. Tan and B. Vermeulen, </author> <title> ``Digital audio tape for data storage'', </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1989, </year> <pages> 34-38. </pages>
Reference-contexts: Two storage metrics have been addressedthroughput and response time. Dramatic improvements to capacity per cost have occurred in magnetic tapes (Figure 6). A new method of reading and writing tapes, helical scan, has increased the capacity of a single tape from .1-.2 GB to 5-20 GB <ref> [Katz91, Tan89, Vermeulen89] </ref>. Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s.
Reference: [Thanhardt88] <author> E. Thanhardt and G. Harano, </author> <title> ``File Migration in the NCAR Mass Storage System'', </title> <booktitle> Proceedings of the Ninth IEEE Symposium on Mass Storage Systems, </booktitle> <month> October </month> <year> 1988. </year>
Reference-contexts: Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s. Current research related to tape devices addresses how to migrate data from tape to faster storage <ref> [Smith81b, Thanhardt88, Hac89, Miller91b, Henderson89] </ref>, how to increase tape throughput using striping [Katz91], and how to decrease response time by prefetching and caching [Gibson92, Fine92]. Reported disk reliability has improved dramatically over the past ten years, though actual reliability has improved more slowly.
Reference: [Vermeulen89] <author> B. Vermeulen, </author> <title> ``Helical Scan and DAT--a Revolution in Computer Tape Technology'', </title> <booktitle> Systems Design and Networks Conference (SDNC), </booktitle> <month> May </month> <year> 1989, </year> <pages> 79-86. </pages>
Reference-contexts: Two storage metrics have been addressedthroughput and response time. Dramatic improvements to capacity per cost have occurred in magnetic tapes (Figure 6). A new method of reading and writing tapes, helical scan, has increased the capacity of a single tape from .1-.2 GB to 5-20 GB <ref> [Katz91, Tan89, Vermeulen89] </ref>. Tapes are extremely slow, however, with response times of 20 seconds to a few minutes. Throughput for these devices is less dismaying, ranging from .1-2.0 MB/s.
References-found: 66

