URL: ftp://ftp.cc.gatech.edu/pub/gvu/tr/1992/92-22.ps.Z
Refering-URL: http://www.cs.gatech.edu/gvu/reports/1992/
Root-URL: 
Email: burgess@cc.gatech.edu  
Title: Real-Time Audio Spatialization with Inexpensive Hardware spatial sound as implemented in Mercator. Section I: Background
Author: David A. Burgess 
Date: 1986, 1990 Americans  
Note: This paper is written in two sections. The first section describes the general spatialization technique. The second section describes  2. Named for Gerhardus Mercator, the cartographer who devised the Mercator map projection. 3. Recent legislation in the United States mandates that computer suppliers ensure accessibility of their systems and that employers provide accessible equipment (Title 508 of the Rehabilitation Act of  
Address: Atlanta, Georgia 30332  
Affiliation: Graphics Visualization and Usability Center Multimedia Group Georgia Institute of Technology  
Abstract: step is to convert monaural sounds to binaural sounds by artificially spatializing them. This goal has lead research interest in the subject by the military, by NASA (Wenzel et al 1988), and by user interface designers (Ludwig et al 1990, 1991). The work described in this paper is part of the Mercator 2 project (Mynatt & Edwards, 1992). The goal of Mercator is to allow visually-impaired computer users to have access to application software packages with graphical user interfaces under the X Window System 3 . Mercator will accomplish this task by mapping the structures and behaviors of X applications to an auditory and tactile space. An important feature of Mercator will be the use of spatial sound as a primary organizational cue. For Mercator to be useful, this feature must be implemented at a reasonable cost with off-the-shelf hardware. How We Localize Sounds Much of our information about the position of a sound source is based on the effects of resonances inside the ear and refraction in the vicinity of the head and upper body. The dominant cues provided by these head and upper body effects are interaural delay time (IDT) (Rayleigh, 1907), head shadow (Mills, 1972), pinna and ear canal response (Gardener, 1973), and shoulder echoes (Searle, et al, 1976). Together, these cues form the head-related transfer function (HRTF). Blauert (1983) provides a comprehensive description of the HRTF, and Searle, et al (1976) give a statistical study of the relative importance of HRTF cues. Of the HRTF cues listed above, only IDT is described in this paper. with Disabilities Act). Abstract There are a variety of potential uses for interactive spatial sound in human-machine interfaces, but tremendous computational costs have made most of these applications impractical. Recently, however, single-chip digital signal processors (DSPs) have made real-time spatial audio an affordable possibility for many workstations. This paper describes a spa-tialization technique based on empirically derived FIR filters. The fundamental performance and quality limits for this technique are discussed as well the minimum bandwidth required for the associated audio channels. It is shown that current single-chip DSPs may be expected to spatialize several sources to different positions in real time. Techniques for improving spatial audio quality and performance are described. As an example application, the spatial sound system of an all-acoustic computer interface is described. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Allen, J.B. & Berkley, D.A. </author> <title> (1979) An image method for efficiently simulating small-room acoustics, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 65, </volume> <pages> 943-950. </pages>
Reference-contexts: However, experiments for Mercator have shown that an echo from a single wall, computed at modest cost, allows reliable front-back differentiation for even our most difficult test subjects. For descriptions of methods for simulating small-room acoustics, see Kendall, Martins, et al (1989) and Allen & Berkley <ref> (1979) </ref>. Control of the perceived distance of a virtual sound source is a difficult problem in spatial sound systems. For a familiar sound, a primary cue for distance perception is intensity (Gardner, 1968, Laws, 1973).
Reference: 2. <author> Asano, F., Suzuki, Y., Toshio, S. </author> <year> (1990), </year> <title> Role of spectral cues in median plane localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 88, </volume> <pages> 159-168. </pages>
Reference: 3. <author> Blauert, J. </author> <title> (1983) Spatial Hearing: The Psychophysics of Human Sound Localization, </title> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The delay varies as a sinusoid with azimuth but is also dependent on both the frequency of the sound and the distance of the source <ref> (Blauert, 1983) </ref>. IDT manifests itself as a phase difference for signals below 1.6kHz and as an envelope delay for higher frequency sounds. Other cues known to be of importance are head motion (Thurlow & Runge, 1967), vision (Thomas, 1940), early echo response (Moore, 1990), and reverberation (Gardner, 1968). <p> For the second estimate, we reasoned that most of the spectral cues of the HRTF are above 3.5kHz. This assumption was based on the fact below this frequency, the HRTF has little fine detail (Gardner, 1969). The spectral resolution of the inner ear is 1/3 octave <ref> (Blauert, 1983) </ref>. For an FIR to match this spectral resolution at 3.5kHz, it should have a duration no shorter than 1.10ms. This is a rough figure, however, because the cutoff frequency for fine details is dependent on ones definition of fine detail.
Reference: 4. <author> Buxton, W., Gaver, W.W. & Bly, S. </author> <title> (1991) The Use of Non-Speech Audio at the Interface, Tutorial No. </title> <booktitle> 8, ACM Conference on Human Factors in Computer Systems, CHI 91, </booktitle> <publisher> ACM Press: </publisher> <address> New York, NY. </address>
Reference: 5. <author> Cherry, </author> <title> E.C. (1953) Some experiments on the recognition of speech with one or two ears, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 22, </volume> <pages> 61-62. </pages>
Reference-contexts: Along with greater realism, binaural sound conveys spatial information about each sound source to the listener. Furthermore, when sounds are spatially separated, a listener can easily distinguish different sources, and focus on those sources which are of interest while ignoring others <ref> (Cherry, 1953) </ref>. If sounds can be recorded in this manner, an obvious next 1. In strict terms, binaural and stereo mean exactly the same thingtwo channels of sound.
Reference: 6. <author> Gardner, </author> <title> M.B. (1968) Distance estimation of or apparent -oriented speech signals in anechoic space, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 45, </volume> <pages> 47-53. </pages>
Reference-contexts: IDT manifests itself as a phase difference for signals below 1.6kHz and as an envelope delay for higher frequency sounds. Other cues known to be of importance are head motion (Thurlow & Runge, 1967), vision (Thomas, 1940), early echo response (Moore, 1990), and reverberation <ref> (Gardner, 1968) </ref>. We tend to move our heads to get a better sense of a sounds direction (Thurlow, et al, 1967). This closed-loop cue can be added to a spatial sound system through the use of a head-tracking device. <p> The azimuth of 90 degrees was used because listeners seemed to be most sensitive to changes of elevation in the 5. It has also been found that product currently under development at Crystal River Engineering is expected to use a 74 point filter <ref> (1.68ms) </ref> at 44.1kHz. regions to the immediate left and right. The second arc, B, had a constant elevation of 0 degrees with azimuth changing from 90 degrees to 180 degrees (behind the head) in steps of 15 degrees over a period of 6 seconds 6 . <p> For descriptions of methods for simulating small-room acoustics, see Kendall, Martins, et al (1989) and Allen & Berkley (1979). Control of the perceived distance of a virtual sound source is a difficult problem in spatial sound systems. For a familiar sound, a primary cue for distance perception is intensity <ref> (Gardner, 1968, Laws, 1973) </ref>. At low frequencies (below 1kHz) the intensity of a sound varies inversely with the square of the distance from its source. At higher frequencies (above 3kHz), dispersion causes an inverse cubic variation.
Reference: 7. <author> Gardner, </author> <title> M.B. (1973) Some monaural and binaural facets of median plane localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 54, </volume> <pages> 1489-1495. </pages>
Reference-contexts: The first had a length of 128 points, which contained the full duration of the original fil-tersonly silence was removed. These 128 point filters were meant as a control set. The second set was truncated to 32 points (1.45ms). A third set was truncated to only 16 points <ref> (0.73ms) </ref>. To produce these filter sets, the original 50kHz filters were resampled at 22.05kHz, giving a length of 226 samples.
Reference: 8. <author> Gaver, </author> <title> W.W. (1986) Auditory icons: Using sound in computer interfaces, </title> <journal> Human-Computer Interaction, </journal> <volume> 2, </volume> <pages> 167-177. </pages>
Reference-contexts: Section II: An Example Application Spatial Sound for a Computer Interface The Mercator interface, which is currently under development, is designed to map X Window System applications to an acoustic virtual world in which interface objects, such as pull-down menus and buttons, are represented by sound sources called auditory icons <ref> (Gaver, 1986) </ref>. Just as the interface objects of a graphical user interface are organized by their positions on a screen or in a window, the icons of the Mercator interface are organized by their positions in a virtual acoustic space. <p> A higher level of organization is provided by the notion of virtual rooms, with are logical division of the users workspace based on the Xerox PARC Rooms interface <ref> (Henderson & Card, 1986) </ref>. Much of the work performed by Mercator involves tracking and interpreting the actions of application programs (Mynatt & Edwards, 1992), and is outside of the scope of this paper. Here, we describe only the lowest levels of the Mercator spa tial sound interface.
Reference: 9. <author> Gaver, </author> <title> W.W. (1989) The sonicfinder: An interface that uses auditory icons, </title> <journal> Human-Computer Interaction, </journal> <volume> 4, </volume> <pages> 67-94. </pages>
Reference-contexts: However, experiments for Mercator have shown that an echo from a single wall, computed at modest cost, allows reliable front-back differentiation for even our most difficult test subjects. For descriptions of methods for simulating small-room acoustics, see Kendall, Martins, et al <ref> (1989) </ref> and Allen & Berkley (1979). Control of the perceived distance of a virtual sound source is a difficult problem in spatial sound systems. For a familiar sound, a primary cue for distance perception is intensity (Gardner, 1968, Laws, 1973).
Reference: 10. <author> Henderson, D.A. & Card, S.K. </author> <year> (1986) </year> <month> Rooms: </month> <title> The use of multiple virtual workspaces to reduce space contention in a window-based graphical used interface, </title> <journal> ACM 0 Transactions on Graphics, </journal> <month> July </month> <year> 1986, </year> <pages> 211-243. </pages>
Reference: 11. <author> Kendall, G.S., Martins, W.L. & Decker, </author> <title> S.L. (1989) Spatial reverberation: discussion and demonstration, </title> <booktitle> Current Directions in Computer Music Research, </booktitle> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference: 12. <author> Laws, P. </author> <title> (1973) Entfernungshren und das Problem der Im-Kopf-Lokalisierheit von Hrereignissen [Auditory distance perception and the problem of in-head localization of sound images], </title> <journal> Acustica, </journal> <volume> 29, </volume> <pages> 243-259. </pages>
Reference: 13. <author> Liang, J., Shaw, C. & Green, M. </author> <booktitle> (1991) On temporal-spatial realism in the virtual reality environment, Proceedings of the ACM Symposium on User Interface Software and Technology, UIST 91, </booktitle> <pages> 19-25. </pages>
Reference: 14. <author> Loomis, J.M, Hebert, C. & Cicinelli, J.G. </author> <title> (1990) Active localization of virtual sounds, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 88, </volume> <pages> 1757-1764. </pages>
Reference: 15. <author> Ludwig, L.F., Pincever, N. & Cohen, M. </author> <title> (1990) Extending the notion of a window system to audio, </title> <booktitle> Computer, </booktitle> <month> Aug. </month> <year> 1990, </year> <pages> 66-72. </pages>
Reference: 16. <author> Ludwig, L.F. & Cohen, M. </author> <title> (1991) Multidimensional audio window management, </title> <journal> International J. Man-Machine Studies, </journal> <volume> 34(3), </volume> <pages> 319-336 </pages>
Reference: 17. <author> Mills, A.W. </author> <title> (1972) Auditory localization, </title> <booktitle> Foundations of Modern Auditory Theory, </booktitle> <volume> Vol. </volume> <publisher> II/8, Academic: </publisher> <address> New York, NY. </address>
Reference: 18. <author> Moore, F.R. </author> <title> (1990) Elements of Computer Music, </title> <publisher> Pren-tice Hall: </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: 19. <institution> Motorola (1990) DSP56000/DSP56001 Digital Signal Processor Users Manual (Rev. </institution> <month> 2), </month> <title> Motorola Literature Distribution: </title> <address> Phoenix, AZ. </address>
Reference-contexts: IDT manifests itself as a phase difference for signals below 1.6kHz and as an envelope delay for higher frequency sounds. Other cues known to be of importance are head motion (Thurlow & Runge, 1967), vision (Thomas, 1940), early echo response <ref> (Moore, 1990) </ref>, and reverberation (Gardner, 1968). We tend to move our heads to get a better sense of a sounds direction (Thurlow, et al, 1967). This closed-loop cue can be added to a spatial sound system through the use of a head-tracking device. <p> This speaker transfer function remains as an artifact. Other possible artifacts from the recording process are echoes from the gimbals used to support the moveable speakers and a distortion of ear canal response due to the fact that the probe cannot actually touch the eardrum <ref> (Asano, et al, 1990) </ref>. The resulting HRTFs are transformed back to the time domain to yield a set of finite impulse response (FIR) filters. For each position (azimuth, elevation), there is a pair of fil-tersone for each ear. <p> This problem is not necessarily due to some failing of the spatial sound system, because front-back confusion can occur with real sound sources as well. However, as HRTF accuracy is degraded, rates of front-back reversal increase <ref> (Asano, et al 1990) </ref>. Another common shortcoming is lack of externalization. As a result, sound may appear to emanate from points inside the head. Externalization is lost when signals reaching the ears are not adequately consistent with those from external sources (Plenge, 1974). <p> It should be noted that this is significantly higher than the reversal rates observed with a sample rate of 50kHz using the full bandwidth <ref> (Asano, et al, 1990) </ref>, or with a sample rate of 50kHz using a 14kHz bandwidth (Wightman & Kistler, 1989b). (this could be normal front-back confusion). There was one case of loss of localization at higher elevations along arc A. For the other eight subjects, however, monotonicity and localization were maintained. <p> It was also found that a more realistic effect could be achieved by adding a small amount of reverberation to the original signal before spatial-ization. Generating realistic reverberation is not a simple task. Moore <ref> (1990) </ref> provides a summary of several techniques. Head tracking devices can greatly improve the effectiveness of a spatial sound system. <p> Moore (1990) provides a summary of several techniques. Head tracking devices can greatly improve the effectiveness of a spatial sound system. At least one experiment has shown that with head tracking, a full implementation of the HRTF is not necessary for blind navigation in an acoustic virtual environment <ref> (Loomis, et al, 1990) </ref>. <p> Auditory icons are short in duration and formed by passing a base sound though a series of linear and nonlinear functions such as filters, distortion generators, and pitch shifters. The modifying functions provide information to the user by reecting the state of the interface object which the icon represents <ref> (Ludwig, et al, 1990) </ref>. The base sounds are typically sampled from natural sources, stored on disk until needed, and cached in memory while in use. Auditory icons are spatialized for organizational and navigational purposes. <p> The system is diagrammed in figure 1. (Only those components related to sound generation are show.) A sample rate of 32kHz has been chosen for Mercators spatial sound system to insure adequate spatial sound quality. The S56-X uses a DSP56001 <ref> (Motorola, 1990) </ref> with a clock rate of 27MHz. Thus, to generate binaural output, the DSP is 8. Although the DECtalk is expensive, it is already widely installed in the visually-impaired community. 9. Many installed Unix workstations include CODECs. 10. <p> Threads provide the same multiprocessing features as heavyweight operating system processes, but at a lower overhead cost. All of the threads in a given address space are considered to be a single process by the operating system. The SPARCstations operating system supports threads via its LWP library <ref> (Sun, 1990) </ref>. Many computers in this class provide similar support for multi-threaded programming. The highest priority task is to keep the DSP supplied with a stream of audio samples. Once the DSP has issued a data demand interrupt, a new audio packet must be completely transmitted within 128ms.
Reference: 20. <author> Mynatt, E. & Edwards, </author> <title> W.K. (1992) The Mercator environment: A nonvisual interface to X Windows and Unix workstations, </title> <booktitle> Proceedings of the ACM Symposium on User Interface Software and Technology, UIST 92. </booktitle>
Reference: 21. <author> Plenge, G. </author> <title> (1974) On the differences between localization and lateralization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 56, </volume> <pages> 944-951. </pages>
Reference: 22. <author> Lord Rayleigh [Strutt, J.W.] </author> <title> (1907) On our perception of sound direction, </title> <journal> Phil. Mag., </journal> <volume> 13, </volume> <pages> 214-232. </pages>
Reference: 23. <author> Searle, C.L., Braida, L.D., Davis, M.F. & Colburn, H.S. </author> <title> (1976) Model for auditory localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 60, </volume> <pages> 1164-1175. </pages>
Reference: 24. <institution> Sun Microsystems (1990) SunOS Programming Utilities and Libraries, Chap. </institution> <month> 2, </month> <title> Lightweight Processes, Sun Microsystems: </title> <address> Palo Alto, CA. </address>
Reference: 25. <author> Thomas, </author> <title> G.J (1940) Experimental study of the inuence of vision on sound localization, </title> <journal> J. Exper. Psych., </journal> <volume> 28, </volume> <pages> 163-177. </pages>
Reference: 26. <author> Thurlow, W.R. </author> & <title> Runge, P.S. (1967) Effect of induced head movements on localization of direction of sounds, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 42, </volume> <pages> 480-488. </pages>
Reference: 27. <author> Thurlow, R.W., Mangels, J.W. </author> & <title> Runge P.S. (1967) Head movements during sound localization, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 42, </volume> <pages> 489-493. </pages>
Reference: 28. <author> Wenzel, </author> <title> E.M. (1992) Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, </volume> <pages> 80-107. </pages>
Reference: 29. <author> Wenzel, E.M., Wightman, F.L. & Foster S.H. </author> <title> (1988) A virtual display system for conveying three-dimensional acoustic information, </title> <booktitle> Proceedings of the Human Factors Society - 32nd Annual Meeting, </booktitle> <pages> 86-90. </pages>
Reference: 30. <author> Wightman, F.L. & Kistler, </author> <title> D.J. (1989a) Headphone simulation of free-field listening I: stimulus synthesis, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85, </volume> <pages> 858-867. </pages>
Reference: 31. <author> Wightman, F.L. & Kistler, </author> <title> D.J. (1989b) Headphone simulation of free-field listening II: psychophysical validation, </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85, </volume> <pages> 868-878. </pages>
References-found: 31

