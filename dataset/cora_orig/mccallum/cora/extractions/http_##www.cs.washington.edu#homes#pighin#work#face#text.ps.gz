URL: http://www.cs.washington.edu/homes/pighin/work/face/text.ps.gz
Refering-URL: http://www.cs.washington.edu/research/projects/grail2/www/pub/abstracts.html
Root-URL: 
Title: Realistic Facial Animation Using Image-Based 3D Morphing intimately scrutinized piece of territory in existence, examined
Author: Frederic Pighin Joel Auslander Dani Lischinski David H. Salesin Richard Szeliski 
Note: is the most  
Affiliation: University of Washington Microsoft Research  
Date: May 9, 1997  
Pubnum: Technical report UW-CSE-97-01-03  
Abstract: We present new techniques for creating a realistic textured 3D facial model from several photographs of a human subject and for performing facial animation by morphing between models corresponding to different facial expressions. Starting from several uncalibrated views of an individual, we employ a user assisted technique to recover the camera poses corresponding to the views, as well as the 3D coordinates of a sparse set of chosen locations on the individuals face. A scattered data interpolation technique is then used to deform a generic face mesh into a 3D model of the individual's face. Having recovered the camera poses and the facial geometry, we extract from the input images a texture map for the model. An optical flow technique is used for improving the registration of the input images in texture space. This process is repeated for several facial expressions of a particular individual. To animate between these facial expressions we use 3D shape morphing between the corresponding facial models, while at the same time blending the corresponding textures. Using our technique we have been able to generate highly realistic facial models and natural looking transitions between different expressions. There is no landscape that we know as well as the human face. The twenty-five-odd square inches containing the features
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wallace. </author> <title> Automatic creation of 3D facial models. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(5) </volume> <pages> 16-22, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: One consequence of using these grids, however, is that the images used to construct geometry can no longer be used as a valid texture maps for the subject. More recently, several methods have been proposed for modeling the face photogrammetrically without the use 3 of grids <ref> [19, 1, 16] </ref>. These modeling methods are quite similar in their basic concept to the modeling technique described in this paper. <p> Previous authors used weighting function that satisfy only a subset of these requirements. For example, Kurihara and Arai [19] use positional uncertainty as their weighting function, but they do not account for self-occlusion. Akimoto et al. <ref> [1] </ref> as well as Ip and Yin [16] blend the images smoothly, but address neither self-occlusion nor positional certainty. De-bevec et al. [7], who describe a view-dependent texture mapping technique for modeling and rendering buildings from photographs, do address occlusion but do not account for positional certainty.
Reference: [2] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <editor> In Edwin E. Catmull, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '92 Proceedings), </booktitle> <volume> volume 26, </volume> <pages> pages 35-42, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, for sheer 2 realism, one of the most effective approaches to date has been the use of 2D morphing between photographic images <ref> [2] </ref>. Indeed, some quite remarkable results have been achieved in this way | most notably, perhaps, the Michael Jackson video produced by PDI in which very different-looking actors are seemingly transformed into one another as they move about. <p> Thus, a satisfactory morphing sequence can be obtained by simple linear interpolation between the geometric coordinates of corresponding vertices in each of the two face meshes. In order to morph between a pair of textures we also need correspondences between the two texture images <ref> [2] </ref>. These correspondences are given to us by means of the texture coordinates of the facial mesh vertices in each of the two meshes.
Reference: [3] <author> J. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Second European Conference on Computer Vision (ECCV'92), </booktitle> <pages> pages 237-252, </pages> <address> Santa Margherita Liguere, Italy, May 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To improve the quality of the composite textures, we can locally warp each component texture (and weight) map before blending. To compute good values for the warping (i.e., the displacement map), we use a classical optical flow technique <ref> [22, 3] </ref>, as described in Appendix A. <p> This leads to smoother changes in the flow estimates near the borders of the visibility map. Since the initial misregistration may be large, we use a coarse-to-fine algorithm based on an image pyramid to improve the convergence <ref> [3] </ref>. Figures 4a-b show two texture maps, in cylindrical coordinates, that have been extracted from the original photos through projection onto the geometric model. Because the geometry is imperfect, the extraction process is inexact, which leads to local misregistrations between the two texture maps.
Reference: [4] <author> Philippe Bergeron and Pierre Lachapelle. </author> <title> Controlling facial expressions and body movements in the computer-generated animated short "tony de peltrie". </title> <booktitle> In SIGGRAPH '85 Advanced Computer Animation seminar notes. </booktitle> <month> July </month> <year> 1985. </year>
Reference-contexts: A number of approaches have been developed to model and animate realistic facial expressions in three dimensions, including simple geometric interpolation between digitized [25] or laser-scanned models; performance-based animation, in which measurements from real human actors are used to drive synthetic characters <ref> [4, 36] </ref>; and various forms of physically-based animation, in which musculo-skeletal controls modeled with various degrees of realism are used to create facial animations [34, 31]. However, for sheer 2 realism, one of the most effective approaches to date has been the use of 2D morphing between photographic images [2].
Reference: [5] <author> David T. Chen, Andrei State, and David Banks. </author> <title> Interactive shape metamorphosis. </title> <editor> In Pat Hanrahan and Jim Winget, editors, </editor> <booktitle> 1995 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 43-44. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> April </month> <year> 1995. </year> <note> ISBN 0-89791-736-7. </note>
Reference-contexts: The texture extraction stage proceeds as follows: 8 1. A mapping between 3D coordinates on the facial mesh and a 2D texture space is defined using a cylindrical projection, similarly to several previous authors (e.g., <ref> [5, 21, 19] </ref>). 2.
Reference: [6] <author> C. S. Choi, H. Harashima, and T. Takebe. </author> <title> Highly accurate estimation of head motion and facial action information on knowledge-based image coding. </title> <address> IEICEJ, PRU90-68:1-8, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Indeed, attempts to model and animate realistic human faces date back to the early 70's [25], with many dozens of research papers published since. The applications of facial animation include such diverse fields as character animation for films and advertising, computer games [15], video teleconferencing <ref> [6] </ref>, user-interface agents and avatars [32], and medical facial surgery planning [33, 18]. Yet it is fair to say that no perfectly realistic facial animation has ever been generated by computer: no "facial animation Turing test" has ever been passed.
Reference: [7] <author> Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIGGRAPH 96 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year> <title> held in New Orleans, </title> <address> Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: For example, Kurihara and Arai [19] use positional uncertainty as their weighting function, but they do not account for self-occlusion. Akimoto et al. [1] as well as Ip and Yin [16] blend the images smoothly, but address neither self-occlusion nor positional certainty. De-bevec et al. <ref> [7] </ref>, who describe a view-dependent texture mapping technique for modeling and rendering buildings from photographs, do address occlusion but do not account for positional certainty. <p> Once the viewpoint has been specified, we increase the relative weights of those texture maps whose corresponding camera poses are closer to the new viewpoint. Blending the individual texture maps together in this fashion results in a view-dependent texture map <ref> [7] </ref>.
Reference: [8] <author> Eben Ostby, </author> <title> Pixar Animation Studios. </title> <type> Personal communication, </type> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: There are several factors that make realistic facial animation so elusive. First, the human face is an extremely complex geometric form. For example, the human face models used in Pixar's Toy Story were modeled using several thousand control points each <ref> [8] </ref>. Moreover, the face exhibits countless tiny creases and wrinkles, as well as subtle variations in color and texture | all of which are crucial for our comprehension and appreciation of facial expressions.
Reference: [9] <author> P. Ekman and W. V. Friesen. </author> <title> Manual for the Facial Action Coding System. </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <address> Palo Alto, California, </address> <year> 1978. </year>
Reference-contexts: There are several potential ways to attack this problem. One would be to adopt and adapt an action unit-based system such as the Facial Action Coding System (FACS) <ref> [9] </ref>. Another possibility would be to apply modal analysis (principal component analysis) techniques to describe facial expression changes using a small number of motions [20]. Finding natural control parameters to facilitate animation, and developing realistic looking temporal profiles for such movements, is also challenging.
Reference: [10] <author> Gary Faigin. </author> <title> The Artist's Complete Guide to Facial Animation. </title> <publisher> Watson-Guptill Publications, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [11] <author> O. Faugeras. </author> <title> Three-dimensional computer vision: A geometric viewpoint. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: The angular rotation can be converted to a true incremental rotation matrix using Rodriguez's formula <ref> [11] </ref>: R ( ^ n; ) = I + sin X ( ^ n) + (1 cos )X 2 ( ^ n); (6) where = kv k and ^ n = v =, and the new rotation matrix computed using R k R ( ^ n k ; k )R k <p> This process can be repeated until we are satisfied with the shape. correspondences have been specified. Since the pose of each camera is known, we can easily compute and display the epipolar lines corresponding to a point selected in one image <ref> [11] </ref>. This could be used to facilitate the process of manually establishing correspondences.
Reference: [12] <author> P. Fua and Y. G. Leclerc. </author> <title> Registration without correspondences. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), </booktitle> <pages> pages 121-128, </pages> <address> Seattle, Washington, June 1994. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: In principle, small changes in geometry correspond to motions in the (u; v) coordinate space, so it should be possible to incrementally improve our 3D vertex positions by minimizing misregistration errors. This idea is similar to model-based stereo matching 15 <ref> [12] </ref>, and will use optimization techniques similar to those used in through--the-lens camera control [13].
Reference: [13] <author> M. Gleicher and A. Witkin. </author> <title> Through-the-lens camera control. </title> <journal> Computer Graphics (SIGGRAPH'92), </journal> <volume> 26(2) </volume> <pages> 331-340, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This idea is similar to model-based stereo matching 15 [12], and will use optimization techniques similar to those used in through--the-lens camera control <ref> [13] </ref>. In addition to refining the positions of the face vertices, misregistrations error could also be used to guide a localized subdivision step, which would enable us to refine the generic face model to better preserve fine facial features.
Reference: [14] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: Unlike this previous algorithm, which uses the Levenberg-Marquardt algorithm to perform a complete iterative minimization over all of the unknowns simultaneously, we break the problem down into a series of linear least squares problems which can be solved using very simple and numerically stable techniques <ref> [14, 28] </ref>.
Reference: [15] <institution> Bright Star Technologies Inc. Beginning Reading Software. Sierra OnLine, Inc., </institution> <year> 1993. </year>
Reference-contexts: Indeed, attempts to model and animate realistic human faces date back to the early 70's [25], with many dozens of research papers published since. The applications of facial animation include such diverse fields as character animation for films and advertising, computer games <ref> [15] </ref>, video teleconferencing [6], user-interface agents and avatars [32], and medical facial surgery planning [33, 18]. Yet it is fair to say that no perfectly realistic facial animation has ever been generated by computer: no "facial animation Turing test" has ever been passed.
Reference: [16] <author> Horace H. S. Ip and Lijun Yin. </author> <title> Constructing a 3D individualized head model from two orthogonal views. </title> <journal> The Visual Computer, </journal> <volume> 12 </volume> <pages> 254-266, </pages> <year> 1996. </year>
Reference-contexts: One consequence of using these grids, however, is that the images used to construct geometry can no longer be used as a valid texture maps for the subject. More recently, several methods have been proposed for modeling the face photogrammetrically without the use 3 of grids <ref> [19, 1, 16] </ref>. These modeling methods are quite similar in their basic concept to the modeling technique described in this paper. <p> Previous authors used weighting function that satisfy only a subset of these requirements. For example, Kurihara and Arai [19] use positional uncertainty as their weighting function, but they do not account for self-occlusion. Akimoto et al. [1] as well as Ip and Yin <ref> [16] </ref> blend the images smoothly, but address neither self-occlusion nor positional certainty. De-bevec et al. [7], who describe a view-dependent texture mapping technique for modeling and rendering buildings from photographs, do address occlusion but do not account for positional certainty.
Reference: [17] <author> James R. Kent, Wayne E. Carlson, and Richard E. Parent. </author> <title> Shape transformation for polyhedral objects. </title> <editor> In Edwin E. Catmull, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '92 Proceedings), </booktitle> <volume> volume 26, </volume> <pages> pages 47-54, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Obtain a texture map for the intermediate expression by image mor phing between the corresponding texture maps. 3. Render the resulting texture-mapped facial model from the desired viewpoint. The problem of morphing between general polygonal meshes in 3D is a difficult one <ref> [17] </ref>. In our case, however, the topology of the two key face meshes is identical. Thus, the correspondences between vertices in the key meshes need not be computed. Furthermore, for most individuals, and most pairs of facial expressions, we can assume that the geometry of the meshes is fairly similar.
Reference: [18] <author> R. M. Koch, M. H. Gross, F. R. Carls, D. F. von Buren, G. Fankhauser, and Y. Parish. </author> <title> Simulating facial surgery using finite element methods. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIGGRAPH 96 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 421-428. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year> <title> held in New Orleans, </title> <address> Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: The applications of facial animation include such diverse fields as character animation for films and advertising, computer games [15], video teleconferencing [6], user-interface agents and avatars [32], and medical facial surgery planning <ref> [33, 18] </ref>. Yet it is fair to say that no perfectly realistic facial animation has ever been generated by computer: no "facial animation Turing test" has ever been passed. There are several factors that make realistic facial animation so elusive. First, the human face is an extremely complex geometric form.
Reference: [19] <author> Tsuneya Kurihara and Kiyoshi Arai. </author> <title> A transformation method for modeling and animation of the human face from photographs. </title> <editor> In Na-dia Magnenat Thalmann and Daniel Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag, </publisher> <address> Tokyo, </address> <year> 1991. </year> <month> 18 </month>
Reference-contexts: One consequence of using these grids, however, is that the images used to construct geometry can no longer be used as a valid texture maps for the subject. More recently, several methods have been proposed for modeling the face photogrammetrically without the use 3 of grids <ref> [19, 1, 16] </ref>. These modeling methods are quite similar in their basic concept to the modeling technique described in this paper. <p> The texture extraction stage proceeds as follows: 8 1. A mapping between 3D coordinates on the facial mesh and a 2D texture space is defined using a cylindrical projection, similarly to several previous authors (e.g., <ref> [5, 21, 19] </ref>). 2. <p> is not visible in the j-th image, the weight w j (u; v) should be zero. * The weights in each weight map should vary smoothly, in order to ensure a seamless blend between the input images. * The weight w j (u; v) should depend on the "positional certainty" <ref> [19] </ref> of the corresponding point p in the j-th image. <p> Previous authors used weighting function that satisfy only a subset of these requirements. For example, Kurihara and Arai <ref> [19] </ref> use positional uncertainty as their weighting function, but they do not account for self-occlusion. Akimoto et al. [1] as well as Ip and Yin [16] blend the images smoothly, but address neither self-occlusion nor positional certainty.
Reference: [20] <author> A. Lanitis, C. J. Taylor, and T. F. Cootes. </author> <title> A unified approach for coding and interpreting face images. </title> <booktitle> In Fifth International Conference on Computer Vision (ICCV'95), </booktitle> <pages> pages 368-373, </pages> <address> Cambridge, Mas-sachusetts, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Our ultimate goal, as far as the modeling stage is concerned, is to construct a fully automated modeling system, which would automatically find features and correspondences with minimal user intervention. This is a challenging problem indeed, but recent results on 2D face modeling in computer vision <ref> [20] </ref> give us cause for hope. If we are successful in our quest, our techniques could then be applied to a wider range of 3D image-based reconstruction problems. Creating photorealistic models from images is just one of the challenges that stand before us. <p> One would be to adopt and adapt an action unit-based system such as the Facial Action Coding System (FACS) [9]. Another possibility would be to apply modal analysis (principal component analysis) techniques to describe facial expression changes using a small number of motions <ref> [20] </ref>. Finding natural control parameters to facilitate animation, and developing realistic looking temporal profiles for such movements, is also challenging.
Reference: [21] <author> Yuencheng Lee, Demetri Terzopoulos, and Keith Waters. </author> <title> Realistic modeling for facial animation. </title> <editor> In Robert Cook, editor, </editor> <booktitle> SIGGRAPH 95 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 55-62. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1995. </year> <title> held in Los Angeles, </title> <address> Cali-fornia, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: The texture extraction stage proceeds as follows: 8 1. A mapping between 3D coordinates on the facial mesh and a 2D texture space is defined using a cylindrical projection, similarly to several previous authors (e.g., <ref> [5, 21, 19] </ref>). 2.
Reference: [22] <author> B. D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application in stereo vision. </title> <booktitle> In Seventh International Joint Conference on Artificial Intelligence (IJCAI-81), </booktitle> <pages> pages 674-679, </pages> <address> Van-couver, </address> <year> 1981. </year>
Reference-contexts: To improve the quality of the composite textures, we can locally warp each component texture (and weight) map before blending. To compute good values for the warping (i.e., the displacement map), we use a classical optical flow technique <ref> [22, 3] </ref>, as described in Appendix A.
Reference: [23] <author> Jackie Neider, Tom Davis, and Mason Woo. </author> <title> OpengGL Programming Guide. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: In this case, instead of morphing between the two key texture maps, we simply render the intermediate facial mesh twice, once for each key expression, using the appropriate texture map. The resulting images are then blended together. The blending can be performed rapidly using the accumulation buffer in OpenGL <ref> [23] </ref>. 5 Results In order to put our technique to the test, we photographed a female and a male model in a variety of facial expressions. The photography was performed using five cameras simultaneously. The cameras were not calibrated in any particular way and the lenses had different focal lengths.
Reference: [24] <author> Gregory M. Nielson. </author> <title> Scattered data modeling. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(1) </volume> <pages> 60-70, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We construct a smooth interpolation function to create a displacement map which describes, the 3D displacement between the original point positions and the new adapted positions for every vertex in the original generic face mesh. Constructing such an interpolation function is a standard problem in scattered data interpolation <ref> [24] </ref>. Given a set of known displacements u i = p i p i at vertices i, we construct a function that gives us the displacement u j for every unconstrainted vertex j. There are several general approaches to solving this problem. <p> Again, several choices exist for how to construct the interpolating function <ref> [24] </ref>. We use a method based on radial basis functions, i.e., functions of the form, f (p) = i where the (r) are the radially symmetric basis functions (a more general form of this interpolant also adds some low-order polynomial terms to model global, e.g., affine, deformations). <p> This matrix computation and inversion can be performed once (it depends only on the original configuration of the vertices), unless more constraints are selected during a subsequent correspondence-based shape refinement stage. 7 Many different functions for (r) have been proposed <ref> [24] </ref>. After exper-imenting with a number of functions, we have chosen to use (r) = e r=64 , where the diameter of the face model measured from side to side is on the order of 6 units.
Reference: [25] <author> Frederic I. Parke. </author> <title> Computer generated animation of faces. </title> <booktitle> Proc. ACM annual conf., </booktitle> <month> August </month> <year> 1972. </year>
Reference-contexts: 1 Introduction Realistic facial animation is one of the most fundamental problems in computer graphics | and one of the most difficult. Indeed, attempts to model and animate realistic human faces date back to the early 70's <ref> [25] </ref>, with many dozens of research papers published since. The applications of facial animation include such diverse fields as character animation for films and advertising, computer games [15], video teleconferencing [6], user-interface agents and avatars [32], and medical facial surgery planning [33, 18]. <p> For facial expressions, the slightest deviation from truth is something any human will immediately detect. A number of approaches have been developed to model and animate realistic facial expressions in three dimensions, including simple geometric interpolation between digitized <ref> [25] </ref> or laser-scanned models; performance-based animation, in which measurements from real human actors are used to drive synthetic characters [4, 36]; and various forms of physically-based animation, in which musculo-skeletal controls modeled with various degrees of realism are used to create facial animations [34, 31]. <p> Our approach utilizes a photogrammetric technique, in which images are used to create precise geometry. The earliest such techniques applied to facial modeling and animation employed grids that were drawn directly on the human subject's face <ref> [25, 26] </ref>. One consequence of using these grids, however, is that the images used to construct geometry can no longer be used as a valid texture maps for the subject.
Reference: [26] <author> Frederic I. Parke. </author> <title> A Parametric Model for Human Faces. </title> <type> Phd thesis, </type> <institution> University of Utah, </institution> <address> Salt Lake City, Utah, </address> <month> December </month> <year> 1974. </year> <month> UTEC-CSc-75-047. </month>
Reference-contexts: Our approach utilizes a photogrammetric technique, in which images are used to create precise geometry. The earliest such techniques applied to facial modeling and animation employed grids that were drawn directly on the human subject's face <ref> [25, 26] </ref>. One consequence of using these grids, however, is that the images used to construct geometry can no longer be used as a valid texture maps for the subject.
Reference: [27] <author> Frederic I. Parke and Keith Waters. </author> <title> Computer Facial Animation. A K Peters, </title> <type> Wellesley, </type> <institution> Massachusetts, </institution> <year> 1996. </year>
Reference-contexts: For our work, we use a generic face model made available by Keith Waters <ref> [27] </ref> (Figure 2b). As inputs to this stage, we take several images of the face from different viewpoints (the exact camera arrangement is described in Section 5).
Reference: [28] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: Unlike this previous algorithm, which uses the Levenberg-Marquardt algorithm to perform a complete iterative minimization over all of the unknowns simultaneously, we break the problem down into a series of linear least squares problems which can be solved using very simple and numerically stable techniques <ref> [14, 28] </ref>.
Reference: [29] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View morphing. </title> <editor> In Holly Rush-meier, editor, </editor> <booktitle> SIGGRAPH 96 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 21-30. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year> <title> held in New Orleans, </title> <address> Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference: [30] <author> R. Szeliski and S. B. Kang. </author> <title> Recovering 3D shape and motion from image streams using nonlinear least squares. </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 5(1) </volume> <pages> 10-28, </pages> <month> March </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: Our formulation is based on the non-linear least squares structure from motion algorithm presented in <ref> [30] </ref>. Unlike this previous algorithm, which uses the Levenberg-Marquardt algorithm to perform a complete iterative minimization over all of the unknowns simultaneously, we break the problem down into a series of linear least squares problems which can be solved using very simple and numerically stable techniques [14, 28]. <p> Instead of using the above equation, we reformulate the problem to es timate inverse distances to the object instead <ref> [30] </ref>. Let k = 1=t k z be this inverse distance and s k = f k =t k z be a world-to-image scale factor.
Reference: [31] <author> D. Terzopoulos and K. Waters. </author> <title> Physically-based facial modeling, anal-ysis, and animation. </title> <journal> J. of Visualization and Computer Animation, </journal> <volume> 1(4) </volume> <pages> 73-80, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: dimensions, including simple geometric interpolation between digitized [25] or laser-scanned models; performance-based animation, in which measurements from real human actors are used to drive synthetic characters [4, 36]; and various forms of physically-based animation, in which musculo-skeletal controls modeled with various degrees of realism are used to create facial animations <ref> [34, 31] </ref>. However, for sheer 2 realism, one of the most effective approaches to date has been the use of 2D morphing between photographic images [2].
Reference: [32] <author> K. R. Thorisson. </author> <title> Gandalf: An embodied humanoid capable of real-time multimodal dialogue with people. </title> <booktitle> In First ACM International Conference on Autonomous Agents, 1997. </booktitle> <address> Mariott Hotel, Marina del Rey, California, </address> <month> February 5-8. </month>
Reference-contexts: The applications of facial animation include such diverse fields as character animation for films and advertising, computer games [15], video teleconferencing [6], user-interface agents and avatars <ref> [32] </ref>, and medical facial surgery planning [33, 18]. Yet it is fair to say that no perfectly realistic facial animation has ever been generated by computer: no "facial animation Turing test" has ever been passed. There are several factors that make realistic facial animation so elusive.
Reference: [33] <author> M. W. Vannier, J. F. Marsh, and J. O. Warren. </author> <title> Three-dimentional computer graphics for craniofacial surgical planning and evaluation. </title> <journal> Computer Graphics, </journal> <volume> 17(3) </volume> <pages> 263-273, </pages> <year> 1983. </year>
Reference-contexts: The applications of facial animation include such diverse fields as character animation for films and advertising, computer games [15], video teleconferencing [6], user-interface agents and avatars [32], and medical facial surgery planning <ref> [33, 18] </ref>. Yet it is fair to say that no perfectly realistic facial animation has ever been generated by computer: no "facial animation Turing test" has ever been passed. There are several factors that make realistic facial animation so elusive. First, the human face is an extremely complex geometric form.
Reference: [34] <author> Keith Waters. </author> <title> A muscle model for animating three-dimensional facial expression. </title> <editor> In Maureen C. Stone, editor, </editor> <booktitle> Computer Graphics (SIG-GRAPH '87 Proceedings), </booktitle> <volume> volume 21, </volume> <pages> pages 17-24, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: dimensions, including simple geometric interpolation between digitized [25] or laser-scanned models; performance-based animation, in which measurements from real human actors are used to drive synthetic characters [4, 36]; and various forms of physically-based animation, in which musculo-skeletal controls modeled with various degrees of realism are used to create facial animations <ref> [34, 31] </ref>. However, for sheer 2 realism, one of the most effective approaches to date has been the use of 2D morphing between photographic images [2].
Reference: [35] <author> H. Weghorst, G. Hooper, and Donald P. Greenberg. </author> <title> Improved computational methods for ray tracing. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 3(1) </volume> <pages> 52-69, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: This map is constructed in a pre-processing stage by rendering the cylindrical projection of each face triangle using a unique color. (This technique is essentially a cylindrical variant of a well-known ray tracing acceleration technique, known as the item 9 buffer <ref> [35] </ref>.) Once the containing triangle is established, p is obtained by performing a single ray-plane intersection. 3.1 Weight-map construction Constructing the weight maps for blending the input images into a single texture map is probably the trickiest and the most interesting component of our texture-mapping technique.
Reference: [36] <author> Lance Williams. </author> <title> Performance-driven facial animation. </title> <editor> In Forest Bas-kett, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '90 Proceedings), </booktitle> <volume> volume 24, </volume> <pages> pages 235-242, </pages> <month> August </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: A number of approaches have been developed to model and animate realistic facial expressions in three dimensions, including simple geometric interpolation between digitized [25] or laser-scanned models; performance-based animation, in which measurements from real human actors are used to drive synthetic characters <ref> [4, 36] </ref>; and various forms of physically-based animation, in which musculo-skeletal controls modeled with various degrees of realism are used to create facial animations [34, 31]. However, for sheer 2 realism, one of the most effective approaches to date has been the use of 2D morphing between photographic images [2].
References-found: 36

