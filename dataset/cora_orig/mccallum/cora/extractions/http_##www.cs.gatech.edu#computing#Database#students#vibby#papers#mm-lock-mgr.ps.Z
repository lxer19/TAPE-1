URL: http://www.cs.gatech.edu/computing/Database/students/vibby/papers/mm-lock-mgr.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Database/students/vibby/vibby.html
Root-URL: 
Email: toby@almaden.ibm.com vibby@cc.gatech.edu  
Title: The Design and Performance Evaluation of a Lock Manager for a Memory-Resident Database System  
Author: Tobin J. Lehman Vibby Gottemukkala 
Date: January 14, 1994  
Affiliation: IBM Almaden Research Center Georgia Institute of Technology  
Abstract: In the last fifteen years, lock managers for regular disk-based database systems have seen little change. This is not without reason, since traditional memory-resident lock managers have always been much faster than disk-based database storage managers and disk-based database systems had few alternative design options. However, the introduction of memory-resident database systems has created both new requirements and new opportunities for better lock managers. We present the design of a lock manager that exploits the special nature of the memory-resident storage component in the Starburst experimental database system. To achieve a performance advantage over traditional lock managers, our lock manager physically attaches concurrency control meta-data to the database data itself, thereby making the meta-data directly accessible, rather than indirectly accessible via a hash-table structure. Furthermore, our lock manager eliminates intention locks and changes the lock granularity of each relation dynamically to achieve high performance while ensuring a high level of sharing. Performance experiments comparing our lock manager with the default Starburst lock manager show that these basic design choices can provide significant performance gains. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, M. Carey, and M. Livny. </author> <title> Models for Studying Concurrency Control Performance: Alternativves and Implications. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1985. </year>
Reference-contexts: Compared to the large body of work in the locking family of concurrency control methods (for example, <ref> [1, 2, 3, 4] </ref>, there is little or no published work in the area of changing the locking mechanism itself. The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems.
Reference: [2] <author> P. Bernstein and N. Goodman. </author> <title> Concurrency Control in Distributed Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 13(2), </volume> <month> June </month> <year> 1981. </year>
Reference-contexts: Compared to the large body of work in the locking family of concurrency control methods (for example, <ref> [1, 2, 3, 4] </ref>, there is little or no published work in the area of changing the locking mechanism itself. The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems.
Reference: [3] <author> M. Carey and M. Stonebraker. </author> <title> The Performance of Concurrency Control Algorithms for Database Management Systems. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: Since studies have shown that lock-based concurrency control, compared to optimistic or timestamp methods, have the best overall performance in terms of throughput and resource consumption <ref> [3] </ref>, we chose locking as our CC method. There are two key aspects to MRDBs that can be exploited in implementing a CC mechanism. First, data resides permanently in memory and can be directly referenced by memory address. <p> Compared to the large body of work in the locking family of concurrency control methods (for example, <ref> [1, 2, 3, 4] </ref>, there is little or no published work in the area of changing the locking mechanism itself. The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems.
Reference: [4] <author> K. Eswaran, J. Gray, R. Lorie, and I. Traiger. </author> <title> The Notions of Consistency and Predicate Locks in a Database System. </title> <journal> Communications of the ACM, </journal> <month> 19(11), November </month> <year> 1976. </year>
Reference-contexts: Compared to the large body of work in the locking family of concurrency control methods (for example, <ref> [1, 2, 3, 4] </ref>, there is little or no published work in the area of changing the locking mechanism itself. The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems.
Reference: [5] <author> V. Gottemukkala and T. Lehman. </author> <title> Locking and Latching in a Memory-Resident Database System. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Furthermore, the penalty for using a single latch to guard a table, its related indexes and its related lock information (i.e. the lack of concurrency) was not significant <ref> [5] </ref>, since the lack of any I/O operations made the latch hold times relatively short. 1 Furthermore, RDBMSs often charge a sizable space overhead for storing data. Although the exact amount is data dependent, Starburst often charges an additional 50%.
Reference: [6] <author> J. Gray. </author> <title> Notes on Database Operating Systems. In Operating Systems, An Advanced Course. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: database systems. 1 However, although the lock manager is usually coded very carefully, with performance as the utmost concern, the basic design of database system lock managers has not changed since Jim Gray outlined the basic design of a lock manager in "Gray's Notes on Database Operating Systems," in 1978 <ref> [6] </ref>. In this paper we concentrate on changing the basic design of the lock manager, exploiting new design options that are made available by the base memory-resident database system. <p> A latch, or short-term lock, is a low level primitive that provides a cheap serialization mechanism with shared and exclusive lock modes, but no deadlock detection <ref> [6] </ref>. Latches are physical and are typically built into the resource (s) that they protect. In Starburst, latches are used to gain exclusive or shared access to shared resources, such as buffer pool pages or internal system control blocks.
Reference: [7] <author> J. N. Gray, R. A. Lorie, and G. R. Putzolu. </author> <title> Granularity of Locks and Degrees of Consistency in a Shared Database. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <year> 1975. </year>
Reference-contexts: No single locking granularity is sufficient. Table locks cannot be used universally they would overly restrict sharing. Tuple locks cannot be used everywhere they would be too expensive, especially for table-level operations. Multi-granular locking was proposed in <ref> [7] </ref> as a mechanism that combines both coarse and fine-granularity locking. In this method, transactions are allowed to lock data items of different sizes and a hierarchy of data granularities is defined (e.g. table-level, page-level, and tuple-level could be one such hierarchy).
Reference: [8] <author> E. Hansen. </author> <title> The interval skip skip list: A data structure for finding all intervals that overlap a point". </title> <type> Technical Report WSU-CS-91-01, </type> <institution> Wright State University, </institution> <year> 1991. </year>
Reference-contexts: Second, it 19 is not clear how one can efficiently test the right set of read predicates. It appears that a separate list of read predicates per index is needed. Although a set of read predicates can be searched with an interval search tree (or skip list) <ref> [8] </ref>, the bookkeeping for all of the predicates begins to look unwieldy. Finally, updaters need to post both the old and new values as both write values and read predicates. The cost savings of this scheme seems less when examined closely.
Reference: [9] <author> L. Hobbs and K. </author> <title> England. Rdb/VMS A Comprehensive Guide. </title> <publisher> Digital Press, </publisher> <year> 1991. </year>
Reference-contexts: The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock. <p> local lock manager and are not sent to the global lock manager, thus saving the message costs of those global page lock calls. 5.3 Adjustable Lock Granularity Digital Equipment Corporation's Rdb/VMS database product, which runs on VAX Clusters, uses a somewhat different scheme to reduce the number of lock calls <ref> [9, 13] </ref>. In Rdb, there is only one global lock manager, the VMS Distributed Lock Manager (DLM). Rather than use a fixed set of lock granule sizes, such as table, page, and tuple, an Rdb application can set arbitrary Lock granule sizes.
Reference: [10] <author> IBM. </author> <title> IMS Version 1, Release 1.5 Fast Path Feature Description and Design Guide. IBM World Trade Systems Centers (G320-5775), </title> <year> 1979. </year>
Reference-contexts: The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock. <p> On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost [10, 11, 14, 9, 13, 21] 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH <ref> [10, 11] </ref> is that the first transaction to lock a data item does not pay the full cost of a lock. Instead, it simply leaves behind a flag, warning all other transactions that the item is really locked.
Reference: [11] <author> IBM. </author> <title> Guide to IMS?VS V1 R3 Data Entry Database (DEDB) Facility. IBM International Systems Centers (GG24-1633-0), </title> <year> 1984. </year>
Reference-contexts: The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock. <p> On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost [10, 11, 14, 9, 13, 21] 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH <ref> [10, 11] </ref> is that the first transaction to lock a data item does not pay the full cost of a lock. Instead, it simply leaves behind a flag, warning all other transactions that the item is really locked.
Reference: [12] <author> J. Jordan, J. Bannerjee, and R. Batman. </author> <title> Precision Locks. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: Predicate Locks an example of something that, at least initially, appeared to be tailor made to our needs. 6.1 Predicate Locks A particular implementation of predicate locks, known as Precision Locks <ref> [12] </ref> also seemed appealing, both for the novelty and for the opportunity to lock at the column level.
Reference: [13] <author> A. Joshi. </author> <title> Adaptive Locking Strategies in a Multi-node Data Sharing Environment. </title> <booktitle> In Pro ceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock. <p> local lock manager and are not sent to the global lock manager, thus saving the message costs of those global page lock calls. 5.3 Adjustable Lock Granularity Digital Equipment Corporation's Rdb/VMS database product, which runs on VAX Clusters, uses a somewhat different scheme to reduce the number of lock calls <ref> [9, 13] </ref>. In Rdb, there is only one global lock manager, the VMS Distributed Lock Manager (DLM). Rather than use a fixed set of lock granule sizes, such as table, page, and tuple, an Rdb application can set arbitrary Lock granule sizes.
Reference: [14] <author> T. Lehman. </author> <title> Design and Performance Evaluation of a Main Memory Relational Database System. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: As main-memory size increases, several design constraints disappear, thus making more options available. Previous work explored new designs for high-performance memory-resident database (MRDB) system subcomponents, such as the table storage manager <ref> [14] </ref>, the index manager [15], and the log/recovery manager [16]. However, one vital subcomponent has remained relatively untouched: the lock manager. <p> Starburst memory constraints prevented us from creating a lock hash table larger than 1,000 slots, so lock numbers greater than 500 caused SB LM's performance to decrease. 8 The best performance was 8 By using a dynamic hashing algorithm, such as linear hashing [20] or modified linear hashing <ref> [14] </ref>, we could eliminate this problem for any number of locks. 13 Operation Cost (microseconds) SB LM Lock 25 s SB LM Unlock 12 s SB LM Combined 37 s MMM LM (seg + Remembered) Lock 6 s MMM LM (seg + Remembered) Unlock 5 s MMM LM (seg + Remembered) <p> The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock. <p> level, the cost of de-escalating a node's locks from table-level all the way down the ALG tree to tuple-level may become high. 5.4 The Original MRDB Work In his PhD thesis, Lehman presented a design for a lock manager that uses lock de-escalation to reduce the number of lock calls <ref> [14, 17] </ref>. The lock manager described in this paper is based on that original design. 6 Interesting Alternatives Besides work that was directly related to the design of our concurrency control mechanism, there were other various works that caught our attention because they might help us to improve the design.
Reference: [15] <author> T. Lehman and M. Carey. </author> <title> A Study of Index Structures for Main Memory Database Man agement Systems. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: As main-memory size increases, several design constraints disappear, thus making more options available. Previous work explored new designs for high-performance memory-resident database (MRDB) system subcomponents, such as the table storage manager [14], the index manager <ref> [15] </ref>, and the log/recovery manager [16]. However, one vital subcomponent has remained relatively untouched: the lock manager.
Reference: [16] <author> T. Lehman and M. Carey. </author> <title> A Recovery Algorithm for a High-Performance Memory-Resident Database System. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: As main-memory size increases, several design constraints disappear, thus making more options available. Previous work explored new designs for high-performance memory-resident database (MRDB) system subcomponents, such as the table storage manager [14], the index manager [15], and the log/recovery manager <ref> [16] </ref>. However, one vital subcomponent has remained relatively untouched: the lock manager.
Reference: [17] <author> T. Lehman and M. Carey. </author> <title> A Concurrency Control Algorithm for a Memory-Resident Database System. </title> <booktitle> In Proc. of the Foundations of Data Organization and Algorithms (FODO), </booktitle> <year> 1989. </year>
Reference-contexts: level, the cost of de-escalating a node's locks from table-level all the way down the ALG tree to tuple-level may become high. 5.4 The Original MRDB Work In his PhD thesis, Lehman presented a design for a lock manager that uses lock de-escalation to reduce the number of lock calls <ref> [14, 17] </ref>. The lock manager described in this paper is based on that original design. 6 Interesting Alternatives Besides work that was directly related to the design of our concurrency control mechanism, there were other various works that caught our attention because they might help us to improve the design.
Reference: [18] <author> T. Lehman, E. Shekita, and L. F. Cabrera. </author> <title> An Evaluation of the Starburst Memory-Resident Storage Component. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 4(6), </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Second, it has been shown that transactions accessing data in an MRDB are typically shorter in duration compared to those accessing the same data in a disk-based database (even when the data in the disk-based database was memory-resident at all times). <ref> [18] </ref> Thus, for most of the data, there is likely to be less contention in an MRDB than in a disk-based database. <p> In this paper, we are concerned with a particular subset of the Starburst components. These are the traditional disk-oriented storage method (known as the Vanilla Relation Manager (VRM)), the memory-resident storage method (known as the Main-Memory Manager (MMM) <ref> [18] </ref>), and the 2 regular Starburst lock manager (SB LM) that implements a traditional lock-based CC mechanism. The MMM storage component is intended to store data for which fast response-time is crucial. <p> In the Starburst MMM storage manager, we've taken a different approach. The memory-resident nature of the database data allows for a different style of table manager. <ref> [18] </ref> First of all, index entries in MMM indexes contain pointers to the actual tuples, rather than tuple identifiers, as in disk systems. Second, lock control blocks are also attached to tuples via memory pointers.
Reference: [19] <author> B. Lindsay, J. McPherson, and H. Pirahesh. </author> <title> A Data Management Extension Architecture. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: These two aspects of MRDBs allow us to change the basic design of the CC mechanism and achieve higher performance than that can be obtained from a traditional CC mechanism. We implemented our design in the Starburst extensible Relational database system prototype. Starburst's Data Management Extension Architecture (DMEA) <ref> [19] </ref> formalizes and simplifies the interface for creating new storage methods and attachments. Storage methods manage tables and their associated tuples, and attachments manage data structures related to tables, such as indexes.
Reference: [20] <author> W. Litwin. </author> <title> Linear Hashing: A New Tool for File and Table Addressing. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> October </month> <year> 1980. </year>
Reference-contexts: Starburst memory constraints prevented us from creating a lock hash table larger than 1,000 slots, so lock numbers greater than 500 caused SB LM's performance to decrease. 8 The best performance was 8 By using a dynamic hashing algorithm, such as linear hashing <ref> [20] </ref> or modified linear hashing [14], we could eliminate this problem for any number of locks. 13 Operation Cost (microseconds) SB LM Lock 25 s SB LM Unlock 12 s SB LM Combined 37 s MMM LM (seg + Remembered) Lock 6 s MMM LM (seg + Remembered) Unlock 5 s
Reference: [21] <author> C. Mohan and I. Narang. </author> <title> Recovery and Coherency-Control Protocols for Fast Inter-system Page Transfer and Fine-Granularity Locking in Shared Disks Transaction Environment. </title> <booktitle> In Proceedings of the Int. Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The System R lock manager described in [Gray 78] appears to be the basic design choice of most database systems. On the other hand, there are a few methods published for reducing the number of lock calls, and thus reducing overall locking cost <ref> [10, 11, 14, 9, 13, 21] </ref> 16 5.1 Cheap First Locks The idea behind "Cheap-First-Locks", used by IMS FASTPATH [10, 11] is that the first transaction to lock a data item does not pay the full cost of a lock.
Reference: [22] <author> K. Salem and H. Garcia-Molina. </author> <title> Crash Recovery Mechanisms for Main Storage Database Systems. </title> <type> Technical Report CS-TR-0340-86, </type> <institution> Computer Science Dept., Princeton University, </institution> <month> April </month> <year> 1986. </year>
Reference-contexts: In this section we deal with the policy of the concurrency control algorithm, namely, the dynamic decision between low cost and high concurrency. In a uniprocessor, the cost of locking could be eliminated by serializing transactions, effectively having transactions set a database-level lock <ref> [22] </ref>. Of course, to guarantee a reasonable response time in this environment, all transactions must be small. A less extreme and more general solution would be to use table locks, and thus reduce the granularity of sharing to tables.
Reference: [23] <author> K. Shoens. </author> <title> Data Sharing vs Paritioning for Capacity and Availability. </title> <booktitle> In Int. Workshop on High Performance Transaction Systems, </booktitle> <month> September </month> <year> 1985. </year> <month> 22 </month>
Reference-contexts: This was not the case for IMS FASTPATH, where the shortcut for a Cheap-First-Lock was significantly cheaper than a regular lock call, as the lock call required a context switch. 5.2 Amoeba Amoeba <ref> [23] </ref>, a data sharing project at the IBM Almaden Research Center, created a lock manager protocol that reduced the number of global lock calls generated. Amoeba is a data sharing system comprising many nodes and a collection of disks.
References-found: 23

