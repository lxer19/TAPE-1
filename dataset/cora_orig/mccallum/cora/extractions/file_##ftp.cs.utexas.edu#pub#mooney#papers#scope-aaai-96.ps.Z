URL: file://ftp.cs.utexas.edu/pub/mooney/papers/scope-aaai-96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/estlin/pubs.html
Root-URL: 
Email: festlin,mooneyg@cs.utexas.edu  
Title: Multi-Strategy Learning of Search Control for Partial-Order Planning  
Author: Tara A. Estlin and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: Appears in the Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96)  
Abstract: Most research in planning and learning has involved linear, state-based planners. This paper presents Scope, a system for learning search-control rules that improve the performance of a partial-order planner. Scope integrates explanation-based and inductive learning techniques to acquire control rules for a partial-order planner. Learned rules are in the form of selection heuristics that help the planner choose between competing plan refinements. Specifically, Scope learns domain-specific control rules for a version of the UCPOP planning algorithm. The resulting system is shown to produce significant speedup in two different planning domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barrett, A., and Weld, D. </author> <year> 1994. </year> <title> Partial order planning: Evaluating possible efficiency gains. </title> <booktitle> Artificial Intelligence 67 </booktitle> <pages> 71-112. </pages>
Reference: <author> Bhatnagar, N., and Mostow, J. </author> <year> 1994. </year> <title> On-line learning from search failure. </title> <booktitle> Machine Learning 15 </booktitle> <pages> 69-117. </pages>
Reference: <author> Borrajo, D., and Veloso, M. </author> <year> 1994. </year> <title> Incremental learning of control knowledge for nonlinear problem solving. </title> <booktitle> In Proc. of ECML-94, </booktitle> <pages> 64-82. </pages>
Reference: <author> Cohen, W. W. </author> <year> 1990. </year> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proc. of ML-90, </booktitle> <pages> 268-276. </pages>
Reference-contexts: By incorporating induction to learn simpler, approximate control rules, we can greatly improve the utility of acquired knowledge <ref> (Cohen 1990) </ref>. Scope (Search Control Optimization of Planning through Experience) integrates explanation-based generalization (EBG) (Mitchell et al., 1986; DeJong & Mooney, 1986) with techniques from inductive logic programming (ILP) (Quinlan 1990; Muggleton 1992) to learn high-utility rules that can generalize well to new planning situations.
Reference: <author> DeJong, G. F., and Mooney, R. J. </author> <year> 1986. </year> <title> Explanation-based learning: An alternative view. </title> <booktitle> Machine Learning 1(2) </booktitle> <pages> 145-176. </pages>
Reference: <author> Estlin, T. A. </author> <year> 1996. </year> <title> Integrating explanation-based and inductive learning techniques to acquire search-control for planning. </title> <type> Technical report, </type> <institution> Dept. of Computer Sciences, University of Texas, Austin, TX. </institution> <note> Forthcoming. URL: http://net.cs.utexas.edu/ml/ Kambhampati, </note> <author> S.; Katukam, S.; and Qu, Y. </author> <year> 1996. </year> <title> Failure driven search control for partial order planners: An explanation based approach. </title> <journal> Artificial Intelligence. Forthcoming. </journal>
Reference-contexts: Scope uses the examples to induce a set of control heuristics which are then incorporated into the original planner. Scope's algorithm has three main phases, which are presented in the next few sections. A more detailed description can be found in <ref> (Estlin 1996) </ref>. Example Analysis In the example analysis phase, two outputs are produced: a set of selection-decision examples and a set of generalized proof trees. Selection-decision examples record successful and unsuccessful applications of plan refinements. Generalized proofs provide a background context that explains the success of all correct planning decisions.
Reference: <author> Langley, P., and Allen, J. </author> <year> 1991. </year> <title> The acquisition of human planning expertise. </title> <booktitle> In Proc. of ML-91, </booktitle> <pages> 80-84. </pages>
Reference: <author> Leckie, C., and Zuckerman, I. </author> <year> 1993. </year> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proc. of IJCAI-93, </booktitle> <pages> 1100-1105. </pages>
Reference: <author> Minton, S.; Drummond, M.; Bresina, J. L.; and Phillips, A. B. </author> <year> 1992. </year> <title> Total order vs. partial order planning: Factors influencing performance. </title> <booktitle> In Proc. of the 3rd Int. Conf. on Principles of Knowledge Rep. and Reasoning, </booktitle> <pages> 83-92. </pages>
Reference: <author> Minton, S. </author> <year> 1989. </year> <title> Explanation-based learning: A problem solving perspective. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 63-118. </pages>
Reference-contexts: Past systems have often employed explanation-based learning (EBL) to learn control knowledge. Unfortunately, standard EBL can frequently produce complex, overly-specific control rules that decrease rather than improve overall planning fl This research was supported by the NASA Graduate Student Researchers Program, grant number NGT-51332. performance <ref> (Minton 1989) </ref>. By incorporating induction to learn simpler, approximate control rules, we can greatly improve the utility of acquired knowledge (Cohen 1990).
Reference: <author> Mitchell, T. M.; Keller, R. M.; and Kedar-Cabelli, S. T. </author> <year> 1986. </year> <title> Explanation-based generalization: A unifying view. </title> <booktitle> Machine Learning 1(1) </booktitle> <pages> 47-80. </pages>
Reference: <editor> Muggleton, S. H., ed. </editor> <booktitle> 1992. Inductive Logic Programming. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Pazzani, M., and Kibler, D. </author> <year> 1992. </year> <title> The utility of background knowledge in inductive learning. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 57-94. </pages>
Reference-contexts: Such a bias is important for learning rules with a low match cost, which helps avoid the utility problem. Foil is also relatively easy to bias with prior knowledge <ref> (Pazzani & Kibler 1992) </ref>. In our case, we can utilize the information contained in the generalized proof trees of planning solution traces. Foil attempts to learn a concept definition in terms of a given set of background predicates.
Reference: <author> Penberthy, J., and Weld, D. S. </author> <year> 1992. </year> <title> UCPOP: A sound, complete, partial order planner for ADL. </title> <booktitle> In Proc. of the 3rd Int. Conf. on Principles of Knowledge Rep. and Reasoning, </booktitle> <pages> 113-114. </pages>
Reference-contexts: These heuristics greatly reduce backtracking by directing a planner to immediately select appropriate plan refinements. Scope is implemented in Prolog, which provides a good framework for learning control knowledge. A version of the UCPOP planning algorithm <ref> (Penberthy & Weld 1992) </ref> was implemented as a Prolog program to provide a testbed for Scope. Experimental results are presented on two domains that show Scope can significantly increase partial-order planning efficiency.
Reference: <author> Quinlan, J. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5(3) </booktitle> <pages> 239-266. </pages>
Reference-contexts: In the blocksworld domain, such a definition is learned for each of the candidates shown in Figure 1. In this context, control rule learning can be viewed as relational concept learning. Scope employs a version of the Foil algorithm <ref> (Quinlan 1990) </ref> to learn control rules through induction. Foil has proven efficient in a number of domains, and has a "most general" bias which tends to produce simple definitions. Such a bias is important for learning rules with a low match cost, which helps avoid the utility problem.
Reference: <author> Silverstein, G., and Pazzani, M. J. </author> <year> 1991. </year> <title> Relational cliches: Constraining constructive induction during relational learning. </title> <booktitle> In Proc. of ML-91, </booktitle> <pages> 203-207. </pages>
Reference-contexts: Scope also considers several other types of control rule antecedents during induction. Besides pulling literals directly from generalized proof trees, Scope can use negated proof literals, determinate literals (Mug-gleton 1992), variable codesignation constraints, and relational cliches <ref> (Silverstein & Pazzani 1991) </ref>. Incorporating different antecedent types helps Scope learn expressive control rules that can describe partial-order planning situations. Program Specialization Phase Once refinement selection rules have been learned, they are passed to the program specialization phase which adds this control information into the original planner.
Reference: <author> Veloso, M. M. </author> <year> 1992. </year> <title> Learning by Analogical Reasoning in General Problem Solving. </title> <type> Ph.D. Dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: The second and third rule allow putdown (A) to be applied only when A should be placed on the table and not stacked on another block. Experimental Evaluation The blocksworld and logistics transportation domains were used to test the Scope learning system. In the logistics domain <ref> (Veloso 1992) </ref>, packages must be delivered to different locations in several cities. A test set of 100 independently generated problems was used to evaluate performance in both domains. Scope was trained on separate example sets of increasing size.
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1993. </year> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proc. of IJCAI-93, </booktitle> <pages> 1106-1111. </pages>
Reference-contexts: Scope is then used to learn refinement-selection rules which are incorporated into the original planning program in the form of clause-selection heuristics. The Scope Learning System Scope is based on the Dolphin learning system <ref> (Zelle & Mooney 1993) </ref>, which optimizes logic programs by learning clause-selection rules. Dolphin has been shown successful at improving program performance in several domains, including planning domains which employed a simple state-based planner.
References-found: 18

