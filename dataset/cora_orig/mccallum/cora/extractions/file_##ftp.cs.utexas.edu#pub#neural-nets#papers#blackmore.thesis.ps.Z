URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/blackmore.thesis.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: http://www.cs.utexas.edu
Title: Visualizing High-Dimensional Structure with the Incremental Grid Growing Neural Network  
Author: Justine Blackmore 
Address: Austin, TX 78712  
Affiliation: Artificial Intelligence Laboratory The University of Texas at Austin  
Date: August 1995  
Pubnum: Report AI95-238  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bauer, H., Pawelzik, K. and Geisel, T. </author> <year> (1992). </year> <title> A Topographic Product for the Optimization of Self-Organizing Feature Maps. </title> <editor> In Moody, J. E., Hanson, J. and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 1141-1147. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Blayo, F. and Demartines, P. </author> <year> (1991). </year> <title> Data analysis: How to compare Kohonen neural networks to other techniques? In Brieto, </title> <editor> A. editor, </editor> <booktitle> Lecture Notes in Computer Sciences 540: Artificial Neural Networks (IWANN 1991). </booktitle> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer. </publisher>
Reference: <author> Bouton, C. and Pages, G. </author> <year> (1993). </year> <title> Self-organization of the one-dimensional Kohonen algorithm with non-uniformly distributed stimuli. </title> <booktitle> Stochastic Processes and their Applications 47 </booktitle> <pages> 249-274. </pages>
Reference: <author> Cavalli-Sforza, L. L, Piazza, A. </author> <year> (1993). </year> <title> Human Genomic Diversity in Europe: A Summary of Recent Research and Prospects for the Future. </title> <journal> European Journal of Human Genetics, </journal> <volume> 1 </volume> <pages> 3-18. </pages>
Reference: <author> Cavalli-Sforza, L. L, Menozzi, P., and Piazza, A. </author> <year> (1994). </year> <title> The History and Geography of Human Genes. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Cottrell, M., and Fort, J. C. </author> <year> (1987). </year> <title> Etude d'un algorithme d'auto-organisation. </title> <journal> Ann. Inst. Henri Poincare, </journal> <volume> 23 </volume> <pages> 1-20. </pages>
Reference: <editor> Cottrell, M., Fort, J. C., and Pages, G. </editor> <year> (1992). </year> <title> Two or three things that we know about the Kohonen Algorithm. </title>
Reference: <author> Demartines, P. </author> <year> (1992). </year> <title> Organization measures and representations of the Kohonen maps. </title> <editor> In Herault, J., editor, </editor> <booktitle> Proceedings of the First IFIP Working Group (Grenoble, </booktitle> <address> France). </address>
Reference-contexts: Another goal is determining how the final state depends on the time evolution and shape of the neighborhood function and learning rates. Several researchers have proposed order definitions to quantify how well the 2-D map preserves the topology of the input space <ref> (Demartines 1992, Zrehen and Blayo 1992) </ref>. These definitions of topology preservation are then used to study the effect of neighborhood size and shape on convergence of the 2-D map. For example, Lo, Fujita and Bavarian (1991) use a definition of order localized to the neighborhood of winner units. <p> However, that work has not yet been successfully extended to 2 dimensions and arbitrary input. Several authors have discussed the necessity for developing some kind of "goodness" measure for 2-D maps <ref> (Blayo and Demartines l99O, Zrehen 1992) </ref>. There has been some research into the evolution of easily calculable map quantities (e.g. the input mapping error and standard deviation, and the similarity between map's point density and input's probability distribution).
Reference: <author> Fritzke, B. (199la). </author> <title> Let it grow|Self-organizing feature maps with problem dependent cell structure. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland), 403-408. Amsterdam; New York: </address> <publisher> North-Holland. 49 Fritzke, </publisher> <editor> B. (199lb). </editor> <title> Unsupervised clustering with growing cell structures. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (Seattle, WA), </booktitle> <volume> vol. II, </volume> <pages> 531-536. </pages> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference: <author> Fritzke, B. </author> <year> (1992). </year> <institution> Wachsende Zellstrukturen|ein selbstorganisierendes neuronales Net-zwerkmodell. </institution> <type> PhD thesis, </type> <institution> Technischen Fakultat, Universitat Erlangen-Nurnberg, Er-langen, Germany. </institution>
Reference: <author> Hertz, J., Krogh, A. and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The weight vectors organize themselves in a way that closely resembles in the input space. Note that in the case of convex input (c), the mapping is less accurate. <ref> (From Hertz, Krogh and Palmer 1991) </ref> is as follows: Given the current input vector x, let i be the winning node chosen such that jw i xj jw j xj; 8j 6= i (2.1) where w k denotes the weight vector of node k. <p> The array is lower dimensional than the input, so its nodes must curve around in order for the weight vectors to cover the whole input space. As a result, the overall topology of the array is somewhat disturbed. <ref> (From Hertz, Krogh and Palmer 1991) </ref> fluctuations can occur in the weight vectors during convergence, and that their character can depend on the time dependence of the learning rate.
Reference: <author> Jockusch, S. </author> <year> (1990). </year> <title> A neural network which adapts its structure to a given set of patterns. </title> <editor> In Eckmiller, R., Hartmann, G. and Hauske, G., editors, </editor> <booktitle> Parallel Processing in Neural Systems and Computers, </booktitle> <pages> 169-172. </pages> <address> Amsterdam; New York: </address> <publisher> North-Holland. </publisher>
Reference: <author> Jockusch, S. and Ritter, H. </author> <year> (1994). </year> <title> Self-organizing Maps: Local Competition and Evolutionary Optimization. </title> <booktitle> Neural Networks 7 </booktitle> <pages> 1229-1240. </pages>
Reference: <author> Kangas, J., Kohonen, T. and Laaksonen, J. </author> <year> (1990). </year> <title> Variants of self-organizing maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 93-99. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982a). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982b). </year> <title> Analysis of a simple self-organizing process. </title> <journal> Biological Cybernetics, </journal> <volume> 44 </volume> <pages> 135-140. </pages>
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organization and Associative Memory. </title> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-organization and Associative Memory. </title> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer. </publisher> <address> Third edition. </address>
Reference-contexts: The topology preserving property of the SOM algorithm makes this possible. 2.2 SOM theory Most of the theoretical work on Kohonen's algorithm has focused on the ordering and convergence of 1-dimensional input mapped onto a 1-dimensional network <ref> (Kohonen 1989, Cottrell and Fort 1987) </ref>. In addition, the stability and convergence properties of 2-dimensional maps given a specific input distribution (uniform) has also been analyzed (Ritter and Schulten 1988). Generally, stochastic methods are used to analyze the self-organizing algorithm as a 9 indicated by the bounding box.
Reference: <author> Kohonen, T. </author> <year> (1990). </year> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1464-1480. </pages>
Reference-contexts: New learning and representation methods are often demonstrated using such semantic data (Miikkulainen 1993; 24 (a) merge clustering algorithm. Cluster boundaries are apparent in the structure, but the global and local topology is not. (b) Map derived by the standard self-organizing algorithm <ref> (Kohonen 1990) </ref>. The map is hexagonally connected. The spanning tree structure is clearly present in the map; however, the full connectivity makes it difficult to extract exact neighborhood relations between units. Ritter and Kohonen 1989; Schutze 1993; Scholtes 1993).
Reference: <author> Lo, Z., and Bavarian, B. </author> <year> (1991). </year> <title> On the rate of convergence in topology preserving neural networks. </title> <journal> Biological Cybernetics, </journal> <volume> 65 </volume> <pages> 55-63. </pages>
Reference-contexts: Because the input here is 2-D, simply plotting the weight vectors of the network reveals the poor topology. However, if the weight vectors were high dimensional, they could not be plotted. In that case, the twisted topology might go undetected. <ref> (From Lo, Fujita and Bavarian 1991) </ref> never become disordered proceeds r faster when the neighborhood function more closely resembles the "Mexican hat" of lateral interaction profiles. The inability to provide a clear mathematical formalism which describes the function computed by the self-organizing process has frustrated many researchers.
Reference: <author> Lo, Z., Fujita, M. and Bavarian, B. </author> <year> (1991). </year> <title> Analysis of Neighborhood Interaction in Ko-honen Neural Networks. </title> <booktitle> In Proceedings of the Fifth International Parallel Processing Symposium (Los Alamitos, </booktitle> <address> CA), </address> <pages> 246-249. </pages>
Reference-contexts: Because the input here is 2-D, simply plotting the weight vectors of the network reveals the poor topology. However, if the weight vectors were high dimensional, they could not be plotted. In that case, the twisted topology might go undetected. <ref> (From Lo, Fujita and Bavarian 1991) </ref> never become disordered proceeds r faster when the neighborhood function more closely resembles the "Mexican hat" of lateral interaction profiles. The inability to provide a clear mathematical formalism which describes the function computed by the self-organizing process has frustrated many researchers.
Reference: <author> Lo, Z. Yu, Y. and Bavarian, B. </author> <year> (1993). </year> <title> Analysis of the convergence properties of topology preserving neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <pages> 4207-220. </pages> <note> 50 Martinetz, </note> <author> T. and Schulten, K. J. </author> <year> (1991). </year> <title> A "neural gas" network learns topologies. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland), 197402. Amsterdam; New York: </address> <publisher> North-Holland. </publisher>
Reference: <author> Martinetz, T. and Schulten, K. J. </author> <year> (1994). </year> <title> Topology Representing Networks. </title> <booktitle> Neural Networks 7 </booktitle> <pages> 507-522. </pages>
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon and Memory. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: As organization proceeds, the topology of the weight vectors begins to resemble the topology of the input. By 100 epochs, a gross, global order has emerged. At 10000 epochs, the weight vectors have settled into an excellent representation of the input space. <ref> (From Miikkulainen 1993) </ref> and obtain the desired angle combinations by examining the node's weight vector.
Reference: <author> Nichols, J. </author> <year> (1992). </year> <title> Linguistic Diversity in Space and Time. </title> <address> Chicago, IL: </address> <publisher> University of Chicago Press. </publisher>
Reference-contexts: This research will continue to concentrate on discovering the unknown structure of large high-dimensional data sets. Specifically, the full genetics data set from Cavalli-Sforza consisting of 491 populations described by 128 gene frequencies, will be investigated. IGG will also be applied to high-dimensional linguistics data for the same populations <ref> (Nichols 1992) </ref> for comparison. Clearly, the relationships and interactions between human populations are complex. Visualizing the relationships within and between genetic and linguistic clusters may aid in understanding the evolution and migration of populations. Incremental Grid Growing was designed with just this kind of application in mind. 48
Reference: <author> Ritter, H. J. </author> <year> (1991). </year> <title> Learning with the self-organizing map. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland), 379-384. Ams-terdam; New York:North-Holland. </address>
Reference: <author> Ritter, H. J., and Kohonen, T. </author> <year> (1989). </year> <title> Self-organizing semantic maps. </title> <journal> Biological Cybernetics, </journal> <volume> 61 </volume> <pages> 241-254. </pages>
Reference: <author> Ritter, H. J., and Schulten, K.J. </author> <year> (1988). </year> <title> Convergence properties of Kohonen's topology conserving maps: Fluctuations, Stability and Dimension Selection. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 59-71. </pages>
Reference-contexts: In addition, the stability and convergence properties of 2-dimensional maps given a specific input distribution (uniform) has also been analyzed <ref> (Ritter and Schulten 1988) </ref>. Generally, stochastic methods are used to analyze the self-organizing algorithm as a 9 indicated by the bounding box. The network weights organize themselves to cover the entire input space while preserving the topology of the input space. <p> The fact that none of the network connections must cross another indicates that local topology is preserved in the map|nearby points in the input space are mapped onto neighbors of the map. <ref> (From Ritter and Schulten 1988) </ref> Markov process. The weight vectors of the map constitute the states of the Markov process, and the state transitions are defined by the self-organizing learning rule above. Transitions are triggered by random selection of input vectors. <p> So far this type of analysis has only been carried out for the case of 3-D uniform input <ref> (Ritter and Schulten 1988) </ref>. Extension to more general input spaces should provide important clues to predicting the final state of the map for arbitrary input. In practice, it is possible for twists and other distortions to appear in the network topology (figure 2.5).
Reference: <author> Rodriques, J.S., and Almeida, L.B. </author> <year> (1990). </year> <title> Improving the learning speed in topological maps of patterns. </title> <booktitle> In Proceedings of the International Neural Networks Conference (Paris, France), </booktitle> <pages> 813-816. </pages> <address> Dordrecht; Boston: </address> <publisher> Kluwer. Scholtes, </publisher> <month> J.C </month> <year> (1993). </year> <title> Neural Networks in Natural Language Processing and Information Retrieval. </title> <type> PhD thesis, </type> <institution> Universiteit van Amsterdam, </institution> <address> Amsterdam, the Netherlands. </address>
Reference: <author> Schutze, H. </author> <year> (1993). </year> <title> Word Space. </title> <editor> In Giles, C. L., Hanson, S. J., and Cowan, J. D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Xu, L., and Oja, E. </author> <year> (1990). </year> <title> Adding top-down expectation into the learning procedure of self-organizing maps. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (Washington, DC), </booktitle> <volume> vol. II, </volume> <pages> 531-534. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Zrehen, S. and Blayo, F. </author> <year> (1992). </year> <title> A geometric organization measure for Kohonen maps. </title> <booktitle> In Proceedings of Neuro-Nimes, </booktitle> <pages> 603-610. 51 </pages>
Reference-contexts: Another goal is determining how the final state depends on the time evolution and shape of the neighborhood function and learning rates. Several researchers have proposed order definitions to quantify how well the 2-D map preserves the topology of the input space <ref> (Demartines 1992, Zrehen and Blayo 1992) </ref>. These definitions of topology preservation are then used to study the effect of neighborhood size and shape on convergence of the 2-D map. For example, Lo, Fujita and Bavarian (1991) use a definition of order localized to the neighborhood of winner units. <p> However, that work has not yet been successfully extended to 2 dimensions and arbitrary input. Several authors have discussed the necessity for developing some kind of "goodness" measure for 2-D maps <ref> (Blayo and Demartines l99O, Zrehen 1992) </ref>. There has been some research into the evolution of easily calculable map quantities (e.g. the input mapping error and standard deviation, and the similarity between map's point density and input's probability distribution).
References-found: 32

