URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.tr540.Loop_scheduling_for_heterogeneity.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/cierniak/research/papers-short.html
Root-URL: 
Title: Loop Scheduling for Heterogeneity  
Author: Micha l Cierniak Wei Li Mohammed Javeed Zaki 
Date: 1994  
Note: October  
Abstract: Technical Report 540 Department of Computer Science University of Rochester Rochester, NY 14627 fcierniak,wei,zakig@cs.rochester.edu Abstract In this paper, we study the problem of scheduling parallel loops at compile-time for a heterogeneous network of machines. We consider heterogeneity in three aspects of parallel programming: program, processor and network. A heterogeneous program has parallel loops with different amount of work in each iteration; heterogeneous processors have different speeds; and a heterogeneous network has different cost of communication between processors. We propose a simple yet comprehensive model for use in compiling for a network of processors, and develop compiler algorithms for generating optimal and sub-optimal schedules of loops for load balancing, communication optimizations and network contention. Experiments show that significant improvement of performance is achieved using our techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy 23 Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM--12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The last part of this section gives results for our approach to contention avoidance. All experiments were performed on Sun workstations (SPARCstation 1, SPARCstation LX and SPARCstation 10) connected with an Ethernet network. The Programs were written in C and Fortran, and PVM <ref> [1] </ref> was used to parallelize them. 8.1 Heterogeneous programs We have used the code fragment presented in Figure 5 as an example of a heterogeneous loop. The workstations used are all SPARCstation LX's.
Reference: [2] <author> A. S. Grimshaw, J. B. Weissman, E. A. West, and E. C. Loyot. Metasystems: </author> <title> An approach combining parallel processing and heterogeneous distributed computing systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(3) </volume> <pages> 257-270, </pages> <year> 1994. </year>
Reference-contexts: Processor i works on iterations b P i1 k=1 z k c + 1 through b P i schedule obtained in this way is optimal. A similar approach, by distributing the load proportionally to the relative speeds of the processors, has been used with success in <ref> [2] </ref>. 5.2 Homogeneous parallel loops, with communication When there is communication, the algorithm in Section 5.1 will not necessarily generate an optimal schedule. Here we present an optimal solution.
Reference: [3] <author> Joseph L. Hammond and Peter J.P. O'Reilly. </author> <title> Performance analysis of local computer networks. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: It serves as a conceptual starting point in compiling for load balancing and communication in such an environment. We consider heterogeneity in both the processor and the network dimensions. We take a very different approach than the work on stochastic models <ref> [3] </ref>, which are intended to model the performance behavior of the whole system with possibly many jobs running at the same time in an unpredictable way, and are, therefore, more detailed and complicated. We develop this model in the context of compiler code generation and optimizations.
Reference: [4] <author> S. Hummel, E. Schonberg, and L. Flynn. </author> <title> Factoring: A practical and robust method for scheduling parallel loops. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 610-619, </pages> <year> 1991. </year>
Reference-contexts: Self-scheduling [11] is the simplest approach in which processors ask for additional work from the task queue when they become idle. Guided self-scheduling [9] reduces the runtime cost by allocating a block of iterations every time. Mathematical analysis has been used to determine the effectiveness of dynamic scheduling <ref> [5, 4] </ref>. Affinity scheduling [7] is a hybrid static and dynamic scheduling algorithm that takes data locality into account. The rest of this paper is organized as follows. In Section 2, we introduce our program model, which is followed by our machine model in Section 3.
Reference: [5] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-16, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Self-scheduling [11] is the simplest approach in which processors ask for additional work from the task queue when they become idle. Guided self-scheduling [9] reduces the runtime cost by allocating a block of iterations every time. Mathematical analysis has been used to determine the effectiveness of dynamic scheduling <ref> [5, 4] </ref>. Affinity scheduling [7] is a hybrid static and dynamic scheduling algorithm that takes data locality into account. The rest of this paper is organized as follows. In Section 2, we introduce our program model, which is followed by our machine model in Section 3.
Reference: [6] <author> W. Li and K. Pingali. </author> <title> Access Normalization: Loop restructuring for NUMA compilers. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> November </month> <year> 1993. </year>
Reference-contexts: Compile-time static loop scheduling is efficient and introduces no additional runtime overhead. For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion [10]. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account <ref> [6] </ref>. When the execution time of loop iterations is not predictable at compile-time, runtime dynamic scheduling can be used at the additional runtime cost of managing task allocation. Self-scheduling [11] is the simplest approach in which processors ask for additional work from the task queue when they become idle.
Reference: [7] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> In Proc. Supercomputing '92, </booktitle> <pages> pages 104-113, </pages> <year> 1992. </year>
Reference-contexts: Guided self-scheduling [9] reduces the runtime cost by allocating a block of iterations every time. Mathematical analysis has been used to determine the effectiveness of dynamic scheduling [5, 4]. Affinity scheduling <ref> [7] </ref> is a hybrid static and dynamic scheduling algorithm that takes data locality into account. The rest of this paper is organized as follows. In Section 2, we introduce our program model, which is followed by our machine model in Section 3.
Reference: [8] <author> A. Nagurney, C. F. Nicholson, and P. M. Bishop. </author> <title> Spatial price equilibrium models with discriminatory ad valorem tariffs: formulation and comparative computation using variational inequalities. In Recent Advances in Spatial Equilibrium Modelling: Methodology and Applications. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1995. </year> <month> forthcoming. </month>
Reference-contexts: We show that the architecture-conscious scheduling algorithms result in much better performance than the naive architecture-oblivious scheduling approach. Examples are drawn from a mix of synthetic and real applications, from scientific computing and economics modeling <ref> [8] </ref>. Previous researchers have looked at scheduling for homogeneous machines only. Compile-time static loop scheduling is efficient and introduces no additional runtime overhead. For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion [10]. <p> In particular, there may be many different configurations with the same base processor equivalent, but their speedups may be different. 20 The second example of the homogeneous loop case is a program for spatial price equi-librium modeling in economics <ref> [8] </ref>. The program applies the methodology of the theory of variational inequalities for formulation and computation of spatial price equilibrium models with discriminatory ad valorem tariffs, which is a widely-used trade policy instrument.
Reference: [9] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guilded self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36 </volume> <pages> 1425-39, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Self-scheduling [11] is the simplest approach in which processors ask for additional work from the task queue when they become idle. Guided self-scheduling <ref> [9] </ref> reduces the runtime cost by allocating a block of iterations every time. Mathematical analysis has been used to determine the effectiveness of dynamic scheduling [5, 4]. Affinity scheduling [7] is a hybrid static and dynamic scheduling algorithm that takes data locality into account.
Reference: [10] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Previous researchers have looked at scheduling for homogeneous machines only. Compile-time static loop scheduling is efficient and introduces no additional runtime overhead. For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion <ref> [10] </ref>. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [6]. When the execution time of loop iterations is not predictable at compile-time, runtime dynamic scheduling can be used at the additional runtime cost of managing task allocation.
Reference: [11] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. of '86 International Conference On Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year> <month> 24 </month>
Reference-contexts: For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [6]. When the execution time of loop iterations is not predictable at compile-time, runtime dynamic scheduling can be used at the additional runtime cost of managing task allocation. Self-scheduling <ref> [11] </ref> is the simplest approach in which processors ask for additional work from the task queue when they become idle. Guided self-scheduling [9] reduces the runtime cost by allocating a block of iterations every time. Mathematical analysis has been used to determine the effectiveness of dynamic scheduling [5, 4].
References-found: 11

