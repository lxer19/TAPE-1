URL: http://www.research.att.com/~lewis/papers/kwok95.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: email: kklqc@cunyvm.cuny.edu  email: lewis@research.att.com  
Title: TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experiments using  
Author: PIRCS K.L. Kwok L. Grunfeld D. D. Lewis 
Address: Flushing, NY 11367.  600 Mountain Avenue, Murray Hill, NJ 07974.  
Affiliation: Computer Science Dept., Queens College, CUNY,  AT&T Bell Labs,  
Abstract: The PIRCS retrieval system has been upgraded in TREC-3 to handle the full English collections of 2 GB in an efficient manner. For ad-hoc retrieval, we use recurrent spreading of activation in our network to implement query learning and expansion based on the best-ranked subdocuments of an initial retrieval. We also augment our standard retrieval algorithm with a soft-Boolean component. For routing, we use learning from signal-rich short documents or subdocument segments. For the optional thresholding experiment, we tried two approaches to transforming retrieval status values (RSV's) so that they could be used to partition documents into retrieved and nonretrieved sets. The first method normalizes RSV's using a query self-retrieval score. The second, which requires training data, uses logistic regression to convert RSV's into estimates of probability of relevance. Overall, our results are highly competitive with those of other participants. 
Abstract-found: 1
Intro-found: 1
Reference: [BuAS94] <author> Buckley, C., Allan, J. & Salton, G. </author> <title> Automatic Routing and Ad Hoc Retrieval using SMART: </title> <booktitle> TREC2. In: The Second Text REtrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 45-55. </pages>
Reference-contexts: Effectiveness of our retrieval algorithm seems to plateau at the high end of the expansion. The behavior is quite similar to what we observed in our TREC-2 'further' experiments. We did not perform massive query expansion as done in <ref> [BuAS94] </ref>.
Reference: [CCG94] <author> Cooper, W.S., Chen, A. & Gey, F.C. </author> <title> Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression. </title> <booktitle> In: The Second Text Retrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 57-65. </pages>
Reference-contexts: Batch1 retrieved 14,306 documents which were not judged for relevance, and estimates that 2,118 of these are in fact relevant. Thus the effectiveness of Batch1, as well as of all the thresholding methods, is understated. Batch1's 15% overestimation is similar to that observed by Cooper <ref> [CCG94] </ref> when applying a different logistic model to the TREC-2 data. A plausible explanation is that the training data, since it consists of best-ranked documents from a number of systems, contains more relevant documents than a random sample with the same distribution of RSV's.
Reference: [DuHa73] <author> Duda, R.O. & Hart, P.E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York, </address> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: set B, and under the TREC-3 rules there would be no way to distinguish the two test sets and thus, in general, no way to meet the goal of the thresholding experiment given the TREC-3 rules. (It is worth noting that there are effectiveness measures, notably decision theoretic loss measures <ref> [DuHa73] </ref>, which could have been optimized under the TREC-3 restrictions.) 5.2 Approach Despite the problems with the thresholding evaluation, we chose to test several approaches to it. A common goal of these approaches was to normalize the PIRCS RSV's so that a threshold could be determined automatically for each topic.
Reference: [Harm94] <author> Harman, D.K. </author> <title> Overview of the Second Text REtrieval Conference (TREC-2). </title> <booktitle> In: The Second Text REtrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 1-20. </pages>
Reference-contexts: It can be seen that both pircs1 and pircs2 results outperform the median. Since different sites make use of different techniques and methodologies, it would be interesting to see for each query which site has done best as tabulated in <ref> [Harm94] </ref> and why. We also introduce in TREC-2 MAXI-retrieval as a hypothetical system that returns the best performance for each query among all sites.
Reference: [Jaco94] <author> Jacobs, </author> <title> P.S. GE in TREC-2: Results of a boolean approximation method for routing and retrieval. </title> <booktitle> In: The Second Text Retrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 191-199. </pages>
Reference: [KwGr94a] <author> Kwok, K.L. & Grunfeld, L. </author> <title> Learning from relevant documents in large scale routing retrieval. </title> <booktitle> In: Proc. Human Language Technology Workshop, ARPA, </booktitle> <pages> Mar 8-11, </pages> <address> 1994 Plainsboro, NJ. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1994, </year> <pages> pp. 358-363. </pages>
Reference-contexts: The actual process of training on our network is unchanged from TREC-2. 4.2 Results Comparison of our two approaches for routing (see the appendix of this volume) shows that training by short relevant documents only (pircs4) is both effective and efficient, as observed previously <ref> [KwGr94a] </ref>. It gives an average precision of 0.3749, R-precision of 0.3899 and rel-ret at 1000 documents of 7318 (78.24% of all relevants).
Reference: [KwGr94b] <author> Kwok, K.L. & Grunfeld, L. </author> <title> TREC-2 Document retrieval experiments using PIRCS. </title> <booktitle> In: The Second Text REtrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 233-242. </pages>
Reference-contexts: This is an advantage of segmentation because using the top whole document may add noise in the learning process if the document happens to be long and most of it is irrelevant. However, a ranking operation is required. In <ref> [KwGr94b] </ref> we also show that the usual feedback strategy of using the x best-ranked items for training can lead to inferior retrieval results.
Reference: [Kwok90] <author> Kwok, </author> <title> K.L (1990). Experiments with a component theory of probabilistic information retrieval based on single terms as document components. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 8 </volume> <pages> 363-386. </pages>
Reference-contexts: Our conclusion is in Section 6. 2. PIRCS' DESIGN 2.1 Effectiveness Aspects PIRCS is based on the probabilistic indexing and retrieval models of [MaKu60, RoSp76] but extended with the concept of document components <ref> [Kwok90] </ref>. Both documents and queries are represented as lists of independent conceptual components that are approximated with content terms. These terms can be single words or two-word phrases and are obtained automatically from the corpus. Additionaly, queries can also have a Boolean expression structure.
Reference: [Kwok85] <author> Kwok,K.L (1985). </author> <title> A probabilistic theory of indexing and similarity measure based on cited and citing documents. </title> <journal> J. of American Society for Information Science, </journal> <volume> 36(5) </volume> <pages> 342-351. </pages>
Reference-contexts: In practice, so many approximations are made that the raw RSV's are only useful for ranking. 5.2.1 Normalization Using Query Self-Relevance Our first approach to normalizing RSV's relies on the property of 'self-relevance' as discussed in <ref> [Kwok85] </ref>. A query describes what a user needs. Suppose there were a 'document' identical to the query. We can presume this hypothetical document would be relevant to the query. Thus, given a query we always have one 'relevant document' as reference.
Reference: [Kwok9x] <author> Kwok, </author> <title> K.L (199x). A network approach to probabilistic information retrieval. </title> <journal> ACM Transactions on Office Information Systems, </journal> <note> to appear. </note>
Reference-contexts: With these learning capabilities, the network behaves like a 2-layer neural network with adaptive architecture <ref> [Kwok9x] </ref>. 2.2 Efficiency Aspects PIRCS is also unique in its processing design [KwGr94].
Reference: [KwPK93] <author> Kwok, K.L., Papadopolous, L & Kwan, Y.Y. </author> <title> Retrieval experiments with a large collection using PIRCS. </title> <booktitle> In: The First Text REtrieval Conference (TREC-1). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-207, </note> <year> 1993, </year> <pages> pp. 153-172. </pages>
Reference-contexts: In this process we let the system do some learning and query expansion from a few of the best-ranked sub-documents of each query. Ad-hoc is an initial retrieval and therefore no known relevant documents are available for training. However, previous experiments <ref> [KwPK93, KwGr94] </ref> have shown that PIRCS' ad-hoc operation can achieve average precision close to 60% in the first 5 to 10 retrieved. Hence such feedback without user judgments could be useful, and some of our limited experiments showed that it was indeed the case. <p> For TREC-3, we segment long documents and documents with multiple unrelated stories as previously, but use a larger chunk size of 550 words instead of 360 <ref> [KwPK93, KwGr94] </ref>. This saves some space by reducing the number of our network nodes. Also, if we get comparably good results as in our previous TREC-2 'further' experiments, it can be argued that segmentation size is not critical.
Reference: [LeGa94] <author> Lewis, </author> <title> D.D. & Gale, W.A. A sequential algorithm for training text classifiers. </title> <booktitle> In: SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. </booktitle> <editor> Croft, W.B. and van Rijsbergen, C.J. (Eds.). </editor> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1994, </year> <pages> pp. 3-12. </pages>
Reference-contexts: a pair of parameters a and b for each topic, such that exp (a + b * RSV) 1 + exp (a + b * RSV) (where exp is the exponential function) should be a good estimate of P (T|D), the probability that document D is relevant to the topic <ref> [LeGa94] </ref>. The routing system can then generate an estimate of P (T|D) for new documents by first computing an RSV for the document and then applying the above formula with the appropriate a and b for that topic. Note that unlike Self1, this method requires relevance judgments.
Reference: [LPY94] <author> Liddy, E.D., Park, W. and Yu, E.S. </author> <year> (1994). </year> <title> Text categorization for multiple users based on semantic features from a machine-readable dictionary. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 12(3) </volume> <pages> 278-295. </pages>
Reference-contexts: The threshold of 0.5 was chosen arbitrarily, and could be improved by learning from other queries, or from relevance judgments on a particular query. A related strategy, based on the score of the top ranked document rather than a self-retrieval value, has been used elsewhere <ref> [LPY94] </ref>. 5.2.2 Normalization Using Logistic Regression For our other approaches, the normalized RSV was an explicit estimate of the probability of relevance. We began by finding, for each topic, the RSV for all training documents which were judged for relevance to that topic.
Reference: [MaKu60] <author> Maron M.E & Kuhns, </author> <title> L.J (1960). On relevance, probabilistic indexing and information retrieval. </title> <editor> J. </editor> <booktitle> ACM 7 </booktitle> <pages> 216-244. </pages>
Reference-contexts: Sections 3 and 4 discuss our ad-hoc and routing experiments respectively, while Section 5 is on thresholding. Our conclusion is in Section 6. 2. PIRCS' DESIGN 2.1 Effectiveness Aspects PIRCS is based on the probabilistic indexing and retrieval models of <ref> [MaKu60, RoSp76] </ref> but extended with the concept of document components [Kwok90]. Both documents and queries are represented as lists of independent conceptual components that are approximated with content terms. These terms can be single words or two-word phrases and are obtained automatically from the corpus.
Reference: [RoSp76] <author> Robertson, S.E. & Sparck Jones, K. </author> <year> (1976). </year> <title> Relevance weighting of search terms. </title> <journal> J. of American Society for Information Science, </journal> <volume> 27 </volume> <pages> 129-146. </pages>
Reference-contexts: Sections 3 and 4 discuss our ad-hoc and routing experiments respectively, while Section 5 is on thresholding. Our conclusion is in Section 6. 2. PIRCS' DESIGN 2.1 Effectiveness Aspects PIRCS is based on the probabilistic indexing and retrieval models of <ref> [MaKu60, RoSp76] </ref> but extended with the concept of document components [Kwok90]. Both documents and queries are represented as lists of independent conceptual components that are approximated with content terms. These terms can be single words or two-word phrases and are obtained automatically from the corpus.
Reference: [ToAp94] <author> Tong, </author> <title> R.M. & Appelbaum, L.A. Machine learning for knowledge-based document routing (a report on the TREC-2 experiment). </title> <booktitle> In: The Second Text Retrieval Conference (TREC-2). Harman, </booktitle> <address> D.K. (Ed.). </address> <note> NIST Special Publication 500-215, </note> <year> 1994, </year> <pages> pp. 253-264. </pages>
References-found: 16

