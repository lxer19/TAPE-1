URL: ftp://ftp.cs.rochester.edu/pub/u/rao/papers/space-time.ps.Z
Refering-URL: http://www.cs.rochester.edu/users/faculty/dana/index.html
Root-URL: 
Email: frao,danag@cs.rochester.edu  
Title: Efficient Encoding of Natural Time Varying Images Produces Oriented Space-Time Receptive Fields  
Author: Rajesh P. N. Rao and Dana H. Ballard 
Address: Rochester, NY 14627  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Technical Report 97.4 National Resource Laboratory for the Study of Brain and Behavior Department of Computer Science, University of Rochester August 1997 Abstract The receptive fields of neurons in the mammalian primary visual cortex are oriented not only in the domain of space, but in most cases, also in the domain of space-time. While the orientation of a receptive field in space determines the selectivity of the neuron to image structures at a particular orientation, a receptive field's orientation in space-time characterizes important additional properties such as velocity and direction selectivity. Previous studies have focused on explaining the spatial receptive field properties of visual neurons by relating them to the statistical structure of static natural images. In this report, we examine the possibility that the distinctive spatiotemporal properties of visual cortical neurons can be understood in terms of a statistically efficient strategy for encoding natural time varying images. We describe an artificial neural network that attempts to accurately reconstruct its spatiotemporal input data while simultaneously reducing the statistical dependencies between its outputs. The network utilizes spatiotemporally summating neurons and learns efficient sparse distributed representations of its spatiotemporal input stream by using recurrent lateral inhibition and a simple threshold nonlinearity for rectification of neural responses. When exposed to natural time varying images, neurons in a simulated network developed localized receptive fields oriented in both space and space-time, similar to the receptive fields of neurons in the primary visual cortex.
Abstract-found: 1
Intro-found: 1
Reference: [ Adelson and Bergen, 1985 ] <author> E.H. Adelson and J. Bergen. </author> <title> Spatiotemporal energy models for the perception of motion. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 2(2):284299, </volume> <year> 1985. </year>
Reference-contexts: The inverse of the slope of the oriented subregions in the space-time receptive field provides an estimate of the neuron's preferred velocity (see, for example, <ref> [ Adelson and Bergen, 1985 ] </ref> ). Thus, in the case of the top two rows in the figure, the slope is approximately 1 indicating a preferred speed of approximately 1 pixel/time step for these two neurons in their respective directions. <p> Note that orientation in x-t space indicates the neuron's preferred direction of motion (in this case, upwards or downwards). In addition, the slope of approximately 1 in both cases indicates a preferred speed of approximately 1 pixel/time step for these neurons (since slope is inverse with the preferred velocity <ref> [ Adelson and Bergen, 1985 ] </ref> ). (b) through (l) show the space-time receptive fields for the model neurons in Figure 8 (b) through (l).
Reference: [ Adelson and Movshon, 1982 ] <author> E.H. Adelson and J.A. Movshon. </author> <title> Phenomenal coherence of moving visual patterns. </title> <booktitle> Nature, </booktitle> <address> 300:523525, </address> <year> 1982. </year>
Reference-contexts: Note that even though the training image window moved at 1 pixel/time step, in some cases, such as (d) and (h), the preferred speed is less than 1 pixel/time step due to the well-known aperture effect <ref> [ Adelson and Movshon, 1982 ] </ref> . In the extreme case of an approximately stationary receptive field as in (l), the space-time receptive field indicates a preferred speed of zero. <p> In some cases, such as (d) and (h), the preferred speed is less than 1 pixel/time step due to the aperture effect <ref> [ Adelson and Movshon, 1982 ] </ref> . In the case of (l), the preferred speed is zero. 17 network and a companion network without the lateral inhibitory weights L to a sequence of images depicting a bright vertical bar moving to the right on a dark background (Figure 10).
Reference: [ Albrecht and Geisler, 1991 ] <author> D.G. Albrecht and W.S. Geisler. </author> <title> Motion sensitivity and the contrast-response function of simple cells in the visual cortex. Visual Neurosci., </title> <address> 7:531546, </address> <year> 1991. </year>
Reference-contexts: A possibility that we are currently investigating is to use a Taylor series expansion of an image in both space and time, and to ascertain whether such a strategy produces separable filters that compute derivatives in both space and time. Other issues being pursued include explaining contrast normalization effects <ref> [ Albrecht and Geisler, 1991; Heeger, 1991 ] </ref> and recasting the hierarchical framework proposed in [ Rao and Ballard, 1997a ] to accommodate the spatiotemporal generative model proposed herein.
Reference: [ Atick and Redlich, 1992 ] <author> J.J. </author> <title> Atick and A.N. Redlich. What does the retina know about natural scenes? Neural Computation, </title> <address> 4(2):196210, </address> <year> 1992. </year>
Reference-contexts: An attractive approach to understanding the receptive field properties of visual neurons is to relate them to the statistical structure of natural images. Motivated by the property that natural images possess a 1=f 2 power spectrum [ Field, 1987 ] , Atick and Redlich <ref> [ Atick, 1992; Atick and Redlich, 1992 ] </ref> provided an explanation of the center-surround structure of retinal ganglion receptive fields in terms of whitening or decorrelation of outputs in response to natural images. <p> The images were of size 484fi484 pixels with grayscale pixel values between 0 and 255. Each image was preprocessed by filtering with a circularly symmetrical zero-phase whitening/low-pass filter with the spatial frequency profile <ref> [ Olshausen and Field, 1996; Atick and Redlich, 1992 ] </ref> : K (f ) = f e (f=f 0 ) 4 where the cut-off frequency f 0 = 200 cycles/image. <p> The corresponding spatial profile obtained via inverse Fourier transform is shown in Figure 3D. The spatial profile resembles the well-known center-surround receptive fields characteristic of retinal ganglion cells. Atick and Redlich <ref> [ Atick and Redlich, 1992; Atick, 1992 ] </ref> have shown that the measured spatial frequency profiles of retinal ganglion cells are well approximated by filters resembling K (f ). Figure 3E shows the results of filtering an image from the training set using K (f ).
Reference: [ Atick and Redlich, 1993 ] <author> J.J. </author> <title> Atick and A.N. Redlich. Convergent algorithm for sensory receptive field development. </title> <booktitle> Neural Computation, </booktitle> <address> 5:4560, </address> <year> 1993. </year>
Reference: [ Atick, 1992 ] <author> J.J. Atick. </author> <title> Could information theory provide an ecological theory of sensory processing. Network, </title> <address> 3:213251, </address> <year> 1992. </year>
Reference-contexts: An attractive approach to understanding the receptive field properties of visual neurons is to relate them to the statistical structure of natural images. Motivated by the property that natural images possess a 1=f 2 power spectrum [ Field, 1987 ] , Atick and Redlich <ref> [ Atick, 1992; Atick and Redlich, 1992 ] </ref> provided an explanation of the center-surround structure of retinal ganglion receptive fields in terms of whitening or decorrelation of outputs in response to natural images. <p> The corresponding spatial profile obtained via inverse Fourier transform is shown in Figure 3D. The spatial profile resembles the well-known center-surround receptive fields characteristic of retinal ganglion cells. Atick and Redlich <ref> [ Atick and Redlich, 1992; Atick, 1992 ] </ref> have shown that the measured spatial frequency profiles of retinal ganglion cells are well approximated by filters resembling K (f ). Figure 3E shows the results of filtering an image from the training set using K (f ).
Reference: [ Baddeley and Hancock, 1991 ] <author> R.J. Baddeley and P.J.B. Hancock. </author> <title> A statistical analysis of natural images matches psychophysically derived orientation tuning curves. </title> <journal> Proc. R. Soc. Lond. Ser. B, </journal> <volume> 246:219223, </volume> <year> 1991. </year>
Reference-contexts: Although the PCA of natural images produces lower order components that resemble oriented filters <ref> [ Baddeley and Hancock, 1991; Hancock et al., 1992 ] </ref> , the higher order components are unlike any known neural receptive field profiles. In addition, the receptive fields obtained are global rather than localized feature detectors.
Reference: [ Barlow, 1961 ] <author> H.B. Barlow. </author> <title> Possible principles underlying the transformation of sensory messages. In W.A. Rosenblith, editor, </title> <type> Sensory Communication, </type> <pages> pages 217234. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1961. </year> <month> 22 </month>
Reference-contexts: Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images [ Bell and Sejnowski, 1997 ] . These algorithms are all based directly or indirectly on Barlow's principle of redundancy reduction <ref> [ Barlow, 1961; 1972; 1989; 1994 ] </ref> , where the goal is to learn feature detectors whose outputs are as statistically independent as possible. The underlying motivation is that sensory inputs such as images are generally comprised of a set of independent objects or features whose components are highly correlated.
Reference: [ Barlow, 1972 ] <author> H.B. Barlow. </author> <title> Single units and cognition: A neurone doctrine for perceptual psychology. </title> <journal> Perception, </journal> <volume> 1:371394, </volume> <year> 1972. </year>
Reference: [ Barlow, 1989 ] <author> H.B. Barlow. </author> <title> Unsupervised learning. </title> <booktitle> Neural Computation, </booktitle> <address> 1:295311, </address> <year> 1989. </year>
Reference: [ Barlow, 1994 ] <author> H.B. Barlow. </author> <title> What is the computational goal of the neocortex? In C. </title> <editor> Koch and J.L. Davis, editors, </editor> <booktitle> Large-Scale Neuronal Theories of the Brain, </booktitle> <pages> pages 122. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: [ Barrow, 1987 ] <author> H.G. Barrow. </author> <title> Learning receptive fields. </title> <booktitle> In Proceedings of the IEEE Int. Conf. on Neural Networks, </booktitle> <pages> pages 115121, </pages> <year> 1987. </year>
Reference: [ Bell and Sejnowski, 1997 ] <author> A.J. Bell and T.J. Sejnowski. </author> <title> The `independent components' of natural scenes are edge filters. Vision Research (in press), </title> <year> 1997. </year>
Reference-contexts: Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images <ref> [ Bell and Sejnowski, 1997 ] </ref> . These algorithms are all based directly or indirectly on Barlow's principle of redundancy reduction [ Barlow, 1961; 1972; 1989; 1994 ] , where the goal is to learn feature detectors whose outputs are as statistically independent as possible.
Reference: [ Bienenstock et al., 1982 ] <author> E. L. Bienenstock, L. N. Cooper, and P. W. Munro. </author> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <editor> J. Neurosci., 2:3248, </editor> <year> 1982. </year>
Reference: [ Bryson and Ho, 1975 ] <author> A.E. Bryson and Y.-C. Ho. </author> <title> Applied Optimal Control. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: The resulting stochastic generative model becomes: I = U r + n (5) If a zero mean Gaussian noise process n with unit covariance is assumed, one can show that E 1 is the negative log likelihood of generating the input I (see, for example, <ref> [ Bryson and Ho, 1975 ] </ref> ). Thus, minimizing E 1 is equivalent to maximizing the likelihood of the observed data.
Reference: [ Burr et al., 1986 ] <author> D.C. Burr, J. Ross, and M.C. Morrone. </author> <title> Seeing objects in motion. </title> <journal> Proc. R. Soc. Lond. Ser. B, </journal> <volume> 227:249265, </volume> <year> 1986. </year>
Reference: [ Daugman, 1980 ] <author> J.G. Daugman. </author> <title> Two-dimensional spectral analysis of cortical receptive field profiles. </title> <booktitle> Vision Research, </booktitle> <address> 20:847856, </address> <year> 1980. </year>
Reference-contexts: The synaptic profiles at each time step resemble oriented Gabor wavelets <ref> [ Daugman, 1980; Marcelja, 1980 ] </ref> . (C) depicts these synaptic weights using a classical 2D receptive field representation, where the dark regions are inhibitory and the bright regions are excitatory. <p> The receptive fields after learning resemble localized Gabor wavelets which have previously been shown to well approximate the receptive field weighting profiles of simple cells in the mammalian primary visual cortex <ref> [ Daugman, 1980; Marcelja, 1980; Olshausen and Field, 1996 ] </ref> . In addition, the model neuron can be seen to be tuned towards dark bars moving diagonally from the bottom left corner of the receptive field to the top right corner.
Reference: [ Daugman, 1988 ] <author> J.G. Daugman. </author> <title> Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression. </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Proc., </journal> <volume> 36(7):11691179, </volume> <year> 1988. </year>
Reference: [ Dayan et al., 1995 ] <author> P. Dayan, G.E. Hinton, R.M. Neal, </author> <title> and R.S. Zemel. The Helmholtz machine. </title> <booktitle> Neural Computation, </booktitle> <address> 7:889904, </address> <year> 1995. </year>
Reference: [ DeAngelis et al., 1993a ] <author> G.C. DeAngelis, I. Ohzawa, and R.D. Freeman. </author> <title> Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. I. General characteristics and postnatal development. </title> <editor> J. Neurophysiol., 69(4):10911117, </editor> <year> 1993. </year>
Reference: [ DeAngelis et al., 1993b ] <author> G.C. DeAngelis, I. Ohzawa, and R.D. Freeman. </author> <title> Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. II. Linearity of temporal and spatial summation. </title> <editor> J. Neurophysiol., 69(4):10911117, </editor> <year> 1993. </year>
Reference: [ DeAngelis et al., 1995 ] <author> G.C. DeAngelis, I. Ohzawa, and R.D. Freeman. </author> <title> Receptive-field dynamics in the central visual pathways. </title> <booktitle> Trends in Neuroscience, </booktitle> <address> 18:451458, </address> <year> 1995. </year>
Reference-contexts: In recent years, new mapping techniques have allowed the characterization of receptive fields in both space and time [ Emerson et al., 1987; McLean and Palmer, 1989; Shapley et al., 1992; DeAngelis et al., 1993a ] (see <ref> [ DeAngelis et al., 1995 ] </ref> for a review). The new mapping results indicate that in most cases, the receptive field of a visual neuron changes over time. <p> The consideration of only the past k inputs rather than the entire input history in the equation above is consistent with the observation that cortical neurons process stimuli within restricted temporal epochs of time (see, for example, <ref> [ DeAngelis et al., 1995 ] </ref> ). 4.3 Learning Rules A learning rule for determining the optimal estimate for each U (t) can be obtained by performing gradient descent on E with respect to U (t), for each t = 1; : : : ; k: _ U (t) = 2
Reference: [ Dempster et al., 1977 ] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39:138, </volume> <year> 1977. </year> <month> 23 </month>
Reference-contexts: In the case where the input consists of batch data, as in Section 4.1, one may alternate between the optimization of r for fixed U (t) and L, and the optimization of U (t) and L for fixed r, thereby implementing a form of the expectation-maximization (EM) algorithm <ref> [ Dempster et al., 1977 ] </ref> . In the case of on-line data, which is considered in Section 4.2, the optimization of r occurs simultaneously with U (t) and L.
Reference: [ Dong and Atick, 1995 ] <author> D.W. Dong and J.J. Atick. </author> <title> Statistics of natural time-varying images. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <address> 6(3):345358, </address> <year> 1995. </year>
Reference-contexts: In this paper, we explore the possibility that the distinctive spatiotemporal receptive field properties of visual cortical neurons can be understood in terms of a statistically efficient strategy for encoding natural time varying images <ref> [ Eckert and Buchsbaum, 1993; Dong and Atick, 1995 ] </ref> . We describe an artificial neural network that attempts to accurately reconstruct its spatiotemporal input data while simultaneously reducing the statistical dependencies between its outputs, as advocated by the redundancy reduction principle.
Reference: [ Eckert and Buchsbaum, 1993 ] <author> M.P. Eckert and G. Buchsbaum. </author> <title> Efficient encoding of natural time varying images in the early visual system. </title> <journal> Phil. Trans. R. Soc. Lond. B, </journal> <volume> 339:385395, </volume> <year> 1993. </year>
Reference-contexts: In this paper, we explore the possibility that the distinctive spatiotemporal receptive field properties of visual cortical neurons can be understood in terms of a statistically efficient strategy for encoding natural time varying images <ref> [ Eckert and Buchsbaum, 1993; Dong and Atick, 1995 ] </ref> . We describe an artificial neural network that attempts to accurately reconstruct its spatiotemporal input data while simultaneously reducing the statistical dependencies between its outputs, as advocated by the redundancy reduction principle.
Reference: [ Emerson et al., 1987 ] <author> R.C. Emerson, M.C. Citron, W.J. Vaughn, and S.A. Klein. </author> <title> Nonlinear directionally selective subunits in complex cells of cat striate cortex. </title> <editor> J. Neurophysiol., 58:3365, </editor> <year> 1987. </year>
Reference: [ Field, 1987 ] <author> D.J. </author> <title> Field. Relations between the statistics of natural images and the response properties of cortical cells. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 4:23792394, </volume> <year> 1987. </year>
Reference-contexts: An attractive approach to understanding the receptive field properties of visual neurons is to relate them to the statistical structure of natural images. Motivated by the property that natural images possess a 1=f 2 power spectrum <ref> [ Field, 1987 ] </ref> , Atick and Redlich [ Atick, 1992; Atick and Redlich, 1992 ] provided an explanation of the center-surround structure of retinal ganglion receptive fields in terms of whitening or decorrelation of outputs in response to natural images.
Reference: [ Field, 1994 ] <author> D.J. </author> <title> Field. What is the goal of sensory coding? Neural Computation, </title> <address> 6:559601, </address> <year> 1994. </year>
Reference-contexts: This is perfectly adequate in the case where the input data clouds are Gaussian and capturing pairwise statistics suffices. However, statistical studies have shown that natural image distributions are highly non-Gaussian and cannot be adequately described using orthogonal bases <ref> [ Field, 1994 ] </ref> . Thus, additional constraints are required in order to guide the optimization process towards solutions that more accurately reflect the input generation process. One way of adding constraints is to take into account the prior distributions of the parameters r and U .
Reference: [ Foldiak, 1990 ] <author> P. Foldiak. </author> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biol. Cybern., </journal> <volume> 64:165170, </volume> <year> 1990. </year>
Reference-contexts: The network learns efficient sparse distributed representations of its spatiotemporal input stream by utilizing lateral inhibition <ref> [ Foldiak, 1990 ] </ref> and a simple threshold nonlinearity for rectification of neural responses [ Lee and Seung, 1997; Hinton and Ghahramani, 1997 ] . <p> The matrix ffL represents the inverse covariance matrix of r. We show in the next section that this choice enforces lateral inhibition among the output neurons, thereby encouraging sparse distributed representations, and leads to an anti-Hebbian learning rule for the lateral weights L equivalent to Foldiak's well-known adaptation rule <ref> [ Foldiak, 1990 ] </ref> . 4 Rao and Ballard, 1997a ] . A given input image I is assumed to be generated by multiplying a basis vector matrix U with a set of hidden causes represented by the spatial response vector r. (B) Spatiotemporal generative model used in this paper. <p> In conjunction with the inhibition in the dynamics of r (Equation 18), the above learning rule can be seen to be equivalent to Foldiak's anti-Hebbian learning rule <ref> [ Foldiak, 1990 ] </ref> , if the diagonal terms of L, which implement self-inhibition, are set to zero. 5 Experimental Results The algorithms derived in the previous section were tested on a set of five digitized natural images from the Ansel Adams Fiat Lux collection at the UCR/California Museum of Photography
Reference: [ Hancock et al., 1992 ] <author> P.J.B. Hancock, R.J. Baddeley, and L.S. Smith. </author> <title> The principal components of natural images. Network, </title> <address> 3:6170, </address> <year> 1992. </year>
Reference-contexts: Although the PCA of natural images produces lower order components that resemble oriented filters <ref> [ Baddeley and Hancock, 1991; Hancock et al., 1992 ] </ref> , the higher order components are unlike any known neural receptive field profiles. In addition, the receptive fields obtained are global rather than localized feature detectors.
Reference: [ Harpur and Prager, 1996 ] <author> G.F. Harpur and R.W. Prager. </author> <title> Development of low-entropy coding in a recurrent network. Network, </title> <address> 7:277284, </address> <year> 1996. </year>
Reference-contexts: Recently, Olshausen and Field showed that a neural network that includes the additional constraint of maximizing the sparseness of the distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also <ref> [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] </ref> and related work on projection pursuit [ Huber, 1985 ] based learning methods [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] ). <p> Our approach utilizes a spatiotemporal generative model that can be viewed as a simple extension of the spatial generative model used by Harpur and Prager <ref> [ Harpur and Prager, 1996 ] </ref> , Olshausen and Field [ Olshausen and 2 Field, 1996 ] , Rao and Ballard [ Rao and Ballard, 1997a ] , and others.
Reference: [ Hartline, 1940 ] <author> H.K. Hartline. </author> <title> The receptive fields of optic nerve fibers. Am. </title> <editor> J. Physiol., 130:690699, </editor> <year> 1940. </year>
Reference-contexts: The receptive field of a neuron is classically defined as the area of visual space within which stimuli such as bars or edges can elicit responses fl This research was supported by NIH/PHS research grant 1-P41-RR09283. 1 from the neuron <ref> [ Hartline, 1940 ] </ref> . Although they are a function of both space and time, early depictions of visual receptive fields were confined to spatial coordinates.
Reference: [ Heeger, 1991 ] <author> D.J. Heeger. </author> <title> Non-linear model of neural responses in cat visual cortex. </title> <booktitle> In Computational models of visual processing, </booktitle> <pages> pages 119133. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: A possibility that we are currently investigating is to use a Taylor series expansion of an image in both space and time, and to ascertain whether such a strategy produces separable filters that compute derivatives in both space and time. Other issues being pursued include explaining contrast normalization effects <ref> [ Albrecht and Geisler, 1991; Heeger, 1991 ] </ref> and recasting the hierarchical framework proposed in [ Rao and Ballard, 1997a ] to accommodate the spatiotemporal generative model proposed herein.
Reference: [ Hinton and Ghahramani, 1997 ] <author> G.E. Hinton and Z. Ghahramani. </author> <title> Generative models for discovering sparse distributed representations. </title> <journal> Phil. Trans. Roy. Soc. Lond. B, </journal> <note> 1997. To appear. </note>
Reference-contexts: The network learns efficient sparse distributed representations of its spatiotemporal input stream by utilizing lateral inhibition [ Foldiak, 1990 ] and a simple threshold nonlinearity for rectification of neural responses <ref> [ Lee and Seung, 1997; Hinton and Ghahramani, 1997 ] </ref> . <p> j ) to be non-negative, acknowledging the fact that the firing rate of a neuron cannot be negative. 1 The non-negativity constraint is especially attractive in information coding terms since it causes an infinite density at 0 for the rectified r j and a consequent low coding cost at 0 <ref> [ Hinton and Ghahramani, 1997 ] </ref> , which encourages sparseness among the outputs of the network. 1 We are overlooking the possibility that a single neuron can signal both positive and negative quantities by raising or lowering its firing rate with respect to a fixed background firing rate corresponding to zero.
Reference: [ Hinton and Sejnowski, 1986 ] <author> G.E. Hinton and T.J. Sejnowski. </author> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 7, </volume> <pages> pages 282317. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference: [ Hubel and Wiesel, 1962 ] <author> D.H. Hubel and T.N. Wiesel. </author> <title> Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex. </title> <journal> Journal of Physiology (London), </journal> <volume> 160:106154, </volume> <year> 1962. </year>
Reference-contexts: 1 Introduction Since the seminal experiments of Hubel and Wiesel over 30 years ago <ref> [ Hubel and Wiesel, 1962; 1968 ] </ref> , it has been known that neurons in the mammalian primary visual cortex respond selectively to stimuli such as edges or bars at particular orientations. In many cases, the neurons are directionally selective i.e. they respond only to motion in a particular direction.
Reference: [ Hubel and Wiesel, 1968 ] <author> D.H. Hubel and T.N. Wiesel. </author> <title> Receptive fields and functional architecture of monkey striate cortex. </title> <journal> Journal of Physiology (London), </journal> <volume> 195:215243, </volume> <year> 1968. </year>
Reference: [ Huber, 1985 ] <author> P.J. Huber. </author> <title> Projection pursuit (with discussion). </title> <journal> Annals of Statistics, </journal> <volume> 13:435525, </volume> <year> 1985. </year>
Reference-contexts: constraint of maximizing the sparseness of the distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] and related work on projection pursuit <ref> [ Huber, 1985 ] </ref> based learning methods [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] ). Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images [ Bell and Sejnowski, 1997 ] .
Reference: [ Intrator, 1992 ] <author> N. Intrator. </author> <title> Feature extraction using an unsupervised neural network. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):98107, </address> <year> 1992. </year>
Reference-contexts: distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] and related work on projection pursuit [ Huber, 1985 ] based learning methods <ref> [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] </ref> ). Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images [ Bell and Sejnowski, 1997 ] .
Reference: [ Jordan and Rumelhart, 1992 ] <author> M.I. Jordan and D.E. Rumelhart. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <booktitle> Cognitive Science, </booktitle> <address> 16:307354, </address> <year> 1992. </year> <month> 24 </month>
Reference: [ Kalman, 1960 ] <author> R.E. </author> <title> Kalman. A new approach to linear filtering and prediction theory. </title> <journal> Trans. ASME J. Basic Eng., </journal> <volume> 82:3545, </volume> <year> 1960. </year>
Reference-contexts: neural coding [ Hinton and Sejnowski, 1986; Jordan and Rumelhart, 1992; Zemel, 1994; Dayan et al., 1995; Hinton and Ghahramani, 1997 ] , although the roots of the approach can be traced back to early ideas in control theory such as Wiener filtering [ Wiener, 1949 ] and Kalman filtering <ref> [ Kalman, 1960 ] </ref> . In this section, we first consider a class of spatial generative models that have previously been used in the neural modeling literature for explaining spatial receptive field properties [ Harpur and Prager, 1996; Olshausen and Field, 1996; Rao and Ballard, 1997a ] .
Reference: [ Law and Cooper, 1994 ] <author> C.C. Law and L.N. Cooper. </author> <title> Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, 91:77977801, </institution> <year> 1994. </year>
Reference-contexts: distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] and related work on projection pursuit [ Huber, 1985 ] based learning methods <ref> [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] </ref> ). Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images [ Bell and Sejnowski, 1997 ] .
Reference: [ Lee and Seung, 1997 ] <author> D.D. Lee and H.S. Seung. </author> <title> Unsupervised learning by convex and conic coding. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: The network learns efficient sparse distributed representations of its spatiotemporal input stream by utilizing lateral inhibition [ Foldiak, 1990 ] and a simple threshold nonlinearity for rectification of neural responses <ref> [ Lee and Seung, 1997; Hinton and Ghahramani, 1997 ] </ref> . <p> It is interesting to note the similarity between the dynamics as given by the stochastic gradient descent rule above and those proposed by Lee and Seung for their Conic network <ref> [ Lee and Seung, 1997 ] </ref> . In particular, the above equation can regarded as a spatiotemporal extension of the dynamics used in the Conic network.
Reference: [ Linsker, 1988 ] <author> R. Linsker. </author> <title> Self-organization in a perceptual network. </title> <booktitle> Computer, </booktitle> <address> 21(3):105117, </address> <year> 1988. </year>
Reference: [ Maex and Orban, 1996 ] <author> R. Maex and G.A. Orban. </author> <title> Model circuit of spiking neurons generating directional selectivity in simple cells. </title> <editor> J. Neurophysiol., 75(4):15151545, </editor> <year> 1996. </year>
Reference: [ Marcelja, 1980 ] <author> S. Marcelja. </author> <title> Mathematical description of the responses of simple cortical cells. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 70:12971300, </volume> <year> 1980. </year>
Reference-contexts: The synaptic profiles at each time step resemble oriented Gabor wavelets <ref> [ Daugman, 1980; Marcelja, 1980 ] </ref> . (C) depicts these synaptic weights using a classical 2D receptive field representation, where the dark regions are inhibitory and the bright regions are excitatory. <p> The receptive fields after learning resemble localized Gabor wavelets which have previously been shown to well approximate the receptive field weighting profiles of simple cells in the mammalian primary visual cortex <ref> [ Daugman, 1980; Marcelja, 1980; Olshausen and Field, 1996 ] </ref> . In addition, the model neuron can be seen to be tuned towards dark bars moving diagonally from the bottom left corner of the receptive field to the top right corner.
Reference: [ McLean and Palmer, 1989 ] <author> J. McLean and L.A. Palmer. </author> <title> Contribution of linear spatiotemporal receptive field structure to velocity selectivity of simple cells in the cat's striate cortex. </title> <journal> Vision Research, </journal> <volume> 29:675 679, </volume> <year> 1989. </year>
Reference: [ McLean et al., 1994 ] <author> J. McLean, S. Raab, and L.A. Palmer. </author> <title> Contribution of linear mechanisms to the specification of local motion by simple cells in areas 17 and 18 of the cat. Visual Neurosci., </title> <address> 11:271294, </address> <year> 1994. </year>
Reference: [ Mineiro and Zipser, 1997 ] <author> P. Mineiro and D. Zipser. </author> <title> Analysis of direction selectivity arising from recurrent cortical interactions. </title> <type> Technical Report 97.03, </type> <institution> Dept. of Cog. Science, UCSD, </institution> <year> 1997. </year>
Reference: [ Mumford, 1994 ] <author> D. Mumford. </author> <title> Neuronal architectures for pattern-theoretic problems. </title> <editor> In C. Koch and J.L. Davis, editors, </editor> <booktitle> Large-Scale Neuronal Theories of the Brain, </booktitle> <pages> pages 125152. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: [ Oja, 1989 ] <author> E. Oja. </author> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1:6168, </volume> <year> 1989. </year>
Reference: [ Olshausen and Field, 1996 ] <author> B.A. </author> <title> Olshausen and D.J. Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <booktitle> Nature, </booktitle> <address> 381:607609, </address> <year> 1996. </year>
Reference-contexts: Recently, Olshausen and Field showed that a neural network that includes the additional constraint of maximizing the sparseness of the distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields <ref> [ Olshausen and Field, 1996 ] </ref> (see also [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] and related work on projection pursuit [ Huber, 1985 ] based learning methods [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] ). <p> One may also note that minimizing E is equivalent to maximizing the posterior probability of input data (maximum a posteriori (MAP) estimation). Specific choices of f and g determine the the nature of internal representations that will be learned. For example, Olshausen and Field <ref> [ Olshausen and Field, 1996 ] </ref> proposed functions of the form f (x) = log (1 + x 2 ), f (x) = jxj and f (x) = e x 2 to encourage sparseness in r. <p> The images were of size 484fi484 pixels with grayscale pixel values between 0 and 255. Each image was preprocessed by filtering with a circularly symmetrical zero-phase whitening/low-pass filter with the spatial frequency profile <ref> [ Olshausen and Field, 1996; Atick and Redlich, 1992 ] </ref> : K (f ) = f e (f=f 0 ) 4 where the cut-off frequency f 0 = 200 cycles/image. <p> The receptive fields after learning resemble localized Gabor wavelets which have previously been shown to well approximate the receptive field weighting profiles of simple cells in the mammalian primary visual cortex <ref> [ Daugman, 1980; Marcelja, 1980; Olshausen and Field, 1996 ] </ref> . In addition, the model neuron can be seen to be tuned towards dark bars moving diagonally from the bottom left corner of the receptive field to the top right corner.
Reference: [ Olshausen and Field, 1997 ] <author> B.A. </author> <title> Olshausen and D.J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, </title> <note> 1997. To appear. </note>
Reference-contexts: As described in <ref> [ Olshausen and Field, 1997 ] </ref> , the whitening component of the filter W (f ) = f performs sphering for natural image data by attenuating the low frequencies and boosting the higher frequencies.
Reference: [ Pearl, 1988 ] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: The network also utilizes other lateral connections which are derived from the generative model. These lateral excitatory/inhibitory connections play a role similar to those used in Bayesian belief networks for facilitating the phenomenon of explaining away <ref> [ Pearl, 1988 ] </ref> . The network additionally includes rectification of outputs to encourage sparseness among the network activities. Although rectification helps in the development of sparse distributed representations, rectification alone was found to be insufficient in producing satisfactory space-time receptive fields.
Reference: [ Pece, 1992 ] <author> A.E.C. Pece. </author> <title> Redundancy reduction of a Gabor representation: a possible computational role for feedback from primary visual cortex to lateral geniculate nucleus. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <pages> pages 865868. </pages> <address> Amsterdam: </address> <publisher> Elsevier Science, </publisher> <year> 1992. </year>
Reference: [ Rao and Ballard, 1997a ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> Dynamic model of visual recognition predicts neural response properties in the visual cortex. </title> <booktitle> Neural Computation, </booktitle> <address> 9(4):721763, </address> <year> 1997. </year>
Reference-contexts: Recently, Olshausen and Field showed that a neural network that includes the additional constraint of maximizing the sparseness of the distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also <ref> [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] </ref> and related work on projection pursuit [ Huber, 1985 ] based learning methods [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] ). <p> Our approach utilizes a spatiotemporal generative model that can be viewed as a simple extension of the spatial generative model used by Harpur and Prager [ Harpur and Prager, 1996 ] , Olshausen and Field [ Olshausen and 2 Field, 1996 ] , Rao and Ballard <ref> [ Rao and Ballard, 1997a ] </ref> , and others. The spatiotemporal generative model al-lows neurons in the network to perform not just a spatial summation of the current input, but a spatiotemporal summation of both current and past inputs over a finite spatiotemporal extent. <p> Alternately, one can use a zero-mean multivariate Gaussian prior on r <ref> [ Rao and Ballard, 1997a ] </ref> to yield the negative log of the prior density: f (r) = ffr T Lr (7) where ff is a positive constant and L denotes a set of lateral weights. The matrix ffL represents the inverse covariance matrix of r. <p> Other issues being pursued include explaining contrast normalization effects [ Albrecht and Geisler, 1991; Heeger, 1991 ] and recasting the hierarchical framework proposed in <ref> [ Rao and Ballard, 1997a ] </ref> to accommodate the spatiotemporal generative model proposed herein.
Reference: [ Rao and Ballard, 1997b ] <author> R.P.N. Rao and D.H. Ballard. </author> <title> Localized receptive fields may mediate transformation-invariant recognition in the visual cortex. </title> <type> Technical Report 97.2, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: The lack of larger numbers of separable receptive fields in the trained networks suggests an alternative mechanism for the generation of such receptive fields. Most cortical receptive fields that are space-time separable have a temporal weighting profile that approximates a derivative in time. We have previously shown <ref> [ Rao and Ballard, 1997b ] </ref> that a generative model based on a first-order Taylor series approximation of an image produces localized oriented filters that compute spatial derivatives for estimating translations in the image plane.
Reference: [ Reid et al., 1991 ] <author> R.C. Reid, R.E. Soodak, and R.M. Shapley. </author> <title> Directional selectivity and spatiotemporal structure of receptive fields of simple cells in cat striate cortex. </title> <editor> J. Neurophysiol., 66:505529, </editor> <year> 1991. </year>
Reference: [ Rissanen, 1989 ] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> Singapore: World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: Thus, the function E can be given an interpretation in terms of the minimum description length (MDL) principle <ref> [ Rissanen, 1989; Zemel, 1994 ] </ref> , namely, that solutions are required not only to be accurate but also to be cheap in terms of coding length. This formalizes the well-known Occam's Razor principle that advocates simplicity over complexity among solutions to a problem.
Reference: [ Sanger, 1989 ] <author> T.D. Sanger. </author> <title> Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks, </title> <address> 2:459473, </address> <year> 1989. </year>
Reference: [ Shapley et al., 1992 ] <author> R.M. Shapley, R.C. Reid, and R. Soodak. </author> <title> Spatiotemporal receptive fields and direction selectivity. In M.S. </title> <editor> Landy and J.A. Movshon, editors, </editor> <booktitle> Computational Models of Visual Processing, </booktitle> <pages> pages 109118. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [ Shouval, 1995 ] <author> H. Shouval. </author> <title> Formation and organisation of receptive fields, with an input environment composed of natural scenes. </title> <type> PhD thesis, </type> <institution> Dept. of Physics, Brown University, </institution> <year> 1995. </year>
Reference-contexts: distribution of output activities develops, when trained on static natural images, synaptic weights with localized, oriented spatial receptive fields [ Olshausen and Field, 1996 ] (see also [ Harpur and Prager, 1996; Rao and Ballard, 1997a ] and related work on projection pursuit [ Huber, 1985 ] based learning methods <ref> [ Intrator, 1992; Law and Cooper, 1994; Shouval, 1995 ] </ref> ). Similar results have also been obtained using an algorithm that extracts the independent components of a set of static natural images [ Bell and Sejnowski, 1997 ] .
Reference: [ Suarez et al., 1995 ] <author> H. Suarez, C. Koch, and R. Douglas. </author> <title> Modeling direction selectivity of simple cells in striate visual cortex with the framework of the canonical microcircuit. </title> <editor> J. Neurosci., 15(10):67006719, </editor> <year> 1995. </year>
Reference: [ Watson and Ahumada, 1985 ] <author> A.B. Watson and A.J. Ahumada. </author> <title> Model of human visual-motion sensing. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 2(2):322341, </volume> <year> 1985. </year>
Reference: [ Wiener, 1949 ] <author> N. Wiener. </author> <title> The Extrapolation, Interpolation, and Smoothing of Stationary Time Series with Engineering Applications. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1949. </year>
Reference-contexts: considerable attention in recent studies pertaining to neural coding [ Hinton and Sejnowski, 1986; Jordan and Rumelhart, 1992; Zemel, 1994; Dayan et al., 1995; Hinton and Ghahramani, 1997 ] , although the roots of the approach can be traced back to early ideas in control theory such as Wiener filtering <ref> [ Wiener, 1949 ] </ref> and Kalman filtering [ Kalman, 1960 ] .
Reference: [ Williams, 1985 ] <author> R.J. Williams. </author> <title> Feature discovery through error-correction learning. </title> <type> Technical Report 8501, </type> <institution> Institute for Cognitive Science, University of California at San Diego, </institution> <year> 1985. </year>
Reference: [ Zemel, 1994 ] <author> R.S. Zemel. </author> <title> A Minimum Description Length Framework for Unsupervised Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1994. </year> <month> 26 </month>
Reference-contexts: Thus, the function E can be given an interpretation in terms of the minimum description length (MDL) principle <ref> [ Rissanen, 1989; Zemel, 1994 ] </ref> , namely, that solutions are required not only to be accurate but also to be cheap in terms of coding length. This formalizes the well-known Occam's Razor principle that advocates simplicity over complexity among solutions to a problem.
References-found: 67

