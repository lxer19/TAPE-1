URL: http://www.cs.rice.edu/~rrk/publications/pact97.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Email: frrk,alcg@rice.edu  
Title: on Parallel Architectures and Compilation Techniques Optimally Synchronizing DOACROSS Loops on Shared Memory Multiprocessors  
Author: Ramakrishnan Rajamony and Alan L. Cox 
Address: Houston, TX 77005-1892  
Affiliation: Departments of Electrical Computer Engineering and Computer Science Rice University  
Note: To appear in the 1997 International Conference  
Abstract: We present two algorithms to minimize the amount of synchronization added when parallelizing a loop with loop-carried dependences. In contrast to existing schemes, our algorithms add lesser synchronization, while preserving the parallelism that can be extracted from the loop. Our first algorithm uses an interval graph representation of the dependence "overlap" to find a synchronization placement in time almost linear in the number of dependences. Although this solution may be suboptimal, it is still better than that obtained using existing methods, which first eliminate redundant dependences and then synchronize the remaining ones. Determining the optimal synchronization is an NP-complete problem. Our second algorithm therefore uses integer programming to determine the optimal solution. We first use a polynomial-time algorithm to find a minimal search space that must contain the optimal solution. Then, we formulate the problem of choosing the minimal synchronization from the search space as a set-cover problem, and solve it exactly using 0-1 integer programming. We show the performance impact of our algorithms by synchronizing a set of synthetic loops on an 8-processor Convex Exemplar. The greedily synchronized loops ran between 7% and 22% faster than those synchronized by the best existing algorithm. Relative to the same base, the optimally synchronized loops ran between 10% and 22% faster. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bixby. </author> <title> Implementing the simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: We also prune this space, eliminating candidates that cannot be a part of the optimal solution. Then, we formulate the problem of choosing the optimal synchronization from this search space as a set cover problem, and solve it exactly using CPLEX <ref> [1] </ref>, a linear integer programming tool. We describe our techniques only for the case of singly-nested loops with uniform dependences, i.e. where the dependence distances are all constant and known. We do this because the main ideas in this paper are orthogonal to these issues. <p> Even though some redundant acquires are removed in step 3, the number of acquires resulting from the greedy solution may be suboptimal. For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S [2] <ref> [1] </ref> ; S [3] [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S [2] <ref> [1] </ref> ; S [3] [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> By pruning the search space, we can reduce its size, speeding up the subsequent search. Consider figure 5, which shows part of an iteration dependence graph. Here, the dependence between S <ref> [1] </ref> [1] and S [3] [2] can be enforced by the two synchronization edges, a and b. But as both edges originate from the same node, and as edge a renders edge b redundant, we can exclude b from the search space. <p> By pruning the search space, we can reduce its size, speeding up the subsequent search. Consider figure 5, which shows part of an iteration dependence graph. Here, the dependence between S <ref> [1] </ref> [1] and S [3] [2] can be enforced by the two synchronization edges, a and b. But as both edges originate from the same node, and as edge a renders edge b redundant, we can exclude b from the search space. <p> In practice, this will be a lot smaller, because several synchronization edges may not enforce any dependences. 4.3 The Set-Cover Formulation Consider the iteration dependence graph from figure 5. The two primary dependences shown are: * x: from S <ref> [1] </ref> [1] to S [3] [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by <p> In practice, this will be a lot smaller, because several synchronization edges may not enforce any dependences. 4.3 The Set-Cover Formulation Consider the iteration dependence graph from figure 5. The two primary dependences shown are: * x: from S <ref> [1] </ref> [1] to S [3] [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b <p> The two primary dependences shown are: * x: from S <ref> [1] </ref> [1] to S [3] [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where enf by indicates "enforced by". <p> We first illustrate our rationale for favoring the minimization of releases over that of acquires. Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S <ref> [1] </ref> [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. <p> We first illustrate our rationale for favoring the minimization of releases over that of acquires. Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S <ref> [1] </ref> [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S <ref> [1] </ref> [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S <ref> [1] </ref> [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Our second algorithm uses integer programming to exactly determine the optimal synchronization placement. We use a polynomial-time algorithm to construct a search space. Then, we formulate a set cover problem for choosing the optimal synchronization from this search space, and solve it using CPLEX <ref> [1] </ref>, a linear integer programming tool. We use a set of synthetic loops on an 8-processor Convex Exemplar to show the performance impact of minimizing synchronization. The greedily synchronized loops ran between 7% and 22% faster than those synchronized by the best existing algorithm.
Reference: [2] <author> Ding-Kai Chen. </author> <title> Compiler Optimizations for Parallel Loops with Fine-Grained Synchronization. </title> <type> PhD thesis, </type> <institution> University of Illinios, Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: We describe our techniques only for the case of singly-nested loops with uniform dependences, i.e. where the dependence distances are all constant and known. We do this because the main ideas in this paper are orthogonal to these issues. Previous research <ref> [2, 13, 17, 23] </ref> has described how to apply redundant dependence elimination to multiply nested loops, and to cases where the dependences are not uniform. These techniques can be directly applied to extend our work as well. <p> Even though some redundant acquires are removed in step 3, the number of acquires resulting from the greedy solution may be suboptimal. For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S <ref> [2] </ref> [1] ; S [3] [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. <p> For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S <ref> [2] </ref> [1] ; S [3] [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S <ref> [2] </ref> [1] ; S [3] [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> By pruning the search space, we can reduce its size, speeding up the subsequent search. Consider figure 5, which shows part of an iteration dependence graph. Here, the dependence between S [1] [1] and S [3] <ref> [2] </ref> can be enforced by the two synchronization edges, a and b. But as both edges originate from the same node, and as edge a renders edge b redundant, we can exclude b from the search space. By doing so, we will not affect the final minimal solution. <p> In practice, this will be a lot smaller, because several synchronization edges may not enforce any dependences. 4.3 The Set-Cover Formulation Consider the iteration dependence graph from figure 5. The two primary dependences shown are: * x: from S [1] [1] to S [3] <ref> [2] </ref> (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where enf <p> The two primary dependences shown are: * x: from S [1] [1] to S [3] <ref> [2] </ref> (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where enf by indicates "enforced by". <p> We first illustrate our rationale for favoring the minimization of releases over that of acquires. Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S <ref> [2] </ref> [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S <ref> [2] </ref> [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S <ref> [2] </ref> [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Their paper also surveys the existing techniques to eliminate redundant dependences. Ding-Kai Chen presents a modification to the SEODFS algorithm, that enables it to handle synchronization at the statement level [3]. He also addresses two related problems <ref> [2] </ref>. First, he addresses limitations in Midkiff and Padua's extension of redundant dependence elimination for multiply-nested loops. Second, he presents heuristics to reorder the statements in a loop to increase the extractable parallelism. 6 These heuristics are applied in two steps.
Reference: [3] <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> Redundant synchro nization elimination for doacross loops. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 477-481, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: By eliminating redundant dependences, the number of dependences that have to be synchronized is reduced. Surprisingly, all the related methods that we have seen <ref> [3, 13, 15, 17] </ref> first eliminate redundant dependences, and then synchronize all remaining dependences. As we have shown in figure 2, this may not result in the minimal synchronization. <p> Redundant dependences can be identified by performing a transitive closure on 4 This is overkill, but simplifies the algebra. the iteration dependence graph. Chen presents a more efficient algorithm that exploits the regular structure of the dependences <ref> [3] </ref>. We use this to first eliminate the redundant dependences in D, and then place edges for which b a + 1 in D 0 . <p> For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S [2] [1] ; S <ref> [3] </ref> [2] ) and (S [4] [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> By pruning the search space, we can reduce its size, speeding up the subsequent search. Consider figure 5, which shows part of an iteration dependence graph. Here, the dependence between S [1] [1] and S <ref> [3] </ref> [2] can be enforced by the two synchronization edges, a and b. But as both edges originate from the same node, and as edge a renders edge b redundant, we can exclude b from the search space. By doing so, we will not affect the final minimal solution. <p> In practice, this will be a lot smaller, because several synchronization edges may not enforce any dependences. 4.3 The Set-Cover Formulation Consider the iteration dependence graph from figure 5. The two primary dependences shown are: * x: from S [1] [1] to S <ref> [3] </ref> [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where <p> The two primary dependences shown are: * x: from S [1] [1] to S <ref> [3] </ref> [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where enf by indicates "enforced by". <p> The two primary dependences shown are: * x: from S [1] [1] to S <ref> [3] </ref> [2] (indicated by *) * y: from S [2] [1] to S [3] [3] (indicated by ) These dependences can be enforced by the following synchronization edges. * x: edge b or c * y: edge d We can represent this as: Dep x enf by b + c where enf by indicates "enforced by". <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S [2] [2] ), (S [2] [1] ; S <ref> [3] </ref> [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S [2] [2] ), (S [2] [1] ; S <ref> [3] </ref> [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Consider loop (c) from figure 7. If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S [2] [2] ), (S [2] [1] ; S <ref> [3] </ref> [3] ), and (S [3] [1] ; S [4] [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. <p> Their paper also surveys the existing techniques to eliminate redundant dependences. Ding-Kai Chen presents a modification to the SEODFS algorithm, that enables it to handle synchronization at the statement level <ref> [3] </ref>. He also addresses two related problems [2]. First, he addresses limitations in Midkiff and Padua's extension of redundant dependence elimination for multiply-nested loops. Second, he presents heuristics to reorder the statements in a loop to increase the extractable parallelism. 6 These heuristics are applied in two steps.
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <booktitle> Intro duction to Algorithms, </booktitle> <pages> pages 574-575. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachussets, </address> <year> 1990. </year>
Reference-contexts: For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S [2] [1] ; S [3] [2] ) and (S <ref> [4] </ref> [1] ; S [5] [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> The search space is made up of U [ S 0 . From these, we directly obtain equations similar to equation (1). We use an algorithm similar to the directed-path labeling algorithm from <ref> [4] </ref> to enumerate the primary edges corresponding to those making up a path. Since our goal is to determine the minimum number of releases and acquires, we transform equation 1 by replacing each edge by the conjunction of its release and acquire. <p> If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S <ref> [4] </ref> [5] ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization.
Reference: [5] <author> Intel Corporation. </author> <title> Pentium Pro Family Developer's Man ual, </title> <year> 1996. </year>
Reference-contexts: In general, synchronization writes are many times more expensive than synchronization reads. On a release, all preceding writes must be made visible to the other processors before the release itself can be made visible. However, using non-binding hardware prefetch and speculative load execution [9], most current processors <ref> [5, 12, 18] </ref> allow operations following an acquire to speculatively execute. Hence, we favor the minimization of releases over the minimization of acquires. <p> For instance, in figure 7 (b), the algorithm places three acquires in the loop, whereas two suffice. The optimal primary synchronization edges are (S [2] [1] ; S [3] [2] ) and (S [4] [1] ; S <ref> [5] </ref> [2] ). 4.2 An Optimal Algorithm In this section, we describe an integer programming formulation for OPTSYNC, that lets us determine the optimal solution. We decompose this problem into a pruning phase followed by a search phase. <p> If the objective function used in the optimization minimizes the total number of acquires and releases, the minimal primary synchronization edges would be (S [1] [1] ; S [2] [2] ), (S [2] [1] ; S [3] [3] ), and (S [3] [1] ; S [4] <ref> [5] </ref> ). Since the spans of these edges are 1, 2, and 4, they can together cover all 7 dependences. Only three releases and three acquires are required per iteration for this synchronization. In contrast, favoring the minimal number of releases results in one release and seven acquires per iteration.
Reference: [6] <author> R. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinios, Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: We have not, however, shown that 6 By reducing the directed bandwidth problem [7], Cytron has shown that finding the optimal statement order that maximizes the extractable parallelism is NP-complete <ref> [6] </ref>. it is strong NP-complete. We present two algorithms to minimize the synchronization placement. The first is a fast, greedy algorithm, that finds an optimal release placement. Even though the acquire placement it determines may be suboptimal, the solution is still better than that obtained through existing methods.
Reference: [7] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractibility: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: In the next section, we show that the acquire minimization problem is NP-complete. 3.1 Minimizing acquires is NP-Complete We reduce an arbitrary instance of the exact cover by 3-sets (X3C) problem <ref> [7] </ref> to a restricted version of the acquire minimization problem (MINACQ). Specifically, from an instance of X3C, we construct a loop which has only one minimal placement of releases. Note that in general, there may be several minimal release placements (see section 4.1). <p> For convenience, we use n to refer to 3p. We restrict ourselves to instances of X3C where the elements of each C i are distinct. 3 3 Since 3DM <ref> [7] </ref> can be reduced to X3C, this restriction does not affect the NP-completeness of X3C. <p> Although we have shown that minimizing acquires is NP-complete, the reduction does not prove that it is NP-complete in the strong sense. In particular, since the magnitude of the largest number in the transformed problem, O (2 n ), is not polynomially related <ref> [7, pages 92-103] </ref> to the size of the problem instance, O (q), the possibility of a pseudo-polynomial time algorithm to find the minimal number of acquires cannot be ruled out. <p> For singly-nested loops, only ffi max + 1 iterations need to be considered, where ffi max is the largest dependence distance in the loop. While eliminating redundant synchronization is an NP-complete problem, it is not so in the strong sense <ref> [7] </ref>. They present a pseudo-polynomial time algorithm that computes the transitive closure of the controlled path graph to identify the redundant dependences. Midkiff and Padua's algorithm requires the transitive closure to be computed multiple times. <p> We show that even for loops with regular (uniform) dependences, determining the minimal number of acquires for a given placement of releases is an NP-complete problem. We have not, however, shown that 6 By reducing the directed bandwidth problem <ref> [7] </ref>, Cytron has shown that finding the optimal statement order that maximizes the extractable parallelism is NP-complete [6]. it is strong NP-complete. We present two algorithms to minimize the synchronization placement. The first is a fast, greedy algorithm, that finds an optimal release placement.
Reference: [8] <author> F. Gavril. </author> <title> Algorithms for minimum coloring, maximum clique, minimum covering by cliques and maximum independent set of chordal graph. </title> <journal> SIAM Journal on Computing, </journal> <volume> 1(2) </volume> <pages> 180-187, </pages> <year> 1972. </year>
Reference-contexts: Hence, the minimum number of releases can be obtained by finding a minimum cover by cliques of the graph. Although this problem is NP-complete for general graphs, Gavril has described a polynomial-time algorithm to solve this for interval graphs <ref> [8] </ref>. The greedy algorithm we described finds the minimal clique cover by starting at one end of the interval representation of the statement "overlaps". Figure 4 shows the interval graphs resulting from the three iteration dependence graphs in figure 7. <p> Their algorithm is based on solving the minimum cover by cliques problem on a graph representing the dependence intervals that must be synchronized. Although this problem is NP-complete in general, for interval graphs, Gavril has described a polynomial time algorithm <ref> [8] </ref>. 7 Conclusions Synchronization is a significant source of overhead in parallel programs. Here, we present the first approach to inserting the optimal amount of synchronization when parallelizing a loop with loop-carried dependences.
Reference: [9] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Two tech niques to enhance the performance of memory consistency models. </title> <booktitle> In 1991 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: In general, synchronization writes are many times more expensive than synchronization reads. On a release, all preceding writes must be made visible to the other processors before the release itself can be made visible. However, using non-binding hardware prefetch and speculative load execution <ref> [9] </ref>, most current processors [5, 12, 18] allow operations following an acquire to speculatively execute. Hence, we favor the minimization of releases over the minimization of acquires.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Even for loops with regular (uniform) dependences, we show that determining the minimal number of acquires needed for synchronization is an NP-complete problem. An acquire <ref> [10] </ref> is a synchronization read used for gaining access to shared locations. A release [10] is the synchronization write that grants access. These are analogous to wait and post operations, and constitute the basic synchronization mech anism we use. <p> Even for loops with regular (uniform) dependences, we show that determining the minimal number of acquires needed for synchronization is an NP-complete problem. An acquire <ref> [10] </ref> is a synchronization read used for gaining access to shared locations. A release [10] is the synchronization write that grants access. These are analogous to wait and post operations, and constitute the basic synchronization mech anism we use. The key contributions of this paper are two algorithms that minimize synchronization added during parallelization. <p> An acquire operation <ref> [10] </ref> is used to gain access to shared locations. A release operation [10] grants permission for the reads. The hardware typically provides mechanisms to make all writes to shared data that precede a release in program order, globally visible before the release itself is made globally visible. <p> An acquire operation <ref> [10] </ref> is used to gain access to shared locations. A release operation [10] grants permission for the reads. The hardware typically provides mechanisms to make all writes to shared data that precede a release in program order, globally visible before the release itself is made globally visible.
Reference: [11] <author> Martin C. Golumbic. </author> <title> Algorithmic Graph Theory and Per fect Graphs. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1980. </year>
Reference-contexts: The algorithm results in the minimal number of extra releases (i.e. in excess to those in S 0 ) that are required to synchronize the dependences in D 00 . It does so by computing a minimum cover by cliques on an interval graph <ref> [11] </ref> representation of the dependence statement intervals. More specifically, consider the dependences in D 00 for which a minimal placement of releases must be found. Based on these dependences, construct an interval graph as follows. For each dependence, create a node in the graph. For simplicity, assume is 0.
Reference: [12] <author> D. Hunt. </author> <title> Advanced Features of the 64-bit PA-8000. </title> <booktitle> In Proceedings of the '95 CompCon Conference, </booktitle> <year> 1995. </year>
Reference-contexts: In general, synchronization writes are many times more expensive than synchronization reads. On a release, all preceding writes must be made visible to the other processors before the release itself can be made visible. However, using non-binding hardware prefetch and speculative load execution [9], most current processors <ref> [5, 12, 18] </ref> allow operations following an acquire to speculatively execute. Hence, we favor the minimization of releases over the minimization of acquires.
Reference: [13] <author> V. P. Krothapalli and P. Sadayappan. </author> <title> Removal of redun dant dependences in doacross loops with constant dependencies. </title> <booktitle> In Proceedings of the 1991 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: By eliminating redundant dependences, the number of dependences that have to be synchronized is reduced. Surprisingly, all the related methods that we have seen <ref> [3, 13, 15, 17] </ref> first eliminate redundant dependences, and then synchronize all remaining dependences. As we have shown in figure 2, this may not result in the minimal synchronization. <p> We describe our techniques only for the case of singly-nested loops with uniform dependences, i.e. where the dependence distances are all constant and known. We do this because the main ideas in this paper are orthogonal to these issues. Previous research <ref> [2, 13, 17, 23] </ref> has described how to apply redundant dependence elimination to multiply nested loops, and to cases where the dependences are not uniform. These techniques can be directly applied to extend our work as well. <p> and Sadayappan point out that this is not necessary and provide an efficient algorithm (SEODFS) to 5 The +O4 level turns on link level optimization and causes the compiler to move code around the reads and writes we used for synchronization. determine redundant dependences, that can also handle multiply-nested loops <ref> [13] </ref>. However, their algorithm synchronizes at the iteration level as opposed to the statement level synchronization discussed above. Midkiff and Padua point out this drawback, and also discuss how the single loop techniques can be extended to multiple loops [16].
Reference: [14] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Similarly, it provides mechanisms to prevent reads following an acquire (in program order) from being performed until the acquire succeeds. In systems that enforce sequential consistency <ref> [14] </ref>, 2 no special mechanisms are required. Acquires and releases can be implemented as ordinary reads and writes to memory and do not have to be distinguished in any way.
Reference: [15] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> A technique for reducing sunchronization overhead in large scale multiprocessors. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 284-291, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: However, as shown in figure 2 (c), by signaling the write to A after the write to B, the accesses to B need not be explicitly synchronized. This is the key insight motivating this paper. Early work in minimizing synchronization, by Li and Abu-Safah <ref> [15] </ref>, and Midkiff and Padua [17], was on detecting and eliminating redundant dependences. <p> By eliminating redundant dependences, the number of dependences that have to be synchronized is reduced. Surprisingly, all the related methods that we have seen <ref> [3, 13, 15, 17] </ref> first eliminate redundant dependences, and then synchronize all remaining dependences. As we have shown in figure 2, this may not result in the minimal synchronization. <p> Li and Abu-sufah present a technique to remove redundant synchronization interlocks (for instance, enforced with a post and wait condition) from doacross loops <ref> [15] </ref>. They detect simple cases where synchronization inserted to satisfy a dependence satisfies another as well, and implement their technique as a pass in the Parafrase source-to-source restructurer. Midkiff and Padua [17] present an algorithm to eliminate redundant dependences from loops with arbitrary control flow.
Reference: [16] <author> S. P. Midkiff and D. A. Padua. </author> <title> A comparison of four synchronization optimization techniques. </title> <booktitle> In 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II9-II16, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: However, their algorithm synchronizes at the iteration level as opposed to the statement level synchronization discussed above. Midkiff and Padua point out this drawback, and also discuss how the single loop techniques can be extended to multiple loops <ref> [16] </ref>. Their paper also surveys the existing techniques to eliminate redundant dependences. Ding-Kai Chen presents a modification to the SEODFS algorithm, that enables it to handle synchronization at the statement level [3]. He also addresses two related problems [2].
Reference: [17] <author> S.P. Midkiff and D.A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C 36(12) </volume> <pages> 1485-1495, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: However, as shown in figure 2 (c), by signaling the write to A after the write to B, the accesses to B need not be explicitly synchronized. This is the key insight motivating this paper. Early work in minimizing synchronization, by Li and Abu-Safah [15], and Midkiff and Padua <ref> [17] </ref>, was on detecting and eliminating redundant dependences. A dependence such as that between the write of A (I) and its subsequent read in figure 1 is redundant because it is satisfied by enforcing another dependence such as that between the write and read of B (I). <p> By eliminating redundant dependences, the number of dependences that have to be synchronized is reduced. Surprisingly, all the related methods that we have seen <ref> [3, 13, 15, 17] </ref> first eliminate redundant dependences, and then synchronize all remaining dependences. As we have shown in figure 2, this may not result in the minimal synchronization. <p> We describe our techniques only for the case of singly-nested loops with uniform dependences, i.e. where the dependence distances are all constant and known. We do this because the main ideas in this paper are orthogonal to these issues. Previous research <ref> [2, 13, 17, 23] </ref> has described how to apply redundant dependence elimination to multiply nested loops, and to cases where the dependences are not uniform. These techniques can be directly applied to extend our work as well. <p> Directed edges are added between nodes to reflect the execution order of statements within an iteration. Directed edges are also added between the source and sink nodes of loop-carried dependences. This is very similar to the controlled path graph used by Mid-kiff and Padua <ref> [17] </ref>, only without the synchronization nodes. Figure 3 shows the iteration dependence graph for the loop in figure 1. Note that instead of representing individual statements, the nodes can also represent 1 Calculated as (base opt) fi 100 ffi base. components of statements, such as the constituent operations. <p> Note that instead of representing individual statements, the nodes can also represent 1 Calculated as (base opt) fi 100 ffi base. components of statements, such as the constituent operations. In addition, statements that are not involved in any dependences can be excluded. When synchronizing singly-nested loops, Mid-kiff <ref> [17] </ref> has shown that only a portion of the iteration dependence graph needs to be examined. Specifically, if ffi max is the maximum dependence distance in the loop, only ffi max + 1 iterations need to be considered. <p> They detect simple cases where synchronization inserted to satisfy a dependence satisfies another as well, and implement their technique as a pass in the Parafrase source-to-source restructurer. Midkiff and Padua <ref> [17] </ref> present an algorithm to eliminate redundant dependences from loops with arbitrary control flow. Their algorithm first computes a controlled path graph, which specifies the execution order that must be enforced in order for the program to execute correctly.
Reference: [18] <institution> MIPS Technologies Inc. </institution> <note> R10000 Microprocessor User's Manual, Version 1.1, </note> <year> 1996. </year>
Reference-contexts: In general, synchronization writes are many times more expensive than synchronization reads. On a release, all preceding writes must be made visible to the other processors before the release itself can be made visible. However, using non-binding hardware prefetch and speculative load execution [9], most current processors <ref> [5, 12, 18] </ref> allow operations following an acquire to speculatively execute. Hence, we favor the minimization of releases over the minimization of acquires.
Reference: [19] <author> Robert H. B. Netzer. </author> <title> Race condition detection for debug ging shared-memory parallel programs. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: In the absence of cross-iteration dependences, the loop iterations can be executed, in parallel, by different processors. In the presence of loop-carried dependences, synchronization must be inserted before the iterations can be executed on different processors. This synchronization enforces the dependences between iterations, preventing data races <ref> [19] </ref> which can lead to incorrect execution. Synchronization can be a significant source of overhead in parallel programs. First, by requiring processors to interact, it imposes a communication overhead. This can be significant in large systems.
Reference: [20] <author> M. Quinn, P. Hatcher, and B. Seevers. </author> <title> Implementing a data parallel language on a tighly coupled multiprocessor. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing. </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: However, the minimum number of releases alone can be easily determined using the following simple greedy algorithm. This algorithm shares similarities with one used by Quinn, Hatcher and Seevers to insert the minimal number of barriers when synchronizing a data parallel program <ref> [20] </ref>. 1. Find the dependences in D 00 for which a release is not provided by the synchronization in S 0 . Let L be this set of dependences. <p> The algorithms we describe in this paper could instead be used here to determine the optimal synchronization to satisfy these dependences. Quinn, Hatcher and Seevers describe an algorithm to insert the minimal number of barriers when compiling a synchronous data parallel program <ref> [20] </ref>. Their algorithm is based on solving the minimum cover by cliques problem on a graph representing the dependence intervals that must be synchronized.
Reference: [21] <author> D. Shasha and M. Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 232-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: In this context, being able to introduce the minimal synchronization can play a significant role in determining whether it is worthwhile to parallelize the loop or not. When must synchronization be introduced? While all conflicting accesses to data (read-write or write-write accesses to the same location <ref> [21] </ref>) need to be ordered, it is not necessary to synchronize every conflicting access in the program. Enforcing the required order on some conflicting accesses may suffice to make the program free of data races.
Reference: [22] <author> SPARC International, Inc. </author> <title> The SPARC Architecture Man ual, </title> <type> Version 9, </type> <year> 1994. </year>
Reference-contexts: In non-sequentially consistent systems such as those based on the SPARC V9, DEC Alpha, and PowerPC processors, this mechanism takes the form of memory barrier, or fence instructions (for example, see <ref> [22] </ref>). Acquires and releases can be used as building blocks for other synchronization constructs. For instance, in an n processor system, a barrier can be implemented as n 1 releases (to notify the other processors) followed by n1 acquires (to wait for the other processors to reach the barrier).
Reference: [23] <author> Ten H. Tzen and Lionel M. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(5) </volume> <pages> 547-558, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We describe our techniques only for the case of singly-nested loops with uniform dependences, i.e. where the dependence distances are all constant and known. We do this because the main ideas in this paper are orthogonal to these issues. Previous research <ref> [2, 13, 17, 23] </ref> has described how to apply redundant dependence elimination to multiply nested loops, and to cases where the dependences are not uniform. These techniques can be directly applied to extend our work as well.
References-found: 23

