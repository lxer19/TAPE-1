URL: http://www.cs.iastate.edu/~honavar/Papers/icgi98.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/homepage.html
Root-URL: http://www.cs.iastate.edu
Email: rpare@allstate.com  Codrin.Nichitiu@ens-lyon.fr  honavar@cs.iastate.edu  
Phone: 2  3  
Title: A Polynomial Time Incremental Algorithm for Learning DFA  
Author: Rajesh Parekh Codrin Nichitiu and Vasant Honavar 
Note: home page:  
Web: WWW  http://www.cs.iastate.edu/~honavar/aigroup.html  
Address: 321 Middlefield Road, Menlo Park CA 94025, USA  46 Allee d'Italie, 69364 Lyon Cedex 07, France  Ames IA 50011, USA  
Affiliation: 1 Allstate Research and Planning Center  Ecole Normale Superieure de Lyon  Department of Computer Science, Iowa State University  
Abstract: We present an efficient incremental algorithm for learning deterministic finite state automata (DFA) from labeled examples and membership queries. This algorithm is an extension of Angluin's ID procedure to an incremental framework. The learning algorithm is intermittently provided with labeled examples and has access to a knowledgeable teacher capable of answering membership queries. The learner constructs an initial hypothesis from the given set of labeled examples and the teacher's responses to membership queries. If an additional example observed by the learner is inconsistent with the current hypothesis then the hypothesis is modified minimally to make it consistent with the new example. The update procedure ensures that the modified hypothesis is consistent with all examples observed thus far. The algorithm is guaranteed to converge to a minimum state DFA corresponding to the target when the set of examples observed by the learner includes a live complete set. We prove the convergence of this algorithm and analyze its time and space complexities. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang81] <author> D. Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference-contexts: Angluin's ID algorithm learns the target DFA given a live-complete sample and a knowledgeable teacher to answer membership queries posed by the learner <ref> [Ang81] </ref>. The interested reader is referred to [MQ86,Pit89,Lan95,PH98] for recent surveys of different approaches to grammar inference. In many practical learning scenarios, a live-complete sample may not be available to the learner at the outset. <p> Given A, a finite set of strings P is said to be live complete if for every live state q i of A there exists a string ff 2 P such that ffi fl (q 0 ; ff) = q i <ref> [Ang81] </ref>. For example, P = f; a; b; aag is a live complete set for the DFA in Fig. 1. Any superset of a live complete set is also live complete. <p> f; a; b; aag corresponding to the DFA in Fig. 1 we obtain the set T = f; a; b; aa; ab; ba; bb; aaa; aabg. 3 The ID Algorithm The ID algorithm for inference of the target DFA, its correctness proof, and complexity analysis are described in detail in <ref> [Ang81] </ref>. To keep the discussion that follows self-contained, we review ID briefly in this section. <p> Each successive column represents the function E i corresponding to the string v i (listed in the second row of the table). Note that the DFA returned by the procedure is exactly the DFA in Fig. 1. Angluin <ref> [Ang81] </ref> has shown that number of membership queries needed is O (jj N jP j). Thus, ID runs in time polynomial in jj, N , and jP j. Algorithm: ID Input: A live complete set P and a teacher to answer membership queries. <p> Given a live complete set of examples, ID outputs a canonical representation of the target DFA A <ref> [Ang81] </ref>. From above we know that at k = l the current model (M t ) of the target automaton maintained by IID is identical to one arrived at by ID. Thus, we have proved that IID converges to a canonical representation of the target DFA. 2 Theorem 2.
Reference: [Ang87] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Infor--mation and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: The algorithm is guaranteed to converge to the target DFA and has polynomial time and space complexities. Angluin's L fl algorithm for learning the target DFA is based on membership and equivalence queries <ref> [Ang87] </ref>. The equivalence queries can be replaced by a polynomial number of calls to an oracle that supplies labeled examples to give an efficient PAC algorithm for learning DFA.
Reference: [BF72] <author> A. Biermann and J. Feldman. </author> <title> A survey of results in grammatical inference. </title> <editor> In S. Watanabe, editor, </editor> <booktitle> Frontiers of Pattern Recognition, </booktitle> <publisher> Academic Press, </publisher> <pages> pages 31-54, </pages> <year> 1972. </year>
Reference: [CM96] <author> D. Carmel and S. Markovitch. </author> <title> Learning models of intelligent agents. </title> <booktitle> In Proceedings of the AAAI-96 (vol. </booktitle> <volume> 1), </volume> <publisher> AAAI Press/MIT Press, </publisher> <pages> pages 62 - 67, </pages> <year> 1996. </year>
Reference-contexts: In such scenarios, an online or incremental model of learning that is guaranteed to eventually converge to the target DFA in the limit is of interest. Particularly, in the case of intelligent autonomous agents, incremental learning offers an attractive framework for characterizing the behavior of the agents <ref> [CM96] </ref>. Against this background, we present a provably correct, polynomial time, incremental, interactive algorithm for learning the target DFA from labeled examples and membership queries.
Reference: [Dup96] <author> P. Dupont. </author> <title> Incremental regular inference. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Montpellier, France, Lecture Notes in Artificial Intelligence 1147, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 222-237, </pages> <year> 1996. </year>
Reference-contexts: Though provably correct, this algorithm has practical limitations because the size of the lattice grows exponentially with the number of states of the PTA. The incremental version of the regular positive and negative inference (RPNI) algorithm [OG92] for regular grammar inference <ref> [Dup96] </ref> is also based on the idea of a lattice of partitions of the states of a PTA for a set of positive examples. It uses information from a set of negative examples to guide the ordered search through the lattice. <p> We are exploring the possibility of learning in an environment where the learner does not have access to a teacher. The algorithms due to Dupont <ref> [Dup96] </ref> and Porat & Feldman [PF91] operate in this framework. Some open problems include whether the limitations of these algorithms (e.g., need for storage of all the previously seen examples and complete lexicographic ordering of examples) can be overcome without sacrificing efficiency and guaranteed convergence to the target.
Reference: [FB75] <author> K. S. Fu and T. L. Booth. </author> <title> Grammatical inference: Introduction and survey (part 1). </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 5 </volume> <pages> 85-111, </pages> <year> 1975. </year>
Reference: [Gol78] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37(3) </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: However, given a finite set of positive examples and a finite (possibly empty) set of negative examples the problem of learning a minimum state DFA equivalent to the unknown target is NP-hard <ref> [Gol78] </ref>.
Reference: [Lan95] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kauffman, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference: [MQ86] <author> L. Miclet and J. Quinqueton. </author> <title> Learning from examples in sequences and grammatical inference. </title> <editor> In G. Ferrate et al, editors, </editor> <title> Syntactic and Structural Pattern Recognition, </title> <booktitle> NATO ASI Series Vol. F45, </booktitle> <pages> pages 153-171, </pages> <year> 1986. </year>
Reference: [OG92] <author> J. Oncina and P. Garca. </author> <title> Inferring regular languages in polynomial update time. </title> <editor> In N. Perez et al, editors, </editor> <title> Pattern Recognition and Image Analysis, </title> <publisher> World Scientific, </publisher> <pages> pages 49-61, </pages> <year> 1992. </year>
Reference-contexts: The learner's task can be simplified by requiring that the set of examples provided meet certain desired criteria (like structural completeness [PC78,PH96] or characteristic sample <ref> [OG92] </ref>), or by providing the learner with access to sources of additional information, like a knowledgeable teacher who responds to queries generated by the learner. Angluin's ID algorithm learns the target DFA given a live-complete sample and a knowledgeable teacher to answer membership queries posed by the learner [Ang81]. <p> Though provably correct, this algorithm has practical limitations because the size of the lattice grows exponentially with the number of states of the PTA. The incremental version of the regular positive and negative inference (RPNI) algorithm <ref> [OG92] </ref> for regular grammar inference [Dup96] is also based on the idea of a lattice of partitions of the states of a PTA for a set of positive examples. It uses information from a set of negative examples to guide the ordered search through the lattice. <p> The algorithm runs in time that is polynomial in the sum of lengths of the positive and negative examples and is guaranteed to converge to the target DFA when the set of examples seen by the learner include a characteristic sample (see <ref> [OG92] </ref>) for the target automaton as a subset. Porat and Feldman's incremental algorithm for learning automata uses a complete ordered sample [PF91]. A complete ordered sample includes all the strings in fl in strict lexicographic order.
Reference: [PC78] <author> T. Pao and J. Carr. </author> <title> A solution of the syntactic induction-inference problem for regular languages. </title> <journal> Computer Languages, </journal> <volume> 3 </volume> <pages> 53-64, </pages> <year> 1978. </year>
Reference: [PF91] <author> S. Porat and J. Feldman. </author> <title> Learning automata from ordered examples. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 109-138, </pages> <year> 1991. </year>
Reference-contexts: Porat and Feldman's incremental algorithm for learning automata uses a complete ordered sample <ref> [PF91] </ref>. A complete ordered sample includes all the strings in fl in strict lexicographic order. The algorithm maintains a current hypothesis which is updated upon seeing a counter example and is guaranteed to converge in the limit provided the examples appear in strict lexicographic order. <p> We are exploring the possibility of learning in an environment where the learner does not have access to a teacher. The algorithms due to Dupont [Dup96] and Porat & Feldman <ref> [PF91] </ref> operate in this framework. Some open problems include whether the limitations of these algorithms (e.g., need for storage of all the previously seen examples and complete lexicographic ordering of examples) can be overcome without sacrificing efficiency and guaranteed convergence to the target. <p> Porat and Feldman proved a strong negative result stating that there exists no algorithm which when operating with finite working storage can incrementally learn the target DFA from an arbitrary presentation <ref> [PF91] </ref>.
Reference: [PH96] <author> R. G. Parekh and V. G. Honavar. </author> <title> An incremental interactive algorithm for regular grammar inference. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Montpellier, France, Lecture Notes in Artificial Intelligence 1147, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 238-250, </pages> <year> 1996. </year>
Reference-contexts: In contrast, the PAC version of L fl guarantees that with very high probability the DFA output by the algorithm would make very low error (when compared to the unknown target). Parekh and Honavar's algorithm for regular inference <ref> [PH96] </ref> searches a lattice of FSA generated by successive state mergings of a prefix tree automaton (PTA) for a set of positive examples of the target grammar. The lattice is represented compactly as a version space and is searched using membership queries.
Reference: [PH97] <author> R. G. Parekh and V. G. Honavar. </author> <title> Learning dfa from simple examples. </title> <booktitle> In Proceedings of the Eighth International Workshop on Algorithmic Learning Theory (ALT'97), Sendai, Japan, Lecture Notes in Artificial Intelligence 1316, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 116-131, </pages> <year> 1997. </year> <title> Also presented at the Workshop on Grammar Inference, Automata Induction, and Language Acquisition (ICML'97), </title> <address> Nashville, TN. </address> <month> July 12, </month> <year> 1997. </year>
Reference-contexts: style approximate learning of the target within a given error bound); provide for some additional hints to the learning algorithm (like a bound on the number of states of the target DFA); include a helpful teacher that carefully guides the learner perhaps by providing simple examples first (for example see <ref> [PH97] </ref>). Acknowledgements The authors are grateful to the Department of Computer Science at Iowa State University (where a major portion of this research was undertaken) for the research and computing facilities made available to them.
Reference: [PH98] <author> R. G. Parekh and V. G. Honavar. </author> <title> Grammar inference, automata induction, and language acquisition. </title> <editor> In R. Dale, H. Moisl, and H. Somers, editors, </editor> <booktitle> Handbook of Natural Language Processing. </booktitle> <publisher> Marcel Dekker, </publisher> <year> 1998. </year> <note> (To appear). </note>
Reference: [Pit89] <author> L. Pitt. </author> <title> Inductive inference, dfas and computational complexity. In Analogical and Inductive Inference, </title> <booktitle> Lecture Notes in Artificial Intelligence 397, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 18-44, </pages> <year> 1989. </year>
References-found: 16

