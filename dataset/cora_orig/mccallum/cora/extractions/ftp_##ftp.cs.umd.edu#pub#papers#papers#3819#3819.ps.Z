URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3819/3819.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Compiler Optimizations for Eliminating Cache Conflict Misses  
Author: Gabriel Rivera Chau-Wen Tseng 
Address: College Park, Maryland 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Limited set-associativity in hardware caches can cause conflict misses when multiple data items map to the same cache locations. Conflict misses have been found to be a significant source of poor cache performance in scientific programs, particularly within loop nests. We present two compiler transformations to eliminate conflict misses: 1) modifying variable base addresses, 2) padding inner array dimensions. Unlike compiler transformations that restructure the computation performed by the program, these two techniques modify its data layout. Using cache simulations of a selection of kernels and benchmark programs, we show these compiler transformations can eliminate conflict misses for applications with regular memory access patterns. Cache miss rates for a 16K, direct-mapped cache are reduced by 35% on average for each program. For some programs, execution times on a DEC Alpha can be improved up to 60%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Data layout optimizations such as inter and intra-variable padding modify how variables in a program are laid out in memory, with the goal of improving spatial locality and avoiding adverse memory effects such as conflict misses or false sharing in parallel programs <ref> [1, 5, 15] </ref>. In this paper, we focus on data layout transformations to eliminate conflict misses for sequential programs. Most data layout optimizations can be applied at compile time, but link-time and run-time optimizations (for heap-allocated objects) are also possible. <p> They use an algebraic formulation to combine nonsingular loop transformations with array transpose, but do not discuss detailed heuristics for resolving optimization conflicts when when multiple references to the same variable exist. Amarasinghe et al. demonstrated the utility of array reindexing for parallel applications <ref> [1] </ref>. They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts. 7 Conclusions Conflict misses have been pointed out as a significant source of poor cache performance in scientific programs, particularly within loop nests.
Reference: [2] <author> W. Bolosky, R. Fitzgerald, and M. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Another transformation, intra-variable (array) padding, is required. Intra-variable padding differs from modifying variable base addresses in that it increases internal array dimension sizes, changing the relative layout for higher dimensions of the array <ref> [2, 19] </ref>. Intra-array padding can thus eliminate conflict misses between different sections of the same array. Because it also changes the size of an array, intra-array padding also changes base addresses of variables and may achieve benefits similar to inter-variable padding.
Reference: [3] <author> D. Callahan and A. Porterfield. </author> <title> Data cache performance of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: They give no details of their heuristic to order loops for locality. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [3, 9, 8, 7] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior [12].
Reference: [4] <author> S. Carr, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Reuse may be in the form of temporal locality, where the same data is accessed multiple times, or spatial locality, where nearby data is accessed together. Fortunately, scientific applications tend to have large amounts of reuse <ref> [4] </ref>. Due to speed constraints, hardware caches tend to have limited set associativity, where memory addresses can only be mapped to one of k locations in a k-way associative cache. On-chip primary caches typically are direct-mapped (1-way set associative). <p> Previous research has shown that compilers can automatically restructure programs to improve the data locality of applications <ref> [4, 22] </ref>. Example optimizations include loop permutation, tiling, loop fission, and loop fusion. These computation-reordering optimizations are generally based on changing the order iterations of a loop nest are executed. <p> Several models have been developed to calculate spatial and temporal locality for sequential programs <ref> [4, 7, 11, 22] </ref>. However, these models are suitable for predicting the occurrence of conflict misses. More recently, Ghosh et al. have developed a method for detecting conflict misses by calculating cache miss equations which summarize a loop's memory behavior [12]. <p> Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24]. Loop permutation and loop tiling are the primary optimization techniques used <ref> [4, 6, 10, 14, 22] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [4]. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality. <p> Loop permutation and loop tiling are the primary optimization techniques used [4, 6, 10, 14, 22], though loop fission (distribution) and loop fusion have also been found to be helpful <ref> [4] </ref>. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality.
Reference: [5] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Data layout optimizations such as inter and intra-variable padding modify how variables in a program are laid out in memory, with the goal of improving spatial locality and avoiding adverse memory effects such as conflict misses or false sharing in parallel programs <ref> [1, 5, 15] </ref>. In this paper, we focus on data layout transformations to eliminate conflict misses for sequential programs. Most data layout optimizations can be applied at compile time, but link-time and run-time optimizations (for heap-allocated objects) are also possible. <p> Jeremiassen and Eggers have performed the most extensive examination of automatically eliminating false sharing in explicitly parallel programs in a compiler [15]. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs <ref> [5] </ref>. They use an algebraic formulation to combine nonsingular loop transformations with array transpose, but do not discuss detailed heuristics for resolving optimization conflicts when when multiple references to the same variable exist. Amarasinghe et al. demonstrated the utility of array reindexing for parallel applications [1].
Reference: [6] <author> S. Coleman and K. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24]. Loop permutation and loop tiling are the primary optimization techniques used <ref> [4, 6, 10, 14, 22] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [4]. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality.
Reference: [7] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Several models have been developed to calculate spatial and temporal locality for sequential programs <ref> [4, 7, 11, 22] </ref>. However, these models are suitable for predicting the occurrence of conflict misses. More recently, Ghosh et al. have developed a method for detecting conflict misses by calculating cache miss equations which summarize a loop's memory behavior [12]. <p> They give no details of their heuristic to order loops for locality. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [3, 9, 8, 7] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior [12].
Reference: [8] <author> K. Gallivan, W. Jalby, and D. Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The intuition is that references in the same uniformly generated group have very similar memory access patterns that may significantly overlap. Both Gannon et al. and Wolf and Lam use uniformly generated references as way of calculating group-reuse when estimating cache locality <ref> [11, 8, 22] </ref>. Carr et al. utilize a similar concept called reference groups, customized for individual loops. To check for severe conflict misses within an array, the compiler can examine members of a group of uniformly generated references to determine whether they differ in non-contiguous array dimensions. <p> They give no details of their heuristic to order loops for locality. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [3, 9, 8, 7] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior [12].
Reference: [9] <author> K. Gallivan, W. Jalby, A. Maloney, and H. Wijshoff. </author> <title> Performance prediction of loop constructs on multiprocessor hierarchical memory systems. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Supercomputing, </booktitle> <pages> pages 433-442, </pages> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: They give no details of their heuristic to order loops for locality. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [3, 9, 8, 7] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior [12].
Reference: [10] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24]. Loop permutation and loop tiling are the primary optimization techniques used <ref> [4, 6, 10, 14, 22] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [4]. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality.
Reference: [11] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Several models have been developed to calculate spatial and temporal locality for sequential programs <ref> [4, 7, 11, 22] </ref>. However, these models are suitable for predicting the occurrence of conflict misses. More recently, Ghosh et al. have developed a method for detecting conflict misses by calculating cache miss equations which summarize a loop's memory behavior [12]. <p> We plan to adopt a simplified version of cache miss equations to help guide our array padding heuristics. To predict severe conflict misses, we propose a modification of the algorithm by Gannon et al. for calculation of uniformly generated references <ref> [11] </ref>. Two references to the same variable are uniform if their subscripts have the same index variables and differ only by a constant term. For instance, assuming i, j are loop index variables, A (i,j) and A (i-1,j+2) are uniformly generated references, but A (i,j) and A (j,i) are not. <p> The intuition is that references in the same uniformly generated group have very similar memory access patterns that may significantly overlap. Both Gannon et al. and Wolf and Lam use uniformly generated references as way of calculating group-reuse when estimating cache locality <ref> [11, 8, 22] </ref>. Carr et al. utilize a similar concept called reference groups, customized for individual loops. To check for severe conflict misses within an array, the compiler can examine members of a group of uniformly generated references to determine whether they differ in non-contiguous array dimensions. <p> Wolf and Lam provide a concise definition and summary of important types of data locality [22]. Gannon et al. introduce the notion of uniformly generated references as a means of discovering group reuse between references to the same array <ref> [11] </ref>. McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [17].
Reference: [12] <author> S. Ghosh, M. Martonosi, and S. Malik. </author> <title> Cache miss equations: An analytical representation of cache misses. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: However, these models are suitable for predicting the occurrence of conflict misses. More recently, Ghosh et al. have developed a method for detecting conflict misses by calculating cache miss equations which summarize a loop's memory behavior <ref> [12] </ref>. They demonstrate how cache miss equations may be used to directly compute the number of conflict misses in a loop nest. We plan to adopt a simplified version of cache miss equations to help guide our array padding heuristics. <p> We are currently in the process of incorporating a limited version of cache miss equations to detect severe conflict misses and select array paddings <ref> [12] </ref>. Preliminary results indicate it can consistently match the best result from the set of current compiler padding heuristics. 6 Related Work Data locality has been recognized as a significant performance issue for both scalar and parallel architectures. <p> These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior <ref> [12] </ref>. They demonstrate how to use cache miss equations to select array paddings to eliminate conflict misses, and block sizes for tiling. We plan to adopt simplified versions of cache miss equations to help guide our array padding heuristics.
Reference: [13] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Array padding must be performed at compile time. It is interesting to note that in many scientific applications, users have already padded arrays by hand to avoid conflict misses. 3 Prototype Compiler Implementation The data layout optimizations were implemented as passes in the Stanford SUIF compiler <ref> [13, 20] </ref>. SUIF is a compiler infrastructure designed to support research in both optimizing and parallelizing compilers. Independently developed compilation passes work together by using a common intermediate format to represent programs. SUIF has been tested and validated on a large number of programs.
Reference: [14] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24]. Loop permutation and loop tiling are the primary optimization techniques used <ref> [4, 6, 10, 14, 22] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [4]. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality.
Reference: [15] <author> T. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Data layout optimizations such as inter and intra-variable padding modify how variables in a program are laid out in memory, with the goal of improving spatial locality and avoiding adverse memory effects such as conflict misses or false sharing in parallel programs <ref> [1, 5, 15] </ref>. In this paper, we focus on data layout transformations to eliminate conflict misses for sequential programs. Most data layout optimizations can be applied at compile time, but link-time and run-time optimizations (for heap-allocated objects) are also possible. <p> Researchers have previously examined changing data layout in parallel applications to eliminate false sharing and co-locate data and computation, have not studied its effect on sequential applications. Jeremiassen and Eggers have performed the most extensive examination of automatically eliminating false sharing in explicitly parallel programs in a compiler <ref> [15] </ref>. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs [5]. They use an algebraic formulation to combine nonsingular loop transformations with array transpose, but do not discuss detailed heuristics for resolving optimization conflicts when when multiple references to the same variable exist.
Reference: [16] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <booktitle> In Proceedingsof the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Li and Pingali use linear transformations (any linear mapping from one loop nest to another loop nest) to optimize for both data locality and parallelism <ref> [16] </ref>. They do not propose exhaustive search, since the search space becomes infinite, but transform the loop nest based on certain references in the program. They give no details of their heuristic to order loops for locality.
Reference: [17] <author> K. S. McKinley and O. Temam. </author> <title> A quantitative analysis of loop nest locality. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Conflict misses have been found to be a significant source of poor performance in scientific programs, particularly within loop nests <ref> [17] </ref>. In this paper, we show that compiler transformations can be very effective in eliminating conflict misses for scientific programs with regular access patterns. 1.1 Motivating Examples We begin by providing some simple examples to motivate the need for eliminating conflict misses. <p> McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses <ref> [17] </ref>. Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24].
Reference: [18] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Scientific programs tend to be particularly memory-intensive and dependent on the memory hierarchy. A simulation study by Mowry et al. discovered scientific programs approximate spend from a quarter to half of overall execution time waiting for data to be fetched from memory during sequential execution <ref> [18] </ref>. In order to exploit the greater speed of higher levels of the memory hierarchy, applications must possess data locality, where an application reuses data.
Reference: [19] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Another transformation, intra-variable (array) padding, is required. Intra-variable padding differs from modifying variable base addresses in that it increases internal array dimension sizes, changing the relative layout for higher dimensions of the array <ref> [2, 19] </ref>. Intra-array padding can thus eliminate conflict misses between different sections of the same array. Because it also changes the size of an array, intra-array padding also changes base addresses of variables and may achieve benefits similar to inter-variable padding.
Reference: [20] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Array padding must be performed at compile time. It is interesting to note that in many scientific applications, users have already padded arrays by hand to avoid conflict misses. 3 Prototype Compiler Implementation The data layout optimizations were implemented as passes in the Stanford SUIF compiler <ref> [13, 20] </ref>. SUIF is a compiler infrastructure designed to support research in both optimizing and parallelizing compilers. Independently developed compilation passes work together by using a common intermediate format to represent programs. SUIF has been tested and validated on a large number of programs.
Reference: [21] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality. They prune their search space by ignoring loops that do not carry reuse and loops that cannot be permuted due to legality constraints <ref> [22, 21] </ref>. Li and Pingali use linear transformations (any linear mapping from one loop nest to another loop nest) to optimize for both data locality and parallelism [16].
Reference: [22] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Previous research has shown that compilers can automatically restructure programs to improve the data locality of applications <ref> [4, 22] </ref>. Example optimizations include loop permutation, tiling, loop fission, and loop fusion. These computation-reordering optimizations are generally based on changing the order iterations of a loop nest are executed. <p> Several models have been developed to calculate spatial and temporal locality for sequential programs <ref> [4, 7, 11, 22] </ref>. However, these models are suitable for predicting the occurrence of conflict misses. More recently, Ghosh et al. have developed a method for detecting conflict misses by calculating cache miss equations which summarize a loop's memory behavior [12]. <p> The intuition is that references in the same uniformly generated group have very similar memory access patterns that may significantly overlap. Both Gannon et al. and Wolf and Lam use uniformly generated references as way of calculating group-reuse when estimating cache locality <ref> [11, 8, 22] </ref>. Carr et al. utilize a similar concept called reference groups, customized for individual loops. To check for severe conflict misses within an array, the compiler can examine members of a group of uniformly generated references to determine whether they differ in non-contiguous array dimensions. <p> Wolf and Lam provide a concise definition and summary of important types of data locality <ref> [22] </ref>. Gannon et al. introduce the notion of uniformly generated references as a means of discovering group reuse between references to the same array [11]. <p> Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers [23, 24]. Loop permutation and loop tiling are the primary optimization techniques used <ref> [4, 6, 10, 14, 22] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [4]. Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality. <p> Wolf and Lam use unimodular transformations (a combination of permutation, skewing, and reversal) and tiling with estimates of temporal and spatial reuse to improve data locality. They prune their search space by ignoring loops that do not carry reuse and loops that cannot be permuted due to legality constraints <ref> [22, 21] </ref>. Li and Pingali use linear transformations (any linear mapping from one loop nest to another loop nest) to optimize for both data locality and parallelism [16].
Reference: [23] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [17]. Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers <ref> [23, 24] </ref>. Loop permutation and loop tiling are the primary optimization techniques used [4, 6, 10, 14, 22], though loop fission (distribution) and loop fusion have also been found to be helpful [4].
Reference: [24] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <month> 14 </month>
Reference-contexts: McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [17]. Most researchers exploring compiler optimizations to improve data locality have concentrated on computation-reordering transformations derived from shared-memory parallelizing compilers <ref> [23, 24] </ref>. Loop permutation and loop tiling are the primary optimization techniques used [4, 6, 10, 14, 22], though loop fission (distribution) and loop fusion have also been found to be helpful [4].
References-found: 24

