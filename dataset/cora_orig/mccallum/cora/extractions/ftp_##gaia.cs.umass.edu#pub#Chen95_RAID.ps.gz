URL: ftp://gaia.cs.umass.edu/pub/Chen95:RAID.ps.gz
Refering-URL: http://www-net.cs.umass.edu/papers/papers.html
Root-URL: 
Title: A Performance Evaluation of RAID Architectures  
Author: Shenze Chen Don Towsley 
Keyword: Index terms Disk arrays, I/O subsystems, performance analysis, queuing model, RAID, scheduling policies.  
Note: This work is supported, in part, by the Office of Naval Research under contract N00014-87-K-0796, by NSF under contract IRI-8908693, and by an NSF equipment grant CERDCR 8500332. Shenze Chen is currently with Hewlett-Packard Labs in  
Address: Amherst, MA 01003  Palo Alto, CA.  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: In today's computer systems, the disk I/O subsystem is often identified as the major bottleneck to system performance. One proposed solution is the so-called redundant array of inexpensive disks (RAID). In this paper, we examine the performance of two of the most promising RAID architectures, the mirrored array and the rotated parity array. First, we propose several scheduling policies for the mirrored array and a new data layout, group-rotate declustering, and compare their performance with each other and in combination with other data layout schemes. We observe that a policy that routes reads to the disk with the smallest number of requests provides the best performance, especially when the load on the I/O system is high. Second, through a combination of simulation and analysis, we compare the performance of this mirrored array architecture to the rotated parity array architecture. This latter study shows that, i) given the same storage capacity (approximately double the number of disks), the mirrored array considerably outperforms the rotated parity array and, ii) given the same number of disks, the mirrored array still outperforms the rotated parity array in most cases, even for applications where I/O requests are for large amounts of data. The only exception occurs when the I/O size is very large; most of the requests are writes, and most of these writes perform full stripe write operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bell, C. G. </author> <title> The mini and micro industries. </title> <journal> IEEE Comp. Mag., </journal> <volume> 17 </volume> <pages> 14-30, </pages> <month> October </month> <year> 1984. </year> <month> 35 </month>
Reference-contexts: 1 Introduction In many computing systems, the disk I/O subsystem is often identified as the major bottleneck to system performance. During the past several years, CPU speeds have increased at a rate of 40% to 100% per year <ref> [1, 13] </ref>, whereas disk seek times have only improved by 7% per year [11, 32]. This has led to a big gap between the speed of the CPU/main memory and that of the disk I/O subsystem [30] which is expected to increase further in the near future.
Reference: [2] <author> Bitton, D. and Gray, J. </author> <title> Disk shadowing. </title> <booktitle> In Proc. 14th VLDB Conf., </booktitle> <pages> pages 331-338, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Removal of the conditioning yields P fD = ig = p s ; i = 0; 2 (Ci) (2) We also use the following expression for the seek time, S, as a function of the seek distance D <ref> [44, 2, 3, 31] </ref>, ( a + b D; D &gt; 0; where a is the arm acceleration time and b is the mechanical seek factor.
Reference: [3] <author> Bitton, D. </author> <title> Arm scheduling in shadowed disks. </title> <booktitle> In Proc. IEEE Spring ComCon, </booktitle> <pages> pages 132-136, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Removal of the conditioning yields P fD = ig = p s ; i = 0; 2 (Ci) (2) We also use the following expression for the seek time, S, as a function of the seek distance D <ref> [44, 2, 3, 31] </ref>, ( a + b D; D &gt; 0; where a is the arm acceleration time and b is the mechanical seek factor.
Reference: [4] <author> Bultman, D. L. </author> <title> High performance SCSI using parallel drive technology. </title> <booktitle> In Proc. BUS-CON Conf., </booktitle> <pages> pages 40-44, </pages> <address> Anaheim, CA, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [5] <author> Chen, P., Gibson, G., Katz, R. H., and Patterson, D. A. </author> <title> An evaluation of redundant arrays of disks using an Amdahl 5890. Perf. Eval. </title> <journal> Rev., </journal> <volume> 18(1) </volume> <pages> 74-85, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For simplicity, we assume that a partial stripe write is only required for the last stripe of each I/O, i.e., the starting block is always aligned with the stripe boundary <ref> [5] </ref>. This may yield an optimistic estimate of the performance of RAID 5. Let p f be the probability that a write request is a full stripe write.
Reference: [6] <author> Chen, S.-Z. </author> <title> Design, Modeling, and Evaluation of High Performance I/O Subsystems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In the following, we will focus on three policies. Discussions of other policies can be found in <ref> [6] </ref>. For all three policies to be described below, two queues are maintained by the system, one for each copy (Figure 6). Each queue is served in a first-come-first-serve manner. At the time of arrival, a write request generates two tasks that enter each of the two queues.
Reference: [7] <author> Chen, S.-Z. and Towsley, D. </author> <title> Raid 5 vs. parity striping: Their design and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17 </volume> <pages> 58-74, </pages> <year> 1992. </year>
Reference-contexts: The cumulative distribution function (CDF ) of S, F S (x) = P fS &lt; xg, is (see <ref> [7] </ref> for details), F S (x) = &gt; &gt; &lt; F D (0); 0 x a; b ) 2 ; a &lt; x S max ; (3) where the maximum seek time S max = a + b p C 1. <p> The mean seek time can be accurately approximated by (see <ref> [7] </ref> for details), E [S] (1 p s ) a + b C 1 15 : (4) The rotation time is assumed to be uniformly distributed in [0; R max ] with pdf f R (x) = 1=R max 0 x R max : (5) Let X = S + R <p> If a write request desires to modify more than one block on a stripe, then the parity request is generated when the last old data block has been read out. Remark: Another policy, called the BS policy, was proposed in <ref> [7] </ref>. This policy generates a parity update request as soon as a write begins its service. We have observed little difference in the performances of theses two policies (the BS policy performs slightly better at low loads).
Reference: [8] <author> Copeland, G. and Keller, T. </author> <title> A comparison of high-availability media algorithms. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 98-109, </pages> <address> Portland, OR, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability <ref> [43, 8, 27] </ref>. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID). <p> For example, the RAID 5 architecture is approximately but accurately analyzed via decomposition where each disk is modeled as a priority queuing system. This and other analyses are of independent interest. Other related works on performance evaluation of disk arrays can be found in <ref> [8, 22, 37, 31] </ref>. A recent paper, Lee and Katz [21] presents an analytic model for disk arrays. However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined. <p> We consider several variations that differ from each other according to how the data is placed on the disks. These include mirrored declustering <ref> [34, 8] </ref>, chained declustering [12], and a new variant, group-rotate declustering, which combines advantages of both mirrored and chained declustering. Mirrored Declustering: In mirrored declustering, the N disks are configured as N=2 pairs of mirrored disks, with the i-th pair termed mirror i.
Reference: [9] <author> Douglis, F. and Ousterhout, J. </author> <title> Log-structured file systems. </title> <booktitle> In Proc. IEEE Spring ComCon., </booktitle> <pages> pages 124-129, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Last, from the above studies, we observe that the main impact on the performance of RAID 5 is the high cost of the partial stripe write. Researchers have looked at different ways 33 to reduce this cost. The most attractive way is the Log-Structured File System <ref> [32, 9, 38, 39] </ref>, in which many small writes are accumulated in cache and converted into a large full stripe write. Another way is called floating parity [24], where parity blocks are clustered into cylinders each containing a spare track.
Reference: [10] <author> Gray, J., Host, B., and Walker, M. </author> <title> Parity striping of disk arrays: Low-cost reliable storage with acceptable throughput. </title> <booktitle> In Proc. 16th VLDB Conf., </booktitle> <pages> pages 148-161, </pages> <address> Brisbane, Australia, </address> <year> 1990. </year>
Reference-contexts: We present analytic and simulation results for disk array architectures coupled with different scheduling policies. Since it has been observed in the literature that for applications such as transaction processing systems and workstation/engineering environments, I/O requests typically access a small amount of data <ref> [25, 10, 26, 33] </ref>, we assume that each request accesses a single block of data (4096 bytes [31]) and that each distinct block is equally likely to be accessed.
Reference: [11] <author> Harker, J. M. et al. </author> <title> A quarter century of disk file innovation. </title> <journal> IBM J. Res. Dev., </journal> <volume> 25 </volume> <pages> 677-689, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: During the past several years, CPU speeds have increased at a rate of 40% to 100% per year [1, 13], whereas disk seek times have only improved by 7% per year <ref> [11, 32] </ref>. This has led to a big gap between the speed of the CPU/main memory and that of the disk I/O subsystem [30] which is expected to increase further in the near future.
Reference: [12] <author> Hsiao, H. and DeWitt, D. </author> <title> Chained declustering: A new availibility strategy for multiprocessor database machines. </title> <booktitle> In Proc. 6th Int'l Conf. on Data Engineering, </booktitle> <pages> pages 456-465, </pages> <address> Los Angeles, CA, </address> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: We consider several variations that differ from each other according to how the data is placed on the disks. These include mirrored declustering [34, 8], chained declustering <ref> [12] </ref>, and a new variant, group-rotate declustering, which combines advantages of both mirrored and chained declustering. Mirrored Declustering: In mirrored declustering, the N disks are configured as N=2 pairs of mirrored disks, with the i-th pair termed mirror i. Two copies of data are striped over these N=2 pairs. <p> that when a disk fails and the disk array operates in a failure or rebuild mode, all of the traffic originally directed to the failed disk is now redirected to its mirror, which may become a bottleneck. 4 Chained Declustering: To overcome the above drawback during recovery, Hsiao and DeWitt <ref> [12] </ref> proposed the chained declustering architecture, where two physical copies of data, termed the primary copy and the backup copy, are maintained.
Reference: [13] <author> Joy, W. </author> <note> Presentation at ISSCC'85 Panel Session, </note> <month> February </month> <year> 1985. </year>
Reference-contexts: 1 Introduction In many computing systems, the disk I/O subsystem is often identified as the major bottleneck to system performance. During the past several years, CPU speeds have increased at a rate of 40% to 100% per year <ref> [1, 13] </ref>, whereas disk seek times have only improved by 7% per year [11, 32]. This has led to a big gap between the speed of the CPU/main memory and that of the disk I/O subsystem [30] which is expected to increase further in the near future.
Reference: [14] <author> Katz, R. H., Gibson, G., and Patterson, D. A. </author> <title> Disk system architectures for high performance computing. </title> <journal> Proc. of the IEEE, </journal> <volume> 77(12) </volume> <pages> 1842-1858, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in <ref> [35, 34, 14] </ref> referred to as Redundant Arrays of Inexpensive Disks (RAID). Among the five, the two most promising candidates for high performance computing systems appear to be the mirrored disk array (RAID 1) and the rotated parity array (RAID 5). <p> However, each I/O request typically accesses a large amount of data. Generally, computation parameters are moved in bulk from disks to memory resident data structures, and results are periodically written back to disks <ref> [14] </ref>. In this case, multiple disks work together as a single logical device providing a faster transfer rate [16]. As stated at the beginning of Section 2, the rotations of all disks in an array are assumed to be synchronized.
Reference: [15] <author> Khinchine, A. Y. </author> <title> Mathematical Methods in the Theory of Queueing. </title> <publisher> Hafner Publishing Co., </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: Arrivals to the P queue of disk i are described by a superposition of N 1 parity request generating processes which direct requests to disk i with probability 1=(N 1). When N is large, it can be approximated by a Poisson process <ref> [15] </ref> with parameter p (i) = (1 p i )p w =(N 1). Strictly speaking, the N 1 parity generating processes are not independent of each other.
Reference: [16] <author> Kim, M. Y. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Trans. on Comp., </journal> <volume> C-35 </volume> (11):978-988, Nov. 1986. 
Reference-contexts: One attractive idea is the so-called disk array, where the disk I/O subsystem consists of multiple disks with data spread over these disks. The ideas of disk interleaving and disk striping were first introduced by Kim <ref> [16] </ref> and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability [43, 8, 27]. <p> Generally, computation parameters are moved in bulk from disks to memory resident data structures, and results are periodically written back to disks [14]. In this case, multiple disks work together as a single logical device providing a faster transfer rate <ref> [16] </ref>. As stated at the beginning of Section 2, the rotations of all disks in an array are assumed to be synchronized. Data is assumed to be striped across the disks in the same cylinder.
Reference: [17] <author> Kim, M. Y. and Tantawi, A. N. </author> <title> Asynchronized disk interleaving: Approximating access delays. </title> <journal> IEEE Trans. on Comp., </journal> <volume> C-40 </volume> (7):801-810, July 1991. 
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID). <p> Z r ; Z w ; Z: r:v:'s denoting read, write, and overall I/O response times. First, consider the disk seek pattern. Since it is observed that in reality, most of the time the disk arm doesn't move <ref> [23, 17] </ref>, we introduce a sequential access probability, p s , which is defined to be the probability that the seek distance is equal to zero. We identify two kinds of access patterns, the sequential access pattern and non-sequential access pattern. <p> The performance results are obtained from simulation experiments on a disk array of 16 disks. In <ref> [17] </ref>, Kim reported several real disk reference traces, which show the percentage of sequential accesses ranging from 25% to 50%. For disk arrays, however, since data is spread over multiple disks, the sequential access probability on each disk is expected to be smaller than that of conventional disk systems.
Reference: [18] <author> Kleinrock, L. </author> <title> Queueing Systems, volume 1. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1975. </year> <month> 36 </month>
Reference-contexts: For ease of analysis, we approximate the seek distance D as a continuous r:v: with probability density function (pdf ) f D (x) = p s u 0 (x); x = 0; 2 (Cx) where u 0 (x) is the unit impulse function (see <ref> [18] </ref> p. 342). <p> Thus, this system can be modeled as a M=G=1 queue. The service time is Y = Y r + (1 )Y w with moments Y = X + T + p w (R + T ); By the Pollaczek-Khinchin formula <ref> [18] </ref>, the mean waiting time in the queue is Q = 2 (1 ) where = Y is the device utilization.
Reference: [19] <author> Kleinrock, L. </author> <title> Queueing Systems, volume 2. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference: [20] <author> Lee, E. K. and Katz, R. H. </author> <title> Performance consequences of parity placement in disk arrays. </title> <booktitle> In Proc. 4th Int'l Conf. on Archi. Sup. for Prog. Lang. and OS, </booktitle> <pages> pages 190-199, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID). <p> Thus it is not necessary in this case to read the old data and old parity blocks. In order to prevent the parity update operation from becoming a bottleneck, parity blocks are rotated among the N disks in RAID 5. Numerous parity placement strategies have been studied in <ref> [20] </ref>. Notice that for a partial stripe write, the parity update operation cannot proceed before the corresponding data block (s) has (have) been read out, since the calculation of a new parity block is based on the old data information (see Equation (1)).
Reference: [21] <author> Lee, E. K. and Katz, R. H. </author> <title> An analytic performance model of disk arrays. </title> <booktitle> In Proc. SIGMETRICS'93, </booktitle> <pages> pages 98-109, </pages> <year> 1993. </year>
Reference-contexts: This and other analyses are of independent interest. Other related works on performance evaluation of disk arrays can be found in [8, 22, 37, 31]. A recent paper, Lee and Katz <ref> [21] </ref> presents an analytic model for disk arrays. However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined. Therefore, the high cost suffered by partial stripe writes under RAID 5 is ignored.
Reference: [22] <author> Livny, M., Khoshafian, S., and Boral, H. </author> <title> Multi-disk management algorithm. </title> <booktitle> In Proc. SIGMETRICS Conf., </booktitle> <pages> pages 69-77, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: For example, the RAID 5 architecture is approximately but accurately analyzed via decomposition where each disk is modeled as a priority queuing system. This and other analyses are of independent interest. Other related works on performance evaluation of disk arrays can be found in <ref> [8, 22, 37, 31] </ref>. A recent paper, Lee and Katz [21] presents an analytic model for disk arrays. However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined.
Reference: [23] <author> Lynch, W. C. </author> <title> Do disk arms move? Perform. Eval. </title> <journal> Rev., </journal> <volume> 1 </volume> <pages> 3-16, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: Z r ; Z w ; Z: r:v:'s denoting read, write, and overall I/O response times. First, consider the disk seek pattern. Since it is observed that in reality, most of the time the disk arm doesn't move <ref> [23, 17] </ref>, we introduce a sequential access probability, p s , which is defined to be the probability that the seek distance is equal to zero. We identify two kinds of access patterns, the sequential access pattern and non-sequential access pattern.
Reference: [24] <author> Menon, J. and Kasson, J. </author> <title> Methods for improved update performance of disk arrays. </title> <type> Technical Report RJ 6928 (66034), </type> <institution> IBM Almaden Research Center, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Researchers have looked at different ways 33 to reduce this cost. The most attractive way is the Log-Structured File System [32, 9, 38, 39], in which many small writes are accumulated in cache and converted into a large full stripe write. Another way is called floating parity <ref> [24] </ref>, where parity blocks are clustered into cylinders each containing a spare track. A new parity block, instead of being written in place, is written on the nearest unallocated block following the old parity block.
Reference: [25] <author> Menon, J., Mattson, D., and Ng, S. </author> <title> Performance of disk arrays in transaction processing environments. </title> <type> Technical Report RJ 8230 (75424), </type> <institution> IBM Almaden Research Center, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: We present analytic and simulation results for disk array architectures coupled with different scheduling policies. Since it has been observed in the literature that for applications such as transaction processing systems and workstation/engineering environments, I/O requests typically access a small amount of data <ref> [25, 10, 26, 33] </ref>, we assume that each request accesses a single block of data (4096 bytes [31]) and that each distinct block is equally likely to be accessed.
Reference: [26] <author> Merchant, A. and Yu, P. S. </author> <title> Modeling and comparisons of striping strategies in data replication architectures. </title> <type> Technical report, </type> <institution> IBM Thomas J. Watson Research Center, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: We present analytic and simulation results for disk array architectures coupled with different scheduling policies. Since it has been observed in the literature that for applications such as transaction processing systems and workstation/engineering environments, I/O requests typically access a small amount of data <ref> [25, 10, 26, 33] </ref>, we assume that each request accesses a single block of data (4096 bytes [31]) and that each distinct block is equally likely to be accessed.
Reference: [27] <author> Muntz, R. R. and Lui, J. C. </author> <title> Performance analysis of disk arrays under failure. </title> <booktitle> In Proc. 16th VLDB Conf., </booktitle> <pages> pages 162-173, </pages> <address> Brisbane, Australia, </address> <year> 1990. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability <ref> [43, 8, 27] </ref>. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [28] <author> Nelson, M., Welch, B., and Ousterhout, J. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Trans. on Comp. Sys., </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Therefore, the high cost suffered by partial stripe writes under RAID 5 is ignored. A measurement study of RAID 3 can be found in [40]. I/O performance can also be improved by introducing a disk cache <ref> [42, 28] </ref>. The effectiveness of a disk cache depends on the I/O access pattern as well as the cache size. For applications where disk accesses show a high locality, disk caching may satisfy most of the read requests and therefore reduce the I/O traffic to the disk.
Reference: [29] <author> Ng, S. </author> <title> Some design issues of disk array. </title> <booktitle> In Proc. IEEE Spring ComCon, </booktitle> <pages> pages 137-142, </pages> <year> 1989. </year>
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [30] <author> Ng, S., Lang, D., and Selinger, R. </author> <title> Trade-offs between devices and paths in achieving disk interleaving. </title> <booktitle> In Proc. 15th Intern. Sym. on Comp. Archit., </booktitle> <pages> pages 196-201, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: This has led to a big gap between the speed of the CPU/main memory and that of the disk I/O subsystem <ref> [30] </ref> which is expected to increase further in the near future. Based on these observations, we predict that further increases in the processor speed will bring little gain to the overall system performance. <p> The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [31] <author> Ogata, M. and Flynn, M. </author> <title> A queueing analysis for disk array systems. </title> <type> Technical Report CSL-TR-90-443, </type> <institution> Stanford Univ., </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: For example, the RAID 5 architecture is approximately but accurately analyzed via decomposition where each disk is modeled as a priority queuing system. This and other analyses are of independent interest. Other related works on performance evaluation of disk arrays can be found in <ref> [8, 22, 37, 31] </ref>. A recent paper, Lee and Katz [21] presents an analytic model for disk arrays. However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined. <p> Removal of the conditioning yields P fD = ig = p s ; i = 0; 2 (Ci) (2) We also use the following expression for the seek time, S, as a function of the seek distance D <ref> [44, 2, 3, 31] </ref>, ( a + b D; D &gt; 0; where a is the arm acceleration time and b is the mechanical seek factor. <p> used in our study are summarized as follows: number of cylinders C = 1200; transfer rate= 3MB/sec; full rotation time R max = 16:7 ms; block size = 4096 bytes; number of blocks per track N b = 12; acceleration time a = 3 ms; seek factor b = 0:5 <ref> [31] </ref>. <p> Since it has been observed in the literature that for applications such as transaction processing systems and workstation/engineering environments, I/O requests typically access a small amount of data [25, 10, 26, 33], we assume that each request accesses a single block of data (4096 bytes <ref> [31] </ref>) and that each distinct block is equally likely to be accessed. In this case, although the rotations of the disks are synchronized, the arms are not and, consequently, the disks can service requests independently of each other.
Reference: [32] <author> Ousterhout, J. K. and Douglis, F. </author> <title> Beating the I/O bottleneck: A case for log-structured file systems. </title> <journal> Operating System Rev., </journal> <volume> 23(1) </volume> <pages> 11-28, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: During the past several years, CPU speeds have increased at a rate of 40% to 100% per year [1, 13], whereas disk seek times have only improved by 7% per year <ref> [11, 32] </ref>. This has led to a big gap between the speed of the CPU/main memory and that of the disk I/O subsystem [30] which is expected to increase further in the near future. <p> Last, from the above studies, we observe that the main impact on the performance of RAID 5 is the high cost of the partial stripe write. Researchers have looked at different ways 33 to reduce this cost. The most attractive way is the Log-Structured File System <ref> [32, 9, 38, 39] </ref>, in which many small writes are accumulated in cache and converted into a large full stripe write. Another way is called floating parity [24], where parity blocks are clustered into cylinders each containing a spare track.
Reference: [33] <author> Ousterhout, J. K. et al. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proc. 10th Sym. on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: We present analytic and simulation results for disk array architectures coupled with different scheduling policies. Since it has been observed in the literature that for applications such as transaction processing systems and workstation/engineering environments, I/O requests typically access a small amount of data <ref> [25, 10, 26, 33] </ref>, we assume that each request accesses a single block of data (4096 bytes [31]) and that each distinct block is equally likely to be accessed.
Reference: [34] <author> Patterson, D. A., Chen, P., Gibson, G., and Katz, R. H. </author> <title> Introduction to redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proc. IEEE Spring ComCon., </booktitle> <pages> pages 112-117, </pages> <year> 1989. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in <ref> [35, 34, 14] </ref> referred to as Redundant Arrays of Inexpensive Disks (RAID). Among the five, the two most promising candidates for high performance computing systems appear to be the mirrored disk array (RAID 1) and the rotated parity array (RAID 5). <p> We consider several variations that differ from each other according to how the data is placed on the disks. These include mirrored declustering <ref> [34, 8] </ref>, chained declustering [12], and a new variant, group-rotate declustering, which combines advantages of both mirrored and chained declustering. Mirrored Declustering: In mirrored declustering, the N disks are configured as N=2 pairs of mirrored disks, with the i-th pair termed mirror i.
Reference: [35] <author> Patterson, D. A., Gibson, G., and Katz, R. H. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 109-116, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in <ref> [35, 34, 14] </ref> referred to as Redundant Arrays of Inexpensive Disks (RAID). Among the five, the two most promising candidates for high performance computing systems appear to be the mirrored disk array (RAID 1) and the rotated parity array (RAID 5).
Reference: [36] <author> Poston, A. </author> <title> A high performance file system for UNIX. </title> <booktitle> In Proc. USENIX, </booktitle> <pages> pages 215-226, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [37] <author> Reddy, A. L. N. and Banerjee, P. </author> <title> An evaluation of multiple-disk I/O systems. </title> <journal> IEEE Trans. Comp., </journal> <volume> 38(12) </volume> <pages> 1680-1690, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: For example, the RAID 5 architecture is approximately but accurately analyzed via decomposition where each disk is modeled as a priority queuing system. This and other analyses are of independent interest. Other related works on performance evaluation of disk arrays can be found in <ref> [8, 22, 37, 31] </ref>. A recent paper, Lee and Katz [21] presents an analytic model for disk arrays. However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined.
Reference: [38] <author> Rosenblum, M. and Ousterhout, J. K. </author> <title> The LFS storage manager. </title> <booktitle> In Proc. Summer '90 USENIX Tech. Conf., </booktitle> <pages> pages 315-324, </pages> <address> Anaheim, CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Last, from the above studies, we observe that the main impact on the performance of RAID 5 is the high cost of the partial stripe write. Researchers have looked at different ways 33 to reduce this cost. The most attractive way is the Log-Structured File System <ref> [32, 9, 38, 39] </ref>, in which many small writes are accumulated in cache and converted into a large full stripe write. Another way is called floating parity [24], where parity blocks are clustered into cylinders each containing a spare track.
Reference: [39] <author> Rosenblum, M. and Ousterhout, J. K. </author> <title> The design and implementation of a log-structured fils system. </title> <type> Draft, </type> <month> March </month> <year> 1991. </year>
Reference-contexts: Last, from the above studies, we observe that the main impact on the performance of RAID 5 is the high cost of the partial stripe write. Researchers have looked at different ways 33 to reduce this cost. The most attractive way is the Log-Structured File System <ref> [32, 9, 38, 39] </ref>, in which many small writes are accumulated in cache and converted into a large full stripe write. Another way is called floating parity [24], where parity blocks are clustered into cylinders each containing a spare track.
Reference: [40] <author> Ruwart, T. and O'Keefe, M. </author> <title> Performance characteristics of a 100 megabyte/second disk array. </title> <booktitle> Presented in Storage & Interface'94, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: However, in these works, reads and writes are treated in the same way when the performance of RAID 5 is examined. Therefore, the high cost suffered by partial stripe writes under RAID 5 is ignored. A measurement study of RAID 3 can be found in <ref> [40] </ref>. I/O performance can also be improved by introducing a disk cache [42, 28]. The effectiveness of a disk cache depends on the I/O access pattern as well as the cache size.
Reference: [41] <author> Salem, K. and Garcia-Molina, H. </author> <title> Disk striping. </title> <booktitle> In Proc. IEEE Data Engineering Conf., </booktitle> <pages> pages 336-342, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1986. </year>
Reference-contexts: One attractive idea is the so-called disk array, where the disk I/O subsystem consists of multiple disks with data spread over these disks. The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al <ref> [41] </ref>, respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability [43, 8, 27].
Reference: [42] <author> Schroeder, M., Gifford, D., and Needham, R. </author> <title> A caching file system for a programmer's workstation. </title> <booktitle> In Proc. 10th Sym. on Operating System Principles, </booktitle> <pages> pages 25-34, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Therefore, the high cost suffered by partial stripe writes under RAID 5 is ignored. A measurement study of RAID 3 can be found in [40]. I/O performance can also be improved by introducing a disk cache <ref> [42, 28] </ref>. The effectiveness of a disk cache depends on the I/O access pattern as well as the cache size. For applications where disk accesses show a high locality, disk caching may satisfy most of the read requests and therefore reduce the I/O traffic to the disk.
Reference: [43] <author> Schulze, M., Gibson, G., Katz, R., and Patterson, D. </author> <title> How reliable is a RAID? In Proc. </title> <booktitle> IEEE Spring ComCon Conf., </booktitle> <pages> pages 118-123, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: Since then, a great deal of work has focused on various design issues related to the performance of disk arrays [30, 29, 17, 20, 36, 4, 45] and to their reliability <ref> [43, 8, 27] </ref>. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
Reference: [44] <author> Scranton, R. A. and Thompson, D. A. </author> <title> The access time myth. </title> <type> Technical Report RC 10197, </type> <institution> IBM, </institution> <month> Sept. </month> <year> 1983. </year>
Reference-contexts: Removal of the conditioning yields P fD = ig = p s ; i = 0; 2 (Ci) (2) We also use the following expression for the seek time, S, as a function of the seek distance D <ref> [44, 2, 3, 31] </ref>, ( a + b D; D &gt; 0; where a is the arm acceleration time and b is the mechanical seek factor.
Reference: [45] <author> Stonebraker, M. and Schloss, G. A. </author> <title> Distributed RAID anew multiple copy algorithm. </title> <booktitle> In Proc. 6th Int'l. Conf. on Data Engineering, </booktitle> <pages> pages 430-437, </pages> <month> February </month> <year> 1990. </year> <month> 38 </month>
Reference-contexts: The ideas of disk interleaving and disk striping were first introduced by Kim [16] and Salem et al [41], respectively. Since then, a great deal of work has focused on various design issues related to the performance of disk arrays <ref> [30, 29, 17, 20, 36, 4, 45] </ref> and to their reliability [43, 8, 27]. Disk array architectures fall into one of five different classes proposed in [35, 34, 14] referred to as Redundant Arrays of Inexpensive Disks (RAID).
References-found: 45

