URL: ftp://ftp.cs.unc.edu/pub/users/manocha/PAPERS/LOCALIZATION/localization.ps.Z
Refering-URL: http://www.cs.unc.edu/~geom/chron.html
Root-URL: http://www.cs.unc.edu
Email: wallack@robotics.eecs.berkeley.edu  manocha@cs.unc.edu  
Title: Robust Algorithms for Object Localization  
Author: Aaron S. Wallack Dinesh Manocha 
Note: Supported in part by Junior Faculty Award, University Research Award, NSF Grant CCR-9319957 and ARPA Contract DABT63-93-C-0048.  
Address: Berkeley, CA 94720  Chapel Hill, NC 27599-3175  
Affiliation: Computer Science Division University of California  Department of Computer Science University of North Carolina  
Abstract: Object localization using sensed data features and corresponding model features is a fundamental problem in machine vision. We reformulate object localization as a least squares problem: the optimal pose estimate minimizes the squared error (discrepancy) between the sensed and predicted data. The resulting problem is non-linear and previous attempts to estimate the optimal pose using local methods such as gradient descent suffer from local minima and, at times, return incorrect results. In this paper, we describe an exact, accurate and efficient algorithm based on resultants, linear algebra, and numerical analysis, for solving the nonlinear least squares problem associated with localizing two-dimensional objects given two-dimensional data. This work is aimed at tasks where the sensor features and the model features are of different types and where either the sensor features or model features are points. It is applicable to localizing modeled objects from image data, and estimates the pose using all of the pixels in the detected edges. The algorithm's running time depends mainly on the type of non-point features, and it also depends to a small extent on the number of features. On a SPARC 10, the algorithm takes a few microseconds for rectilinear features, a few milliseconds for linear features, and a few seconds for circular features. fl Supported by Fannie and John Hertz Fellowship
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Standard Mathematical Tables. Chemical Rubber Company, </institution> <year> 1973. </year>
Reference-contexts: Thereby rectilinear features can be localized faster because the fourth order polynomial in t can be solved exactly in constant time <ref> [1] </ref>. jM (t)j a 0 = 2 fl ax2 fl ay1 fl ay1t1 2 fl ax1 fl ax1t1 fl ay2 + 4 fl at1 fl ax2 fl ay2 2 fl ax1t1 2 fl ay2 4 fl ax1 fl ax1t4 fl ay2 16 fl a fl ax2 fl ay2 a 2 =
Reference: [2] <author> Anderson, E., Bai, Z., Bischof, C., Demmel, J., Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., Sorensen, and D. </author> <title> LAPACK User's Guide, Release 1.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Routines for these matrix computations are available as part of standard numerical linear algebra libraries. In particular, we used LAPACK <ref> [2] </ref> and EISPACK [8] routines in our implementation of the algorithm. 2.4 Problem Specification Given a set of point features in one frame of reference and non-point features defined in another frame of reference, we determine the transformation which optimally maps the features onto each other. <p> It involves matrix inversion and matrix products. In this section we assume that the leading matrix, M 4 , is numerically well-conditioned. The eigenvalues of E are exactly the roots of the equations jM (t)j = 0 [19] computed using routines from LAPACK <ref> [2] </ref> and EISPACK [8]. Solving for the roots of M (t) in this manner is numerically better conditioned than solving the resultant polynomial formed by expanding the resultant matrix.
Reference: [3] <author> N. Ayache and O. D. Faugeras. </author> <title> Hyper: A new approach for the recognition and positioning of two-dimensional objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1) </volume> <pages> 44-54, </pages> <year> 1986. </year>
Reference-contexts: In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23]. <p> Many localization algorithms have relied on Kalman filters to match points to non-point features. Kalman filters suffer from local minima problems as well. Given a set of points and corresponding non-point features, Ayache and Faugeras <ref> [3] </ref> attempted to compute the least squares error pose by iteration and by Kalman filters. Kalman filter-based approaches have demonstrated success in various machine-vision minimization applications [17, 28, 4]. Kalman filter-based approaches suffer from two problems: sensitivity to local minima problems, and the requirement of multiple data points.
Reference: [4] <author> R. Deriche and O. Faugeras. </author> <title> Tracking line segments. </title> <journal> Image and Vision Computing, </journal> <volume> 8 </volume> <pages> 261-270, </pages> <year> 1990. </year>
Reference-contexts: Kalman filters suffer from local minima problems as well. Given a set of points and corresponding non-point features, Ayache and Faugeras [3] attempted to compute the least squares error pose by iteration and by Kalman filters. Kalman filter-based approaches have demonstrated success in various machine-vision minimization applications <ref> [17, 28, 4] </ref>. Kalman filter-based approaches suffer from two problems: sensitivity to local minima problems, and the requirement of multiple data points.
Reference: [5] <author> O. D. Faugeras and M. Hebert. </author> <title> The representation, recognition, and locating of 3-d objects. </title> <journal> International Journal of Robotics Research, </journal> <volume> 5(3) </volume> <pages> 27-52, </pages> <year> 1986. </year>
Reference-contexts: Unlike Kalman filtering based approaches to the same problem, this algorithm is immune to problems of local minima, and unlike approaches which reduce the problem to matching point features which can be solved exactly <ref> [16, 26, 5] </ref>, this algorithm does not introduce error by sampling points from model features. Object Edge Detected Pixels from the edge detected pixels and the model features. <p> In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23]. <p> The crucial observation was that the relative position, (x; y), of the point sets could be determined exactly irrespective of the orientation, since the average of the x-coordinates and y-coordinates was invariant to rotation. This decoupling technique is valid for three dimensions as well <ref> [5] </ref>. To reduce the problem of matching points to non-point features to the solvable problem of matching points to points, points are sampled from the non-point features. <p> Such approaches solve an artificial problem, that of minimizing the error between virtual features and model features rather than minimizing the error between the sensed data and the modeled objects. Faugeras and Hebert <ref> [5] </ref> presented an algorithm for localizing three-dimensional objects using pointwise positional information. In their approach, virtual features were constructed from sensed data and then, objects were identified and localized from the features, rather than the sensed data.
Reference: [6] <author> O.D. Faugeras and S. Maybank. </author> <title> Motion from point matches: Multiplicity of solutions. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4 </volume> <pages> 225-246, </pages> <year> 1990. </year>
Reference-contexts: In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23]. <p> Kalman filter-based approaches suffer from two problems: sensitivity to local minima problems, and the requirement of multiple data points. A great deal of work has been done on matching features (points to points, lines to lines, points to features etc.) in the context of interpreting structure from motion <ref> [6, 13] </ref>. The problem involves matching sensor features from two distinct two-dimensional views of an object; the problem of motion computation is reduced to solving systems of nonlinear algebraic equations. In practice, the resulting system of equations is overconstrained and the coefficients of the equations are inaccurate due to noise.
Reference: [7] <author> D. Forsyth, L. Mundy, A. Zisserman, A. Heller, and C. Rothwell. </author> <title> Invariant descriptors fo 3-d object recognition and pose. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 971-991, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23]. <p> In their approach, virtual features were constructed from sensed data and then, objects were identified and localized from the features, rather than the sensed data. Forsyth et al. described a three-dimensional localization technique which represented the data by parameterized conic sections <ref> [7] </ref>. Objects were recognized and localized using the algebraic parameters characterizing the conic sections. Recently, Ponce, Hoogs, and Kriegman have presented algorithms to compute the pose of curved three-dimensional objects [21, 22].
Reference: [8] <author> B.S. Garbow, J.M. Boyle, J. Dongarra, and C.B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: Routines for these matrix computations are available as part of standard numerical linear algebra libraries. In particular, we used LAPACK [2] and EISPACK <ref> [8] </ref> routines in our implementation of the algorithm. 2.4 Problem Specification Given a set of point features in one frame of reference and non-point features defined in another frame of reference, we determine the transformation which optimally maps the features onto each other. <p> It involves matrix inversion and matrix products. In this section we assume that the leading matrix, M 4 , is numerically well-conditioned. The eigenvalues of E are exactly the roots of the equations jM (t)j = 0 [19] computed using routines from LAPACK [2] and EISPACK <ref> [8] </ref>. Solving for the roots of M (t) in this manner is numerically better conditioned than solving the resultant polynomial formed by expanding the resultant matrix.
Reference: [9] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: model position (in the sensor reference frame). (b): The model's position can be expressed as the transformation between the sensor frame and the model frame. 9 The algorithm for solving non-linear equations linearizes the problem using resultants and uses matrix computations such as eigendecomposition, singular value decomposition and Gaussian elimination <ref> [9] </ref>. Routines for these matrix computations are available as part of standard numerical linear algebra libraries. <p> lead to numerically invertible largest exponent matrices, we can use the generalized eigenvalue formulation. 4.3.3 Generalized Eigenvalue Formulation In case M 4 is singular or numerically ill-conditioned and none of the transformations produces a well conditioned matrix, we reduce root finding to a generalized eigenvalue problem of a matrix pencil <ref> [9] </ref>. The matrix pencil for the generalized eigenvalue problem is constructed from the matrix polynomial in the following manner.
Reference: [10] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Model-based recognition and localization are fundamental tasks in analyzing and understanding two-dimensional and three-dimensional scenes <ref> [12, 10] </ref>. There are many applications for vision systems: inspection and quality control, mobile robot navigation, monitoring and surveillance. The input to such systems typically consists of two-dimensional image data or range data. <p> Since the object O can have any orientation or location, the localization problem is to compute the pose p for a given object O which best fits the sensed data. The problems of recognition and localization have been extensively studied in the vision literature <ref> [12, 10] </ref>. The majority of the object recognition algorithms utilize edge-based data, where edges are extracted from image data using filter-based methods. Edge data is either comprised of pixels which have exceeded some threshold, or edge segments formed by straight line approximations of those pixels.
Reference: [11] <author> W. Eric L. Grimson and Tomas Lozano-Perez. </author> <title> Model-based recognition and localization from sparse range or tactile data. </title> <journal> International Journal of Robotics Research, </journal> <volume> 3(3) </volume> <pages> 3-35, </pages> <year> 1984. </year>
Reference-contexts: In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23].
Reference: [12] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> McGraw-Hill, </publisher> <address> seventh edition, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Model-based recognition and localization are fundamental tasks in analyzing and understanding two-dimensional and three-dimensional scenes <ref> [12, 10] </ref>. There are many applications for vision systems: inspection and quality control, mobile robot navigation, monitoring and surveillance. The input to such systems typically consists of two-dimensional image data or range data. <p> Since the object O can have any orientation or location, the localization problem is to compute the pose p for a given object O which best fits the sensed data. The problems of recognition and localization have been extensively studied in the vision literature <ref> [12, 10] </ref>. The majority of the object recognition algorithms utilize edge-based data, where edges are extracted from image data using filter-based methods. Edge data is either comprised of pixels which have exceeded some threshold, or edge segments formed by straight line approximations of those pixels.
Reference: [13] <author> B. K. P. Horn. </author> <title> Relative orientation revisited. </title> <journal> Journal of Optical Society of America, </journal> <volume> 8(10) </volume> <pages> 1630-1638, </pages> <year> 1991. </year>
Reference-contexts: Kalman filter-based approaches suffer from two problems: sensitivity to local minima problems, and the requirement of multiple data points. A great deal of work has been done on matching features (points to points, lines to lines, points to features etc.) in the context of interpreting structure from motion <ref> [6, 13] </ref>. The problem involves matching sensor features from two distinct two-dimensional views of an object; the problem of motion computation is reduced to solving systems of nonlinear algebraic equations. In practice, the resulting system of equations is overconstrained and the coefficients of the equations are inaccurate due to noise. <p> They are rather slow in practice and in the context of floating point computations suffer from numerical problems. Other global techniques include homotopy methods. However, they are rather slow for practical applications <ref> [13] </ref>. In this paper, we use some recently developed algorithms based on multipolynomial resultants and matrix computations described by Manocha [19].
Reference: [14] <author> D. P. Huttenlocher and S. Ullman. </author> <title> Recognizing solid objects by alignment with an image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2) </volume> <pages> 195-212, </pages> <year> 1990. </year>
Reference-contexts: In machine vision applications, localization is usually reformulated as a least squares error problem. Some methods have focused on interesting sensor features such as vertices, and ignored the remainder of the image data <ref> [11, 5, 3, 14, 6, 7] </ref>, thereby reducing the problem to a problem of matching similar features. These approaches are likely to be suboptimal because they utilize a subset of the data [23]. <p> Gradient descent methods require an initial search point and suffer from problems of local minima. Huttenlocher and Ullman used gradient descent techniques to improve the pose estimates developed from comparing sets of three model point features and three image point features <ref> [14] </ref>. Many localization algorithms have relied on Kalman filters to match points to non-point features. Kalman filters suffer from local minima problems as well.
Reference: [15] <author> C. Jerian and R. Jain. </author> <title> Polynomial methods for structure from motion. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(12) </volume> <pages> 1150-1166, </pages> <year> 1990. </year>
Reference-contexts: As a result, the optimal solution is usually found using gradient descent methods on related least squares problems. Most of the local methods known in the literature do not work well in practice <ref> [15] </ref> and an algorithm based on global minimization is greatly desired. Sparse sensors have recently demonstrated success in recognition and localization applications [27, 20]. The need for accurate localization algorithms has arisen with the emergence of these sparse probing sensor techniques.
Reference: [16] <author> A. Kalvin, E. Schonberg, J. T. Schwartz, and M. Sharir. </author> <title> Two-dimensional model-basedboundary matching using footprints. </title> <journal> International Journal of Robotics Research, </journal> <volume> 5(4) </volume> <pages> 38-55, </pages> <year> 1986. </year>
Reference-contexts: Unlike Kalman filtering based approaches to the same problem, this algorithm is immune to problems of local minima, and unlike approaches which reduce the problem to matching point features which can be solved exactly <ref> [16, 26, 5] </ref>, this algorithm does not introduce error by sampling points from model features. Object Edge Detected Pixels from the edge detected pixels and the model features. <p> The main advantage of reducing the problem of matching points to non-point features to the problem of matching point sets is that the problem is exactly solvable in constant time. For two-dimensional rigid transformations, there are only three degrees of freedom: x; y; <ref> [16, 26] </ref>. By decoupling these degrees of freedom, the resultant problems become linear, and are exactly solvable in constant time.
Reference: [17] <author> D. Koller, K. Daniilidis, and H.-H. Nagel. </author> <title> Model-based object tracking in monocular image sequences of road traffic scenes. </title> <journal> International Journal of Computer Vision, </journal> <volume> 10(3) </volume> <pages> 257-281, </pages> <year> 1993. </year>
Reference-contexts: Kalman filters suffer from local minima problems as well. Given a set of points and corresponding non-point features, Ayache and Faugeras [3] attempted to compute the least squares error pose by iteration and by Kalman filters. Kalman filter-based approaches have demonstrated success in various machine-vision minimization applications <ref> [17, 28, 4] </ref>. Kalman filter-based approaches suffer from two problems: sensitivity to local minima problems, and the requirement of multiple data points.
Reference: [18] <author> F.S. </author> <title> Macaulay. On some formula in elimination. </title> <booktitle> Proceedings of London Mathematical Society, </booktitle> <pages> pages 3-27, </pages> <month> May </month> <year> 1902. </year>
Reference-contexts: Other global techniques include homotopy methods. However, they are rather slow for practical applications [13]. In this paper, we use some recently developed algorithms based on multipolynomial resultants and matrix computations described by Manocha [19]. Resultants are well known in classical algebraic geometry literature <ref> [24, 18] </ref> and have recently been effectively used in many problems in robotics, computer graphics and geometric modeling [19]. 0 1 Sensed data point corresponding to feature i i Feature i corresponding to sensed data point i 2 3 0 Sensor Frame i Model Frame 2 3 Model Frame 0 1 <p> In this algorithm, we utilize elimination techniques to solve for all of the roots of the multivariate system of equations and thus avoid the problems of local methods. 3.3 Symbolic Representation Elimination theory deals with solving for coordinates of simultaneous solutions of multi-variable systems <ref> [18] </ref>. Resultant methods are used for projection and elimination. We use resultants to solve for one particular coordinate at a time; these methods involve constructing a resultant equation in each coordinate, where the resultant construction depends on the structure of the underlying system of equations. <p> @F F total (X;Y;t) @t 4tF F total (X; Y; t)) (1 + t 2 ) 3 (43) @F F total (X; Y; t) @t 4tF F total (X; Y; t)) (44) The resultant of this system of equations is obtained by eliminating X and Y using the Macaulay construction <ref> [18] </ref>. <p> (Y t )X t + u 0 (Y t ) = 0 (46) t + v 1 (Y t )X t + v 0 (Y t ) = 0 (47) 37 We use the Macaulay resultant to eliminate X t and produce a function in Y t (refer equation (48)) <ref> [18] </ref>. This determinant of the resultant matrix is a third-order matrix polynomial in Y t . This construction is more complex than the construction for the case of linear equations (refer section 4.2.1).
Reference: [19] <author> D. Manocha. </author> <title> Algebraic and Numeric Techniques for Modeling and Robotics. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Other global techniques include homotopy methods. However, they are rather slow for practical applications [13]. In this paper, we use some recently developed algorithms based on multipolynomial resultants and matrix computations described by Manocha <ref> [19] </ref>. Resultants are well known in classical algebraic geometry literature [24, 18] and have recently been effectively used in many problems in robotics, computer graphics and geometric modeling [19]. 0 1 Sensed data point corresponding to feature i i Feature i corresponding to sensed data point i 2 3 0 Sensor <p> In this paper, we use some recently developed algorithms based on multipolynomial resultants and matrix computations described by Manocha <ref> [19] </ref>. Resultants are well known in classical algebraic geometry literature [24, 18] and have recently been effectively used in many problems in robotics, computer graphics and geometric modeling [19]. 0 1 Sensed data point corresponding to feature i i Feature i corresponding to sensed data point i 2 3 0 Sensor Frame i Model Frame 2 3 Model Frame 0 1 Sensed data point corresponding to feature i i Feature i corresponding to sensed data point i Sensor Frame <p> It involves matrix inversion and matrix products. In this section we assume that the leading matrix, M 4 , is numerically well-conditioned. The eigenvalues of E are exactly the roots of the equations jM (t)j = 0 <ref> [19] </ref> computed using routines from LAPACK [2] and EISPACK [8]. Solving for the roots of M (t) in this manner is numerically better conditioned than solving the resultant polynomial formed by expanding the resultant matrix.
Reference: [20] <author> E. Paulos and J. Canny. </author> <title> Informed peg-in-hole insertion using optical sensors. </title> <booktitle> In SPIE Conference on Sensor Fusion VI, 1993. </booktitle> <address> Boston Massachusetts. </address> <month> 50 </month>
Reference-contexts: Most of the local methods known in the literature do not work well in practice [15] and an algorithm based on global minimization is greatly desired. Sparse sensors have recently demonstrated success in recognition and localization applications <ref> [27, 20] </ref>. The need for accurate localization algorithms has arisen with the emergence of these sparse probing sensor techniques.
Reference: [21] <author> J. Ponce, A. Hoogs, and D. J. Kriegman. </author> <title> On using cad models to compute the pose of curved 3d objects. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 55(2) </volume> <pages> 184-197, </pages> <year> 1992. </year>
Reference-contexts: Forsyth et al. described a three-dimensional localization technique which represented the data by parameterized conic sections [7]. Objects were recognized and localized using the algebraic parameters characterizing the conic sections. Recently, Ponce, Hoogs, and Kriegman have presented algorithms to compute the pose of curved three-dimensional objects <ref> [21, 22] </ref>. They relate image observables to models, reformulate the problem geometrically, and utilize algebraic elimination theory to solve a least squares error problem. One drawback of their approach is that they minimize the resultant equation, not the actual error function. <p> One drawback of their approach is that they minimize the resultant equation, not the actual error function. Ponce et al. use the Levenberg-Marquardt algorithm for the least square solution, which may return a local minimum <ref> [21] </ref>. 5 Another approach to object localization involves reducing it to the correct least squares problem, but then solving that least squares problem using gradient descent techniques.
Reference: [22] <author> J. Ponce and D. J. Kriegman. </author> <title> Elimination theory and computer vision: Recognition and positioning of curved 3d objects from range, intensity, or contours. </title> <booktitle> In Symbolic and Numerical Computation for Artificial Intelligence, </booktitle> <pages> pages 123-146, </pages> <year> 1992. </year>
Reference-contexts: Forsyth et al. described a three-dimensional localization technique which represented the data by parameterized conic sections [7]. Objects were recognized and localized using the algebraic parameters characterizing the conic sections. Recently, Ponce, Hoogs, and Kriegman have presented algorithms to compute the pose of curved three-dimensional objects <ref> [21, 22] </ref>. They relate image observables to models, reformulate the problem geometrically, and utilize algebraic elimination theory to solve a least squares error problem. One drawback of their approach is that they minimize the resultant equation, not the actual error function.
Reference: [23] <author> D. Rosen. </author> <title> Errors in digital area measurement of regular 2d figures. </title> <booktitle> In Vision Geometry II, </booktitle> <pages> pages 26-32, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: These approaches are likely to be suboptimal because they utilize a subset of the data <ref> [23] </ref>. In order to utilize all of the edge data, modeled objects must be compared directly to edge pixel data.
Reference: [24] <author> G. Salmon. </author> <title> Lessons Introductory to the Modern Higher Algebra. G.E. </title> <publisher> Stechert & Co., </publisher> <address> New York, </address> <month> 1885. </month>
Reference-contexts: Other global techniques include homotopy methods. However, they are rather slow for practical applications [13]. In this paper, we use some recently developed algorithms based on multipolynomial resultants and matrix computations described by Manocha [19]. Resultants are well known in classical algebraic geometry literature <ref> [24, 18] </ref> and have recently been effectively used in many problems in robotics, computer graphics and geometric modeling [19]. 0 1 Sensed data point corresponding to feature i i Feature i corresponding to sensed data point i 2 3 0 Sensor Frame i Model Frame 2 3 Model Frame 0 1
Reference: [25] <author> H. Schenck. </author> <title> Theories of Engineering Experimentation. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1979. </year>
Reference-contexts: Finally, we conclude by highlighting the contributions of this work. 2 Background 2.1 Error Model In this section, we describe the error model used for localization. Probability theory provides the basis or reducing the object localization to a least squares error problem <ref> [25] </ref>. Assuming imperfect sensing and imperfect models, our goal is to tolerate these discrepancies and provide the best estimate of the object's pose.
Reference: [26] <author> J. T. Schwartz and M. Sharir. </author> <title> Identification of partially obscured objects in three dimensions by matching noisy characteristic curves. </title> <journal> International Journal of Robotics Research, </journal> <volume> 6(2) </volume> <pages> 29-44, </pages> <year> 1987. </year>
Reference-contexts: Unlike Kalman filtering based approaches to the same problem, this algorithm is immune to problems of local minima, and unlike approaches which reduce the problem to matching point features which can be solved exactly <ref> [16, 26, 5] </ref>, this algorithm does not introduce error by sampling points from model features. Object Edge Detected Pixels from the edge detected pixels and the model features. <p> The main advantage of reducing the problem of matching points to non-point features to the problem of matching point sets is that the problem is exactly solvable in constant time. For two-dimensional rigid transformations, there are only three degrees of freedom: x; y; <ref> [16, 26] </ref>. By decoupling these degrees of freedom, the resultant problems become linear, and are exactly solvable in constant time.
Reference: [27] <author> A. Wallack, J. Canny, and D. Manocha. </author> <title> Object localization using crossbeam sensing. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <volume> volume 1, </volume> <pages> pages 692-699, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Most of the local methods known in the literature do not work well in practice [15] and an algorithm based on global minimization is greatly desired. Sparse sensors have recently demonstrated success in recognition and localization applications <ref> [27, 20] </ref>. The need for accurate localization algorithms has arisen with the emergence of these sparse probing sensor techniques. <p> The need for accurate localization algorithms has arisen with the emergence of these sparse probing sensor techniques. This technique has been applied to recognition and localization using beam sensor probe data and results in pose estimation with positional accuracy of 1 1000 " and orientational accuracy of 0:3 o <ref> [27, ?] </ref>. The algorithm is applicable to both sparse and dense sensing paradigms; in sparse sensing applications, such as probing, a small set of data is collected in each experiment, whereas in dense sensing applications, such as machine vision, an inordinate amount of data is collected in each experiment.

References-found: 27

