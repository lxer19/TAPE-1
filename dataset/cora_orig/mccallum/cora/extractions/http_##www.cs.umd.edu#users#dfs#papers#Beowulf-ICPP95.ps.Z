URL: http://www.cs.umd.edu/users/dfs/papers/Beowulf-ICPP95.ps.Z
Refering-URL: http://www.cs.umd.edu/users/dfs/publications.html
Root-URL: 
Email: ftron, beckerg@cesdis.gsfc.nasa.gov  dfs@cs.umd.edu  
Title: BEOWULF: A PARALLEL WORKSTATION FOR SCIENTIFIC COMPUTATION  
Author: Thomas Sterling Donald J. Becker Daniel Savarese John E. Dorband Udaya A. Ranawake Charles V. Packer 
Address: Greenbelt, MD 20771  College Park, MD 20742  
Note: Excellence in Space Data and Information Sciences Code 930.5 NASA Goddard Space  
Affiliation: Center of  Flight Center  Department of Computer Science University of Maryland  NASA Goddard Space Flight Center  Hughes STX Corp.  
Abstract: Network-of-Workstations technology is applied to the challenge of implementing very high performance workstations for Earth and space science applications. The Beowulf parallel workstation employs 16 PC-based processing modules integrated with multiple Ethernet networks. Large disk capacity and high disk to memory bandwidth is achieved through the use of a hard disk and controller for each processing module supporting up to 16 way concurrent accesses. The paper presents results from a series of experiments that measure the scaling characteristics of Beowulf in terms of communication bandwidth, file transfer rates, and processing performance. The evaluation includes a computational fluid dynamics code and an N-body gravitational simulation program. It is shown that the Beowulf architecture provides a new operating point in performance to cost for high performance workstations, especially for file transfers under favorable conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, K. Johnson, et al. </author> <title> "The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor," </title> <editor> M. Dubois and S.S. Thakkar, editors, </editor> <title> Scalable Shared Memory Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992, </year> <pages> pp. 239-261. </pages>
Reference-contexts: The potential benefits in performance to cost are derived through the exploitation of commodity components while the performance gains are achieved through the concurrent application of multiple processors. The MIT Alewife project <ref> [1] </ref> seeks to provide a fully cache coherent multiprocessor workstation through modifications of the SPARC processor. The Princeton SHRIMP project [2] employs standard low cost Intel Pentium microprocessors in a distributed shared memory context through the addition of a custom communication chip.
Reference: [2] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg, </author> <title> "Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer," </title> <booktitle> Proceedings of the Twenty-First International Symposium on Computer Architecture (ISCA), </booktitle> <address> Chicago, </address> <month> April </month> <year> 1994, </year> <pages> pp. 142-153. </pages>
Reference-contexts: The MIT Alewife project [1] seeks to provide a fully cache coherent multiprocessor workstation through modifications of the SPARC processor. The Princeton SHRIMP project <ref> [2] </ref> employs standard low cost Intel Pentium microprocessors in a distributed shared memory context through the addition of a custom communication chip. While both projects make heavy use of available VLSI components, they require some special purpose elements, extending development time and incurring increased cost.
Reference: [3] <author> D. Boggs, J. Mogul, and C. Kent, </author> <title> "Measured Capacity of an Ethernet: Myths and Reality," </title> <note> WRL Research Report 88/4, </note> <institution> Western Research Laboratory, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Throughput for the 8192 byte tokens at 4 and 7 token exchanges is less than that for 1024 byte tokens processors) because of additional network packet collisions. The minimum and maximum sizes of an Ethernet packet are 64 and 1536 bytes respectively <ref> [3] </ref>. Thus a 64 byte token and a 1024 byte token each require only one Ethernet packet for transmission. However, an 8192 byte token must be broken up into 6 Ethernet packets, increasing the likelihood of collisions on a 1 channel network.
Reference: [4] <author> K. Castagnera, D. Cheng, R. Fatoohi, et al. </author> <title> "Clustered Workstations and their Potential Role as High Speed Compute Processors," </title> <institution> NAS Computational Services Technical Report RNS-94-003, NAS Systems Division, NASA Ames Research Center, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Networks Of Workstations, or NOW <ref> [4] </ref> technology, is emerging as a powerful resource capable of replacing conventional supercomputers for certain classes of applications requiring high performance computers, and at substantially lower cost. Another, less frequently considered, domain is the realization of the high performance workstations themselves from ensembles of less powerful microprocessors.
Reference: [5] <author> B. Fryxell and R. Taam, </author> <title> "Numerical Simulations of Non-Axisymmetric Accretion Flow," </title> <journal> Astrophysical Journal, </journal> <volume> 335, </volume> <year> 1988, </year> <pages> pp. 862-880. </pages>
Reference-contexts: To this end, two full applications from the Earth and space sciences community were selected to bracket the dimension of communication and load balancing demands. A 2-dimensional compressible fluid dynamics code, called Prometheus <ref> [5] </ref>, has been implemented on a number of high performance computers including vector, shared memory, distributed memory, and SIMD architectures. The code solves Euler's equations for gas dynamics on a logically rectangular mesh using the Piecewise Parabolic Method (PPM).
Reference: [6] <author> Intel Corporation, </author> <title> "DX4 Processor Data Book," </title> <year> 1993. </year>
Reference-contexts: The DX4 delivers greater computational power than other members of the 486 family not only from its higher clock speed, but also from its 16 KByte primary cache (twice the size of other 486 primary caches) <ref> [6] </ref>. Each motherboard also contains a 256 KByte secondary cache. Two Ethernets running at peak bandwidths of 10 Mbits per second are used for internode communications, one a twisted pair 10baseT with hub and the other a multidrop 10Base2.
Reference: [7] <author> Linux Documentation Project, </author> <title> Accessible on the Internet at World Wide Web URL http://sunsite.unc.edu/mdw/linux.html. </title>
Reference-contexts: This opportunity is also facilitated by the availability of the Linux operating system <ref> [7] </ref>, a robust Unix-like system environment with source code that is targeted for the x86 family of microprocessors including the Intel Pentium.
Reference: [8] <author> K. Olson and J. Dorband, </author> <title> "An Implementation of a Tree Code on a SIMD Parallel Computer," </title> <journal> Astrophysical Journal Supplement Series, </journal> <month> September </month> <year> 1994. </year>
Reference-contexts: A tree code for performing gravitational N-body simulations has been developed to reduce a classically O (n 2 ) computation to O (n log n) and has been applied to shared memory [9], distributed memory, and SIMD parallel architectures <ref> [8] </ref>. The code is being used to study the structure of gravitating, star forming, interstellar clouds as well as to model the fragmentation of comet Shoemaker-Levy 9 in its close encounter with Jupiter. A range of number of particles were used from 32K to 256K.
Reference: [9] <author> T. Sterling, D. Savarese, P. Merkey, J. Gardner, </author> <title> "An Initial Evaluation of the Convex SPP-1000 for Earth and Space Science Applications," </title> <booktitle> Proceedings of the First International Symposium on High Performance Computing Architecture, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: A tree code for performing gravitational N-body simulations has been developed to reduce a classically O (n 2 ) computation to O (n log n) and has been applied to shared memory <ref> [9] </ref>, distributed memory, and SIMD parallel architectures [8]. The code is being used to study the structure of gravitating, star forming, interstellar clouds as well as to model the fragmentation of comet Shoemaker-Levy 9 in its close encounter with Jupiter.
Reference: [10] <author> V. Sunderam, </author> <title> "PVM: A Framework for Parallel Distributed Computing," </title> <journal> Concurrency: Practice and Experience, </journal> <month> December </month> <year> 1990, </year> <pages> pp. 315-339. </pages>
Reference-contexts: System level applications like NFS are written making direct use of sockets, but most user level parallel programs use some higher level interface usually PVM <ref> [10] </ref>. message-passing interfaces. It shows the round trip time on one network channel across 16 processors for tokens of sizes ranging from 4 to 16384 bytes using PVM 3.3 versus BSD sockets and UDP.
References-found: 10

