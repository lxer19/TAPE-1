URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3738.alternative.discrete.time.operators-RNNs.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Title: Alternative Discrete-Time Operators and Their Application to Nonlinear Models  
Author: Andrew D. Back Ah Chung Tsoi Bill G. Horne C. Lee Giles 
Note: The authors were previously with the  Also with  
Address: 2-1 Hirosawa, Wako-shi, Saitama 351-01, Japan  Northfields Avenue, Wollongong Australia  9 Pace Farm Rd. Califon, NJ 07830 USA  4 Independence Way Princeton, NJ 08540. USA  College Park, Md 20742  4072 Australia.  College Park, MD 20742 USA.  
Affiliation: Laboratory for Artificial Brain Systems, Frontier Research Program RIKEN, The Institute of Physical and Chemical Research,  Faculty of Informatics University of Wollongong  AADM Consulting  NEC Research Institute  Institute for Advanced Computer Studies University of Maryland,  Department of Electrical and Computer Engineering, University of Queensland, Brisbane Qld.  Institute for Advanced Computer Studies, University of Maryland,  
Pubnum: Technical Report CS-TR-3738 and UMIACS-TR-97-03  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R.C. Agarwal and C.S. Burrus. </author> <title> New recursive digital filter structures having very low sensitivity and roundoff noise. </title> <journal> IEEE Trans. Circuits, Syst., </journal> <volume> CAS-22(12):921-927, </volume> <year> 1975. </year>
Reference-contexts: From the experiments, some observations and conclusions are drawn in section 9. 2 Alternative Discrete Time Operators There have been a number of ADTOs proposed by various researchers. These include the following: 1. Delta operator. This is defined as <ref> [1] </ref> ffi = where is the discrete-time sampling interval. Agarwal and Burrus first proposed the use of this operator in digital filters to replace the shift operator in an attempt to overcome the LDLF problems [1]. <p> These include the following: 1. Delta operator. This is defined as <ref> [1] </ref> ffi = where is the discrete-time sampling interval. Agarwal and Burrus first proposed the use of this operator in digital filters to replace the shift operator in an attempt to overcome the LDLF problems [1]. Williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from the direct form structure [61], and a number of authors have considered using it in linear filtering, estimation and control [16, 33, 42]. 2. The modified delta operator.
Reference: [2] <author> C. Alippi, V. Piuri, and M. Sami. </author> <title> Sensitivity to errors in artificial neural networks: a behavioural approach. </title> <journal> IEEE Trans. Circuits, Syst. I: Fundamental Theory and Applications, </journal> <volume> 42(6) </volume> <pages> 358-361, </pages> <year> 1995. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [3] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modelling. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 375-385, </pages> <year> 1991. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively <ref> [3, 59] </ref>. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example [8, 12, 15, 18, 20, 25, 36, 37, 43]). <p> k (t) (23) ^x l ( P N l ki z l1 P N 1 ki i x (t) l = 1 The case we consider employs the -operator at the input layer only, however, it is also possible to introduce operators throughout the network as required (see for example <ref> [3] </ref>). The case where each synapse is replaced by gamma operators is considered in [28]. We will refer to the special case when = q as a MLP (q) model.
Reference: [4] <author> A.D. Back and A.C. Tsoi. </author> <title> Stabilisation properties of multilayer feedforward networks with time-delay synapses. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks 1, </booktitle> <volume> volume 2, </volume> <pages> pages 1113-1116, </pages> <address> Helsinki, 1992. </address> <publisher> Elsevier Science Publishers B.V. (North Holland). </publisher>
Reference-contexts: The sigmoid function provides a self-stabilization effect, bounding the output of nonlinear units and preventing any `flow-on' of instabilities from upsetting the rest of the network <ref> [4] </ref>.
Reference: [5] <author> A.D. Back and A.C. Tsoi. </author> <title> A comparison of discrete-time operator models for nonlinear system identification. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 883-890, </pages> <address> Cambridge, MA, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: For adaptive control systems, this gives robustness advantages for systems with unmodelled high frequency characteristics [40]. 6. Pi operator. By defining the bilinear transformation (BLT) as an operator, it is possible to introduce an operator which generalizes all of the above operators. We have proposed the pi operator <ref> [5] </ref> as = (c 3 q + c 4 ) with the restriction that c 1 c 4 6= c 2 c 3 (to ensure is not a constant function [50]). The bilinear mapping produced has a pole at q = c 4 =c 3 .
Reference: [6] <author> A.D. Back and A.C. Tsoi. </author> <title> Constrained pole-zero filters as discrete-time operators for system approximation. </title> <editor> In F. Girosi, J. Makhoul, E. Manolakos, and E. Wilson, editors, </editor> <booktitle> Proc. of the 1995 IEEE Workshop Neural Networks for Signal Processing 5 (NNSP95), </booktitle> <pages> pages 191-200, </pages> <address> New York, NY, 1995. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: This means that the operator is only capable of producing either low pass (for 0 &lt; c 1 &lt; 1), or high pass filtering characteristics (1 &lt; c 1 &lt; 2). More general second order operators and their properties have been considered in <ref> [6] </ref>. 5. Rho operator. The rho operator is defined as ae = c 2 where c 1 ; c 2 are adjustable parameters. The rho operator generalizes the delta and gamma operators. <p> knowledge of the system structure, it may be possible to ensure the model has a unique global minimum which would overcome the problem. 7.2.3 Numerical conditioning as a cost function As indicated above, it is possible to select operator values which improve the numerical conditioning of the data covariance matrix <ref> [6, 19] </ref>. It was shown in [6] that significant improvements can be obtained in the conditioning of the data covariance matrix by appropriate selection of the operator parameters. This provides a means of improving the convergence of the rest of the model. <p> It was shown in <ref> [6] </ref> that significant improvements can be obtained in the conditioning of the data covariance matrix by appropriate selection of the operator parameters. This provides a means of improving the convergence of the rest of the model.
Reference: [7] <author> S.P. </author> <title> Banks. Mathematical Theories of Nonlinear Systems. </title> <publisher> Prentice-Hall, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: In fact, this perception is incorrect, and a substantial amount of work has been done in extending the concept of transfer functions and hence poles and zeros, to nonlinear systems (see for example <ref> [7, 49, 51] </ref>).
Reference: [8] <author> Y. Bengio, R. de Mori, and M. Gori. </author> <title> Learning the dynamic nature of speech with backpropagation for sequences. </title> <journal> Pattern Recogition Letters, </journal> <volume> 13 </volume> <pages> 375-385, </pages> <year> 1992. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [9] <author> J.Y. Choi and C-H. Choi. </author> <title> Sensitivity analysis of multilayer perceptrons with differentiable activation functions. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3 </volume> <pages> 101-107, </pages> <year> 1992. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [10] <author> T.O. de Silva, P.G. de Oliveira, J.C. Principe, and B. de Vries. </author> <title> Generalized feedforward filters with complex poles. </title> <editor> In S.Y. Kung, F. Fallside, J. Aa. Sorenson, and C.A. Kamm, editors, </editor> <booktitle> Proc. of the 1992 IEEE Workshop Neural Networks for Signal Processing 2 (NNSP92), </booktitle> <pages> pages 503-510, </pages> <address> Piscataway, NJ, 1992. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: This operator was originally introduced in [11, 13] as a means of studying neural network models for processing time-varying patterns. 6 4. Second order gamma operators. It is possible to introduce complex poles and zeros into the fl operator <ref> [10] </ref>. This operator is defined as fl 2 = [z (1 c 1 )] + c 2 c 2 (13) where c i , i = 1; 2 are parameters of the fl 2 operator. The results in [10] indicate some success with this method, however, it was observed that a <p> is possible to introduce complex poles and zeros into the fl operator <ref> [10] </ref>. This operator is defined as fl 2 = [z (1 c 1 )] + c 2 c 2 (13) where c i , i = 1; 2 are parameters of the fl 2 operator. The results in [10] indicate some success with this method, however, it was observed that a multimodal mean square error surface may occur in some modeling situations. In the fl 2 operator proposed in [10], while there may be a complex pole pair within the unit circle (if (1 c 1 ) 2 + <p> The results in <ref> [10] </ref> indicate some success with this method, however, it was observed that a multimodal mean square error surface may occur in some modeling situations. In the fl 2 operator proposed in [10], while there may be a complex pole pair within the unit circle (if (1 c 1 ) 2 + c 2 1 c 2 &lt; 1), there is only one zero which lies on the real axis.
Reference: [11] <author> B. de Vries and J.C. Principe. </author> <title> A theory for neural networks with time delays. </title> <editor> In R.P. Lippman, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 162-168, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The modified delta operator. The delta operator can be modified to ensure that it is not marginally stable. The modified delta operator can be defined as ffi m = where 0 &lt; c &lt; 1. 3. Gamma operator. This operator is defined as <ref> [11] </ref> fl = c This is a generalization of the delta operator with the adjustable parameter c. The explicit sampling time variable is implicitly set to one or incorporated in the variable c. <p> Gamma operator. This operator is defined as [11] fl = c This is a generalization of the delta operator with the adjustable parameter c. The explicit sampling time variable is implicitly set to one or incorporated in the variable c. This operator was originally introduced in <ref> [11, 13] </ref> as a means of studying neural network models for processing time-varying patterns. 6 4. Second order gamma operators. It is possible to introduce complex poles and zeros into the fl operator [10]. <p> De Vries and Principe et. al., proposed stochastic gradient descent type algorithms for adjusting the c operator coefficient using a least-squares error criterion <ref> [11, 44] </ref>. For brevity, we omit the updating procedures for the MLP network weights; a variety of methods may be applied (see for example [46, 52]), but instead concentrate on the equations for updating the parameters in the ADTOs.
Reference: [12] <author> B. de Vries and J.C. Principe. </author> <title> The Gamma model anew neural model for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(4) </volume> <pages> 565-576, </pages> <year> 1992. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper. <p> Hence it is evident that the Gamma model will have performance advantages in finite word length models in terms of modelling accuracy and sensitivity, in addition to the architectural advantages described by de Vries and Principe in <ref> [12, 45, 44] </ref>. In this paper, models based on the delta operator, rho operator, and pi operator have been presented which can provide modelling advantages over models based on the shift operator in terms of sensitivity and hence improved robustness properties for finite word length implementations.
Reference: [13] <author> B. de Vries, J.C. Principe, and P.G. de Oliveira. </author> <title> Adaline with adaptive recursive memory. In B.H. Juang, S.Y. Kung, and C.A. Kamm, </title> <editor> editors, </editor> <booktitle> Proc. of the 1991 IEEE Workshop Neural Networks for Signal Processing 1 (NNSP91), </booktitle> <pages> pages 101-110, </pages> <address> New York, NY, 1991. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Gamma operator. This operator is defined as [11] fl = c This is a generalization of the delta operator with the adjustable parameter c. The explicit sampling time variable is implicitly set to one or incorporated in the variable c. This operator was originally introduced in <ref> [11, 13] </ref> as a means of studying neural network models for processing time-varying patterns. 6 4. Second order gamma operators. It is possible to introduce complex poles and zeros into the fl operator [10].
Reference: [14] <author> G. Dundar and K. Rose. </author> <title> The effects of quantization on multilayer perceptrons. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 6(6) </volume> <pages> 1446-1451, </pages> <year> 1995. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [15] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper. <p> TDNN, FIR MLP). Thus, our results highlight the problem of high 2 This is in contrast with modification of the underlying structure of the MLP, e.g., Elman model <ref> [15] </ref>. For further discussion on the differences between these two types of models, see, for example, the survey paper [57]. 5 sensitivity in neural network modelling and indicate a solution which seems to be a promising area of further investigation.
Reference: [16] <author> H. Fan and Q. Li. </author> <title> A ffi-operator recursive gradient algorithm for adaptive signal processing. </title> <booktitle> In Proc. IEEE Int. Conf. Acoust. Speech, Signal Proc., volume III, </booktitle> <pages> pages 492-495. </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: Williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from the direct form structure [61], and a number of authors have considered using it in linear filtering, estimation and control <ref> [16, 33, 42] </ref>. 2. The modified delta operator. The delta operator can be modified to ensure that it is not marginally stable. The modified delta operator can be defined as ffi m = where 0 &lt; c &lt; 1. 3. Gamma operator. <p> One method to do this is by the use of on-line learning algorithms. On-line algorithms to update the operator parameters in the MA () model can be found readily. A recursive least squares (RLS) algorithm was recently derived for delta operator ARMA filters by Fan and Li <ref> [16] </ref>. In the case of the MLP () model, we approach the problem by backpropagating the error information to the input layer and using this to update the operator coefficients. <p> In these experiments we did not seek to minimize the sensitivity by using the regularization term. Experiment 1 The first problem considered is a system identification task arising in the context of high bit rate echo cancellation <ref> [16] </ref>. In this case, the system is described by H (z) = 1 1:957q 1 + 0:957q 2 (54) This system has poles on the real axis at 0.9994, and 0.9577, thus it is an LDLF system.
Reference: [17] <author> H. Fan and M. Nayeri. </author> <title> On error surfaces of sufficient order adaptive IIR filters: proofs and counterexamples to a unimodality conjecture. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37(9):1436, </volume> <year> 1989. </year>
Reference-contexts: It has been shown that the problem of multimodality in linear IIR models lies largely with the difference between the order of the system and the order of the model <ref> [17, 48] </ref>. In particular, reduced order models are known to contribute to the problem of multimodality [24]. For more complex models such as those we consider here, where there are a number of repeated IIR type operators, the situation is not so obvious.
Reference: [18] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 120-130, </pages> <year> 1992. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [19] <author> M. Gevers and G. Li. </author> <title> Parameterizations in control, estimation and filtering problems: accuracy aspects. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: By appropriate setting of the c 1 ; c 2 ; c 3 ; c 4 parameters, the pi operator can be reduced to each of the previous operators. Independently, the same operator was proposed in <ref> [19] </ref> with all coefficients set to unity. <p> Moreover, f j (q)g can be replaced by f j ()g which will result in lower parameter sensitivity in the transfer function, provided the appropriate parametrization of the alternative discrete-time operator is selected <ref> [19] </ref>. <p> knowledge of the system structure, it may be possible to ensure the model has a unique global minimum which would overcome the problem. 7.2.3 Numerical conditioning as a cost function As indicated above, it is possible to select operator values which improve the numerical conditioning of the data covariance matrix <ref> [6, 19] </ref>. It was shown in [6] that significant improvements can be obtained in the conditioning of the data covariance matrix by appropriate selection of the operator parameters. This provides a means of improving the convergence of the rest of the model. <p> This provides a means of improving the convergence of the rest of the model. For discussions on this topic using the delta operator and a special case of the Pi operator, see <ref> [19] </ref>. 8 Experimental results In this section, we provide some basic results which indicate the possible improvements in performance that can be obtained through the use of the proposed alternative discrete-time operators.
Reference: [20] <author> C.L. Giles, G.M. Kuhn, and R.J. Williams. </author> <title> Dynamic recurrent neural networks: </title> <journal> Theory and applications. IEEE Transactions on Neural Networks, </journal> <volume> 5(2), </volume> <year> 1994. </year> <note> Special Issue. </note>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [21] <author> G.C. Goodwin and K.S. </author> <title> Sin. Adaptive Filtering, Prediction and Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year> <month> 22 </month>
Reference-contexts: that if we introduce a shift operator q 1 z (t) 4 = z (t 1), then equation (3) can be written equivalently as follows AE = 1 q 1 : : : q +1 fl 0 Equation (5) is sometimes referred to as the regression vector in the literature <ref> [21] </ref>. Equation (4) can also be expressed in a regression vector form quite easily. It is noted that the performance of the TDNN as a signal model for time varying signals is dependent on the parameters and , as well as the weights.
Reference: [22] <author> J. Hertz, A. Krogh, and R. Palmer. </author> <title> Introduction to Neural Computation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Recent interest has concentrated in deriving various neural network architectures, often based on a modification of the classic multilayer perceptron (MLP) <ref> [22] </ref> for nonlinear functional mapping approximations, for modelling time-dependent signals.
Reference: [23] <author> C.R. Johnson. </author> <title> Adaptive IIR filtering: Current results and open issues. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 30(2) </volume> <pages> 237-250, </pages> <year> 1984. </year>
Reference-contexts: Since the main purpose in this paper is to raise the issue of ADTOs in the context of neural network signal processing, we do not attempt to consider these approaches in detail. Indeed, the problem is a significant open problem in linear systems theory <ref> [23, 53, 55] </ref> and we do not seek to solve the problem in this paper. Rather we present these ideas as possible avenues for future research and as a means of overcoming the difficulties encountered.
Reference: [24] <author> C.R. Johnson and M.G. Larimore. </author> <title> Comments on and additions to `an adaptive recursive filter'. </title> <journal> Proc. IEEE, </journal> <volume> 65(9) </volume> <pages> 1399-1402, </pages> <year> 1977. </year>
Reference-contexts: It is known however, that for IIR filters, the mean square output error (MSOE) surface can be multimodal with respect to parameters, <ref> [24] </ref>. That is, there may exist more than one minimum on the surface. This poses a difficulty for gradient descent algorithms which may become lodged in a local minimum which has a solution significantly worse than an optimal global minimum point. <p> It has been shown that the problem of multimodality in linear IIR models lies largely with the difference between the order of the system and the order of the model [17, 48]. In particular, reduced order models are known to contribute to the problem of multimodality <ref> [24] </ref>. For more complex models such as those we consider here, where there are a number of repeated IIR type operators, the situation is not so obvious. <p> There is a problem with this approach however, in that while we have observed good results using the gradient descent algorithm to adjust the operator parameters, it is known that the mean square error surface of IIR filters may be multimodal <ref> [24, 44] </ref>. Hence there is no guarantee that a gradient descent algorithm will converge to the optimal solution. There are different ways to view this situation. One way is to treat it as a significant problem and seek to find ways to obtain the global minimum.
Reference: [25] <author> M.I. Jordan. </author> <title> Supervised learning and systems with excess degrees of freedom. </title> <type> Technical Report 88-27, </type> <institution> Massachusetts Institute of Technology, COINS, </institution> <year> 1988. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [26] <author> P. Kerlirzin and P. Refregier. </author> <title> Theoretical investigation of the robustness of multilayer perceptrons: analysis of the linear case and extension to nonlinear networks. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 6 </volume> <pages> 560-571, </pages> <year> 1995. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [27] <author> K. Lang, A.H. Waibel, and G.E. Hinton. </author> <title> A time delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-44, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Recent interest has concentrated in deriving various neural network architectures, often based on a modification of the classic multilayer perceptron (MLP) [22] for nonlinear functional mapping approximations, for modelling time-dependent signals. For example, a popular architecture, commonly known as the time delay neural network (TDNN) model <ref> [27, 58] </ref>, is based on the MLP, except that the input signal to each node (input or hidden) can include delayed versions of the same signal. It is known that this method works quite well in a number of applications, e.g., speech processing [58].
Reference: [28] <author> S. Lawrence, A.C. Tsoi, and A.D. </author> <title> Back. The gamma MLP for speech phoneme recognition. </title> <editor> In D.S. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The case where each synapse is replaced by gamma operators is considered in <ref> [28] </ref>. We will refer to the special case when = q as a MLP (q) model. Note that this class of models is new, in that it is specifically designed to reduce the sensitivity of the MLP (q) models.
Reference: [29] <author> A. Von Lehman, E.G. Paek, P.F. Liao, A. Marrakchi, and J.S. Patel. </author> <title> Influence of interconnection weight discretization and noise in an optoelectronic neural network. </title> <journal> Optics Letters, </journal> <volume> 14 </volume> <pages> 928-930, </pages> <year> 1989. </year>
Reference-contexts: Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network. Von Lehman et. al. <ref> [29] </ref> showed that weight discretization in a feedforward network resulted in very poor learning and performance. They observed that networks with less than 300 levels of quantization did not converge, but that adding noise to the network significantly improved the probability of convergence.
Reference: [30] <author> L. Ljung and T. Soderstrom. </author> <title> Theory and Practice of Recursive Identification. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1983. </year>
Reference-contexts: A more powerful updating procedure can be obtained by using the Gauss-Newton method <ref> [30] </ref>. In this case, we replace (46) with (omitting i subscripts for clarity), ^ (t + 1) = ^ (t) + fl (t)R 1 (t) (t)fl 1 ffi (t) (50) where fl (t) is the gain sequence (see [30] for details), fl 1 is a weighting matrix which may be replaced <p> more powerful updating procedure can be obtained by using the Gauss-Newton method <ref> [30] </ref>. In this case, we replace (46) with (omitting i subscripts for clarity), ^ (t + 1) = ^ (t) + fl (t)R 1 (t) (t)fl 1 ffi (t) (50) where fl (t) is the gain sequence (see [30] for details), fl 1 is a weighting matrix which may be replaced by the identity matrix [54], or estimated as [30] ^ fl (t) = ^ fl (t 1) + fl (t) ffi 2 (t) ^ fl (t 1) (51) R (t) is an approximate Hessian matrix, defined by R <p> i subscripts for clarity), ^ (t + 1) = ^ (t) + fl (t)R 1 (t) (t)fl 1 ffi (t) (50) where fl (t) is the gain sequence (see <ref> [30] </ref> for details), fl 1 is a weighting matrix which may be replaced by the identity matrix [54], or estimated as [30] ^ fl (t) = ^ fl (t 1) + fl (t) ffi 2 (t) ^ fl (t 1) (51) R (t) is an approximate Hessian matrix, defined by R (t + 1) = (t)R (t) + i (t) (t) 0 (t) (52) where (t) = 1 i (t). <p> Efficient computation of R 1 may be performed using the matrix inversion lemma [55], factorization methods such as Cholesky decomposition or other fast algorithms. Using the well known matrix inversion lemma <ref> [30] </ref>, we substitute P (t) = R 1 (t), where P (t) = (t) i (t) (t) + i (t) 0 (t)P (t) (t) (53) The initial values of the coefficients are important in determining convergence. Principe et. al. [44] have shown that the error surface may be multimodal.
Reference: [31] <author> M. Mansour, F.J. Kraus, and E.I. Jury. </author> <title> On robust stability of discrete-time systems using delta-operators. </title> <booktitle> In Proc. American Control Conference, </booktitle> <pages> pages 1417-1418. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: In this section, we will only consider the case of operator stability. 3.1 Delta Operator Mansour, Kraus and Jury considered the problem of robust stability of a system described by a delta polynomial whose parameters may be perturbed in a prescribed interval <ref> [31] </ref>.
Reference: [32] <author> P.E. Mantey. </author> <title> Eigenvalue sensitivity and state-variable selection. </title> <journal> IEEE Trans. Automat. Control, </journal> <volume> AC-13(3):263-269, </volume> <year> 1968. </year>
Reference-contexts: The sensitivity of this polynomial is defined as <ref> [32] </ref> d i = i Q (8) It is observed in equation (8), there are two conditions under which the sensitivity is high: 1. i k , i 6= k. This indicates that the roots are close to one another. 2. i has a large magnitude.
Reference: [33] <author> R.H. Middleton and G.C. Goodwin. </author> <title> Digital Control and Estimation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from the direct form structure [61], and a number of authors have considered using it in linear filtering, estimation and control <ref> [16, 33, 42] </ref>. 2. The modified delta operator. The delta operator can be modified to ensure that it is not marginally stable. The modified delta operator can be defined as ffi m = where 0 &lt; c &lt; 1. 3. Gamma operator.
Reference: [34] <author> A.A. Minai and R.D. Williams. </author> <title> Perturbation response in feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(5) </volume> <pages> 783-796, </pages> <year> 1994. </year>
Reference-contexts: Typically, these analyses are based on probabilistic methods. Minai and Williams <ref> [34] </ref> considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network. Von Lehman et. al. [29] showed that weight discretization in a feedforward network resulted in very poor learning and performance.
Reference: [35] <author> T. Mori and I. Troch. </author> <title> Delta domain lyapunov matrix equation a link between continuous and discrete equations. </title> <journal> IEICE Trans. Fundamentals, </journal> <volume> E75-A:451-454, </volume> <year> 1992. </year>
Reference-contexts: The stability region of the delta operator is the interior of the circle given by C : jq + 1=j = 1=, i.e., centered at (1=; 0) with radius 1= <ref> [35] </ref>. 3.2 Modified Delta Operator The stability region for the modified delta operator is the interior of the circle C : jq + c=j = c=, i.e., centered at (c=; 0) with radius c=.
Reference: [36] <author> K. S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 4-27, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [37] <author> K. S. Narendra and K. Parthasarathy. </author> <title> Gradient methods for the optimization of dynamical systems containing neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 252-262, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [38] <author> S.-H. Oh and Y. Lee. </author> <title> Sensitivity analysis of single hidden-layer neural networks with threshold functions. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 6 </volume> <pages> 1005-1007, </pages> <year> 1995. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [39] <author> N.S. Orzechowski, S.R.T. Kumara, and C.R. Das. </author> <title> Performance of multilayer neural networks in binary-to-binary mappings under weight errors. </title> <booktitle> In Proc. ICNN93, </booktitle> <address> San Francisco, </address> <pages> pages 1684-1689. </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [40] <author> M. Palaniswami. </author> <title> A new discrete-time operator for digital estimation and control. </title> <type> Technical Report No. 1, </type> <institution> The University of Melbourne, Department of Electrical Engineering, </institution> <year> 1989. </year>
Reference-contexts: When c 1 = 0, and c 2 = 1, the rho operator reduces to the delta operator [41]. For c 1 = c 2 = c, the rho operator is equivalent to the gamma operator . This operator was introduced in the context of robust adaptive control <ref> [40] </ref>. It has been shown useful improvements over the performance of the delta operator [40, 41] can be obtained. One advantage of the rho operator over the delta operator is that it is stably invertible, allowing the derivation of simpler algorithms [40]. <p> For c 1 = c 2 = c, the rho operator is equivalent to the gamma operator . This operator was introduced in the context of robust adaptive control [40]. It has been shown useful improvements over the performance of the delta operator <ref> [40, 41] </ref> can be obtained. One advantage of the rho operator over the delta operator is that it is stably invertible, allowing the derivation of simpler algorithms [40]. <p> was introduced in the context of robust adaptive control <ref> [40] </ref>. It has been shown useful improvements over the performance of the delta operator [40, 41] can be obtained. One advantage of the rho operator over the delta operator is that it is stably invertible, allowing the derivation of simpler algorithms [40]. The ae operator can be considered as a stable low pass filter, and parameter estimation using the ae operator is low frequency biased. For adaptive control systems, this gives robustness advantages for systems with unmodelled high frequency characteristics [40]. 6. Pi operator. <p> that it is stably invertible, allowing the derivation of simpler algorithms <ref> [40] </ref>. The ae operator can be considered as a stable low pass filter, and parameter estimation using the ae operator is low frequency biased. For adaptive control systems, this gives robustness advantages for systems with unmodelled high frequency characteristics [40]. 6. Pi operator. By defining the bilinear transformation (BLT) as an operator, it is possible to introduce an operator which generalizes all of the above operators.
Reference: [41] <author> M. Palaniswami. </author> <title> Digital estimation and control with a new discrete time operator. </title> <booktitle> In Proc. 30th IEEE Conf. Decision and Control, </booktitle> <pages> pages 1631-1632, </pages> <address> New York, NY, 1991. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: The rho operator generalizes the delta and gamma operators. For the case where c 1 = c 2 = 1, the rho operator reduces to the usual shift operator . When c 1 = 0, and c 2 = 1, the rho operator reduces to the delta operator <ref> [41] </ref>. For c 1 = c 2 = c, the rho operator is equivalent to the gamma operator . This operator was introduced in the context of robust adaptive control [40]. It has been shown useful improvements over the performance of the delta operator [40, 41] can be obtained. <p> For c 1 = c 2 = c, the rho operator is equivalent to the gamma operator . This operator was introduced in the context of robust adaptive control [40]. It has been shown useful improvements over the performance of the delta operator <ref> [40, 41] </ref> can be obtained. One advantage of the rho operator over the delta operator is that it is stably invertible, allowing the derivation of simpler algorithms [40].
Reference: [42] <author> V. Peterka. </author> <title> Control of uncertain processes: Applied theory and algorithms. </title> <journal> Kybernetika, </journal> <volume> 22 </volume> <pages> 1-102, </pages> <year> 1986. </year>
Reference-contexts: Williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from the direct form structure [61], and a number of authors have considered using it in linear filtering, estimation and control <ref> [16, 33, 42] </ref>. 2. The modified delta operator. The delta operator can be modified to ensure that it is not marginally stable. The modified delta operator can be defined as ffi m = where 0 &lt; c &lt; 1. 3. Gamma operator.
Reference: [43] <author> P. Poddar and K. P. Unnikrishnan. </author> <title> Non-linear prediction of speech signals using memory neuron networks. </title> <editor> In B. H. Juang, S. Y. Kung, and C. A. Kamm, editors, </editor> <booktitle> Neural Networks for Signal Processing: Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 1-10. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively [3, 59]. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example <ref> [8, 12, 15, 18, 20, 25, 36, 37, 43] </ref>). In this paper we will consider feedforward neural network structures which have an FIR sub-structure, but the results are generalizable to these more complex network architectures. The implications of this work for recurrent neural networks will be considered in another paper.
Reference: [44] <author> J.C. Principe, B. de Vries, and P. Guedes de Oliveira. </author> <title> The Gamma filter anew class of adaptive IIR filters with restricted feedback. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> 41 </volume> <pages> 649-656, </pages> <year> 1993. </year> <month> 23 </month>
Reference-contexts: De Vries and Principe et. al., proposed stochastic gradient descent type algorithms for adjusting the c operator coefficient using a least-squares error criterion <ref> [11, 44] </ref>. For brevity, we omit the updating procedures for the MLP network weights; a variety of methods may be applied (see for example [46, 52]), but instead concentrate on the equations for updating the parameters in the ADTOs. <p> Using the well known matrix inversion lemma [30], we substitute P (t) = R 1 (t), where P (t) = (t) i (t) (t) + i (t) 0 (t)P (t) (t) (53) The initial values of the coefficients are important in determining convergence. Principe et. al. <ref> [44] </ref> have shown that the error surface may be multimodal. For the gamma operator, setting the coefficients to unity can provide the best approach for certain problems. <p> Hence we would expect that a multimodal error surface may result in terms of the operator parameters. Indeed, Principe et. al. have shown this to be the case for the gamma operator <ref> [44] </ref> and it is simple to show that this would also be the same for slightly more complex operators. What can be done about this problem ? There are a number of possible solutions which we propose here. <p> There is a problem with this approach however, in that while we have observed good results using the gradient descent algorithm to adjust the operator parameters, it is known that the mean square error surface of IIR filters may be multimodal <ref> [24, 44] </ref>. Hence there is no guarantee that a gradient descent algorithm will converge to the optimal solution. There are different ways to view this situation. One way is to treat it as a significant problem and seek to find ways to obtain the global minimum. <p> Hence it is evident that the Gamma model will have performance advantages in finite word length models in terms of modelling accuracy and sensitivity, in addition to the architectural advantages described by de Vries and Principe in <ref> [12, 45, 44] </ref>. In this paper, models based on the delta operator, rho operator, and pi operator have been presented which can provide modelling advantages over models based on the shift operator in terms of sensitivity and hence improved robustness properties for finite word length implementations.
Reference: [45] <author> J.C. Principe, B. de Vries, J.M. Kuo, and P. Guedes de Oliveira. </author> <title> Modeling applications with the focused gamma net. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 143-150, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An MLP () model which has multiple operators, each with different poles, was proposed in <ref> [45] </ref>. This was termed a "focussed network". <p> Experiment 2 The second experiment used a model described by H (z) = 1 2:8653q 1 + 2:7505q 2 0:8843q 3 (55) This system is a 3rd order lowpass filter tested in <ref> [45] </ref>. The same experimental procedures as used in Experiment 6.1 were followed in this case. For the second experiment (see Table 2), it was found that the pi operator gave the best results recorded over all the tests. On average however, the improvement for this identification problem is less. <p> Hence it is evident that the Gamma model will have performance advantages in finite word length models in terms of modelling accuracy and sensitivity, in addition to the architectural advantages described by de Vries and Principe in <ref> [12, 45, 44] </ref>. In this paper, models based on the delta operator, rho operator, and pi operator have been presented which can provide modelling advantages over models based on the shift operator in terms of sensitivity and hence improved robustness properties for finite word length implementations.
Reference: [46] <author> G.V. Puskorius and L.A. Feldkamp. </author> <title> Decoupled extended kalman filter training of feedforward layered networks. </title> <booktitle> In Proc. Int Joint Conf. Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 771-777, </pages> <address> Seattle, </address> <year> 1991. </year>
Reference-contexts: De Vries and Principe et. al., proposed stochastic gradient descent type algorithms for adjusting the c operator coefficient using a least-squares error criterion [11, 44]. For brevity, we omit the updating procedures for the MLP network weights; a variety of methods may be applied (see for example <ref> [46, 52] </ref>), but instead concentrate on the equations for updating the parameters in the ADTOs.
Reference: [47] <author> L.R. Rabiner and B. Gold. </author> <title> Theory and Application of Digital Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1975. </year>
Reference-contexts: In this section, we discuss some aspects of the convergence of the operator learning algorithms proposed in Section 7. 7.2.1 Modality of mean square output error surface The ADTOs discussed in this paper are all IIR filters <ref> [47] </ref>. It is known however, that for IIR filters, the mean square output error (MSOE) surface can be multimodal with respect to parameters, [24]. That is, there may exist more than one minimum on the surface.
Reference: [48] <author> P.A. Regalia. </author> <title> Adaptive IIR Filtering in Signal Processing and Control. </title> <address> Marcel-Dekker, New York, NY, </address> <year> 1995. </year>
Reference-contexts: It has been shown that the problem of multimodality in linear IIR models lies largely with the difference between the order of the system and the order of the model <ref> [17, 48] </ref>. In particular, reduced order models are known to contribute to the problem of multimodality [24]. For more complex models such as those we consider here, where there are a number of repeated IIR type operators, the situation is not so obvious.
Reference: [49] <author> W.J. Rugh. </author> <title> Nonlinear Theory: The Volterra-Wiener Approach. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1981. </year>
Reference-contexts: In fact, this perception is incorrect, and a substantial amount of work has been done in extending the concept of transfer functions and hence poles and zeros, to nonlinear systems (see for example <ref> [7, 49, 51] </ref>). <p> Following the usual derivation of a Volterra series <ref> [49] </ref>, the activation function f () can be approximated by a power series expansion [62] f (u) = j=0 where for a finite pth-order expansion around u = 0, 0 = 1=2, 1 = ff=4, 3 = ff 3 =48, xi 5 = ff 5 =480, xi 7 = 17 80640
Reference: [50] <author> E.B. Saff and A.D. Snider. </author> <title> Fundamentals of Complex Analysis for Mathematics, </title> <booktitle> Science and Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: We have proposed the pi operator [5] as = (c 3 q + c 4 ) with the restriction that c 1 c 4 6= c 2 c 3 (to ensure is not a constant function <ref> [50] </ref>). The bilinear mapping produced has a pole at q = c 4 =c 3 . By appropriate setting of the c 1 ; c 2 ; c 3 ; c 4 parameters, the pi operator can be reduced to each of the previous operators. <p> This region can be found by applying the bilinear transformation in the following stages <ref> [50] </ref>: 8 1. Linear transformation 1 = c 3 q + c 4 In this case, a unit disc in the qplane is transformed to a disk of radius c 3 , and centre (c 4 ; 0) in the 1 plane. 2.
Reference: [51] <author> M. Schetzen. </author> <title> The Volterra and Wiener Theory of Nonlinear Systems. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: In fact, this perception is incorrect, and a substantial amount of work has been done in extending the concept of transfer functions and hence poles and zeros, to nonlinear systems (see for example <ref> [7, 49, 51] </ref>). <p> i=0 i 12 i 0 =0 i 1 =0 p X fi (i 0 ; i 1 ; :::; i n )x i 0 (t)x i 1 (t 1):::x i n (t n) (32) Eqn (32) defines the usual Volterra series expansion and ffig are normally termed the Volterra kernels <ref> [51] </ref> (see also [62] for a discussion on some issues of extracting the Volterra kernels from MLPs). Note that for the activation function considered here, only odd fg terms are nonzero, however for other activation functions, and for the general MLP case, this will not necessarily be so.
Reference: [52] <author> S. Shah and F. Palmieri. </author> <title> Meka a fast local algorithm for training feedfoward neural networks. </title> <booktitle> In Proc. Int Joint Conf. Neural Networks, volume III, </booktitle> <pages> pages 41-46, </pages> <year> 1990. </year>
Reference-contexts: De Vries and Principe et. al., proposed stochastic gradient descent type algorithms for adjusting the c operator coefficient using a least-squares error criterion [11, 44]. For brevity, we omit the updating procedures for the MLP network weights; a variety of methods may be applied (see for example <ref> [46, 52] </ref>), but instead concentrate on the equations for updating the parameters in the ADTOs.
Reference: [53] <author> J.J. Shynk. </author> <title> Adaptive IIR filtering. </title> <journal> IEEE ASSP Magazine, </journal> <volume> April:4-21, </volume> <year> 1989. </year>
Reference-contexts: Since the main purpose in this paper is to raise the issue of ADTOs in the context of neural network signal processing, we do not attempt to consider these approaches in detail. Indeed, the problem is a significant open problem in linear systems theory <ref> [23, 53, 55] </ref> and we do not seek to solve the problem in this paper. Rather we present these ideas as possible avenues for future research and as a means of overcoming the difficulties encountered.
Reference: [54] <author> J.J. Shynk. </author> <title> Adaptive IIR filtering using parallel-form realizations. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37 </volume> <pages> 519-533, </pages> <year> 1989. </year>
Reference-contexts: replace (46) with (omitting i subscripts for clarity), ^ (t + 1) = ^ (t) + fl (t)R 1 (t) (t)fl 1 ffi (t) (50) where fl (t) is the gain sequence (see [30] for details), fl 1 is a weighting matrix which may be replaced by the identity matrix <ref> [54] </ref>, or estimated as [30] ^ fl (t) = ^ fl (t 1) + fl (t) ffi 2 (t) ^ fl (t 1) (51) R (t) is an approximate Hessian matrix, defined by R (t + 1) = (t)R (t) + i (t) (t) 0 (t) (52) where (t) = 1
Reference: [55] <author> T. Soderstrom and P. Stoica. </author> <title> System Identification. </title> <publisher> Prentice-Hall, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Efficient computation of R 1 may be performed using the matrix inversion lemma <ref> [55] </ref>, factorization methods such as Cholesky decomposition or other fast algorithms. <p> Since the main purpose in this paper is to raise the issue of ADTOs in the context of neural network signal processing, we do not attempt to consider these approaches in detail. Indeed, the problem is a significant open problem in linear systems theory <ref> [23, 53, 55] </ref> and we do not seek to solve the problem in this paper. Rather we present these ideas as possible avenues for future research and as a means of overcoming the difficulties encountered.
Reference: [56] <author> M. Stevenson, R. Winter, and B. Widrow. </author> <title> Sensitivity analysis of feedforward neural networks to weight errors. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 1 </volume> <pages> 71-90, </pages> <year> 1990. </year>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
Reference: [57] <author> A.C. Tsoi and A.D. </author> <title> Back. Locally recurrent globally feedforward networks, a critical review of architectures. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5 </volume> <pages> 229-239, </pages> <year> 1994. </year>
Reference-contexts: TDNN, FIR MLP). Thus, our results highlight the problem of high 2 This is in contrast with modification of the underlying structure of the MLP, e.g., Elman model [15]. For further discussion on the differences between these two types of models, see, for example, the survey paper <ref> [57] </ref>. 5 sensitivity in neural network modelling and indicate a solution which seems to be a promising area of further investigation. The organisation of this paper is as follows: in section 2, we will consider briefly, the various common ADTOs which have been introduced.
Reference: [58] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phonemic recognition using time delay neural networks. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37(3) </volume> <pages> 328-339, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Recent interest has concentrated in deriving various neural network architectures, often based on a modification of the classic multilayer perceptron (MLP) [22] for nonlinear functional mapping approximations, for modelling time-dependent signals. For example, a popular architecture, commonly known as the time delay neural network (TDNN) model <ref> [27, 58] </ref>, is based on the MLP, except that the input signal to each node (input or hidden) can include delayed versions of the same signal. It is known that this method works quite well in a number of applications, e.g., speech processing [58]. <p> It is known that this method works quite well in a number of applications, e.g., speech processing <ref> [58] </ref>. However, it is a common observation that this model may be computationally expensive.
Reference: [59] <author> E.A. Wan. </author> <title> Time series prediction by using a connectionist network with internal delay lines. In A.S. Weigend and N.A. Gershenfeld, editors, Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <booktitle> volume Proc. Volume XV Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <pages> pages 195-217, </pages> <address> Reading,MA, 1994. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For convenience, we will refer to these models as FIR MLP and IIR MLP respectively <ref> [3, 59] </ref>. Other dynamic neural network structures have been proposed in the literature which involve some form of time delayed inputs, outputs, or hidden states (see for example [8, 12, 15, 18, 20, 25, 36, 37, 43]).
Reference: [60] <author> N. Wiener. </author> <title> Nonlinear Problems in Random Theory. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1958. </year>
Reference-contexts: Note that for G (q) = B (q), (29) is equivalent to a TDNN. It is well known that such a model (29) can be approximated by a Volterra series <ref> [60] </ref>. It is desired to obtain a measure of the coefficient sensitivity for the model (29). In order to simplify the analysis presented here, we consider a simple TDNN model structure which has only one hidden unit and one layer.
Reference: [61] <author> D. Williamson. </author> <title> Delay replacement in direct form structures. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 34(4) </volume> <pages> 453-460, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from the direct form structure <ref> [61] </ref>, and a number of authors have considered using it in linear filtering, estimation and control [16, 33, 42]. 2. The modified delta operator. The delta operator can be modified to ensure that it is not marginally stable.
Reference: [62] <author> J. Wray and G.G.R. Green. </author> <title> Calculation of the Volterra kernels of non-linear dynamic sytems using an artificial neural network. </title> <journal> Biological Cybernetics, </journal> <volume> 71 </volume> <pages> 187-195, </pages> <year> 1994. </year>
Reference-contexts: Following the usual derivation of a Volterra series [49], the activation function f () can be approximated by a power series expansion <ref> [62] </ref> f (u) = j=0 where for a finite pth-order expansion around u = 0, 0 = 1=2, 1 = ff=4, 3 = ff 3 =48, xi 5 = ff 5 =480, xi 7 = 17 80640 ff 7 , ... <p> i 0 =0 i 1 =0 p X fi (i 0 ; i 1 ; :::; i n )x i 0 (t)x i 1 (t 1):::x i n (t n) (32) Eqn (32) defines the usual Volterra series expansion and ffig are normally termed the Volterra kernels [51] (see also <ref> [62] </ref> for a discussion on some issues of extracting the Volterra kernels from MLPs). Note that for the activation function considered here, only odd fg terms are nonzero, however for other activation functions, and for the general MLP case, this will not necessarily be so.
Reference: [63] <author> Y. Xie and M.A. Jabri. </author> <title> Analysis of the effects of quantization in multilayer neural networks using a statistical model. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3 </volume> <pages> 334-338, </pages> <year> 1992. </year> <month> 24 </month>
Reference-contexts: It is desirable however, that efforts be made to ensure stability of the individual operators. 6 Parameter Sensitivity Analysis Various authors have considered the issue of sensitivity to errors in the weights in feedforward neural networks <ref> [2, 9, 14, 26, 38, 39, 56, 63] </ref>. Typically, these analyses are based on probabilistic methods. Minai and Williams [34] considered the issue of performance changes of a network due to perturbations in the individual output response of units within the network.
References-found: 63

