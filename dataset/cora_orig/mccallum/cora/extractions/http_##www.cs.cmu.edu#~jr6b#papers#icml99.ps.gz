URL: http://www.cs.cmu.edu/~jr6b/papers/icml99.ps.gz
Refering-URL: http://www.cs.cmu.edu/~jr6b/
Root-URL: http://www.cs.cmu.edu/~jr6b
Title: Using Reinforcement Learning to Spider the Web Efficiently  
Author: Jason Rennie Andrew Kachites McCallum ; 
Keyword: reinforcement learning, text classification, World Wide Web, spidering, crawling, regression  
Note: Email address of contact author:  number of contact author: 412-683-9132 Electronic version: PostScript emailed to icml99@ijs.si  
Address: Pittsburgh, PA 15213  4616 Henry Street, Pittsburgh, PA 15213  
Affiliation: 1 School of Computer Science, Carnegie Mellon Univ.,  Just Research,  
Email: jrennie@justresearch.com  
Phone: 2  Phone  
Abstract: Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. This paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward by using naive Bayes text classifiers. Experiments on two real-world spidering tasks show a three-fold improvement in spider-ing efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference: [ Boyan et al., 1996 ] <author> Justin Boyan, Dayne Freitag, and Thorsten Joachims. </author> <title> A machine learning architecture for optimizing web search engines. </title> <booktitle> In AAAI workshop on Internet-Based Information Systems, </booktitle> <year> 1996. </year>
Reference-contexts: WebWatcher uses a combination of supervised and reinforcement learning to learn the value of each word on a hyperlink. Our work is not user-centric and strives to find a method for learning an optimal decision policy for locating relevant documents when hyperlink selection is unlimited. Laser <ref> [ Boyan et al., 1996 ] </ref> is a search engine that automatically optimizes a number of parameters to achieve improved retrieval performance. The CMU CS Web is used as the testbed and evaluation is based on the user's selection of links presented by Laser.
Reference: [ Cho et al., 1998 ] <author> Junhoo Cho, Hector Garcia-Molina, and Lawrence Page. </author> <title> Efficient crawling through URL ordering. In Computer Networks and ISDN Systems (WWW7), volume 30, </title> <year> 1998. </year>
Reference: [ Craven et al., 1998 ] <author> Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, and Sean Slattery. </author> <title> Learning to extract symbolic knowledge from the world wide web. </title> <booktitle> In Proceedings of 15th National Conference on Artificial Intelligence (AAAI-98), </booktitle> <year> 1998. </year>
Reference-contexts: Avoiding whole branches and neighborhoods of departmental web graphs can significantly improve efficiency and increase the number of research papers found given a finite amount of crawling time. Our work is also driven by the WebKB project <ref> [ Craven et al., 1998 ] </ref> . Here the focus is on automatically populating a knowledge base with information that is available on the World Wide Web.
Reference: [ Joachims et al., 1997 ] <author> T. Joachims, D. Freitag, and T. Mitchell. Webwatcher: </author> <title> A tour guide for the World Wide Web. </title> <booktitle> In Proceedings of IJCAI-97, </booktitle> <year> 1997. </year> <month> 15 </month>
Reference-contexts: Our research focuses on exactly that aspect: creating a framework to locate web documents that are related to a particular topic. Additionally, there are systems that use reinforcement learning for non-spidering Web tasks. WebWatcher <ref> [ Joachims et al., 1997 ] </ref> is a browsing assistant that helps a user find information by recommending which hyperlinks to choose. It thus restricts its action space to only hyperlinks from the current page.
Reference: [ Kaelbling et al., 1996 ] <author> Leslie Pack Kaelbling, Michael L. Littman, and An--drew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <pages> pages 237-285, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In our WebKB experiments, modeling future reward increases efficiency by a factor of two or more. 2 Reinforcement Learning The term "reinforcement learning" refers to a framework for learning optimal decision making from rewards or punishment <ref> [ Kaelbling et al., 1996 ] </ref> .
Reference: [ Lewis, 1998 ] <author> David D. Lewis. </author> <title> Naive (Bayes) at forty: The independence assumption in information retrieval. </title> <booktitle> In ECML-98, </booktitle> <year> 1998. </year>
Reference-contexts: j jd i ) 8 The class frequency parameters are set in the same way, where jCj indicates the number of classes: P (c j ) = P jCj + jDj Empirically, when given a large number of training documents, naive Bayes does a good job of classifying text documents <ref> [ Lewis, 1998 ] </ref> .
Reference: [ McCallum and Nigam, 1998 ] <author> Andrew McCallum and Kamal Nigam. </author> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization, </booktitle> <year> 1998. </year> <note> http://www.cs.cmu.edu/~mccallum. </note>
Reference: [ McCallum et al., 1998 ] <author> Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and Andrew Ng. </author> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In ICML-98, </booktitle> <pages> pages 359-367, </pages> <year> 1998. </year>
Reference-contexts: Experiments with a five-bin classifier result in worse performance|roughly equivalent to the RL-Immediate spider, following an average of 27% of available hyperlinks before locating the target page. Better features and other methods for improving classifier accuracy (such as shrinkage <ref> [ McCallum et al., 1998 ] </ref> ) should allow the more sensitive multi-bin 12 Q = 1 1 &lt; Q 0:5 0:5 &lt; Q 0:25 0:25 &lt; Q 0 immediate two-step three-step other board (url) aboutbb (url) applications (url) clp (url) information (url) bb (link) aerial (head) elk (url) directors (link)
Reference: [ McCallum et al., 1999 ] <author> Andrew McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. </author> <title> Building domain-specific search engines with machine learning techniques. </title> <booktitle> In AAAI-99 Spring Symposium on Intelligent Agents in Cyberspace, </booktitle> <year> 1999. </year>
Reference-contexts: Our research in efficient spidering is part of a larger project that has created Cora, a domain-specific search engine containing computer science research pa 2 pers <ref> [ McCallum et al., 1999 ] </ref> . We have spidered computer science departments and labs, so far finding over 50,000 research papers in postscript format.
Reference: [ Menczer, 1997 ] <author> Filippo Menczer. ARACHNID: </author> <title> Adaptive retrieval agents choosing heuristic neighborhoods for information discovery. </title> <booktitle> In ICML '97, </booktitle> <year> 1997. </year>
Reference-contexts: ARACHNID <ref> [ Menczer, 1997 ] </ref> is a system that uses a collection of agents for finding information on the Web. Each agent competes for limited computational resources, procreating and mutating proportionally to its success in finding relevant documents.
Reference: [ Mitchell, 1997 ] <author> Tom M. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1997. </year>
Reference: [ Torgo and Gama, 1997 ] <author> Luis Torgo and Joao Gama. </author> <title> Regression using classification algorithms. </title> <journal> Intelligent Data Analysis, </journal> <volume> 1(4), </volume> <year> 1997. </year> <month> 16 </month>
Reference-contexts: The mapping from the text to a scalar is performed by casting regression as classification <ref> [ Torgo and Gama, 1997 ] </ref> . <p> The mapping should be efficient, and should generalize to future, unseen hyperlinks. We represent the value function using a collection of naive Bayes text classifiers, performing the mapping by casting this regression problem as classification <ref> [ Torgo and Gama, 1997 ] </ref> .
References-found: 13

