URL: ftp://ftp.cs.cornell.edu/pub/yanhong/Cir-PEPM95.ps.Z
Refering-URL: http://www.cs.cornell.edu/faculty/home/tt/vita/vita.html
Root-URL: 
Email: Email: fyanhong, ttg@cs.cornell.edu  
Author: A. Liu Tim Teitelbaum 
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science, Cornell University,  
Note: Yanhong  
Abstract: Caching Intermediate Results for Program Improvement Abstract A systematic approach is given for symbolically caching intermediate results useful for deriving incremental programs from non-incremental programs. Our method can be applied straightforwardly to provide a systematic approach to program improvement via caching. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: The above drawback can be overcome by transforming subject programs to integrate caching into the transformed programs. In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming <ref> [1] </ref>, schemas of redundancies [10], and tupling [7, 8, 34, 35]. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers <ref> [2, 9, 11] </ref>, transformational programming [30, 33, 43], interactive editing systems [4, 39], etc. Deriving incremental programs.
Reference: [3] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: In essence, every program computes by fixed point iteration, which is why loop optimizations are so important. A straightforward idea then is to compute the result of each iteration incrementally using the stored result of the previous iteration, which is why strength reduction <ref> [3] </ref> and related techniques [32] are crucial for performance. Observe that, most of the time, not only the result, but also the intermediate results computed in one iteration can be useful for efficiently computing the result of the next iteration. <p> The contributions of this paper are as follows. 1. We give a systematic approach for caching intermediates results useful for computing f incrementally under , and for constructing a corresponding program that incrementally maintains these intermediate results. Previous work on this relies on a fixed set of rules <ref> [3, 32] </ref>, applies only to programs with certain properties or schemas [5, 10, 34, 35], or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. <p> A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 <ref> [3] </ref> and finite differencing technique [32] used in the APTS system [30, 31]. Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions.
Reference: [4] <author> R. A. Ballance, S. L. Graham, and M. L. Van De Van-ter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread application, e.g., optimizing compilers [2, 9, 11], transformational programming [30, 33, 43], interactive editing systems <ref> [4, 39] </ref>, etc. Deriving incremental programs. Given a program f and an input change , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under .
Reference: [5] <author> R. S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <month> De-cember </month> <year> 1980. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas <ref> [5, 10, 34, 35] </ref>, or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. <p> Bird <ref> [5] </ref> and Cohen [10] provide nice overviews. Most of the techniques fall into one of the following three classes.
Reference: [6] <author> R. Cartwright. </author> <title> Recursive programs as definitions in first order logic. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13(2) </volume> <pages> 374-408, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The transformation Ext is similar to the construction of call-by-value complete recursive programs by Cartwright <ref> [6] </ref>. However, a call-by-value computation sequence returned by such a program is a flat list of all intermediate results, while our extended function returns a computation tree, a structure that mirrors the hierarchy of function calls.
Reference: [7] <author> W.-N. Chin. </author> <title> Towards an automated tupling strategy. </title> <booktitle> In Proceedings of the ACM Symposium on PEPM, </booktitle> <address> Copen-hagen, Denmark, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming [1], schemas of redundancies [10], and tupling <ref> [7, 8, 34, 35] </ref>. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [8] <author> W.-N. Chin and S.-C. Khoo. </author> <title> Tupling functions with multiple recursion parameters. </title> <editor> In P. Cousot, M. Falaschi, G. File, and A. Rauzy, editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Static Analysis, </booktitle> <pages> pages 124-140. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1993. </year> <note> LNCS 724. </note>
Reference-contexts: In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming [1], schemas of redundancies [10], and tupling <ref> [7, 8, 34, 35] </ref>. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [9] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers <ref> [2, 9, 11] </ref>, transformational programming [30, 33, 43], interactive editing systems [4, 39], etc. Deriving incremental programs.
Reference: [10] <author> N. H. Cohen. </author> <title> Eliminating redundant recursive calls. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 265-299, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas <ref> [5, 10, 34, 35] </ref>, or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. <p> Bird [5] and Cohen <ref> [10] </ref> provide nice overviews. Most of the techniques fall into one of the following three classes. <p> In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming [1], schemas of redundancies <ref> [10] </ref>, and tupling [7, 8, 34, 35]. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [11] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers <ref> [2, 9, 11] </ref>, transformational programming [30, 33, 43], interactive editing systems [4, 39], etc. Deriving incremental programs.
Reference: [12] <author> C. A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: We first depict the transitive dependencies and address a cost issue. Then we give an algorithm that computes the needed components based on a de pendency analysis using domain projections <ref> [42, 12] </ref>. With this result, we prune the function f 0 to return only the inter mediate results that are useful for computing 1st ( f 0 0 (x; y; r)).
Reference: [13] <author> R. J. Hall. </author> <title> Program improvement by automatic redistribution of intermediate results: An overview. </title> <editor> In M. R. Lowry and R. D. McCartney, editors, </editor> <booktitle> Automating Software Design, chapter 14, </booktitle> <pages> pages 339-372. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <booktitle> Proceedings of the Workshop on Automating Software Design, AAAI '88. </booktitle>
Reference-contexts: Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions. Recently, certain principles that can directly guide program transformations have been proposed <ref> [13, 47] </ref>, but implementations based on these principles employ heavy inference engines that are compu-tationally exorbitant. Our approach to the problem of program improvement via caching is a principled approach that integrates caching in the transformed programs.
Reference: [14] <author> R. Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas [5, 10, 34, 35], or requires program annotations <ref> [14, 19, 44] </ref>. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. Stage II uses these intermediate results for the exclusive purpose of in-crementalization. <p> Work in recent years includes [15, 29, 37]. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs <ref> [14, 19, 44] </ref>. A drawback of these methods is that they are based on dynamic methods, which are fundamentally interpretive and are hard to be simultaneously both general and powerful. The above drawback can be overcome by transforming subject programs to integrate caching into the transformed programs.
Reference: [15] <author> J. Hughes. </author> <title> Lazy memo-functions. </title> <booktitle> In Proceedings of the 2nd Conference on FPCA, </booktitle> <pages> pages 129-146, </pages> <address> Nancy, France, </address> <month> September </month> <year> 1985. </year> <note> Springer-Verlag. LNCS 201. </note>
Reference-contexts: In the first class, a global cache separate from a subject program is employed to record values of subcomputations that may be needed later, and certain strategies are chosen for using and managing the cache [28]. Work in recent years includes <ref> [15, 29, 37] </ref>. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs [14, 19, 44].
Reference: [16] <author> F. Jalili and J. H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: We call the values of these computations intermediate results useful for computing f incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing <ref> [16] </ref> and incremental attribute evaluation [24, 40, 50]. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction.
Reference: [17] <author> S. B. Jones and D. Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 54-74, </pages> <address> London, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: The necessity interpretation by Jones and Le Metayer <ref> [17] </ref> is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections. While necessity patterns specify heads and tails of list values, our projections specify specific components of tuple values and thus provide more accurate information.
Reference: [18] <author> T. Katayama. </author> <title> Translation of attribute grammars into procedures. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 345-369, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction. An attribute evaluator may only return some designated synthesized attribute of the root <ref> [18] </ref>, but the corresponding incremental attribute evaluator may cache the whole attributed tree. Program improvement via caching. Deriving incremental programs and caching intermediate results provide a principled approach for program improvement using caching. In essence, every program computes by fixed point iteration, which is why loop optimizations are so important. <p> Attribute evaluation. Given an attribute grammar, a set of recursive functions can be constructed to evaluate the attribute values for any derivation tree of the grammar <ref> [18] </ref>. Basically, each function evaluates a synthesized attribute of a non-terminal, and the value of a synthesized attribute of the root symbol is the final return value of interest.
Reference: [19] <author> R. M. Keller and M. R. Sleep. </author> <title> Applicative caching. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 88-108, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas [5, 10, 34, 35], or requires program annotations <ref> [14, 19, 44] </ref>. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. Stage II uses these intermediate results for the exclusive purpose of in-crementalization. <p> Work in recent years includes [15, 29, 37]. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs <ref> [14, 19, 44] </ref>. A drawback of these methods is that they are based on dynamic methods, which are fundamentally interpretive and are hard to be simultaneously both general and powerful. The above drawback can be overcome by transforming subject programs to integrate caching into the transformed programs.
Reference: [20] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on PLDI, </booktitle> <pages> pages 147-158, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: (x y) = r 0 and ^ f 0 (x) = ^r, then 1st ( ^ f 0 0 (x; y; ^r) = ^ f 0 (x y); 0 (x; y; ^r)) t (f 0 (x y)): 2 Note that this is different from the partial dead code elimination in <ref> [20] </ref>, where partial dead code refers to code that is dead on some but not all computation paths. 3 If f oo (x) returns r, then f oo 0 (x; r) computes f oo (x + 1).
Reference: [21] <author> J. Launchbury. </author> <title> Projection factorisations in partial evaluation. </title> <type> Ph.d. thesis, </type> <institution> Department of Computing, University of Glasgow, </institution> <year> 1989. </year>
Reference-contexts: Other uses of projections include the strictness analysis by Wadler and Hughes [46], where necessary information needs to be specified and thus accounts for some complications, and the binding time analysis by Launchbury <ref> [21] </ref>, which is a forward analysis and is proved equivalent to strictness analysis [22]. The necessity interpretation by Jones and Le Metayer [17] is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections.
Reference: [22] <author> J. Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 80-91, </pages> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Stages I and III are simple, clean, and fully-automatable. 3. We develop in Stage III a backward dependency analysis that uses domain projections to specify sufficient information, which is a natural application of the techniques previously used for other analyses <ref> [22, 46] </ref>. Our projections specify specific components of compound values, rather than just heads or tails of list values, and thus provide more accurate information. The technique may also be used to assist general program optimizations in context, like tuple elimination [45]. 4. <p> Other uses of projections include the strictness analysis by Wadler and Hughes [46], where necessary information needs to be specified and thus accounts for some complications, and the binding time analysis by Launchbury [21], which is a forward analysis and is proved equivalent to strictness analysis <ref> [22] </ref>. The necessity interpretation by Jones and Le Metayer [17] is in the same spirit of our analysis, where their notion of necessity patterns correspond to our notion of projections.
Reference: [23] <author> J. L. Lawall and O. Danvy. </author> <title> Separating stages in the continuation-passing style transformation. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <pages> pages 124-136, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: However, a call-by-value computation sequence returned by such a program is a flat list of all intermediate results, while our extended function returns a computation tree, a structure that mirrors the hierarchy of function calls. The transformations in this stage also mimic the CPS transformations in some aspects <ref> [36, 23] </ref>: sequencing subexpressions, naming intermediate results, passing the collected information, 4 Ext [[v]] = &lt; v &gt; Ext [[g (e 1 ; :::; e n )]] where g is c or p = let v 1 = Ext [[e 1 ]] in ::: let v n = Ext [[e n
Reference: [24] <author> P. Lipps, U. Moncke, M. Olk, and R. Wilhelm. </author> <title> Attribute (re)evaluation in OPTRAN. </title> <journal> Acta Informatica, </journal> <volume> 26 </volume> <pages> 213-239, </pages> <year> 1988. </year>
Reference-contexts: We call the values of these computations intermediate results useful for computing f incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing [16] and incremental attribute evaluation <ref> [24, 40, 50] </ref>. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction.
Reference: [25] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <type> Technical Report TR 95-1498, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Sections 4, 5, and 6 describe caching, incrementalization, and pruning, respectively. Several examples are given in Section 7. Finally, we discuss related work and conclude in Section 8. For more details of some transformations, examples, and related work, see the extended version of this paper <ref> [25] </ref>. 2 Defining the problem We use a simple first-order functional programming language.
Reference: [26] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Incremental computation for transformational software development. </title> <type> Technical Report TR 95-1499, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Although we present the approach in a first-order functional language, the underlying principle is general and can be applied to other languages as well, e.g., higher-oder functional languages, functional languages with lazy semantics, and especially imperative languages with complex data structures and side effects. We have given an example <ref> [26] </ref> where the principle is applied to improve imperative programs with arrays for the local neighborhood problems in image processing [49, 51]. Further application of our principles to language with these features is a subject for future study.
Reference: [27] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <year> 1995. </year>
Reference-contexts: Deriving incremental programs. Given a program f and an input change , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under . Liu and Teitelbaum <ref> [27] </ref> give a systematic transformational approach for deriving an incremental program f 0 from a given program f and an input change . <p> Stage III gives us a kind of minimality by preserving only the intermediate results actually used by Stage II. Therefore, the whole method is optimal with respect to the incremental techniques of Stage II (for which we use <ref> [27] </ref>). Stages I and III are simple, clean, and fully-automatable. 3. We develop in Stage III a backward dependency analysis that uses domain projections to specify sufficient information, which is a natural application of the techniques previously used for other analyses [22, 46]. <p> There are standard constructions for mechanical time analysis [41, 48], but automatic space analysis and the trade-off between time and space are problems open for study. Given a program f 0 and an input change , we can use the approach in <ref> [27] </ref> to derive a program f 0 0 , an incremental version of f 0 under , such that, if f 0 (x) = r, then whenever f 0 (x y) returns a value, f 0 0 (x; y; r) returns the same value and is asymptotically at least as fast. <p> A number of examples like outer product and selection sort are given in <ref> [27] </ref>. For the function f oo in Figure 1 and input change x 1 y = x + 1, the function f oo 0 given in Figure 2 can be derived. <p> Note that some of the parameters of f 0 0 may be dead and eliminated <ref> [27] </ref>. 2 f oo (x) : sum three preceding "foo" numbers f oo (x) = if x 2 then 1 else boo (x) + f oo (x 3) boo (x) = f oo (x 1) + f oo (x 2) f ib (x) : compute the x-th Fibonacci number f ib <p> and the value of f 0 (x), such that 1st ( f 0 (x)) = f 0 (x) and t ( f 0 (x)) t (f 0 (x)): (3) Stage II derives a function f 0 0 , an incremental version of f 0 under , using the approach in <ref> [27] </ref>, such that if f 0 (x) = r, then we have if f 0 (x y) = r 0 , then f 0 (x; y; r) = r and t ( f 0 (x; y; r)) t ( f 0 (x y)) (4) and thus, together with (3), we have <p> Such a derivation method is given in <ref> [27] </ref>, and, depending on the power one expects from the derivation, the method can be made semiautomatic or fully-automatic. This stage is not the subject of this paper. We give only the result of incrementalization using [27], namely, a function f oo 0 , an incremental version of f oo in <p> Such a derivation method is given in <ref> [27] </ref>, and, depending on the power one expects from the derivation, the method can be made semiautomatic or fully-automatic. This stage is not the subject of this paper. We give only the result of incrementalization using [27], namely, a function f oo 0 , an incremental version of f oo in (13) under 1 , such that, if f oo (x) = r, then f oo 0 f oo (x; r) = if x 1 then &lt; 1; ; &gt; else if x = 2 then &lt;
Reference: [28] <author> D. Michie. </author> <title> "memo" functions and machine learning. </title> <journal> Nature, </journal> <volume> 218 </volume> <pages> 19-22, </pages> <month> April </month> <year> 1968. </year>
Reference-contexts: Most of the techniques fall into one of the following three classes. In the first class, a global cache separate from a subject program is employed to record values of subcomputations that may be needed later, and certain strategies are chosen for using and managing the cache <ref> [28] </ref>. Work in recent years includes [15, 29, 37]. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs [14, 19, 44].
Reference: [29] <author> D. J. Mostow and D. Cohen. </author> <title> Automating program speedup by deciding what to cache. </title> <booktitle> In Proceedings of the Ninth IJCAI, </booktitle> <pages> pages 165-172, </pages> <address> Los Angeles, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: In the first class, a global cache separate from a subject program is employed to record values of subcomputations that may be needed later, and certain strategies are chosen for using and managing the cache [28]. Work in recent years includes <ref> [15, 29, 37] </ref>. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs [14, 19, 44].
Reference: [30] <author> R. Paige. </author> <title> Transformational programming applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers [2, 9, 11], transformational programming <ref> [30, 33, 43] </ref>, interactive editing systems [4, 39], etc. Deriving incremental programs. <p> The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 [3] and finite differencing technique [32] used in the APTS system <ref> [30, 31] </ref>. Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions.
Reference: [31] <author> R. Paige. </author> <title> Symbolic finite differencing part I. </title> <booktitle> In Proceedings of the 3rd ESOP, </booktitle> <pages> pages 36-56, </pages> <address> Copenhagen, Denmark, </address> <month> May </month> <year> 1990. </year> <note> Springer-Verlag. LNCS 432. </note>
Reference-contexts: The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 [3] and finite differencing technique [32] used in the APTS system <ref> [30, 31] </ref>. Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions.
Reference: [32] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: In essence, every program computes by fixed point iteration, which is why loop optimizations are so important. A straightforward idea then is to compute the result of each iteration incrementally using the stored result of the previous iteration, which is why strength reduction [3] and related techniques <ref> [32] </ref> are crucial for performance. Observe that, most of the time, not only the result, but also the intermediate results computed in one iteration can be useful for efficiently computing the result of the next iteration. Thus, these intermediate results need to be identified, used, and maintained as well. <p> The contributions of this paper are as follows. 1. We give a systematic approach for caching intermediates results useful for computing f incrementally under , and for constructing a corresponding program that incrementally maintains these intermediate results. Previous work on this relies on a fixed set of rules <ref> [3, 32] </ref>, applies only to programs with certain properties or schemas [5, 10, 34, 35], or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. <p> A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 [3] and finite differencing technique <ref> [32] </ref> used in the APTS system [30, 31]. Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions.
Reference: [33] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers [2, 9, 11], transformational programming <ref> [30, 33, 43] </ref>, interactive editing systems [4, 39], etc. Deriving incremental programs. <p> Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 [3] and finite differencing technique [32] used in the APTS system [30, 31]. Seeking more flexibility and broader applicability, KIDS [43] and CIP <ref> [33] </ref> propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions. Recently, certain principles that can directly guide program transformations have been proposed [13, 47], but implementations based on these principles employ heavy inference engines that are compu-tationally exorbitant.
Reference: [34] <author> A. Pettorossi. </author> <title> A powerful strategy for deriving efficient programs by transformation. </title> <booktitle> In Proceedings of the ACM '84 Symposium on LFP, </booktitle> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas <ref> [5, 10, 34, 35] </ref>, or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. <p> In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming [1], schemas of redundancies [10], and tupling <ref> [7, 8, 34, 35] </ref>. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [35] <author> A. Pettorossi. </author> <title> Strategical derivation of on-line programs. </title> <editor> In L. G. L. T.Meertens, editor, </editor> <booktitle> Program Specification and Transformation, </booktitle> <pages> pages 73-88, </pages> <address> Amsterdam, </address> <year> 1987. </year> <title> The Netherlands: </title> <booktitle> North-Holland. Proceedings of the IFIP TC2/WG 2.1 Working Conference on Program Specification and Transformation, </booktitle> <month> April </month> <year> 1986. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas <ref> [5, 10, 34, 35] </ref>, or requires program annotations [14, 19, 44]. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. <p> In particular, some techniques apply transformations based on special properties and schemas of subject programs, and they form the second class. Typical examples of these techniques are dynamic programming [1], schemas of redundancies [10], and tupling <ref> [7, 8, 34, 35] </ref>. A drawback of these techniques is their lack of generality. The third class analyzes and transforms programs under general principles. Often, a set of rules are derived from such principles and are used to transform programs.
Reference: [36] <author> G. D. Plotkin. </author> <title> Call-by-name, call-by-value and the -calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1 </volume> <pages> 125-159, </pages> <year> 1975. </year>
Reference-contexts: However, a call-by-value computation sequence returned by such a program is a flat list of all intermediate results, while our extended function returns a computation tree, a structure that mirrors the hierarchy of function calls. The transformations in this stage also mimic the CPS transformations in some aspects <ref> [36, 23] </ref>: sequencing subexpressions, naming intermediate results, passing the collected information, 4 Ext [[v]] = &lt; v &gt; Ext [[g (e 1 ; :::; e n )]] where g is c or p = let v 1 = Ext [[e 1 ]] in ::: let v n = Ext [[e n
Reference: [37] <author> W. Pugh. </author> <title> An improved cache replacement strategy for function caching. </title> <booktitle> In Proceedings of the ACM '88 Conference on LFP, </booktitle> <pages> pages 269-276, </pages> <year> 1988. </year>
Reference-contexts: In the first class, a global cache separate from a subject program is employed to record values of subcomputations that may be needed later, and certain strategies are chosen for using and managing the cache [28]. Work in recent years includes <ref> [15, 29, 37] </ref>. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs [14, 19, 44].
Reference: [38] <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Work in recent years includes [15, 29, 37]. Two trends seem obvious: studying specialized cache strategies for classes of problems <ref> [38] </ref>, and adding annotations or certain specifications to subject programs [14, 19, 44]. A drawback of these methods is that they are based on dynamic methods, which are fundamentally interpretive and are hard to be simultaneously both general and powerful.
Reference: [39] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Methods of incremental computation have widespread application, e.g., optimizing compilers [2, 9, 11], transformational programming [30, 33, 43], interactive editing systems <ref> [4, 39] </ref>, etc. Deriving incremental programs. Given a program f and an input change , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under .
Reference: [40] <author> T. Reps, T. Teitelbaum, and A. Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: We call the values of these computations intermediate results useful for computing f incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing [16] and incremental attribute evaluation <ref> [24, 40, 50] </ref>. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction. <p> derived incremental program computes in O (jPATHj+jAFFECTEDj) time, where PATH is the path from the root of the whole tree to the root of the new subtree, and AFFECTED is the set of attributes whose values are different in the new tree than in the old after the subtree replacement <ref> [40] </ref>. 8 Related work and conclusion Caching has been the basis of many techniques for developing efficient programs and optimizing programs. Bird [5] and Cohen [10] provide nice overviews. Most of the techniques fall into one of the following three classes.
Reference: [41] <author> M. Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 144-156, </pages> <address> London, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: We assume that we have unlimited space to be used for achieving the least asymptotic time possible. The pruning saves time as well as space for computing and maintaining intermediate results that are not useful for incremental computation. There are standard constructions for mechanical time analysis <ref> [41, 48] </ref>, but automatic space analysis and the trade-off between time and space are problems open for study.
Reference: [42] <author> D. S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In M. Broy and G. Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year> <booktitle> Lecture Notes of 1981 Marktoberdorf Summer School on Theoretical Foundations of Programming Methodology, directed by F.L. Bauer, E.W. Dijkstra, and C.A.R. Hoare. </booktitle>
Reference-contexts: We first depict the transitive dependencies and address a cost issue. Then we give an algorithm that computes the needed components based on a de pendency analysis using domain projections <ref> [42, 12] </ref>. With this result, we prune the function f 0 to return only the inter mediate results that are useful for computing 1st ( f 0 0 (x; y; r)).
Reference: [43] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Incremental programs take advantage of repeated computations on inputs that differ only slightly from one another, making use of the old output in computing a new output rather than computing from scratch. Methods of incremental computation have widespread application, e.g., optimizing compilers [2, 9, 11], transformational programming <ref> [30, 33, 43] </ref>, interactive editing systems [4, 39], etc. Deriving incremental programs. <p> Often, a set of rules are derived from such principles and are used to transform programs. Examples are the conventional strength reduction technique 10 [3] and finite differencing technique [32] used in the APTS system [30, 31]. Seeking more flexibility and broader applicability, KIDS <ref> [43] </ref> and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions. Recently, certain principles that can directly guide program transformations have been proposed [13, 47], but implementations based on these principles employ heavy inference engines that are compu-tationally exorbitant.
Reference: [44] <author> R. S. Sundaresh and P. Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Previous work on this relies on a fixed set of rules [3, 32], applies only to programs with certain properties or schemas [5, 10, 34, 35], or requires program annotations <ref> [14, 19, 44] </ref>. 2. Our cache-and-prune method consists of three independent stages, and thus is modular. It has certain nice properties. Stage I gives us maximality by providing all the intermediate results possibly used by Stage II. Stage II uses these intermediate results for the exclusive purpose of in-crementalization. <p> Work in recent years includes [15, 29, 37]. Two trends seem obvious: studying specialized cache strategies for classes of problems [38], and adding annotations or certain specifications to subject programs <ref> [14, 19, 44] </ref>. A drawback of these methods is that they are based on dynamic methods, which are fundamentally interpretive and are hard to be simultaneously both general and powerful. The above drawback can be overcome by transforming subject programs to integrate caching into the transformed programs.
Reference: [45] <author> K. R. Traub. </author> <title> A compiler for the MIT tagged-token dataflow architecture. </title> <type> Master's Thesis LCS TR-370, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Our projections specify specific components of compound values, rather than just heads or tails of list values, and thus provide more accurate information. The technique may also be used to assist general program optimizations in context, like tuple elimination <ref> [45] </ref>. 4. Our result can be applied straightforwardly and systematically to program improvement via caching. The classical example of the Fibonacci function, which can be improved dramatically by various caching techniques, is shown in Section 7.
Reference: [46] <author> P. Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on FPCA, </booktitle> <pages> pages 385-407, </pages> <address> Portland, Ore-gon, </address> <month> September </month> <year> 1987. </year> <note> LNCS 274. </note>
Reference-contexts: Stages I and III are simple, clean, and fully-automatable. 3. We develop in Stage III a backward dependency analysis that uses domain projections to specify sufficient information, which is a natural application of the techniques previously used for other analyses <ref> [22, 46] </ref>. Our projections specify specific components of compound values, rather than just heads or tails of list values, and thus provide more accurate information. The technique may also be used to assist general program optimizations in context, like tuple elimination [45]. 4. <p> For any projection , BOT v . Dependency analysis. To compute which components of r are needed for computing certain components of f 0 0 (x; y; r), we apply a backward dependency analysis to the program for f 0 0 . Following the style of <ref> [46] </ref>, for each function f of n parameters, and each i from 1 to n, we define f i to be a dependency transformer that takes a projection that is applied to the result of f and returns a projection that is sufficient to be applied to the ith parameter. <p> This always terminates since the ascending chains are finite. Our backward dependency analysis uses domain projections to specify sufficient information, which is natural and thus simple. Other uses of projections include the strictness analysis by Wadler and Hughes <ref> [46] </ref>, where necessary information needs to be specified and thus accounts for some complications, and the binding time analysis by Launchbury [21], which is a forward analysis and is proved equivalent to strictness analysis [22].
Reference: [47] <author> A. B. Webber. </author> <title> A formal definition of unnecessary computation in functional programs. </title> <type> Technical Report TR 92-1260, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Seeking more flexibility and broader applicability, KIDS [43] and CIP [33] propose certain high-level strategies, but leave the choice of which intermediate results to maintain to manual decisions. Recently, certain principles that can directly guide program transformations have been proposed <ref> [13, 47] </ref>, but implementations based on these principles employ heavy inference engines that are compu-tationally exorbitant. Our approach to the problem of program improvement via caching is a principled approach that integrates caching in the transformed programs.
Reference: [48] <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 528-538, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: We assume that we have unlimited space to be used for achieving the least asymptotic time possible. The pruning saves time as well as space for computing and maintaining intermediate results that are not useful for incremental computation. There are standard constructions for mechanical time analysis <ref> [41, 48] </ref>, but automatic space analysis and the trade-off between time and space are problems open for study.
Reference: [49] <author> W. M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: We have given an example [26] where the principle is applied to improve imperative programs with arrays for the local neighborhood problems in image processing <ref> [49, 51] </ref>. Further application of our principles to language with these features is a subject for future study.
Reference: [50] <author> D. Yeh and U. Kastens. </author> <title> Improvements on an incremental evaluation algorithm for ordered attribute grammars. </title> <journal> SIGPLAN Notices, </journal> <volume> 23(12) </volume> <pages> 45-50, </pages> <year> 1988. </year>
Reference-contexts: We call the values of these computations intermediate results useful for computing f incrementally under . Examples where intermediate results are needed for incremental computation include incremental parsing [16] and incremental attribute evaluation <ref> [24, 40, 50] </ref>. An incremental parser may cache, in addition to the derived parse tree, the LR (0) state corresponding to each shift and reduction.
Reference: [51] <author> R. Zabih. </author> <title> Individuating unknown objects by combining motion and stereo. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, Cal-ifornia, </institution> <year> 1994. </year> <month> 12 </month>
Reference-contexts: We have given an example [26] where the principle is applied to improve imperative programs with arrays for the local neighborhood problems in image processing <ref> [49, 51] </ref>. Further application of our principles to language with these features is a subject for future study.
References-found: 51

