URL: http://www.cs.virginia.edu/~gsw2c/research/thesis.ps.Z
Refering-URL: http://www.cs.virginia.edu/~gsw2c/research/research.html
Root-URL: http://www.cs.virginia.edu
Title: A Thesis  Using Acoustic Information to Control Visual Attention  
Author: Glenn Scott Wasson 
Degree: Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree Master of Science (Computer Science)  
Date: May 1995  
Note: by  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Agre, P.E. and Chapman, D., Pengi: </author> <title> An Implementation of a Theory of Activ ity, </title> <booktitle> Proceedings of AAAI-87, </booktitle> <pages> 17-34. </pages>
Reference: [2] <author> Batteau, D. W., </author> <title> The Role of the Pinna in Human Localization, </title> <journal> Proceedings of the Royal Society of London, Series B 168, </journal> <year> 1967, </year> <pages> 158-180. </pages>
Reference-contexts: While evidence suggests that delay line computation occurs in humans, we do not possess the skeletal ear asymmetry that owls use to measure elevation. Instead, Batteau <ref> [2] </ref> theorized that the rigid cartilage of the outer ear, called the pinna, produces IAD information which can be used to locate a sound monaurally. Middlebrooks and Green [16] summarize the process as follows.
Reference: [3] <author> Blauert, J. and Cobben, W., </author> <title> Some Consideration of Binaural Cross Correlation Analysis, </title> <journal> Acustica, </journal> <volume> Vol. 39, </volume> <year> 1978. </year>
Reference-contexts: Another of their weighting functions, the SCOT or Smoothed Coherence Transform, is discussed in the context of spectral whitening in section 4.1. There are also various computer models of sound localization involving cross-correlation, such as those proposed by Blauert and Cobben <ref> [3] </ref> and by MacPherson [15]. Both of these models filter the incoming signals and then perform a running cross-correlation. Blauert and Cobben record the signals produced by real-world stimuli, deep within a subjects ear to insure that pinna filtering has taken 6 place.
Reference: [4] <author> Brill, F.Z., </author> <title> Perception and action in a dynamic, three-dimensional world, </title> <booktitle> Pro ceedings of the IEEE Workshop on Visual Behaviors, </booktitle> <month> June 19, </month> <year> 1994, </year> <pages> 60-67. </pages>
Reference: [5] <author> Burt, P.J., </author> <title> Smart Sensing within Pyramid Vision, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 76, No. 8, </volume> <month> August </month> <year> 1988, </year> <pages> 1006-1014. </pages>
Reference: [6] <author> Carlile, S. and Pralong, D., </author> <title> The Location Dependant Nature of Perceptually Salient Features of the Human Head Related Transfer Functions, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 95, No. 6, </volume> <month> June </month> <year> 1994, </year> <pages> 3445-3459. </pages>
Reference-contexts: Measurements taken at a number of sound source locations (343 locations were used in by Carlile and Pralong <ref> [6] </ref>) can create a three-dimensional map of ear drum pressure versus sound source location. <p> The use of the HRTF for sound localization is discussed by Carlile and Pral-ong <ref> [6] </ref>. It is worth noting that, like the visual reconstruction problem, the multi-source sound localization problem is ill-posed; the mapping from physical sound sources to microphone traces is generally many-to-one.
Reference: [7] <author> Carr, C.E., </author> <title> Delay Line Models of Sound Localization in the Barn Owl, </title> <journal> Amer ican Zoologist, </journal> <volume> Vol. 33, No. 1, </volume> <year> 1993, </year> <pages> 79-85. </pages>
Reference-contexts: In 1948, Jeffress proposed a theory of delay lines and coincidence detectors to explain ITD processing [9]. The auditory hair cells of the cochlea encode and transmit stimulus phase information along many nerve fibers which vary in how quickly they deliver signals to way stations in the brain <ref> [7] </ref>. Each pair of fibres from the left and right ears connect at a cell known as a coincidence detector. These cells only fire when the two signals arrive at the same time. Various coincidence detectors are connected to left-right delay nerve pairs corresponding to various delays.
Reference: [8] <author> Carter, </author> <title> G.C., Time Delay Estimation for Passive Sonar Signal Processing, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> Vol. ASSP-29, No. 3, </volume> <month> June </month> <year> 1981, </year> <pages> 463-470. </pages>
Reference-contexts: Several methods of estimating delay have been proposed, each involving some form of cross-correlation analysis. Various mathematical models of received signals and pre-correlation filters, as well as, limits on estimation algorithms are discussed by Piersol [24] and Carter <ref> [8] </ref>. Knapp and Carter [10] propose several weighted cross-correlation functions to estimate delay, one of which (the Phase Transform or PHAT), is essentially identical to the algorithm used in this paper, as discussed in section 3.2.
Reference: [9] <author> Jeffress, L.A., </author> <title> A Place Theory of Sound Localization, </title> <journal> Journal of Compara tive Physiology Psychology, </journal> <volume> Vol. 41, </volume> <year> 1948, </year> <pages> 35-39. </pages>
Reference-contexts: The interaural time difference allows the owl to determine left/right placement of the source. In 1948, Jeffress proposed a theory of delay lines and coincidence detectors to explain ITD processing <ref> [9] </ref>. The auditory hair cells of the cochlea encode and transmit stimulus phase information along many nerve fibers which vary in how quickly they deliver signals to way stations in the brain [7].
Reference: [10] <author> Knapp, C.H. and Carter, </author> <title> G.C., The Generalized Correlation Method for Esti mation of Time Delay, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> Vol. ASSP-24, No. 4, </volume> <month> August </month> <year> 1976, </year> <pages> 320-327. </pages>
Reference-contexts: Several methods of estimating delay have been proposed, each involving some form of cross-correlation analysis. Various mathematical models of received signals and pre-correlation filters, as well as, limits on estimation algorithms are discussed by Piersol [24] and Carter [8]. Knapp and Carter <ref> [10] </ref> propose several weighted cross-correlation functions to estimate delay, one of which (the Phase Transform or PHAT), is essentially identical to the algorithm used in this paper, as discussed in section 3.2. <p> White noise makes signals less likely to match at multiple offsets and increases the height of the match peak at the correct offset. The PHAT transform <ref> [10] </ref> is described as a weighted cross-correlation where the weighting is microphone sound source robot head y f ( ) 19 where is the Fourier transform of the correlation of the left and right sig nals. So, the PHAT divides the cross-correlation vector by its magnitude. <p> So, the PHAT divides the cross-correlation vector by its magnitude. This provides for a significant amount of whitening, producing sharp match peaks, such as in Figure 17a. The difficulty with the PHAT (as described by Knapp and Carter in <ref> [10] </ref>) is that it requires a high input signal to noise ratio so that the actual signal can still be distinguished from the added white noise. <p> The disadvantage of this approach is that the peaks are not as sharp as those produced by the basic PHAT (see Figures 16a and 16b). Knapp and Carter <ref> [10] </ref> state that the PHAT transform was developed as an ad-hoc technique to overcome the peak spreading behavior of another of their transforms, the Smoothed Coherence Transform or SCOT.
Reference: [11] <author> Knuth, D.E., Marriages Stables, Les Presses de lUniversit de Montral, </author> <year> 1976. </year>
Reference-contexts: in a single frame time, so the assumption that the object is still at its stored location, even though it did not appear that in the last image, seems valid. 5.4.1 Stable Marriage Algorithm The stable-marriage algorithm is a matching algorithm for determining pairs of elements from two distinct sets <ref> [11] </ref>. Each element of both sets has an ordered list of its preferences for members of the opposite set. The algorithm iterates through one of the sets attempting to create pairs of elements, one from each set, with a mutual preference for each other.
Reference: [12] <author> Konishi, M., </author> <title> Listening with Two Ears, </title> <publisher> Scientific American, </publisher> <month> April </month> <year> 1993, </year> <pages> 66 73. </pages>
Reference-contexts: The best understood auditory system in the animal king 3 dom belongs to the barn owl (Tyto Alba). Konishi <ref> [12] </ref> suggests that, in the owl, the localization computation is divided into two parallel paths, one for left/right information and one for elevation. <p> The ITD and the IAD are combined only late in the owls perceptual process to achieve a unique location for the source. Evidence for the owls parallel processing exists at the lowest levels of the brain, where magnocellular neurons carry timing information and angular neurons carry intensity data <ref> [12] </ref>. The interaural time difference allows the owl to determine left/right placement of the source. In 1948, Jeffress proposed a theory of delay lines and coincidence detectors to explain ITD processing [9].
Reference: [13] <author> Kuglin, </author> <title> C.D. and Hines, D.C., The Phase Correlation Image Alignment Method, </title> <booktitle> Proceedings of the IEEE Conference on Cybernetics and Society, </booktitle> <month> Sep 52 tember </month> <year> 1975, </year> <pages> 163-165. </pages>
Reference: [14] <author> Kuperman, W.A., Werby, M.F., Gilbert, K.E., and Tango, G.J., </author> <title> Beam Forming on Bottom-Interacting Tow-Ship Noise, </title> <journal> IEEE Journal of Oceanic Engineering, </journal> <month> July </month> <year> 1985, </year> <pages> 290-298. </pages>
Reference-contexts: The reflections received off the ocean floor have characteristics which give clues as to the sea-bottoms properties. Techniques even exist to use the noise emitted by the tow ship as a signal source <ref> [14] </ref> and to locate multiple interfering signal sources, e.g. seismic activity or whale moans [17][26]. Typically, long arrays (64 were used in the work of Kuperman [14]) of omni-directional microphones are used to simulate a group of directional microphones. <p> Techniques even exist to use the noise emitted by the tow ship as a signal source <ref> [14] </ref> and to locate multiple interfering signal sources, e.g. seismic activity or whale moans [17][26]. Typically, long arrays (64 were used in the work of Kuperman [14]) of omni-directional microphones are used to simulate a group of directional microphones. The advantage of large arrays is that many microphones provide a tight beam which can be effectively steered with less beam width distortion.
Reference: [15] <author> MacPherson, </author> <title> E.A., A Computer Model of Binaural Localization for Stereo Imaging Measurement, </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> Vol. 39, No. 9, </volume> <month> September </month> <year> 1991, </year> <pages> 604-622. </pages>
Reference-contexts: Another of their weighting functions, the SCOT or Smoothed Coherence Transform, is discussed in the context of spectral whitening in section 4.1. There are also various computer models of sound localization involving cross-correlation, such as those proposed by Blauert and Cobben [3] and by MacPherson <ref> [15] </ref>. Both of these models filter the incoming signals and then perform a running cross-correlation. Blauert and Cobben record the signals produced by real-world stimuli, deep within a subjects ear to insure that pinna filtering has taken 6 place.
Reference: [16] <author> Middlebrooks, J.C., and Green, D. M., </author> <title> Sound Localization by Human Listen ers, </title> <journal> Annual Review of Psychology, </journal> <volume> No. 42, </volume> <year> 1991, </year> <pages> 135-159. </pages>
Reference-contexts: Instead, Batteau [2] theorized that the rigid cartilage of the outer ear, called the pinna, produces IAD information which can be used to locate a sound monaurally. Middlebrooks and Green <ref> [16] </ref> summarize the process as follows. The shape of the pinna causes sound waves emanating from a source to reach the ear canal via multiple paths.
Reference: [17] <author> Mirkin, </author> <title> A.N. and Sibul, L.H., Maximum Likelihood Estimation of the Loca tions of Multiple Sources in an Acoustic Waveguide, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 95, No. 2, </volume> <month> February </month> <year> 1994, </year> <pages> 877-888. </pages>
Reference: [18] <author> Moore, D., </author> <title> Sound Localization Mechanisms, </title> <journal> Nature, </journal> <volume> Vol. 337, </volume> <month> January 19, </month> <year> 1989, </year> <pages> 208-209. </pages>
Reference: [19] <author> Munier, J. and Delisle, G.Y., </author> <title> Spatial Analysis in Passive Listening Using Adaptive Techniques, </title> <booktitle> Proc. IEEE v. </booktitle> <volume> 75 no. 11, </volume> <year> 1987, </year> <pages> pp. 1458-1471. </pages>
Reference-contexts: These methods typically make use of linear or planar arrays of microphones, and use the techniques of phased array beamforming to localize sound sources with great precision <ref> [19] </ref>. The concept of beamforming arises in the study of radar. The most commonly known type of radar system is based on a reflector or lens.
Reference: [20] <author> Niyogi, S.A., and Adelson, </author> <title> E.H., Analyzing and Recognizing Walking Fig ures in XYT, </title> <publisher> IEEE, </publisher> <year> 1994, </year> <pages> 469-474. </pages>
Reference-contexts: of the audio sub-system is to read location information T heard T seen 34 from the Macintosh and keep track of the most recent angle from which sound emanated. 5.3 Vision Sub-system The vision sub-systems goal is to identify people, but recognition of humans from images is quite difficult (see <ref> [20] </ref> and [22] for different approaches). In our laboratory environment an acceptable substitute is to detect appropriately sized regions of visual change. On start-up, the camera is panned to several discrete viewing positions in the empty lab. A reference image is captured at each position.
Reference: [21] <author> Olson, T.J., Lockwood R.J., and Taylor J.R., </author> <title> Programming a Pipelined Image Processor, </title> <booktitle> Proceedings of the IEEE Workshop on Computer Architectures for Machine Perception (CAMP93), </booktitle> <address> New Orleans, </address> <year> 1993, </year> <pages> 93-100. </pages>
Reference-contexts: The last section gives an example trace of the system in operation. The visual processing, marker maintenance and control computations are performed on a Sun Sparcstation 2, with assistance from a DataCube MV200 for the vision sub-system. The MV200 is programmed using the VEIL system <ref> [21] </ref>. The sound localization sub-system runs on a Macintosh as described above, and relays its results to the control process via Ethernet. A ZebraKinesis Pan/Tilt/Verge unit controls the direction of the camera.
Reference: [22] <author> ORourke, J. and Badler, N.I., </author> <title> Model-based Image Analysis of Human Motion using Constraint Propagation, </title> <journal> IEEE Transactions PAMI 2, </journal> <note> 4, Novem ber 1980. </note>
Reference-contexts: audio sub-system is to read location information T heard T seen 34 from the Macintosh and keep track of the most recent angle from which sound emanated. 5.3 Vision Sub-system The vision sub-systems goal is to identify people, but recognition of humans from images is quite difficult (see [20] and <ref> [22] </ref> for different approaches). In our laboratory environment an acceptable substitute is to detect appropriately sized regions of visual change. On start-up, the camera is panned to several discrete viewing positions in the empty lab. A reference image is captured at each position.
Reference: [23] <author> Perrott, D.R., Costantino, B., and Cisneros, J., </author> <title> Auditory and Visual Localiza tion Performance in a Sequential Discrimination Task, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 93, </volume> <month> April </month> <year> 1993, </year> <pages> 2134-2138. </pages>
Reference-contexts: Perrott, Costantino, and Cis-neros <ref> [23] </ref> found that the time required to locate and identify a target with spatially correlated audio stimulus decreased by hundreds of milliseconds, provided the target was initially in the central field of view.
Reference: [24] <author> Piersol, A., </author> <title> Time Delay Estimation Using Phase Data, </title> <journal> IEEE Transactions on Acoustic, Speech and Signal Processing, </journal> <volume> Vol. ASSP-29, No. 3, </volume> <month> June </month> <year> 1981, </year> <pages> 471 477. </pages>
Reference-contexts: Several methods of estimating delay have been proposed, each involving some form of cross-correlation analysis. Various mathematical models of received signals and pre-correlation filters, as well as, limits on estimation algorithms are discussed by Piersol <ref> [24] </ref> and Carter [8]. Knapp and Carter [10] propose several weighted cross-correlation functions to estimate delay, one of which (the Phase Transform or PHAT), is essentially identical to the algorithm used in this paper, as discussed in section 3.2. <p> The calculation of azimuth from ITD is discussed briefly below. For a more thorough discussion see <ref> [24] </ref>, for example. 3.1 Calculating Azimuth from ITD The relationship between ITD and source location is easily determined from the geometry of the situation (see Figure 3). Points L and R represent the left and right microphones respectively, while point P represents the sound source.
Reference: [25] <author> Sedgewick, R., </author> <title> Algorithms in C, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <month> 53 </month>
Reference-contexts: The algorithm is where N is the number of elements in set A. For a more complete discussion of this algorithm and its correctness, see <ref> [25] </ref>. 5.5 Example The following is an actual example of the system running on real world input. Two people enter the systems environment, a room in this case, and are tracked as they move about and make noise. A diagram depicting the systems operation appears in Figure 33.
Reference: [26] <author> Seem, D.A., and Rowe, </author> <title> N.C., Shape Correlation of Low-frequency Underwa ter Sounds, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 95, </volume> <month> April </month> <year> 1994, </year> <pages> 2099-2103. </pages>
Reference: [27] <author> Skolnik, M.I., </author> <title> Introduction to Radar Systems, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: Generally this is accomplished with multiple phase shifters for each element. Antenna arrays may be used for reception as well as transmission. By using multiple phase shifters for each element, the array may be used to look in all directions at once. Figure 2, adapted from Skolnik <ref> [27] </ref>, will help illustrate this idea. f 2p d l ( ) q sin= c f 9 R indicates a receiving element, fs are phase shifters and Ss are summations. The example of Figure 2 shows a 3 element array receiving 3 beams.
Reference: [28] <author> Wallach, H. </author> <title> On Sound Localization, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 10, </volume> <year> 1939, </year> <pages> 270-274. </pages>
Reference-contexts: However, a single ITD does not produce a unique source location. Instead, it places the source on the surface of revolution of a hyperboloid, called the cone of confusion <ref> [28] </ref>. This is an inherent limitation of the information available from time delay.
Reference: [29] <author> Wightman, F. and Kistler, D., </author> <title> The Dominant Role of Low-Frequency Interau ral Time Differences in Sound Localization, </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> Vol. 91, </volume> <month> March </month> <year> 1992, </year> <pages> 1648-1661. </pages>
Reference-contexts: The relative delays of the two neurons corresponds to the ITD [12][7][18]. Humans use both ITDs and IADs, but their interrelation is more complex and they are derived from different portions of the audible spectrum. ITDs are generated from mainly low-frequency sounds <ref> [29] </ref>, as the measurement of ITDs and IADs by humans, becomes ambiguous for frequencies of greater than approximately 1500Hz. While evidence suggests that delay line computation occurs in humans, we do not possess the skeletal ear asymmetry that owls use to measure elevation.
Reference: [30] <author> Williams, J.R., </author> <title> Fast Beam-Forming Algorithm, </title> <journal> Journal of the Acoustic Soci ety of America, </journal> <volume> Vol. 44, No. 5, </volume> <year> 1968, </year> <pages> 1454-1455. </pages>
Reference-contexts: The disadvantage is that computationally, the beamforming algorithm involves taking the FFT of each microphones signal, point-wise multiplying it by the appropriate time shift, summing the results for each microphone and computing the inverse FFT <ref> [30] </ref>. This is expensive in large arrays, though it can be parallelized. 11 3. Recovering Sound Source Direction using Two Microphones Handling the sound source localization problem in its full generality is a complex and computationally expensive task.
Reference: [31] <author> Wixson, L.E. and Ballard D.H., </author> <title> Using Intermediate Objects to Improve the Efficiency of Visual Search, </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 12, No. </volume> <pages> 2-3, </pages> <month> April </month> <year> 1994, </year> <pages> 209-230. </pages>
Reference: [32] <author> Zeira, A., </author> <title> Realizable Lower Bounds for Time Delay Estimation, </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 41, No. 11, </volume> <month> November </month> <year> 1993, </year> <pages> 3102 3113. </pages>
References-found: 32

