URL: http://ftp.eecs.umich.edu/people/neuhoff/bcq.ps
Refering-URL: http://ftp.eecs.umich.edu/people/neuhoff/
Root-URL: http://www.eecs.umich.edu
Title: Block-Constrained Methods of Fixed-Rate, Entropy-Coded, Scalar Quantization  
Author: Ahmed S. Balamesh and David L. Neuhoff 
Keyword: Key Words Entropy Coding, Scalar Quantization, Structured Vector Quantiza tion, Block-Constrained Quantization, Fixed-Rate Quantization.  
Date: September 24, 1992  
Address: Ann Arbor, Michigan 48109  
Affiliation: Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: Motivated by the recent work of Laroia and Farvardin, this paper presents new reduced-complexity methods for avoiding the buffering problems associated with entropy-coded, scalar quantization. Basically, given a fixed-size source block, these methods use dynamic programming and other techniques to search the sequences produceable by an entropy-coded, scalar quantizer for one with minimum distortion subject to a constraint on the number bits produced by some binary encoding of these sequences. The result is that although some encoding methods might have a variable rate on the sample level, the overall quantizer has a fixed rate on the block level. A general class of such methods, called block-constrained quantization, is introduced. A way to reduce the encoding complexity and several ways to to simplify the search complexity are found. A node-varying method with improved performance is given. New insight into the performance of block-constrained quantizers is presented. Compared to the original Laroia-Farvardin method, the results presented here show small improvements in performance and large reductions in complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Farvardin and J. W. Modestino, </author> <title> "Adaptive buffer-instrumented entropy-coded quantizer performance for memoryless sources," </title> <journal> IEEE Trans. Infrom. Theory, </journal> <volume> vol. IT-32, </volume> <pages> pp. 9-22, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: The major disadvantage of entropy-coded quantizers is their variable output rate, which requires buffering and causes synchronization problems. Buffer-instrumented strategies have been used <ref> [1] </ref> to minimize such problems. However, such schemes degrade the performance of the system for future samples of the source if the past ones were bad. This means that the wrong samples get the "punishment".
Reference: [2] <author> R. Laroia and N. Farvardin, </author> <title> "A structured fixed-rate vector quantizer derived from a variable-length encoded scalar quantizer," </title> <note> submitted to IEEE Trans. In-from. Theory. </note>
Reference-contexts: However, such schemes degrade the performance of the system for future samples of the source if the past ones were bad. This means that the wrong samples get the "punishment". A new fixed-rate method that resembles entropy-coded, scalar quantization has recently been proposed by Laroia and Farvardin <ref> [2] </ref>. With this approach, given a block of source symbols, dynamic programming is used to find a sequence of quantization levels that can be encoded with a fixed number of bits, using a kind of lexicographic indexing. <p> However, at moderate-to-high rates, only small dimensions are practical, which greatly limits VQ performance. Even relatively low-complexity structured VQ's (such as two-stage and tree-structured) have large complexities in this range of rates. Thus, there is considerable need for low-complexity methods. Block-constrained methods such as those presented in <ref> [2] </ref> and here have complexity that increases slowly with rate. Thus, they are well suited to coding in the moderate-to-high rate range. This paper is oriented towards achieving the performance of scalar quantization with first-order entropy coding, but with fixed-rate, low-complexity schemes. <p> The more sophisticated Laroia-Farvardin approach is postponed to Section 4, where it will be seen to overcome some shortcomings of our approach at the expense of increased complexity. Section 4, presents our approach and that of Laroia and Farvardin <ref> [2] </ref> in a more unified framework. We use the term block-constrained, fixed-rate, entropy-coded quantization (or block-constrained quantization (BCQ) for short) to describe such schemes. Section 5 describes the implementation of the dynamic programming search. Section 6 discusses design and optimization of BCQ's. <p> This method is actually just one of a family of methods that we call block-constrained, fixed-rate, entropy-coded quantization (or block-constrained quantization (BCQ) for short) that were inspired by the original pioneering work of Laroia and Far-vardin <ref> [2] </ref>. We now give the details of the specific block-constrained method described above. <p> As shown above, this loss is inherited by any pe-BCQ. Of course, this redundancy can be reduced by applying a prefix code to blocks of quantizer levels. However, this will increase the complexity of the BCQ. As we will explain later, Laroia and Farvardin <ref> [2] </ref> avoid this redundancy by assigning non-integer lengths to the quantization levels. This, however, means that the binary encoding/decoding can no longer be done on a sample-by-sample basis. <p> We will discuss this in more detail in the next section. 4 General Block-Constrained Quantizers In this section, we describe a more general class of block-constrained quantizers motivated by the case explained above and by the work of Laroia and Farvardin <ref> [2] </ref>. 13 As so far introduced, a BCQ is characterized by quantization levels q, lengths l (positive integers satisfying Kraft's inequality), a blocklength n and a threshold L: Its codebook consists of the set of quantization level sequences y, called codevectors, whose cumulative length l (y) does not exceed the threshold <p> If l consists of rational (or rationalizable) lengths with a relatively small denominator, then the quantization can be implemented efficiently using dynamic programming as proposed in <ref> [2] </ref> and explained in Section 5. If l consists of arbitrary real numbers, the implementation of (3) can be complex; however, low-complexity, approximations are possible. Such methods will be considered in Sections 9 and 10. There are many ways to binary-encode the codevectors into bits. <p> Its OPTA is ffi be 4 n;m;q;l;L:R be D bcq (q; l; n; L): Laroia and Farvardin <ref> [2] </ref> showed 5 ffi be sq (r): (9) Therefore, be-BCQ is, asymptotically, at least as good as optimum, entropy-constrained scalar quantization. <p> In this case the minimization can be implemented using dynamic programming as in <ref> [2] </ref>. For completeness and for future reference, we describe it here. Assume that a source sequence x is given. Let y fl denote a minimizing sequence of levels and D fl denote the resulting distortion. The minimization in (3) is simplified by the introduction of the notion of state. <p> To do so, one may alternately optimize the lengths l for given levels q; and vice versa. 1. Optimizing q given l; n and L Given l; one can optimize q using the method described in <ref> [2] </ref>. This is, actually, a generalized LBG [14, 15] algorithm for trellises, which guarantees monotonic decrease in distortion per every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors. <p> Thus, y k is no longer guaranteed to belong to C bcq (q; ~ l; n; L). Nevertheless, this algorithm can be tried. In many cases, it results in a reduction in distortion. Another less intuitive algorithm has been proposed by Laroia and Farvardin in <ref> [2] </ref>, which they argue is asymptotically optimum in n: 3. Choice of initial q and l Initially, q and l can be obtained using a good ECSQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. <p> Also, the optimized q; l found for high dimensions are good initial choices for smaller dimensions. For BCQ's using other encoding schemes similar considerations may still hold, as in the case of lf-BCQ <ref> [2] </ref>. 7 Comparison of pe-BCQ and lf-BCQ Complexity Table 1 gives rough complexity estimates for the dynamic programming implementation of the codebook search for pe-BCQ (L = nr) and lf-BCQ using rational lengths with denominator b (L 1:5bnr:) Only the dominant terms are shown. <p> Performance Table 2 shows the signal-to-noise ratios of pe-BCQ for rates 1:5; 2; 2:5; 3 and dimensions 48; 96; 144; 192; for IID Gaussian and Laplacian sources. The signal-to-noise ratio of lf-BCQ with dimension 32 and b = 4, as reported in <ref> [2] </ref>, is also included, to compare with the pe-BCQ with dimension 192. In this case, pe-BCQ has the same search complexity as lf-BCQ, while the former has comparatively negligible encoding complexity and the latter requires six times less storage for the search. <p> Moreover, its larger dimension reduces its code-space loss. The fact that pe-BCQ performs better, relative to lf-BCQ, on the Laplacian 7 These are quoted from <ref> [2] </ref>. 25 source can be understood by noting that the Laplacian density has heavier tails than the Gaussian (in a sense, it's more nearly uniform), so the codeword lengths are more uniform. <p> For these reasons, we feel that the effect of m needs to be extensively investigated and, thus, we did not try to investigate it here. We leave it for future research. The reader is referred to <ref> [2] </ref> for some discussion of the effect of m on be-BCQ's. 8 Reduced-Complexity Almost-Optimal Search Motivation Consider a block-constrained quantizer BCQ (q; l; n; L); and let Q () be the nearest-neighbor, scalar quantization rule corresponding to q: Given a source vector x; if 26 l ( Q (x)) L; then <p> The richness of 42 such schemes necessitates deeper understanding. this to future studies. We leave this to future studies. 12 Conclusion In this paper, we have explored a quite general class of fixed-rate quantizers that we called block-constrained quantizers, motivated by the work of Laroia and Far-vardin <ref> [2] </ref>. We proposed methods to reduce the binary encoding complexities of such quantizers. We have discussed the relationship between block-constrained quantizers, entropy-coded/entropy-constrained quantizers and permutation codes. Moreover, we have explored some basic properties of BCQ's that give insight into their behavior.
Reference: [3] <author> R. Laroia and N. Farvardin, </author> <title> "Extension of of the fixed-rate structured vector quantizer to vector sources," </title> <booktitle> presented in the twenty-fifth annual conference on information sciences and systems, </booktitle> <publisher> Johns Hopkins, </publisher> <address> Baltimore, </address> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: For memoryless sources and moderate-to-high rates, such performance is better than that achievable by practical VQ's. For sources with memory, VQ may work better, but it is believed that th methods described here can be extended to exploit source correlation, as for example in <ref> [3] </ref>. In this case, future work may find these methods to be 3 competitive with the best fixed-rate quantization techniques. <p> Throughout this paper, we use the squared-error distortion measure and results are given only for independent, identically-distributed (IID) sources, Gaussian and Laplacian; however, it is anticipated that the results extend to sources with memory, as for example in <ref> [3] </ref>. Section 2 gives background material on entropy-coded, scalar quantizers (ECSQ's). Section 3 introduces the problem of converting ECSQ's to fixed-rate. Although the first work in this area is that of Laroia and Farvardin, we describe our approach first, as it is more elementary.
Reference: [4] <author> T. J. Goblick, Jr. and J. L. Holsinger, </author> <title> "Analog source digitization: A comparison of theory and practice," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-13, </volume> <pages> pp. 323-326, </pages> <month> Apr. </month> <year> 1967. </year>
Reference-contexts: The first discovery was that of Goblick and Holsinger <ref> [4] </ref> who numerically demonstrated that, for large values of r; uniform quantizers with output entropy r have distortion about 1:5 dB larger than D (r).
Reference: [5] <author> H. Gish and J. N. Pierce, </author> <title> "Asymptotically efficient quantizing," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-14, </volume> <pages> pp. 676-683, </pages> <month> Sept. </month> <year> 1968. </year>
Reference-contexts: The first discovery was that of Goblick and Holsinger [4] who numerically demonstrated that, for large values of r; uniform quantizers with output entropy r have distortion about 1:5 dB larger than D (r). Subsequently, Gish and Pierce <ref> [5] </ref> proved the optimality of uniform scalar quantizers for sufficiently smooth source densities and asymptotically large rates r: They also proved that lim r!1 (ffi ent sq (r)=D (r)) = e=6 (i.e. an increase of 1:53 dB). Wood [6] proved similar results.
Reference: [6] <author> R. C. Wood, </author> <title> "On optimum quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-15, </volume> <pages> pp. 248-252, </pages> <month> Mar. </month> <year> 1969. </year>
Reference-contexts: The rate of the prefix code (and the resulting ECSQ) equals the quantizer entropy plus a number between 0 and 1; called the redundancy of the prefix code. Algorithms for designing quantizers with minimum distortion subject to a con straint on their entropy have been proposed in <ref> [6, 7, 8, 10, 11] </ref>. Such quantizers are called entropy-constrained, scalar quantizers, to distinguish them from the abovementioned entropy-coded, scalar quantizers 1 . <p> Subsequently, Gish and Pierce [5] proved the optimality of uniform scalar quantizers for sufficiently smooth source densities and asymptotically large rates r: They also proved that lim r!1 (ffi ent sq (r)=D (r)) = e=6 (i.e. an increase of 1:53 dB). Wood <ref> [6] </ref> proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see [6, 7, 9, 10]). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [10], for a Gaussian source. <p> Wood [6] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [6, 7, 9, 10] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [10], for a Gaussian source.
Reference: [7] <author> T. Berger, </author> <title> "Optimum quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 759-765, </pages> <month> Nov. </month> <year> 1972. </year>
Reference-contexts: The rate of the prefix code (and the resulting ECSQ) equals the quantizer entropy plus a number between 0 and 1; called the redundancy of the prefix code. Algorithms for designing quantizers with minimum distortion subject to a con straint on their entropy have been proposed in <ref> [6, 7, 8, 10, 11] </ref>. Such quantizers are called entropy-constrained, scalar quantizers, to distinguish them from the abovementioned entropy-coded, scalar quantizers 1 . <p> Wood [6] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [6, 7, 9, 10] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [10], for a Gaussian source. <p> Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 7, 9] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation codebooks.
Reference: [8] <author> A. N. Netravali and R. Saigal, </author> <title> "Optimum quantizer design using a fixed-point algorithm," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 55, </volume> <pages> pp. 1423-1435, </pages> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: The rate of the prefix code (and the resulting ECSQ) equals the quantizer entropy plus a number between 0 and 1; called the redundancy of the prefix code. Algorithms for designing quantizers with minimum distortion subject to a con straint on their entropy have been proposed in <ref> [6, 7, 8, 10, 11] </ref>. Such quantizers are called entropy-constrained, scalar quantizers, to distinguish them from the abovementioned entropy-coded, scalar quantizers 1 .
Reference: [9] <author> T. Berger, </author> <title> "Minimum entropy quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 149-157, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: Wood [6] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [6, 7, 9, 10] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [10], for a Gaussian source. <p> Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 7, 9] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation codebooks.
Reference: [10] <author> N. Farvardin and J. W. Modestino, </author> <title> "Optimum quantizer performance for a class of non-Gaussian memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-30, </volume> <pages> pp. 485-497, </pages> <month> May </month> <year> 1984. </year> <month> 50 </month>
Reference-contexts: The rate of the prefix code (and the resulting ECSQ) equals the quantizer entropy plus a number between 0 and 1; called the redundancy of the prefix code. Algorithms for designing quantizers with minimum distortion subject to a con straint on their entropy have been proposed in <ref> [6, 7, 8, 10, 11] </ref>. Such quantizers are called entropy-constrained, scalar quantizers, to distinguish them from the abovementioned entropy-coded, scalar quantizers 1 . <p> Wood [6] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [6, 7, 9, 10] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [10], for a Gaussian source. <p> Wood [6] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see [6, 7, 9, 10]). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of <ref> [10] </ref>, for a Gaussian source. <p> Choice of initial q and l Initially, q and l can be obtained using a good ECSQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. The method of Chou, et al., [11] (which is a generalization of Algorithm 2 in <ref> [10] </ref>) is quite a good choice. We tried both the training-sequence and density versions of such. For high rates, 23 we found that a uniform scalar quantizer with a Huffman code is a sufficiently good initial choice. <p> Finally, we comment on the effect of m on the performance of BCQ's. One is tempted to believe that the performance of a BCQ must improve as m increases. However, the subtle behavior of the number of levels in entropy-constrained quantizers <ref> [12, 10] </ref>, as well as entropy-coded quantizers, makes the issue less trivial. Moreover, the effects of m on the code-space loss is not at all clear.
Reference: [11] <author> P. A. Chou, T. Lookabaugh and R. M. Gray, </author> <title> "Entropy-constrained vector quan-tization," </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol. ASSP-37, </volume> <pages> pp. 31-42, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: The rate of the prefix code (and the resulting ECSQ) equals the quantizer entropy plus a number between 0 and 1; called the redundancy of the prefix code. Algorithms for designing quantizers with minimum distortion subject to a con straint on their entropy have been proposed in <ref> [6, 7, 8, 10, 11] </ref>. Such quantizers are called entropy-constrained, scalar quantizers, to distinguish them from the abovementioned entropy-coded, scalar quantizers 1 . <p> Thus, the comparison in Figure 3 is unfair to the naive system. To make a fairer comparison, we ran the naive system with a variety of threshold sets, and for each r we plot in Figure 4 the best signal-to-noise ratio attained. Motivated by the work of <ref> [11] </ref>, for a range of 0; we considered sets of thresholds that minimized the functional (x q j ) 2 + l j over j. The same figure also shows the performance of BCQ as well as its asymptotic distortion limit ffi sq (q; l; r). <p> Choice of initial q and l Initially, q and l can be obtained using a good ECSQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. The method of Chou, et al., <ref> [11] </ref> (which is a generalization of Algorithm 2 in [10]) is quite a good choice. We tried both the training-sequence and density versions of such. For high rates, 23 we found that a uniform scalar quantizer with a Huffman code is a sufficiently good initial choice.
Reference: [12] <author> J. C. Kieffer, T. M. Jahns and V. A. Obuljen, </author> <title> "New results on optimal entropy-constrained quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-34, </volume> <pages> pp. 1250-1258, </pages> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: Finally, we comment on the effect of m on the performance of BCQ's. One is tempted to believe that the performance of a BCQ must improve as m increases. However, the subtle behavior of the number of levels in entropy-constrained quantizers <ref> [12, 10] </ref>, as well as entropy-coded quantizers, makes the issue less trivial. Moreover, the effects of m on the code-space loss is not at all clear.
Reference: [13] <author> T. Berger, F. Jelinek and J. K. Wolf, </author> <title> "Permutation codes for sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 160-169, </pages> <month> Jan. </month> <year> 1972. </year>
Reference-contexts: Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 7, 9] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation codebooks. <p> : : : ; m; P m n; j=1 n j l j Lg: Then, the block-constrained quantizer codebook is given by C bcq (q; l; n; L) = n2N (l;n;L) q (n): Moreover, for any n; C n q (n) is a the codebook of a variant-I permutation code <ref> [13] </ref>. Thus, a BCQ codebook is the union of permutation codebooks. It is interesting to note that the lengths l and L determine which types are included and which are not. Thus, careful choice of l and L can be used to construct codes with special features. <p> Permutation codes do this. However, if n is not sufficiently high, then it is better to distribute the codewords over a spherical shell whose thickness increases with rate. This explains the observation in <ref> [13] </ref> that for a given dimension n; the distortion of permutation codes diverges from ffi ent sq (r) as r increases.
Reference: [14] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-28, </volume> <pages> pp. 84-95, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: To do so, one may alternately optimize the lengths l for given levels q; and vice versa. 1. Optimizing q given l; n and L Given l; one can optimize q using the method described in [2]. This is, actually, a generalized LBG <ref> [14, 15] </ref> algorithm for trellises, which guarantees monotonic decrease in distortion per every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors.
Reference: [15] <author> L. C. Stewart, R. M. Gray and Y. Linde, </author> <title> "The design of trellis waveform coders," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. COM-30, </volume> <pages> pp. 702-710, </pages> <month> Apr. </month> <year> 1982. </year>
Reference-contexts: To do so, one may alternately optimize the lengths l for given levels q; and vice versa. 1. Optimizing q given l; n and L Given l; one can optimize q using the method described in [2]. This is, actually, a generalized LBG <ref> [14, 15] </ref> algorithm for trellises, which guarantees monotonic decrease in distortion per every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors.
Reference: [16] <author> A. S. Balamesh and D. L. Neuhoff, </author> <note> in preparation. </note>
Reference-contexts: t 0 such that l (t 0 ; l) &lt; r; it follows that lim sup D bcq (q; l; n; nr) ffi sq (q; l; r); where ffi sq (q; l; r) is defined by (2) and assumed to be continuous at r. 3 Indeed, it is shown in <ref> [16] </ref> that inf D bcq (q; l; n; nr) = lim D bcq (q; l; n; nr) = ffi sq (q; l; r); (5) assuming ffi sq (q; l; r) is continuous at r: Moreover, it is clear that lim R bcq (n; nr) = lim bnrc n Therefore, the limiting <p> This means that the best performance achieved by the class of BCQ's introduced so far (i.e. prefix-encoded BCQ's) is, exactly, the best performance achieved by ECSQ's. (See <ref> [16] </ref> for more details.) It is important to notice that (5) holds for any choice of l; it does not require l to satisfy Kraft's inequality. Indeed, it holds for any real lengths (even negative ones). <p> We let L be (l; n; r) denote any such value. With this choice of threshold, we are guaranteed to have R be bcq (l; n; L be (l; n; r)) r; and it can happen that the inequality is strict. However, it is shown in <ref> [16] </ref> that lim n!1 R be bcq (l; n; L be (l; n; r)) = r: Moreover, a constant c l (r) is found such that lim n!1 L be (l; n; r)=n = c l (r); which shows the asymptotic behavior of L be (l; n; r) for large n. <p> Moreover, for any fixed q and l; they show that the limiting performance of be-BCQ cannot be better than entropy-constrained scalar quantization, i.e. lim inf D bcq (q; l; n; L be (l; n; r)) ffi ent 4 See <ref> [16] </ref> for a more general version that does not require Kraft's condition. 5 Although they do not explicitly mention it, this, and the following, result require ffi ent sq to be continuous at r: 15 They also make the plausible conjecture that, even for small values of n; be-BCQ cannot do <p> Alternative, more constructive proofs for (9) and (10) are given in <ref> [16] </ref>. In view of the above, a be-BCQ based on an optimum, entropy-constrained scalar quantizer seems a good idea. <p> This loss, which is an example of what we call a code-space loss, and other losses encountered in BCQ will be discussed later. 6 For this case, as shown in <ref> [16] </ref>, c l (r) = 1; so L be (l; n; r) nr is a good choice. 16 Laroia-Farvardin Approach As we have seen, the pe-BCQ's introduced in Section 3 inherited the redundancy loss of first-order prefix codes.
Reference: [17] <author> M. S. Bazaraa and C. M. Shetty, </author> <title> Nonlinear Programming: Theory and Algorithms, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <year> 1979. </year>
Reference-contexts: of entropy-constrained, scalar quantization and by the work of Chou, et al.[11], we consider scalar quantizer thresholds that minimize the quantity (x q j ) 2 + l j for different values of the Lagrange multiplier : To further motivate this collection of scalar quantizers, consider the 34 Lagrangian relaxation <ref> [17] </ref> of (3): Minimize n X (x i y i ) 2 + l (y i ); (13) subject to y i 2 fq 1 ; q 2 ; : : : ; q m g for all i; where 0 is a Lagrange multiplier. <p> By Chernoff's bound, Pr i=1 ) = exp (n (su n ln M (s))); for all s 0; which implies Pr i=1 ) s0 = exp (nI (u n )) Since I ( l) = I 0 ( l) = 0; then by the second-order Taylor's theorem (e.g. <ref> [17, p. 504] </ref>), there exists fl n ; l &lt; fl n &lt; u n ; such that I (u n ) = 2 or v u I 00 (fl n ) v u I 00 (fl n ) p + l 2 ln (1=ff)K 1 p + l = p
Reference: [18] <author> W. H. Press, et al., </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing. </booktitle>
Reference-contexts: We did not spend any effort in optimizing the strategy of varying : Methods from non-linear programming can also be used to optimize the strategy, e.g. bisection and related methods (see <ref> [18] </ref>). Also, it should be noted that in the above method, iterations continue until L () = L or the maximum number of iterations is exceeded. In many cases, no improvement is obtained after a certain number of iterations.
Reference: [19] <author> H. L. Royden, </author> <title> Real Analysis, </title> <editor> 3rd Ed., NY:Macmillan, </editor> <year> 1988. </year>
Reference-contexts: Define a n 4 = E [f n (X)]: First, we show that for any convergent subsequence a n k ; a n k ! 0 as k ! 1: Note that f n k (X ) ! 0; in probability, as k ! 1: Then, by Proposition 4.18 of <ref> [19] </ref>, there exists a further subsequence f n k i (X) such that f n k i (X) ! 0; with probability 1, as i ! 1: Now, by Theorem 4.17 of [19], a n k i = E [f n k i (X )] ! 0 as i ! 1 <p> f n k (X ) ! 0; in probability, as k ! 1: Then, by Proposition 4.18 of <ref> [19] </ref>, there exists a further subsequence f n k i (X) such that f n k i (X) ! 0; with probability 1, as i ! 1: Now, by Theorem 4.17 of [19], a n k i = E [f n k i (X )] ! 0 as i ! 1 and since a n k is convergent, then a n k ! 0 as k ! 1: Now, it suffices to show that a n ! 0 as n ! 1: To

References-found: 19

