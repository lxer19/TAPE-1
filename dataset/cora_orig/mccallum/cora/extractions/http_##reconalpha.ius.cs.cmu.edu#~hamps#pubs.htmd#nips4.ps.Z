URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/nips4.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/index.html
Root-URL: 
Email: hamps@speech1.cs.cmu.edu and kumar@gauss.ece.cmu.edu  
Title: Shooting Craps in Search of an Optimal Strategy for Training Connectionist Pattern Classifiers  
Author: J. B. Hampshire II and B. V. K. Vijaya Kumar 
Address: Pittsburgh, PA 15213-3890  
Affiliation: Department of Electrical Computer Engineering Carnegie Mellon University  
Note: January 9, 1992 11:43 am. Preprinted from Advances in Neural Information Processing Systems, vol. 4 Morgan-Kaufmann, 1992, pp. 1125 1132  Page 1 of 9  
Abstract: We compare two strategies for training connectionist (as well as non-connectionist) models for statistical pattern recognition. The probabilistic strategy is based on the notion that Bayesian discrimination (i.e., optimal classification) is achieved when the classifier learns the a posteriori class distributions of the random feature vector. The differential strategy is based on the notion that the identity of the largest class a posteriori probability of the feature vector is all that is needed to achieve Bayesian discrimination. Each strategy is directly linked to a family of objective functions that can be used in the supervised training procedure. We prove that the probabilistic strategy linked with error measure objective functions such as mean-squared-error and cross-entropy typically used to train classifiers necessarily requires larger training sets and more complex classifier architectures than those needed to approximate the Bayesian discriminant function. In contrast, we prove that the differential strategy linked with classification figure-of-merit objective functions (CFM mono ) [3] requires the minimum classifier functional complexity and the fewest training examples necessary to approximate the Bayesian discriminant function with specified precision (measured in probability of error). We present our proofs in the context of a game of chance in which an unfair C-sided die is tossed repeatedly. We show that this rigged game of dice is a paradigm at the root of all statistical pattern recognition tasks, and demonstrate how a simple extension of the concept leads us to a general information-theoretic model of sample complexity for statistical pattern recognition. fl Copyright c fl1992 by J. B. Hampshire II and B. V. K. V. Kumar: all rights reserved. This research was funded by the Air Force Office of Scientific Research (grant AFOSR-89-0551). The views and conclusions contained in this paper are the authors' and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Air Force or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <year> 1992. </year> <note> manuscript in progress. Page 8 of 9 NIPS-4: Shooting Craps & Pattern Classifiers Hampshire & Kumar Preprint January 9, 1992: 11:43 am </note>
Reference: [2] <author> J. B. Hampshire II and B. A. Pearlmutter. </author> <title> Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function. </title> <editor> In Touretzky, Elman, Sejnowski, and Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 159-172, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan-Kaufmann. </publisher>
Reference: [3] <author> J. B. Hampshire II and A. H. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 216-228, </pages> <month> June </month> <year> 1990. </year> <booktitle> A revised and extended version of work first presented at the 1989 International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 235-241. </pages>
Reference: [4] <author> A. N. </author> <title> Kolmogorov. Three Approaches to the Quantitative Definition of Information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1(1) </volume> <pages> 1-7, </pages> <month> Jan. Mar. </month> <year> 1965. </year> <note> Faraday Press translation of Problemy Peredachi Informatsii. </note>
Reference: [5] <author> M. D. Richard and R. P. Lippmann. </author> <title> Neural Network Classifiers Estimate Bayesian a posteriori Probabilities. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference: [6] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year> <note> Page 9 of 9 </note>
References-found: 6

