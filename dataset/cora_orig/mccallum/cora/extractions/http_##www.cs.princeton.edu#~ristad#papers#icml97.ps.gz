URL: http://www.cs.princeton.edu/~ristad/papers/icml97.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/bibliography.html
Root-URL: http://www.cs.princeton.edu
Email: ristad@cs.princeton.edu  pny@research.nj.nec.com  
Title: Learning String Edit Distance  
Author: Eric Sven Ristad Peter N. Yianilos 
Address: 35 Olden Street Princeton, NJ 08544  4 Independence Way Princeton, NJ 08540  
Affiliation: Department of Computer Science Princeton University  NEC Research Institute  
Abstract: In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other. Here we provide a stochastic model for string edit distance. Our stochastic model allows us to learn the optimal string edit distance function from a corpus of examples. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech. In this application, we learn a string edit distance function with one third the error rate of the untrained Levenshtein distance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> COMLEX pronouncing lexicon, version 0.2. </editor> <booktitle> Linguistic Data Consortium LDC95L3, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Each step of the transduction generates either a substitution pair ha; bi, a deletion pair ha; *i, or an insertion pair h*; bi according to a probability function ffi : E ! <ref> [0; 1] </ref>. Being a probability function, ffi () satisfies the following constraints: a: 8z 2 E [ 0 ffi (z) 1 ] P Note that the null operation h*; *i is not included in the alphabet E of edit operations. <p> Approximately 280,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet [6]. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) <ref> [1] </ref>. In order to make the pronouncing lexicon compatible with the ICSI corpus of phonetic transcripts, we removed 148 entries from the lexicon and 73,068 samples from the ICSI corpus. 3 After filtering, our pronouncing lexicon had 70,952 entries for 66,284 syntactic words over an alphabet of 42 phonemes.
Reference: [2] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps.
Reference: [3] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps.
Reference: [4] <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> Maximum likelihood from incomplete data via 0 0.4 0.8 0 1 2 3 4 5 6 7 8 9 10 WER EM Iterations Switchboard Experiment E3 Levenshtein Stochastic Viterbi ICSI training corpus. The test corpus has 512 samples whose words do not appear in the resulting lexicon. The transduction distances have less than one third the error rate of the Levenshtein distance, and their performance is continuing to improve at 10 EM iterations. the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B (methodological) 39 (1977), </journal> <pages> 1-38. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) algorithm <ref> [2, 3, 4] </ref>. The EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See [15] for a review. As its name suggests, the EM algorithm consists of two steps. <p> We estimate the parameters of our model (5) using expectation maximization for finite mixture models <ref> [4] </ref>.
Reference: [5] <author> Godfrey, J., Holliman, E., and McDaniel, J. </author> <title> Switchboard: telephone speech corpus for research and development. </title> <booktitle> In Proc. IEEE ICASSP (Detroit, </booktitle> <year> 1995), </year> <pages> pp. 517-520. </pages>
Reference-contexts: It also leads to a variant of string edit distance, that aggregates the many different ways to transform one string into another. We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in the Switchboard corpus of conversational speech <ref> [5] </ref>. In this application, we learn a string edit distance function that reduces the error rate of the untrained Levenshtein distance by a factor of three. fl Peter Yianilos is also with the Department of Computer Science, Princeton University. Let us first define our notation. <p> We do not believe that this substitution had a sig nificant effect on our experiments. Let us now apply our stochastic approach to the Switchboard corpus of conversational speech. 3.1 Switchboard Corpus The Switchboard corpus contains over 3 million words of spontaneous telephone speech conversations <ref> [5] </ref>. It is considered one of the most difficult corpora for speech recognition (and pronunciation recognition) because of the tremendous variability of spontaneous speech. Current speech recognition technology has a word error rate above 45% on the Switchboard corpus.
Reference: [6] <author> Greenberg, S., Hollenbach, J., and Ellis, D. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. ICSLP (Philadelphia, </booktitle> <month> October </month> <year> 1996). </year>
Reference-contexts: As a point of contrast, the same speech recognition technology achieves a word error rate of less than 5% on read speech. Approximately 280,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI using a proprietary phonetic alphabet <ref> [6] </ref>. The Switchboard corpus also includes a pronouncing lexicon with 71,100 entries using a modified Pronlex phonetic alphabet (long form) [1].
Reference: [7] <author> Hall, P., and Dowling, G. </author> <title> Approximate string matching. </title> <journal> Computing Surveys 12, </journal> <volume> 4 (1980), </volume> <pages> 381-402. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 18] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11]. <p> Many excellent reviews of the string edit distance literature are available [7, 9, 14, 18]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11]. A stochastic interpretation of string edit distance was first noted by Hall and Dowling <ref> [7, p.390-1] </ref> in their review of approximate string matching algorithms, but without a proposal for learning the edit costs. The principal contribution of this article is an effective algorithm for learning the primitive edit costs.
Reference: [8] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: Such a stochastic transducer can be further strengthened with state-conditional interpolation <ref> [8] </ref>. We could also condition our probabilities ffi (zjhx t tn ; y v vn i; s) on a hidden state s drawn from a finite state space.
Reference: [9] <author> Kukich, K. </author> <title> Techniques for automatically correcting words in text. </title> <booktitle> ACM Compute. Surveys 24 (1992), </booktitle> <pages> 377-439. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 18] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [10] <author> Levenshtein, V. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Soviet Physics Doklady 10, </journal> <volume> 10 (1966), </volume> <pages> 707-710. </pages>
Reference-contexts: 1 Introduction In many applications, it is necessary to determine the similarity of two strings. A widely-used notion of string similarity is the edit distance: the minimum number of insertions, deletions, and substitutions required to transform one string into the other <ref> [10] </ref>. In this article, we provide a stochastic model for string edit distance. Our stochastic interpretation allows us to learn the optimal string edit distance function from a corpus of examples. <p> In all cases, we partitioned our corpus of 214,310 samples 9:1 into 192,879 training samples and 21,431 test samples. Our seven models consist of Levenshtein distance <ref> [10] </ref> as well as six variants resulting from our two inter 3 From the lexicon, we removed 148 entries whose words had unusual punctuation ([&lt;!.]).
Reference: [11] <author> Marzal, A., and Vidal, E. </author> <title> Computation of normalized edit distance and applications. </title> <journal> IEEE Trans. </journal> <volume> PAMI 15, </volume> <month> 9 </month> <year> (1993), </year> <month> 926-932. </month> <title> 0 0.2 0.6 1 WER EM Iterations Switchboard Experiment E4 Levenshtein Stochastic Viterbi portions. The transduction distances have less than one third the error rate of the Levenshtein distance, and their performance is continuing to improve at 10 EM iterations. </title>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 18]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance <ref> [11] </ref>. A stochastic interpretation of string edit distance was first noted by Hall and Dowling [7, p.390-1] in their review of approximate string matching algorithms, but without a proposal for learning the edit costs. The principal contribution of this article is an effective algorithm for learning the primitive edit costs.
Reference: [12] <author> Masek, W., and Paterson, M. </author> <title> A faster algorithm computing string edit distances. </title> <journal> J. Com-put. System Sci. </journal> <volume> 20 (1980), </volume> <pages> 18-31. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming <ref> [12, 19] </ref>. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 18]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [13] <author> Oomman, B. </author> <title> Constrained string editing. </title> <booktitle> Information Sciences 40 (1986), </booktitle> <pages> 267-284. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 18]. Several variants have been proposed, including the constrained edit distance <ref> [13] </ref> and the normalized edit distance [11]. A stochastic interpretation of string edit distance was first noted by Hall and Dowling [7, p.390-1] in their review of approximate string matching algorithms, but without a proposal for learning the edit costs.
Reference: [14] <author> Peterson, J. </author> <title> Computer programs for detecting and correcting spelling errors. </title> <booktitle> Comm. ACM 23 (1980), </booktitle> <pages> 676-687. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 18] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [15] <author> Redner, R. A., and Walker, H. F. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review 26, </journal> <volume> 2 (1984), </volume> <pages> 195-239. </pages>
Reference-contexts: For this task, we employ the powerful expectation maximization (EM) algorithm [2, 3, 4]. The EM algorithm is an iterative algorithm that maximizes the probability of the training data according to the model. See <ref> [15] </ref> for a review. As its name suggests, the EM algorithm consists of two steps. In the expectation step, we accumulate the expectation of each hidden event on the training corpus. In our case the hidden events are the edit operations used to generate the string pairs.
Reference: [16] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Finite growth models. </title> <type> Tech. Rep. </type> <institution> CS-TR-533-96, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Each iteration of the EM algorithm is guaranteed to either increase the probability of the training corpus or not change the model parameters. The correctness of our algorithm is shown in related work <ref> [16] </ref>. expectation-maximization (, C) 1. until convergence 2. forall z in E [ fl (z) := 0; ] 3. for i = 1 to n 4. expectation-step (x T i ; y V i ; ; fl); 5. maximization-step (,fl); The fl (z) variable is used to accumulate the expected number <p> Convergence. The expectation-maximization () algorithm given above is guaranteed to converge to a local maximum on a given corpus C, by a reduction to finite growth models <ref> [16] </ref>. Here we demonstrate that there may be multiple local maxima, and that only one of these need be a global maxima. Consider a transducer with alphabets A = fa; bg and B = fcg being trained on a corpus C consisting of exactly one string pair habb; cci.
Reference: [17] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Learning string edit distance. </title> <type> Tech. Rep. </type> <institution> CS-TR-532-96, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: In related work <ref> [17] </ref>, we consider an alternate parameterization of the memoryless stochastic transducer that more cleanly supports transductions conditioned on string lengths. The remainder of this section explains how to use the memoryless stochastic transducer as a string edit distance.
Reference: [18] <author> Sankoff, D., and Kruskal, J. B., Eds. </author> <title> Time warps, string edits, and macromolecules: the theory and practice of sequence comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming [12, 19]. Many excellent reviews of the string edit distance literature are available <ref> [7, 9, 14, 18] </ref>. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
Reference: [19] <author> Wagner, R., and Fisher, M. </author> <title> The string to string correction problem. </title> <booktitle> JACM 21 (1974), </booktitle> <pages> 168-173. </pages>
Reference-contexts: The edit distance may be computed efficiently using dynamic programming <ref> [12, 19] </ref>. Many excellent reviews of the string edit distance literature are available [7, 9, 14, 18]. Several variants have been proposed, including the constrained edit distance [13] and the normalized edit distance [11].
References-found: 19

