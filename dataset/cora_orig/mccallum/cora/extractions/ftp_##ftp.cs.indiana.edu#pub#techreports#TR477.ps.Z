URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR477.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Loop Optimization for Aggregate Array Computations  
Author: Yanhong A. Liu and Scott D. Stoller 
Date: March 1997  
Abstract: An aggregate array computation is a loop that computes accumulated quantities over array elements. Such computations are common in programs that use arrays, and the array elements involved in such computations often overlap, especially across iterations of loops, resulting in significant redundancy in the overall computation. This paper presents a method and algorithms that eliminate such overlapping aggregate array redundancies and shows both analytical and experimental performance improvements. The method is based on incrementalization, i.e., updating the values of aggregate array computations from iteration to iteration rather than computing them from scratch in each iteration. This involves maintaining additional information not maintained in the original program and performing additionally enabled optimizations. We reduce various analysis problems to solving inequality constraints on loop variables and array subscripts, and we apply results from work on array data dependence analysis. Incremental-izing aggregate array computations produces drastic program speedup compared to previous optimizations. Previous methods for loop optimizations of arrays do not perform incremental ization, and previous techniques for loop incrementalization do not handle arrays.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year> <month> 17 </month>
Reference-contexts: A SUO for a parameter w is denoted w ; in contexts where it is irrelevant to the discussion which parameter is being considered, we simply write . Parameters of AACs can be identified using def-use chains <ref> [1] </ref>. Redefinitions of parameters that satisfy the above are SUOs. The heart of our approach is incrementalization of an AAC with respect to a SUO. For simplicity of exposition, we consider in this paper only updates to parameters that are themselves loop variables of loops enclosing the AAC. <p> The programs were compiled using Sun Microsystems' f77 compiler, with optimization flags -O4 and -fast. 6.1 Partial sum Partial sum is a simple but interesting and illustrative example. Given an array a [1::n] of numbers, for each index i (line <ref> [1] </ref>), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (17) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (17) It can be optimized using our algorithm. <p> Given an array a [1::n] of numbers, for each index i (line <ref> [1] </ref>), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (17) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (17) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. Now, consider the outer loop. Step 1. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (18). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (18). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (18). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. Running times for programs (17) and (18) are plotted in When code is written as in (17), previous techniques can recognize that each inner loop is independent. <p> We obtain s [i + 1] := s [i] + a [i + 1]. Step 4. Pruning leaves the code unchanged. Step 5. Initializing s <ref> [1] </ref> to a [1] and forming the rest of the loop for i = 2::n, we obtain the program (18). s [1] := a [1]; s [i] := s [i1] + a [i] This program takes only O (n) time. Running times for programs (17) and (18) are plotted in When code is written as in (17), previous techniques can recognize that each inner loop is independent.
Reference: [2] <author> V. H. Allan, R. B. Jones, R. M. Lee, and S. J. Allan. </author> <title> Software pipelining. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(3):366--432, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining <ref> [2] </ref>, and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines <ref> [2] </ref> to [4]). The straightforward program (17) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (17) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining <ref> [2] </ref>, and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [3] <author> F. E. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In R. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers, </booktitle> <pages> pages 1-30. </pages> <publisher> Prentice Hall, </publisher> <year> 1971. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion <ref> [3, 5, 29, 66] </ref>, pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines [2] to [4]). The straightforward program (17) takes O (n 2 ) time. [1] for i := 1 to n do <ref> [3] </ref> for j := 1 to i do (17) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. Now, consider the outer loop. Step 1. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion <ref> [3, 5, 13, 29, 66] </ref>, deforestation [65], and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [4] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 3, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations <ref> [4, 14, 31, 32] </ref>, finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> Given an array a [1::n] of numbers, for each index i (line [1]), compute the sum of elements 1 to i (lines [2] to <ref> [4] </ref>). The straightforward program (17) takes O (n 2 ) time. [1] for i := 1 to n do [3] for j := 1 to i do (17) It can be optimized using our algorithm. First, consider the inner loop. Its loop body does not contain any AACs. <p> For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century [30]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [4, 14, 31, 32, 64] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54].
Reference: [5] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1983. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion <ref> [3, 5, 29, 66] </ref>, pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion <ref> [3, 5, 13, 29, 66] </ref>, deforestation [65], and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [6] <author> J. Auslander, M. Philipose, C. Chambers, S. J. Eggers, and B. N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on PLDI, </booktitle> <pages> pages 149-159, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Specialization techniques, such as data specialization [35, 40], run-time specialization and code generation [15, 41, 42], and dynamic compilation and code generation <ref> [6, 18] </ref>, have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [7] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering <ref> [7, 39, 50, 56, 62] </ref>, but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] and source-to-source transformation <ref> [7, 39, 50, 56, 62] </ref> that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. Section 4 describes how to maintain additional information to facilitate incrementalization. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [7, 39, 50, 56, 62] </ref>, pipelining [2], and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [8] <author> F. L. Bauer, B. Moller, H. Partsch, and P. Pepper. </author> <title> Formal program construction by transformations|Computer-aided, </title> <journal> intuition-guided programming. IEEE Transactions on Software Engineering, </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [12, 38] can be used for optimization, as demonstrated in projects like CIP <ref> [8, 11, 55] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61].
Reference: [9] <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays.
Reference: [10] <author> M. Bromley, S. Heller, T. McNerney, and G. L. Steele Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 145-156, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs <ref> [10, 19] </ref>, etc. For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example [71, 73]. <p> In particular, we have used tools developed by Pugh's group [57, 58, 59, 60]. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs <ref> [10, 19] </ref>. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods. Besides achieving optimizations not previously possible, our techniques fall out of one general approach, rather than simply being yet another new but ad hoc method.
Reference: [11] <author> M. Broy. </author> <title> Algebraic methods for program construction: The project CIP. </title> <editor> In P. Pepper, editor, </editor> <booktitle> Program Transformation and Programming Environments, </booktitle> <pages> pages 199-222. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [12, 38] can be used for optimization, as demonstrated in projects like CIP <ref> [8, 11, 55] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61].
Reference: [12] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions. Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations <ref> [12, 38] </ref> can be used for optimization, as demonstrated in projects like CIP [8, 11, 55]. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations.
Reference: [13] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LFP, </booktitle> <pages> pages 11-20, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion <ref> [3, 5, 13, 29, 66] </ref>, deforestation [65], and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [14] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations <ref> [4, 14, 31, 32] </ref>, finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century [30]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [4, 14, 31, 32, 64] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54].
Reference: [15] <author> C. Consel and F. Noel. </author> <title> A general approach for run-time specialization and its application to C. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35, 40], run-time specialization and code generation <ref> [15, 41, 42] </ref>, and dynamic compilation and code generation [6, 18], have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [16] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <booktitle> Prentice-Hall Series in Automatic Computation. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [16, 33, 34, 61] </ref>. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language [43, 45, 46, 47]. The idea has been used in optimizing imperative programs that do not use arrays [44].
Reference: [17] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [18] <author> D. R. Engler. </author> <title> VCODE: A retragetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on PLDI, </booktitle> <pages> pages 160-170, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Specialization techniques, such as data specialization [35, 40], run-time specialization and code generation [15, 41, 42], and dynamic compilation and code generation <ref> [6, 18] </ref>, have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [19] <author> M. D. Ernst. </author> <title> Serializing parallel programs by removing redundant computation. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> August </month> <year> 1992, </year> <note> Revised August 1994. </note>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs <ref> [10, 19] </ref>, etc. For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example [71, 73]. <p> In particular, we have used tools developed by Pugh's group [57, 58, 59, 60]. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs <ref> [10, 19] </ref>. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods. Besides achieving optimizations not previously possible, our techniques fall out of one general approach, rather than simply being yet another new but ad hoc method.
Reference: [20] <author> P. Feautrier. </author> <title> Parametric integer programming. </title> <journal> Operationnelle/Operations Research, </journal> <volume> 22(3) </volume> <pages> 243-268, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [21] <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1), </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [22] <author> A. L. Fisher and P. T. Highnam. </author> <title> Communication and code optimization in simd programs. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73]. <p> The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [22, 23, 37] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputations. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [23] <author> A. L. Fisher, J. Leon, and P. T. Highnam. </author> <title> Design and performance of an optimizing simd compiler. </title> <booktitle> In Frontiers of Massively Parallel Computation, </booktitle> <year> 1990. </year>
Reference-contexts: The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [22, 23, 37] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputations. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [24] <author> A. C. Fong. </author> <title> Inductively computable constructs in very high level languages. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on POPL, </booktitle> <pages> pages 21-28, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [24, 25, 53, 54] </ref>. The speedup obtained from incrementalizing aggregate computations can be enormous compared to what is offered by previous compiler optimizations. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [25] <author> A. C. Fong and J. D. Ullman. </author> <title> Inductive variables in very high level languages. </title> <booktitle> In Conference Record of the 3rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 104-112, </pages> <address> Atlanta, Georgia, </address> <month> January </month> <year> 1976. </year>
Reference-contexts: This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [24, 25, 53, 54] </ref>. The speedup obtained from incrementalizing aggregate computations can be enormous compared to what is offered by previous compiler optimizations. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [26] <author> O. I. Franksen. </author> <title> Mr. Babbage's Secret : The Tale of a Cypher and APL. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1985. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations <ref> [26, 36, 49] </ref>, loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [26, 36, 49, 70] </ref>. The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions [67, 68].
Reference: [27] <author> V. K. Garg. </author> <booktitle> Principles of Distributed Systems. </booktitle> <publisher> Kluwer, </publisher> <year> 1996. </year>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection <ref> [27, 28] </ref>, serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example [71, 73].
Reference: [28] <author> V. K. Garg and J. R. Mitchell. </author> <title> An efficient algorithm for detecting conjunctions of general global predicates. </title> <type> Technical Report TR-PDS-1996-005, </type> <institution> University of Texas at Austin, </institution> <year> 1996. </year>
Reference-contexts: There are many applications where programs can be written easily and clearly using arrays but with a great deal of overlapping aggregate array redundancy. These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection <ref> [27, 28] </ref>, serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example [71, 73].
Reference: [29] <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LFP, </booktitle> <pages> pages 53-62, </pages> <month> August </month> <year> 1984. </year> <month> 18 </month>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion <ref> [3, 5, 29, 66] </ref>, pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion <ref> [3, 5, 13, 29, 66] </ref>, deforestation [65], and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [30] <author> H. H. Goldstine. </author> <title> Charles Babbage and his analytical engine. In The Computer from Pascal to von Neumann, </title> <booktitle> chapter 2, </booktitle> <pages> pages 10-26. </pages> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1972. </year>
Reference-contexts: For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century <ref> [30] </ref>. Strength reduction is the first realization of this idea in optimizing compilers [4, 14, 31, 32, 64]. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations.
Reference: [31] <author> A. A. Grau, U. Hill, and H. Langmaac. </author> <title> Translation of ALGOL 60, volume 1 of Handbook for automatic computation. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1967. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations <ref> [4, 14, 31, 32] </ref>, finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century [30]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [4, 14, 31, 32, 64] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54].
Reference: [32] <author> D. Gries. </author> <title> Compiler Construction for Digital Computers. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations <ref> [4, 14, 31, 32] </ref>, finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century [30]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [4, 14, 31, 32, 64] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54].
Reference: [33] <editor> D. Gries. </editor> <booktitle> The Science of Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [16, 33, 34, 61] </ref>. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language [43, 45, 46, 47]. The idea has been used in optimizing imperative programs that do not use arrays [44].
Reference: [34] <author> D. Gries. </author> <title> A note on a standard strategy for developing loop invariants and loops. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 2 </volume> <pages> 207-214, </pages> <year> 1984. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [16, 33, 34, 61] </ref>. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language [43, 45, 46, 47]. The idea has been used in optimizing imperative programs that do not use arrays [44].
Reference: [35] <author> B. Guenter, T. B. Knoblock, and E. Ruf. Specializing shaders. </author> <booktitle> In Proceedings of ACM SIGGRAPH '95 (Computer Graphics Proceedings, Annual Conference Series), </booktitle> <pages> pages 343-349, </pages> <year> 1996. </year>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization <ref> [35, 40] </ref>, run-time specialization and code generation [15, 41, 42], and dynamic compilation and code generation [6, 18], have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [36] <author> L. Guibas and K. Wyatt. </author> <title> Compilation and delayed evaluation in APL. </title> <booktitle> In Conference Record of the 5th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-8, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations <ref> [26, 36, 49] </ref>, loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [26, 36, 49, 70] </ref>. The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions [67, 68].
Reference: [37] <author> P. T. Highnam. </author> <title> Systems and Programming Issues in the Design and Use of a SIMD Linear Array for Image Processing. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CMU-CS-91-136. </note>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73]. <p> The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains. Directionals are unary operations, such as LEFT and UP, invented by Fisher and Highnam <ref> [22, 23, 37] </ref>, to describe computations involving small numbers of neighboring nodes on grid structures. Such computations are optimized by directional rule-based transformations and common subexpression elimination, which essentially eliminate overlapping subcomputations. Their experiments show that the Cray Fortran compiler cannot perform these optimizations.
Reference: [38] <author> S. Katz. </author> <title> Program optimization using invariants. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-4(5):378-389, </volume> <month> November </month> <year> 1978. </year>
Reference-contexts: These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions. Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations <ref> [12, 38] </ref> can be used for optimization, as demonstrated in projects like CIP [8, 11, 55]. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations.
Reference: [39] <author> W. Kelly and W. Pugh. </author> <title> Finding legal reordering transformations using mappings. </title> <booktitle> In Proceedings of the 7th Annual Workshop on Programming Languages and Compilers for Parallel Computing, volume 892 of Lecture Notes in Computer Science, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering <ref> [7, 39, 50, 56, 62] </ref>, but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] and source-to-source transformation <ref> [7, 39, 50, 56, 62] </ref> that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. Section 4 describes how to maintain additional information to facilitate incrementalization. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [7, 39, 50, 56, 62] </ref>, pipelining [2], and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [40] <author> T. B. Knoblock and E. Ruf. </author> <title> Data specialization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on PLDI, </booktitle> <address> Philadelphia, Pennsylvania, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization <ref> [35, 40] </ref>, run-time specialization and code generation [15, 41, 42], and dynamic compilation and code generation [6, 18], have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [41] <author> M. Leone and P. Lee. </author> <title> A declarative approach to run-time code generation. </title> <booktitle> In Workshop on Compiler Support for System Software (WCSSS), </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35, 40], run-time specialization and code generation <ref> [15, 41, 42] </ref>, and dynamic compilation and code generation [6, 18], have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [42] <author> M. Leone and P. Lee. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on PLDI, </booktitle> <pages> pages 137-148, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can. Specialization techniques, such as data specialization [35, 40], run-time specialization and code generation <ref> [15, 41, 42] </ref>, and dynamic compilation and code generation [6, 18], have been used in 16 program optimizations and achieved certain large speedups. These optimizations allow subcompu--tations repeated on fixed dynamic values to be computed once and reused in loops or recursions.
Reference: [43] <author> Y. A. Liu. </author> <title> Incremental Computation: A Semantics-Based Systematic Transformational Approach. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays. <p> Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization [47], cache-and-prune [46], and use of auxiliary information [45] were first formulated for a functional language <ref> [43] </ref>. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [44]. This paper extends that work to handle imperative programs that use arrays. <p> It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language <ref> [43, 45, 46] </ref>. Here we apply them to AACs, using a variant of the cache-and-prune method [46]. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61]. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language <ref> [43, 45, 46, 47] </ref>. The idea has been used in optimizing imperative programs that do not use arrays [44]. The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains.
Reference: [44] <author> Y. A. Liu. </author> <title> Principled strength reduction. </title> <booktitle> In Proceedings of the IFIP TC2 Working Conference on Algorithmic Languages and Calculi, </booktitle> <address> Le Bischenberg, France, February 1997. </address> <publisher> Chapman & Hall. </publisher>
Reference-contexts: Methods of explicit incrementalization [47], cache-and-prune [46], and use of auxiliary information [45] were first formulated for a functional language [43]. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction <ref> [44] </ref>. This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL [24, 25, 53, 54]. <p> l := 0 to n 2 1 do s [0] := s [0] + s 1 [k]; 6 s 1 [i1+m] := 0; s 1 [i1+m] := s 1 [i1+m] + a [i1+m; l]; (16) Further optimizations to the resulting loop may be enabled by the incrementalization, as described in <ref> [44] </ref>; these optimizations include folding initialization, replacing termination test, and minimizing information maintained in the loop. <p> Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language [43, 45, 46, 47]. The idea has been used in optimizing imperative programs that do not use arrays <ref> [44] </ref>. The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains.
Reference: [45] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 157-170, </pages> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays. <p> Each of these components is relatively simple, and the overall optimization algorithm is modular. Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization [47], cache-and-prune [46], and use of auxiliary information <ref> [45] </ref> were first formulated for a functional language [43]. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [44]. This paper extends that work to handle imperative programs that use arrays. <p> Such results, if not stored already, and, often, additional information need to be maintained for efficient incremental computation <ref> [45, 46] </ref>. Such information often comes from intermediate results computed in the middle of the original computation [46]. It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. <p> Such information often comes from intermediate results computed in the middle of the original computation [46]. It may also come from auxiliary information that is not computed at all in the original computation <ref> [45] </ref>. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [43, 45, 46]. Here we apply them to AACs, using a variant of the cache-and-prune method [46]. <p> It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language <ref> [43, 45, 46] </ref>. Here we apply them to AACs, using a variant of the cache-and-prune method [46]. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61]. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language <ref> [43, 45, 46, 47] </ref>. The idea has been used in optimizing imperative programs that do not use arrays [44]. The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains.
Reference: [46] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays. <p> Each of these components is relatively simple, and the overall optimization algorithm is modular. Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization [47], cache-and-prune <ref> [46] </ref>, and use of auxiliary information [45] were first formulated for a functional language [43]. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [44]. This paper extends that work to handle imperative programs that use arrays. <p> Such results, if not stored already, and, often, additional information need to be maintained for efficient incremental computation <ref> [45, 46] </ref>. Such information often comes from intermediate results computed in the middle of the original computation [46]. It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. <p> Such results, if not stored already, and, often, additional information need to be maintained for efficient incremental computation [45, 46]. Such information often comes from intermediate results computed in the middle of the original computation <ref> [46] </ref>. It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [43, 45, 46]. <p> It may also come from auxiliary information that is not computed at all in the original computation [45]. The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language <ref> [43, 45, 46] </ref>. Here we apply them to AACs, using a variant of the cache-and-prune method [46]. <p> The central issues are how to find, use, and maintain appropriate information. General methods have been proposed and formulated for a functional language [43, 45, 46]. Here we apply them to AACs, using a variant of the cache-and-prune method <ref> [46] </ref>. <p> The analysis starts with the uses of such information in computing the original accumulating variables and follows dependencies back to the definitions of such information. The dependencies are transitive <ref> [46] </ref> and can be used to compute all the information that is useful. Pruning then eliminates useless information, saving both space and time. Pruning of unused intermediate results presents no special difficulties. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61]. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language <ref> [43, 45, 46, 47] </ref>. The idea has been used in optimizing imperative programs that do not use arrays [44]. The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains.
Reference: [47] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays. <p> Each of these components is relatively simple, and the overall optimization algorithm is modular. Both analytical and experimental results show drastic speedups that are not achievable by previous compiler optimizations. Methods of explicit incrementalization <ref> [47] </ref>, cache-and-prune [46], and use of auxiliary information [45] were first formulated for a functional language [43]. They have been adopted for loop incrementalization of imperative programs with no arrays, generalizing traditional strength reduction [44]. This paper extends that work to handle imperative programs that use arrays. <p> Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61]. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language <ref> [43, 45, 46, 47] </ref>. The idea has been used in optimizing imperative programs that do not use arrays [44]. The optimizations for arrays described here greatly extend the scope of that work, since arrays are widely used in so many application domains.
Reference: [48] <author> V. Maslov. </author> <title> Lazy array data-flow dependence analysis. </title> <booktitle> In Conference Record of the 21th Annual ACM Symposium on POPL, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [49] <author> J. A. Mason. </author> <title> Learning APL : an array processing language. </title> <publisher> Harper & Row, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations <ref> [26, 36, 49] </ref>, loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [26, 36, 49, 70] </ref>. The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions [67, 68].
Reference: [50] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering <ref> [7, 39, 50, 56, 62] </ref>, but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] and source-to-source transformation <ref> [7, 39, 50, 56, 62] </ref> that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. Section 4 describes how to maintain additional information to facilitate incrementalization. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [7, 39, 50, 56, 62] </ref>, pipelining [2], and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another.
Reference: [51] <author> B. Paige and J. T. Schwartz. </author> <title> Expression continuity and the formal differentiation of algorithms. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 58-71, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations <ref> [51, 52, 54, 53, 72] </ref>, and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [52] <author> R. Paige. </author> <title> Transformational programming|Applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations <ref> [51, 52, 54, 53, 72] </ref>, and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [53] <author> R. Paige. </author> <title> Symbolic finite differencing|Part I. </title> <booktitle> In Proceedings of the 3rd ESOP, volume 432 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36-56, </pages> <address> Copenhagen, Denmark, May 1990. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations <ref> [51, 52, 54, 53, 72] </ref>, and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [24, 25, 53, 54] </ref>. The speedup obtained from incrementalizing aggregate computations can be enormous compared to what is offered by previous compiler optimizations.
Reference: [54] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations <ref> [51, 52, 54, 53, 72] </ref>, and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> This paper extends that work to handle imperative programs that use arrays. It presents a broad generalization of strength reduction from arithmetics to aggregates in common high-level languages, such as FORTRAN, rather than to aggregates in special very-high-level languages, such as SETL <ref> [24, 25, 53, 54] </ref>. The speedup obtained from incrementalizing aggregate computations can be enormous compared to what is offered by previous compiler optimizations. <p> The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL <ref> [17, 24, 25, 51, 52, 54] </ref>. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC [72], which allows programs to be written using operations on bags, rather than sets.
Reference: [55] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Our optimization exploits subcomputations whose values can be efficiently updated, in addition to directly reused, from one iteration to the next. Thus, it allows far more speedup. General program transformations [12, 38] can be used for optimization, as demonstrated in projects like CIP <ref> [8, 11, 55] </ref>. In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants [16, 33, 34, 61].
Reference: [56] <author> W. Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering <ref> [7, 39, 50, 56, 62] </ref>, but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] and source-to-source transformation <ref> [7, 39, 50, 56, 62] </ref> that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. Section 4 describes how to maintain additional information to facilitate incrementalization. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [7, 39, 50, 56, 62] </ref>, pipelining [2], and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [57] <author> W. Pugh. </author> <title> The Omega Test: A fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 31(8), </volume> <month> August </month> <year> 1992. </year> <month> 19 </month>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> In particular, the methods and tools developed by Pugh et al. in the Omega project <ref> [57, 58, 59, 60] </ref> have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Simplify the constraints in S using the techniques studied by Pugh et al. <ref> [57, 58, 59, 60] </ref>. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [57, 58, 59, 60] </ref>. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs [10, 19]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [58] <author> W. Pugh and D. Wonnacott. </author> <title> Going beyond integer proramming with the omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> December </month> <year> 1992. </year> <note> An earlier version of this paper appeared at the ACM SIGPLAN '92 Conference on PLDI. </note>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> In particular, the methods and tools developed by Pugh et al. in the Omega project <ref> [57, 58, 59, 60] </ref> have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Simplify the constraints in S using the techniques studied by Pugh et al. <ref> [57, 58, 59, 60] </ref>. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [57, 58, 59, 60] </ref>. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs [10, 19]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [59] <author> W. Pugh and D. Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Proceedings of the 6th Annual Workshop on Programming Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> In particular, the methods and tools developed by Pugh et al. in the Omega project <ref> [57, 58, 59, 60] </ref> have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Simplify the constraints in S using the techniques studied by Pugh et al. <ref> [57, 58, 59, 60] </ref>. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [57, 58, 59, 60] </ref>. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs [10, 19]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [60] <author> W. Pugh and D. Wonnacott. </author> <title> Nonlinear array dependence analysis. </title> <type> Technical Report CS-TR-3372, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> and source-to-source transformation [7, 39, 50, 56, 62] that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. <p> l := 0 to n 2 1 do (7) and its contributing set is S (A i To compute the difference of two sets represented in the form (5), we formulate the difference as a single set of constraints and then use well-studied methods developed for array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> to simplify those constraints. In particular, the methods and tools developed by Pugh et al. in the Omega project [57, 58, 59, 60] have been used to produce the desired output. This approach is embodied in the following algorithm. <p> In particular, the methods and tools developed by Pugh et al. in the Omega project <ref> [57, 58, 59, 60] </ref> have been used to produce the desired output. This approach is embodied in the following algorithm. <p> Simplify the constraints in S using the techniques studied by Pugh et al. <ref> [57, 58, 59, 60] </ref>. <p> Finally, they do not give general methods for handling grid margins. Loop reordering [7, 39, 50, 56, 62], pipelining [2], and array data dependence analysis <ref> [20, 21, 48, 50, 57, 58, 59, 60] </ref> have been studied extensively for optimizing|in particular, parallelizing|array computations. While they aim to determine dependencies among uses of array elements, we further seek to determine exactly how subcomputations differ from one another. <p> We reduce our analysis problem to symbolic simplification of constraints on loop variables and array subscripts, so methods and techniques developed for such simplifications for parallelizing compilers can be exploited. In particular, we have used tools developed by Pugh's group <ref> [57, 58, 59, 60] </ref>. Interestingly, ideas of incrementalization are used for optimizations in serializing parallel programs [10, 19]. In conclusion, this work describes a method and algorithms that allow more drastic optimizations of aggregate array computations than previous methods.
Reference: [61] <author> J. C. Reynolds. </author> <title> The Craft of Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: In contrast to such manual or semi-automatic approaches, our optimization of aggregate array computations can be automated and requires no user intervention or annotations. Our method for maintaining additional information is an automatic method for strengthening loop invariants <ref> [16, 33, 34, 61] </ref>. Our optimizations are based on the idea of explicit incremental computation, for which a general systematic transformational approach has been studied and formulated for a functional language [43, 45, 46, 47]. The idea has been used in optimizing imperative programs that do not use arrays [44].
Reference: [62] <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion [3, 5, 29, 66], pipelining [2], and loop reordering <ref> [7, 39, 50, 56, 62] </ref>, but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> In the latter case, resource requirements and communication costs are substantially reduced. Additionally, for this powerful optimization, we make use of techniques and tools for array dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] and source-to-source transformation <ref> [7, 39, 50, 56, 62] </ref> that were developed for parallelizing compilers. This paper is organized as follows. Section 2 gives the programming language. Sections 3 describes how to identify and incrementalize aggregate array computations and form incrementalized loops. Section 4 describes how to maintain additional information to facilitate incrementalization. <p> Also, programs must be written using directionals to take advantage of their optimizations; furthermore, it is inconvenient to write when more than a few neighbors are involved. Finally, they do not give general methods for handling grid margins. Loop reordering <ref> [7, 39, 50, 56, 62] </ref>, pipelining [2], and array data dependence analysis [20, 21, 48, 50, 57, 58, 59, 60] have been studied extensively for optimizing|in particular, parallelizing|array computations.
Reference: [63] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations [51, 52, 54, 53, 72], and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions <ref> [9, 43, 45, 46, 47, 63] </ref>, but no systematic technique handles aggregate computations on arrays.
Reference: [64] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 4th International Joint Conference on TAPSOFT, volume 494 of Lecture Notes in Computer Science, </booktitle> <pages> pages 394-415. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: For the graph on the right, n = 1000. 7 Related work and conclusion The basic idea of incrementalization is at least as old as Babbage's difference machine in the nineteenth century [30]. Strength reduction is the first realization of this idea in optimizing compilers <ref> [4, 14, 31, 32, 64] </ref>. The idea is to compute certain multiplications in loops incrementally using additions. Our work extends traditional strength reduction from arithmetic operations to aggregate array computations. Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54].
Reference: [65] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <booktitle> In Proceedings of the 2nd ESOP, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 344-358, </pages> <address> Nancy, France, March 1988. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation <ref> [65] </ref>, and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [66] <author> J. Warren. </author> <title> A hierarchical basis for reordering transformations. </title> <booktitle> In Conference Record of the 11th Annual ACM Symposium on POPL, </booktitle> <pages> pages 272-282, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: At the same time, many optimizations have been studied for arrays, such as various APL compiler optimizations [26, 36, 49], loop fusion <ref> [3, 5, 29, 66] </ref>, pipelining [2], and loop reordering [7, 39, 50, 56, 62], but none of them achieves incrementalization. This paper presents a method and algorithms for incrementalizing aggregate array computations. <p> APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations [26, 36, 49, 70]. The same basic idea underlies techniques such as fusion <ref> [3, 5, 13, 29, 66] </ref>, deforestation [65], and transformation of series expressions [67, 68]. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [67] <author> R. Waters. </author> <title> Efficient interpretation of synchronizable series expressions. </title> <booktitle> In Proceedings of the SIGPLAN '87 Symposium on Interpreters and Interpretive Techniques, </booktitle> <pages> pages 74-85, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions <ref> [67, 68] </ref>. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [68] <author> R. C. Waters. </author> <title> Automatic transformation of series expressions into loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 52-98, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions <ref> [67, 68] </ref>. These optimizations do not aim to compute each piece of the aggregate operations incrementally using previous pieces and thus cannot produce as much speedup as our method can.
Reference: [69] <author> J. Webb. </author> <title> Steps towards architecture-independent image processing. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73].
Reference: [70] <author> B. Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: APL compilers optimize aggregate array operations by performing computations in a piece-wise and on-demand fashion, avoiding unnecessary storage of large intermediate results in sequences of operations <ref> [26, 36, 49, 70] </ref>. The same basic idea underlies techniques such as fusion [3, 5, 13, 29, 66], deforestation [65], and transformation of series expressions [67, 68].
Reference: [71] <author> W. M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73]. <p> For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example <ref> [71, 73] </ref>. Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking.
Reference: [72] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Optimizations similar to incremen-talization have been studied for various language features, such as strength reduction of arithmetic operations [4, 14, 31, 32], finite differencing of set and bag operations <ref> [51, 52, 54, 53, 72] </ref>, and promotion-and-accumulation, finite differencing, and incrementalization of recursive functions [9, 43, 45, 46, 47, 63], but no systematic technique handles aggregate computations on arrays. <p> Finite differencing generalizes strength reduction to handle set operations in very-high-level languages like SETL [17, 24, 25, 51, 52, 54]. The idea is to replace aggregate operations on sets with incremental operations. Similar ideas are also used in the language INC <ref> [72] </ref>, which allows programs to be written using operations on bags, rather than sets. Our work exploits the semantics underlying finite differencing to handle aggregate computations on arrays, which are more common in high-level languages and are more convenient for expressing many application problems.
Reference: [73] <author> R. Zabih. </author> <title> Individuating Unknown Objects by Combining Motion and Stereo. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73]. <p> For example, in image processing, computing information about local neighborhoods is common [22, 37, 69, 71, 73, 74]. The local summation problem above is a simple but typical example <ref> [71, 73] </ref>. Overlapping aggregate array redundancy can cause severe performance degradation, especially with the increasingly large data sets that many applications are facing, yet methods for eliminating overlapping aggregate array redundancy have been lacking.
Reference: [74] <author> R. Zabih and J. Woodfill. </author> <title> Non-parametric local transforms for computing visual correspondence. </title> <editor> In J.-O. Eklundh, editor, </editor> <booktitle> 3rd European Conference on Computer Vision, volume 801 of Lecture Notes in Computer Science, </booktitle> <pages> pages 151-158. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 20 </month>
Reference-contexts: These include problems in image processing, computational geometry, computer graphics, multimedia, matrix computation, list processing, graph algorithms, distributed property detection [27, 28], serializing parallel programs [10, 19], etc. For example, in image processing, computing information about local neighborhoods is common <ref> [22, 37, 69, 71, 73, 74] </ref>. The local summation problem above is a simple but typical example [71, 73].
References-found: 74

