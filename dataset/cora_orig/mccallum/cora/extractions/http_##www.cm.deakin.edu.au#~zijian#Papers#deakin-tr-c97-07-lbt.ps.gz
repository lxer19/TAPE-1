URL: http://www.cm.deakin.edu.au/~zijian/Papers/deakin-tr-c97-07-lbt.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: (Email: fzijian,webbg@deakin.edu.au)  
Title: Lazy Bayesian Trees  
Author: Zijian Zheng and Geoffrey I. Webb 
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C97/07)  
Abstract: The naive Bayesian classifier is simple and effective, but its attribute independence assumption is often violated in the real world. A number of approaches have been developed that seek to alleviate this problem. A Bayesian tree learning algorithm builds a decision tree, and generates a local Bayesian classifier at each leaf instead of predicting a single class. However, Bayesian tree learning still suffers from the replication and fragmentation problems of tree learning. While inferred Bayesian trees demonstrate low average prediction error rates, there is reason to believe that error rates will be higher for those leaves with few training examples. This paper proposes a novel lazy Bayesian tree learning algorithm. For each test example, it conceptually builds a most appropriate Bayesian tree. In practice, only one path with a local Bayesian classifier at its leaf is created. Experiments with a wide variety of real-world and artificial domains show that this new algorithm has significantly lower overall prediction error rates than a naive Bayesian classifier, C4.5, and a Bayesian tree learning algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [ Aha, 1997 ] <author> D.W. Aha (ed.). </author> <note> Artificial Intelligence Review, Special issue on lazy 14 learning. To appear in 1997. </note>
Reference-contexts: Motivated by this work [ Kohavi, 1996 ] , we propose, in this paper, a new hybrid learning algorithm called LBT (Lazy Bayesian Trees) to further improve the prediction accuracy of the naive Bayesian classifier and decision tree learning algorithms. The term "lazy" comes from lazy learning <ref> [ Aha, 1997 ] </ref> such as instance-based learning. This type of learning algorithm differs from others in that computation is delayed to the classification stage and no concise theories such as decision trees are created. For each test example, LBT builds a decision path specially for it. <p> The term "lazy" came from lazy learning algorithms <ref> [ Aha, 1997 ] </ref> including instance based learning algorithms. LBT can be considered as a combination of two techniques NBTree and LazyDT [ Friedman et al., 1996 ] , although the development of LBT is independent from LazyDT. With caching, LazyDT is reasonably fast.
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification And Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Decision tree learning <ref> [ Quinlan, 1993; Breiman et al., 1984 ] </ref> is a well developed classification learning technique. However, univariate tree learning algorithms suffer from the replication and fragmentation problems [ Pagallo and Haussler, 1990 ] . Constructive induction has been shown to be able to alleviate these problems. <p> These twenty-nine domains cover a wide variety of different domains and all are available 7 from the UCI machine learning repository [ Murphy and Aha, 1996 ] . In each domain, two stratified 10-fold cross-validations <ref> [ Breiman et al., 1984; Kohavi, 1995 ] </ref> are carried out for each algorithm except LazyDT. All the algorithms are run with their default option settings on the same training and test set partitions in every domain.
Reference: [ Cestnik, 1990 ] <author> B. Cestnik. </author> <title> Estimating probabilities: a crucial task in machine learning. </title> <booktitle> In Proceedings of the European Conference on Artificial Intelligence, </booktitle> <pages> pages 147-149, </pages> <year> 1990. </year>
Reference-contexts: Their brief descriptions are as follows. NB is our implementation of the naive Bayesian classifier. Its working principle is as described at the beginning of this paper. When probabilities are estimated from a training set, the m-estimate with m = 2 and the Laplace estimate <ref> [ Cestnik, 1990 ] </ref> are used. It is worth mentioning that NB is used by LBT and BT for local naive Bayesian classifiers. C4.5 [ Quinlan, 1993 ] is a state-of-the-art decision tree learning algorithm. The results of C4.5 reported here are for pruned trees.
Reference: [ Domingos and Pazzani, 1996 ] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: conditions for the optimality of the simple Bayesian classifier. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 105-112, </pages> <address> Bari, Italy, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: Therefore, the following equality can be used: P (V jC i ) = Q It has been shown that the naive Bayesian classifier performs surprisingly well in terms of prediction accuracy compared with other learning algorithms such as decision tree learning, rule learning, and instance-based learning algorithms in many domains <ref> [ Domingos and Pazzani, 1996 ] </ref> . The naive Bayesian classifier considers evidence from many attributes to classify examples. This is important in a situation where many attributes affect the classification, but there is no main effect.
Reference: [ Duda and Hart, 1973 ] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The theory can be used to predict the classes of unseen examples. Bayes' theorem provides an optimal way to predict the class of an unseen instance based on a training set <ref> [ Duda and Hart, 1973 ] </ref> . The predicted class is the one with the highest probability of C i given the instance V : P (C i jV ) = P (C i )P (V jC i )=P (V ).
Reference: [ Fayyad and Irani, 1993 ] <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year> <booktitle> International Joint Committee on Artificial Intelligence. </booktitle>
Reference-contexts: An error rate reported in the following subsection is an average of the 20 trials for an algorithm. Since some of these algorithms can only deal with nominal attributes, continuous-valued attributes are discretized as a pre-process in the experiments. An entropy-based discretization method <ref> [ Fayyad and Irani, 1993; Ting, 1994 ] </ref> is used. For each pair of training set and test set, the test set is discretized by using cut points found from the training set. 3.3 Experimental Results Table 1 summarizes test error rates of NB, C4.5, BT, LazyDT, and LBT.
Reference: [ Friedman et al., 1996 ] <author> J. Friedman, R. Kohavi, and Y. Yun. </author> <title> Lazy decision trees. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <pages> pages 717-724, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: LazyDT, a lazy decision tree learning algorithm <ref> [ Friedman et al., 1996 ] </ref> , was proposed to improve the performance of conventional decision tree learning by building one "best" path for each test example. In this sense, it is very similar to LBT, but at the leaf of a path, LazyDT uses a single class label. <p> In this sense, it is very similar to LBT, but at the leaf of a path, LazyDT uses a single class label. Since the LazyDT system is not available to us at the moment, the results of LazyDT reported in this paper are from the publication <ref> [ Friedman et al., 1996 ] </ref> . Note that these results are not directly comparable with those of other algorithms, since different experimental methods are used. <p> The term "lazy" came from lazy learning algorithms [ Aha, 1997 ] including instance based learning algorithms. LBT can be considered as a combination of two techniques NBTree and LazyDT <ref> [ Friedman et al., 1996 ] </ref> , although the development of LBT is independent from LazyDT. With caching, LazyDT is reasonably fast. This suggests that there a room for improvement to the efficiency of LBT.
Reference: [ Kohavi, 1995 ] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1137-1143, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year> <booktitle> International Joint Committee on Artificial Intelligence. </booktitle>
Reference-contexts: These twenty-nine domains cover a wide variety of different domains and all are available 7 from the UCI machine learning repository [ Murphy and Aha, 1996 ] . In each domain, two stratified 10-fold cross-validations <ref> [ Breiman et al., 1984; Kohavi, 1995 ] </ref> are carried out for each algorithm except LazyDT. All the algorithms are run with their default option settings on the same training and test set partitions in every domain.
Reference: [ Kohavi, 1996 ] <author> R. Kohavi. </author> <title> Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid. </title> <booktitle> In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 202-207, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year> <journal> American Association for Artificial Intelligence. </journal> <volume> 15 </volume>
Reference-contexts: This problem degrades the performance of NBTree in some domains. Motivated by this work <ref> [ Kohavi, 1996 ] </ref> , we propose, in this paper, a new hybrid learning algorithm called LBT (Lazy Bayesian Trees) to further improve the prediction accuracy of the naive Bayesian classifier and decision tree learning algorithms. <p> across all possible domains in the whole universe [ Wolpert, 1994; Schaffer, 1994 ] , the experiments suggest that LBT is significantly 12 superior to NB, C4.5, and BT for the type of problem to which machine learning is applied in practice. 4 Related Work LBT was motivated by NBTree <ref> [ Kohavi, 1996 ] </ref> , a Bayesian tree learning algorithm. The term "lazy" came from lazy learning algorithms [ Aha, 1997 ] including instance based learning algorithms.
Reference: [ Kononenko, 1990 ] <author> I. Kononenko. </author> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga et al. (eds.), </editor> <title> Current Trends in Knowledge Acquisition, </title> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: The prior probabilities and conditional probability at the right hand side of this equation are usually estimated from a training set. For ease of estimation, a well-known learning method called naive Bayesian classification <ref> [ Kononenko, 1990; Langley and Sage, 1994 ] </ref> assumes that all attributes are mutually independent within each class.
Reference: [ Kononenko, 1991 ] <author> I. Kononenko. </author> <title> Semi-naive Bayesian classifier. </title> <booktitle> In Proceedings of European Conference on Artificial Intelligence, </booktitle> <pages> pages 206-219, </pages> <year> 1991. </year>
Reference-contexts: This is important in a situation where many attributes affect the classification, but there is no main effect. However, when the strong attribute independence assumption is violated, which is very common, the performance of the naive Bayesian classifier might be poor. A few techniques <ref> [ Kononenko, 1991; Langley and Sage, 1994; Pazzani, 1996 ] </ref> have been developed to improve the performance of the naive Bayesian classifier. <p> Some other methods have been explored also for qualifying the attribute independence assumption. For example, the "semi-naive Bayesian classifier" <ref> [ Kononenko, 1991 ] </ref> performs exhaustive search to iteratively join pairs of attribute values. The aim is to optimize the tradeoff between the "non-naivety" and the reliability of estimates of probabilities.
Reference: [ Langley and Sage, 1994 ] <author> P. Langley and S. Sage. </author> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 339-406, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year> <booktitle> Association for Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: The prior probabilities and conditional probability at the right hand side of this equation are usually estimated from a training set. For ease of estimation, a well-known learning method called naive Bayesian classification <ref> [ Kononenko, 1990; Langley and Sage, 1994 ] </ref> assumes that all attributes are mutually independent within each class. <p> This is important in a situation where many attributes affect the classification, but there is no main effect. However, when the strong attribute independence assumption is violated, which is very common, the performance of the naive Bayesian classifier might be poor. A few techniques <ref> [ Kononenko, 1991; Langley and Sage, 1994; Pazzani, 1996 ] </ref> have been developed to improve the performance of the naive Bayesian classifier.
Reference: [ Murphy and Aha, 1996 ] <author> P.M. Murphy and D.W. Aha. </author> <title> The UCI repository of machine learning databases. </title> <address> http://www.ics.uci.edu/~mlearn/MLRepository.html. </address>
Reference-contexts: We want to evaluate whether LBT can really increase the prediction accuracy of a naive Bayesian classifier in this type of domain. These twenty-nine domains cover a wide variety of different domains and all are available 7 from the UCI machine learning repository <ref> [ Murphy and Aha, 1996 ] </ref> . In each domain, two stratified 10-fold cross-validations [ Breiman et al., 1984; Kohavi, 1995 ] are carried out for each algorithm except LazyDT.
Reference: [ Pagallo and Haussler, 1990 ] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-100. </pages>
Reference-contexts: Decision tree learning [ Quinlan, 1993; Breiman et al., 1984 ] is a well developed classification learning technique. However, univariate tree learning algorithms suffer from the replication and fragmentation problems <ref> [ Pagallo and Haussler, 1990 ] </ref> . Constructive induction has been shown to be able to alleviate these problems. Kohavi [1996] proposes a hybrid approach to improve the performance of the naive Bayesian classifier and decision tree learning. The algorithm is called NBTree. 2 It generates Bayesian trees.
Reference: [ Pazzani, 1996 ] <author> M.J. Pazzani. </author> <title> Constructive induction of Cartesian product attributes. </title> <booktitle> In Proceedings of the Conference, ISIS'96: Information, Statistics and Induction in Science, </booktitle> <pages> pages 66-77, </pages> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: This is important in a situation where many attributes affect the classification, but there is no main effect. However, when the strong attribute independence assumption is violated, which is very common, the performance of the naive Bayesian classifier might be poor. A few techniques <ref> [ Kononenko, 1991; Langley and Sage, 1994; Pazzani, 1996 ] </ref> have been developed to improve the performance of the naive Bayesian classifier. <p> For example, the "semi-naive Bayesian classifier" [ Kononenko, 1991 ] performs exhaustive search to iteratively join pairs of attribute values. The aim is to optimize the tradeoff between the "non-naivety" and the reliability of estimates of probabilities. For the same purpose, a constructive Bayesian classifier, Bsej <ref> [ Pazzani, 1996 ] </ref> , adopts a wrapper model with the n-CV estimate to find the best Cartesian product attributes for the naive Bayesian classifier. It also considers deleting attributes.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Decision tree learning <ref> [ Quinlan, 1993; Breiman et al., 1984 ] </ref> is a well developed classification learning technique. However, univariate tree learning algorithms suffer from the replication and fragmentation problems [ Pagallo and Haussler, 1990 ] . Constructive induction has been shown to be able to alleviate these problems. <p> When probabilities are estimated from a training set, the m-estimate with m = 2 and the Laplace estimate [ Cestnik, 1990 ] are used. It is worth mentioning that NB is used by LBT and BT for local naive Bayesian classifiers. C4.5 <ref> [ Quinlan, 1993 ] </ref> is a state-of-the-art decision tree learning algorithm. The results of C4.5 reported here are for pruned trees. BT is a Bayesian tree learning algorithm. It is our implementation of NBTree.
Reference: [ Schaffer, 1994 ] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 259-265, </pages> <address> New Brunswick, NJ, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: If LBT also considers "not equal" tests, it can achieve an error rate of 7.9%, close to that of LazyDT. Although it is known that no algorithm can be superior, in terms of generalization performance, to another one across all possible domains in the whole universe <ref> [ Wolpert, 1994; Schaffer, 1994 ] </ref> , the experiments suggest that LBT is significantly 12 superior to NB, C4.5, and BT for the type of problem to which machine learning is applied in practice. 4 Related Work LBT was motivated by NBTree [ Kohavi, 1996 ] , a Bayesian tree learning
Reference: [ Ting, 1994 ] <author> K.M. Ting. </author> <title> Discretization of continuous-valued attributes and instance-based learning. </title> <type> Technical Report 491, </type> <institution> Basser Department of Computer Science, the University of Sydney, </institution> <year> 1994. </year>
Reference-contexts: An error rate reported in the following subsection is an average of the 20 trials for an algorithm. Since some of these algorithms can only deal with nominal attributes, continuous-valued attributes are discretized as a pre-process in the experiments. An entropy-based discretization method <ref> [ Fayyad and Irani, 1993; Ting, 1994 ] </ref> is used. For each pair of training set and test set, the test set is discretized by using cut points found from the training set. 3.3 Experimental Results Table 1 summarizes test error rates of NB, C4.5, BT, LazyDT, and LBT.
Reference: [ Wolpert, 1994 ] <author> D.H. Wolpert. </author> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <editor> In D.H. Wolpert (ed.), </editor> <title> The Mathematics of Generalization, </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year> <month> 16 </month>
Reference-contexts: If LBT also considers "not equal" tests, it can achieve an error rate of 7.9%, close to that of LazyDT. Although it is known that no algorithm can be superior, in terms of generalization performance, to another one across all possible domains in the whole universe <ref> [ Wolpert, 1994; Schaffer, 1994 ] </ref> , the experiments suggest that LBT is significantly 12 superior to NB, C4.5, and BT for the type of problem to which machine learning is applied in practice. 4 Related Work LBT was motivated by NBTree [ Kohavi, 1996 ] , a Bayesian tree learning
References-found: 19

