URL: http://www.cs.berkeley.edu/~ang/papers/sampmdp.ps
Refering-URL: http://www.cs.berkeley.edu/~ang/
Root-URL: http://www.cs.berkeley.edu/~ang/
Email: mkearns@research.att.com  mansour@research.att.com  ang@cs.berkeley.edu  
Title: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes  
Author: Michael Kearns Yishay Mansour Andrew Y. Ng 
Address: Berkeley  
Affiliation: AT&T Labs  AT&T Labs and Tel-Aviv University  UC  
Abstract: An issue that is critical for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or even infinite state spaces, traditional planning and reinforcement learning algorithms are often inapplicable, since their running time typically scales linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (simulator) for an arbitrary MDP, performs near-optimal planning with a running time that has no dependence on the number of states. Although the running time is exponential in the horizon time (which depends only on the discount factor fl and the desired degree of approximation to the optimal policy), our results establish for the first time that there are no theoretical barriers to computing near-optimal policies in arbitrarily large, unstructured MDPs. 
Abstract-found: 1
Intro-found: 1
Reference: [ AHU74 ] <author> A.V. Aho, J.E. Hopcroft, and J.D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: This result is obtained under the assumption that the input state to A requires only O (1) space, a standard assumption known as the uniform cost model <ref> [ AHU74 ] </ref> , that is typically adopted to allow analysis of algorithms that operate on real numbers (such as we require to allow infinite state spaces).
Reference: [ DNM98 ] <author> Scott Davies, Andrew Y. Ng, and Andrew Moore. </author> <title> Applying online-search to reinforcement learning. </title> <booktitle> In Proceedings of AAAI-98, </booktitle> <pages> pages 753-760. </pages> <publisher> AAAI Press, </publisher> <year> 1998. </year>
Reference-contexts: In our case, this would entail simultaneously increasing C and H by decreasing the target *. Also, as studied in Davies et. al. <ref> [ DNM98 ] </ref> , if we have access to an initial estimate of the value function, we can replace our estimates ^ V fl 0 (s) = 0 at the leaves with the estimated value function at those states.
Reference: [ KS99 ] <author> Michael Kearns and Satinder Singh. </author> <title> Finite-sample convergence rates for Q-learning and indirect algorithms. </title> <booktitle> In Neural Information Processing Systems 12. </booktitle> <publisher> MIT Press, (to appear), </publisher> <year> 1999. </year>
Reference-contexts: The analysis relies on a combination of Bellman equation calculations, which are standard in reinforcement learning, and uniform convergence arguments, which are standard in supervised learning; this combina tion of techniques was first applied in <ref> [ KS99 ] </ref> . As men-tioned, the running time required at each state does have an exponential dependence on the horizon time (which can be shown to be unavoidable without further assumptions).
Reference: [ MHK + 98 ] <author> N. Meuleau, M. Hauskrecht, K-E. Kim, L. Peshkin, L.P. Kaelbling, T. Dean, and C. Boutilier. </author> <title> Solving very large weakly coupled Markov decision processes. </title> <booktitle> In Proceedings of AAAI, </booktitle> <pages> pages 165-172, </pages> <year> 1998. </year>
Reference-contexts: Function approximation [ SB98 ] is a well-studied approach to learning value functions in large state spaces, and many authors have recently begun to study the properties of large MDPs that enjoy compact representations, such as MDPs in which the state transition probabilities factor into a small number of components <ref> [ MHK + 98 ] </ref> . In this paper, we are interested in the problem of computing a near-optimal policy in a large or infinite MDP that is given | that is, we are interested in planning. <p> In this sense, if we view the generative model as providing a "compact" representation of the MDP, our algorithm provides a correspondingly "compact" representation of a near-optimal policy. We view our result as complimentary to work that proposes and exploits particular compact representations of MDPs <ref> [ MHK + 98 ] </ref> , with both lines of work beginning to demonstrate the potential feasibility of planning and learning in very large environments. 2 Preliminaries We begin with the definition of a Markov decision process on a set of N = jSj states, explicitly allowing the possibility of the
Reference: [ RN95 ] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence | A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: In this sense, our algorithm is clearly related to and inspired by classical look-ahead search techniques <ref> [ RN95 ] </ref> our main contribution is in showing that in very large stochastic environments, clever random sampling suffices to reconstruct nearly all of the information available in the (exponentially or infinitely) large full look-ahead tree.
Reference: [ SB98 ] <author> Richard S. Sutton and Andrew G. Barto. </author> <title> Reinforcement Learning. </title> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: The desire to attack problems of increasing complexity with this formalism has recently led researchers to focus particular attention on the case of (exponentially or even infinitely) large state spaces. A number of interesting algorithmic and representational suggestions have been made for coping with such large MDPs. Function approximation <ref> [ SB98 ] </ref> is a well-studied approach to learning value functions in large state spaces, and many authors have recently begun to study the properties of large MDPs that enjoy compact representations, such as MDPs in which the state transition probabilities factor into a small number of components [ MHK +
Reference: [ SY94 ] <author> Satinder Singh and Richard Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 227-233, </pages> <year> 1994. </year>
References-found: 7

