URL: http://www.cs.cmu.edu/afs/cs/project/iwarp/member/pdinda/PAPERS/WebFtp/lcr95.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/pdinda/html/papers.html
Root-URL: 
Title: THE PERFORMANCE IMPACT OF ADDRESS RELATION CACHING  
Author: Peter A. Dinda David R. O'Hallaron 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science, Carnegie Mellon University,  
Abstract: An important portion of end-to-end latency in data transfer is spent in address computation, determining a relation between sender and receiver addresses. In deposit model communication, this computation happensonly on the sender and some of its results are embeddedin the message. Conventionally, address computation takes place on-line, as the message is assembled. If the amount of address computation is significant, and the communication is repeated, it may make sense to remove address computation from the critical path by caching its results. However, assembling a message using the cache uses additional memory bandwidth. We present a fine grain analytic model for simple address relation caching in deposit model communication. The model predicts how many times a communication must be repeated in order for the average end-to-end latency of an implementation which caches to break even with that of an implementation which doesn't cache. The model also predicts speedup and those regimes where a caching implementation never breaks even. The model shows that the effectiveness of caching depends on CPU speed, memory bandwidth and the complexity of the address computation. We verify the model on the iWarp and the Paragon and find that, for both machines, caching can improve performance even when address computation is quite simple (one instruction per data word on the iWarp and 16 instructions per data word on the Paragon.) To show the practical benefit of address relation caching, we examine the performance of an HPF distributed array communication library that can be configured to use caching. In some cases, caching can double the performance of the library. Finally, we discuss other benefits to caching and several open issues. This research was sponsored in part by the Advanced Research Projects Agency/CSTO monitored by SPAWAR under contract N00039-93-C-0152, in part by the National Science Foundation under Grant ASC-9318163, and in part by a grant from the Intel Corporation. The authors' email addresses are: pdinda@cs.cmu.edu and droh@cs.cmu.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H. T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P. S. Tseng, J. Sutton, J. Urbanski, and J. Webb. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Supercomputing '88, </booktitle> <pages> pages 330-339, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: 1 13 INF 13 3 5 7 5 5 4 4 4 7 3 3 3 n g Act Pred-Stat Pred-Dyn 1 1.05 1.00 1.07 3 1.22 1.16 1.23 5 1.35 1.33 1.39 7 1.49 1.49 1.54 (a) (b) Speedup in message assembly (Eqn. 1.5) 4.1 iWarp On the iWarp <ref> [1] </ref> system, r rc = r rr = r wc = r wr = 10 7 words/second, and a node can execute r i = 2 fi 10 7 instructions/second. The memory system parameters were derived directly from the iWarp specification.
Reference: [2] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0 draft, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: However, in both cases, even if only a small number of instructions are executed per tuple of the address relation, caching is effective. To demonstrate the practical benefit of caching, we compare the cached and uncached performance of an HPF <ref> [2] </ref> communication library on the Paragon. We find that caching becomes more effective as the distribution of the data becomes more irregular, and thus the address relations become harder to compute.
Reference: [3] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 16-26, </pages> <year> 1994. </year> <title> [4] i860 Microprocessor Family Programmer's Reference Manual. </title> <publisher> Intel Corporation, </publisher> <year> 1992. </year>
Reference-contexts: In a task parallel array assignment statement, the source and destination arrays are distributed over disjoint sets of processors. The test cases presented are written in Fx <ref> [3] </ref> (a dialect of HPF that integrates task and data parallelism) and compiled and run on an Intel Paragon. Communication is performed 10 Chapter 1 with an enhanced version of the Fx run-time library that supports cached and uncached message assembly.
Reference: [5] <author> Intel Corp. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: In fact, the speedups grow arbitrarily large as n g increases (see Equation 1.5.) 4.2 Paragon A Paragon node contains a 50 MHz pipelined, multiple issue RISC processor <ref> [5, 4] </ref> .
Reference: [6] <author> P. Pierce and G. Regnier. </author> <title> The paragon implementation of the nx message passing interface. </title> <booktitle> In Proc. Scalable High Performance Computing Conference, </booktitle> <pages> pages 184-190, </pages> <address> Knoxville, TN, May 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This task is complicated by the fact that applications do not always keep the data they need to communicate in convenient contiguous memory locations. In general, point-to-point communication with conventional message passing systems like PVM [13], NX <ref> [6] </ref>, and MPI [14] involves three basic steps on the sender: (1) address computation, which determines the local addresses of the data items to be transferred, (2) message assembly, which copies the data items into a contiguous buffer, and (3) message transfer, which copies the message from the buffer to the <p> We conclude with discussion about other issues and benefits of caching. 2 DEPOSIT MODEL COMMUNICATION In a conventional communication system, such as PVM [13], NX <ref> [6] </ref>, or MPI [14], the sender computes a binary relation S between local addresses and addresses offset into the message that is transported between it and the receiver. If (s; m) 2 S then the data item at local address s is copied into the message at offset m.
Reference: [7] <author> J. Saltz, , S. Petiton, H. Berryman, and A. Rifkin. </author> <title> Performance effects of irregular communication patterns on massively parallel multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 202-212, </pages> <year> 1991. </year>
Reference-contexts: A primary motivation is to amortize the address computation cost of communications that are repeated (for example, an HPF array assignment statement in a loop.) A similar approach is used by the CHAOS system <ref> [7] </ref> to improve the performance of irregular collective communications. To determine the effectiveness of address relation caching, we develop a simple analytic model for predicting the performance tradeoffs between cached and uncached point-to-point communications for a simple representation of the address relation.
Reference: [8] <author> P. Steenkiste, B. Zill, H. Kung, S. Schlick, J. Hughes, B. Kowalski, and J. Mullaney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 1-16, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: For example, the relation may be transformed into a DMA gather map on the sender to make best use of a network adapter such as the one described in <ref> [8] </ref>.
Reference: [9] <author> J. Stichnoth. </author> <title> Efficient compilation of array statements for private memory multicomput-ers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The appropriate library function is invoked on each source node, with the distributions of the source and destination arrays passed in as arguments. Address computation is performed using the algorithm in <ref> [9] </ref>. The library can either cache the results of this computation, or use them to directly assemble a message. We measured the performance of the library, with and without caching, for a variety of test cases.
Reference: [10] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The receiver performs a symmetric set of steps. The performance measure of interest is end-to-end latency, which is the time to perform all three steps on both the sender and receiver. A different and typically more efficient approach, known as deposit model message passing <ref> [10, 12] </ref>, combines the address computation steps on the sender and the receiver into a single step on the sender. The result of the address computation step on the sender is an address relation that associates addresses on the sender with addresses on the receiver.
Reference: [11] <author> T. Stricker and T. Gross. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In Proc. 22nd Intl. Symp. on Computer Architecture, </booktitle> <address> Portofino, Italy, </address> <month> June </month> <year> 1995. </year> <note> ACM/IEEE. to appear. </note>
Reference-contexts: As such, it presents opportunities for optimization. For example, since the entire address relation is known to the cache, it could optimize it for memory locality on the sender, receiver, or both. As shown in <ref> [11] </ref>, the choice is architecture-dependent.
Reference: [12] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proc. Intl. Conf. on Supercomputing, </booktitle> <pages> page accepted, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: The receiver performs a symmetric set of steps. The performance measure of interest is end-to-end latency, which is the time to perform all three steps on both the sender and receiver. A different and typically more efficient approach, known as deposit model message passing <ref> [10, 12] </ref>, combines the address computation steps on the sender and the receiver into a single step on the sender. The result of the address computation step on the sender is an address relation that associates addresses on the sender with addresses on the receiver. <p> We concentrate on a simple implementation that stages the 4 Chapter 1 message in memory and uses a simple address-data pair message format. Further details of deposit model communication can be found in <ref> [12] </ref>. 3 ANALYTIC MODELS This section presents models for the average end-to-end latency of a point-to-point communication using uncached and cached implementations of deposit model message passing.
Reference: [13] <author> V. S. Sunderam. </author> <title> PVM : A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: This task is complicated by the fact that applications do not always keep the data they need to communicate in convenient contiguous memory locations. In general, point-to-point communication with conventional message passing systems like PVM <ref> [13] </ref>, NX [6], and MPI [14] involves three basic steps on the sender: (1) address computation, which determines the local addresses of the data items to be transferred, (2) message assembly, which copies the data items into a contiguous buffer, and (3) message transfer, which copies the message from the buffer <p> We find that caching becomes more effective as the distribution of the data becomes more irregular, and thus the address relations become harder to compute. We conclude with discussion about other issues and benefits of caching. 2 DEPOSIT MODEL COMMUNICATION In a conventional communication system, such as PVM <ref> [13] </ref>, NX [6], or MPI [14], the sender computes a binary relation S between local addresses and addresses offset into the message that is transported between it and the receiver.
Reference: [14] <author> D. Walker. </author> <title> The design of a standard message passing interface for distributed memory concurrent computers. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 657-673, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This task is complicated by the fact that applications do not always keep the data they need to communicate in convenient contiguous memory locations. In general, point-to-point communication with conventional message passing systems like PVM [13], NX [6], and MPI <ref> [14] </ref> involves three basic steps on the sender: (1) address computation, which determines the local addresses of the data items to be transferred, (2) message assembly, which copies the data items into a contiguous buffer, and (3) message transfer, which copies the message from the buffer to the wire. <p> We conclude with discussion about other issues and benefits of caching. 2 DEPOSIT MODEL COMMUNICATION In a conventional communication system, such as PVM [13], NX [6], or MPI <ref> [14] </ref>, the sender computes a binary relation S between local addresses and addresses offset into the message that is transported between it and the receiver. If (s; m) 2 S then the data item at local address s is copied into the message at offset m.
References-found: 13

