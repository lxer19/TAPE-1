URL: http://www.cs.indiana.edu/hyplan/leake/papers/p-95-04.ps.Z
Refering-URL: http://www.cs.indiana.edu/hyplan/leake/papers/INDEX.html
Root-URL: 
Email: raja@cs.indiana.edu leake@cs.indiana.edu  
Title: AN ARCHITECTURE FOR GOAL-DRIVEN EXPLANATION  
Author: Raja Sooriamurthi David Leake 
Address: 215, Lindley Hall, Indiana University Bloomington, IN 47405  
Affiliation: Computer Science Department  
Note: Proceedings of the Eighth Annual Florida Artificial Intelligence Research Symposium, Melbourne, FL, 1995, pp. 218-222.  
Abstract: In complex and changing environments explanation must be a dynamic and goal-driven process. This paper discusses an evolving system implementing a novel model of explanation generation | Goal-Driven Interactive Explanation | that models explanation as a goal-driven, multi-strategy, situated process inter-weaving reasoning with action. We describe a preliminary implementation of this model in gobie, a system that generates explanations for its internal use to support plan generation and execution. 
Abstract-found: 1
Intro-found: 1
Reference: [Engelmore and Morgan, 1988] <author> Engelmore, R. and Mor-gan, T. </author> <year> (1988). </year> <title> Blackboard systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachuset. </address>
Reference-contexts: To provide the previously described functionality the system is organized around a blackboard architecture <ref> [Engelmore and Morgan, 1988] </ref>. gobie consists of a STRIPS style planner [Fikes and Nilsson, 1971] functioning in a simple simulated world and a case-based explainer [Schank and Leake, 1989, Schank et al., 1994].
Reference: [Fikes and Nilsson, 1971] <author> Fikes, R. E. and Nilsson, N. J. </author> <year> (1971). </year> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208. </pages>
Reference-contexts: To provide the previously described functionality the system is organized around a blackboard architecture [Engelmore and Morgan, 1988]. gobie consists of a STRIPS style planner <ref> [Fikes and Nilsson, 1971] </ref> functioning in a simple simulated world and a case-based explainer [Schank and Leake, 1989, Schank et al., 1994].
Reference: [Hammond and Seifert, 1994] <author> Hammond, K. and Seifert, C. </author> <year> (1994). </year> <title> Opportunistic memory. </title> <editor> In Schank, R. C. and Langer, E., editors, </editor> <title> Beliefs, reasoning, and decision making : psycho-logic in honor of Bob Abelson, </title> <booktitle> chapter 4, </booktitle> <pages> pages 111-142. </pages> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address>
Reference-contexts: Lower priority goals should be suspended allowing the system to focus on achieving higher priority ones. At the same time an interactive explanation system situated in and capable of action in the world should be able to opportunistically notice situations for satisfying the suspended goals <ref> [Hammond and Seifert, 1994] </ref>. 3 SYSTEM ARCHITECTURE gobie is an evolving implementation of GDIE. <p> Goal types: Depending on their priority, goals are categorized into three types. Immediate goals are those the system takes explicit effort to achieve. These goals drive the primary processing in the system. Opportunistic goals are indexed by the features of the situation in which they may be achieved <ref> [Hammond and Seifert, 1994] </ref>. When the system notices the occurrence of these situational features in the simulated world it triggers the achievement of these goals. Policies influence the broad nature of the processing.
Reference: [Hunter, 1990] <author> Hunter, L. </author> <year> (1990). </year> <title> Planning to learn. </title> <booktitle> In 12th Annual Conference of the Cognitive Science Society, </booktitle> <address> Cambridge, Massachusetts, </address> <pages> pages 261-268. </pages> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address>
Reference-contexts: The formation of an explanation goal initiates explanation. To complete the explanation the system needs to gather additional corroborating information by first reasoning about what information 1 GOal Based Interactive Explanation is needed, planning to acquire that information <ref> [Hunter, 1990, Pryor and Collins, 1991] </ref> and then executing the plan to gather the information. This action may either be in the external world (e.g., running a diagnostic and observing the result) or an internal reasoning action (e.g., reminding, inference, transformation).
Reference: [Kedar-Cabelli, 1987] <author> Kedar-Cabelli, S. </author> <year> (1987). </year> <title> Formulating concepts according to purpose. </title> <booktitle> In Proceedings of the Sixth Annual National Conference on Artificial Intelligence, </booktitle> <pages> pages 477-481, </pages> <address> Seattle, WA. </address> <publisher> AAAI. </publisher>
Reference-contexts: For example, the explanation goal of "explaine to effect a repair" produces the explanation of a dead battery while a situational feature such as "there is not much time left to catch the flight" curtails the explanation effort. 5 RELATED WORK Previous work <ref> [Kedar-Cabelli, 1987] </ref> has considered how to generate target concepts for explanations from the need to select artifacts to use in plans. In gobie the explanation process is monitored in order to possibly refine the target concept as explanation proceeds, based on information gathered during explanation.
Reference: [Kolodner, 1994] <author> Kolodner, J. </author> <year> (1994). </year> <title> Case-Based reasoning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Case-based explanation is the application of the case-based reasoning paradigm <ref> [Kolodner, 1994] </ref> to the problem of creating explanations: new explanations are formed by adapting explanations that had been applied to similar prior problems. The schematic representation of the blackboard and the class hierarchy of the blackboard objects is given in is represented in two ways in the system.
Reference: [Krulwich et al., 1990] <author> Krulwich, B., Birnbaum, L., and Collins, G. </author> <year> (1990). </year> <title> Goal-directed diagnosis of expectation failures. </title> <editor> In O'Rorke, P., editor, </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction., </booktitle> <pages> pages 116-119. </pages> <publisher> AAAI. </publisher>
Reference-contexts: In gobie the explanation process is monitored in order to possibly refine the target concept as explanation proceeds, based on information gathered during explanation. The use of explicit goals to direct the processing of our system is in the spirit of <ref> [Krulwich et al., 1990] </ref> and the more recent work on goal directed diagnosis (GDD) [Rymon, 1993]. Our approach differs from theirs in that we use a multi-strategic framework and interaction with the environment is an integral part of our model.
Reference: [Leake, 1992] <author> Leake, D. B. </author> <year> (1992). </year> <title> Evaluating Explanations: A content theory. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address>
Reference-contexts: But to a person whose knowledge about the status of the car is incomplete failure to start would be unexpected requiring the acquisition of information to explain the malfunction. The central requirement of our model is that the information seeking actions during explanation are guided by explanation goals <ref> [Leake, 1992] </ref> (representing what the system is trying to explain) and task goals (representing why the system is trying to explain) [Ram and Leake, 1994]. These goals should be accessible to both the process requesting explanation (in our system the planner) and the explainer. * Interleaved explanation, planning and action.
Reference: [Leake, 1994] <author> Leake, D. B. </author> <year> (1994). </year> <title> Issues in goal-driven explanation. </title> <editor> In desJardins, M. and Ram, A., editors, </editor> <booktitle> Working notes of the AAAI Spring Symposium in goal-driven learning, </booktitle> <pages> pages 72-79. </pages>
Reference-contexts: 1 INTRODUCTION In standard AI models of explanation, explanation is purely a reasoning process done in isolation from the situation being explained. Likewise in models of explanation that consider explanation goals, goals exert a fixed top-down influence. Our model, termed goal-driven interactive explanation (GDIE) <ref> [Leake, 1994] </ref> allows incremental information obtained by interaction with the environment to influence the explainer's goals and dynamically re-focus the explanation process. In this paper we describe an implemented system in which the explanation process is shaped by ongoing and strategic decisions as the explainer interacts with its environment.
Reference: [Pryor and Collins, 1991] <author> Pryor, L. and Collins, G. </author> <year> (1991). </year> <title> Information gathering as a planning task. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 862-866, </pages> <address> Chicago, IL. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: The formation of an explanation goal initiates explanation. To complete the explanation the system needs to gather additional corroborating information by first reasoning about what information 1 GOal Based Interactive Explanation is needed, planning to acquire that information <ref> [Hunter, 1990, Pryor and Collins, 1991] </ref> and then executing the plan to gather the information. This action may either be in the external world (e.g., running a diagnostic and observing the result) or an internal reasoning action (e.g., reminding, inference, transformation).
Reference: [Ram and Leake, 1994] <author> Ram, A. and Leake, D. B. </author> <year> (1994). </year> <title> Learning, goals and learning goals. </title> <publisher> MIT press. (in press). </publisher>
Reference-contexts: The central requirement of our model is that the information seeking actions during explanation are guided by explanation goals [Leake, 1992] (representing what the system is trying to explain) and task goals (representing why the system is trying to explain) <ref> [Ram and Leake, 1994] </ref>. These goals should be accessible to both the process requesting explanation (in our system the planner) and the explainer. * Interleaved explanation, planning and action. The formation of an explanation goal initiates explanation.
Reference: [Redmond, 1992] <author> Redmond, M. A. </author> <year> (1992). </year> <title> Learning by observing and understanding expert problem solving. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology. (GIT-CC-92/43). </institution>
Reference-contexts: Also, in other approaches a single hypothesis is examined until it is either accepted or ruled out. Our approach involves the parallel utility-based pursuit of the alternative possibilities. The process of interleaving reasoning and action during interaction with an expert in novice learning has been studied in <ref> [Redmond, 1992] </ref>.
Reference: [Rymon, 1993] <author> Rymon, R. </author> <year> (1993). </year> <title> Diagnostic Reasoning and Planning in Exploratory-Corrective Domains. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania. </institution>
Reference-contexts: The use of explicit goals to direct the processing of our system is in the spirit of [Krulwich et al., 1990] and the more recent work on goal directed diagnosis (GDD) <ref> [Rymon, 1993] </ref>. Our approach differs from theirs in that we use a multi-strategic framework and interaction with the environment is an integral part of our model. Also, in other approaches a single hypothesis is examined until it is either accepted or ruled out.
Reference: [Schank, 1986] <author> Schank, R. C. </author> <year> (1986). </year> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address>
Reference-contexts: It is only when the planner initiates the action of starting the car does the feedback from the simulator inform the system about the current world condition. The plans generated by the planner, the candidate explanations (XPs) <ref> [Schank, 1986] </ref> under consideration by the explainer, the utility-metrics used to determine which candidate to explore further are all posted to the blackboard.
Reference: [Schank et al., 1994] <author> Schank, R. C., Kass, A., and Ries-beck, C. K. </author> <year> (1994). </year> <title> Inside Case-Based Explanation. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address>
Reference-contexts: To provide the previously described functionality the system is organized around a blackboard architecture [Engelmore and Morgan, 1988]. gobie consists of a STRIPS style planner [Fikes and Nilsson, 1971] functioning in a simple simulated world and a case-based explainer <ref> [Schank and Leake, 1989, Schank et al., 1994] </ref>. Case-based explanation is the application of the case-based reasoning paradigm [Kolodner, 1994] to the problem of creating explanations: new explanations are formed by adapting explanations that had been applied to similar prior problems.
Reference: [Schank and Leake, 1989] <author> Schank, R. C. and Leake, D. B. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <note> 40((1-3)):353-385. Also in Carbonell, </note> <editor> J., editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: To provide the previously described functionality the system is organized around a blackboard architecture [Engelmore and Morgan, 1988]. gobie consists of a STRIPS style planner [Fikes and Nilsson, 1971] functioning in a simple simulated world and a case-based explainer <ref> [Schank and Leake, 1989, Schank et al., 1994] </ref>. Case-based explanation is the application of the case-based reasoning paradigm [Kolodner, 1994] to the problem of creating explanations: new explanations are formed by adapting explanations that had been applied to similar prior problems.
Reference: [Stern and Luger, 1992] <author> Stern, C. and Luger, G. </author> <year> (1992). </year> <title> A model for abductive problem-solving based on explanations templates and lazy evaluation. </title> <journal> International Journal of Expert Systems, </journal> <volume> 5(3) </volume> <pages> 249-265. </pages>
Reference-contexts: The use of a memory of previous diagnosis stored as schemas to aid diagnostic reasoning and making context sensitive choices of what to diagnose first has been examined in [Turner, 1994] and <ref> [Stern and Luger, 1992] </ref>. 6 CONCLUSIONS In gobie explanation is modeled as a goal-driven activity with the aim of serving the over-arching process that 4 initiates the explanation effort. When the explanation process is initiated in real world contexts the explainer often lacks sufficient information to form an explanation.
Reference: [Turner, 1994] <author> Turner, R. M. </author> <year> (1994). </year> <title> Adaptive reasoning for real-world problems: A schema-based approach. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, New Jersey. </address> <month> 5 </month>
Reference-contexts: The use of a memory of previous diagnosis stored as schemas to aid diagnostic reasoning and making context sensitive choices of what to diagnose first has been examined in <ref> [Turner, 1994] </ref> and [Stern and Luger, 1992]. 6 CONCLUSIONS In gobie explanation is modeled as a goal-driven activity with the aim of serving the over-arching process that 4 initiates the explanation effort.
References-found: 18

