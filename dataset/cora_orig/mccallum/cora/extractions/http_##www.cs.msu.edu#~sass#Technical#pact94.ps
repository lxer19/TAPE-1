URL: http://www.cs.msu.edu/~sass/Technical/pact94.ps
Refering-URL: http://www.cs.msu.edu/~sass/Technical/technical.html
Root-URL: http://www.cs.msu.edu
Title: Transformations on Doubly Nested Loops (Extended Abstract)  Parallel Architectures and Compilation Techniques  
Author: Ron Sass Matt Mutka 
Keyword: compilers, parallel loop transformations, reordering loop iterations, restructuring compilers, unimodular matrix transformations.  
Address: A-714 Wells Hall  East Lansing, MI 48824-1027  
Affiliation: Department of Computer Science  Michigan State University  
Note: International Conference on  March 26, 1994  
Email: sass@cps.msu.edu mutka@cps.msu.edu  
Phone: tel. (517) 353-8666 FAX: (517) 336-1061  
Degree: submitted to  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1978. </year>
Reference-contexts: Furthermore, we can define a direction vector corresponding to a given distance vector by applying the sign 3 function to each element (e.g., the direction vector of the distance vector [4; 3] is <ref> [1; 1] </ref>). For a significant number of problems (which includes a number of linear algebra codes and all systolic array algorithms [9]) there is a uniform pattern to all the distance vectors in an iteration space which can be summarized by a few distance vectors. <p> 2 ; N 2 IF i 3 = n 3 THEN IF i 2 = n 2 THEN P 2 [i 1 ] P 3 [i 1 ; i 2 ] S [i 1 ; i 2 ; i 3 ] END DO Most researchers cite Abu-Sufah's non-basic-to-basic loop transformation <ref> [1] </ref> when referring to this technique, but it is probably much older. (Lamport [5] mentions the technique in 1974.) M.E. Wolf [8] points out an important condition for the legality of this transformation; one that has frequently been omitted.
Reference: [2] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <editor> In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219. </pages> <publisher> Pitman Publishing, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The main result presented in this paper is a new algorithm, the phase method. It is closely allied to other unimodular transformation algorithms such as Banerjee's <ref> [2] </ref> except for the important distinction that it accepts imperfectly nested loops. This result is significant for two reasons. <p> These distance vectors and the original loop bounds are the input to unimodular transformation algorithms. Banerjee presents two algorithms based on unimodular transformations in <ref> [2] </ref>. We use the one that produces fine-grain parallelism and its inputs and outputs are detailed in Figure 4. Reference [2] is a very good introduction to unimodular transformations in general and [9] is an important reference on the subject. Finally, we define a dependence graph. <p> These distance vectors and the original loop bounds are the input to unimodular transformation algorithms. Banerjee presents two algorithms based on unimodular transformations in <ref> [2] </ref>. We use the one that produces fine-grain parallelism and its inputs and outputs are detailed in Figure 4. Reference [2] is a very good introduction to unimodular transformations in general and [9] is an important reference on the subject. Finally, we define a dependence graph. A dependence graph is constructed by letting the index points be the vertices of the graph and the dependencies be directed edges. <p> 6:C (I) ! 9:C (I) (0,1) flow 7:COLSUM (I) ! 9:COLSUM (I) (0,1) outp 10:COLSUM (I) ! 7:COLSUM (I) (0,1) flow 9:A (I,J) ! 6:A (I-3,J) (3,0) flow 10:COLSUM (I) ! 10:COLSUM (I) (0,1) Passing the vector set f (0; 1); (3; 0)g and the loop bounds to Banerjee's algorithm <ref> [2] </ref> results in the following transformation matrix U and new loop bounds m 1 ::M 1 and m 2 ()::M 2 (): U = 1 1 ! m 2 (x)::M 2 (x) = dmaxf3; x 10ge::bminf10; x 1gc Next we calculate: A 2 = 1 (10 3) = 7 2 =
Reference: [3] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1993. </year>
Reference-contexts: Although this algorithm only handles loop nests of depth two, it strengthens the argument that unimodular transformations are amenable to generally nested loops. This is also significant because others have expressed interest in solving this problem <ref> [3] </ref>. The rest of this extended abstract is organized as follows: In x2, we present our definitions. In x3 we show how we generate the data dependence vectors and describe our algorithm. The algorithm is demonstrated with an example in x4. <p> Furthermore, we can define a direction vector corresponding to a given distance vector by applying the sign 3 function to each element (e.g., the direction vector of the distance vector <ref> [4; 3] </ref> is [1; 1]). For a significant number of problems (which includes a number of linear algebra codes and all systolic array algorithms [9]) there is a uniform pattern to all the distance vectors in an iteration space which can be summarized by a few distance vectors.
Reference: [4] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-2995.1, </type> <institution> University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Secondly, some researchers are proposing alternative frameworks because unimodular transformations are limited to perfectly nested loops <ref> [4] </ref>. Although this algorithm only handles loop nests of depth two, it strengthens the argument that unimodular transformations are amenable to generally nested loops. This is also significant because others have expressed interest in solving this problem [3]. <p> Furthermore, we can define a direction vector corresponding to a given distance vector by applying the sign 3 function to each element (e.g., the direction vector of the distance vector <ref> [4; 3] </ref> is [1; 1]). For a significant number of problems (which includes a number of linear algebra codes and all systolic array algorithms [9]) there is a uniform pattern to all the distance vectors in an iteration space which can be summarized by a few distance vectors.
Reference: [5] <author> Leslie Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2), </volume> <month> February </month> <year> 1974. </year>
Reference-contexts: 2 = n 2 THEN P 2 [i 1 ] P 3 [i 1 ; i 2 ] S [i 1 ; i 2 ; i 3 ] END DO Most researchers cite Abu-Sufah's non-basic-to-basic loop transformation [1] when referring to this technique, but it is probably much older. (Lamport <ref> [5] </ref> mentions the technique in 1974.) M.E. Wolf [8] points out an important condition for the legality of this transformation; one that has frequently been omitted.
Reference: [6] <author> David Padua, David Kuck, and Duncan Lawrie. </author> <title> High-speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(9), </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: It is closely allied to other unimodular transformation algorithms such as Banerjee's [2] except for the important distinction that it accepts imperfectly nested loops. This result is significant for two reasons. Although other techniques exist to convert imperfect loop nests into perfect loop nests loop distribution <ref> [6] </ref> being the most notable they cannot always be applied or it may be undesirable to apply them. (Loop distribution, for example, cannot break strongly connected components in a data dependence graph.) The phase method can parallelize code that previous techniques cannot. <p> Because the assumption of perfectly nested loops is so common, many researchers do not even explicitly state the assumption. In this section we summarize the research results provided by other researchers and compare results. Loop distribution <ref> [6] </ref> is an old and popular technique for making loops suitable for automatic conversion to vector or parallel form. Nevertheless, a major problem is that loop distribution cannot break a cycle in the dependence graph. It is this problem that most researchers have tried to address.
Reference: [7] <author> William Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8), </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: The most important feature of this approach is that by characterizing the loop nest our algorithm determines at compile time when to execute the leading statements and trailing statements. Calculating data dependence vectors has been studied extensively (see the references in <ref> [7] </ref>) and no algorithm has emerged as clearly the best. Thus in our initial efforts we tried to distance ourselves from any single algorithm and to accomplish this goal we gave our source program to the dependence analyzer and then adjusted its output to fit our iteration space.
Reference: [8] <author> Michael E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Although this was usually effective for doubly nested loops, we have since adopted the method used in <ref> [8] </ref> which is more robust and easily extended to n dimensions. At the heart of most dependence analyzers, there is an integer constraint solver. We can extract the dependence information in the form we want directly by augmenting the input to to the constraint solver within the dependence analyzer. <p> For doubly nested loops, the additional constraints are: P 2 executes only when i 2 = n 2 (i 1 ) and E 2 executes only when i 2 = N 2 (i 1 ). (As Wolf notes <ref> [8] </ref>, this is conceptually the same as Abu-Sufah's non-basic-to-basic transformation. <p> Wolf <ref> [8] </ref> points out an important condition for the legality of this transformation; one that has frequently been omitted. The condition is: For each loop nest j, n j i j N j ! n j+1 (i j ) N j+1 (i j ) must be true. <p> The legality conditions make this method difficult to perform and the smaller class of acceptable loop nests is a drawback. In his Ph.D. thesis, M.E. Wolf <ref> [8] </ref> discusses imperfect loop nests. Similarly, he starts with Abu Sufah's non-basic-to-basic method but he states that under certain conditions, after the unimodular transformations, some of the if statements can be moved out of the innermost loop (and part of the condition removed). <p> That is, i 0 3 = n 3 (i 0 1 ) is only true at the beginning of the execution of the i 0 3 loop. See Wolf <ref> [8] </ref> for more details.) 4 Although Wolfe discusses loop skewing, he does not indicate that these dependenciescan be avoided by the combined effects of loop skewing and loop interchange. 10 Wolf claims (p. 51) that if the unimodular transformation is a skew then the non-perfectly [sic] nested portions would be just
Reference: [9] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: For a significant number of problems (which includes a number of linear algebra codes and all systolic array algorithms <ref> [9] </ref>) there is a uniform pattern to all the distance vectors in an iteration space which can be summarized by a few distance vectors. These distance vectors and the original loop bounds are the input to unimodular transformation algorithms. Banerjee presents two algorithms based on unimodular transformations in [2]. <p> Banerjee presents two algorithms based on unimodular transformations in [2]. We use the one that produces fine-grain parallelism and its inputs and outputs are detailed in Figure 4. Reference [2] is a very good introduction to unimodular transformations in general and <ref> [9] </ref> is an important reference on the subject. Finally, we define a dependence graph. A dependence graph is constructed by letting the index points be the vertices of the graph and the dependencies be directed edges.
Reference: [10] <author> Michael Wolfe. </author> <title> The tiny restructuring research tool. </title> <booktitle> In Proceedings of 1991 Int'l Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <year> 1991. </year>
Reference-contexts: Below, the original code is listed with line numbers and dependence information. (We used a well-known program, Tiny, by M.J. Wolfe to generate the distance vectors <ref> [10] </ref> and then using the constraints we described in the previous section arrived at these distance vectors.) code 5 DO I = 3,10 7 COLSUM (I) = 0 8 DO J = 1,10 10 COLSUM (I) = COLSUM (I)+A (I,J) 8 END DO Dependence Information flow 6:C (I) ! 9:C (I)

References-found: 10

