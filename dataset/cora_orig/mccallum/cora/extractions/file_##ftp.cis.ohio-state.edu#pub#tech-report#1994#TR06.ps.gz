URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR06.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: e-mail: fprakash, pandag@cis.ohio-state.edu  
Title: Task Scheduling in Heterogeneous Systems in the Presence of Channel Contention  
Author: Ravi Prakash Dhabaleswar K. Panda Prof. Dhabaleswar K. Panda 
Keyword: channel contention, heterogeneous system, hierarchical interconnection, meta-parallelism, task scheduling, wavelength division multiplexing.  
Note: Contact Author:  
Address: Columbus, OH 43210.  
Affiliation: Department of Computer and Information Science The Ohio State University,  
Abstract: This paper investigates some of the communication and scheduling issues in het erogeneous systems. A representative heterogeneous system consisting of a set of com puters, each architecturally different and exploiting different kinds of parallelism is considered. These computers are connected by a high speed, passive star-coupled photonic network that employs wavelength division multiplexing (WDM). A model scientific computation, exhibiting meta-parallelism, has been studied on this system. Two scheduling strategies have been considered. One strategy involves both subtask data and code migration to the subtask execution site, while the other (the no-code migration strategy) requires only the data to be transmitted. Simulations of the exe cution of this computation on our heterogeneous system provide the following insights into the design of such a system: A master-slave model of execution can reduce the amount of communication, and is therefore scalable. Contention for communication channels makes a significant contribution towards the overall task completion time. Channel contention increases with an increase in the degree of parallelism. In the presence of fast disk I/O, the no-code-migration strategy performs better than the subtask-code-migration strategy. A hierarchical communication topology implemented on the passive star-coupled network leads to a considerable reduction in contention and task completion time. These results can be used as guidelines for developing scalable heterogeneous systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abrahamson. </author> <title> Development of the ALOHANET. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-31:119-123, </volume> <month> March </month> <year> 1985. </year>
Reference-contexts: When a slave realizes that there has been a collision, it waits for a random period of time before retrying to send the result. Hence, the communication protocol resembles the ALOHA protocol for communication on a bus <ref> [1] </ref>. The impact of collisions on the performance of the system depends on the number of slaves contending for the master's channel. 5.1.2 Communication with Tunable Receivers For the master-slave communication a protocol with tunable receivers and fixed frequency transmitters will have no contention.
Reference: [2] <author> Emmanuel A. Arnould, Francois J. Bitz, Eric C. Cooper, H. T. Kung, Robert D. Sansom, and Peter A. Steenkiste. </author> <title> The Design of Nectar: A Network Backplane for Heterogeneous Multicomputers. </title> <booktitle> In Proceedings of the 3 rd International Conference on Architectural Support for Programming Languages and Operating Systems(ASPLOS III), </booktitle> <pages> pages 205 216. </pages> <address> ACM/IEEE, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: Heterogeneous parallel systems are especially suited for such tasks containing meta parallelism. A heterogeneous parallel computing environment consists of a connected set of heterogeneous computer systems <ref> [2, 13, 19] </ref>. These computers have different architectures and exploit different types of parallelism. They are capable of communicating using a high speed interconnection network to cooperate in solving an application problem [12].
Reference: [3] <author> Kalyani Bogineni, Krishna M. Sivalingam, and Patrick W. Dowd. </author> <title> Low-Complexity Mul tiple Access Protocols for Wavelength-Division Multiplexed Photonic Networks. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 11(4) </volume> <pages> 590-604, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These machine specifications have been derived based on the values reported in [4, 11, 17]. The performance specifications of the communication network have been selected based on the information obtained from <ref> [3, 6] </ref>. 5.1 Communication Architecture and Bandwidth Local area network communication rate has not increased at the same rate as the processing power and memory size. <p> The high performance parallel interface (HIPPI) physical layer stan dard defines full duplex parallel interfaces that can run at speeds up to 200 megabytes/second [17, 18]. Wavelength-division multiplexing (WDM) protocols are being developed to transmit sev 12 eral 1-gigabit data streams through a single optic-fiber cable <ref> [3, 6] </ref>. These WDM protocols assume passive optical star-coupled networks with preassigned channels.
Reference: [4] <author> Charles E. Catlett. </author> <title> Balancing Resources. </title> <journal> IEEE Spectrum, </journal> <pages> pages 48-55, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The specifications of the abstract machines have been chosen to reflect the performance of the current supercomputers and of those expected to be available in the next few years. These machine specifications have been derived based on the values reported in <ref> [4, 11, 17] </ref>. The performance specifications of the communication network have been selected based on the information obtained from [3, 6]. 5.1 Communication Architecture and Bandwidth Local area network communication rate has not increased at the same rate as the processing power and memory size. <p> Emerging high-speed DRAMs like RamBus and RamLink can provide memory bandwidth 15 of up to 500 megabytes/second [11]. The Scalable Coherent Interface (SCI) standard with a potential speed of 1 gigabyte/second has also been proposed <ref> [4] </ref>. With these technological advances, we assume a transfer rate of 1 gigabyte/second between the front-end and the computational processors at a slave. 5.3 I/O Bandwidth The I/O bandwidth provided by the disk storage of the current supercomputers is low as compared to their data processing rate.
Reference: [5] <author> Song Chen, Mary M. Eshaghian, Ashfaq Khokhar, and Muhammad E. Shaaban. </author> <title> A Selection Theory and Methodology for Heterogeneous Supercomputing. </title> <booktitle> In Proceedings of the 2 nd Workshop on Heterogeneous Processing, </booktitle> <pages> pages 15-22. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: We assume that there are always a sufficient number of computers available and each subtask can be allocated to the computer which best exploits the nature of parallelism in that subtask. Description of such a system, called Optimal Selection Theory, can be found in <ref> [7, 5, 12, 19] </ref>. We also assume that subtasks corresponding to only one task are being executed by the system at any given time.
Reference: [6] <author> Renu Chipalkatti, Zhensheng Zhang, and Anthony S. Acampora. </author> <title> Protocols for Optical Star-Coupled Network Using WDM: Performance and Complexity Study. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 11(4) </volume> <pages> 579-589, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These machine specifications have been derived based on the values reported in [4, 11, 17]. The performance specifications of the communication network have been selected based on the information obtained from <ref> [3, 6] </ref>. 5.1 Communication Architecture and Bandwidth Local area network communication rate has not increased at the same rate as the processing power and memory size. <p> The high performance parallel interface (HIPPI) physical layer stan dard defines full duplex parallel interfaces that can run at speeds up to 200 megabytes/second [17, 18]. Wavelength-division multiplexing (WDM) protocols are being developed to transmit sev 12 eral 1-gigabit data streams through a single optic-fiber cable <ref> [3, 6] </ref>. These WDM protocols assume passive optical star-coupled networks with preassigned channels. <p> a great mismatch between the speeds of the optical network interfaces and the bandwidth of the fiber. "Advances in the fast tunable transmitters exceed those in fast tunable receivers; the tuning times for receivers are relatively long as compared to the packet transmission times and their tuning range is small" <ref> [6] </ref>. We present brief descriptions of two WDM protocols, and their relative merits and demerits. 5.1.1 Communication Using Tunable Transmitters In this approach each node gets assigned a home frequency at which it can receive data.
Reference: [7] <author> Mary M. Eshaghian and Richard F. Freund. </author> <title> Cluster-M Paradigm for High-Order Het erogeneous Procedural Specification Computing. </title> <booktitle> In Proceedings of the Workshop on Heterogeneous Processing, </booktitle> <pages> pages 47-49. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: We assume that there are always a sufficient number of computers available and each subtask can be allocated to the computer which best exploits the nature of parallelism in that subtask. Description of such a system, called Optimal Selection Theory, can be found in <ref> [7, 5, 12, 19] </ref>. We also assume that subtasks corresponding to only one task are being executed by the system at any given time.
Reference: [8] <author> Richard F. Freund and Howard Jay Siegel. </author> <title> Heterogeneous Processing. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 13-17, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Research in the area of heterogeneous systems is motivated by the belief that there is not, and will not be a single all-encompassing architecture, suitable for diverse problems <ref> [8] </ref>. 3 In this paper we evaluate some of the factors that influence the performance of a hetero geneous system. These factors should be considered while evolving the strategies of subtask allocation to a suite of loosely coupled heterogeneous computers.
Reference: [9] <institution> Paragon XP/S Product Overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: For example, the basic disk storage node and the data vault of CM-5 provide peak transfer rates of 17 megabytes/second and 25 megabytes/second, respectively. The Intel Paragon's disk provides a peak transfer rate of 100 megabytes/second <ref> [9] </ref>. Disk transfer rates have been rising with technological advancements. They can be further increased through architectural innovations such as connecting an array of disks to the high speed network mentioned above and employing interleaved disk access.
Reference: [10] <author> Christopher R. Johnson, Robert S. MacLeod, and Michael A. Matheson. </author> <title> Computational Medicine: Bioelectric Field Problems. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-67, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Our problem model is based on two bioelectric field problems presented in <ref> [10] </ref>. A large task can be broken down into a number of subtasks. Based on the precedence relations some of the subtasks can be executed concurrently on different computers in the heterogeneous system. When subtask A comes to an end it produces some results. <p> In developing a computational model for elec troencephalography, it should be noted that the electrical currents do not travel from one neuron to all the other neurons. Instead, impulses from a neuron travel along the axons to only some of the other neurons <ref> [10] </ref>. The influences coming along the axons determine the behaviour of the recipient neurons. If there are N neurons and we model direct message exchange between them there can possibly be O (N 2 ) messages at each stage of signal trans mission. <p> The problem can be made computationally tractable by "approximating it on a finite dimensional sub space and reformulating the partial differential equations as a set of linear matrix equations" <ref> [10] </ref>. So, each subtask in our model problem consists of matrix operations. We have used matrix operations just as a tool for modeling purposes. Subtasks may consist of a variety of different operations, depending on the nature of parallelism embedded in them.
Reference: [11] <author> Fred Jones. </author> <title> A New Era of Fast Dynamic RAMs. </title> <journal> IEEE Spectrum, </journal> <pages> pages 43-49, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The specifications of the abstract machines have been chosen to reflect the performance of the current supercomputers and of those expected to be available in the next few years. These machine specifications have been derived based on the values reported in <ref> [4, 11, 17] </ref>. The performance specifications of the communication network have been selected based on the information obtained from [3, 6]. 5.1 Communication Architecture and Bandwidth Local area network communication rate has not increased at the same rate as the processing power and memory size. <p> For this purpose we assume that the subtask code is broadcast to each constituent processor while a small and distinct fraction of the data is copied from the front-end to individual processors. Emerging high-speed DRAMs like RamBus and RamLink can provide memory bandwidth 15 of up to 500 megabytes/second <ref> [11] </ref>. The Scalable Coherent Interface (SCI) standard with a potential speed of 1 gigabyte/second has also been proposed [4].
Reference: [12] <author> Ashfaq Khokhar, Viktor K. Prasanna, Muhammad Shaaban, and Cho-Li Wang. </author> <title> Het erogeneous Supercomputing : Problems and Issues. </title> <booktitle> In Proceedings of the Workshop on Heterogeneous Processing, </booktitle> <pages> pages 3-12. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: All the subtasks that can be executed concurrently are assigned to different groups of identical processors of the parallel computer, given that the computer under consideration has suffi cient number of processors. However, this scheme is not very efficient for tasks exhibiting meta-parallelism. Such tasks exhibit coarse-grained heterogeneity <ref> [12] </ref>, i.e., the task can be divided into subtasks exhibiting different kinds of parallelism. <p> A heterogeneous parallel computing environment consists of a connected set of heterogeneous computer systems [2, 13, 19]. These computers have different architectures and exploit different types of parallelism. They are capable of communicating using a high speed interconnection network to cooperate in solving an application problem <ref> [12] </ref>. The high speed network is required to match the high computing power of the computers and to min imize the I/O overheads by providing low latency data transfer. A subtask may be executed on a computer that provides the most suitable mode of parallelism for it. <p> The operating system has to analyze the nature of the task and find the most suitable selection of component computers on which the various segments of the problem can be executed. This involves system level scheduling <ref> [12] </ref>. As the configuration of the system may change with time, tasks may have to be reassigned from one computer to another. Several scientific applications can be divided into subtasks for which library routines are present at the component computers. <p> We assume that there are always a sufficient number of computers available and each subtask can be allocated to the computer which best exploits the nature of parallelism in that subtask. Description of such a system, called Optimal Selection Theory, can be found in <ref> [7, 5, 12, 19] </ref>. We also assume that subtasks corresponding to only one task are being executed by the system at any given time. <p> For subtask-code-migration to be unconstrained by the nature of the computers in the system, machine-independent and portable parallel programming languages and tools would be needed. This would require cross-parallel compilers for all the machines and appropriate debuggers <ref> [12] </ref>. The subtask-code-migration approach also has a high volume of communica tion. 4.2 No Code Migration In this strategy, if a subtask has to be executed in a phase, the master computer sends only the data (not the executable code) for the subtask to the appropriate slave computer.
Reference: [13] <author> H.T. Kung, Robert Sansom, Steven Schlick, Peter Steenkiste, Matthieu Arnould, Fran cois J. Bitz, Fred Christianson, Eric C. Cooper, Onat Menzilcioglu, Denise Ombres, and Brian Zill. </author> <title> Network-Based Multicomputers : An Emerging Parallel Architecture. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 664-673, </pages> <year> 1991. </year> <month> 24 </month>
Reference-contexts: Heterogeneous parallel systems are especially suited for such tasks containing meta parallelism. A heterogeneous parallel computing environment consists of a connected set of heterogeneous computer systems <ref> [2, 13, 19] </ref>. These computers have different architectures and exploit different types of parallelism. They are capable of communicating using a high speed interconnection network to cooperate in solving an application problem [12].
Reference: [14] <author> T.G. Lewis. </author> <title> Piece-Wise Scheduling of Composite Task Graphs onto Distributed Memory Parallel Computers. </title> <type> Technical report, </type> <institution> Computer Science Department, Oregon State University, </institution> <year> 1992. </year>
Reference-contexts: But, different slave processors may have different speeds. So, intuitively, faster processors should get larger subtasks to execute. Similarly, for some applications, it may not be possible to divide tasks into equal sizes. This will lead to a variation in subtask sizes in a realistic heterogeneous environment. In <ref> [14] </ref>, a heuristic algorithm for piece-wise scheduling has been presented for distributed memory parallel computers. But it does not guarantee the best global schedule. In [16], Sih and Lee present a compile-time heuristic for scheduling tasks, on heterogeneous systems, with irregular interconnection structures.
Reference: [15] <author> Herb Schwetman. </author> <title> CSIM Revision 16. </title> <institution> Microelectronics and Computer Technology Cor poration, 3500 West Balcones Center Drive, </institution> <address> Austin TX 78759, U.S.A., </address> <month> June </month> <year> 1992. </year>
Reference-contexts: soon as this data transfer finishes, the slave can start receiving the subtask code from the disk at the rate of 100 megabytes/second. 16 6 Simulation Experiments and Results We simulated several executions of tasks that has been broken down into 1000 subtasks using CSIM, a process-oriented discrete-event simulation package <ref> [15] </ref>. The number of subtasks that can be executed concurrently (degree of parallelism) was uniformly distributed in the range 7-13. We assumed the executable code size for a subtask to be normally distributed with mean 8 megabytes and a standard deviation of 0.8 megabytes.
Reference: [16] <author> Gilbert C. Sih and Edward A. Lee. </author> <title> A Compile-Time Scheduling Heuristic for Interconnection- Constrained Heterogeneous Processor Architectures. </title> <booktitle> IEEE Transac tions on Parallel and Distributed Systems, </booktitle> <pages> pages 175-187, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This will lead to a variation in subtask sizes in a realistic heterogeneous environment. In [14], a heuristic algorithm for piece-wise scheduling has been presented for distributed memory parallel computers. But it does not guarantee the best global schedule. In <ref> [16] </ref>, Sih and Lee present a compile-time heuristic for scheduling tasks, on heterogeneous systems, with irregular interconnection structures. They assume that the code for the subtask already resides at the computer to which it is mapped.
Reference: [17] <author> J.E. Smith and W.-C. Hsu C. Hsiung. </author> <title> Future General Purpose Supercomputer Archi tectures. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 796-804. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: The specifications of the abstract machines have been chosen to reflect the performance of the current supercomputers and of those expected to be available in the next few years. These machine specifications have been derived based on the values reported in <ref> [4, 11, 17] </ref>. The performance specifications of the communication network have been selected based on the information obtained from [3, 6]. 5.1 Communication Architecture and Bandwidth Local area network communication rate has not increased at the same rate as the processing power and memory size. <p> The high performance parallel interface (HIPPI) physical layer stan dard defines full duplex parallel interfaces that can run at speeds up to 200 megabytes/second <ref> [17, 18] </ref>. Wavelength-division multiplexing (WDM) protocols are being developed to transmit sev 12 eral 1-gigabit data streams through a single optic-fiber cable [3, 6]. These WDM protocols assume passive optical star-coupled networks with preassigned channels.
Reference: [18] <author> Ronald J. Vetter, David H.C. Du, and Alan E. Klietz. </author> <title> Network Supercomputing : Ex periments with a CRAY-2 to CM-2 HIPPI Connection. </title> <booktitle> In Proceedings of the Workshop on Heterogeneous Processing, </booktitle> <pages> pages 87-92. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: The high performance parallel interface (HIPPI) physical layer stan dard defines full duplex parallel interfaces that can run at speeds up to 200 megabytes/second <ref> [17, 18] </ref>. Wavelength-division multiplexing (WDM) protocols are being developed to transmit sev 12 eral 1-gigabit data streams through a single optic-fiber cable [3, 6]. These WDM protocols assume passive optical star-coupled networks with preassigned channels.
Reference: [19] <author> Mu-Cheng Wang, Shin-Dug Kim, Mark A. Nichols, Richard F. Freund, Howard Jay Siegel, and Wayne G. Nation. </author> <title> Augmenting the Optimal Selection Theory for Super computing. </title> <booktitle> In Proceedings of the Workshop on Heterogeneous Processing, </booktitle> <pages> pages 13-21. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Heterogeneous parallel systems are especially suited for such tasks containing meta parallelism. A heterogeneous parallel computing environment consists of a connected set of heterogeneous computer systems <ref> [2, 13, 19] </ref>. These computers have different architectures and exploit different types of parallelism. They are capable of communicating using a high speed interconnection network to cooperate in solving an application problem [12]. <p> Based on the information gathered from code profiling the task is broken down into several subtasks, each exhibiting a different kind of parallelism. Then analytical benchmarking is done to determine the suitability of a particular computer for a given type of parallelism <ref> [19] </ref>. Once code profiling and analytical benchmarking have been done, the subtasks need to be scheduled on the different computers in the heterogeneous system. <p> We assume that there are always a sufficient number of computers available and each subtask can be allocated to the computer which best exploits the nature of parallelism in that subtask. Description of such a system, called Optimal Selection Theory, can be found in <ref> [7, 5, 12, 19] </ref>. We also assume that subtasks corresponding to only one task are being executed by the system at any given time.
References-found: 19

