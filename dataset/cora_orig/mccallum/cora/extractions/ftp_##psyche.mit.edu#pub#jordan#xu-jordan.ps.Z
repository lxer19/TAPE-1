URL: ftp://psyche.mit.edu/pub/jordan/xu-jordan.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/ML98/home.html
Root-URL: 
Title: On Convergence Properties of the EM Algorithm for Gaussian Mixtures  
Author: Lei Xu Michael I. Jordan 
Affiliation: Department of Computer Science The Chinese University of Hong Kong  Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Date: 8, 129-151, 1996.  
Note: Neural Computation,  
Abstract: We build up the mathematical connection between the "Expectation-Maximization" (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite Gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P , and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the effect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of Gaussian mixture models.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <title> (in press) Information geometry of the EM and em algorithms for neural networks, Neural Networks. </title>
Reference: <author> Baum, L.E., and Sell, G.R. </author> <year> (1968), </year> <title> Growth transformation for functions on manifolds, Pac. </title> <journal> J. Math., </journal> <volume> 27, </volume> <pages> 211-227. </pages>
Reference: <author> Bengio, Y., and Frasconi, P., </author> <year> (1995), </year> <title> An input-output HMM architecture. </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Tesauro, G., Touretzky, D.S., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Boyles, R.A. </author> <year> (1983), </year> <title> On the convergence of the EM algorithm, </title> <journal> J. of Royal Statistical Society, B45, </journal> <volume> No.1, </volume> <pages> 47-50. </pages>
Reference: <author> Dempster, A.P., Laird, N.M., and Rubin, D.B. </author> <year> (1977), </year> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> J. of Royal Statistical Society, B39, </journal> <pages> 1-38. </pages>
Reference-contexts: Given K and given N independent, identically distributed samples fx (t) g N 1 , we obtain the following log likelihood: 2 l (fi) = log t=1 N X log P (x (t) jfi); (2) which can be optimized via the following iterative algorithm <ref> (see, e.g, Dempster, Laird & Rubin, 1977) </ref>: (k+1) P N (k) N (k+1) P N (k) P N (k) j = t=1 h j (t)[x (t) m j ][x (t) m j ] T t=1 h j (t)x (t) 2 Although we focus on maximum likelihood (ML) estimation in this paper,
Reference: <author> Ghahramani, Z, and Jordan, M.I. </author> <year> (1994), </year> <title> Function approximation via density estimation using the EM approach, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Cowan, J.D., Tesauro, G., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <month> 120-127. </month> <title> 4 In most applications of HMM's, the "parameter estimation" process is employed solely to yield models with high likelihood; the parameters are not generally endowed with a particular meaning. 16 Jordan, M.I. </title> <editor> and Jacobs, R.A. </editor> <year> (1994), </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference: <author> Jordan, M.I. and Xu, L. </author> <title> (in press), Convergence results for the EM approach to mixtures-of-experts architectures, Neural Networks. </title>
Reference: <author> Levinson, S.E., Rabiner, </author> <title> L.R., and Sondhi, M.M. (1983), An introduction to the application of the theory of probabilistic functions of Markov process to automatic speech recognition, </title> <journal> The Bell System Technical Journal, </journal> <volume> 62, </volume> <pages> 1035-1072. </pages>
Reference-contexts: They showed that the search direction of this recursive formula, i.e., T (x (k) ) x (k) , has a positive projection on the gradient of of J with respect to the x (k) <ref> (see also Levinson, Rabiner & Sondhi, 1983) </ref>. It can be shown that Baum and Sell's recursive formula implies the EM update formula for A in a Gaussian mixture. Thus, the first statement in Theorem 1 is a special case of Baum and Sell's earlier work.
Reference: <author> Neal, R. N. and Hinton, G. E. </author> <year> (1993), </year> <title> A new view of the EM algorithm that justifies incremental and other variants, </title> <institution> University of Toronto, Department of Computer Science preprint. </institution>
Reference-contexts: The popularity of gradient descent algorithms for neural networks is in part to the ease of obtaining on-line variants of gradient descent. It is worth noting that on-line variants of the EM algorithm can be derived <ref> (Neal & Hinton, 1993, Titterington, 1984) </ref>, and this is a further factor that weighs in favor of EM as compared to conjugate gradient and Newton methods. 5 Convergence rate comparisons In this section, we provide a comparative theoretical discussion of the convergence rates of constrained gradient ascent and EM.
Reference: <author> Nowlan, S.J. </author> <year> (1991). </year> <title> Soft competitive adaptation: Neural network learning algorithms based on fitting statistical mixtures. </title> <type> Tech. Rep. </type> <address> CMU-CS-91-126, CMU, Pittsburgh, PA. </address>
Reference: <author> Redner, R.A., and Walker, H.F. </author> <year> (1984), </year> <title> Mixture densities, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review 26, </journal> <pages> 195-239. </pages>
Reference: <author> Titterington, D.M. </author> <year> (1984), </year> <title> Recursive parameter estimation using incomplete data, </title> <journal> J. of Royal Statistical Society, </journal> <volume> B46, </volume> <pages> 257-267. </pages>
Reference: <author> Tresp, V, Ahmad, S. and Neuneier, R. </author> <year> (1994), </year> <title> Training neural networks with deficient data, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds., Cowan, J.D., Tesauro, G., and Alspector, J., </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 128-135. </pages>
Reference: <author> Waterhouse, S. R., and Robinson, A. J., </author> <year> (1994), </year> <title> Classification using hierarchical mixtures of experts, </title> <booktitle> in IEEE Workshop on Neural Networks for Signal Processing. </booktitle>
Reference: <author> Wu. C.F. J. </author> <year> (1983), </year> <title> On the convergence properties of the EM algorithm, </title> <journal> The Annals of Statistics, </journal> <volume> 11, </volume> <pages> 95-103. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993a), </year> <title> Unsupervised learning by EM algorithm based on finite mixture of Gaus-sians, </title> <booktitle> Proc. of WCNN'93, Portland, OR, </booktitle> <volume> Vol. II, </volume> <pages> 431-434. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993b), </year> <title> EM learning on a generalized finite mixture model for combining multiple classifiers, </title> <booktitle> Proc. of WCNN'93, </booktitle> <address> Portland, OR, </address> <booktitle> Vol. IV, </booktitle> <pages> 227-230. </pages>
Reference: <author> Xu, L., and Jordan, M.I. </author> <year> (1993c), </year> <title> Theoretical and experimental studies of the EM algorithm for unsupervised learning based on finite Gaussian mixtures, MIT Computational Cognitive Science, </title> <type> Technical Report 9302, </type> <institution> Dept. of Brain and Cognitive Science, MIT, </institution> <address> Cambridge, MA. </address>
Reference: <author> Xu, L., Jordan, M.I. and Hinton, G.E. </author> <year> (1994), </year> <title> A Modified gating network for the mixtures of experts architecture, </title> <booktitle> Proc. of WCNN'94, San Diego, </booktitle> <volume> Vol. 2, </volume> <pages> 405-410. </pages>

References-found: 19

