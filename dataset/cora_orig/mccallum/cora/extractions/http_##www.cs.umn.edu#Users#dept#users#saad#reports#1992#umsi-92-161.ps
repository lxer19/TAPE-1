URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/umsi-92-161.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/
Root-URL: http://www.cs.umn.edu
Title: Block-ADI Preconditioners for Solving Sparse Nonsymmetric Linear Systems of Equations  
Author: Sangback Ma and Youcef Saad 
Keyword: Key words: Preconditioning; ADI; Block ADI Preconditioned GMRES; Domain Decomposition.  
Note: AMS (MOS) Subject Classification: 65F10.  
Abstract: There is currently a regain of interest in the Alternating Direction Implicit (ADI) algorithms as preconditioners for iterative methods for solving large sparse linear systems, because of their suitability for parallel computing. However, the classical ADI iteration is not directly applicable to Finite Element (FE) matrices, tends to converge too slowly for 3-D problems, and the selection of adequate acceleration parameters, remains a difficult task. In this paper we propose a Block-ADI approach, which overcomes some of these problems. In particular we derive a simple inexpensive heuristic for selecting the acceleration parameters. The new approach can be viewed as a combination of the classical ADI method and a domain decomposition approach. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> E. C. Anderson and Y. Saad, </author> " <title> Solving sparse triangular systems on parallel computers", </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1 </volume> <pages> 73-96, </pages> <year> 1989. </year>
Reference-contexts: When implemented on parallel computers, standard preconditioners based on incomplete factorization, such as ILU [14] or the more elaborate versions such as ILUT [17, 24], using a wavefront or level-scheduling approach realize a reasonable speed-up on vector computers or parallel computers with a small number of processors <ref> [1, 2, 5] </ref>. However, the lengths of the wavefronts are not uniform, and this contributes to poor load balancing, and puts a limit to the maximum achievable speed-up due to Amdahl's law. Alternatively, parallelism can also be obtained through multicoloring.
Reference: [3] <author> O. Axelsson, </author> " <title> Conjugate Gradient Type-methods for Unsymmetric and Inconsistent Systems of Linear Equations", </title> <type> Technical Report 74-10, </type> <institution> CERN, Geneva, </institution> <year> 1974. </year>
Reference-contexts: This eventually becomes difficult to program and, in addition, will yield diminishing returns. Going back to standard preconditioners, we recall that historically SSOR was used first as a pre-conditioner to the conjugate gradient methods <ref> [3, 4] </ref> well before incomplete factorization techniques became popular. Similarly, standard relaxation or block relaxation techniques have often provided easy-to-implement and yet reasonably efficient preconditioners. However, insufficient work has been done on the parallel implementations of these techniques.
Reference: [4] <author> O. Axelsson, </author> " <title> A Survey of Preconditioned Iterative Methods for Linear Systems of Algebraic Equations", </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 166-187, </pages> <year> 1985. </year>
Reference-contexts: This eventually becomes difficult to program and, in addition, will yield diminishing returns. Going back to standard preconditioners, we recall that historically SSOR was used first as a pre-conditioner to the conjugate gradient methods <ref> [3, 4] </ref> well before incomplete factorization techniques became popular. Similarly, standard relaxation or block relaxation techniques have often provided easy-to-implement and yet reasonably efficient preconditioners. However, insufficient work has been done on the parallel implementations of these techniques.
Reference: [5] <author> D. Baxter, J. Saltz, M. H. Schultz, S. C. Eisenstat, and K. Crowley, </author> " <title> An experimental study of methods for parallel preconditioned Krylov methods", </title> <type> Technical Report 629, </type> <institution> Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1988. </year>
Reference-contexts: When implemented on parallel computers, standard preconditioners based on incomplete factorization, such as ILU [14] or the more elaborate versions such as ILUT [17, 24], using a wavefront or level-scheduling approach realize a reasonable speed-up on vector computers or parallel computers with a small number of processors <ref> [1, 2, 5] </ref>. However, the lengths of the wavefronts are not uniform, and this contributes to poor load balancing, and puts a limit to the maximum achievable speed-up due to Amdahl's law. Alternatively, parallelism can also be obtained through multicoloring.
Reference: [6] <author> G. Birkhoff, R. Varga, S. R., and D. Young, </author> <title> "Alternating Direction Implicit Methods", </title> <booktitle> in Advances in Computers, </booktitle> <address> pp.189-273, </address> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1962 </year>
Reference-contexts: The theory of the convergence and the selection of good parameters when more than one one cycle is used, has not been fully developed. For those cases when HV = V H, a satisfactory theory does exist <ref> [6] </ref>. For the model problem with a single optimal the time complexity is O (n 3 ). In [15] a procedure is described to reduce the time complexity to O (n 2 log n) with a sequence of parameters. <p> This makes the algorithm converge within discretization errors, in a number of steps of the order of O (n 1=l ) with a fixed l [15]. If HV = V H, and H and V are symmetric, positive definite, there exists satisfactory theory based on common eigenvectors <ref> [6] </ref>. If V and H commute but are not symmetric with possibly complex eigenvalues, G. Starke derives an algorithm to determine the optimal values for l = 1 and l = 2, and N. Ellner and E.
Reference: [7] <author> J. Douglas, </author> <title> "Alternating Direction Methods for Three Space Variables", </title> <journal> Numerische Mathematik Vol. </journal> <volume> 4, </volume> <year> 1962, </year> <pages> pp. 41-63. </pages>
Reference-contexts: For the model problem with a single optimal the time complexity is O (n 3 ). In [15] a procedure is described to reduce the time complexity to O (n 2 log n) with a sequence of parameters. For 3-D problems Douglas <ref> [7] </ref> proposed a variant of the classical ADI, which has better convergence behavior than the classical ADI. Let A = H + V + W , where H, V , and W contains the x-, y-, z-, directional derivatives, respectively. Algorithm 2.2.
Reference: [8] <author> N. Ellner and E. </author> <title> Wachspress, "Alternating Direction Implicit Iteration for systems with Complex Spectra", </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> Vol. 28, No. 3, </volume> <pages> pp. 859-870, </pages> <year> 1991 </year>
Reference: [9] <author> H. Elman, </author> <title> "Iterative Methods for Large, Sparse, Nonsymmetric Systems of Linear Equations", </title> <type> Ph. D Thesis, </type> <institution> Yale University, </institution> <year> 1982 </year>
Reference-contexts: The mesh sizes vary from test to test and are reported independently in this section. * Problem 1 Poisson Equation on a Square 4u = f; = [0; 1] fi [0; 1] f = x (1 x) + y (1 y) * Problem 2 Elman's problem <ref> [9] </ref> (bu x ) x (cu y ) y + (du) x + du x + (eu) y + eu y + f u = g (11) u = 0 on ffi where b = exp (xy); c = exp (xy); d = fi (x + y); e = fl (x
Reference: [10] <author> D. Gannon and J. van Rosendale, </author> " <title> On the impact of communication complexity in the design of parallel algorithms", </title> <journal> IEEE Trans. Comp., </journal> <volume> C-33(12):1180-1194, </volume> <year> 1984. </year>
Reference-contexts: The degree of parallelism of the algorithm is of the order of the grid points, but the best complexity that can be achieved for each step is O (log n) using cyclic reduction in both directions to solve the tridiagonal systems <ref> [12, 10] </ref>. Other strategies exist which combine divide-and-conquer tridiagonal solvers and cyclic reduction. 3 Block versions of ADI Since H and V depend on the finite difference discretizations of original PDEs, the classical ADI is not defined for FE matrices.
Reference: [11] <author> S. L. Johnsson, </author> " <title> Solving narrow banded systems on ensemble architectures", </title> <journal> ACM, TOMS, </journal> <volume> 11(3), </volume> <year> 1985. </year>
Reference-contexts: Further speed-ups can also be achieved. Although the matrices H (k) and V (k) are not tridiagonal for k &gt; 1, they are banded with a bandwidth of 2k + 1 and parallel banded solvers <ref> [11] </ref> can be used for each subdomain independently to yield higher speed-ups. * If we assume an exact LU factorization is used to solve (9), we expect that the cost will be of the order k times that of the classical ADI.
Reference: [12] <author> S. L. Johnsson, Y. Saad, and M. H. Schultz. </author> " <title> The alternating direction algorithm on multiprocessors", </title> <journal> SIAM J. Sci. Statist. Comp, </journal> <volume> 8 </volume> <pages> 686-700, </pages> <year> 1987. </year>
Reference-contexts: The degree of parallelism of the algorithm is of the order of the grid points, but the best complexity that can be achieved for each step is O (log n) using cyclic reduction in both directions to solve the tridiagonal systems <ref> [12, 10] </ref>. Other strategies exist which combine divide-and-conquer tridiagonal solvers and cyclic reduction. 3 Block versions of ADI Since H and V depend on the finite difference discretizations of original PDEs, the classical ADI is not defined for FE matrices.
Reference: [13] <author> W. J. Layton and P. J. Rabier. </author> <title> "Peaceman Rachford procedure and domain decomposition for finite element problems", </title> <type> Technical report, </type> <institution> University of Pittsburgh, </institution> <address> Pittsburgh, PA, </address> <year> (1991). </year>
Reference: [14] <author> J. A. Meijerink and H. A. van der Vorst, </author> " <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix", </title> <journal> Math. Comp., </journal> <volume> 31(137) </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: 1 Introduction Iterative solutions of large sparse linear systems by Krylov subspace methods, require the use of preconditioning techniques in order to converge in a reasonable number of iterations. When implemented on parallel computers, standard preconditioners based on incomplete factorization, such as ILU <ref> [14] </ref> or the more elaborate versions such as ILUT [17, 24], using a wavefront or level-scheduling approach realize a reasonable speed-up on vector computers or parallel computers with a small number of processors [1, 2, 5].
Reference: [15] <author> D. Peaceman and H. Rachford, </author> <title> "The Numerical Solution of Elliptic and Parabolic Differential Equations", </title> <journal> Journal of SIAM., </journal> <volume> Vol. 3, </volume> <pages> pp. 28-41, </pages> <year> 1955 </year>
Reference-contexts: In addition we would like the method to be applicable to Finite Element applications, possibly for 3-D problems, just as in a domain decomposition approach. 2 Classical ADI Methods The ADI method was introduced by Peaceman and Rachford <ref> [15] </ref> in 1955, to solve the discretized boundary value problems for elliptic and parabolic PDEs. <p> For those cases when HV = V H, a satisfactory theory does exist [6]. For the model problem with a single optimal the time complexity is O (n 3 ). In <ref> [15] </ref> a procedure is described to reduce the time complexity to O (n 2 log n) with a sequence of parameters. For 3-D problems Douglas [7] proposed a variant of the classical ADI, which has better convergence behavior than the classical ADI. <p> In fact for the model problem there is a complete theory on how to select the parameters optimally. This makes the algorithm converge within discretization errors, in a number of steps of the order of O (n 1=l ) with a fixed l <ref> [15] </ref>. If HV = V H, and H and V are symmetric, positive definite, there exists satisfactory theory based on common eigenvectors [6]. If V and H commute but are not symmetric with possibly complex eigenvalues, G.

Reference: [17] <author> Y. Saad, "ILUT: </author> <title> A Dual Threshold Incomplete LU Factorization", </title> <address> UMSI 92/38, </address> <year> 1992 </year>
Reference-contexts: When implemented on parallel computers, standard preconditioners based on incomplete factorization, such as ILU [14] or the more elaborate versions such as ILUT <ref> [17, 24] </ref>, using a wavefront or level-scheduling approach realize a reasonable speed-up on vector computers or parallel computers with a small number of processors [1, 2, 5].
Reference: [18] <author> Y. Saad, </author> <title> "Highly Parallel Preconditioners for General Sparse Matrices", </title> <institution> Technical Report -, University of Minnesota, Army High Performance Computing Research Center, Minneapolis, Minnesota, </institution> <year> 1992. </year>
Reference-contexts: Similarly, standard relaxation or block relaxation techniques have often provided easy-to-implement and yet reasonably efficient preconditioners. However, insufficient work has been done on the parallel implementations of these techniques. In <ref> [18] </ref>, SOR and SSOR and multicoloring techniques were combined and compared with `good' ILUT implementations. The main result in [18] is that these types of techniques can be quite efficient, and far superior to the standard preconditioners on some problems, provided a multi-step approach is used, namely provided that k &gt; <p> Similarly, standard relaxation or block relaxation techniques have often provided easy-to-implement and yet reasonably efficient preconditioners. However, insufficient work has been done on the parallel implementations of these techniques. In <ref> [18] </ref>, SOR and SSOR and multicoloring techniques were combined and compared with `good' ILUT implementations. The main result in [18] is that these types of techniques can be quite efficient, and far superior to the standard preconditioners on some problems, provided a multi-step approach is used, namely provided that k &gt; 1 steps of SOR or SSOR are taken at each preconditioning operation instead of just one as is usually
Reference: [19] <author> Y. Saad, " ILUM: </author> <title> A Parallel Multi-elimination ILU Preconditioner for General Sparse Matrices", </title> <institution> Technical Report -, University of Minnesota, Army High Performance Computing Research Center, Minneapolis, Minnesota, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: This `two-coloring' often referred to as a red-black or checkerboard ordering, can be generalized to arbitrary sparse matrices by using multi-coloring and the generalizations of the standard ILU techniques based on such ideas can be easily derived <ref> [19] </ref>.
Reference: [20] <author> G. Starke, </author> <title> "Optimal Alternating Direction Implicit Parameters for Nonsymmetric Systems of Linear Equations", </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> Vol. 28, No. 5, </volume> <pages> pp. 1431-1445, </pages> <year> 1991 </year>
Reference: [21] <author> R. Varga, </author> <title> Matrix Iterative Analysis, </title> <publisher> Prentice-Hall, </publisher> <address> New York, </address> <year> 1962 </year>
Reference-contexts: Peaceman Rachford (PR) ADI (H + i I)u i+1=2 = (V i I)u i + b (4) 2 where the i 's are positive parameters. The ADI method, applied to positive definite systems, was extensively studied in the 1950s and 1960s, see, e.g, the books of Varga <ref> [21] </ref> and Wachspress [22]. In this case H and V have real eigenvalues and the following is a summary of the main ADI results in this situation. 1.
Reference: [22] <author> E. </author> <title> Wachspress, Iterative Solution of Elliptic Systems, </title> <publisher> Prentice-Hall, </publisher> <address> New York, </address> <year> 1966 </year>
Reference-contexts: The ADI method, applied to positive definite systems, was extensively studied in the 1950s and 1960s, see, e.g, the books of Varga [21] and Wachspress <ref> [22] </ref>. In this case H and V have real eigenvalues and the following is a summary of the main ADI results in this situation. 1. Any stationary iteration ( i = c &gt; 0, for all i) is convergent if H and V are symmetric positive definite. 2.
Reference: [23] <author> D. Young, </author> <title> Iterative Solution of Large Linear Systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: This implies that there no longer exists a common set of eigenvectors, and this makes the analysis difficult. If H and V are symmetric, then we still could derive an upper bound for the spectral radius <ref> [23] </ref>. fl If H and V are not symmetric, which is true in the presence of convection terms in the underlying PDE, then the above upper bounds no longer hold, since kAk 2 is not equal to the spectral radius of A. <p> Also assume that we know a; b such that a (H); (V ) b where (A) denotes the spectrum of the matrix A. Then the optimal is given by p ab <ref> [23] </ref>. For model problem a h 2 ; b 2, hence p 2h. In other words the optimal is linearly proportional to h.
Reference: [24] <author> Z. Zlatev, </author> " <title> Use of iterative refinement in the solution of sparse linear systems", </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 </volume> <pages> 381-399, </pages> <year> 1982. </year> <booktitle> FDM Problem 3 with N = 128 2 ; fl = 50, </booktitle> <volume> fi=1 1 49 48 48 45 43 38 35 35 36 4 16 15 16 18 19 22 27 30 33 Table 9: </volume> <booktitle> Iteration of GMRES(10)-ADI(k,6) for various values of and k 12 </booktitle>
Reference-contexts: When implemented on parallel computers, standard preconditioners based on incomplete factorization, such as ILU [14] or the more elaborate versions such as ILUT <ref> [17, 24] </ref>, using a wavefront or level-scheduling approach realize a reasonable speed-up on vector computers or parallel computers with a small number of processors [1, 2, 5].
References-found: 22

