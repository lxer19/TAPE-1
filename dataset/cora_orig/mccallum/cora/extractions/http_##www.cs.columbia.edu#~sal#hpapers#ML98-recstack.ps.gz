URL: http://www.cs.columbia.edu/~sal/hpapers/ML98-recstack.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: wfan@cs.columbia.edu  
Phone: Phone  
Title: Recursive-Stacking To Improve The Accuracy of Combined Classifiers Author(s) with address(es):  
Author: David W. Fan Salvatore J. Stolfo Philip K. Chan 
Keyword: Conflict, Stacking, Recursive Stacking  
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Note: Title:  Email address of contact author:  number of contact author: 212-939-7078 Multiple submission statement (if applicable): N/A  
Abstract: 250 word maximum): We analyze the mechanism of stacking and point out the conflict problem. Two methods to reduce conflicts are discussed and their equivalence is established. We propose the Recursive-Stacking Algorithm to solve conflicts and demonstrate its effectiveness using two artificial data sets and two real world data sets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali. </author> <title> On Explaining Degree of Error Reduction due to Combining Multiple Decision Trees. Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon. </address>
Reference-contexts: Algorithms with post-learning pruning or using summarization may remove statistically insignificant conflict types completely, but they do not solve the conflict problem. Recent publications on stacking <ref> [1, 2] </ref> have concentrated effort on using metrics (coverage, diversity, specialty, correlated error) to choose classifiers for combining and to explain how stacking increases accuracy. Here, we are concerned with the mechanism of stacking to see what contributes to accuracy increase and what prevents it. Conflicts is not a metric.
Reference: [2] <author> C. Brodley and T. </author> <title> Lane Creating and Exploiting Coverage and Diversity Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon </address>
Reference-contexts: Algorithms with post-learning pruning or using summarization may remove statistically insignificant conflict types completely, but they do not solve the conflict problem. Recent publications on stacking <ref> [1, 2] </ref> have concentrated effort on using metrics (coverage, diversity, specialty, correlated error) to choose classifiers for combining and to explain how stacking increases accuracy. Here, we are concerned with the mechanism of stacking to see what contributes to accuracy increase and what prevents it. Conflicts is not a metric.
Reference: [3] <author> W. Buntime and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning, </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: For experiments on all 4 sets of data, we fixed to be 0.4. We used RIPPER, CART, ID3 and C4.5 as the base learners. RIPPER was obtained from William Cohen [6]. CART, ID3 and C4.5 are part of the IND package <ref> [3] </ref>. 3.2 Boolean Artificial Data Set We generated an artificial data set of 15 boolean variables. The data item is 1 if 5, 6, 7 or 8 variables out of 15 are 1. We call this set Boolean.5678.
Reference: [4] <author> P. </author> <title> Chan An Extensible Meta-learning Approach for Scalable and Accurate Inductive Learning Ph.D. </title> <type> Thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, New York, </address> <year> 1996 </year>
Reference-contexts: The meta-classifier's prediction M C ((C 1 (x); : : : ; C t (x)) is the final outcome. The effectiveness of stacking to increase accuracy has been widely reported <ref> [4, 14, 15] </ref>. Stacking is also a feasible approach to learn over inherently distributed data [4]. The idea of stacking has been deployed and implemented as agents to attack credit card fraud [13]. <p> The meta-classifier's prediction M C ((C 1 (x); : : : ; C t (x)) is the final outcome. The effectiveness of stacking to increase accuracy has been widely reported [4, 14, 15]. Stacking is also a feasible approach to learn over inherently distributed data <ref> [4] </ref>. The idea of stacking has been deployed and implemented as agents to attack credit card fraud [13]. Experiments in different domains have shown accuracy increases from 0.5% to 3% over any of the combined base classifiers.
Reference: [5] <author> R.T. </author> <title> Clemen Combining Forcasts: A Review and Annotated Bibliography. </title> <journal> International Journal of Forcasting, </journal> <volume> 5 </volume> <pages> 559-583. </pages>
Reference-contexts: Empirical studies in the last 5 years have shown that integrating multiple learned models helps to increase both accuracy and efficiency. There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) <ref> [5, 11] </ref> , gating function [12] and stacking [15]. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined. Following Wolpert [15], the general scheme for stacking works as follows.
Reference: [6] <author> W. </author> <title> Cohen Fast Effective Rule Induction In Proc. </title> <booktitle> Twelfth Intl. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufman </publisher>
Reference-contexts: For experiments on all 4 sets of data, we fixed to be 0.4. We used RIPPER, CART, ID3 and C4.5 as the base learners. RIPPER was obtained from William Cohen <ref> [6] </ref>. CART, ID3 and C4.5 are part of the IND package [3]. 3.2 Boolean Artificial Data Set We generated an artificial data set of 15 boolean variables. The data item is 1 if 5, 6, 7 or 8 variables out of 15 are 1. We call this set Boolean.5678.
Reference: [7] <author> T. </author> <title> Dietterich Machine Learning Research: Four Current Directions. </title> <note> submitted for publication, </note> <year> 1997 </year>
Reference-contexts: 1 Introduction Integrating multiple learned models has been identified as one of the current research directions in machine learning <ref> [7] </ref>. Empirical studies in the last 5 years have shown that integrating multiple learned models helps to increase both accuracy and efficiency. There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) [5, 11] , gating function [12] <p> multiple learned models has been identified as one of the current research directions in machine learning <ref> [7] </ref>. Empirical studies in the last 5 years have shown that integrating multiple learned models helps to increase both accuracy and efficiency. There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) [5, 11] , gating function [12] and stacking [15]. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined.
Reference: [8] <author> C. </author> <note> Elkan Boosted Naive Bayes submitted for publication, </note> <year> 1997 </year>
Reference-contexts: The reason appears to be that the data sets from different months are not uniform in both size and fraud distribution. The result of stacking may already be very close to the best we can obtain, which does not leave much room for improvement. We ran Boosted Naive Bayes <ref> [8] </ref> on the same data sets with up to 10 rounds of boosting. The result by boosting and stacking are very close on average. 4 Discussion Combining a large number of classifiers in Recursive Stacking will be a problem since the number of conflict types is 2 t .
Reference: [9] <author> D. Fan,P. Chan and S. </author> <title> Stolfo A Comparative Evaluation of Combiner and Stacked Generalization Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon </address>
Reference-contexts: In this paper, we will discuss a problem called conflicts that hinders the accuracy improvement of stacking. We propose and discuss some methods to solve it. Conflicts were first discussed in <ref> [9, 14] </ref>. In Section 2, we analyze the mechanism of stacking and show the conflict problem. We will discuss two seemingly different approaches, and their intrinsic equivalence. The Recursive-Stacking Algorithm to reduce conflicts is proposed. In Section 3, we analyze experimental results.
Reference: [10] <author> D. Fan, P. Chan and S. </author> <note> Stolfo On different aspects of Recursive-Stacking to be submitted </note>
Reference-contexts: Here, we are concerned with the mechanism of stacking to see what contributes to accuracy increase and what prevents it. Conflicts is not a metric. It is the cause of poor performance of stacking. A companion paper <ref> [10] </ref> discusses the relationship between conflicts and these metrics both conceptually and statistically. 2.2 Approaches We propose two ways to solve the problem of conflicts: conflict resolver and adding base classifiers. <p> Redundant nodes will not improve accuracy since the decisions are the same as the parent conflict. We remove any such nodes from the tree in a depth first search manner after training. 8 There are other minor details about this algorithm left for a companion report <ref> [10] </ref>. 3 Experiments and Results We are interested in finding out answers to the following questions: 1. Will the Recursive-Stacking algorithm improve accuracy over stacking? 2. How deep will the tree grow before we can improve accuracy? 3. How deep will the tree grow before overfitting appears? 4.
Reference: [11] <author> S. </author> <title> Hashem Optimal Linear Combination of Neural Networks. </title> <type> Phd Thesis, </type> <institution> Purdue University, School of Industrial Engineering, Lafayette, IN, </institution> <year> 1993 </year>
Reference-contexts: Empirical studies in the last 5 years have shown that integrating multiple learned models helps to increase both accuracy and efficiency. There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) <ref> [5, 11] </ref> , gating function [12] and stacking [15]. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined. Following Wolpert [15], the general scheme for stacking works as follows.
Reference: [12] <author> M.I. Jordan and R.A. </author> <title> Jacob Hierarchical Mixtures of Experts and the EM Algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2), </volume> <month> p.181-p.214. </month>
Reference-contexts: Empirical studies in the last 5 years have shown that integrating multiple learned models helps to increase both accuracy and efficiency. There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) [5, 11] , gating function <ref> [12] </ref> and stacking [15]. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined. Following Wolpert [15], the general scheme for stacking works as follows.
Reference: [13] <author> S. Stolfo, A. Prodromidis, S. Tselepsis, W. Lee, D. Fan and P. Chan JAM: </author> <title> Java Agents for Meta-learning over Distributed Databases. In Prod. </title> <booktitle> Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: The effectiveness of stacking to increase accuracy has been widely reported [4, 14, 15]. Stacking is also a feasible approach to learn over inherently distributed data [4]. The idea of stacking has been deployed and implemented as agents to attack credit card fraud <ref> [13] </ref>. Experiments in different domains have shown accuracy increases from 0.5% to 3% over any of the combined base classifiers. In this paper, we will discuss a problem called conflicts that hinders the accuracy improvement of stacking. We propose and discuss some methods to solve it.
Reference: [14] <author> S. Stolfo, D. Fan, W. Lee, A. Prodromidis and P. </author> <title> Chan Credit Card Fraud Detection Using Meta-learning: Issues and Initial Results. </title> <booktitle> In Working Notes AAAI-97, </booktitle> <year> 1997 </year>
Reference-contexts: The meta-classifier's prediction M C ((C 1 (x); : : : ; C t (x)) is the final outcome. The effectiveness of stacking to increase accuracy has been widely reported <ref> [4, 14, 15] </ref>. Stacking is also a feasible approach to learn over inherently distributed data [4]. The idea of stacking has been deployed and implemented as agents to attack credit card fraud [13]. <p> In this paper, we will discuss a problem called conflicts that hinders the accuracy improvement of stacking. We propose and discuss some methods to solve it. Conflicts were first discussed in <ref> [9, 14] </ref>. In Section 2, we analyze the mechanism of stacking and show the conflict problem. We will discuss two seemingly different approaches, and their intrinsic equivalence. The Recursive-Stacking Algorithm to reduce conflicts is proposed. In Section 3, we analyze experimental results. <p> These data sets were provided for our research in fraud and intrusion detection. The classification of the data is either legitimate or fraudulent. For a detailed description of the data schema, refer to <ref> [14] </ref>. From each bank, we obtained 0.5 million data spanning a whole year. The First Union data was not uniformally sampled for each month. The percentage of fraud ranges from 4% to over 20% over each month and the size of data for each month varies.
Reference: [15] <author> D. </author> <title> Wolpert Stacked Generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2), p.241-p.260. </volume> <pages> 15 </pages>
Reference-contexts: There are different techniques, but within the scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) [5, 11] , gating function [12] and stacking <ref> [15] </ref>. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined. Following Wolpert [15], the general scheme for stacking works as follows. <p> scope of combining classifiers only [7], there are mainly three proposed methods: voting (unweighted and weighted) [5, 11] , gating function [12] and stacking <ref> [15] </ref>. The goal of stacking is to learn a combining classifier that reflects the correlation of predictions of underlying base classifiers that are combined. Following Wolpert [15], the general scheme for stacking works as follows. We have t different algorithms A 1 ,. . . ,A t and a set S of training examples f (x 1 ; y 1 ); : : : ; (x m ; y m )g. <p> The meta-classifier's prediction M C ((C 1 (x); : : : ; C t (x)) is the final outcome. The effectiveness of stacking to increase accuracy has been widely reported <ref> [4, 14, 15] </ref>. Stacking is also a feasible approach to learn over inherently distributed data [4]. The idea of stacking has been deployed and implemented as agents to attack credit card fraud [13]. <p> prediction C i (x) of classifier C i on data item (x; y) is abbreviated in lowercase c i . ((c 1 ; c 2 ; : : : ; c t ); y) is thus a meta-level training data item (level 1 training data in the terminology of Wolpert <ref> [15] </ref>). The feature (c 1 ; c 2 ; : : : ; c t ) is a vector of predictions generated by base learning algorithms on an unseen item (x; y), and y is the true label (either 1 or 0).
References-found: 15

