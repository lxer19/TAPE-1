URL: http://www.icsi.berkeley.edu/~phlipp/nuclear.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~phlipp/phlipp.publ.html
Root-URL: http://www.icsi.berkeley.edu
Title: Programming Parallel Supercomputers achieve supercomputer performance in real applications. Instead, applications will have to be
Author: Walter F. Tichy and Michael Philippsen puters. 
Keyword: Automatic parallelization of sequential programs  
Note: will not  
Address: D-7500 Karlsruhe, F.R.G.  
Affiliation: Universitat Karlsruhe Fakultat fur Informatik  
Abstract: email: (tichy j phlipp)@ira.uka.de This paper appeared in: H. K uster and W. Werner, editors, Proceedings of International Conference on Mathematical Methods and Supercomputing in Nuclear Applications, volume 2, pages 3-9, Karlsruhe, Germany, April 19-23, 1993 Abstract This paper discusses future directions in tools and techniques for programming parallel supercomputers. We base the discussion on two important observations: We comment on High Performance Fortran (HPF) and conclude that HPF will achieve machine-independence to an initially satisfactory degree, but that another language revision can be expected. Machine-independence does not imply poor performance. We present evidence that explicitly parallel, machine-independent, and problem-oriented programs can be translated automatically into parallel machine code that is competitive in performance with handwritten code. Furthermore, we show that interactive, source-level and problem-oriented debugging of explicitly parallel program has recently become a reality. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Selim G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: of Modula-2* programs, since the translation of the general FORALL statement of Modula-2* is more demanding than translation of less expressive forms of data parallelism, e.g. vector parallelism or the FORALL of HPF. 3.1 The Compiler Benchmark At the moment, our benchmark suite consists of 12 problems collected from literature <ref> [1, 5, 11, 8, 4] </ref>. The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 .
Reference: [2] <institution> American National Standards Institute, Inc., </institution> <address> Washington, D.C. </address> <month> ANSI, </month> <title> Programming Language Fortran Extended (Fortran 90). ANSI X3.198-1992, </title> <year> 1992. </year>
Reference-contexts: These details typically change from one computer model to the next. Any program that depends on such details will either have to be rewritten for every new generation of supercomputers or be thrown away after a short time of use. 2 Whither Fortran? Fortran90 <ref> [2] </ref> provides instructions geared for vector computers, but lacks facilities for SIMD or MIMD parallel machines. Consequently, vendors were forced to extend either Fortran77 or Fortran90 with new language constructs for parallelism. However, these constructs are machine-specific and therefore lock users into a particular vendor.
Reference: [3] <author> Siddhartha Chatterjee, John R. Gilbert, Robert Schreiber, and Shang-Hua Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proc. of the 20th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 16-28, </pages> <address> Charleston, SC, </address> <month> January 10-13, </month> <year> 1993. </year>
Reference-contexts: Again, the compiler may be better equipped to solve this optimization problem, especially because the compiler already has to solve the general alignment and distribution problem for program-generated temporary variables. First promising results <ref> [12, 17, 6, 3] </ref> indicate that the alignment and distribution problem may be compiler-solvable. The FORALL statement in HPF is severely limited. For instance, it is not possible to place subroutine calls, IF statements, or other FORALL statements into its body.
Reference: [4] <author> John T. Feo, ed., </author> <title> A Comparative Study of Parallel Programming Languages: The Salishan Problems. </title> <publisher> Elsevier Science Publishers, Holland, </publisher> <year> 1992. </year>
Reference-contexts: of Modula-2* programs, since the translation of the general FORALL statement of Modula-2* is more demanding than translation of less expressive forms of data parallelism, e.g. vector parallelism or the FORALL of HPF. 3.1 The Compiler Benchmark At the moment, our benchmark suite consists of 12 problems collected from literature <ref> [1, 5, 11, 8, 4] </ref>. The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . <p> Find all integers of the form a i b j c k : : : n in increasing order and without duplicates. * Doctor's Office: Simulate the following queuing problem from <ref> [4] </ref>: a set of patients, a set of doctors, and a receptionist are given. Patients become sick at random, are assigned to one of the doctors by the receptionist, and treated in a random amount of time.
Reference: [5] <author> Alan Gibbons and Wojciech Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: of Modula-2* programs, since the translation of the general FORALL statement of Modula-2* is more demanding than translation of less expressive forms of data parallelism, e.g. vector parallelism or the FORALL of HPF. 3.1 The Compiler Benchmark At the moment, our benchmark suite consists of 12 problems collected from literature <ref> [1, 5, 11, 8, 4] </ref>. The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 .
Reference: [6] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proc. of the 6th Distributed Memory Computing Conference, </booktitle> <pages> pages 43-50, </pages> <address> Portland, Oregon, </address> <month> April 28 May 1, </month> <year> 1991. </year>
Reference-contexts: Again, the compiler may be better equipped to solve this optimization problem, especially because the compiler already has to solve the general alignment and distribution problem for program-generated temporary variables. First promising results <ref> [12, 17, 6, 3] </ref> indicate that the alignment and distribution problem may be compiler-solvable. The FORALL statement in HPF is severely limited. For instance, it is not possible to place subroutine calls, IF statements, or other FORALL statements into its body.
Reference: [7] <author> Stefan U. Hangen. </author> <title> Ein symbolischer X-Windows Debugger fur Modula-2*. </title> <type> Master's thesis, </type> <institution> University of Karlsruhe, Dept. of Informatics, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: These are all taken care of by the compiler. A high-level, parallel language shifts the focus of debugging from machine-dependent to problem dependent issues, such as visualization of data, activity tracking, and performance profiling <ref> [7] </ref>. We have built an interactive, source-oriented debugger and data visualizer for parallel programs written in Modula-2* called MSDB. Figure 1 shows a screendump presenting all of MSDB's display features. MSDB provides the usual debugger features such as setting and examining variables, setting breakpoints and stepping through the program.
Reference: [8] <author> Philip J. Hatcher, Michael J Quinn, Anthony J. Lapadula, Ray J. Anderson, and Robert R. Jones. </author> <title> Dataparallel C: A SIMD programming language for multicomputers. </title> <booktitle> In Proc. of the 6th Distributed Memory Computer Conference, </booktitle> <pages> pages 91-98, </pages> <address> Portland, Oregon, </address> <month> April 28 May 2, </month> <year> 1991. </year>
Reference-contexts: of Modula-2* programs, since the translation of the general FORALL statement of Modula-2* is more demanding than translation of less expressive forms of data parallelism, e.g. vector parallelism or the FORALL of HPF. 3.1 The Compiler Benchmark At the moment, our benchmark suite consists of 12 problems collected from literature <ref> [1, 5, 11, 8, 4] </ref>. The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 .
Reference: [9] <author> Ernst A. Heinz. </author> <title> Automatische Elimination von Synchronisationsbarrieren in synchronen FORALLs. </title> <type> Master's thesis, </type> <institution> University of Karl-sruhe, Deptartment of Informatics, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: The unrestrained orthogonality of Modula-2* (such as mixing data and control parallelism in one program) makes it possible to write parallel programs that are both easy to understand and machine-independent. However, the orthogonality makes great demands on compiler technology. Our compiler research <ref> [16, 9, 13, 15] </ref> indicates that machine-independent, explicitly parallel programs can indeed be compiled into parallel machine code that is competitive in performance with handwritten code.
Reference: [10] <author> High Performance Fortran (HPF): </author> <title> Language specification. </title> <type> Technical report, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <year> 1992. </year>
Reference-contexts: Consequently, vendors were forced to extend either Fortran77 or Fortran90 with new language constructs for parallelism. However, these constructs are machine-specific and therefore lock users into a particular vendor. High Performance Fortran (HPF) <ref> [10] </ref>, however, provides a reasonable chance of achieving machine-independence. The standardization process is fortuitously helped by a consolidation trend among parallel computer architectures to the extent that programmers will be able to rely on a few rules of thumb to hold across a spectrum of parallel computers.
Reference: [11] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: of Modula-2* programs, since the translation of the general FORALL statement of Modula-2* is more demanding than translation of less expressive forms of data parallelism, e.g. vector parallelism or the FORALL of HPF. 3.1 The Compiler Benchmark At the moment, our benchmark suite consists of 12 problems collected from literature <ref> [1, 5, 11, 8, 4] </ref>. The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 .
Reference: [12] <author> Kathleen Knobe and Joan D. Lukas. </author> <title> Data optimization and its effect on communication costs in MIMD Fortran code. </title> <booktitle> In Fifth SIAM Conference on Parallel Processing in Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Again, the compiler may be better equipped to solve this optimization problem, especially because the compiler already has to solve the general alignment and distribution problem for program-generated temporary variables. First promising results <ref> [12, 17, 6, 3] </ref> indicate that the alignment and distribution problem may be compiler-solvable. The FORALL statement in HPF is severely limited. For instance, it is not possible to place subroutine calls, IF statements, or other FORALL statements into its body.
Reference: [13] <institution> Pawel Lukowicz. Code-Erzeugung fur Modula-2* fur verschiedene Maschinenarchitekturen. </institution> <type> Master's thesis, </type> <institution> University of Karlsruhe, Dept. of In-formatics, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: The unrestrained orthogonality of Modula-2* (such as mixing data and control parallelism in one program) makes it possible to write parallel programs that are both easy to understand and machine-independent. However, the orthogonality makes great demands on compiler technology. Our compiler research <ref> [16, 9, 13, 15] </ref> indicates that machine-independent, explicitly parallel programs can indeed be compiled into parallel machine code that is competitive in performance with handwritten code.
Reference: [14] <author> MasPar Computer Corporation. </author> <title> MasPar Parallel Application Language (MPL) Reference Manual, </title> <month> September </month> <year> 1990. </year>
Reference-contexts: The problems are given briefly in the appendix. For each problem, we implemented the same algorithms in Modula-2*, in sequential C, and in MPL 1 . Run-times were measured on a 16K MasPar MP-1 (SIMD) 1 MPL <ref> [14] </ref> is a data-parallel extension of C designed for the MasPar MP series. In MPL, the number of available processors, the SIMD architecture of the machine, its 2D mesh-connected processor network, and the distributed memory are visible.
Reference: [15] <author> Michael Philippsen. </author> <title> Automatic data distribution for nearest neighbor networks. </title> <booktitle> In Frontiers '92:The Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 178-185, </pages> <address> Mc Lean, Virginia, </address> <month> October 19-21, </month> <year> 1992. </year>
Reference-contexts: The unrestrained orthogonality of Modula-2* (such as mixing data and control parallelism in one program) makes it possible to write parallel programs that are both easy to understand and machine-independent. However, the orthogonality makes great demands on compiler technology. Our compiler research <ref> [16, 9, 13, 15] </ref> indicates that machine-independent, explicitly parallel programs can indeed be compiled into parallel machine code that is competitive in performance with handwritten code.
Reference: [16] <author> Michael Philippsen and Walter F. Tichy. </author> <title> Modula-2* and its compilation. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <year> 1991, </year> <pages> pages 169-183. </pages> <publisher> Springer Verlag, Lecture Notes in Computer Science 591, </publisher> <year> 1992. </year>
Reference-contexts: The unrestrained orthogonality of Modula-2* (such as mixing data and control parallelism in one program) makes it possible to write parallel programs that are both easy to understand and machine-independent. However, the orthogonality makes great demands on compiler technology. Our compiler research <ref> [16, 9, 13, 15] </ref> indicates that machine-independent, explicitly parallel programs can indeed be compiled into parallel machine code that is competitive in performance with handwritten code.
Reference: [17] <author> J. Ramanujam and P. Sadayappan. </author> <title> Access based data decomposition for distributed memory machines. </title> <booktitle> In Proc. of the 6th Distributed Memory Computing Conference, </booktitle> <pages> pages 196-199, </pages> <address> Portland, Oregon, </address> <month> April 28 May 1, </month> <year> 1991. </year>
Reference-contexts: Again, the compiler may be better equipped to solve this optimization problem, especially because the compiler already has to solve the general alignment and distribution problem for program-generated temporary variables. First promising results <ref> [12, 17, 6, 3] </ref> indicate that the alignment and distribution problem may be compiler-solvable. The FORALL statement in HPF is severely limited. For instance, it is not possible to place subroutine calls, IF statements, or other FORALL statements into its body.
Reference: [18] <author> Guy L. Steele. </author> <title> High Performance Fortran: Status report. </title> <booktitle> In Proc. of the 1993 Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <pages> pages 1-4, </pages> <address> Boulder, CO, </address> <month> September 30 - October 2, </month> <year> 1992, </year> <month> January </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices 28(1). </journal>
Reference-contexts: These rules of thumb make it possible to expect good performance on a reasonably large subset of parallel ma chines without reprogramming. Steele writes about the purpose of HPF in <ref> [18] </ref>: The goal of High Performance Fortran (HPF) is to extend Fortran90 to provide additional support for data parallel programming (defined as a style of programming with a single conceptual thread of control, a global name space, and loosely synchronous parallel computation) to facilitate top performance on MIMD and SIMD computers <p> Alignment and distribution are critical for distributed memory machines, where the cost of accessing nonlocal memory is high. Since compiler technology has not yet solved the problem of deriving alignment and distribution automatically, HPF leaves the decision to the programmer. A simple example, taken from reference <ref> [18] </ref>, follows. Suppose A is an array of dimensions 1000 x 1000, while B has dimensions 998 x 998. Assume elements A (I,J) interact with elements B (I+1,J+1), so A and B should be aligned accordingly.
Reference: [19] <author> Walter F. Tichy and Christian G. Herter. </author> <title> Modula-2*: An extension of Modula-2 for highly parallel, portable programs. </title> <type> Technical Report No. 4/90, </type> <institution> University of Karlsruhe, Dept. of Infor-matics, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The language we use is Modula-2* <ref> [19] </ref>, an extension of Modula-2. We chose Modula-2, because it is both more modern and smaller than Fortran. The extensions are a superset of those in HPF, however. Just as HPF, Modula-2* provides a single, global name space with potentially non-uniform access cost.
Reference: [20] <author> Walter F. Tichy, Michael Philippsen, Ernst A. Heinz, and Paul Lukowicz. </author> <title> From Modula-2* to efficient parallel code. </title> <type> Technical Report No. 21/92, </type> <institution> University of Karlsruhe, Dept. of Informatics, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: We use optimized sequential libraries wherever possible. The C code does not contain dirty "hacks". In the following, we first present performance results and then compare the resource consumption of these three program classes. We only summarize the results. For detailed information see <ref> [20] </ref>. 3.2 Performance Results For different problem sizes we measured the run-time of each test program on a 16K MasPar MP-1 and a SparcStation-1. Time was measured with the high-resolution DPU timer on the MasPar and the UNIX clock function on the SparcStation (sum of user and system time).
Reference: [21] <author> Skef Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Dept. of Computer Science, Pittsburg, </institution> <address> PA, </address> <month> May </month> <year> 1991. </year> <title> which are shown in a range visualizer. A visualization of the processes as well as the (grouped) dynamic activation tree and a profile graph can also be seen. The left window shows the Modula-2* source with a program counter and a breakpoint, the right window displays the corresponding C source. </title>
Reference-contexts: As compiler technology improves, it may be possible to omit the alignment and distribution directives, because the compiler may be able to generate them and even take the target machine architecture into account. For instance, Wholey <ref> [21] </ref> shows that the best mapping varies with the interconnection structure of the machine, the size of the machine, and the problem size. Therefore, a good compiler might actually override a directive with a better choice for a given architecture and problem size.
References-found: 21

