URL: http://http.cs.berkeley.edu/~ivory/mthesis/mthesis.ps
Refering-URL: http://http.cs.berkeley.edu/~ivory/ResearchInterests.html
Root-URL: 
Title: Modeling Communication Layers in Portable Parallel Applications for Distributed-Memory Multiprocessors  by  
Author: Melody Y. Ivory 
Date: 1996  
Note: Copyright  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David E. Culler Alan M. Mainwaring. </author> <title> Active Messages: Organization and applications programming interface. </title> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: Implementations of MPI, such as MPICH [3], also address the performance penalty of previous PVM releases [35, 42]. The Berkeley community has also addressed these porting and performance issues through standardization of Active Messages (AM) [59] with the Application Programming (AM API) <ref> [1] </ref> and Generic Active Messages (GAM) [19] interfaces. These interfaces specify a library of hardware-independent request and reply routines. The use of portable communication layers, such as MPI, PVM and GAM, substantially reduce application developers' porting efforts. <p> TAM is a compilation target for implicitly parallel languages such as Id90 [50]. There is ongoing work to improve Active Messages at Berkeley and Cornell. This work involves expanding AM to new platforms and implementing portable AM versions, GAM [19] and AM API <ref> [1] </ref>. <p> Native implementations are available for the CM-5 [59], Paragon [44], Meiko [54], SP2 [58], as well as SUN and HP NOWs [49]. 1.4.2 AM API The AM Applications and Programming Interface (API) <ref> [1] </ref> is an expansion of Active Messages to support a broader range of parallel applications such as client/server programs, operating systems and file systems. To achieve this goal, future AM implementations will incorporate new naming, protection, communication error and fault models.
Reference: [2] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, and R. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles. ACM Transactions on Computer Systems, </booktitle> <year> 1995. </year>
Reference-contexts: This work involves expanding AM to new platforms and implementing portable AM versions, GAM [19] and AM API [1]. Work is also underway at Berkeley to implement the MPI 7 standard using GAM [45] and to use GAM in the xFS <ref> [2] </ref> parallel file system. 1.4.1 GAM Early AM parallel applications faced the same portability problems previously mentioned; this lead to the specification of the Generic Active Message (GAM) interface [19]. GAM provides a portable AM interface and implementations of this interface are available on an increasing number of machines.
Reference: [3] <author> P. Bridges, N. Doss, W. Gropp, E. Karrels, E. Lusk, and A. Skjellum. </author> <title> Users' Guide to MPICH, A Portable Implementation of MPI, </title> <year> 1995. </year>
Reference-contexts: High Performance Fortran (HPF) [36] was one of the first standardization efforts with the Message Passing Interface (MPI) [29] being the most recent. The MPI Forum introduced the MPI standard which specifies a library of hardware-independent message passing routines. Implementations of MPI, such as MPICH <ref> [3] </ref>, also address the performance penalty of previous PVM releases [35, 42]. The Berkeley community has also addressed these porting and performance issues through standardization of Active Messages (AM) [59] with the Application Programming (AM API) [1] and Generic Active Messages (GAM) [19] interfaces. <p> The MPI standard has been instrumental in less stressful and faster development of portable parallel applications, software libraries, and tools. Currently, there are several hardware-specific implementations of MPI, such as MPI-F for the SP2 [30]. There are also several portable implementations, such as MPICH <ref> [3] </ref>. The MPI standard is still evolving and there is a MPI-2 Forum actively working on improvements [41]. Proposed improvements include: dynamic spawning of processes, active message style communication, C++ and F90/HPF bindings, better collective communication and an object oriented MPI (OOMPI). 1.3.1 MPICH MPICH [3], implemented by researchers at Argonne <p> portable implementations, such as MPICH <ref> [3] </ref>. The MPI standard is still evolving and there is a MPI-2 Forum actively working on improvements [41]. Proposed improvements include: dynamic spawning of processes, active message style communication, C++ and F90/HPF bindings, better collective communication and an object oriented MPI (OOMPI). 1.3.1 MPICH MPICH [3], implemented by researchers at Argonne National Laboratory and Mis-sissippi State University, is a widely used implementation of the MPI standard. It is freely available and runs on a variety of distributed- and shared-memory multiprocessors (CM-5, Paragon, SGI Challenge and SP1/2) as well as NOWs (Alpha, Sun, RS6000 and HP).
Reference: [4] <author> Kenneth Cameron and Lyndon J. Clarke. </author> <title> CRI/EPCC MPI for Cray T3D. </title> <booktitle> In Proceedings of the First European Cray T3D Workshop. </booktitle> <address> EPFL, </address> <year> 1995. </year>
Reference-contexts: Developers at the Edinburgh Parallel Computing Center worked on CHIMP [6]. There are also commercial implementations for the Fujitsu AP1000 [55], Cray T3D <ref> [4] </ref>, and IBM SP1/2 [30]. Researchers at Mississippi State University have been very active in the MPI implementation effort [8]. They collaborated on MPICH, a Microsoft Windows 3.1 version (WinMPI) and produced Unify.
Reference: [5] <institution> Cornell Theory Center. </institution> <note> What is the SP. http://www.tc.cornell.edu/UserDoc/SP/what.is.sp2.html. </note>
Reference-contexts: The bandwidth and startup parameters are average reported numbers for both machines <ref> [5, 44] </ref>. We measured a 28 Mflop rate on the SP2 for summing a 1024-element single precision array using the program in Appendix B.1. This is reflected in the T flop parameter. We repeated this process for the Paragon's T flop parameter.
Reference: [6] <institution> Edinburgh Parallel Computing Center. CHIMP. ftp://ftp.epcc.ed.ac.uk/pub/chimp/release. </institution>
Reference-contexts: MPI-FM [51], developed by researchers at the 6 University of Illinois at Urbana-Champaign and Mississippi State University, is a high-performance port of MPICH for SPARCstation NOWs over Myrinet using Fast Messages [52]. Developers at the Edinburgh Parallel Computing Center worked on CHIMP <ref> [6] </ref>. There are also commercial implementations for the Fujitsu AP1000 [55], Cray T3D [4], and IBM SP1/2 [30]. Researchers at Mississippi State University have been very active in the MPI implementation effort [8]. They collaborated on MPICH, a Microsoft Windows 3.1 version (WinMPI) and produced Unify.
Reference: [7] <institution> Maui High Performance Computing Center. </institution> <note> Introduction to MPL. http://www.mhpcc.edu/training/workshop/html/mpl/MPLIntro.html. </note>
Reference-contexts: Receives are globally-blocking which means that the receive operation does not complete until the data is actually present. All collective communication operations are globally-blocking. As Figure 4.2 shows, there are a number of BLACS implementations on top of VMPLs, including MPL <ref> [7] </ref>, NX [44], and CMMD [59]. The PVM [32] implementation was an earlier attempt at providing portable code, but it exhibited poor performance.
Reference: [8] <institution> Mississippi State Engineering Research Center and Department of Computer Science. MPI activities. </institution> <note> http://www.erc.msstate.edu/mpi/activities.html. </note>
Reference-contexts: Developers at the Edinburgh Parallel Computing Center worked on CHIMP [6]. There are also commercial implementations for the Fujitsu AP1000 [55], Cray T3D [4], and IBM SP1/2 [30]. Researchers at Mississippi State University have been very active in the MPI implementation effort <ref> [8] </ref>. They collaborated on MPICH, a Microsoft Windows 3.1 version (WinMPI) and produced Unify. Unify [9] runs on top of PVM and allows a single program to use both MPI and PVM.
Reference: [9] <institution> Mississippi State Engineering Research Center and Department of Computer Science. Unify. ftp://ftp.erc.msstate.edu/unify. </institution> <month> 49 </month>
Reference-contexts: There are also commercial implementations for the Fujitsu AP1000 [55], Cray T3D [4], and IBM SP1/2 [30]. Researchers at Mississippi State University have been very active in the MPI implementation effort [8]. They collaborated on MPICH, a Microsoft Windows 3.1 version (WinMPI) and produced Unify. Unify <ref> [9] </ref> runs on top of PVM and allows a single program to use both MPI and PVM.
Reference: [10] <institution> Ohio Supercomputing Center. LAM-MPI network parallel computing. </institution> <note> http://www.osc.edu/lam.html. </note>
Reference-contexts: Researchers at the Ohio Supercomputing Center developed Local Area Multicomputer (LAM) MPI <ref> [10] </ref> to support heterogeneous UNIX NOWs. MPI-FM [51], developed by researchers at the 6 University of Illinois at Urbana-Champaign and Mississippi State University, is a high-performance port of MPICH for SPARCstation NOWs over Myrinet using Fast Messages [52]. Developers at the Edinburgh Parallel Computing Center worked on CHIMP [6].
Reference: [11] <author> Chi-Chao Chang, Grzegorz Czajkowski, and Thorsten von Eicken. </author> <title> Design and performance of active messages on the IBM SP-2. </title> <type> Technical report, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1996. </year>
Reference-contexts: Figure 4.3 depicts the communication structure of GLACS on this machine. Since we wrote GAM COMM using a portable AM implementation, we can also use it with native GAM implementations, such as SP2GAM <ref> [11] </ref> developed by von Eicken and others at Cornell University. We refer to the version of GLACS that uses SP2GAM as NGLACS or native GLACS. We perform a communication cost analysis of these two implementations in the following section. <p> In fact, active messages was developed to replace this sort of send/receive protocol. We explore improving this implementation in Section 4.3 4.2.3 NGLACS SP2 Model We now calculate M C parameters for NGLACS, GLACS on top of the native SP2GAM library <ref> [11] </ref>. NGLACS is similar to the GLACS model without the additional MPL layer. Figure 4.6 shows the GAM layer interacting directly with the hardware through a series of copies. These copies are similar to the copies performed in MPL. <p> Since NGLACS is based on a native GAM implementation independent of MPL, it is unclear how it will perform. It is arguable that because of the synchronization it is possible that it may not take as long to communicate messages since MPL is not involved. In <ref> [11] </ref> the authors show that SP2GAM has a lower message overhead than MPL (50 s vs: 79 s) although the asymptotic point-to-point bandwidth is slightly less than MPL (34 M B=s vs: 35 M B=s).
Reference: [12] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A portable linear algebra library for distributed memory computers Design issues and performance. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-95-283, </type> <institution> University of Tennessee, Knoxville, </institution> <month> March </month> <year> 1995. </year> <note> (LAPACK Working Note #95). </note>
Reference-contexts: With the implementation of the Parallel Virtual Machine (PVM) [32] library, developers were relieved of the porting burden if they could tolerate the performance penalty [53]. Implementors of high performance parallel applications, such as the Scalable Linear Algebra Package (ScaLAPACK) <ref> [12] </ref>, could not tolerate this performance penalty and restructured applications into separate computation and communication modules. The porting work was then limited to rewriting the communication module, the Basic Linear Algebra Communication Subprograms (BLACS) [26] in the case of ScaLAPACK, to use machine-specific message passing libraries. <p> We also discuss several implementations of the BLACS on top of GAM. 4.1.1 ScaLAPACK The Scalable Linear Algebra PACKage (ScaLAPACK) <ref> [12] </ref> is a collection of parallel linear algebra subroutines (matrix multiplication, eigenvalue computation, etc.) for distributed-memory multiprocessors. These parallel routines are re-implementations of the widely used sequential ones in the Linear Algebra PACKage (LAPACK) [22].
Reference: [13] <author> J. Choi, J. J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley. </author> <title> A proposal for a set of parallel basic linear algebra subprograms. </title> <note> Technical Report LAPACK Working Note 100, </note> <institution> University of Tennesee, Knoxville, </institution> <year> 1995. </year>
Reference-contexts: These parallel routines are re-implementations of the widely used sequential ones in the Linear Algebra PACKage (LAPACK) [22]. ScaLAPACK relies on the Basic Linear Algebra Subprograms (BLAS) [43, 25], a collection of primitive matrix operations like matrix-matrix multiplication, for local computation that requires no communication. (PBLAS) <ref> [13] </ref>, the parallel equivalent of the BLAS. The PBLAS subsequently rely on the Basic Linear Algebra Communication Subprograms (BLACS) [26] for actual communication. ScaLAPACK communication is isolated in the BLACS to simplify porting to new machines.
Reference: [14] <author> Brent Chun and David E. Culler. </author> <title> work in progress. </title> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1995. </year>
Reference-contexts: The AM protocol sends fewer messages than the traditional one which means that it costs less. There are AM implementations for the CM-5 [46], nCUBE/2 [59], Paragon [44] and other NOWs [49], as well as versions based on TCP/IP [48] and UDP/IP <ref> [14] </ref>. AM is the communication mechanism used in the Split-C [17] parallel programming language. Split-C provides split-phase remote memory operations, get and put [59], using extensions to the C programming language. AM is also the communication mechanism used by the Threaded Abstract Machine (TAM) [21], a fine-grain parallel execution model. <p> Non-native GAM implementations that rely on VMPLs are available for the IBM SP1/2 on top of MPL, the Intel Paragon on top of NX [44], and on top of both TCP/IP [48] and UDP/IP <ref> [14] </ref>; TCP/IP and UDP/IP implementations are used mostly for NOWs.
Reference: [15] <author> PARKBENCH Committee. </author> <title> Public international benchmarks for parallel computers. </title> <type> Technical report, </type> <year> 1991. </year>
Reference-contexts: Communication microbenchmarking <ref> [15, 27, 47] </ref> has become a popular methodology for assessing message passing costs, but benchmark results secured before the MPI standard are difficult to interpret. Before MPI, each architecture used different message passing libraries with different syntaxes and semantics.
Reference: [16] <author> Intel Corporation. </author> <title> Paragon user's guide, </title> <year> 1992. </year>
Reference-contexts: Each processor is superscalar and can 24 execute up to 4 floating point operations per cycle for a peak Mflop rate of 266. MPL is the native message passing library for this machine. The Intel Paragon <ref> [16] </ref> is a distributed-memory multiprocessor where each node is a separate shared-memory multiprocessor. The nodes are connected by a two-dimensional mesh network with 16-bit links; the typical bandwidth is 175 MB/s per link in each direction.
Reference: [17] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 1993. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: There are AM implementations for the CM-5 [46], nCUBE/2 [59], Paragon [44] and other NOWs [49], as well as versions based on TCP/IP [48] and UDP/IP [14]. AM is the communication mechanism used in the Split-C <ref> [17] </ref> parallel programming language. Split-C provides split-phase remote memory operations, get and put [59], using extensions to the C programming language. AM is also the communication mechanism used by the Threaded Abstract Machine (TAM) [21], a fine-grain parallel execution model.
Reference: [18] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <type> Technical Report CSD-92-713, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: For communication library designers, parallel application developers, and others who need an in-depth understanding of interprocessor communication, parallel computation models that incorporate detailed message passing parameters are an invaluable tool. Many parallel computation models exist [28, 33, 57], but the LogP <ref> [18] </ref> model is one of the most realistic and reliable. The LogP model encompasses the following parameters that are universal across all distributed-memory multiprocessors: latency or communication delay (L), communication overhead (o) 2 , communication gap (g) 3 , and the number of processors (P ). <p> The LogP model encompasses the following parameters that are universal across all distributed-memory multiprocessors: latency or communication delay (L), communication overhead (o) 2 , communication gap (g) 3 , and the number of processors (P ). This model has been used successfully in the design of parallel algorithms <ref> [18, 40] </ref>, analysis of local area networks [39], as well as characterization of network interfaces [20]. One drawback of the LogP model is that it gives only a high level picture of message passing costs.
Reference: [19] <author> D. Culler, K. Keeton, C. Krumbein, L.T. Liu, A. Mainwaring, R. Martin, S. Rodrigues, K. Wright, and C. Yoshikawa. </author> <title> Generic active message interface specification. </title> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1995. </year> <month> 50 </month>
Reference-contexts: Implementations of MPI, such as MPICH [3], also address the performance penalty of previous PVM releases [35, 42]. The Berkeley community has also addressed these porting and performance issues through standardization of Active Messages (AM) [59] with the Application Programming (AM API) [1] and Generic Active Messages (GAM) <ref> [19] </ref> interfaces. These interfaces specify a library of hardware-independent request and reply routines. The use of portable communication layers, such as MPI, PVM and GAM, substantially reduce application developers' porting efforts. <p> TAM is a compilation target for implicitly parallel languages such as Id90 [50]. There is ongoing work to improve Active Messages at Berkeley and Cornell. This work involves expanding AM to new platforms and implementing portable AM versions, GAM <ref> [19] </ref> and AM API [1]. <p> is also underway at Berkeley to implement the MPI 7 standard using GAM [45] and to use GAM in the xFS [2] parallel file system. 1.4.1 GAM Early AM parallel applications faced the same portability problems previously mentioned; this lead to the specification of the Generic Active Message (GAM) interface <ref> [19] </ref>. GAM provides a portable AM interface and implementations of this interface are available on an increasing number of machines. <p> The sender performs a copy, but the receiver only performs a copy if the data is noncontiguous. The implementatons of gam bsend and gam brecv at the AC layer (GAM COMM) actually simulate the synchronous send and receive with calls to the GAM library <ref> [19] </ref>. The first round trip is necessary for the sender to get the remote address (RA) or location to store the data on the receiver.
Reference: [20] <author> David Culler, Lok Tin Liu, Richard P. Martin, and Chad Yoshikawa. </author> <title> LogP performance assessment of fast network interfaces. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 35-43, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: This model has been used successfully in the design of parallel algorithms [18, 40], analysis of local area networks [39], as well as characterization of network interfaces <ref> [20] </ref>. One drawback of the LogP model is that it gives only a high level picture of message passing costs. It does not provide insight into what underlies these costs, such as costs for copying data. Likewise, it does not demonstrate message passing costs at each communcation layer.
Reference: [21] <author> David E. Culler, Anurug Soh, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <address> http://now.cs.berkeley.edu/index.html. </address>
Reference-contexts: AM is the communication mechanism used in the Split-C [17] parallel programming language. Split-C provides split-phase remote memory operations, get and put [59], using extensions to the C programming language. AM is also the communication mechanism used by the Threaded Abstract Machine (TAM) <ref> [21] </ref>, a fine-grain parallel execution model. TAM is a compilation target for implicitly parallel languages such as Id90 [50]. There is ongoing work to improve Active Messages at Berkeley and Cornell. This work involves expanding AM to new platforms and implementing portable AM versions, GAM [19] and AM API [1].
Reference: [22] <author> J. Demmel. </author> <title> LAPACK: A portable linear algebra library for supercomputers. </title> <booktitle> In Proceedings of the 1989 IEEE Control Systems Society Workshop on Computer-Aided Control System Design, </booktitle> <address> Tampa, FL, </address> <month> Dec </month> <year> 1989. </year> <note> IEEE. </note>
Reference-contexts: These parallel routines are re-implementations of the widely used sequential ones in the Linear Algebra PACKage (LAPACK) <ref> [22] </ref>. ScaLAPACK relies on the Basic Linear Algebra Subprograms (BLAS) [43, 25], a collection of primitive matrix operations like matrix-matrix multiplication, for local computation that requires no communication. (PBLAS) [13], the parallel equivalent of the BLAS.
Reference: [23] <institution> U.C. Berkeley CS Division. The Berkeley NOW Project. </institution> <note> http://now.cs.berkeley.edu/index.html. </note>
Reference-contexts: There is also ongoing work to develop high speed implementations of MPI for the Meiko CS-2 and other multiprocessors. 1.4 Active Messages Active Messages [59] is an asynchronous, high performance communication mechanism used by the Berkeley NOW <ref> [23] </ref> and Castle [24] projects. AM seeks to exploit the performance of the underlying hardware by reducing software overhead, such as buffering [38], and overlapping communication with computation.
Reference: [24] <institution> U.C. Berkeley CS Division. Castle project. </institution> <address> http://HTTP.CS.Berkeley.EDU/ projects/parallel/castle/. </address>
Reference-contexts: There is also ongoing work to develop high speed implementations of MPI for the Meiko CS-2 and other multiprocessors. 1.4 Active Messages Active Messages [59] is an asynchronous, high performance communication mechanism used by the Berkeley NOW [23] and Castle <ref> [24] </ref> projects. AM seeks to exploit the performance of the underlying hardware by reducing software overhead, such as buffering [38], and overlapping communication with computation.
Reference: [25] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of FORTRAN Basic Linear Algebra Sub routines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: These parallel routines are re-implementations of the widely used sequential ones in the Linear Algebra PACKage (LAPACK) [22]. ScaLAPACK relies on the Basic Linear Algebra Subprograms (BLAS) <ref> [43, 25] </ref>, a collection of primitive matrix operations like matrix-matrix multiplication, for local computation that requires no communication. (PBLAS) [13], the parallel equivalent of the BLAS. The PBLAS subsequently rely on the Basic Linear Algebra Communication Subprograms (BLACS) [26] for actual communication.
Reference: [26] <author> J. J. Dongarra and R. C. Whaley. </author> <note> A user's guide to the BLACS v.1.0. Technical Report LAPACK Working Note 94, </note> <institution> University of Tennesee, Knoxville, </institution> <year> 1995. </year>
Reference-contexts: Implementors of high performance parallel applications, such as the Scalable Linear Algebra Package (ScaLAPACK) [12], could not tolerate this performance penalty and restructured applications into separate computation and communication modules. The porting work was then limited to rewriting the communication module, the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [26] </ref> in the case of ScaLAPACK, to use machine-specific message passing libraries. There have been several standardization efforts to alleviate the porting burdens that parallel application developers have to bear. <p> ScaLAPACK relies on the Basic Linear Algebra Subprograms (BLAS) [43, 25], a collection of primitive matrix operations like matrix-matrix multiplication, for local computation that requires no communication. (PBLAS) [13], the parallel equivalent of the BLAS. The PBLAS subsequently rely on the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [26] </ref> for actual communication. ScaLAPACK communication is isolated in the BLACS to simplify porting to new machines. Consequently, our cost and performance analyses focus solely on the BLACS. 4.1.2 BLACS The BLACS [26] is a collection of communication routines for transferring dense rectangular and triangular matrices of the following types: integer, <p> The PBLAS subsequently rely on the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [26] </ref> for actual communication. ScaLAPACK communication is isolated in the BLACS to simplify porting to new machines. Consequently, our cost and performance analyses focus solely on the BLACS. 4.1.2 BLACS The BLACS [26] is a collection of communication routines for transferring dense rectangular and triangular matrices of the following types: integer, single, double, complex and double complex. It includes support for multiple logical process grids, scoped operations (row, column or all processors), point-to-point communication, broadcasts, and 34 combines. <p> We refer to these improved implementations as GAM-BLACS (non-native GAM version) and NGAM-BLACS (native GAM version). 4.2 BLACS M C Cost Model Analysis In this section we compute the M C parameters (DM C, CM C, and DCC) for MPL-BLACS, GLACS, and NGLACS using the BLACS sgesd2d and sgerv2d routines <ref> [26] </ref> on the IBM SP2. We compare calculated parameters to gain insight on how to get a high-performance implementation of the BLACS on top of GAM.
Reference: [27] <author> Jack J. Dongarra and Tom Dunigan. </author> <title> Message-passing performance of various computers. </title> <type> Technical Report CS-95-299, </type> <institution> University of Tennesee, Knoxville, </institution> <year> 1995. </year>
Reference-contexts: Communication microbenchmarking <ref> [15, 27, 47] </ref> has become a popular methodology for assessing message passing costs, but benchmark results secured before the MPI standard are difficult to interpret. Before MPI, each architecture used different message passing libraries with different syntaxes and semantics.
Reference: [28] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual Symposium on Theory of Computing, </booktitle> <year> 1978. </year>
Reference-contexts: For communication library designers, parallel application developers, and others who need an in-depth understanding of interprocessor communication, parallel computation models that incorporate detailed message passing parameters are an invaluable tool. Many parallel computation models exist <ref> [28, 33, 57] </ref>, but the LogP [18] model is one of the most realistic and reliable.
Reference: [29] <author> The MPI Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Proceedings of Supercomputing '93, ACM, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: There have been several standardization efforts to alleviate the porting burdens that parallel application developers have to bear. High Performance Fortran (HPF) [36] was one of the first standardization efforts with the Message Passing Interface (MPI) <ref> [29] </ref> being the most recent. The MPI Forum introduced the MPI standard which specifies a library of hardware-independent message passing routines. Implementations of MPI, such as MPICH [3], also address the performance penalty of previous PVM releases [35, 42]. <p> Although recent MPI implementations appear to encompass only one communication layer, internally there are actually two. This is discussed in the MPI section. 5 The MPI Forum began discussions in 1992 to standardize a message passing interface for distributed-memory multiprocessors and NOWs <ref> [29] </ref>. The Forum consisted of about 60 people from 40 organizations, including multiprocessor vendors as well as researchers from universities, government laboratories, and industry. The standard, based largely on current practice, includes point-to-point and collective communication routines, as well as support for communication contexts, process groups, and virtual hardware topologies.
Reference: [30] <author> H. Franke, P. Hochschild, P. Pattnaik, and M. Snir. </author> <title> MPI-F: An efficient implementation of MPI on IBM-SP1. </title> <institution> IBM T. J. Watson Research Center, Yorktown Heights, </institution> <year> 1995. </year>
Reference-contexts: This layer interfaces directly with the underlying hardware and is necessary for communication. The HI layer is usually implemented as a vendor message passing library (VMPL), such as MPL on the IBM SP1/2 <ref> [30] </ref>. The VMPL is usually proprietary and yields nearly optimal communication performance 1 for obvious reasons. Most portable communication libraries, such as PVM and MPICH, interface directly with the underlying VMPL (HI layer) on each machine. <p> The MPI standard has been instrumental in less stressful and faster development of portable parallel applications, software libraries, and tools. Currently, there are several hardware-specific implementations of MPI, such as MPI-F for the SP2 <ref> [30] </ref>. There are also several portable implementations, such as MPICH [3]. The MPI standard is still evolving and there is a MPI-2 Forum actively working on improvements [41]. <p> Developers at the Edinburgh Parallel Computing Center worked on CHIMP [6]. There are also commercial implementations for the Fujitsu AP1000 [55], Cray T3D [4], and IBM SP1/2 <ref> [30] </ref>. Researchers at Mississippi State University have been very active in the MPI implementation effort [8]. They collaborated on MPICH, a Microsoft Windows 3.1 version (WinMPI) and produced Unify. Unify [9] runs on top of PVM and allows a single program to use both MPI and PVM. <p> Thus, VMPLs have been historically treated as black boxes. This practice is slowly changing and some vendors provide full disclosure of message passing activity. This is the case for IBM's MPI-F <ref> [30] </ref> library. Vendors who do not disclose the internals should at least disclose parameters for the M C model to assist application developers with optimization of communication primitives built above this layer, Obtaining accurate message and copy counts along a communication layer may seem like a difficult task.
Reference: [31] <author> H. Franke, C. E. Wu, M. Riviere, P. Pattnaik, and M. Snir. </author> <title> MPI programming environment for IBM-SP1/SP2. </title> <institution> IBM T. J. Watson Research Center, Yorktown Heights, </institution> <year> 1995. </year> <month> 51 </month>
Reference-contexts: The IBM RISC System/6000 Scalable POWERparallel System 2 (SP2) <ref> [31] </ref> is a distributed-memory multiprocessor. It consists of nodes (processing elements coupled with memory and a disk) connected by an Ethernet as well as a high performance two-level cross-bar switch. The switch has a typical bandwidth of 35 MB/s.
Reference: [32] <author> A. Geist, A. Beguelin, J.J. Dongarra, W. Jiang, R. Manchek, and Sunderam V. </author> <title> PVM 3 User's Guide and Reference Manual, </title> <month> September </month> <year> 1994. </year>
Reference-contexts: Introduction Early parallel application development required developers to recode the application to use the message passing library specific to each distributed-memory multiprocessor. With the implementation of the Parallel Virtual Machine (PVM) <ref> [32] </ref> library, developers were relieved of the porting burden if they could tolerate the performance penalty [53]. Implementors of high performance parallel applications, such as the Scalable Linear Algebra Package (ScaLAPACK) [12], could not tolerate this performance penalty and restructured applications into separate computation and communication modules. <p> The following sections provide an overview of three widely-used communication layers: PVM, MPI and AM. For more details on these communication layers, the reader should refer to the references. 1.2 PVM PVM <ref> [32] </ref> development began as a research project in 1989 at Oak Ridge National Laboratories (ORNL). It is now a collaborative effort of many researchers at ORNL and several universities with the goal of advancing computational science. <p> Receives are globally-blocking which means that the receive operation does not complete until the data is actually present. All collective communication operations are globally-blocking. As Figure 4.2 shows, there are a number of BLACS implementations on top of VMPLs, including MPL [7], NX [44], and CMMD [59]. The PVM <ref> [32] </ref> implementation was an earlier attempt at providing portable code, but it exhibited poor performance.
Reference: [33] <author> P. B. Gibbons. </author> <title> A more practical PRAM model. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1978. </year>
Reference-contexts: For communication library designers, parallel application developers, and others who need an in-depth understanding of interprocessor communication, parallel computation models that incorporate detailed message passing parameters are an invaluable tool. Many parallel computation models exist <ref> [28, 33, 57] </ref>, but the LogP [18] model is one of the most realistic and reliable.
Reference: [34] <author> W. Gropp and E. Lusk. </author> <title> MPICH ADI implementation reference manual. </title> <institution> Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Non-native, portable AM implementations, such as GAM, also reside at this layer without additional recoding. The AC layer itself could comprise more than one layer which is the case for MPICH. MPICH, discussed later, uses an additional abstract device interface (ADI) <ref> [34] </ref> which accesses the HI layer or VMPL. This layer separation enables MPICH to support many multiprocessors and networks of workstations (NOWs) with minimal porting efforts. The ADI layer is usually all that needs to be supplied to support a new machine. <p> It is freely available and runs on a variety of distributed- and shared-memory multiprocessors (CM-5, Paragon, SGI Challenge and SP1/2) as well as NOWs (Alpha, Sun, RS6000 and HP). It supports many architectures due to the separate ADI <ref> [34] </ref> layer discussed previously. This layer either uses the VMPL or TCP/IP sockets to access the underlying hardware. The ADI layer also provides support for heterogeneity or communication between different architectures.
Reference: [35] <author> Jonathan C. Hardwick. </author> <title> Porting a vector library: a comparison of MPI, Paris, CMMD and PVM. </title> <type> Technical Report CMU-CS-94-200, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: The MPI Forum introduced the MPI standard which specifies a library of hardware-independent message passing routines. Implementations of MPI, such as MPICH [3], also address the performance penalty of previous PVM releases <ref> [35, 42] </ref>. The Berkeley community has also addressed these porting and performance issues through standardization of Active Messages (AM) [59] with the Application Programming (AM API) [1] and Generic Active Messages (GAM) [19] interfaces. These interfaces specify a library of hardware-independent request and reply routines. <p> The major obstruction is the reliance on "portable" communication libraries, such as PVM and MPI. These libraries either are portable to a certain degree [47], exhibit poor performance <ref> [53, 35, 42] </ref>, or both. Nonetheless, relying on these libraries is much more attractive than coding for the myriad of distributed-memory multiprocessors. Hence, we must make the best use of what is available.
Reference: [36] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 0.4, </note> <year> 1992. </year>
Reference-contexts: There have been several standardization efforts to alleviate the porting burdens that parallel application developers have to bear. High Performance Fortran (HPF) <ref> [36] </ref> was one of the first standardization efforts with the Message Passing Interface (MPI) [29] being the most recent. The MPI Forum introduced the MPI standard which specifies a library of hardware-independent message passing routines.
Reference: [37] <author> Melody Y. </author> <title> Ivory. A GAM implementation of the BLACS. </title> <address> http://http.cs.berkeley.edu/ ivory/pblasmtg.html. </address>
Reference-contexts: Thus, we needed to implement a version of the BLACS, Scalapack's AC layer, on top of GAM. We implemented a naive version of the BLACS on top of GAM, GLACS <ref> [37] </ref>, to meet the integration goal. We also implemented a library of routines, GAM COMM, that call GAM to simulate message passing on top of AM. GLACS, coupled with GAM COMM and a non-native GAM implementation (MPL), exhibited a 36% performance degradation over MPL-BLACS [37]. <p> BLACS on top of GAM, GLACS <ref> [37] </ref>, to meet the integration goal. We also implemented a library of routines, GAM COMM, that call GAM to simulate message passing on top of AM. GLACS, coupled with GAM COMM and a non-native GAM implementation (MPL), exhibited a 36% performance degradation over MPL-BLACS [37]. This motivated us to use our M C communication cost model as a methodology for evaluating alternative approaches to a high-performance implementation of BLACS on top of GAM. The model also provided a realistic bound for the message and copy behavior of our improved GLACS implementation.
Reference: [38] <author> V. Karamcheti and A. A. Chien. </author> <title> Software overhead in message layers: Where does the time go? In Proceedings of ASPLOS-VI. </title> <address> San Jose, CA, </address> <year> 1994. </year>
Reference-contexts: AM seeks to exploit the performance of the underlying hardware by reducing software overhead, such as buffering <ref> [38] </ref>, and overlapping communication with computation. These objectives are accomplished through specification of a user-level handler in each outgoing message that executes on message arrival with the message body as an argument.
Reference: [39] <author> R.M. Karp, A. Sahay, E. Santos, and K.E. Schauser. </author> <title> Optimal broadcast and summation in the LogP model. </title> <booktitle> In Proc. 5th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 142-153, </pages> <year> 1993. </year>
Reference-contexts: This model has been used successfully in the design of parallel algorithms [18, 40], analysis of local area networks <ref> [39] </ref>, as well as characterization of network interfaces [20]. One drawback of the LogP model is that it gives only a high level picture of message passing costs. It does not provide insight into what underlies these costs, such as costs for copying data.
Reference: [40] <author> Kimberly K. Keeton, Thomas E. Anderson, and David A. Patterson. </author> <title> LogP quantified: The case for low-overhed local area networks. </title> <booktitle> In Proceedings of Hot Interconnects III: </booktitle>
Reference-contexts: The LogP model encompasses the following parameters that are universal across all distributed-memory multiprocessors: latency or communication delay (L), communication overhead (o) 2 , communication gap (g) 3 , and the number of processors (P ). This model has been used successfully in the design of parallel algorithms <ref> [18, 40] </ref>, analysis of local area networks [39], as well as characterization of network interfaces [20]. One drawback of the LogP model is that it gives only a high level picture of message passing costs.
References-found: 40

