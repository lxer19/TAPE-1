URL: http://www.is.cs.cmu.edu/papers/speech/ACL97/ACL97-yeyi.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: fyyw,waibelg@cs.cmu.edu  
Title: Decoding Algorithm in Statistical Machine Translation  
Author: Ye-Yi Wang and Alex Waibel 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213, USA  
Affiliation: Language Technology Institute School of Computer Science Carnegie Mellon University  
Abstract: Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brown, P. F., S. A. Della-Pietra, V. J Della-Pietra, and R. L. Mercer. </author> <year> 1993. </year> <title> The Mathematics of Statistical Machine Translation: Parameter Estimation. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 263-311. </pages>
Reference-contexts: The modeling and learning issues have been discussed in <ref> (Brown et al., 1993) </ref>, where ngram model was used for language modeling, and five different translation models were introduced for the translation process. We briefly introduce the model 2 here, for which we built our decoder. <p> sum of the probabilities of generating g from e over all possible alignments A, in which the position i in the target sentence g is aligned to the position a i in the source sentence e: P (g j e) = l X a m =0 j=1 * j=1 i=0 <ref> (Brown et al., 1993) </ref> also described how to use the EM algorithm to estimate the parameters a (i j j; l; m) and t (g j e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation (Brown et al., 1993) and (Vogel, Ney, and Tillman, 1996) have discussed the first <p> (g j e) = l X a m =0 j=1 * j=1 i=0 <ref> (Brown et al., 1993) </ref> also described how to use the EM algorithm to estimate the parameters a (i j j; l; m) and t (g j e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation (Brown et al., 1993) and (Vogel, Ney, and Tillman, 1996) have discussed the first two of the three problems in statistical machine translation. Although the authors of (Brown et al., 1993) stated that they would discuss the search problem in a follow-up article, so far there have no publications devoted to <p> the parameters a (i j j; l; m) and t (g j e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation <ref> (Brown et al., 1993) </ref> and (Vogel, Ney, and Tillman, 1996) have discussed the first two of the three problems in statistical machine translation. Although the authors of (Brown et al., 1993) stated that they would discuss the search problem in a follow-up article, so far there have no publications devoted to the decoding issue for statistical machine translation. On the other side, decoding algorithm is a crucial part in statistical machine translation. <p> This causes problems because it appears as a denominator in (5) and the argument of the log function when calculating g P . We dealt with this by either limiting the translation probability from the null word <ref> (Brown et al., 1993) </ref> at the hypothetical 0-position (Brown et al., 1993) over a threshold during the EM training, or setting S H 0 (j) to a small probability instead of 0 for the initial null hypothesis H 0 . <p> This causes problems because it appears as a denominator in (5) and the argument of the log function when calculating g P . We dealt with this by either limiting the translation probability from the null word <ref> (Brown et al., 1993) </ref> at the hypothetical 0-position (Brown et al., 1993) over a threshold during the EM training, or setting S H 0 (j) to a small probability instead of 0 for the initial null hypothesis H 0 .
Reference: <author> Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, J. D. Lafferty, and R. L. Mercer. </author> <year> 1992. </year> <title> Analysis, Statistical Transfer, and Synthesis in Machine Translation. </title> <booktitle> In Proceedings of the fourth International Conference on Theoretical and Methodological Issues in Machine Translation, </booktitle> <pages> pages 83-100. </pages>
Reference-contexts: A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling. The lexicon contains 2,800 English and 4,800 German words in morphologically inflected form. We did not do any prepro-cessing/analysis of the data as reported in <ref> (Brown et al., 1992) </ref>. 5.1 Decoder Success Rate Table 1 shows the success rate of three mod-els/decoders.
Reference: <author> Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. </author> <year> 1990. </year> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts. </address>
Reference: <author> Magerman, D. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: If the source sentence is a short one, the decoder will never be able to find it, for the hypotheses leading to it have been pruned permanently. This "incomparable" problem was solved with multi-stack search <ref> (Magerman, 1994) </ref>. A separate stack was used for each hypothesized source sentence length l. We do compare hypotheses in different stacks in the following cases.
Reference: <author> Nilsson, N. </author> <year> 1971. </year> <title> Problem-Solving Methods in Arti ficial Intelligence. </title> <publisher> McGraw Hill, </publisher> <address> New York, New York. </address>
Reference-contexts: Our experiments show that = 10 4 gives the best result. 2.1.2 Heuristics To guarantee an optimal search result, the heuristic function must be an upper-bound of the score for all possible extensions e k+1 e k+2 e l <ref> (Nilsson, 1971) </ref> of a hypothesis. In other words, the benefit of extending a hypothesis should never be underestimated. Otherwise the search algorithm will conclude prematurely with a non-optimal hypothesis.
Reference: <author> Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. May field, A. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna, and A. Waibel. </author> <year> 1995. </year> <title> JANUS: Towards multilingual spoken language translation. </title> <booktitle> In Proceedings of the ARPA Speech Spoken Language Technology Workshop, </booktitle> <address> Austin, TX, </address> <year> 1995. </year>
Reference-contexts: structure (Cor-men, Leiserson, and Rivest, 1990) to implement the dynamic set, which guarantees that the above operations take O (log n) time in the worst case, where n is the number of search states in the set. 5 Performance We tested the performance of the decoders with the scheduling corpus <ref> (Suhm et al., 1995) </ref>. Around 30,000 parallel sentences (400,000 words altogether for both languages) were used to train the IBM model 2 and the simplified model with the EM algorithm. A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling.
Reference: <author> Vogel, S., H. Ney, and C. Tillman. </author> <year> 1996. </year> <title> HMM Based Word Alignment in Statistical Translation. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Computational Linguistics: COLING-96, </booktitle> <pages> pages 836-841, </pages> <address> Copenhagen, Den-mark. </address>
Reference-contexts: X a m =0 j=1 * j=1 i=0 (Brown et al., 1993) also described how to use the EM algorithm to estimate the parameters a (i j j; l; m) and t (g j e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation (Brown et al., 1993) and <ref> (Vogel, Ney, and Tillman, 1996) </ref> have discussed the first two of the three problems in statistical machine translation.
References-found: 7

