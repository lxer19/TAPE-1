URL: http://www.cs.rpi.edu/tr/92-34.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Title: Compiler Technology for Parallel Scientific Computation  
Author: Can Ozturan, Balaram Sinharoy and Boleslaw K. Szymanski 
Address: Troy, New York 12180-3590, USA  
Affiliation: Department of Computer Science, Rensselaer Polytechnic Institute  
Note: to appear in Scientific Programming  
Abstract: There is a need for compiler technology that, given the source program, will generate efficient parallel codes for different architectures with minimal user involvement. Parallel computation is becoming indispensable in solving large-scale problems in science and engineering. Yet, the use of parallel computation is limited by the high costs of developing the needed software. To overcome this difficulty we advocate a comprehensive approach to the development of scalable architecture-independent software for scientific computation based on our experience with Equational Programming Language, EPL. Our approach is based on a program decomposition, parallel code synthesis and run-time support for parallel scientific computations. The program decomposition is guided by the source program annotations provided by the user. The synthesis of parallel code is based on configurations that describe the overall computation as a set of interacting components. Run-time support is provided by the compiler-generated code that redistributes computation and data during object program execution. The generated parallel code is optimized using techniques of data alignment, operator placement, wavefront determination and memory optimization. In this paper we discuss annotations, configurations, parallel code generation and run-time support suitable for parallel programs written in the functional parallel programming language EPL and in Fortran. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andrews, G. </author> <year> (1991). </year> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <publisher> Benjamin/Cummings Publishing Company Inc., </publisher> <address> Redwood City, CA. </address>
Reference-contexts: This is distinct from what is usually referred to as Parallel Reduction, which involves the parallel evaluation of a single reduction <ref> (Andrews, 1991) </ref> or its variants. An algorithm for standard parallel reduction that uses a balanced binary tree implementation for mesh-connected architectures has been presented in (Gibbons and Ziani, 1991). <p> For the interval of size n = 2 k and an arbitrary offset p, a modification of the well known Parallel Prefix algorithm <ref> (Andrews, 1991) </ref> achieves the above bounds. The modification defines the direction of the message transfer in each step by the corresponding bit of the binary representation of the offset p.
Reference: <author> Benkner, S., Chapman, B., and Zima, H. </author> <year> (1992). </year> <title> Vienna Fortran 90. </title> <booktitle> In Proc. Scalable High Performance Computing Conference 1992, Williamsburg, </booktitle> <pages> pages 51-59. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> Washington, DC. </address>
Reference-contexts: Traditionally supported compiler optimizations for parallel computation involves subscript analysis or directives for regular problem decompositions and distribution. Language and software tools for dealing with irregularity in parallel computation rely either on user-provided partitioning algorithms, e.g., dynamic distributions in Vienna FORTRAN <ref> (Benkner et al., 1992) </ref> or the tracing of sample executions, e.g., Kali compiler (Mehrotra and Van Rosendale, 1991; Koelbel and Mehrotra, 1991) and the PARTI library (Hiranandani et al., 1991b; Wu et al., 1991)). Communication patterns of many advanced parallel computations are rarely known at compile time.
Reference: <author> Berger, M. and Bokhari, S. </author> <year> (1987). </year> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, C-36:570-580. </journal>
Reference-contexts: Therefore, there is a need for run-time task reallocation of adaptive computation executed on massively parallel distributed-memory machines. Such task reallocation requires different methods than the large-grain, few-processor approaches discussed in the literature <ref> (Berger and Bokhari, 1987) </ref>.
Reference: <author> Bertossi, A. and Gori, A. </author> <year> (1988). </year> <title> Total domination and irredundance in weighted interval graphs. </title> <journal> SIAM J. Disc. Mathematics, </journal> <volume> 1(3) </volume> <pages> 317-327. </pages>
Reference-contexts: The interval graph can be converted to a directed acyclic graph (DAG). The shortest path algorithm applied to this DAG will find the minimum weight dominating set <ref> (Bertossi and Gori, 1988) </ref>.
Reference: <author> Bokhari, S. </author> <year> (1981). </year> <title> A shortest tree algorithm for optimal assignments across space and time in a distributed processor system. </title> <journal> IEEE Trans. Soft. Eng., SE-7(6). </journal>
Reference-contexts: The process interconnection network is decomposed into parallelizable tasks by the compiler. Since the optimal decomposition is NP-hard for machines having more than three processors <ref> (Bokhari, 1981) </ref>, the EPL compiler uses heuristics. * Medium Grain parallelism is sought at the level of equation clusters. Several equations in a program can be clustered into a group. Separate tasks are generated for each of the clusters.
Reference: <author> Bruno, J. and Szymanski, B. </author> <year> (1988). </year> <title> Analyzing conditional data dependencies in an Equational Language Compiler. </title> <booktitle> In Proc. Third Supercomputing Conference, Boston, </booktitle> <pages> pages 358-365. </pages> <institution> Supercomputing Institute, Tampa. </institution>
Reference-contexts: In particular, the reassignments elimination involves replacing the reassigned variables by: * vector (additional dimension) inside loops, * variants in "if" branches and basic blocks. Ozturan et al. 11 2. Program optimization, that consists of: Condition Analysis: Conditions in the transformed program are analyzed using a Sup-Inf inequality prover <ref> (Bruno and Szymanski, 1988) </ref> and the Kaufl variable elimination method (Kaufl, 1988) to find pairwise equivalent or exclusive conditions. Variable's Variants Elimination: Variants created in equivalent and exclusive conditions are merged into a single variable.
Reference: <author> Chapman, B. and Zima, H. </author> <year> (1992). </year> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1 </volume> <pages> 31-50. </pages>
Reference-contexts: Hence, they are used only during the decomposition of a process into smaller fragments. This kind of annotation is similar to ON clause as used in the Kali compiler (Mehrotra and Van Rosendale, 1991), Fortran D (Fox et al., 1991) or Vienna Fortran <ref> (Chapman et al., 1992) </ref>. Annotation does not have any effect on the result computed by a program. Consequently, sequential programs that have manifested their correctness over many years of usage are good candidates for parallelization through annotations.
Reference: <author> Chapman, B. M., Mehrotra, P., and Zima, H. P. </author> <year> (1992). </year> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In Saltz, J. and Mehrotra, P., editors, </editor> <booktitle> Languages Compilers and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 39-62. </pages> <publisher> Elsevier, Amsterdam. </publisher>
Reference-contexts: Hence, they are used only during the decomposition of a process into smaller fragments. This kind of annotation is similar to ON clause as used in the Kali compiler (Mehrotra and Van Rosendale, 1991), Fortran D (Fox et al., 1991) or Vienna Fortran <ref> (Chapman et al., 1992) </ref>. Annotation does not have any effect on the result computed by a program. Consequently, sequential programs that have manifested their correctness over many years of usage are good candidates for parallelization through annotations.
Reference: <author> Darema-Rogers, F., Norton, V.A., and Pfister, </author> <title> G.F. (1985). A VM parallel environment. </title> <type> IBM Research Report RC 11225, </type> <institution> IBM Corp., Yorktown Heights. </institution>
Reference: <author> Duff, I., Grimes, R., and Lewis, J. </author> <year> (1992). </year> <title> User's Guide for the Harwell-Boeing Sparse Matrix Collection. CERFACS, </title> <address> Toulouse Cedex, France, </address> <note> first edition. </note>
Reference: <author> Fahringer, T. and Zima, H. </author> <year> (1993). </year> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In The Seventh ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan. </address> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: Ozturan et al. 28 Cost Measures. There is a part of the system, called the Timer, that provides the user with the execution time estimates for equational programs. As in <ref> (Fahringer and Zima, 1993) </ref>, the Timer relies on a set of architecture measurements that can be established by running initiation programs of the Timer on the given architectures.
Reference: <author> Flaherty, J. E., Paslow, P. J., Shephard, M., and Vasilakis, J. D., </author> <title> editors (1989). Adaptive Methods for Partial Differential Equations. </title> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference-contexts: The area is proportional to the number of processors active in it. Such cost definition is motivated by the mesh refinement techniques used in adaptive numerical methods. Each entry in the workload matrix represents the solution error obtained by an error estimation procedure <ref> (Flaherty et al., 1989) </ref>. The high-error regions need recomputing and the needed work is proportional to the magnitude of the error. Hence, the number of processors reassigned to each solution region should be proportional to the refinement factor.
Reference: <author> Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C., and Wu, W. </author> <year> (1991). </year> <title> Fortran D language specification. </title> <type> Technical Report COMP 90079, </type> <institution> Department of Computer Science, Rice University, Houston. </institution>
Reference-contexts: In our approach annotations limit the feasible mappings of computation onto the processors. Hence, they are used only during the decomposition of a process into smaller fragments. This kind of annotation is similar to ON clause as used in the Kali compiler (Mehrotra and Van Rosendale, 1991), Fortran D <ref> (Fox et al., 1991) </ref> or Vienna Fortran (Chapman et al., 1992). Annotation does not have any effect on the result computed by a program. Consequently, sequential programs that have manifested their correctness over many years of usage are good candidates for parallelization through annotations.
Reference: <author> Ge, X. and Prywes, N. </author> <year> (1990). </year> <title> Reverse software engineering of concurrent programs. </title> <booktitle> In Proc. Fifth Jerusalem Conference on Information Technology, Jerusalem, </booktitle> <pages> pages 731-742, </pages> <publisher> IEEE Computer Science Press, </publisher> <address> Wash-ington, DC. </address>
Reference-contexts: An important step towards an efficient parallelization of Fortran programs with the help of the EPL compiler involves an equational transformation during which the equational equivalent of the program is generated (Szymanski, 1994). The transformed programs obey the single assignment rule and do not contain any control statements <ref> (Ge and Prywes, 1990) </ref>. The transformation is done in the following two steps: 1. Program expansion, during which the variables are expanded to enforce the single assignment rule.
Reference: <author> Gerndt, M. and Zima, H. P. </author> <year> (1992). </year> <title> SUPERB: Experience and future research. </title> <editor> In Saltz, J. and Mehrotra, P., editors, </editor> <booktitle> Languages Compilers and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 1-15. </pages> <publisher> Elsevier, Amsterdam. </publisher>
Reference-contexts: well as differences between the EPL annotations and the Fortran language extensions that have been introduced in many systems, e.g., Vienna Fortran (Chapman et al., 1992; Zima et al., 1992; Benkner et al., 1992), Fortran D (Fox et al., 1991; Hiranandani et al., 1992; Hi-ranandani et al., 1991a) and SUPERB <ref> (Gerndt and Zima, 1992) </ref>. Vienna Fortran provides directives for array-like processor structure definition. The distribution of arrays can be specified at compile-time through the use of a DIST directive with BLOCK or CYCLIC options. INDIRECT directives can be added to indicate run-time distribution.
Reference: <author> Gibbons, A. and Ziani, R. </author> <year> (1991). </year> <title> The balanced binary tree technique on mesh-connected computers. </title> <journal> Information Processing Letters, </journal> <volume> 37(2) </volume> <pages> 101-109. </pages>
Reference-contexts: This is distinct from what is usually referred to as Parallel Reduction, which involves the parallel evaluation of a single reduction (Andrews, 1991) or its variants. An algorithm for standard parallel reduction that uses a balanced binary tree implementation for mesh-connected architectures has been presented in <ref> (Gibbons and Ziani, 1991) </ref>. Another standard parallel reduction algorithm has been introduced in (Miguet and Robert, 1992) for tree topologies of arbitrary but bounded fan-in and arbitrary tree depth.
Reference: <author> Gilbert, J. and Schreiber, R. </author> <year> (1991). </year> <title> Optimal expression evaluation for data parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 58-64. </pages>
Reference-contexts: Hence, such an approach does not succeed when the independently found alignments conflict with each other. Similarly, the algorithm presented in <ref> (Gilbert and Schreiber, 1991) </ref> finds the minimum communication cost of evaluating an expression over a distributed processor array but only for a single expression.
Reference: <author> Golumbic, M. </author> <year> (1980). </year> <title> Algorithmic Graph Theory and Perfect Graphs. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: In contrast, our algorithm can solve more complicated problems with an arbitrary cost function P x k O (Kn 3 ) steps. There is a similarity between the weighted independent set for interval graphs and our problem <ref> (Golumbic, 1980) </ref>. The interval graph for our problem can be created as follows. Each possible subinterval (x k1 ; x k ) is represented by a node of the interval graph.
Reference: <author> Gomory, R. and Hu, T. </author> <year> (1961). </year> <title> Multi-terminal network flows. </title> <journal> SIAM J. of Appl. Math., </journal> <volume> 9 </volume> <pages> 551-570. </pages>
Reference-contexts: Ozturan et al. 12 To minimize the total communication cost, proper cut-tree must be found. It can be done in O (j V j 4 ) steps <ref> (Gomory and Hu, 1961) </ref> by solving j V j maximal flow problems.
Reference: <author> Govindaraju, R. and Szymanski, B. </author> <year> (1992). </year> <title> Synthesizing scalable computations from sequential programs. </title> <booktitle> In Proc. Scalable High Performance Computing Conference, Williamsburg, </booktitle> <pages> pages 228-231. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> Washington, DC. </address>
Reference: <author> Ozturan et al. 30 Hammond, S. W. </author> <year> (1991). </year> <title> Mapping Unstructured Grid Computations to Massively Parallel Computers. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute, Troy. </institution>
Reference: <author> Hiranandani, S., Kennedy, K. Koelbel, C., Kremer, U., and Tseng, C. </author> <year> (1991a). </year> <title> An overview of the Fortran D programming system. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara. </address>
Reference: <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <year> (1992). </year> <title> Compiler support for machine-independent parallel programming in Fortan D. </title> <editor> In Saltz, J. and Mehrotra, P., editors, </editor> <booktitle> Languages, Compilers and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 139-176. </pages> <publisher> Elsevier, Amsterdam. </publisher>
Reference: <author> Hiranandani, S., Saltz, J., Piyush, M., and Berryman, H. </author> <year> (1991b). </year> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(3) </volume> <pages> 315-422. </pages>
Reference: <author> Hudak, P. </author> <year> (1991). </year> <title> Para-Functional programming in Haskell. </title> <editor> In Szymanski, B., editor, </editor> <booktitle> Parallel Functional Languages and Environments, </booktitle> <pages> pages 159-196. </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference: <author> Jordan, H.F. </author> <year> (1985). </year> <title> Parallel computation with the Force. </title> <type> Technical Report 84-45, </type> <institution> ICASE, Hampton, VA. </institution>
Reference: <author> Kaufl, T. </author> <year> (1988). </year> <title> Reasoning about systems of linear inequalities. </title> <booktitle> In Ninth International Conference on Automated Deduction, Aragon. </booktitle> <address> IL, </address> <pages> pages 563-72, </pages> <publisher> Springer-Verlag, Heidelberg-Berlin. </publisher>
Reference-contexts: Ozturan et al. 11 2. Program optimization, that consists of: Condition Analysis: Conditions in the transformed program are analyzed using a Sup-Inf inequality prover (Bruno and Szymanski, 1988) and the Kaufl variable elimination method <ref> (Kaufl, 1988) </ref> to find pairwise equivalent or exclusive conditions. Variable's Variants Elimination: Variants created in equivalent and exclusive conditions are merged into a single variable.
Reference: <author> Kincaid, D. R., Respess, J., Young, D., and Grimes, R. </author> <title> ITPACK 2C: A Fortran package for solving large sparse linear systems by adaptive accelerated iterative methods. </title> <type> Technical Report, </type> <institution> University of Texas at Austin. </institution>
Reference: <author> Knobe, K., Lukas, J., and Steele Jr., G. </author> <year> (1990). </year> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 112-118. </pages>
Reference: <author> Koelbel, C. and Mehrotra, P. </author> <year> (1991). </year> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2 </volume> <pages> 440-451. </pages>
Reference: <author> Lamport, L. </author> <year> (1974). </year> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17 </volume> <pages> 83-93. </pages>
Reference-contexts: On the other hand, the processors may be underutilized, if a large partition of the wavefront is assigned to a single processor. The wavefront approach to finding the set of index points at which evaluation can proceed simultaneously was originally proposed in <ref> (Lamport, 1974) </ref>. However, to find the wavefront minimizing the total execution time, an NP-hard integer programming problem has to be solved.
Reference: <author> Lee, P.-Z. and Kedem, Z. M. </author> <year> (1988). </year> <title> Synthesizing linear array algorithms from nested for loop algorithms. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(12) </volume> <pages> 1578-1598. </pages>
Reference: <author> Lee, P.-Z. and Kedem, Z. M. </author> <year> (1990). </year> <title> Mapping nested loop algorithms into multidimensional systolic arrays. </title> <journal> IEEE Transactions on Parallel and Distributed Processing, </journal> <volume> 1(1) </volume> <pages> 64-76. </pages>
Reference: <author> Li, J. and Chen, M. </author> <year> (1991). </year> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221. </pages>
Reference: <author> Maniatty, B., Szymanski, B., and Sinharoy, B. </author> <year> (1993). </year> <title> Efficiency of data alignment on MasPar. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(1) </volume> <pages> 48-51. </pages>
Reference-contexts: Depending on the relative cost of the increased message and operation counts versus the smaller hop count, this algorithm may or may not outperform intersect for the given interval and offset. 3 This example is based on the computation arising in modeling ecosystem on the MasPar <ref> (Maniatty et al., 1993) </ref>.
Reference: <author> McKenney, B. and Szymanski, B. </author> <year> (1992). </year> <title> Generating parallel code for SIMD machines. </title> <journal> ACM Let. Programming Languages and Systems, </journal> <volume> 1 </volume> <pages> 37-46. </pages>
Reference-contexts: Scheduler also defines the scopes and nesting of the loops in the object program. The output generated by the scheduler is used by the schedule optimizer and the code generator. 8. Schedule Optimization is an architecture-dependent step that customizes the generated schedule to the target architecture (see, for example, <ref> (McKenney and Szymanski, 1992) </ref> for SIMD specific optimizations). 9. <p> To generate efficient code for SIMD machines, one or two dimensions of a data array should be projected along the processor array <ref> (McKenney and Szymanski, 1992) </ref>. For the i-th projected dimension of each array (each equation), we define an alignment function ff i that maps the index of that dimension into the position of the virtual processor that stores (executes) its value.
Reference: <author> Mehrotra, P. and Van Rosendale, J. </author> <year> (1991). </year> <title> Programming distributed memory architectures using Kali. </title> <editor> In Nicolau, A., Gelernter, D., Gross, T., and Padua, D., editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 364-384. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In our approach annotations limit the feasible mappings of computation onto the processors. Hence, they are used only during the decomposition of a process into smaller fragments. This kind of annotation is similar to ON clause as used in the Kali compiler <ref> (Mehrotra and Van Rosendale, 1991) </ref>, Fortran D (Fox et al., 1991) or Vienna Fortran (Chapman et al., 1992). Annotation does not have any effect on the result computed by a program.
Reference: <author> Miguet, S. and Robert, Y. </author> <year> (1992). </year> <title> Reduction operators on a distributed memory machine with a reconfigurable interconnection. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 3(4) </volume> <pages> 501-512. </pages>
Reference-contexts: An algorithm for standard parallel reduction that uses a balanced binary tree implementation for mesh-connected architectures has been presented in (Gibbons and Ziani, 1991). Another standard parallel reduction algorithm has been introduced in <ref> (Miguet and Robert, 1992) </ref> for tree topologies of arbitrary but bounded fan-in and arbitrary tree depth. The segmented prefix problem is a variant of parallel reduction that subdivides a single dimension of processors into non-overlapping contiguous regions of varying size.
Reference: <author> Moldovan, D. I. </author> <year> (1986). </year> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, C-35(1):1-12. </journal>
Reference: <author> Nicol, D. M. </author> <year> (1991). </year> <title> Rectilinear partitioning of irregular data parallel computations. </title> <type> Technical Report 91-55, </type> <institution> ICASE, Hampton, VA. </institution>
Reference-contexts: In <ref> (Nicol, 1991) </ref>, the following Rectilinear Partitioning Problem (RPP) has been proposed and solved: Partition the given n fi m workload matrix into (N + 1) fi (M + 1) rectangles with N + M rectilinear cuts in such a way that the maximum workload among rectangles is minimized. <p> For L N max and f (x 1 k ; x 2 k ) = 1, an instance of RPP is obtained which can be solved in O (Kn) or O (n + (Klogn) 2 ) steps <ref> (Nicol, 1991) </ref>. <p> adaptive PDE solvers on machines where the number of processors exceeds the number of tasks can be obtained by putting L N i.e., when the sum of the workloads in each partition is divided by the interval length (i.e., the number Ozturan et al. 24 Problem L N One-dimensional partitioning <ref> (Nicol, 1991) </ref> min max 1 Density type for PDEs min max (x k x k1 + 1) Shortest path with k arcs min + 1 Partitioning for heterogeneous processors min max s k Table 1: Instances of problem represented by Eq.(1) of processors).
Reference: <author> O'Boyle, M. and Hedayat, G. </author> <year> (1992). </year> <title> Data alignment: Transformation to reduce communication on distributed memory architectures. </title> <booktitle> In Proc. Scalable High Performance Computing Conference 1992, Williamsburg, </booktitle> <pages> pages 366-371. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> Washington. DC. </address>
Reference: <author> Ozturan et al. 31 Ozturan, C., Szymanski, B., and Flaherty, J. E. </author> <year> (1992). </year> <title> Adaptive methods and rectangular partitioning problem. </title> <booktitle> In Proc. Scalable High Performance Computing Conference 1992, Williamsburg, </booktitle> <pages> pages 409-415. </pages> <publisher> IEEE Computer Science Press, </publisher> <address> Washington. DC. </address>
Reference-contexts: Such task reallocation requires different methods than the large-grain, few-processor approaches discussed in the literature (Berger and Bokhari, 1987). We have proposed a new type of so-called density workload problems appropriate for such environments <ref> ( Ozturan et al., 1992) </ref>. 5.1 Run-Time Task Distribution One of the most challenging problems encountered while implementing adaptive scientific computations on distributed-memory machines is run-time mapping of a dynamically changing computational load onto the parallel processors. <p> Such optimization is appropriate for adaptive finite element computations on architectures with local communication that is faster than the global one. Since balanced partitions tend to increase the volume of local versus global communication, the overall communication cost can be decreased by using the optimum rectilinear partition. In <ref> ( Ozturan et al., 1992) </ref>, we investigated the balancing of an adaptive scientific computation on SIMD machines: this is the problem with similar motivation and applications as the RPP problem. <p> To avoid such a waste, partitioning methodology cannot be restricted to rectilinear cuts extending across the whole domain in both dimensions. Hence, in our problem definition and solution <ref> ( Ozturan et al., 1992) </ref>, we require that K selected rectangles cover the whole domain. The heuristics for the two-dimensional case projects the weights to one dimension and results in rectilinear cuts extending across the whole dimension in one direction.
Reference: <author> Pnueli, A., Prywes, N., and Zahri, R. </author> <year> (1984). </year> <title> Scheduling equational specifications and nonprocedural programs. </title> <editor> In Biermann, Guiho, and Kondratoff, editors, </editor> <booktitle> Automatic program construction techniques, </booktitle> <pages> pages 273-287. </pages> <address> McMillan, New York. </address>
Reference-contexts: However, data dependencies often hold under conditions that involve input data and therefore can be resolved only in run-time. Consequently, data-driven scheduling typically relies on run-time distributed synchronization. In the case of functional programs with single assignment and recurrent relations, the compile-time data-driven scheduling is decidable <ref> (Pnueli et al., 1984) </ref>. Such a scheduler has been implemented in the compiler for EPL language (Szy-manski and Prywes, 1988) and is not discussed here. Wavefront Scheduling is presented in Section 4.4.3. Programs written in EPL or transformed from Fortran obey the single assignment rule.
Reference: <author> Rao, S. K. </author> <year> (1985). </year> <title> Regular Iterative Algorithms and their Implementations on Processor Arrays. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, Stanford. </institution>
Reference-contexts: A large class of parallel scientific computations can be expressed as Regular Iterative Algorithms (RIA) <ref> (Rao, 1985) </ref> in which all indexing expressions are of the form "I + c", where I is a subscript and c is an integer constant. <p> The end point of this vector is an index point at which the equation is executed and the starting point of the vector is an index point at which some value used in the definition is evaluated. For Regular Iterative Algorithms <ref> (Rao, 1985) </ref> expressed in EPL, the dependence vectors are defined by the difference between the corresponding subscript expressions used in the left and right side of the equation. In the above computation, there are just two dependence vectors: OA ([4,2]) and OB ([2,-2]).
Reference: <author> Rosing, M., Schnabel, R. B., and Weaver, R. P. </author> <year> (1992). </year> <title> Scientific programming languages for distributed memory multiprocessors: Paradigms and research issues. </title> <editor> In Saltz, J. and Mehrotra, P., editors, </editor> <title> Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier, Amsterdam. </publisher>
Reference-contexts: In this model, off-processor values required to compute a designated block of parallel code are obtained immediately before the beginning of the block, and all off-processor values generated within the block are communicated immediately after the end of the block <ref> (Rosing et al., 1992) </ref>. Typically, packets of values are formed for communication and transferred between non-neighboring processors by means of hopping. The wavefront strip is partitioned among the processors and the width of each partition impacts the total computation time.
Reference: <author> Sanz, J. L. C. and Cypher, R. </author> <year> (1992). </year> <title> Data reduction and fast routing: A strategy for efficient algorithms for message-passing parallel computers. </title> <journal> Algorithmica, </journal> <volume> 7(1) </volume> <pages> 77-89. </pages>
Reference-contexts: The segmented prefix problem is a variant of parallel reduction that subdivides a single dimension of processors into non-overlapping contiguous regions of varying size. A multiple prefix algorithm that reduces non-contiguous regions simultaneously for this variant has been presented in <ref> (Sanz and Cypher, 1992) </ref>. None of the published algorithms cope with the overlapping of the regions being reduced. Efficiency of the simultaneous reduction has been discussed in (Szymanski et al., 1992).
Reference: <author> Sarkar, V. </author> <year> (1991). </year> <title> PTRAN the IBM parallel translation system. </title> <editor> In Szymanski, B., editor, </editor> <booktitle> Parallel Functional Languages and Compilers, </booktitle> <pages> pages 309-391. </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: The transformed Fortran program is then compatible with the programs produced by annotating EPL programs. 4.2 Annotation Processing Each virtual processor produces data, typically used by other virtual processors, and in turn consumes data produced by others. By performing data-dependence analysis in a style of PTRAN <ref> (Sarkar, 1991) </ref>, the annotation processor can find the dependencies local to each virtual processor as well as data structures produced and consumed by this processor. All data produced by the processor become local to it and are placed in the its local memory.
Reference: <author> Sheu, J.-P. and Tai, T.-H. </author> <year> (1991). </year> <title> Partitioning and mapping nested loops on multiprocessor systems. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 430-439. </pages>
Reference: <author> Sinharoy, B. and Szymanski, B. </author> <year> (1993). </year> <title> Memory optimization for parallel functional programs. </title> <booktitle> In Abstracts of International Meeting on Vector and Parallel Processing, </booktitle> <address> CICA, Porto, Portugal, </address> <note> full paper submitted to Computing Structures in Engineering. </note>
Reference-contexts: The EPL compiler can often reduce the memory requirement of a program by replacing the entire dimension of an array by a few elements (Szymanski and Prywes, 1988). However, we have proven <ref> (Sinharoy and Szymanski, 1993) </ref> that the problem of finding the optimum replacement is equivalent to the well-known NP-hard problem of determining the maximum weight clique problem.
Reference: <author> Sinharoy, B. and Szymanski, B. </author> <year> (1994a). </year> <title> Data and task alignment in distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1). </volume>
Reference-contexts: Using these definitions, the cost of the spanning tree T can be defined as: C (T ) = e i;j 2E 2 We refer to this principle as Executor Owns rule, it is an inverse of the more commonly used Owner Computes rule. In <ref> (Sinharoy and Szymanski, 1994a) </ref> we have shown an example of computation for which neither of the two rules results in an optimal solution. Ozturan et al. 12 To minimize the total communication cost, proper cut-tree must be found. <p> In (Szymanski and Sinharoy, 1992), we have shown that the data alignment problem for an entire program is NP-hard for all communication cost metrics. In <ref> (Sinharoy and Szymanski, 1994a) </ref>, we proposed an heuristic that starts with an integer approximation of the rational minimum of the cost function when the distance is defined by the second (Euclidean) norm. The initial solution is then iteratively improved by following the steepest decline direction of the cost function. <p> The initial solution is then iteratively improved by following the steepest decline direction of the cost function. Results of using this algorithm on random graphs are encouraging <ref> (Sinharoy and Szymanski, 1994a) </ref>. Here, we focus on the definition of the problem and its impact upon the code generation.
Reference: <author> Sinharoy, B. and Szymanski, B. </author> <year> (1994b). </year> <title> Finding optimal wavefront for parallel computation. </title> <journal> Journal of Parallel Algorithms and Applications, </journal> <volume> 2(1) </volume> <pages> 1-22. </pages>
Reference: <author> Skillicorn, D. </author> <year> (1994). </year> <title> A Model for Practical Parallelism. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, U.K., </address> <note> to appear. </note>
Reference-contexts: Results from executions presented in Table 2 showed up to a 21 percent cost reduction for the MP-1. 6 Conclusion and Comparison with Other Approaches In this section we characterize EPL in terms of criteria that identify important properties of parallel languages <ref> (Skillicorn, 1994) </ref>. Architecture Independence. The same source code is used by the EPL compiler to produce different parallel executables for different architectures.
Reference: <author> Spier, K. and Szymanski, B. </author> <year> (1990). </year> <title> Interprocess analysis and optimization in the Equational Language Compiler. </title> <booktitle> In CONPAR-90. Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <address> Berlin-Heidelberg-New York. </address>
Reference-contexts: The algorithm for finding external data dependences has been presented in <ref> (Spier and Szymanski, 1990) </ref>. The analysis starts by inspecting all atomic processes and then propagates transitive dependences along the paths of the task communication graph restricted to atomic processes. As a result, a configuration dependence file is created and later used by the synthesizer and the code generator.
Reference: <author> Szymanski, B. </author> <year> (1991). </year> <title> EPL parallel programming with recurrent equations. </title> <editor> In Szymanski, B., editor, </editor> <booktitle> Parallel Functional Languages and Environments, </booktitle> <pages> pages 51-104. </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: The summary view of our approach is given in Figure 1. Program components are created by anno Ozturan et al. 3 tating source programs in Fortran or in the functional parallel programming language EPL <ref> (Szymanski, 1991) </ref>. Fortran programs are transformed into an equational form before decomposition. The configuration definition guides the synthesis of the components into a parallel computation. The synthesized computation together with the architecture description is used by the code generator to produce an object code customized for the target architecture. <p> A more detailed description of the language is given in <ref> (Szymanski, 1991) </ref>. Ozturan et al. 5 2.3 Configurations In our approach a parallel computation is viewed as a collection of cooperating processes. Processes are defined as functional programs. Process cooperation is described by a simple macro dataflow specification, called a configuration. Configurations support programming-in-the-large. <p> The corresponding EPL program is shown in Figure 14. The load-balancing scheme can be implemented solely on the basis of the ranges of rows in S. The scheduler implemented in the EPL compiler <ref> (Szymanski, 1991) </ref> detects that the ranges of the rows in S must be available before the matrix-vector multiplication loop starts. Hence, the last two statements in the above EPL program which explicitly implement a simple load-balancing algorithm will always be scheduled before the loop body.
Reference: <author> Szymanski, B. </author> <year> (1994). </year> <title> Scalable software tools for parallel computations. </title> <editor> In Kowlik, J. and Grandinetti, L., editors, </editor> <booktitle> Software for Parallel Computation, volume 106, NATO ASI Series F, </booktitle> <pages> pages 76-90. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: An important step towards an efficient parallelization of Fortran programs with the help of the EPL compiler involves an equational transformation during which the equational equivalent of the program is generated <ref> (Szymanski, 1994) </ref>. The transformed programs obey the single assignment rule and do not contain any control statements (Ge and Prywes, 1990). The transformation is done in the following two steps: 1. Program expansion, during which the variables are expanded to enforce the single assignment rule. <p> The above computation is an example of a reduction evaluated simultaneously over many overlapping continuous sections of an array. Other examples of usage of such operations are likely to be found in cluster recognition, fractal dimension computation in biological modeling <ref> (Szymanski and Caraco, 1994) </ref>, or in modeling physical phenomena (e.g., solvers of partial differential equations characterizing fluid flow).
Reference: <author> Szymanski, B. and Caraco, T. </author> <year> (1994). </year> <title> Spatial analysis of vector-borne disease: A four species model. </title> <journal> Evolutionary Ecology, </journal> <volume> 8, </volume> <publisher> in press. </publisher>
Reference-contexts: An important step towards an efficient parallelization of Fortran programs with the help of the EPL compiler involves an equational transformation during which the equational equivalent of the program is generated <ref> (Szymanski, 1994) </ref>. The transformed programs obey the single assignment rule and do not contain any control statements (Ge and Prywes, 1990). The transformation is done in the following two steps: 1. Program expansion, during which the variables are expanded to enforce the single assignment rule. <p> The above computation is an example of a reduction evaluated simultaneously over many overlapping continuous sections of an array. Other examples of usage of such operations are likely to be found in cluster recognition, fractal dimension computation in biological modeling <ref> (Szymanski and Caraco, 1994) </ref>, or in modeling physical phenomena (e.g., solvers of partial differential equations characterizing fluid flow).
Reference: <author> Szymanski, B., Maniatty, B., and Sinharoy, B. </author> <year> (1992). </year> <title> Simultaneous parallel reduction. </title> <type> Technical Report CS 92-31, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute, Troy, </institution> <note> submitted to Parallel Processing Letters. </note>
Reference-contexts: Scheduler also defines the scopes and nesting of the loops in the object program. The output generated by the scheduler is used by the schedule optimizer and the code generator. 8. Schedule Optimization is an architecture-dependent step that customizes the generated schedule to the target architecture (see, for example, <ref> (McKenney and Szymanski, 1992) </ref> for SIMD specific optimizations). 9. <p> Given the initial allocation of data, the algorithm determines the processors at which the temporary variables should reside and a subexpression evaluation should take place to minimize the communication cost. In <ref> (Szymanski and Sinharoy, 1992) </ref>, we have shown that the data alignment problem for an entire program is NP-hard for all communication cost metrics. <p> To generate efficient code for SIMD machines, one or two dimensions of a data array should be projected along the processor array <ref> (McKenney and Szymanski, 1992) </ref>. For the i-th projected dimension of each array (each equation), we define an alignment function ff i that maps the index of that dimension into the position of the virtual processor that stores (executes) its value. <p> A multiple prefix algorithm that reduces non-contiguous regions simultaneously for this variant has been presented in (Sanz and Cypher, 1992). None of the published algorithms cope with the overlapping of the regions being reduced. Efficiency of the simultaneous reduction has been discussed in <ref> (Szymanski et al., 1992) </ref>.
Reference: <author> Szymanski, B. and Prywes, N. </author> <year> (1988). </year> <title> Efficient handling of data structures in definitional languages. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 10 </volume> <pages> 221-245. </pages>
Reference-contexts: In particular, the reassignments elimination involves replacing the reassigned variables by: * vector (additional dimension) inside loops, * variants in "if" branches and basic blocks. Ozturan et al. 11 2. Program optimization, that consists of: Condition Analysis: Conditions in the transformed program are analyzed using a Sup-Inf inequality prover <ref> (Bruno and Szymanski, 1988) </ref> and the Kaufl variable elimination method (Kaufl, 1988) to find pairwise equivalent or exclusive conditions. Variable's Variants Elimination: Variants created in equivalent and exclusive conditions are merged into a single variable. <p> Variable's Variants Elimination: Variants created in equivalent and exclusive conditions are merged into a single variable. Additional Dimension Elimination: During scheduling and code generation for individual processes, memory optimization is performed to replace entire dimensions by windows of few elements for multidimensional variables <ref> (Szymanski and Prywes, 1988) </ref>. This step restores the memory efficiency of the original program. <p> This extra temporal dimension allows the program to be specified without any reassignments but, unless optimized, may require an exorbitant amount of memory. The EPL compiler can often reduce the memory requirement of a program by replacing the entire dimension of an array by a few elements <ref> (Szymanski and Prywes, 1988) </ref>. However, we have proven (Sinharoy and Szymanski, 1993) that the problem of finding the optimum replacement is equivalent to the well-known NP-hard problem of determining the maximum weight clique problem.
Reference: <author> Szymanski, B. and Sinharoy, B. </author> <year> (1992). </year> <title> Complexity of the closest vector problem in a lattice generated by (0,1)-matrix. </title> <journal> Information Processing Letters, </journal> <volume> 42 </volume> <pages> 141-146. </pages>
Reference-contexts: Scheduler also defines the scopes and nesting of the loops in the object program. The output generated by the scheduler is used by the schedule optimizer and the code generator. 8. Schedule Optimization is an architecture-dependent step that customizes the generated schedule to the target architecture (see, for example, <ref> (McKenney and Szymanski, 1992) </ref> for SIMD specific optimizations). 9. <p> Given the initial allocation of data, the algorithm determines the processors at which the temporary variables should reside and a subexpression evaluation should take place to minimize the communication cost. In <ref> (Szymanski and Sinharoy, 1992) </ref>, we have shown that the data alignment problem for an entire program is NP-hard for all communication cost metrics. <p> To generate efficient code for SIMD machines, one or two dimensions of a data array should be projected along the processor array <ref> (McKenney and Szymanski, 1992) </ref>. For the i-th projected dimension of each array (each equation), we define an alignment function ff i that maps the index of that dimension into the position of the virtual processor that stores (executes) its value. <p> A multiple prefix algorithm that reduces non-contiguous regions simultaneously for this variant has been presented in (Sanz and Cypher, 1992). None of the published algorithms cope with the overlapping of the regions being reduced. Efficiency of the simultaneous reduction has been discussed in <ref> (Szymanski et al., 1992) </ref>.
Reference: <author> Wu, J., Saltz, J., Berryman, H., and Hiranandani, S. </author> <year> (1991). </year> <title> Distributed memory compiler design for sparse problems. </title> <type> Technical Report 91-13, </type> <institution> ICASE, Hampton, VA. </institution>
Reference: <author> Zima, H., Brezany, P., Chapman, B., Mehrotra, P., and Schwald, A. </author> <year> (1992). </year> <title> Vienna Fortran a language specification version 1.1. </title> <type> Technical Report Interim 21, </type> <institution> ICASE, Hampton, VA. </institution>
Reference-contexts: well as differences between the EPL annotations and the Fortran language extensions that have been introduced in many systems, e.g., Vienna Fortran (Chapman et al., 1992; Zima et al., 1992; Benkner et al., 1992), Fortran D (Fox et al., 1991; Hiranandani et al., 1992; Hi-ranandani et al., 1991a) and SUPERB <ref> (Gerndt and Zima, 1992) </ref>. Vienna Fortran provides directives for array-like processor structure definition. The distribution of arrays can be specified at compile-time through the use of a DIST directive with BLOCK or CYCLIC options. INDIRECT directives can be added to indicate run-time distribution.
References-found: 61

