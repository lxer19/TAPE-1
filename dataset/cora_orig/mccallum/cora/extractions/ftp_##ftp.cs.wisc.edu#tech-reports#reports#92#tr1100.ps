URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/92/tr1100.ps
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/
Root-URL: 
Title: TOWARD THE DESIGN OF LARGE-SCALE, SHARED-MEMORY MULTIPROCESSORS  
Author: By STEVEN LEE SCOTT 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1992  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [Adve90] <author> Adve, S. V. and M. D. Hill, </author> <title> Weak Ordering ANew Definition, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 2-14. </pages>
Reference-contexts: Second, there must exist some global ordering of reads and writes to a given memory location, such that no processor observes any other order. In addition to cache coherence, a multiprocessor may provide sequential consistency [Lamp78], or some weaker form of memory consistency <ref> [Dubo86, Adve90, Ghar90] </ref>, which makes a guarantee about the global ordering of reads and writes to different memory locations . Providing some form of memory consistency can significantly impact the cache coherence mechanism. <p> I restrict my attention to Dir i B, as Dir i NB does not allow data to be globally shared. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 Using relaxed memory semantics <ref> [Dubo86, Adve90, Ghar90] </ref> the latency of most writes (including invalidation of shared copies and collection of the corresponding acknowledgements) can be tolerated. Writes that have not completed at synchronization points, however, can potentially delay program execution.
Reference: [Agar88] <author> Agarwal, A., R. Simoni, J. Hennessy, and M. Horowitz, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: Either the number of shared copies of a line must be limited to i, or when the number exceeds i, this fact must be recorded and a broadcast invalidate must be issued when the line is modified. Agarwal, Simoni, Hennessy and Horowitz <ref> [Agar88] </ref> suggested a label of Dir i B or Dir i NB to represent the versions of this protocol which do, and do not, use broadcast, respectively. <p> They found that the number of shared lines that needed invalidation on a write was typically low, but that the distribution did spread out as the system size increased. It is difficult to say what the distribution would look like with 100, 1000, or 64K processors. Agarwal, et al <ref> [Agar88] </ref>, evaluated Dir 1 NB and Dir 0 B as well as two snooping protocols (Write-Through-With-Invalidate and Dragon) for three 4-way-parallel traces and found Dir 0 B to be competitive with the Dragon protocol. <p> All read requests are routed directly to memory with no Ch. 6 119 combining. This is equivalent to the Dir 1 BC protocol discussed in Chapter 5, Section 1.1 <ref> [Agar88] </ref>. PC own , as described in Section 2.2, is equivalent to PC dir , save that the location of the first reader of a shared line is maintained using a single directory pointer. All read requests are routed directly to memory with no combining. <p> The storage overhead for a full width directory consists simply of the pointers associated with each line of main memory: Ch. 6 138 (6.2)memory (full width) = N 2 M (6.3)cache (full width) = 0 Similarly, the storage overhead for limited pointer directories <ref> [Agar88] </ref> consists of the pointers associated with each line of main memory: (6.4)memory (Dir i B) = NMiP (6.5)cache (Dir i B) = 0 Dir i B reduces the amount of information maintained by the coherence protocol by not fully keeping track of shared data.
Reference: [Agar90] <author> Agarwal, A., B.-H. Lim, D. Kranz, and J. Kubiatowicz, </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Archtecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 104-114. </pages>
Reference-contexts: Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife <ref> [Agar90] </ref>, Tera supercomputer [Alve90], CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor [Leno89]. The most commonly used direct networks are variants of the k-ary n-cube. Recall that a k-ary n-cube consists of N =k n nodes, arranged in n dimensions with k nodes per dimension.
Reference: [Agar91] <author> Agarwal, A., </author> <title> Limits on Interconnection Network Performance, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991, </year> <pages> 398-412. </pages>
Reference-contexts: Chapter 4 extends consideration of pipelined channels to large, multidimensional networks. It is shown that not only do pipelined channels provide performance superior to non-pipelined channels for large networks, but that they fundamentally alter the network design tradeoffs. Previous studies of non-pipelined-channel networks <ref> [Dall90, Agar91] </ref> have shown that low-dimensional networks provide the best performance. By changing the effects of wire length on network performance, pipelined channels argue for higher dimensionality. <p> Chapter 4 demonstrates that a pipelined-channel network is optimally grown by holding the radix (k) constant and increasing the dimensionality (n). This allows networks to scale well, according to the definition of Section 2. In addition to wire length, high-dimensional networks can also be penalized by various wiring constraints <ref> [Dall90, Agar91] </ref>. The constant node size constraint, motivated by board- and chip-level pin limitations, holds the number of wires per node constant as the dimensionality of a fixed sized network is varied. <p> The result of this is that the optimal radix of the network increases significantly for large systems. Agarwal <ref> [Agar91] </ref> extended Dally's analysis in two important ways. First, he included switching time in the latency equations, which was missing in Dally's study. Second, he considered a weaker wiring constraint: constant node size. <p> Ch. 4 47 2.1. Model and assumptions The first assumption made in this work is that we are dealing with uni-directional links. There are two reasons for doing this. First, previous analysis <ref> [Dall90, Agar91] </ref> has focused on uni-directional networks. Second, pipelined channels are naturally uni-directional. While the results are qualitatively similar for bi-directional networks, some of the details change (see Section 2.6). The network node model used here is shown in Figure 4.2. <p> 1 hh J N 1/3 hhhhh M O 1 hh J N 1/3 hhhhh M O if k &gt;2 Since the maximum wire delay is suffered over all links, including the short ones, the total delay due to wire transmission is increased as the dimensionality of a network is increased <ref> [Dall90, Agar91] </ref>. In a pipelined-channel network, the number of cycles spent traversing a given wire is deter mined by that wire's length only. Thus T wire avg is a function of the mean wire length rather than the maximum wire length. <p> The constant link width constraint may be realistic for sufficiently small systems, depending upon the technology used to implement the network. Intra-node data paths, for example, may dictate the link width and the pin limitations may Ch. 4 54 not be restrictive. Agarwal <ref> [Agar91] </ref> considers all three constraints, with the emphasis on the constant node size constraint. 2.6. Bi-directional networks Pipelined channels are naturally uni-directional, and the analysis thus far has assumed unidirectional networks. This section briefly discusses bi-directional networks. <p> Section 4.1 investigates the effect of varying the ratio of switching to transmission time, S. Section 4.2 investigates the effect of varying the packet length. The results differ somewhat from results obtained previously for non-pipelined-channel networks <ref> [Agar91] </ref>. 4.1. Effect of switching overhead pipelined-channel, 4096-node network as the ratio of switch cycle time to base wire delay, S, is varied from 0 to 8. Latencies are normalized to the cycle time with S =1. <p> Constant link width y n t L Dimension 400 300 200 100 0 S=8 S=2 S=0 c e a Dimension 500 400 300 200 100 0 S=4 S=1 S=0.5 (b) Constant node size (c) Constant bisection Ch. 4 75 the optimal dimensionality by increasing the impact of longer wire lengths <ref> [Agar91] </ref>. As switching speeds continue to increase, the value of S in real systems will become smaller. This will cause wire transmission delay to become dominant, and the unloaded latency in pipelined-channel networks will become less sensitive to dimensionality. <p> Effect of packet lengths 4096-node network as the packet length, L, is varied from 64 to 1024 bits. Parts (a), (b) and (c) assume the constant link width, constant node size and constant bisection constraints, respectively. In non-pipelined-channel networks, longer packet lengths always decrease the optimal dimensionality <ref> [Agar91] </ref>. This is because the number of flits, P, becomes dominant over the number of network hops (see Equation (4.5)), making the reduction in number of hops in high-dimensional networks less important relative to the increased wire delay.
Reference: [Alve90] <author> Alverson, R., D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith, </author> <title> The Tera Computer System, </title> <booktitle> Proc. 1990 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: At what size this asymptotic behavior becomes relevant is not clear, and will depend upon the technology being used. However, as an example of this principle applied, the Tera Computer System populates a 3-dimensional mesh of size r 3 with only r 2 processors <ref> [Alve90] </ref>. Considering this, the tightest absolute requirement that we can impose is that latency cannot grow faster than O (N 1/2 ). <p> Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife [Agar90], Tera supercomputer <ref> [Alve90] </ref>, CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor [Leno89]. The most commonly used direct networks are variants of the k-ary n-cube. Recall that a k-ary n-cube consists of N =k n nodes, arranged in n dimensions with k nodes per dimension. Figure 4.1 illustrates several different k-ary n-cubes.
Reference: [Amda67] <author> Amdahl, G. M., </author> <title> Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities, </title> <booktitle> Proc. AFIPS Conference, </booktitle> <month> April </month> <year> 1967, </year> <pages> 483-485. </pages>
Reference-contexts: Section 3 briefly discusses the effect of software on scalability. Finally, sections 4 and 5 apply scalability arguments to network topologies and cache coherence mechanisms, respectively. 1. Background The issue of scalability has surfaced many times since the introduction of parallel computers. Amdahl <ref> [Amda67] </ref> originally observed that the efficiency of a parallel computer will Ch. 2 6 decrease as more processors are used to solve a fixed-size problem. This is due to the (reasonable) assumption of a fixed serial portion of the algorithm.
Reference: [Arch84] <author> Archibald, J. and J. -L. Baer, </author> <title> An Economical Solution to the Cache Coherence Problem, </title> <booktitle> Proc. 11th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 355-362. </pages>
Reference-contexts: Topologies that provide scalable bandwidth necessarily lack the property that all communications are broadcast, and thus traditional snooping protocols are not feasible. Directory-based protocols rely instead on coherence information stored in special-purpose directories associated with main memory. While broadcast invalidations can still be used <ref> [Arch84] </ref>, most proposed designs maintain records of cached data, and selectively invalidate cached lines when they are modified. The former approach doesn't scale because of bandwidth constraints, and the latter poses difficulties because of the possibly very large amount of storage necessary to keep track of heavily shared cache lines.
Reference: [Baer88] <author> Baer, J.-L. and W.-H. Wang, </author> <title> On the Inclusion Properties for Multi-Level Cache Hierarchies, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> 73-80. </pages>
Reference-contexts: A broadcast operation is simulated by passing a message completely around the ring. This does not provide global event ordering, however, so modifications are necessary for snooping protocols that depend on this. One topology that has been widely studied for large numbers of processors <ref> [Wils87, Baer88, Wins88, Vern89, Yang92] </ref> is the bus hierarchy (see Figure 2.1). This topology will not scale, however, for uniform workloads. <p> Thus, just as the global bit vector scheme, the hierarchical scheme requires O (N 2 ) storage for the directories. 1.9. Multi-level inclusion A coherence mechanism that is similar to hierarchical directories in some ways is the multi-level inclusion (MLI) property <ref> [Lam79, Wils87, Baer88] </ref>. The MLI property requires that a cache in a hierarchy contain a superset of all lines residing beneath it in the hierarchy. When the line is invalidated, a parent only propagates the invalidate to its children if it has a copy of the line. <p> This means that either an inclusion cache must be built such that all lines that can possibly reside simultaneously in the subtree beneath it can also reside simultaneously in the inclusion cache <ref> [Baer88] </ref>, or when an inclusion cache has to eject an entry, it must invalidate the corresponding line in the subtree beneath it [Wils87]. The first solution is only practical in a single tree system, where the number of caches decreases at each higher level.
Reference: [Bell85] <author> Bell, C. G., Multis: </author> <title> a New Class of Multiprocessor Computers, </title> <booktitle> Science 228, </booktitle> <month> April </month> <year> 1985, </year> <pages> 462-467. </pages>
Reference-contexts: practical result regarding pipelined channels and dimensionality, then, may be to simply choose a higher dimensionality for the desired range of system sizes than would have been chosen using non-pipelined-channel networks. 79 Chapter 5 Cache Coherence for Large-Scale Multiprocessors The most common form of shared-memory multiprocessor today is the multi <ref> [Bell85] </ref>, characterized by a shared bus and per-processor snooping caches. A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88].
Reference: [Benn88] <author> Benner, R. E., J. L. Gustafson, and R. E. Montry, </author> <title> Development and analysis of scientific application programs on a 1024-processor hypercube, </title> <type> SAND 88-0317, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: This reflects the nature of at least some actual parallel codes <ref> [Benn88] </ref> and would appear to be a necessary condition in order to maintain high processor efficiency for very large systems.
Reference: [Bork90] <author> Borkar, S., R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. </author> <title> 163 Webb, Supporting Systolic and Memory Communication in iWarp, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 70-81. </pages>
Reference-contexts: Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife [Agar90], Tera supercomputer [Alve90], CMU-Intel iWarp <ref> [Bork90] </ref>, and Stanford DASH multiprocessor [Leno89]. The most commonly used direct networks are variants of the k-ary n-cube. Recall that a k-ary n-cube consists of N =k n nodes, arranged in n dimensions with k nodes per dimension. Figure 4.1 illustrates several different k-ary n-cubes. <p> Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp <ref> [Bork90] </ref>, various Cray Research machines [Smit92] and the Thinking Machines CM5 [TMC91]. A higher degree of pipelining is achievable using the Caltech Slack chip [Seit92], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables.
Reference: [Burc90] <author> Burch, J. R., E. M. Clarke, K. L. McMillan, D. L. Dill, and J. Hwang, </author> <title> Symbolic Model Checking: 1020 States and Beyond, </title> <booktitle> Proc. Fifth Anual Symposium on Login in Computer Science, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Testing via stochastic simulation has proven an effective means of verification [Wood90]. However, before implementing such a complex protocol in hardware, more rigorous verification techniques are recommended <ref> [Burc90] </ref>. The four protocols differ in how they handle shared read misses, and write misses that find the data globally shared (requiring invalidations). The base protocol (labeled Direct in Figures 6.1 through 6.6) routes all read requests directly to memory, performing no read combining.
Reference: [Cens78] <author> Censier, L. M. and P. Feautrier, </author> <title> A New Solution to Coherence Problems in Multicache Systems, </title> <journal> IEEE Transactions on Computers C-27(12), </journal> <month> December </month> <year> 1978, </year> <pages> 1112-1118. </pages>
Reference-contexts: Full-width global directory Another possible coherence mechanism is to keep track of all shared copies of a line in a global directory. The directory can be distributed along with the memory of the system (as opposed to a "centralized" directory as proposed by Tang [Tang76] ). Censier and Feautrier <ref> [Cens78] </ref> proposed keeping a bit vector of size N for each line, with the corresponding bit set for every processor that has a copy of the line. When the line is invalidated, individual messages are sent to each processor whose bit is set. <p> Section 3 presents a mechanism for hierarchical read combining in a k-ary n-cube, based upon the read combining in multis. The hierarchical nature of pruning-cache directories makes them compatible with this read combining mechanism. 1. Survey of Cache Coherence Mechanisms Recall that a full-width global directory <ref> [Cens78] </ref> includes an N-bit vector along with each line of main memory. As discussed in Chapter 2, the full-width directory does not scale in cost, as it requires O (N 2 ) directory space (assuming a total memory size that grows linearly with the number of processors).
Reference: [Chai90] <author> Chaiken, D., C. Fields, K. Kurihara, and A. Agarwal, </author> <title> Directory-Based Cache Coherence in Large-Scale Multiprocessors, </title> <booktitle> IEEE Computer 23(6), </booktitle> <month> June </month> <year> 1990, </year> <pages> 49-58. </pages>
Reference-contexts: In more recent work, Chaiken, Fields, Kurihara and Agarwal <ref> [Chai90] </ref> compared limited directories (without broadcast) to full map directories for several 16- and 64-way parallel codes running on a theoretical multiprocessor with a multi-stage interconnection network. They found that half of their codes performed significantly worse with limited pointer directories than with the full map directory.
Reference: [Chai91] <author> Chaiken, D., J. Kubiatowicz, and A. Agarwal, </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme, </title> <booktitle> Proc. ASPLOS IV, </booktitle> <month> April </month> <year> 1991, </year> <pages> 224-234. </pages>
Reference-contexts: Ch. 5 82 1.2. LimitLESS directories Another way to deal with overflow in a limited pointer directory is to handle it in software. This is done in the LimitLESS (Limited directories, Locally Extended through Software Support) cache coherence protocol for the MIT Alewife machine <ref> [Chai91] </ref>. The LimitLESS protocol maintains a distributed hardware directory with a small number of pointers per cache line.
Reference: [Cher89] <author> Cheriton, D. R., H. A. Goosen, and P. D. Boyle, </author> <title> Multi-Level Shared Caching Techniques for Scalability in VMP-MC, </title> <booktitle> Proc. 16th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1989, </year> <pages> 16-24. </pages>
Reference-contexts: Using asymptotic behavior, he defines 11 classes of scalability, including uniformly, architecturally and implementationally scalable. On the other hand, Hill [Hill90], questions whether scalability can be usefully defined at all. He challenges the technical community to either define the term rigorously, or stop using it altogether. Others <ref> [Leno90, Cher89, Hage89] </ref> use the term without accompanying definition, relying on the readers' intuitive definitions. I believe that a rigorous definition of scalability may be of little use, but that we can arrive at a useful working definition. Several qualifications need to be offered at the start, however. <p> The logical inclusion bit for a line is considered to be set if an entry for that line exists in the inclusion cache, and cleared if there is no entry. The VMP-MC system <ref> [Cher89] </ref> enforces multi-level inclusion within a VMP node, but uses directory entries similar to pruning vectors rather than inclusion bits. A pruning vector at a given Ch. 5 88 node is simply the collection of its childrens' inclusion bits for the same line.
Reference: [Dall87] <author> Dally, W. J., </author> <title> Deadlock Free Message Routing in Multiprocessor Interconnection Networks, </title> <journal> IEEE Transactions on Computers C-36(5), </journal> <month> May </month> <year> 1987, </year> <pages> 547-553. </pages>
Reference-contexts: This decouples link throughput from link latency, and fundamentally changes the network design tradeoffs. The pipelined-channel routing protocol described in Section 2.1 of this chapter also provides some of the same benefits as virtual channels <ref> [Dall87, Dall92] </ref>. By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [Scot92] and Section 4 of this chapter). <p> With conventional blocking networks, the packet would simply make its way around the ring, blocking when necessary to allow queues to drain, and allowing each marked node to retain a copy. Normal deadlock prevention techniques, such as virtual channels <ref> [Dall87] </ref>, must be used in this case. Pipelined-channel rings as discussed in this thesis could also implement broadcast. Nodes would clear their respective bits in the bit mask as the packet traversed the ring.
Reference: [Dall90] <author> Dally, W. J., </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks, </title> <journal> IEEE Transactions on Computers 39(6), </journal> <month> June </month> <year> 1990, </year> <pages> 775-785. </pages>
Reference-contexts: Chapter 4 extends consideration of pipelined channels to large, multidimensional networks. It is shown that not only do pipelined channels provide performance superior to non-pipelined channels for large networks, but that they fundamentally alter the network design tradeoffs. Previous studies of non-pipelined-channel networks <ref> [Dall90, Agar91] </ref> have shown that low-dimensional networks provide the best performance. By changing the effects of wire length on network performance, pipelined channels argue for higher dimensionality. <p> When the number of logical dimensions exceeds the number of physical dimensions in which the network is implemented (which, alas, is fundamentally limited to three), then the processors can no longer be connected solely to physically close neighbors, and the network wire lengths must grow <ref> [Dall90] </ref>. This means that the hop latency is no longer O (1) for high-dimensional networks. A similar phenomenon occurs for omega networks, where even small- to medium-scale networks have long wires. <p> Chapter 4 demonstrates that a pipelined-channel network is optimally grown by holding the radix (k) constant and increasing the dimensionality (n). This allows networks to scale well, according to the definition of Section 2. In addition to wire length, high-dimensional networks can also be penalized by various wiring constraints <ref> [Dall90, Agar91] </ref>. The constant node size constraint, motivated by board- and chip-level pin limitations, holds the number of wires per node constant as the dimensionality of a fixed sized network is varied. <p> Under these assumptions, binary hypercubes appear very attrac tive, delivering the lowest latency and link traffic of any k-ary n-cube configuration. These assumptions are unrealistic, however. Dally has investigated network performance while taking wire delay into account and applying the constant bisection constraint <ref> [Dall90] </ref>. This constraint holds the number of wires Ch. 4 45 crossing the bisection of a network constant as the dimensionality is varied, which causes the link width to decrease as dimensionality is increased. <p> In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [Scot92] and Section 4 of this chapter). Conventional networks using flit-level flow control typically provide half or less of their maximum throughput, due to coupled resource allocation <ref> [Dall90] </ref>. Note, however, that the switches described in Section 2.1 require buffers able to contain an entire packet. Pipelined channels have long been used in wide area networks and local area networks, since the physical delays involved compel their use. <p> Ch. 4 47 2.1. Model and assumptions The first assumption made in this work is that we are dealing with uni-directional links. There are two reasons for doing this. First, previous analysis <ref> [Dall90, Agar91] </ref> has focused on uni-directional networks. Second, pipelined channels are naturally uni-directional. While the results are qualitatively similar for bi-directional networks, some of the details change (see Section 2.6). The network node model used here is shown in Figure 4.2. <p> Wire length Wire lengths depend upon network dimensionality and layout. I assume that the network is being implemented in three physical dimensions (this differs from <ref> [Dall90] </ref>, where two dimensions are assumed). I also make the simplifying assumption that interprocessor spacing is equal in all dimensions. The long wraparound links in a torus (see Figure 4.1 (a)) can be avoided by folding the network as shown in Figure 4.1 (b). <p> 1 hh J N 1/3 hhhhh M O 1 hh J N 1/3 hhhhh M O if k &gt;2 Since the maximum wire delay is suffered over all links, including the short ones, the total delay due to wire transmission is increased as the dimensionality of a network is increased <ref> [Dall90, Agar91] </ref>. In a pipelined-channel network, the number of cycles spent traversing a given wire is deter mined by that wire's length only. Thus T wire avg is a function of the mean wire length rather than the maximum wire length. <p> The number of wires across the bisection is B = 2Wk n -1 , so link width is given by (4.10) W const_bisec = k I L 2N M O The constant bisection constraint was used by Dally <ref> [Dall90] </ref> in order to reflect the limited wiring area of a network implemented on a single VLSI substrate. A multiprocessor implemented across multiple boards may also be bisection constrained, or may be node-size constrained due to pin limitations off-chip and off-board.
Reference: [Dall92] <author> Dally, W. J., </author> <title> Virtual-Channel Flow Control, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 3(2), </journal> <month> March </month> <year> 1992, </year> <pages> 194-205. </pages>
Reference-contexts: This decouples link throughput from link latency, and fundamentally changes the network design tradeoffs. The pipelined-channel routing protocol described in Section 2.1 of this chapter also provides some of the same benefits as virtual channels <ref> [Dall87, Dall92] </ref>. By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see [Scot92] and Section 4 of this chapter).
Reference: [Dubo86] <author> Dubois, M., C. Scheurich, and F. Briggs, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1986, </year> <pages> 434-442. </pages>
Reference-contexts: Second, there must exist some global ordering of reads and writes to a given memory location, such that no processor observes any other order. In addition to cache coherence, a multiprocessor may provide sequential consistency [Lamp78], or some weaker form of memory consistency <ref> [Dubo86, Adve90, Ghar90] </ref>, which makes a guarantee about the global ordering of reads and writes to different memory locations . Providing some form of memory consistency can significantly impact the cache coherence mechanism. <p> I restrict my attention to Dir i B, as Dir i NB does not allow data to be globally shared. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 Using relaxed memory semantics <ref> [Dubo86, Adve90, Ghar90] </ref> the latency of most writes (including invalidation of shared copies and collection of the corresponding acknowledgements) can be tolerated. Writes that have not completed at synchronization points, however, can potentially delay program execution.
Reference: [Egge89] <author> Eggers, S. J. and R. Katz, </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs, </title> <booktitle> Proc. ASPLOS III, </booktitle> <month> April </month> <year> 1989, </year> <pages> 257-270. </pages>
Reference-contexts: However, when one of the above conditions does not hold, a write update protocol may cause excessive update traffic and a write invalidate protocol may perform better. The choice of whether an update or invalidate protocol should be used has been analyzed by Eggers and Katz for single-bus multis <ref> [Egge89] </ref>. They concluded that invalidate protocols were preferable for the workloads they analyzed. I argue that given that write invalidate performs better than write update for a single-bus multi, that its performance advantages remain the same or increase as system size increases.
Reference: [Enco86] <institution> Encore,, Multimax Technical Summary, Encore Computer Corporation, </institution> <year> 1986. </year>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Feng81] <author> Feng, T., </author> <title> A Survey of Interconnection Networks, </title> <booktitle> Computer, </booktitle> <month> December, </month> <year> 1981, </year> <pages> 12-27. 164 </pages>
Reference-contexts: The task of this chapter, then, is to thoroughly analyze the effects of pipelined channels on large, multidimensional networks. 1. Background A wide variety of interconnection networks have been proposed in the literature (see <ref> [Feng81] </ref> or [Sieg79] for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network [Lawr75], connect processors and memories through multiple intermediate stages of switching elements.
Reference: [Flat89] <author> Flatt, H. P. and K. Kennedy, </author> <title> Performance of Parallel Processors, </title> <booktitle> Parallel Computation 31, </booktitle> <year> 1989, </year> <pages> 1-20. </pages>
Reference-contexts: This eventually dooms attempts to speed up the execution of the program by using more processors. Flatt and Kennedy <ref> [Flat89] </ref>, in fact, showed that for real programs a maximum speedup is obtained using some number of processors, beyond which additional processors can only increase the running time.
Reference: [Fort78] <author> Fortune, S. and J. Wyllie, </author> <title> Parallelism in Random Access Machines, </title> <booktitle> Proc. Tenth ACM Symposium on Theory of Computing, </booktitle> <year> 1978, </year> <pages> 114-118. </pages>
Reference-contexts: Their metric is the asymptotic speedup of a given, fixed-size algorithm on a real machine with unlimited processors relative to the speedup of the same algorithm on an EREW PRAM <ref> [Fort78] </ref>. Their definition is precise, but rather theoretical in nature, dealing only with asymptotic behavior and ignoring any cost or efficiency considerations. Johnson [John90] presents a rigorous set of definitions for scalable hardware which is independent of the workload.
Reference: [Fran84] <author> Frank, S. J., </author> <title> Tightly Coupled Multiprocessor System Speeds Up Memory Access Times, </title> <publisher> Electronics 5(1), </publisher> <month> January </month> <year> 1984, </year> <pages> 164-169. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Ghar90] <author> Gharachorloo, K., D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hen-nessy, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: Second, there must exist some global ordering of reads and writes to a given memory location, such that no processor observes any other order. In addition to cache coherence, a multiprocessor may provide sequential consistency [Lamp78], or some weaker form of memory consistency <ref> [Dubo86, Adve90, Ghar90] </ref>, which makes a guarantee about the global ordering of reads and writes to different memory locations . Providing some form of memory consistency can significantly impact the cache coherence mechanism. <p> I restrict my attention to Dir i B, as Dir i NB does not allow data to be globally shared. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 Using relaxed memory semantics <ref> [Dubo86, Adve90, Ghar90] </ref> the latency of most writes (including invalidation of shared copies and collection of the corresponding acknowledgements) can be tolerated. Writes that have not completed at synchronization points, however, can potentially delay program execution.
Reference: [Good83] <author> Goodman, J. R., </author> <title> Using Cache Memory to Reduce Processor-Memory Traffic, </title> <booktitle> Proc. 10th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1983, </year> <pages> 124-131. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Good88] <author> Goodman, J. R. and P. J. Woest, </author> <title> The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> 422-431. </pages>
Reference-contexts: For example, a 2-dimensional torus with 64 processors is a 8-ary 2-cube. While k-ary n-cube generally refers to a network of point-to-point links, a similar network can be implemented using buses, with k processors per bus and n buses per processor. Goodman and Woest <ref> [Good88] </ref> refer to such a network as a multicube. Figure 2.3 shows a 4-ary 3-cube multiprocessor, implemented with unidirectional links.
Reference: [Good89] <author> Goodman, J. R., M. D. Hill, and P. J. Woest, </author> <title> Scalability and Its Application to Multicube, </title> <type> Computer Sciences Technical Report #835, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The focus of Chapters 5 and 6 switches to cache coherence mechanisms. Chapter 5 presents a summary of existing and proposed cache coherence mechanisms and describes a novel approach for maintaining coherence in hierarchical or cube-based multiprocessors. Approximate directory information is held in special-purpose pruning caches <ref> [Good89, Scot91] </ref>, and used to limit the propagation of broadcast invalidations to those parts of the hierarchy that need to receive them. This scheme approximates the performance of a full hierarchical directory, but at considerably less cost. <p> There have been several suggestions as to what scalability means. Patton [Patt85] stated that a scalable design can be adjusted up or down in size without loss of functionality to scale effects. Goodman, Hill and Woest <ref> [Good89] </ref> define a scalable algorithm as one whose serial portion does not grow with problem size and whose parallel portion contains parallelism at least proportional to the algorithm's complexity. <p> Although the directory overhead for MLI scales, it can still be quite large. This is addressed further in the next chapter. 2. Pruning-Cache Directories A novel way of scaling hierarchical directories is to limit their size and manage them as caches (pruning caches) <ref> [Good89, Scot91] </ref>. We no longer require a sub-directory to contain the pruning vector for a given line when it is accessed. If it contains the entry, then we proceed as with the hierarchical directory.
Reference: [Good89a] <author> Goodman, J. R., M. K. Vernon, and P. J. Woest, </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors, </title> <booktitle> Proc. ASPLOS III, </booktitle> <month> April </month> <year> 1989, </year> <pages> 64-75. </pages>
Reference-contexts: Synchronization can be handled in a variety of manners, both in hardware and in software. Examples include software combining [Yew87], software queueing [Mell91] and hardware queueing <ref> [Good89a] </ref>. Although synchronization issues are important, I do not address them in this thesis. I do address the problem of read contention, however, as it is closely related to the cache coherence mechanism. <p> Mechanisms that support locks using software or hardware queueing can prevent lock contention, and can be used as primitives to support contention free barriers and other synchronization operations as well <ref> [Mell91, Good89a] </ref>. In addition, the QOLB hardware synchronization primitive [Good89a] can improve the performance of pruning-cache directories by removing interference from migratory data. QOLB automatically migrates lock-protected data from one cache to another, allowing the data to remain in globally modified state. <p> Mechanisms that support locks using software or hardware queueing can prevent lock contention, and can be used as primitives to support contention free barriers and other synchronization operations as well [Mell91, Good89a]. In addition, the QOLB hardware synchronization primitive <ref> [Good89a] </ref> can improve the performance of pruning-cache directories by removing interference from migratory data. QOLB automatically migrates lock-protected data from one cache to another, allowing the data to remain in globally modified state. Producer-consumer and pairwise-shared data can likewise be managed directly using QOLB, thus circumventing the cache coherence mechanism.
Reference: [Gott83] <author> Gottlieb, A., R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir, </author> <title> The NYU Ultracomputer -- Designing a MIMD, Shared Memory Parallel Machine, </title> <journal> IEEE Transactions on Computers C-32(2), </journal> <month> February </month> <year> 1983, </year> <pages> 175-189. </pages>
Reference-contexts: Although the length of the longest wire is O (N ) if the network is laid out as shown in Figure 2.2, a good three dimensional layout can result in maximum wire lengths of O (N 1/2 ) <ref> [Gott83] </ref>. The remainder of this thesis will consider only k-ary n-cube networks, although much of the analysis would be germane to multistage networks as well. <p> Contention for a particular memory location or module in a large system is known as a hot spot, and has been the focus of much research. Hardware mechanisms for combining requests in the network have been proposed <ref> [Gott83, Pfis85] </ref>, as well as mechanisms to improve network performance in the face of contention [Tami88, Scot90, Lang88]. These mechanisms are not necessary under the uniform workload assumption, as the average rate of read requests to a line is independent of system size. <p> Any reads that the compiler or programmer had reason to suspect would cause read contention would be marked as combinable, and would use the standard combining pruning cache protocol discussed in Section 1. This is quite similar to the idea of having two networks in the NYU Ultracomputer <ref> [Gott83] </ref>, one for regular traffic and one for combinable traffic, except that here it would be done by specifying the type of read requests traversing a single network in order to change the way in which the communication protocol handled the messages.
Reference: [Gupt90] <author> Gupta, A., W. Weber, and T. Mowry, </author> <title> Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes, </title> <booktitle> Proc. 1990 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990, </year> <month> I312-I321. </month>
Reference-contexts: This will prevent workloads with any significant read contention from performing well on large systems. 1.3. Coarse vectors Another way to limit the size of directories is to use coarse vectors for the entries <ref> [Gupt90] </ref>. A coarse vector is similar to a full width directory entry, except that each bit represents a region of two or more processors. When a processor reads a line, the bit for that processor's region is set in the line's directory entry. <p> When a processor reads a line, the bit for that processor's region is set in the line's directory entry. When a line is invalidated, messages are sent to all processors in the regions whose bits are set. Gupta, et al <ref> [Gupt90] </ref>, suggest that the directories be structured such that an entry can hold one or more processor pointers and then switch over to a coarse vector representation when the pointers overflow. <p> Directory caches Several researchers have proposed caching directory information to exploit the fact that most lines of memory are not cached by any processor. Weber, Gupta and Mowry <ref> [Gupt90] </ref> referred to these directory caches as sparse directories, and suggested that entries could be either full width, limited pointer or coarse vector, depending on the size of the system and the amount of memory allotted for directory storage. <p> The performance of this mechanism depends upon the hit ratio, h, of the pruning caches. When h =1, the pruning caches act identically to a full hierarchical directory. When h =0, the top level pruning vector (stored with memory) acts as a coarse vector <ref> [Gupt90] </ref>. <p> The memory overhead is the same as for Dir i B, and the cache overhead is now (6.6) cache (Dir i vector) = m =i +1 N The directory cache scheme <ref> [O'Kr90, Gupt90] </ref> associates no directory information with main memory, but use two kinds of caches, one with i processor pointers per entry and one with full width vectors.
Reference: [Gust88] <author> Gustafson, J. L., Reevaluating Amdahl's Law, </author> <booktitle> Communications of the ACM 31(5), </booktitle> <month> May </month> <year> 1988, </year> <pages> 532-533. </pages>
Reference-contexts: This eventually dooms attempts to speed up the execution of the program by using more processors. Flatt and Kennedy [Flat89], in fact, showed that for real programs a maximum speedup is obtained using some number of processors, beyond which additional processors can only increase the running time. Gustafson <ref> [Gust88] </ref> suggested that instead of increasing the number of processors used to solve a fixed-size problem, we should measure speedup by holding execution time constant, and growing the parallel portion of the problem with the number of processors.
Reference: [Gust92] <author> Gustavson, D. B., </author> <title> The Scalable Coherent Interface and Related Standards Projects, </title> <booktitle> IEEE Micro 12(1), </booktitle> <month> February </month> <year> 1992, </year> <pages> 10-22. 165 </pages>
Reference-contexts: An ideal method of doing this is to study an existing, industrial strength design. A prime candidate for this purpose is the Scalable Coherent Interface (SCI). 1. Background SCI is a new IEEE standard (1596) that provides very-high-performance, shared-bus-like functionality to a large number of processor nodes <ref> [IEEE92, Jame90, Gust92] </ref>. Although SCI is an interface standard, rather than a network standard, the nature of its operation implies pipelined channels. The cycle time is fixed and independent of wire lengths, so that sufficiently long wires will have multiple bits in flight concurrently. <p> However, the SCI standard is being extended to address the serialization problem, which will allow read requests to be handled concurrently in large systems <ref> [Gust92] </ref>. Optimizations have been designed that use combining in the network and a third hardware pointer per cache line to construct tree structures rather than flat linked lists. These trees can be used to distribute shared data or invalidations to x processors in O (log x) sequential messages.
Reference: [Hage89] <author> Hagersten, E. and S. Haridi, </author> <title> The Cache Coherence Protocol of the Data Diffusion Machine, </title> <institution> SICS Research Report R-89004, Swedish Institute of Computer Science, Kista, Sweden, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Using asymptotic behavior, he defines 11 classes of scalability, including uniformly, architecturally and implementationally scalable. On the other hand, Hill [Hill90], questions whether scalability can be usefully defined at all. He challenges the technical community to either define the term rigorously, or stop using it altogether. Others <ref> [Leno90, Cher89, Hage89] </ref> use the term without accompanying definition, relying on the readers' intuitive definitions. I believe that a rigorous definition of scalability may be of little use, but that we can arrive at a useful working definition. Several qualifications need to be offered at the start, however.
Reference: [Hill90] <author> Hill, M. D., </author> <title> What is Scalability?, Computer Architecture News, </title> <month> December </month> <year> 1990, </year> <pages> 18-21. </pages>
Reference-contexts: Johnson [John90] presents a rigorous set of definitions for scalable hardware which is independent of the workload. Using asymptotic behavior, he defines 11 classes of scalability, including uniformly, architecturally and implementationally scalable. On the other hand, Hill <ref> [Hill90] </ref>, questions whether scalability can be usefully defined at all. He challenges the technical community to either define the term rigorously, or stop using it altogether. Others [Leno90, Cher89, Hage89] use the term without accompanying definition, relying on the readers' intuitive definitions.
Reference: [Hill85] <author> Hillis, W. D., </author> <title> The Connection Machine, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors, and therefore allowing communication locality to be exploited [Seit84]. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 <ref> [Hill85] </ref>, Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife [Agar90], Tera supercomputer [Alve90], CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor [Leno89]. The most commonly used direct networks are variants of the k-ary n-cube.
Reference: [IEEE92] <editor> IEEE,, </editor> <booktitle> IEEE Std 1596-1992 (Scalable Coherent Interface). </booktitle> <year> 1992. </year>
Reference-contexts: Thus, multiple bits may be simultaneously in flight on a single wire. Chapters 3 and 4 investigate pipelined-channel interconnection networks in more detail. Chapter 3 presents a performance study of the Scalable Coherent Interface (SCI), a new IEEE standard that uses pipelined channels <ref> [IEEE92] </ref>. Modeling and simulation are used to analyze the performance of a single SCI ring, the basic building block of all SCI systems. The study serves two primary purposes. It is important in its own right, as the first comprehensive analysis of a new IEEE standard. <p> An ideal method of doing this is to study an existing, industrial strength design. A prime candidate for this purpose is the Scalable Coherent Interface (SCI). 1. Background SCI is a new IEEE standard (1596) that provides very-high-performance, shared-bus-like functionality to a large number of processor nodes <ref> [IEEE92, Jame90, Gust92] </ref>. Although SCI is an interface standard, rather than a network standard, the nature of its operation implies pipelined channels. The cycle time is fixed and independent of wire lengths, so that sufficiently long wires will have multiple bits in flight concurrently. <p> The cache coherence layer of the SCI standard is not considered at all. Much more detail can be found in the standard <ref> [IEEE92] </ref>. A packet traversing an SCI ring is sent from a source node to a target node in the form of a send packet. The target node then strips the send packet, and returns an echo packet around the remainder of the ring. <p> A higher degree of pipelining is achievable using the Caltech Slack chip [Seit92], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) <ref> [IEEE92] </ref>, on which I base my model, allows an arbitrary amount of pipelining. The remainder of this chapter presents a performance study of pipelined-channel k-ary n-cube networks, with particular emphasis on how the design tradeoffs differ from those of non-pipelined-channel networks. <p> The ring interfaces are based loosely on the IEEE SCI logical layer protocol discussed in Chapter 3 <ref> [Scot92, IEEE92] </ref>. The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (4.1) P = J L hhh J Routing is similar to virtual cut through [Kerm79]. <p> In this case, of course, the switching delay would be greater for packets chang ing to other than the next highest dimension. Low-level routing in each dimension is modeled after the logical layer of the SCI protocol <ref> [IEEE92] </ref>. When a packet is removed from a ring (either switched to the processor or to a different dimension), an echo packet is routed the remainder of the way around the ring to acknowledge the receipt of the packet on this ring. <p> Ring acknowledgements avoid handshaking latency altogether and retain performance as transmission delays increase. In addition, they can be used to provide fault tolerance by checking a CRC (cyclic redundancy check) at the end of the packet <ref> [IEEE92] </ref>. Unlike the input queue, the ring buffer is guaranteed to be able to accept a flit on every cycle. An output queue may only initiate a packet transmission on the output link if the ring buffer has enough free space to hold a packet of equal length. <p> The latency used to determine the optimal network configuration is the sum of two packet transmission latencies: one with a 16 byte packet and one with an 80 byte packet. These correspond to the address and data packet sizes in SCI <ref> [IEEE92] </ref>. To compute the optimal dimensionality, all discrete quantities are treated as as continuous. Thus, wire and decode delay cycles as well as the dimensionality and radix of the networks can be fractional. <p> Thus, it is not compatible with read combining mechanisms and would not perform well on large systems for workloads with significant read contention. Ch. 5 85 1.7. Distributed linked list directories The cache coherence protocol used in the Scalable Coherent Interface <ref> [IEEE92] </ref>, also uses linked lists to keep track of shared copies of each line, but the lists are physically distributed among the caches of the system. Associated with each cache line in the system are two hardware pointers capable of pointing to other caches. <p> The echo packet is used for fault tolerance and is similar to that used in SCI <ref> [IEEE92] </ref>. For simplicity, I assume that an echo packet is the same size as an address packet (although in SCI, echo packets are smaller). <p> When the packet arrived back at the source, if at least one node did not retain a copy, then the packet would be retransmitted with the modified bit mask. The SCI protocol implements broadcast as one of its non-coherent transactions <ref> [IEEE92] </ref>. Rather than use a full bitmask, a packet that cannot be accepted at a node is immediately converted to a negative echo, disallowing further downstream nodes from receiving the packet on the first pass. <p> This problem exists for point-to-point transmission as well, however; it is not unique to broadcasts. SCI simply traps to software to handle this presumably rare event <ref> [IEEE92] </ref>. The same can be done when a broadcast transmission fails, although it may increase the complexity of the software recovery. Broadcasts that are used for invalidations pose a more difficult problem, because the recipients on a ring cannot, in general, respond directly after receiving the invalidation.
Reference: [Jame90] <author> James, D. V., A. T. Laundrie, G. S. Sohi, and S. Gjessing, </author> <title> SCI (Scalable Coherent Interface) Cache Coherence, </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: An ideal method of doing this is to study an existing, industrial strength design. A prime candidate for this purpose is the Scalable Coherent Interface (SCI). 1. Background SCI is a new IEEE standard (1596) that provides very-high-performance, shared-bus-like functionality to a large number of processor nodes <ref> [IEEE92, Jame90, Gust92] </ref>. Although SCI is an interface standard, rather than a network standard, the nature of its operation implies pipelined channels. The cycle time is fixed and independent of wire lengths, so that sufficiently long wires will have multiple bits in flight concurrently.
Reference: [John90] <author> Johnson, R. E., </author> <title> Scalable Read-Sharing in SCI, </title> <publisher> Ph. </publisher> <address> D. </address> <note> Preliminary Exam, </note> <institution> Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Their definition is precise, but rather theoretical in nature, dealing only with asymptotic behavior and ignoring any cost or efficiency considerations. Johnson <ref> [John90] </ref> presents a rigorous set of definitions for scalable hardware which is independent of the workload. Using asymptotic behavior, he defines 11 classes of scalability, including uniformly, architecturally and implementationally scalable. On the other hand, Hill [Hill90], questions whether scalability can be usefully defined at all.
Reference: [John91] <author> Johnson, Ross E. and James R. Goodman, </author> <title> Interconnect Topologies with Point-To-Point Rings, </title> <type> Computer Sciences Technical Report #1058, </type> <institution> University of Wisconsin-Madison, </institution> <month> December </month> <year> 1991. </year> <note> Condensed version to appear in ICPP, </note> <month> August </month> <year> 1992. </year>
Reference-contexts: Figure 3.1, for instance, shows a two-dimensional torus constructed from rings. A multistage interconnection network (such as the Omega network) could be implemented by replacing every bidirectional link with a ring of size two, or by more elaborate, higher-performance schemes <ref> [John91] </ref>. The ring is unusual in that each node provides a bypass buffer capable of temporarily storing a packet arriving from its upstream neighbor while it is transmitting a packet.
Reference: [Katz85] <author> Katz, R. H., S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon, </author> <title> Implementing a Cache Consistency Protocol, </title> <booktitle> Proc. 12th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1985, </year> <pages> 276-283. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Kerm79] <author> Kermani, P. and L. Kleinrock, </author> <title> Virtual Cut-Through: A New Computer Communication Switching Technique, </title> <booktitle> Computer Networks 3, </booktitle> <year> 1979, </year> <pages> 267-286. </pages>
Reference-contexts: The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (4.1) P = J L hhh J Routing is similar to virtual cut through <ref> [Kerm79] </ref>. On being placed in the input queue by the CPU, a packet is switched to the output queue for the appropriate dimension and gated onto the output link (requiring T switch cycles).
Reference: [Klei75] <author> Kleinrock, L., </author> <title> Queueing Systems, Volume I, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Results from the model, as well as from detailed simulations, are presented in section 4. The model is based upon an approximate, iterative solution of the M/G/1 queue <ref> [Klei75] </ref> (see Figure 3.3). It does not consider flow control, limited active buffers or target queue overflow. The effect of these factors can be determined from simulation results.
Reference: [Lam79] <author> Lam, C.-Y. and S. E. Madnick, </author> <title> Properties of Storage Hierarchy Systems with Multiple Page Sizes and Redundant Data, </title> <journal> ACM Transactions on Database Systems 4(3), </journal> <month> September </month> <year> 1979, </year> <pages> 345-367. </pages>
Reference-contexts: Thus, just as the global bit vector scheme, the hierarchical scheme requires O (N 2 ) storage for the directories. 1.9. Multi-level inclusion A coherence mechanism that is similar to hierarchical directories in some ways is the multi-level inclusion (MLI) property <ref> [Lam79, Wils87, Baer88] </ref>. The MLI property requires that a cache in a hierarchy contain a superset of all lines residing beneath it in the hierarchy. When the line is invalidated, a parent only propagates the invalidate to its children if it has a copy of the line.
Reference: [Lamp78] <author> Lamport, L., </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM 21(7), </journal> <month> July </month> <year> 1978, </year> <pages> 558-565. </pages>
Reference-contexts: Second, there must exist some global ordering of reads and writes to a given memory location, such that no processor observes any other order. In addition to cache coherence, a multiprocessor may provide sequential consistency <ref> [Lamp78] </ref>, or some weaker form of memory consistency [Dubo86, Adve90, Ghar90], which makes a guarantee about the global ordering of reads and writes to different memory locations . Providing some form of memory consistency can significantly impact the cache coherence mechanism.
Reference: [Lang88] <author> Lang, T. and L. Kurisaki, </author> <title> Nonuniform Traffic Spots (NUTS) in Multistage Interconnection Networks, </title> <booktitle> Proc. 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988. </year> <month> 166 </month>
Reference-contexts: Hardware mechanisms for combining requests in the network have been proposed [Gott83, Pfis85], as well as mechanisms to improve network performance in the face of contention <ref> [Tami88, Scot90, Lang88] </ref>. These mechanisms are not necessary under the uniform workload assumption, as the average rate of read requests to a line is independent of system size.
Reference: [Lawr75] <author> Lawrie, D. H., </author> <title> Access and Alignment of Data in an Array Processor, </title> <journal> IEEE Transactions on Computers C-24(12), </journal> <month> December </month> <year> 1975, </year> <pages> 1145-1155. </pages>
Reference-contexts: Their analysis showed that when f RI is small, the system is extremely inefficient for large numbers of processors (due to contention for the root bus). Another widely studied class of network topologies is multistage interconnection networks, such as the omega network <ref> [Lawr75] </ref>. The omega network uses log k N stages of kk crossbar switches to connect N inputs to N outputs. <p> Background A wide variety of interconnection networks have been proposed in the literature (see [Feng81] or [Sieg79] for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network <ref> [Lawr75] </ref>, connect processors and memories through multiple intermediate stages of switching elements. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors, and therefore allowing communication locality to be exploited [Seit84].
Reference: [Leno89] <author> Lenoski, D., J. Laudon, K. Gharachorloo, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> Design of the Stanford DASH Multiprocessor, </title> <type> CSL Technical Report 89-403, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife [Agar90], Tera supercomputer [Alve90], CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor <ref> [Leno89] </ref>. The most commonly used direct networks are variants of the k-ary n-cube. Recall that a k-ary n-cube consists of N =k n nodes, arranged in n dimensions with k nodes per dimension. Figure 4.1 illustrates several different k-ary n-cubes.
Reference: [Leno90] <author> Lenoski, D., J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 148-158. </pages>
Reference-contexts: Using asymptotic behavior, he defines 11 classes of scalability, including uniformly, architecturally and implementationally scalable. On the other hand, Hill [Hill90], questions whether scalability can be usefully defined at all. He challenges the technical community to either define the term rigorously, or stop using it altogether. Others <ref> [Leno90, Cher89, Hage89] </ref> use the term without accompanying definition, relying on the readers' intuitive definitions. I believe that a rigorous definition of scalability may be of little use, but that we can arrive at a useful working definition. Several qualifications need to be offered at the start, however.
Reference: [Love88] <author> Lovett, T. and S. Thakkar, </author> <title> The Symetry Multiprocessor System, </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <year> 1988, </year> <pages> 303-310. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [McCr84] <author> McCreight, E. M., </author> <title> The Dragon Computer System: An Early Overview, </title> <type> Technical Report, </type> <institution> Xerox Corp., </institution> <month> September </month> <year> 1984. </year>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Mell91] <author> Mellor-Crummey, J. M. and M. L. Scott, </author> <title> Synchronization Without Contention, </title> <booktitle> Proc. ASPLOS IV, </booktitle> <month> April </month> <year> 1991, </year> <pages> 269-278. </pages>
Reference-contexts: Read Combining in Cube Networks Contention for data and synchronization objects is a potential problem that must be addressed when designing very large systems. Synchronization can be handled in a variety of manners, both in hardware and in software. Examples include software combining [Yew87], software queueing <ref> [Mell91] </ref> and hardware queueing [Good89a]. Although synchronization issues are important, I do not address them in this thesis. I do address the problem of read contention, however, as it is closely related to the cache coherence mechanism. <p> Mechanisms that support locks using software or hardware queueing can prevent lock contention, and can be used as primitives to support contention free barriers and other synchronization operations as well <ref> [Mell91, Good89a] </ref>. In addition, the QOLB hardware synchronization primitive [Good89a] can improve the performance of pruning-cache directories by removing interference from migratory data. QOLB automatically migrates lock-protected data from one cache to another, allowing the data to remain in globally modified state.
Reference: [Nuss91] <author> Nussbaum, D. and A. Agarwal, </author> <title> Scalability of Parallel Machines, </title> <journal> Communications of the ACM 34(3), </journal> <month> March </month> <year> 1991, </year> <pages> 56-61. </pages>
Reference-contexts: They define a scalable system according to the speed at which it executes a scalable algorithm, allowing limited reductions in speed due to communication latency. Ch. 2 7 Nussbaum and Agarwal <ref> [Nuss91] </ref> provide a more general definition of scalability, that assigns a scalability value to any pair of machine and algorithm.
Reference: [O'Kr90] <author> O'Krafka, B. W. and A. R. </author> <title> Newton, An Empirical Evaluation of Two Memory-Efficient Directory Methods, </title> <booktitle> Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 138-147. </pages>
Reference-contexts: Weber, Gupta and Mowry [Gupt90] referred to these directory caches as sparse directories, and suggested that entries could be either full width, limited pointer or coarse vector, depending on the size of the system and the amount of memory allotted for directory storage. O'Krafka and Newton <ref> [O'Kr90] </ref> independently proposed the use of directory caches, and suggested that two separate caches be maintained at each node: a small cache of full width entries and a larger cache of limited pointer entries. Using directory caches provides a large constant-factor reduction in storage overhead. <p> The use of multiple caches with different types of entries in each cache can provide a partial solution to this problem. Directory caches do not address the problem of serialization at the directories. 1.5. Sectored directories O'Krafka and Newton investigated another coherence mechanism called the sectored directory <ref> [O'Kr90] </ref>. This scheme keeps track of the individual state of all blocks (called sub blocks in the paper), but keeps track of the sharing information over sets of n blocks. <p> The memory overhead is the same as for Dir i B, and the cache overhead is now (6.6) cache (Dir i vector) = m =i +1 N The directory cache scheme <ref> [O'Kr90, Gupt90] </ref> associates no directory information with main memory, but use two kinds of caches, one with i processor pointers per entry and one with full width vectors.
Reference: [Olso83] <author> Olson, R. A., B. Kumar, and L. E. Shar, </author> <title> Messages and Multiprocessing in the ELXSI System 6400, </title> <address> COMPCON83, </address> <month> February </month> <year> 1983, </year> <pages> 21-24. </pages>
Reference-contexts: The Stardent Titan graphics supercomputer uses a 31.25 ns bus [Siew91], for example, and the Silicon Graphics Power Series computers use a 30 ns bus [SGI89]. The ELXSI System 6400 used expensive twisted-pair ECL with differential signaling for their shared backplane, achieving a cycle time of 25 ns <ref> [Olso83] </ref>.
Reference: [Papa84] <author> Papamarcos, M. S. and J. H. Patel, </author> <title> A Low-Overhead Coherence Solution for Multiprocessors with Private Cache Memories, </title> <booktitle> Proc. 11th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 348-354. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Patt85] <author> Patton, P. C., </author> <title> Multiprocessors: </title> <booktitle> architecture and applications, IEEE Computer 18(6), </booktitle> <month> June </month> <year> 1985, </year> <pages> 29-40. </pages>
Reference-contexts: Given this primary assumption of scalable software, we need not quit before we have begun, and can continue to define a scalable system. There have been several suggestions as to what scalability means. Patton <ref> [Patt85] </ref> stated that a scalable design can be adjusted up or down in size without loss of functionality to scale effects.
Reference: [Pfis85] <author> Pfister, G. F. and et al, </author> <title> The IBM Research Parallel Processor Prototype (RP3): introduction and architecture, </title> <booktitle> Proc. 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 764-771. </pages>
Reference-contexts: Contention for a particular memory location or module in a large system is known as a hot spot, and has been the focus of much research. Hardware mechanisms for combining requests in the network have been proposed <ref> [Gott83, Pfis85] </ref>, as well as mechanisms to improve network performance in the face of contention [Tami88, Scot90, Lang88]. These mechanisms are not necessary under the uniform workload assumption, as the average rate of read requests to a line is independent of system size. <p> These requests would use the combining mechanism, resulting in higher latency, but avoiding possible serialization from read contention. This solution does not require a separate combining network, as was proposed for the RP3 <ref> [Pfis85] </ref>; it is simply built into the coherence protocol. With the use of pruning caches, performance is somewhat improved by partitioning work such that sharing takes place along lower dimensions. <p> However, since some conflict and capacity misses are likely to occur, it may make sense to allocate some pages of memory locally to a node rather than spread across nodes, as has been suggested in this thesis (a similar capability was included in the RP3 <ref> [Pfis85] </ref> ). This could be used to store local/private data. Since data sharing involves accesses to the directory information for the data, is may also make sense to allocate shared data in contiguous memory at a node and partition problems to increase locality.
Reference: [Scot90] <author> Scott, S. L. and G. S. Sohi, </author> <title> The Use of Feedback in Multiprocessors and Its Application to Tree Saturation Control, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1(4), </journal> <month> October </month> <year> 1990, </year> <pages> 385-398. 167 </pages>
Reference-contexts: Hardware mechanisms for combining requests in the network have been proposed [Gott83, Pfis85], as well as mechanisms to improve network performance in the face of contention <ref> [Tami88, Scot90, Lang88] </ref>. These mechanisms are not necessary under the uniform workload assumption, as the average rate of read requests to a line is independent of system size.
Reference: [Scot91] <author> Scott, S. L., </author> <title> A Cache Coherence Mechanism for Scalable, Shared Memory Multiprocessors, </title> <booktitle> Proc. 1991 International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991, </year> <pages> 49-59. </pages>
Reference-contexts: The focus of Chapters 5 and 6 switches to cache coherence mechanisms. Chapter 5 presents a summary of existing and proposed cache coherence mechanisms and describes a novel approach for maintaining coherence in hierarchical or cube-based multiprocessors. Approximate directory information is held in special-purpose pruning caches <ref> [Good89, Scot91] </ref>, and used to limit the propagation of broadcast invalidations to those parts of the hierarchy that need to receive them. This scheme approximates the performance of a full hierarchical directory, but at considerably less cost. <p> The analysis thus far has considered only unloaded latency. When bandwidth is considered, the argument for keeping the radix small becomes more compelling. Under uniform traffic, the Ch. 4 60 rate of traffic across a link is proportional to the radix of the network <ref> [Scot91] </ref>. Thus, if the network is grown by increasing the radix, the rate of traffic per link will also increase. Although this will be offset by larger link widths when wiring is constrained, the total throughput per processor still argues for lower radix. <p> Although the directory overhead for MLI scales, it can still be quite large. This is addressed further in the next chapter. 2. Pruning-Cache Directories A novel way of scaling hierarchical directories is to limit their size and manage them as caches (pruning caches) <ref> [Good89, Scot91] </ref>. We no longer require a sub-directory to contain the pruning vector for a given line when it is accessed. If it contains the entry, then we proceed as with the hierarchical directory. <p> Pruning caches (partial directories) are compared against two extremes: no directories (requiring broadcasts when a shared line is invalidated) and full hierarchical directories (pruning caches with a hit rate of 1). A similar analysis for bus-based systems can be found elsewhere <ref> [Scot91] </ref>. In order to analyze the performance of pruning caches, we must first make some assumptions regarding the operation of the system. I will assume two lengths of messages: one for messages containing a line of data, and one for address-only messages. <p> The choice of formula depends somewhat on the pruning-cache management policy (see Chapter 6, Section 3). Previous published analysis <ref> [Scot91] </ref> assumed a uniform hit rate for all pruning cache accesses. However, recent simulation results reveal that pruning-cache hit rates are very low for runaway invalidations, so I conservatively assume the use of Equation (5.6) throughout this study.
Reference: [Scot92] <author> Scott, S. L., J. R. Goodman, and M. K. Vernon, </author> <title> Performance of the SCI Ring, </title> <booktitle> Proc. the 19th Annual International Symposium on Computer Archtec-ture, </booktitle> <month> May </month> <year> 1992, </year> <pages> 403-414. </pages>
Reference-contexts: Unlimited active buffers are assumed at each node, but only one or two active buffers are actually needed to approximate this <ref> [Scot92] </ref>. The simulator implements the protocol described in section 2 on a cycle by cycle basis, explicitly tracking each symbol on the ring. Simulations were run for 9.3 million cycles each, and 90% confidence intervals were computed using the method of batched means. <p> 80 byte packets 16 byte packets Flow Control With Flow Control Without (a) N = 4 (b) N = 16 Ch. 3 31 Simulations indicate that the throughput degradation from flow control is greatest for ring sizes in the 10 to 20 range, and actually lessens slightly for larger rings <ref> [Scot92] </ref>. 4.2. Node starvation This section examines the situation in which a node is inhibited from transmitting by reducing the number of breaks it sees in its pass-though traffic. <p> By guaranteeing that buffer congestion occurs only when entering/exiting a dimension, deadlock free routing is provided for uni-directional k-ary n-cubes. In addition, sustained throughput is able to come close to maximum throughput, especially when k is large (see <ref> [Scot92] </ref> and Section 4 of this chapter). Conventional networks using flit-level flow control typically provide half or less of their maximum throughput, due to coupled resource allocation [Dall90]. Note, however, that the switches described in Section 2.1 require buffers able to contain an entire packet. <p> The ring interfaces are based loosely on the IEEE SCI logical layer protocol discussed in Chapter 3 <ref> [Scot92, IEEE92] </ref>. The links are W bits wide, so a packet of L bits is decomposed into P flits, with P given by (4.1) P = J L hhh J Routing is similar to virtual cut through [Kerm79].
Reference: [Seit84] <author> Seitz, C. L., </author> <title> Concurrent VLSI Architectures, </title> <journal> IEEE Transactions on Computers 33(12), </journal> <month> December </month> <year> 1984, </year> <pages> 775-785. </pages>
Reference-contexts: Indirect networks, such as the omega network [Lawr75], connect processors and memories through multiple intermediate stages of switching elements. Direct networks incorporate the processing elements within the network itself, allowing for direct communication between processors, and therefore allowing communication locality to be exploited <ref> [Seit84] </ref>. Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube [Seit85], MIT Alewife [Agar90], Tera supercomputer [Alve90], CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor [Leno89].
Reference: [Seit85] <author> Seitz, C. L., </author> <title> The Cosmic Cube, </title> <journal> Communications of the ACM 28(1), </journal> <month> January </month> <year> 1985, </year> <pages> 22-33. </pages>
Reference-contexts: Direct networks are gaining in popularity and have been employed in many recent existing or proposed machines, including the Thinking Machines CM2 [Hill85], Intel iPSC and Paragon, Cosmic Cube <ref> [Seit85] </ref>, MIT Alewife [Agar90], Tera supercomputer [Alve90], CMU-Intel iWarp [Bork90], and Stanford DASH multiprocessor [Leno89]. The most commonly used direct networks are variants of the k-ary n-cube. Recall that a k-ary n-cube consists of N =k n nodes, arranged in n dimensions with k nodes per dimension.
Reference: [Seit92] <author> Seitz, C. L., </author> <type> Personal correspondence. </type> <month> April </month> <year> 1992. </year>
Reference-contexts: Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [Bork90], various Cray Research machines [Smit92] and the Thinking Machines CM5 [TMC91]. A higher degree of pipelining is achievable using the Caltech Slack chip <ref> [Seit92] </ref>, which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) [IEEE92], on which I base my model, allows an arbitrary amount of pipelining.
Reference: [SGI89] <author> SGI,, </author> <title> Power Series, </title> <type> Silicon Graphics Technical Report, </type> <year> 1989. </year>
Reference-contexts: Realistic bus cycle times range from 20 to 100 ns. A typical high performance shared bus has a 30 ns cycle time. The Stardent Titan graphics supercomputer uses a 31.25 ns bus [Siew91], for example, and the Silicon Graphics Power Series computers use a 30 ns bus <ref> [SGI89] </ref>. The ELXSI System 6400 used expensive twisted-pair ECL with differential signaling for their shared backplane, achieving a cycle time of 25 ns [Olso83].
Reference: [Sieg79] <author> Siegel, H. J., </author> <title> Interconnection Networks for SIMD Machines, </title> <booktitle> Computer 12(6), </booktitle> <month> June, </month> <year> 1979, </year> <pages> 57-65. </pages>
Reference-contexts: The task of this chapter, then, is to thoroughly analyze the effects of pipelined channels on large, multidimensional networks. 1. Background A wide variety of interconnection networks have been proposed in the literature (see [Feng81] or <ref> [Sieg79] </ref> for a summary), each of which can be classified as either direct or indirect. Indirect networks, such as the omega network [Lawr75], connect processors and memories through multiple intermediate stages of switching elements.
Reference: [Siew91] <author> Siewiorek, D. P. and P. J. Koopman, Jr., </author> <title> The Architecture of Supercomputers: Titan, A Case Study, </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1991. </year>
Reference-contexts: Realistic bus cycle times range from 20 to 100 ns. A typical high performance shared bus has a 30 ns cycle time. The Stardent Titan graphics supercomputer uses a 31.25 ns bus <ref> [Siew91] </ref>, for example, and the Silicon Graphics Power Series computers use a 30 ns bus [SGI89]. The ELXSI System 6400 used expensive twisted-pair ECL with differential signaling for their shared backplane, achieving a cycle time of 25 ns [Olso83].
Reference: [Simo91] <author> Simoni, R. and M. Horowitz, </author> <title> Dynamic Pointer Allocation for Scalable Cache Coherence Directories, </title> <booktitle> Proc. 1991 International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991, </year> <pages> 72-81. </pages>
Reference-contexts: O'Krafka and Newton found that this scheme led to heavy invalidation traffic and performed worse than a directory cache scheme using a similar amount of storage. Ch. 5 84 1.6. Dynamic pointer allocation Simoni and Horowitz <ref> [Simo91] </ref> proposed a dynamic pointer allocation protocol that avoids the problem of statically assigning processor pointers to directory entries. Their scheme maintains a cache of single processor pointers at each node. <p> However, since memory may be non-uniformly cached, the number of entries per pointer cache should be some constant factor larger than the number of lines per cache. Simoni and Horowitz <ref> [Simo91] </ref> suggest that directory storage requirements can be further reduced by using a cache for the head links (the pointers into the pointer cache that are associated with each line of main memory). <p> The storage overhead is thus (6.7)memory (directory cache i ) = 0 (6.8) cache (directory cache i ) = m =1 i m =i +1 N The dynamic pointer allocation scheme <ref> [Simo91] </ref> uses a single pointer for every cached line, independent of the sharing distribution. Associated with each line of main memory is a pointer into the local pointer cache (assumed to have C entries).
Reference: [Smit92] <author> Smith, J. E., </author> <type> Personal correspondence. </type> <month> April </month> <year> 1992. </year>
Reference-contexts: Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [Bork90], various Cray Research machines <ref> [Smit92] </ref> and the Thinking Machines CM5 [TMC91]. A higher degree of pipelining is achievable using the Caltech Slack chip [Seit92], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables.
Reference: [Stal84] <author> Stallings, W., </author> <title> Local Networks, </title> <journal> Computing Surveys 16(1), </journal> <month> March </month> <year> 1984, </year> <pages> 3-41. </pages>
Reference-contexts: This buffer allows nodes to transmit concurrently rather than having to wait for a token, but results in long latency if all nodes happen to initiate transmission simultaneously on an idle ring. The basic structure of the SCI ring is similar in nature to the register insertion ring <ref> [Stal84] </ref>. Because of the novel construction of the ring and the attendant clock rates achievable in the design, very high performance is expected, and a peak bandwidth of one gigabyte per second is easy to demonstrate.
Reference: [Sull77] <author> Sullivan, H. and T. R. Bashkow, </author> <title> A Large Scale, Homogeneous, Fully Distributed Parallel Machine, </title> <booktitle> Proc. 4th Annual International Symposium on Computer Architecture, </booktitle> <month> March </month> <year> 1977, </year> <pages> 105-117. </pages>
Reference-contexts: The omega network, therefore, scales better than the bus hierarchy. It does not, however, allow locality to be exploited; all memory modules and processors are equi-distant from each other. Another widely advocated topology is the k-ary n-cube <ref> [Sull77] </ref>. Rings, 2-dimensional tori, 3-dimensional tori and hypercubes are all sub-classes of this topology. The k-ary n-cube has n dimensions, with k processors in each dimension, for a total of N = k n processors. For example, a 2-dimensional torus with 64 processors is a 8-ary 2-cube.
Reference: [Tami88] <author> Tamir, Y. and G. L. Frazier, </author> <title> High-Performance Multi-Queue Buffers for VLSI Communication Switches, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> 343-354. </pages>
Reference-contexts: Hardware mechanisms for combining requests in the network have been proposed [Gott83, Pfis85], as well as mechanisms to improve network performance in the face of contention <ref> [Tami88, Scot90, Lang88] </ref>. These mechanisms are not necessary under the uniform workload assumption, as the average rate of read requests to a line is independent of system size.
Reference: [Tane88] <author> Tanenbaum, A. S., </author> <title> Computer Networks, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year> <month> 168 </month>
Reference-contexts: Pipelined channels have long been used in wide area networks and local area networks, since the physical delays involved compel their use. There is an abundance of research in the literature regarding transmission protocols, reliability, flow control, routing, performance and Ch. 4 46 many other issues regarding these networks <ref> [Tane88] </ref>. Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [Bork90], various Cray Research machines [Smit92] and the Thinking Machines CM5 [TMC91].
Reference: [Tang76] <author> Tang, C. K., </author> <title> Cache System Design in the Tightly Coupled Multiprocessor System, </title> <booktitle> Proc. AFIPS, </booktitle> <year> 1976, </year> <pages> 749-753. </pages>
Reference-contexts: Full-width global directory Another possible coherence mechanism is to keep track of all shared copies of a line in a global directory. The directory can be distributed along with the memory of the system (as opposed to a "centralized" directory as proposed by Tang <ref> [Tang76] </ref> ). Censier and Feautrier [Cens78] proposed keeping a bit vector of size N for each line, with the corresponding bit set for every processor that has a copy of the line. When the line is invalidated, individual messages are sent to each processor whose bit is set.
Reference: [Thac87] <author> Thacker, C. P. and L. C. Stewart, Firefly: </author> <title> a Multiprocessor Workstation, </title> <booktitle> Proc. ASPLOS II, </booktitle> <month> October </month> <year> 1987, </year> <pages> 164-172. </pages>
Reference-contexts: A shared bus has the natural property that all communications are broadcast, which explains the attractiveness of snooping cache protocols <ref> [Good83, Papa84, McCr84, Fran84, Katz85, Enco86, Thac87, Love88] </ref>. In these protocols, caches maintain coherence by monitoring all bus traffic and detecting when shared data is modified.
Reference: [Thap90] <author> Thapar, M. and B. Delagi, </author> <title> Stanford Distributed-Directory Protocol, </title> <booktitle> IEEE Computer 23(6), </booktitle> <month> June </month> <year> 1990, </year> <pages> 78-81. </pages>
Reference-contexts: These trees can be used to distribute shared data or invalidations to x processors in O (log x) sequential messages. Distributed linked lists are also used in the Stanford Distributed-Directory protocol <ref> [Thap90] </ref>. This protocol uses singly linked lists rather than doubly linked lists. Although requiring less storage, the Stanford protocol does not provide the same level of robustness as SCI, nor does it allow extensions for concurrent read combining. 1.8.
Reference: [TMC91] <author> TMC,, </author> <title> CM5 Reference Manual, Thinking Machines, </title> <publisher> Inc., </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Pipelined channels are not widely used in multiprocessor interconnects, however. Limited pipelined channels (allowing a small, fixed number of bits on a wire) are used in the CMU-Intel iWarp [Bork90], various Cray Research machines [Smit92] and the Thinking Machines CM5 <ref> [TMC91] </ref>. A higher degree of pipelining is achievable using the Caltech Slack chip [Seit92], which is designed to allow tightly-coupled Mosaic channels to communicate efficiently over long cables. The IEEE Scalable Coherent Interface (SCI) [IEEE92], on which I base my model, allows an arbitrary amount of pipelining.
Reference: [Vern89] <author> Vernon, M. K., R. Jog, and G. S. Sohi, </author> <title> Performance Analysis of Hierarchical Cache-Consistent Multiprocessors, Performance Evaluation 9, </title> <booktitle> 1989, </booktitle> <pages> 287-302. </pages>
Reference-contexts: A broadcast operation is simulated by passing a message completely around the ring. This does not provide global event ordering, however, so modifications are necessary for snooping protocols that depend on this. One topology that has been widely studied for large numbers of processors <ref> [Wils87, Baer88, Wins88, Vern89, Yang92] </ref> is the bus hierarchy (see Figure 2.1). This topology will not scale, however, for uniform workloads. <p> For a conspirator workload, where the fraction of writes to shared variables decreases with N, the traffic over the root bus can remain O (1). Root traffic can also remain constant with respect to N if strict communication locality can be maintained. Vernon, Jog and Sohi <ref> [Vern89] </ref>, in an analysis of hierarchical bus-based multiprocessors, concluded that the systems scale well only if the average fraction of processors that read a shared line between writes, f RI , remains large as system size increases.
Reference: [Webe89] <author> Weber, W. and A. Gupta, </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors, </title> <booktitle> Proc. ASPLOS III, </booktitle> <month> April </month> <year> 1989, </year> <pages> 243-256. </pages>
Reference-contexts: Certain workloads may display the correct behavior for large systems, but it is not clear whether programs will behave correctly in general. Weber and Gupta <ref> [Webe89] </ref> analyzed five parallel traces and recorded the distribution of the number of shared copies that needed to be invalidated on a write, for 4, 8 and 16 processor systems. <p> Each class of information has different needs or costs for maintenance of coherence. Below, I discuss the various classes and how the pruning cache protocol does or could support them. These classes are quite similar to those defined by Weber and Gupta <ref> [Webe89] </ref>. Private instructions and data can be placed exclusively into one cache and kept track of with a single directory pointer.
Reference: [Wils87] <author> Wilson, A. W., </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors, </title> <booktitle> Proc. 14th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1987, </year> <pages> 244-252. </pages>
Reference-contexts: A broadcast operation is simulated by passing a message completely around the ring. This does not provide global event ordering, however, so modifications are necessary for snooping protocols that depend on this. One topology that has been widely studied for large numbers of processors <ref> [Wils87, Baer88, Wins88, Vern89, Yang92] </ref> is the bus hierarchy (see Figure 2.1). This topology will not scale, however, for uniform workloads. <p> Thus, just as the global bit vector scheme, the hierarchical scheme requires O (N 2 ) storage for the directories. 1.9. Multi-level inclusion A coherence mechanism that is similar to hierarchical directories in some ways is the multi-level inclusion (MLI) property <ref> [Lam79, Wils87, Baer88] </ref>. The MLI property requires that a cache in a hierarchy contain a superset of all lines residing beneath it in the hierarchy. When the line is invalidated, a parent only propagates the invalidate to its children if it has a copy of the line. <p> an inclusion cache must be built such that all lines that can possibly reside simultaneously in the subtree beneath it can also reside simultaneously in the inclusion cache [Baer88], or when an inclusion cache has to eject an entry, it must invalidate the corresponding line in the subtree beneath it <ref> [Wils87] </ref>. The first solution is only practical in a single tree system, where the number of caches decreases at each higher level. The second solution, however, is feasible in a cube-based system.
Reference: [Wins88] <author> Winsor, D. C. and T. N. Mudge, </author> <title> Analysis of Bus Hierarchies for Multiprocessors, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> 100-107. </pages>
Reference-contexts: In addition, though it may not be immediately apparent, the latency of shared bus accesses increases with N (even without the increased queueing delay). This is because the physical length of the bus, as well as its capacitance, increases with N <ref> [Wins88] </ref>. This suggests an important point: a ring of point-to-point links can be used to simulate a bus. Both have the same scaling properties, but point-to-point links can be clocked at a higher rate (independent of N), leading to higher performance. <p> A broadcast operation is simulated by passing a message completely around the ring. This does not provide global event ordering, however, so modifications are necessary for snooping protocols that depend on this. One topology that has been widely studied for large numbers of processors <ref> [Wils87, Baer88, Wins88, Vern89, Yang92] </ref> is the bus hierarchy (see Figure 2.1). This topology will not scale, however, for uniform workloads.
Reference: [Wood90] <author> Wood, D. A., G. Gibson, and R. H. Katz, </author> <title> Verifying a Multiprocessor Cache Controller Using Random Case Generation, </title> <booktitle> IEEE Design and Test of Computers, </booktitle> <year> 1990. </year>
Reference-contexts: However, it has been extensively tested using random synthetic workloads, a technique that proved extremely useful for detecting bugs in both the simulator and earlier versions of the protocol. Testing via stochastic simulation has proven an effective means of verification <ref> [Wood90] </ref>. However, before implementing such a complex protocol in hardware, more rigorous verification techniques are recommended [Burc90]. The four protocols differ in how they handle shared read misses, and write misses that find the data globally shared (requiring invalidations).
Reference: [Yang92] <author> Yang, Q., G. Thangadurai, and L. Bhuyan, </author> <title> Design of an Adaptive Cache Coherence Protocol for Large Scale Multiprocessors, </title> <journal> IEEE Transactions on Computers 3(3), </journal> <month> May </month> <year> 1992, </year> <pages> 281-293. </pages>
Reference-contexts: A broadcast operation is simulated by passing a message completely around the ring. This does not provide global event ordering, however, so modifications are necessary for snooping protocols that depend on this. One topology that has been widely studied for large numbers of processors <ref> [Wils87, Baer88, Wins88, Vern89, Yang92] </ref> is the bus hierarchy (see Figure 2.1). This topology will not scale, however, for uniform workloads.
Reference: [Yew87] <author> Yew, P.-C., N.-F. Tzeng, and D. H. Lawrie, </author> <title> Distributing Hot-Spot Addressing in Large Scale Multiprocessors, </title> <journal> IEEE Transactions on Computers C-36(4), </journal> <month> April </month> <year> 1987, </year> <pages> 388-395. </pages> <note> Throw this page away clxix </note>
Reference-contexts: Read Combining in Cube Networks Contention for data and synchronization objects is a potential problem that must be addressed when designing very large systems. Synchronization can be handled in a variety of manners, both in hardware and in software. Examples include software combining <ref> [Yew87] </ref>, software queueing [Mell91] and hardware queueing [Good89a]. Although synchronization issues are important, I do not address them in this thesis. I do address the problem of read contention, however, as it is closely related to the cache coherence mechanism.
References-found: 86

