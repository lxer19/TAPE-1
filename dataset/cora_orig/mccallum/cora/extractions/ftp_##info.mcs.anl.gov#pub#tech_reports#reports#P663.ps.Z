URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P663.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: RSL: A PARALLEL RUNTIME SYSTEM LIBRARY FOR REGIONAL ATMOSPHERIC MODELS WITH NESTING  
Author: JOHN G. MICHALAKES 
Keyword: Key words. Weather modeling, parallel computing, mesh refinement, dynamic load balancing.  
Abstract: RSL is a parallel runtime system library developed at Argonne National Laboratory that is tailored to regular-grid atmospheric models with mesh refinement in the form of two-way interacting nested grids. RSL provides high-level stencil and interdomain communication, irregular domain decomposition, automatic local/global index translation, distributed I/O, and dynamic load balancing. RSL was used with Fortran90 to parallelize a well-known and widely used regional weather model, the Penn State/NCAR Mesoscale Model. 
Abstract-found: 1
Intro-found: 1
Reference: <author> RSL: </author> <title> A PARALLEL RUNTIME SYSTEM LIBRARY 15 </title>
Reference: [1] <author> S. Balay, W. D. Gropp, L. C. McInnes, and B. F. Smith, </author> <title> Efficient Management of Parallelism in Object-Oriented Numerical Software Libraries, in Modern Software Tools in Scientific Computing, </title> <editor> E. Arge, A. M. Bruaset, and H. P. Langtangen, eds., </editor> <publisher> Birkhauser Press, </publisher> <year> 1997. </year> <note> To appear (also Argonne National Laboratory Mathematics and Computer Science Division preprint P634-0197). </note>
Reference: [2] <author> N. Chrisochoides, K. Droegemeier, G. Fox, K. Mills, and M. Xue, </author> <title> A Methodology for Developing High Performance Computing Models: Storm-Scale Weather Prediction, in High Performance Computing, </title> <booktitle> 1993: Grand Challenges in Computer Simulation, The Society for Computer Simulation, </booktitle> <year> 1993. </year>
Reference-contexts: The application of adaptive mesh refinement to regional weather models is an area of active research. The problem for weather models is the representation of small-scale features (clouds, complex terrain) in large-scale atmospheric flows while conserving computation <ref> [2] </ref>. The solution is nesting | the ability to create or delete finer subgrids in a background mesh to obtain a given level of accuracy with a minimum number of grid points.
Reference: [3] <author> J. Drake and I. Foster, </author> <title> Introduction to the Special Issue on Parallel Computing in Climate and Weather Modeling, </title> <booktitle> Parallel Computing, 21 (1995), </booktitle> <pages> pp. 1539-1544. </pages>
Reference-contexts: 1. Introduction. Models of the earth's atmosphere were among the first applications for supercomputers and continue to push the limits of available resources today <ref> [3] </ref>. Dynamic models of the atmosphere are used for forecasting and climate prediction. Such models may be categorized as global and regional. Global models provide relatively low-resolution predictive capabilities and are crucial to providing large-scale long-range simulations.
Reference: [4] <author> R. Ford, D. Snelling, and A. Dickinson, </author> <title> Controlling Load Balance, Cache Use and Vector Length in the Unified Model, </title> <booktitle> in Coming of Age: Proceedings of the Sixth ECMWF Workshop on the Use of Parallel Processors in Meteorology, World Scientific, </booktitle> <address> River Edge, New Jersey, </address> <year> 1995, </year> <pages> pp. 195-205. </pages>
Reference-contexts: Load imbalance is a source of inefficiency that results in some fraction of available processing power being lost as lightly loaded processors wait for more heavily loaded processors to finish. Load imbalance in atmospheric codes comes from a number of sources [11] [16] <ref> [4] </ref>. * The number of processors may not evenly divide the data domain. * Domain boundaries entail less work than the interior. * Model physics (the parameterization of solar radiation, cloud processes, boundary layer physics, etc.) can perform different amounts of computation depending on the local state of the model in
Reference: [5] <author> I. Foster and J. Michalakes, MPMM: </author> <title> A Massively Parallel Mesoscale Model, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science, </booktitle> <editor> G.-R. Hoffmann and T. Kauranne, eds., </editor> <publisher> World Scientific, </publisher> <address> River Edge, New Jersey, </address> <year> 1993, </year> <pages> pp. 354-363. </pages>
Reference: [6] <author> G. Grell, A. Pfeiffer, L. Schade, and J. Michalakes, </author> <title> Regional and Local Climate Modeling in Bavaria: Verification with Lightning Statistics, </title> <booktitle> presented to Sixth Annual MM5 Users Group Workshop, NCAR, </booktitle> <address> Boulder, Colorado, </address> <month> July 22-24, </month> <year> 1996. </year>
Reference-contexts: Load balancing takes occurs at 3.5 hours, reducing the increase in half. The load balancing itself required about 18 seconds each time it was invoked. supplied irregular boundary information when an irregularly shaped domain is specified. used for a climate simulation <ref> [6] </ref>. The nest, at 5-km resolution, is irregularly shaped and fitted over a region of the Alps (map not shown). The irregular domain requires only 6972 horizontal grid cells, compared with 9592 cells for the enclosing rectangle, for a savings of roughly 27 percent.
Reference: [7] <author> G. A. Grell, J. Dudhia, and D. R. Stauffer, </author> <title> A Description of the Fifth-Generation Penn State/NCAR Mesoscale Model (MM5), </title> <type> Tech. Rep. </type> <institution> NCAR/TN-398+STR, National Center for Atmospheric Research, Boulder, Colorado, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: MM5. The PSU/NCAR MM5 models limited-area atmospheric systems ranging from several thousand kilometers to several hundred. It is a primitive-equations model employing finite differencing for atmospheric dynamics and has a rich complement of physics parameterization packages: solar radiation, cumulus, moisture physics, and boundary layer physics <ref> [7] </ref>. It allows multiple grids for nesting high-resolution computations over regions of interest in a simulation with two-way interaction between nest levels. Four-dimensional data assimilation, in the form of Newtonian nudging, is provided to allow the incorporation of observational data to refine a forecast at run time.
Reference: [8] <author> W. Gropp and D. Keyes, </author> <title> Semi-structured refinement and parallel domain decomposition methods, in Unstructured Scientific Computation on Scale Multiprocessors, </title> <editor> P. Mehrotra, J. Saltz, and R. Voigt, eds., </editor> <year> 1990, </year> <pages> pp. 187-203. </pages>
Reference-contexts: The solution is nesting | the ability to create or delete finer subgrids in a background mesh to obtain a given level of accuracy with a minimum number of grid points. The Penn State NCAR Mesoscale Model (MM5), for example, uses an approach involving quasi-uniform grids <ref> [8] </ref>; that is, the model domain is divided into tiles, which are then further divided as necessary to provide higher resolution, preserving the alignment and orientation of grid points. The realization of "adaptive" mesh refinement in MM5 and most other regional weather models is primitive, however.
Reference: [9] <author> R. Hempel and H. Ritzdorf, </author> <title> The GMD Communications Library for Grid-oriented Problems, </title> <type> Tech. Rep. </type> <institution> GMD-0589, German National Research Center for Information Technology, </institution> <year> 1991. </year>
Reference: [10] <author> S. R. Kohn and S. B. Baden, </author> <title> A Parallel Software Infrastructure for Structured Adaptive Mesh Methods, </title> <booktitle> in Proceedings of Supercomputing '95, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>

Reference: [15] <author> J. Michalakes, T. Canfield, R. Nanjundiah, S. Hammond, and G. Grell, </author> <title> Parallel Implementation, Validation, and Performance of MM5, </title> <booktitle> in Coming of Age: Proceedings of the Sixth ECMWF Workshop on the Use of Parallel Processors in Meteorology, World Scientific, </booktitle> <address> River Edge, New Jersey, </address> <year> 1995, </year> <pages> pp. 266-276. </pages>
Reference: [16] <author> J. Michalakes and R. Nanjundiah, </author> <title> Computational Load in Model Physics of the 16 John G. Michalakes Parallel NCAR Community Climate Model, </title> <type> Tech. Rep. </type> <institution> ANL/MCS-TM-186, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Load imbalance is a source of inefficiency that results in some fraction of available processing power being lost as lightly loaded processors wait for more heavily loaded processors to finish. Load imbalance in atmospheric codes comes from a number of sources [11] <ref> [16] </ref> [4]. * The number of processors may not evenly divide the data domain. * Domain boundaries entail less work than the interior. * Model physics (the parameterization of solar radiation, cloud processes, boundary layer physics, etc.) can perform different amounts of computation depending on the local state of the model
Reference: [17] <author> M. Parashar and J. C. Browne, </author> <title> Distributed dynamic data-structures for parallel adaptive mesh-refinement, </title> <booktitle> Proceedings of the International Conference for High Performance Computing, </booktitle> <year> 1995, </year> <pages> pp. 22-27. </pages>
Reference: [18] <author> B. Rodriguez, L. Hart, and T. Henderson, </author> <title> A Library for the Portable Paral-lelization of Operational Weather Forecast Models, </title> <booktitle> in Coming of Age: Proceedings of the Sixth ECMWF Workshop on the Use of Parallel Processors in Meteorology, World Scientific, </booktitle> <address> River Edge, New Jersey, </address> <year> 1995, </year> <pages> pp. 148-161. </pages>
Reference: [19] <author> K.L. Simunich and S. Pinkerton and J. </author> <title> Michalakes System Implementation for Global Theater Weather Analysis and Prediction System (GTWAPS), </title> <booktitle> in proceedings of the 13th International Conference on Interactive Information and Processing Systems (IIPS) for Meteorology, Oceanography, and Hydrology. </booktitle> <address> Long Beach, CA, </address> <month> February </month> <year> 1997, </year> <pages> pp. 217-220. </pages>
Reference-contexts: The resulting remapping restores efficiency to 96 percent (Period C). The cost for each remapping is approximately 18 seconds. The usefulness of MM90 load-balancing is demonstrated in a production setting. As part of the U.S. Air Force Global Theater Weather Analysis and Prediction System (GTWAPS) <ref> [19] </ref>, MM90 produces twice-daily real-time forecasts at 10-km resolution covering a 1000-km by 1000-km domain centered over Bosnia.
References-found: 16

