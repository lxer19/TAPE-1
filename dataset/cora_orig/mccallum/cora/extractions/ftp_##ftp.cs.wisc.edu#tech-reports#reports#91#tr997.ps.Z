URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr997.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Implementing Stack Simulation for Highly-Associative Memories  
Author: Yul H. Kim Mark D. Hill David A. Wood 
Abstract: Prior to this work, all implementations of stack simulation [MGS70] required more than linear time to process an address trace. In particular these implementations are often slow for highly-associative memories and traces with poor locality, as can be found in simulations of file systems. We describe a new implementation of stack simulation where the referenced block and its stack distance are found using a hash table rather than by traversing the stack. This allows the trace-driven simulation of multiple alternative memories with the same block size, the same number of sets (e.g., fully associative), and using the least-recently-used replacement policy, with one pass through the trace in linear time. The key to this implementation is that designers are rarely interested in a continuum of memory sizes, but instead desire metrics for only a small, discrete set of alternatives (e.g., powers of two). We determine the memories in which a block resides by augmenting the state of each block with an index to the largest memory that contains that block. We update this state by using pointers to the block below the least-recently-used block in each memory. Our experimental evaluation confirms that the run-time of the new implementation is linear in address trace length and independent of trace locality. KEY WORDS: trace-driven simulation, stack simulation, caches, memory systems, file systems. 
Abstract-found: 1
Intro-found: 1
Reference: [BeK75] <author> B. T. Bennett and V. J. Kruskal, </author> <title> LRU Stack Processing, </title> <journal> IBM Journal of R & D, </journal> <month> July </month> <year> 1975, </year> <pages> 353-357. </pages>
Reference-contexts: Since finding references in the stack (and therefore the reference's stack distance) involves traversing the linked list, these implementations have running times proportional to the length of the trace times the mean stack distance [Tho87]. Bennett and Kruskal <ref> [BeK75] </ref> and Thompson [Tho87] use hash tables to supplement their linked-list stacks. The tables determine if a reference is not contained in the stack, in which case a futile search of the stack can be avoided. <p> Simulations for large virtual memories, file systems, or that use traces reduced through stack deletion will therefore have poor running times with linked-list stack simulations due to their large mean stack distances. Bennett and Kruskal <ref> [BeK75] </ref> and Olken [Olk81] have proposed LRU simulation algorithms that encode the stack's state in a tree. These methods reduce the number of elements that must be searched to determine a reference's stack distance (or determine that the reference is not in the stack).
Reference: [BKW90] <author> A. Borg, R. E. Kessler and D. W. Wall, </author> <title> Generation and Analysis of Very Long Address Traces, </title> <booktitle> Proceedings Seventeenth International Symposium on Computer Architecture, </booktitle> <address> Seattle, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: A major shortcoming is the hours of execution time a single simulation can consume, particularly when it is applied to highly-associative memories with traces having poor locality. Traces of billions of references are being used today <ref> [BKW90] </ref>, which only exacerbates the problem. Simple and efficient algorithms are needed to reduce simulation time. A key approach is to evaluate multiple alternative memories with a single pass through an address trace. <p> Traces The input traces were selected portions of long CPU address traces generated through link-time code modification techniques <ref> [BKW90] </ref>. We would have preferred to use file system traces, but had none at our disposal. <p> It should also be noted that the link-time modification techniques used to generate the five main traces lose some information about the interleaving of instruction fetches and data and stack accesses within basic blocks of instructions <ref> [BKW90] </ref>. As a result, these main traces may exhibit more locality and thus lower mean stack distances than might actually observed from the unaltered reference stream as seen by a unified memory.
Reference: [HiS89] <author> M. D. Hill and A. J. Smith, </author> <title> Evaluating Associativity in CPU Caches, </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1612-1630. </pages>
Reference-contexts: Simple and efficient algorithms are needed to reduce simulation time. A key approach is to evaluate multiple alternative memories with a single pass through an address trace. But while good algorithms exist for direct-mapped and set-associative caches <ref> [HiS89] </ref>, the algorithms for fully-associative memories are less attractive. Mattson et al. [MGS70] describe a single-pass technique called stack simulation that can efficiently simulate multiple memories with the same block size (line, page), the same number of sets, and using the least-recently-used (LRU) replacement policy. <p> in the reference, locate the reference in the stack (or determine that the reference is not in the stack) and determine its stack distance, update metrics to indicate which memories contain the reference, and then update the stack to reflect changes in the contents of the memories after this reference <ref> [HiS89] </ref>. We call these stages INPUT, FIND, METRIC, and UPDATE, respectively. After first describing the data structures, we examine each of the four stages in turn. - -- 2.1. <p> A key point in incorporating write backs into our algorithm is that we need to update a block's dirty level only when it is referenced. This hashing algorithm is not adaptable to all-associativity simulation <ref> [HiS89] </ref>. All-associativity simulation relies on traversing the stack and recording the number of blocks that would have appeared in the same set as the referenced block over a range of number of sets.
Reference: [HoS76] <author> E. Horowitz and S. Sahni, </author> <title> Fundamentals of Data Structures, </title> <publisher> Computer Science Press, Inc., </publisher> <year> 1976. </year>
Reference-contexts: The average number of elements that must be searched to find a block in the hash table is approximately 1 + 2 a hh where a equals the number of blocks hashed into the table divided by the number of buckets in the table <ref> [HoS76] </ref>. For a sufficiently large hash table, FIND can be done in essentially constant time. - -- FIND, in addition to finding a block in the stack, must determine its stack distance.
Reference: [Knu73] <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year> - -- 
Reference-contexts: Addresses are hashed by block number (the integer address divided by the block size in bytes). Each non-empty hash bucket points into the doubly-linked stack to the block in which the reference occurred, and uses chaining to resolve collisions <ref> [Knu73] </ref>. In our implementation, the hash table entries are combined with the stack nodes to simplify storage management. The hash function (bucket# = reference-block# MOD hash-table-size) is simple and fast, while causing few collisions. We chose the size of the hash table rather arbitrarily to be 8201 or 10001 buckets.
Reference: [MGS70] <author> R. L. Mattson, J. Gecsei, D. R. Slutz and I. L. Traiger, </author> <title> Evaluation techniques for storage hierarchies, </title> <journal> IBM Systems Journal 9, </journal> <volume> 2 (1970), 78 - 117. </volume>
Reference-contexts: Simple and efficient algorithms are needed to reduce simulation time. A key approach is to evaluate multiple alternative memories with a single pass through an address trace. But while good algorithms exist for direct-mapped and set-associative caches [HiS89], the algorithms for fully-associative memories are less attractive. Mattson et al. <ref> [MGS70] </ref> describe a single-pass technique called stack simulation that can efficiently simulate multiple memories with the same block size (line, page), the same number of sets, and using the least-recently-used (LRU) replacement policy.
Reference: [Olk81] <author> F. Olken, </author> <title> Efficient Methods for Calculating the Success Function of Fixed Space Replacement Policies, </title> <type> Masters Report, </type> <institution> Lawrence Berkeley Laboratory LBL-12370, University of California, Berkeley, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: Simulations for large virtual memories, file systems, or that use traces reduced through stack deletion will therefore have poor running times with linked-list stack simulations due to their large mean stack distances. Bennett and Kruskal [BeK75] and Olken <ref> [Olk81] </ref> have proposed LRU simulation algorithms that encode the stack's state in a tree. These methods reduce the number of elements that must be searched to determine a reference's stack distance (or determine that the reference is not in the stack).
Reference: [ODH85] <author> J. Ousterhout, H. DaCosta, D. Harrison, J. Kunze, M. Kupfer and J. Thompson, </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> Proceedings Tenth Symposium on Operating System Principles, </booktitle> <address> Orcas Island, Washington, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: For implementation reasons, CPU caches typically limit associativities to at most four, further restricting the mean stack distance. Thus simple linked-list implementations give good performance for CPU cache simulation. However, simulations are also done for file systems <ref> [ODH85] </ref> and disk caches which are generally fully-associative and exhibit much higher mean stack distances; Thompson [Tho87] observed mean stack distances between 200 and 500 for disk and file system traces. Filtered input traces, used to reduce simulation time, may also experience large mean stack distances.
Reference: [RoD90] <author> J. T. Robinson and M. V. Devarakonda, </author> <title> Data Cache Management Using Frequency-Based Replacement, </title> <booktitle> Proceedings SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boulder, Colorado, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: table, and associated data structures. 2 previous element in stack previous element of same hash table bucket next element of same hash table bucket next element in stack in-memory tag In-memory field number of largest memory size of interest to the user containing this block hhhhhhhhhhhhhhhhhh 1 Robinson and Devarakonda <ref> [RoD90] </ref> use a similar set of pointers indexing into an LRU stack to implement frequency-based replacement in a disk cache. 2 In our implementation of the algorithm, we added a backpointer per element to make pruning in UPDATE more convenient. - -- 2 8 1 3 blocks size in # Top
Reference: [Smi77] <author> A. J. Smith, </author> <title> Two Methods for the Efficient Analysis of Memory Address Trace Data, </title> <journal> IEEE Trans. on Software Eng. </journal> <volume> SE-3, </volume> <month> 1 (January </month> <year> 1977), </year> <pages> 94-101. </pages>
Reference-contexts: Filtered input traces, used to reduce simulation time, may also experience large mean stack distances. One filtering technique is stack deletion, in which references that would otherwise hit near the top of stack are deleted <ref> [Smi77] </ref> (see Section 3.1 for a more complete description). Stack deletion increases the mean stack distance by greatly reducing the locality in a trace. <p> Table 2 lists additional traces derived from the original five using stack deletion, a reduction technique that deletes from the trace all references that hit within the top d blocks of an LRU stack <ref> [Smi77] </ref>. The parameter d is called the deletion depth. Besides reducing trace length, stack deletion also reduces the trace's locality, and thus increases the mean stack distance that the trace would induce in a stack simulation. <p> The depths of misses are recorded as the size of the stack at the time of the reference. Stack deletion is an approximate technique, and can introduce error by skewing the simulation's distance counts and miss ratios <ref> [Smi77] </ref>. However, in this paper we are interested primarily in our simulator's efficiency rather than its results. Stack deletion gives us a convenient way to vary the amount of locality and mean stack distance of the traces used in our evaluation.
Reference: [Tho87] <author> J. G. Thompson, </author> <title> Efficient Analysis of Caching Systems, </title> <institution> Computer Science Division Technical Report UCB/Computer Science Dept. 87/374, University of California, Berkeley, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: Since finding references in the stack (and therefore the reference's stack distance) involves traversing the linked list, these implementations have running times proportional to the length of the trace times the mean stack distance <ref> [Tho87] </ref>. Bennett and Kruskal [BeK75] and Thompson [Tho87] use hash tables to supplement their linked-list stacks. The tables determine if a reference is not contained in the stack, in which case a futile search of the stack can be avoided. <p> Since finding references in the stack (and therefore the reference's stack distance) involves traversing the linked list, these implementations have running times proportional to the length of the trace times the mean stack distance <ref> [Tho87] </ref>. Bennett and Kruskal [BeK75] and Thompson [Tho87] use hash tables to supplement their linked-list stacks. The tables determine if a reference is not contained in the stack, in which case a futile search of the stack can be avoided. <p> However, traversal of the stack is still required to determine references' stack distances, such that these algorithms still run in time proportional to the length of the trace times the mean stack distance <ref> [Tho87] </ref>. CPU traces usually exhibit good locality, so CPU cache simulations usually have small mean stack distances (even with fully-associative caches); Thompson reports mean stack distances between seven and twenty [Tho87]. For implementation reasons, CPU caches typically limit associativities to at most four, further restricting the mean stack distance. <p> such that these algorithms still run in time proportional to the length of the trace times the mean stack distance <ref> [Tho87] </ref>. CPU traces usually exhibit good locality, so CPU cache simulations usually have small mean stack distances (even with fully-associative caches); Thompson reports mean stack distances between seven and twenty [Tho87]. For implementation reasons, CPU caches typically limit associativities to at most four, further restricting the mean stack distance. Thus simple linked-list implementations give good performance for CPU cache simulation. <p> Thus simple linked-list implementations give good performance for CPU cache simulation. However, simulations are also done for file systems [ODH85] and disk caches which are generally fully-associative and exhibit much higher mean stack distances; Thompson <ref> [Tho87] </ref> observed mean stack distances between 200 and 500 for disk and file system traces. Filtered input traces, used to reduce simulation time, may also experience large mean stack distances. <p> Bennett and Kruskal's algorithm takes advantage of the fact that a block's stack distance is just the number of unique blocks referenced since the block's last reference <ref> [Tho87] </ref>. Simulation times become proportional to the log of the average inter-reference time. Olken further refines Bennett and Kruskal's algorithm to run in bounded space where the alternative memories are smaller than some given size. <p> The tree is dynamically rebalanced, and thus exhibits a better worst-case running time than Bennett and Kruskal's algorithm. Thompson shows that further improvements, to maintain a better balanced tree with fewer rebalancing actions, reduce the algorithm's complexity <ref> [Tho87] </ref>. However, Thompson's measurements indicate that Bennett and Kruskal's algorithm typically gives the best running time of the tree-based methods. <p> In practice, file system traces have mean stack distances much larger than those of CPU traces; Thompson reports CPU traces with mean stack distances in the range of 7.5 to 71 and disk and file-system traces in the range of 240 to 500, not including misses <ref> [Tho87] </ref>. The last column of Table 2 lists the relative error caused by stack deletion for a 1M fully-associative memory with a 1K byte block size. <p> This is especially true for CPU cache simulations, where small associativities limit stack depths. But, for traces with poor locality, like disk or file system traces, hashing is the better choice. Thompson suggests that file and disk system traces have mean stack distances between 200 and 500 <ref> [Tho87] </ref>. Our results indicate that hashing would give ten to twenty times the performance of a linked-list-based simulator for these traces. 4. Extensions and Limitations The hashing algorithm presented in this paper can be extended to work with memories that use write back of dirty blocks to reduce memory traffic.
Reference: [ThS89] <author> J. G. Thompson and A. J. Smith, </author> <title> Efficient (Stack) Algorithms for Write-Back and Sector Memories, </title> <journal> ACM Trans. on Computer Systems 7, </journal> <month> 1 (February </month> <year> 1989), </year> <pages> 78-116. </pages> - -- 
Reference-contexts: Extensions and Limitations The hashing algorithm presented in this paper can be extended to work with memories that use write back of dirty blocks to reduce memory traffic. Thompson and Smith <ref> [ThS89] </ref> have proposed an efficient method for recording the number of write backs, by recording a dirty level within each block. The dirty - -- level indicates the stack distance at or below which all memories have that block dirty.
References-found: 12

