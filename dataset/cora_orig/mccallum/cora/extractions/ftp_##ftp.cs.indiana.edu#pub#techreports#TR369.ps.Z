URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR369.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Matrix Chain Ordering in Polylog Time with n=lg n Processors  
Author: Phillip G. Bradford Gregory J. E. Rawlins Gregory E. Shannon 
Date: February 19, 1993  
Abstract: This paper gives a O(lg 4 n) time and n=lg n processor algorithm for solving the matrix chain ordering problem and for finding optimal triangulations of a convex polygon on the Common CRCW PRAM model. This algorithm works by finding shortest paths in special digraphs modeling dynamic programming tables. These shortest paths are found cheaply using new and efficient techniques for exploiting monotonic problem constraints.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Apostolico, M. J. Atallah, L. L. Larmore and S. H. McFaddin: </author> <title> "Efficient Parallel Algorithms for String Editing and Related Problems," </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 19, No. 5, </volume> <pages> 968-988, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing <ref> [1, 3] </ref>, context free grammar recognition [22, 21], and optimal tree building [2, 19]. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms.
Reference: [2] <author> M. J. Atallah, S. R. Kosaraju, L. L. Larmore, G. L. Miller, and S.-H. Teng: </author> <title> "Constructing Trees in Parallel," </title> <booktitle> Proc. 1st Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 499-533, </pages> <year> 1989 </year>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing [1, 3], context free grammar recognition [22, 21], and optimal tree building <ref> [2, 19] </ref>. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms. Many efficient parallel algorithms designed to date rely on monotonicity conditions to give divide and conquer schemes.
Reference: [3] <author> A. Aggarwal and J. Park: </author> <title> "Notes on Searching Multidimensional Monotone Arrays," </title> <booktitle> Proceedings of the 29 th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> 497-512, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing <ref> [1, 3] </ref>, context free grammar recognition [22, 21], and optimal tree building [2, 19]. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms.
Reference: [4] <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber and U. Vishkin: </author> <title> "Highly Parallelizable Problems," </title> <booktitle> Symposium on the Theory on Computing, </booktitle> <pages> 309-319, </pages> <year> 1989. </year> <month> 22 </month>
Reference-contexts: Given an associative product with the level of each parenthesis known, then for each parenthesis find its matching parenthesis by solving the all nearest smaller value (ANSV) problem <ref> [4] </ref>: Given w 1 ; w 2 ; : : : ; w n , for each w find the indices, if they exist, of the nearest proceeding and succeeding weights both less than w. Let's call this pair of indices, if they exist, an ANSV match. <p> Figure 2 shows a weight list at a 45 o angle below the x-axis and dashed lines representing four key ANSV matches. The four corresponding critical nodes are circled in D n . In our nomenclature, <ref> [4] </ref> shows that: Theorem 2 Computing all critical nodes costs O (lg n) time with n=lg n processors. Two critical nodes on the same diagonal are compatible if no vertices other than (0; 0) can reach both of them by a unit path.
Reference: [5] <author> P. G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <booktitle> Extended Abstract in the Proceed- ings of the 30 th Allerton Conference on Communication, Control and Computation, </booktitle> <institution> University of Illinois at Urbana-Champaign, </institution> <month> 185-194, </month> <year> 1992. </year>
Reference-contexts: Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email: bradford@cs.indiana.edu. y Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email: rawlins@cs.indiana.edu. z Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email: shannon@cs.indiana.edu. 1 1.1 Main Results of this Paper Our approach follows <ref> [5] </ref>, recasting the MCOP as a shortest path problem in a graph modeling a dynamic programming table. This graph has O (n 2 ) nodes and with an all-pairs shortest paths algorithm finding a shortest path in this graph results in a n 6 =lg n processor MCOP algorithm. <p> Their algorithms require fi (lg 2 n) time and n 9 processors. Using pebbling games, Rytter [26] gave more efficient parallel algorithms for a similar class of optimization problems costing O (lg 2 n) time with n 6 =lg n processors. In <ref> [5] </ref>, an algorithm was given that takes O (lg 3 n) time and n 3 =lg n processors and [9] gave an algorithm that takes O (lg 3 n) time and n 2 =lg 3 n processors. <p> In addition, there are serial and parallel approximation algorithms for the MCOP <ref> [5, 6, 8, 15] </ref>. 1.3 Structure of the Paper Section 2, briefly reviews the interpretation of the MCOP as a shortest path graph problem from [5] and then summarizes the n 3 =lg n processor algorithm. Section 3, isolates this algorithm's n 3 =lg n processor bottlenecks. <p> In addition, there are serial and parallel approximation algorithms for the MCOP [5, 6, 8, 15]. 1.3 Structure of the Paper Section 2, briefly reviews the interpretation of the MCOP as a shortest path graph problem from <ref> [5] </ref> and then summarizes the n 3 =lg n processor algorithm. Section 3, isolates this algorithm's n 3 =lg n processor bottlenecks. The n 3 =lg n processor cost of these bottlenecks is from an all-pairs shortest paths algorithm. <p> Finally, section 5 replaces the all-pairs comparison algorithm with applications of parallel prefix and binary search. 2 An O (lg n) Time and n 3 =lg n Processor MCOP Algorithm This section briefly reviews the polylog time and n 3 =lg n processor MCOP Algorithm from <ref> [5] </ref>. Let T be an n fi n dynamic programming table for the matrix chain ordering problem, it has entries T [i; k] representing the cheapest cost of the matrix products M i * * M k . <p> Given a chain of n matrices finding a shortest path from (0; 0) to (1; n) in D n solves the MCOP <ref> [5] </ref>. <p> Where sp (j + 1; k) is the cost of a shortest path to node (j + 1; k) and f (i; j; k) = w i w j+1 w k+1 . The jumper (i; j) =) (i; t) is of length t j. See Figure 1. In <ref> [5] </ref> the MCOP was solved in polylog time with n 6 =lg n processors by using an all-pairs shortest path algorithm and exploiting the following Theorem: Theorem 1 (Duality Theorem [5]) If a shortest path from (0; 0) to (i; k) contains the jumper (i; j) =) (i; k) then there <p> The jumper (i; j) =) (i; t) is of length t j. See Figure 1. In <ref> [5] </ref> the MCOP was solved in polylog time with n 6 =lg n processors by using an all-pairs shortest path algorithm and exploiting the following Theorem: Theorem 1 (Duality Theorem [5]) If a shortest path from (0; 0) to (i; k) contains the jumper (i; j) =) (i; k) then there is a dual shortest path containing the jumper (j + 1; k) * (i; k). <p> Further, using a tree decomposition of D n and an all-pairs shortest path algorithm the MCOP was solved in polylog time using n 3 =lg n processors <ref> [5] </ref>. 2.1 Matrix Dimensions as Nesting Levels of Matching Parentheses The next four subsections show that using the list of matrix dimensions as nesting levels of matching parentheses gives a tree decomposition of D n that leads to efficient solutions of the MCOP. <p> Since a path of critical nodes represents a parenthesization, then all critical nodes are compatible. Also, D n has at most n 1 critical nodes and there is at least one path from (0; 0) to (1; n) that includes all critical nodes <ref> [5] </ref>. 2.2 Canonical Subgraphs of D n This subsection investigates the interaction between subgraphs containing critical nodes. All vertices and edges that can reach (i; t) by a unit path form the subgraph D (i; t). <p> On the other hand, if D (i; t) has no critical nodes, then its associated weight list is monotonic. As in <ref> [16, 17, 5] </ref> let kw i : w k k = P k1 j=i w j w j+1 , which is easily computable using differences of components of the parallel partial prefixes kw 1 : w i k for 2 i n + 1. <p> In addition, any shortest path not including critical nodes is a straight path of unit edges. Thus, any shortest path to a critical node that contains no other critical nodes is a straight path of unit edges <ref> [5] </ref>. Now a polylog time algorithm for finding shortest paths to all critical nodes in D (1;m) graphs is given. This algorithm takes O (lg 2 m) time and uses m 3 =lg m processors. <p> angular path as an edge and applying a parallel all-pairs shortest path algorithm. 2.3 Combining the Canonical Graphs for an Efficient Parallel Algorithm This subsection discusses a tree contraction algorithm that contracts the tree structure joining the canonical subgraphs to form a shortest path in D n , see also <ref> [16, 17, 5] </ref>. In D n a canonical tree joins all of the canonical subgraphs. Initially, for every leaf D (i;j) the critical node (i; j) is the tree leaf (i; j). <p> So in the rest of this paper let w 1 denote the smallest weight in any weight list. A result of Hu and Shing [16] and independently Yao [28] gives the next Corollary. 5 Corollary 1 (Atomicity Corollary <ref> [5] </ref>) Given a weight list w 1 ; : : : ; w n+1 , with the three small-est weights w 1 ; w j+1 , and w k+1 , such that 1 &lt; j &lt; k 1, then the critical nodes (1; j) and (1; k) are in a shortest
Reference: [6] <author> F. Y. Chin: </author> <title> "An O(n) Algorithm for Determining Near-Optimal Computation Order of Matrix Chain Products," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 21, No. 7, </volume> <pages> 544-549, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: In addition, there are serial and parallel approximation algorithms for the MCOP <ref> [5, 6, 8, 15] </ref>. 1.3 Structure of the Paper Section 2, briefly reviews the interpretation of the MCOP as a shortest path graph problem from [5] and then summarizes the n 3 =lg n processor algorithm. Section 3, isolates this algorithm's n 3 =lg n processor bottlenecks.
Reference: [7] <author> T. H. Cormen, C. E. Leiserson and R. L. Rivest: </author> <title> Introduction to Algorithms, </title> <publisher> McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: By efficient we mean that the processor-time product is within a polylog factor of the best sequential time. The matrix chain ordering problem (MCOP) is to find the cheapest way to multiply a chain of n matrices, where the matrices are pairwise compatible but of varying dimensions <ref> [7] </ref>. The MCOP is often the focus of dynamic programming research and pedagogy because of its amenability to an elementary dynamic programming solution. <p> All of this results in a polylog-time (O (lg 4 n)) and linear-processor (n=lg n) parallel algorithm for the MCOP on the Common CRCW PRAM. 1.2 Previous Results Elementary dynamic programming algorithms sequentially solve the matrix chain ordering problem in O (n 3 ) time <ref> [7] </ref>. However, the best serial solution of the MCOP is Hu and Shing's O (n lg n) algorithm [16, 17]. Using straight line arithmetic programs, Valiant et al. [27] showed that many classical optimization problems with efficient sequential dynamic programming solutions were in N C.
Reference: [8] <author> A. Czumaj: </author> <title> "An Optimal Parallel Algorithm for Computing a Near-Optimal Order of Matrix Multiplications," </title> <publisher> SWAT, Springer Verlag, </publisher> <pages> LNCS # 621 , 62-72, </pages> <year> 1992. </year>
Reference-contexts: In addition, there are serial and parallel approximation algorithms for the MCOP <ref> [5, 6, 8, 15] </ref>. 1.3 Structure of the Paper Section 2, briefly reviews the interpretation of the MCOP as a shortest path graph problem from [5] and then summarizes the n 3 =lg n processor algorithm. Section 3, isolates this algorithm's n 3 =lg n processor bottlenecks.
Reference: [9] <author> A. Czumaj: </author> <title> "Parallel algorithm for the matrix chain product and the optimal triangulation problem," </title> <note> to appear in the proceedings of STACS '93. </note>
Reference-contexts: Using pebbling games, Rytter [26] gave more efficient parallel algorithms for a similar class of optimization problems costing O (lg 2 n) time with n 6 =lg n processors. In [5], an algorithm was given that takes O (lg 3 n) time and n 3 =lg n processors and <ref> [9] </ref> gave an algorithm that takes O (lg 3 n) time and n 2 =lg 3 n processors.
Reference: [10] <author> L. E. Deimel, Jr. and T. A. Lampe: </author> <title> "An Invariance Theorem Concerning Optimal Computation of Matrix Chain Products," </title> <institution> North Carolina State Univ. </institution> <note> Tech Report # TR79-14. </note>
Reference-contexts: the MCOP with the weight list l 1 = w 1 ; w 2 ; : : :w n+1 , then cyclically rotating it getting l 2 and finding an optimal parenthesization for l 2 gives an optimal solution to the original instance of the MCOP with l 1 , <ref> [16, 10] </ref>. So in the rest of this paper let w 1 denote the smallest weight in any weight list.
Reference: [11] <author> Z. Galil and K. Park: </author> <title> Parallel Dynamic Programming, </title> <type> Manuscript, </type> <year> 1992. </year>
Reference: [12] <author> D. Hillis and G. L. Steele, Jr.: </author> <title> "Data Parallel Algorithms," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29, No. 12, </volume> <pages> 1170-1183, </pages> <month> Dec. </month> <year> 1986. </year>
Reference: [13] <author> S.-H. S. Huang, H. Liu, V. Viswanathan: </author> <title> "Parallel Dynamic Programming," </title> <booktitle> Proceedings of the 2 nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> 497-500, </pages> <year> 1990. </year>
Reference: [14] <author> S.-H. S. Huang, H. Liu, V. Viswanathan: </author> <title> "A Sublinear Parallel Algorithm for Some Dynamic Programming Problems," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 106, </volume> <pages> 361-371, </pages> <year> 1992. </year>
Reference: [15] <author> T. C. Hu and M. T. Shing: </author> <title> "An O(n) Algorithm to Find a Near-Optimum Partition of a Convex Polygon," </title> <journal> J. of Algorithms, </journal> <volume> Vol. 2, </volume> <pages> 122-138, </pages> <year> 1981. </year>
Reference-contexts: In addition, there are serial and parallel approximation algorithms for the MCOP <ref> [5, 6, 8, 15] </ref>. 1.3 Structure of the Paper Section 2, briefly reviews the interpretation of the MCOP as a shortest path graph problem from [5] and then summarizes the n 3 =lg n processor algorithm. Section 3, isolates this algorithm's n 3 =lg n processor bottlenecks.
Reference: [16] <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part I," </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 11, No. 3, </volume> <pages> 362-373, </pages> <year> 1982. </year>
Reference-contexts: However, the best serial solution of the MCOP is Hu and Shing's O (n lg n) algorithm <ref> [16, 17] </ref>. Using straight line arithmetic programs, Valiant et al. [27] showed that many classical optimization problems with efficient sequential dynamic programming solutions were in N C. Their algorithms require fi (lg 2 n) time and n 9 processors. <p> On the other hand, if D (i; t) has no critical nodes, then its associated weight list is monotonic. As in <ref> [16, 17, 5] </ref> let kw i : w k k = P k1 j=i w j w j+1 , which is easily computable using differences of components of the parallel partial prefixes kw 1 : w i k for 2 i n + 1. <p> angular path as an edge and applying a parallel all-pairs shortest path algorithm. 2.3 Combining the Canonical Graphs for an Efficient Parallel Algorithm This subsection discusses a tree contraction algorithm that contracts the tree structure joining the canonical subgraphs to form a shortest path in D n , see also <ref> [16, 17, 5] </ref>. In D n a canonical tree joins all of the canonical subgraphs. Initially, for every leaf D (i;j) the critical node (i; j) is the tree leaf (i; j). <p> the MCOP with the weight list l 1 = w 1 ; w 2 ; : : :w n+1 , then cyclically rotating it getting l 2 and finding an optimal parenthesization for l 2 gives an optimal solution to the original instance of the MCOP with l 1 , <ref> [16, 10] </ref>. So in the rest of this paper let w 1 denote the smallest weight in any weight list. <p> So in the rest of this paper let w 1 denote the smallest weight in any weight list. A result of Hu and Shing <ref> [16] </ref> and independently Yao [28] gives the next Corollary. 5 Corollary 1 (Atomicity Corollary [5]) Given a weight list w 1 ; : : : ; w n+1 , with the three small-est weights w 1 ; w j+1 , and w k+1 , such that 1 &lt; j &lt; k <p> Given a triangle with vertices w i ; w j and w k its cost is w i w j w k , see <ref> [16] </ref>.
Reference: [17] <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part II," </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 13, No. 2, </volume> <pages> 228-251, </pages> <year> 1984. </year>
Reference-contexts: However, the best serial solution of the MCOP is Hu and Shing's O (n lg n) algorithm <ref> [16, 17] </ref>. Using straight line arithmetic programs, Valiant et al. [27] showed that many classical optimization problems with efficient sequential dynamic programming solutions were in N C. Their algorithms require fi (lg 2 n) time and n 9 processors. <p> On the other hand, if D (i; t) has no critical nodes, then its associated weight list is monotonic. As in <ref> [16, 17, 5] </ref> let kw i : w k k = P k1 j=i w j w j+1 , which is easily computable using differences of components of the parallel partial prefixes kw 1 : w i k for 2 i n + 1. <p> angular path as an edge and applying a parallel all-pairs shortest path algorithm. 2.3 Combining the Canonical Graphs for an Efficient Parallel Algorithm This subsection discusses a tree contraction algorithm that contracts the tree structure joining the canonical subgraphs to form a shortest path in D n , see also <ref> [16, 17, 5] </ref>. In D n a canonical tree joins all of the canonical subgraphs. Initially, for every leaf D (i;j) the critical node (i; j) is the tree leaf (i; j).
Reference: [18] <author> J. JaJa: </author> <title> An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference: [19] <author> D. G. Kirkpatrick and T. Przytycka: </author> <title> "Parallel Construction of Near Optimal Binary Search Trees," </title> <booktitle> Proceedings of Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> 234-243, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing [1, 3], context free grammar recognition [22, 21], and optimal tree building <ref> [2, 19] </ref>. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms. Many efficient parallel algorithms designed to date rely on monotonicity conditions to give divide and conquer schemes.
Reference: [20] <author> R. M. Karp and V. Ramachandran: </author> <title> "Parallel Algorithms for Shared Memory Machines," </title> <booktitle> Chapter in Handbook of Theoretical Computer Science, </booktitle> <publisher> V. Van Leeuwen|editor, Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: Use the Euler tour technique <ref> [20] </ref> when the raking order is arbitrary. Given two nested bands, say D (i;v) (j;u) is nested around D (k;t) (r;s) , that is j k &lt; t u.
Reference: [21] <author> P. N. Klein and J. H. Reif: </author> <title> "Parallel Time O(lg n) Acceptance of Deterministic CFLs on an Exclusive-Write P-RAM," </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 17, </volume> <pages> 463-485, </pages> <year> 1988. </year> <month> 23 </month>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing [1, 3], context free grammar recognition <ref> [22, 21] </ref>, and optimal tree building [2, 19]. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms. Many efficient parallel algorithms designed to date rely on monotonicity conditions to give divide and conquer schemes.
Reference: [22] <author> L. L. Larmore and W. Rytter: </author> <title> "Efficient Sublinear Time Parallel Algorithms for the Recogni--tion of Context-Free Languages," </title> <publisher> SWAT, Springer Verlag, LNCS #577, </publisher> <pages> 121-132, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, much research has gone into designing efficient parallel algorithms for problems with elementary serial dynamic programming solutions. These problems include string editing [1, 3], context free grammar recognition <ref> [22, 21] </ref>, and optimal tree building [2, 19]. Polylog time parallel algorithms for solving these problems use new approaches since straightforward parallelization of sequential dynamic programming algorithms produces very slow (linear-time) parallel algorithms. Many efficient parallel algorithms designed to date rely on monotonicity conditions to give divide and conquer schemes.
Reference: [23] <author> P. Ramanan: </author> <title> "A New Lower Bound Technique and its Application: Tight Lower Bounds for a Polygon Triangularization Problem," </title> <booktitle> Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 281-290, </pages> <year> 1991. </year>
Reference: [24] <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for Finding an Optimal Order of Computing a Matrix Chain Product," </title> <type> Technical Report, </type> <institution> WSUCS-92-2, Wichita State University, </institution> <month> June, </month> <year> 1992. </year>
Reference-contexts: However, until recently none of this work has given an efficient (linear-processor) polylogarithmic time algorithm for the MCOP. In <ref> [24, 25] </ref> Ramanan very recently has independently given an O (lg 4 n) time and n processor algorithm for solving the MCOP. fl Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email: bradford@cs.indiana.edu. y Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email:
Reference: [25] <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for the Matrix Chain Product Problem," </title> <type> Technical Report, </type> <institution> WSUCS-93-1, Wichita State University, </institution> <month> January, </month> <year> 1993. </year>
Reference-contexts: However, until recently none of this work has given an efficient (linear-processor) polylogarithmic time algorithm for the MCOP. In <ref> [24, 25] </ref> Ramanan very recently has independently given an O (lg 4 n) time and n processor algorithm for solving the MCOP. fl Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email: bradford@cs.indiana.edu. y Department of Computer Science, Indiana University, 215 Lindley Hall, Bloomington, Indiana 47405. email:
Reference: [26] <author> W. Rytter: </author> <title> "On Efficient Parallel Computation for Some Dynamic Programming Problems," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 59, </volume> <pages> 297-307, </pages> <year> 1988. </year>
Reference-contexts: Using straight line arithmetic programs, Valiant et al. [27] showed that many classical optimization problems with efficient sequential dynamic programming solutions were in N C. Their algorithms require fi (lg 2 n) time and n 9 processors. Using pebbling games, Rytter <ref> [26] </ref> gave more efficient parallel algorithms for a similar class of optimization problems costing O (lg 2 n) time with n 6 =lg n processors.
Reference: [27] <author> L. G. Valiant, S. Skyum, S. Berkowitz and C. Rackoff: </author> <title> "Fast Parallel Computation of Polynomials Using Few Processors," </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 12, No. 4, </volume> <pages> 641-644, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: However, the best serial solution of the MCOP is Hu and Shing's O (n lg n) algorithm [16, 17]. Using straight line arithmetic programs, Valiant et al. <ref> [27] </ref> showed that many classical optimization problems with efficient sequential dynamic programming solutions were in N C. Their algorithms require fi (lg 2 n) time and n 9 processors.
Reference: [28] <author> F. F. Yao: </author> <title> "Speed-Up in Dynamic Programming," </title> <journal> SIAM J. on Algebraic and Discrete Methods, </journal> <volume> Vol. 3, No. 4, </volume> <pages> 532-540, </pages> <year> 1982. </year> <month> 24 </month>
Reference-contexts: So in the rest of this paper let w 1 denote the smallest weight in any weight list. A result of Hu and Shing [16] and independently Yao <ref> [28] </ref> gives the next Corollary. 5 Corollary 1 (Atomicity Corollary [5]) Given a weight list w 1 ; : : : ; w n+1 , with the three small-est weights w 1 ; w j+1 , and w k+1 , such that 1 &lt; j &lt; k 1, then the critical
References-found: 28

