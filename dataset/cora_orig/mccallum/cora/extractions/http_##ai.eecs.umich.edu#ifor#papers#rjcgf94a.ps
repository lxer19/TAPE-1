URL: http://ai.eecs.umich.edu/ifor/papers/rjcgf94a.ps
Refering-URL: http://ai.eecs.umich.edu/ifor/papers/index.html
Root-URL: http://www.eecs.umich.edu
Phone: 2  
Title: Generating Behavior in Response to Interacting Goals example from the tactical flight domain To illustrate
Author: Randolph M. Jones, John E. Laird, Milind Tambe, and Paul S. Rosenbloom 
Note: An  
Address: 1101 Beal Avenue Ann Arbor, MI 48109-2110  4676 Admiralty Way Marina Del Rey, CA 90292  
Affiliation: 1 Artificial Intelligence Laboratory University of Michigan  Information Sciences Institute University of Southern California  
Abstract: The domains that computer-generated forces address (such as tactical flight) are more complex than have generally been used in artificial-intelligence research. A particular characteristic of this complexity is that a reasonable agent must attend to a large number of goals at the same time. Moreover, some of these goals are independent, while others interact with each other in a variety of ways. This research focuses on a number of issues involved in representing, reasoning about, and learning about such complex goal structures. We discuss a number of approaches that we have examined within the framework of the TacAir-Soar system. The Soar-IFOR project aims to build believable agents for tactical air simulation. We have constructed a system, called TacAir-Soar, that embodies a large amount of knowledge for carrying out tactical naval air missions (Jones, Tambe, Laird, & Rosenbloom, 1993; Rosenbloom et al., 1994). In the course of our research, we have developed a large ontology of the knowledge required to generate human-like behavior in flight simulation. This includes knowledge about mission goals, doctrine, equipment specifications, survival, situational awareness and interpretation, cooperation, and other aspects of the task. Although each of these types of knowledge is relatively independent, their impact on behavior is highly interdependent. This paper investigates various representations for sets of interacting goals that arise from such a complex knowledge base. We have identified five issues that we wish to address in our examination of the candidate approaches. First, it appears to be necessary to represent agent goals as a forest of interacting goal hierarchies. Second, existing goal-driven systems are not designed for such a goal representation, so we must find an appropriate mapping between agent goals and the types of goals that current architectures for intelligence allow (e.g., we want the architecture to do as much maintenance of goals as possible). Third, the agent must reason about how well different actions achieve combinations of goals. Fourth, the ideal knowledge representation should facilitate effective learning within the architecture. Finally, the representation should also allow the knowledge base to be updated by subject-matter experts and knowledge engineers with a minimum of effort. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chapman, D. </author> <year> (1987). </year> <title> Planning for conjunctive goals. </title> <journal> In Artificial Intelligence, </journal> <volume> 32, </volume> <pages> 333-377. </pages>
Reference: <author> Cohen, P. R., Greenberg, M. L., </author> <title> Hart, </title> <address> D. </address>
Reference: <author> M., & Howe, A. E. </author> <year> (1989). </year> <title> Understanding the design requirements for agents in complex environments. </title> <journal> AI magazine, </journal> <volume> 10(3), </volume> <pages> 32-48. </pages>
Reference: <author> Jones, R. M., Tambe, M., Laird, J. E., & Rosenbloom, P. S. </author> <year> (1993). </year> <title> Intelligent automated agents for flight training simulators. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation (pp. </booktitle> <pages> 33-42). </pages> <address> Orlando, FL. </address>
Reference: <editor> Minton, S., Knoblock, C. A., Kuokka, </editor> <address> D. </address>
Reference: <author> R., Gil, Y., & Carbonell, J. G. </author> <year> (1989). </year> <title> Prodigy 2.0: The manual and tutorial. </title> <type> Technical report no. </type> <institution> CMU-CS-89-146, School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Mitchell, T. M., Allen, J., Chalasani, P., Cheng, J., Etzioni, O., Ringuette, M., & Schlimmer, J. </author> <year> (1991). </year> <title> Theo: A framework for self-improving systems. </title>
Reference-contexts: Mapping agent goals to architectural goals As the previous section illustrated, agent goals are best represented as a set of goal hierarchies. However, traditional AI systems do not encourage this type of representation. For example, architectures such as Soar, Prodigy (Minton et al, 1989), and Theo <ref> (Mitchell et al., 1991) </ref> make goals a first-class object type, and they include specific mechanisms for representing, posting, and learning about goals. But these goals can only be expressed easily in a single stack or hierarchy.
Reference: <editor> In K. VanLehn (Ed.), </editor> <booktitle> Architectures for intelligence: The 22nd Carnegie Mellon Symposium on Cognition. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Rosenbloom, P. S., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E., Lehman, J. F., Rubinoff, R., Schwamb, K. B., & Tambe, M. </author> <year> (1994). </year> <title> Intelligent automated agents for tactical air simulation: A progress report. </title> <booktitle> In Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation. </booktitle> <address> Orlando, FL. </address>
Reference: <author> Rosenbloom, P. S., Laird, J. E., Newell, A., & McCarl, R. </author> <year> (1991). </year> <title> A preliminary analysis of the Soar architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 47, </volume> <pages> 289-325. </pages>
Reference-contexts: All of our efforts have been developed with variations of the TacAir-Soar agent (Jones et al., 1993; Rosenbloom et al., 1994), which is implemented within the Soar architecture for cognition <ref> (Rosenbloom, Laird, Newell, & McCarl, 1991) </ref>. Mapping agent goals to architectural goals As the previous section illustrated, agent goals are best represented as a set of goal hierarchies. However, traditional AI systems do not encourage this type of representation.
Reference: <author> Veloso, M. M. </author> <year> (1989). </year> <title> Nonlinear problem solving using intelligent casual commitment. </title> <type> Technical Report no. </type> <institution> CMU-CS-89-210, School of Computer Science, Carnegie Mellon University. Biographies Randolph M. </institution> <type> Jones received his Ph.D. </type> <institution> in Information and Computer Science from the University of California, Irvine, </institution> <month> in </month> <year> 1989. </year> <title> He is currently an assistant research scientist in the Artificial Intelligence Laboratory at the University of Michigan. His research interests lie in the areas of intelligent agents, problem solving, machine learning, and psychological modeling. </title>

Reference: <author> Paul S. </author> <title> Rosenbloom is an associate professor of computer science at the University of Southern California and the acting deputy director of the Intelligent Systems Division at the Information Sciences Institute. He received his B.S. </title> <note> degree in mathematical sciences from Stan-ford University in 1976 and his M.S. and Ph.D. degrees in computer science from Carnegie-Mellon University in 1978 and 1983, </note> <author> respectively. </author> <title> His research centers on integrated intelligent systems (in particular, Soar), but also covers other areas such as machine learning, production systems, planning, and cognitive modeling. He is a Councillor of the AAAI and a past Chair of ACM SIGART. </title>
References-found: 12

