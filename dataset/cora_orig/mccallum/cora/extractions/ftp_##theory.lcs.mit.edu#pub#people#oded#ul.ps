URL: ftp://theory.lcs.mit.edu/pub/people/oded/ul.ps
Refering-URL: http://theory.lcs.mit.edu/~oded/papers.html
Root-URL: 
Email: E-mail: oded@wisdom.weizmann.ac.il.  E-mail: danar@theory.lcs.mit.edu.  
Title: On Universal Learning Algorithms  
Author: Oded Goldreich Dana Ron 
Address: Rehovot, Israel.  Cambridge 02139, USA.  
Affiliation: Department of Computer Science and Applied Mathematics, Weizmann Institute of Science,  MIT. Laboratory for Computer Science, MIT,  
Note: Presented in the Impromptu Session of COLT96.  On sabbatical leave at LCS,  Research was supported in part by an NSF Postdoctoral Fellowship.  
Date: July 5, 1996  
Abstract: We observe that there exists a universal learning algorithm which PAC-learns every concept class within complexity which is linearly related to the complexity of the best learning algorithm for this class. This observation is derived by a straightforward adaptation, to the learning context, of Levin's proof of the existence of optimal algorithms for NP. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and Concept Learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: We wish to stress that this result has no practical significance since the constant hidden in the O-notation is huge (see the proof). On the other hand, the result holds also for many extensions of the PAC-learning model [12] such as learning with queries <ref> [1] </ref>, learning under a fixed distribution (e.g., [3]), learning with statistical queries [7], etc. <p> That is, given a hypothesis h one should determine, within time O ( log (1=ffi) * ), whether h approximates the target concept. Models in which the condition holds include learning with (membership and/or equivalence) queries <ref> [1] </ref>, learning under a fixed distribution (e.g., [3]), learning with statistical queries [7], etc. Theorem 4 also holds with respect to proper (representation dependent) learning, provided that membership in the hypothesis class can be easily decided.
Reference: [2] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: We remark that for any "non-trivial" concept class 1 C, any PAC-learning algorithm must take log (1=ffi) * many examples <ref> [2] </ref>, and so t (; ; *; ffi) &gt; log (1=ffi) * and the additive log (1=ffi) * term can be omitted. 1 A concept class is trivial if it consists of a single concept or of two disjoint concepts whose union equals the instance space. 1 2.1 Proof in a
Reference: [3] <author> G. M. Benedek and A. Itai. </author> <title> Learnability with Respect to Fixed Distributions. </title> <journal> Theoretical Computer Science, </journal> <volume> 86(2) </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: On the other hand, the result holds also for many extensions of the PAC-learning model [12] such as learning with queries [1], learning under a fixed distribution (e.g., <ref> [3] </ref>), learning with statistical queries [7], etc. In general, the result holds in any model which enables efficient hypothesis testing (i.e., given a hypothesis h one should determine whether h approximates the target concept within a given approximation parameter). 2 Formal Setting A formal statement of Theorem 3 follows. <p> That is, given a hypothesis h one should determine, within time O ( log (1=ffi) * ), whether h approximates the target concept. Models in which the condition holds include learning with (membership and/or equivalence) queries [1], learning under a fixed distribution (e.g., <ref> [3] </ref>), learning with statistical queries [7], etc. Theorem 4 also holds with respect to proper (representation dependent) learning, provided that membership in the hypothesis class can be easily decided.
Reference: [4] <author> S. A. Cook. </author> <title> The Complexity of Theorem-Proving Procedures. </title> <booktitle> In 3rd STOC, </booktitle> <pages> pages 151-158, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction In his seminal paper on NP-completeness [9], in addition to proving (independently of Cook <ref> [4] </ref> and Karp [6]) the existence of NP-complete problems, Levin presented an optimal algorithm for solving any NP-complete problem.
Reference: [5] <author> A. Ehrenfeucht, D. Haussler, M. J. Kearns and L. G. Valiant. </author> <title> A General Lower Bound on the Number of Examples Needed for Learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference: [6] <author> R.M. Karp. </author> <title> Reducibility Among Combinatorial Problems. In Complexity of Computer Computations, </title> <editor> (Raymond E. Miller and James W. Thatcher, eds.), </editor> <publisher> Plenum Press, </publisher> <pages> pages 85-103, </pages> <year> 1972. </year>
Reference-contexts: 1 Introduction In his seminal paper on NP-completeness [9], in addition to proving (independently of Cook [4] and Karp <ref> [6] </ref>) the existence of NP-complete problems, Levin presented an optimal algorithm for solving any NP-complete problem.
Reference: [7] <author> M. J. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: On the other hand, the result holds also for many extensions of the PAC-learning model [12] such as learning with queries [1], learning under a fixed distribution (e.g., [3]), learning with statistical queries <ref> [7] </ref>, etc. In general, the result holds in any model which enables efficient hypothesis testing (i.e., given a hypothesis h one should determine whether h approximates the target concept within a given approximation parameter). 2 Formal Setting A formal statement of Theorem 3 follows. <p> That is, given a hypothesis h one should determine, within time O ( log (1=ffi) * ), whether h approximates the target concept. Models in which the condition holds include learning with (membership and/or equivalence) queries [1], learning under a fixed distribution (e.g., [3]), learning with statistical queries <ref> [7] </ref>, etc. Theorem 4 also holds with respect to proper (representation dependent) learning, provided that membership in the hypothesis class can be easily decided.
Reference: [8] <author> M. J. Kearns and U.V. Vazirani. </author> <title> An introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: introduced by Valiant in [12]. (Extensions will be discussed later.) Another two changes with respect to Theorem 3 are the introduction of the additive log (1=ffi)=* term (which can be eliminated in most cases- see below) and the specification of the (standard) parameters on which the time complexity depends (cf., <ref> [8] </ref>).
Reference: [9] <author> L. Levin. </author> <title> Universal'nye perebornye zadachi (Universal Search Problems: in Russian). </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> 9 (3), </volume> <pages> pages 265-266, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction In his seminal paper on NP-completeness <ref> [9] </ref>, in addition to proving (independently of Cook [4] and Karp [6]) the existence of NP-complete problems, Levin presented an optimal algorithm for solving any NP-complete problem. That is, Theorem 1 ([9] see Appendix for terminology): For every NP-relation, R, there exists an algorithm A, which solves R, and a polynomial
Reference: [10] <author> L. Levin. </author> <title> One-Way Function and Pseudorandom Generators. </title> <journal> Combinatorica, </journal> <volume> 7 (4), </volume> <pages> pages 357-363, </pages> <year> 1987. </year>
Reference-contexts: More than a decade later, the same underlying idea was used by Levin to prove the following: Proposition 2 <ref> [10] </ref>: There exists a polynomial-time computable function F which is one-way, unless no one-way functions exist. (F is explicitly given!) Here we employ the same idea to prove Theorem 3 (a universal learning algorithm informal version): There exists a universal learning algorithm U , so that for any concept class C,
Reference: [11] <author> N. Linial, Y. Mansour, and R. L. Rivest. </author> <title> Results on Learnability and the Vapnik-Chervonenkis Dimension. </title> <journal> Information and Computation, </journal> <volume> 90(1) </volume> <pages> 33-49, </pages> <year> 1991. </year>
Reference-contexts: Thus, we need to replace T in the above running-time bound by T + log (c 2 * . Theorem 4 follows. Remark: Enumerating all possible algorithms should not be confused with enumerating all possible hypotheses (e.g., as done explicitly in <ref> [11] </ref>). 2.3 Extensions As stated in the introduction, Theorem 4 applies also to many extensions and variants of the basic PAC-learning model provided that these extensions/variants allow efficient single-hypothesis-testing.
Reference: [12] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 5 </month>
Reference-contexts: We wish to stress that this result has no practical significance since the constant hidden in the O-notation is huge (see the proof). On the other hand, the result holds also for many extensions of the PAC-learning model <ref> [12] </ref> such as learning with queries [1], learning under a fixed distribution (e.g., [3]), learning with statistical queries [7], etc. <p> We first stress that by PAC-learning we mean the distribution-free model of learning from examples as introduced by Valiant in <ref> [12] </ref>. (Extensions will be discussed later.) Another two changes with respect to Theorem 3 are the introduction of the additive log (1=ffi)=* term (which can be eliminated in most cases- see below) and the specification of the (standard) parameters on which the time complexity depends (cf., [8]).
References-found: 12

