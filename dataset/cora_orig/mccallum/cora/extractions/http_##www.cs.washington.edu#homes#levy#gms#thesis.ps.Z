URL: http://www.cs.washington.edu/homes/levy/gms/thesis.ps.Z
Refering-URL: http://www.cs.washington.edu/homes/levy/gms/
Root-URL: 
Title: Global Memory Management for Workstation Networks  
Author: by Michael Joseph Feeley 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1996  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS Memo TM-454, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife <ref> [1] </ref>, Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each
Reference: [2] <author> American National Standards Institute, ANSI X3.139-1987. </author> <title> Fiber-Distributed Data Interface (FDDI) Token Ring Media Access Control (MAC), </title> <month> November </month> <year> 1987. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM [4, 32], FDDI <ref> [2] </ref>, and 100-Mb/s Ethernet [52]) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet [11] and Memory Channel [37]) supply throughput in the 1-Gb/s range. <p> Network Throughput Latency (Mb/s) (s) LANs 100-Base-T 100 120 FDDI 100 420 AN2-ATM 155 64 Fore-ATM 140 80 CANs Myrinet 250 - 1250 8 - 38 Memory Channel 800 5 - 20 FDDI Fiber Distributed Data Interface (FDDI) is a 100-Mb/s shared token-passing ring; dual counter-rotating rings provide reliability <ref> [2] </ref>. ATM Asynchronous-Transfer-Mode networks (ATM) have gained great popularity [4, 32]. Link bandwidth is typically between 100 and 155 Mb/s. Messages are divided into small fixed-size cells before injecting them into the network, thus simplifying switching, flow control, and bandwidth guarantee.
Reference: [3] <author> T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang. </author> <title> Serverless network file systems. </title> <journal> ATM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 41-79, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Franklin extended the Exodus [14] database to incorporate several features that address these concerns [33]. His ideas inspired a similar technique for distributed filesystems called cooperative caching. Cooperative caching was introduced as part of the xFS serverless filesystem <ref> [22, 3] </ref>. In xFS, a file access can be satisfied from any client cache. Each client cache is managed using 22 LRU, extended to provide a degree of global cooperation. <p> network memory that have been proposed by others N-Chance Forwarding and Hints and a third algorithm used for workstation-network load balancing, a similar distributed-decision problem Random Probe. 5.4.1 N-Chance Forwarding N-Chance Forwarding was proposed by Dahlin et al. as an algorithm for cooperative client caching in the xFS distributed filesystem <ref> [22, 3] </ref>; we discussed cooperative caching in Section 2.2.3. <p> The xFS filesystem uses this approach <ref> [3] </ref>. The advantage of this scheme over GMS is that it replaces individual PFD-update messages, needed when pages transition from replicated to unique, with messages that provide bulk information for groups of pages.
Reference: [4] <author> Thomas E. Anderson, Susan S. Owicki, James B. Saxe, and Charles P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM <ref> [4, 32] </ref>, FDDI [2], and 100-Mb/s Ethernet [52]) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet [11] and Memory Channel [37]) supply throughput in the 1-Gb/s range. <p> ATM Asynchronous-Transfer-Mode networks (ATM) have gained great popularity <ref> [4, 32] </ref>. Link bandwidth is typically between 100 and 155 Mb/s. Messages are divided into small fixed-size cells before injecting them into the network, thus simplifying switching, flow control, and bandwidth guarantee. <p> ATM Asynchronous-Transfer-Mode networks (ATM) have gained great popularity [4, 32]. Link bandwidth is typically between 100 and 155 Mb/s. Messages are divided into small fixed-size cells before injecting them into the network, thus simplifying switching, flow control, and bandwidth guarantee. Digital's AN2 network <ref> [4] </ref>, for example uses credit-based flow control to provide reliable in-order message delivery in hardware, permitting a substantial reduction in software protocol overheads. Survey of Cluster-Area Network Technology Table 2.2 also details the performance of several CAN technologies, which are described below. <p> In our current implementation, we assume that the network is reliable and we marshal and unmarshal to and from IP datagrams directly. This is justified primarily by the increased reliability of modern local area networks such as AN2 that have flow control to eliminate cell loss due to congestion <ref> [4] </ref>. To date, we have not noticed a dropped packet in any of our experiments.
Reference: [5] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D <ref> [5, 21] </ref>, KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a
Reference: [6] <author> O. Babaoglu and W. Joy. </author> <title> Converting a swap-based system to do paging in an architecture lacking page-reference bits. </title> <booktitle> In Proceedings of the 8th ACM Symposium and Operating Systems Principles, </booktitle> <pages> pages 78-86, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: When a node has no idle pages, faulted pages are allocated on remote nodes that do have idle pages. On the BBN Butterfly, on which their study was conducted, each node runs a version of the LRU Clock algorithm <ref> [6] </ref> to identify the idle pages on that node; a similar algorithm is used by most workstation systems such as Digital Unix. <p> Before describing our algorithm, we review its basis, the standard 91 Clock-like algorithm found in most systems, including Digital Unix. Standard Two-Handed Clock and FIFO-With-Second-Chance Two-Handed Clock <ref> [6, 79] </ref> maintains a single bit of information for each page, the referenced bit, set by the system when a page is accessed. Physical pages are logically organized into a circular list that is swept by two hands as replacement victims are needed.
Reference: [7] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirrif, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Both of these goals were also explored by Franklin et al. in their work on client-server object databases [33]. In [22], Dahlin et al. studied several cooperative-cache management algorithms using the simulations of the Sprite filesystem traces <ref> [7] </ref>. Their simulations showed that N-Chance Forwarding performed best of the alternatives they studied. Algorithm Description Each client uses LRU to independently manage its local cache. When a client replaces a local victim from its cache, it checks whether the victim is unique or replicated. <p> In the Sprite workloads used to evaluate N-Chance, between 66% and 90% of the nodes were idle in each 10-minute period <ref> [7] </ref>. As a result, the simulations showed that N-Chance's performance was nearly optimal. We shall see in a moment that Sarkar and Hartman used these same traces to evaluate their Hints algorithm. Both studies lack any analysis of how their algorithms would perform on workloads with different characteristics.
Reference: [8] <author> L.A. Belady. </author> <title> A study of replacement algorithms for virtual-storage computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year> <month> 150 </month>
Reference-contexts: M s if d s 2 M s M s v s + d s otherwise 3.1.2 Optimal Offline Algorithm for Local Memory Management Belady showed that the optimal replacement strategy is to replace the resident page that will not be not accessed for the longest time in the future <ref> [8] </ref>.
Reference: [9] <author> Brian N. Bershad, Mathew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of the 93 COMPCON Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: We analyze both of these algorithms in detail, and compare them to our algorithm for global memory management, in Sections 5.4.1, 5.4.2, and 7.5. Distributed Shared Memory Distributed shared virtual memory (DSM), an active research area for several years <ref> [50, 15, 42, 9] </ref>, is a software-implemented shared-memory abstraction for the distributed memories of a collection of workstations (or a distributed-memory multiprocessor). This abstraction permits a workstation network to be programmed as if it were a shared-memory multiprocessor but with certain performance and functionality limitations.
Reference: [10] <author> D. P. Bertsekas and D. A. Castinon. </author> <title> Auction algorithm for transportation problem. </title> <journal> Annals of Operations Research, </journal> <pages> pages 67-96, </pages> <year> 1989. </year>
Reference-contexts: Formulated in this way, memory management can be regarded as a File Allocation Problem [25, 81], which Leff et al. show can be transformed to an equivalent transportation problem [36] and thus efficiently solved <ref> [10] </ref>. While useful from a theoretical point of view, the assumption of static access probabilities is highly 32 unsatisfactory for understanding memory management in real systems. The fact that access patterns are not static makes replacement necessary and makes the problem of finding an optimal solution more complex.
Reference: [11] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L Seitz, J. N. Seizovic, and Wen King Su. Myrinet: </author> <title> a gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM [4, 32], FDDI [2], and 100-Mb/s Ethernet [52]) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet <ref> [11] </ref> and Memory Channel [37]) supply throughput in the 1-Gb/s range. <p> Myrinet Myrinet is limited to 16 nodes connected by either 8-m or 25-m cables. Theoretical peek bandwidth is 1.28 Gb/s for 8-m cables, 550 Mb/s for 25-m cables. Small-message latency is 38 s for standard interfaces and as low as around 8 s for customized interfaces <ref> [11] </ref>. Memory Channel Memory Channel (MC) [37] connects a maximum of 10 to 20 nodes, with 800 Mb/s bandwidth and a raw 5 s hardware latency (20 s total latency using a minimal-overhead software messaging protocol).
Reference: [12] <author> Pei Cao, E. W. Felten, and Kai Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 14-17, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Another approach is to allow applications to explicitly specify which files are sequentially accessed or which file pages are no longer needed (e.g., the madvise () Unix call provides these capabilities). LRU-SP <ref> [12] </ref> is another approach, which uses LRU as the system-wide allocation policy and allows applications to make replacement decisions about their own pages.
Reference: [13] <author> M. J. Carey, D. J. Dewitt, and J. F. Naughton. </author> <title> The OO7 benchmark. </title> <booktitle> In Proc. of the ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: By far, the most time is spent in file I/O for compiler and linker access to temporary, source, and object files. OO7 is an object-oriented database benchmark that builds a parts-assembly database in virtual memory and then performs several traversals of this database <ref> [13] </ref>. The benchmark is designed to synthesize the characteristics of MCAD design data and has been used to evaluate the performance of commercial and research object databases. Render is a graphics rendering program that displays a computer-generated scene from a pre-computed 178-Mbyte database [16].
Reference: [14] <author> Michael J. Carey, David J. DeWitt, Daniel Frank, Goetz Graefe, M. Muralikrishna, Joel E. Richardson, and Eugene J. Shekita. </author> <title> The architecture of the EXODUS extensible DBMS. </title> <booktitle> In Proceedings of the 12th VLDB Conference, </booktitle> <pages> pages 52-65, </pages> <year> 1986. </year>
Reference-contexts: By forwarding the page, the system permits a subsequent access to read the page from memory instead of from disk. Franklin extended the Exodus <ref> [14] </ref> database to incorporate several features that address these concerns [33]. His ideas inspired a similar technique for distributed filesystems called cooperative caching. Cooperative caching was introduced as part of the xFS serverless filesystem [22, 3]. In xFS, a file access can be satisfied from any client cache.
Reference: [15] <author> John B. Carter, John K. Bennet, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We analyze both of these algorithms in detail, and compare them to our algorithm for global memory management, in Sections 5.4.1, 5.4.2, and 7.5. Distributed Shared Memory Distributed shared virtual memory (DSM), an active research area for several years <ref> [50, 15, 42, 9] </ref>, is a software-implemented shared-memory abstraction for the distributed memories of a collection of workstations (or a distributed-memory multiprocessor). This abstraction permits a workstation network to be programmed as if it were a shared-memory multiprocessor but with certain performance and functionality limitations.
Reference: [16] <author> Bradford Chamberlain, Tony DeRose, Dani Lischinski, David Salesin, and John Snyder. </author> <title> Fast rendering of complex environments using a spatial hierarchy. </title> <booktitle> In Proc. of Graphics Interface `96, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: The benchmark is designed to synthesize the characteristics of MCAD design data and has been used to evaluate the performance of commercial and research object databases. Render is a graphics rendering program that displays a computer-generated scene from a pre-computed 178-Mbyte database <ref> [16] </ref>. In our experiment, we measured the elapsed time for a sequence of operations that move the viewpoint progressively closer to the scene without changing the viewpoint angle. <p> Atom [69] is the instrumentation software we used to generate the traces. To obtain this trace, we instrumented Atom itself (using Atom), then traced it while processing the gzip binary. The trace includes 73 million memory accesses and from 1175 to 5275 page faults. Render <ref> [16] </ref> is a graphics rendering program that displays a computer-generated scene from a large (over 100MB) precomputed database. The trace includes 245 million memory references and from 1433 to 6145 page faults. gdb is the GNU debugger. We traced the initialization phase of the debugger, run without loading a program.
Reference: [17] <author> Albert Chang and Mark F. </author> <title> Merge. 801 storage: Architecture and programming. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Similar techniques have been used for trace production as well [78]. 2 The IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line <ref> [17] </ref>. 126 Table 8.2: Page-fault latencies for Eager-Fullpage Fetch from remote memory. Latencies are arrival times of subpage and rest of page. Improvement potential is given as a percentage of fullpage latency.
Reference: [18] <author> Douglas W. Clark, Butler W. Lampson, and Kenneth A. Pier. </author> <title> The memory system of a high-performance personal computer. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(10), </volume> <month> October </month> <year> 1981. </year> <month> 151 </month>
Reference-contexts: As we will 123 see, the first subpages to arrive following arrival of the faulted subpage take relatively little additional latency. This scheme is similar in some ways to the pipelining of cache data into cache lines over small buses <ref> [18] </ref>. It is interesting to compare our subpage schemes with an architecture that uses small pages (i.e., the size of one of our subpages).
Reference: [19] <author> Douglas Comer and James Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This system allowed local users to statically restrict the amount of physical memory available to the paging server. Comer and Griffioen described a remote memory model in which the cluster contains workstations, disk servers, and remote memory servers <ref> [19] </ref>. The remote memory servers were dedicated machines whose large primary memories could be allocated by workstations with heavy paging activity. No client-to-client resource sharing occurred, except through the servers. Felten and Zahorjan generalized this idea to use memory on idle client machines as paging backing store [31].
Reference: [20] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: When data is shared among processors, however, migration and replication strategies are needed in order to achieve a good placement for all processors. Migration is an effective strategy for data that is shared sequentially among processors. In Platinum <ref> [20] </ref>, for example, remote pages are moved into local memory when they are accessed.
Reference: [21] <author> Cray Research Incorporated. </author> <title> The CRAY T3D Hardware Reference Manual, </title> <year> 1993. </year>
Reference-contexts: For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D <ref> [5, 21] </ref>, KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a
Reference: [22] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and David A. Pat-terson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Franklin extended the Exodus [14] database to incorporate several features that address these concerns [33]. His ideas inspired a similar technique for distributed filesystems called cooperative caching. Cooperative caching was introduced as part of the xFS serverless filesystem <ref> [22, 3] </ref>. In xFS, a file access can be satisfied from any client cache. Each client cache is managed using 22 LRU, extended to provide a degree of global cooperation. <p> network memory that have been proposed by others N-Chance Forwarding and Hints and a third algorithm used for workstation-network load balancing, a similar distributed-decision problem Random Probe. 5.4.1 N-Chance Forwarding N-Chance Forwarding was proposed by Dahlin et al. as an algorithm for cooperative client caching in the xFS distributed filesystem <ref> [22, 3] </ref>; we discussed cooperative caching in Section 2.2.3. <p> Both of these goals were also explored by Franklin et al. in their work on client-server object databases [33]. In <ref> [22] </ref>, Dahlin et al. studied several cooperative-cache management algorithms using the simulations of the Sprite filesystem traces [7]. Their simulations showed that N-Chance Forwarding performed best of the alternatives they studied. Algorithm Description Each client uses LRU to independently manage its local cache.
Reference: [23] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and David A. Pat-terson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: forwarding, the best algorithm defined by Dahlin et al.<ref> [23] </ref>. It is interesting to compare with N-chance, because it differs from our algorithm in significant ways, as described in Section 5.4.1. To the best of our knowledge, there is no existing implementation of the N-chance algorithm described by Dahlin et al. [23]. Consequently, we implemented N-chance in Digital Unix. We made a few minor modifications to the original algorithm. Our implementation is as follows. Unique pages are forwarded by nodes with an initial recirculation count of N = 2.
Reference: [24] <author> P. J. Denning. </author> <title> The Working Set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: If accessed rapidly enough, all of these pages will be newer than pages belonging to other programs. Such a strategy can result in a program growing to consume all of the memory of a node. Working Set LRU (WS) <ref> [24, 77] </ref> is an approach used by some virtual memory systems that provides a framework for solving these three problems.
Reference: [25] <author> L. Dowdey and D. Foster. </author> <title> Comparative models of the file assignment problem. </title> <journal> Computing Surveys, </journal> <volume> 14(2) </volume> <pages> 287-313, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: This assignment of data to memories does not change over time because the access probabilities are static; as a result, replacement is not needed. Formulated in this way, memory management can be regarded as a File Allocation Problem <ref> [25, 81] </ref>, which Leff et al. show can be transformed to an equivalent transportation problem [36] and thus efficiently solved [10]. While useful from a theoretical point of view, the assumption of static access probabilities is highly 32 unsatisfactory for understanding memory management in real systems.
Reference: [26] <author> R. P. Draves. </author> <title> Page replacement and reference bit emulation in Mach. </title> <booktitle> In Proceedings of the USENIX Mach Symposium, </booktitle> <pages> pages 201-212, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: That is, any page that is not referenced between the time the first and second hands pass over it is deemed to be old and is therefore replaced. FIFO-with-Second-Chance <ref> [77, 26] </ref> is a common variant of Two-Handed Clock, implemented by most systems including Digital Unix. The system maintains three FIFO queues of pages: active, inactive, and free. Only active pages are directly accessible to applications.
Reference: [27] <author> S. Haridi E. Hagersten, A. Landin. </author> <title> Ddm a cache-only memory architecture. </title> <journal> Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <year> 1992. </year>
Reference-contexts: Each processor is directly connected to a local memory and by a slower network to all of the other memories in the system, the remote memories. Cache-only systems (COMA) are a type of NUMA that implement key memory management operations in hardware <ref> [43, 27] </ref>. COMA hardware is unique in two key ways. First, the unit of data movement is a cache line instead of a page. These cache lines are typically much smaller than a page (the KSR, for example, has 128-byte cache lines and 16-Kbyte pages).
Reference: [28] <author> Derek L. Eager, Edward D. Lazowska, and John Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-12(5), </volume> <month> May </month> <year> 1986. </year> <month> 152 </month>
Reference-contexts: Algorithm Description Eager et al. have shown that a type-two algorithm (from Section 5.1.3) called Random Probe provides the best solution for workstation load balancing <ref> [28] </ref>. Using their algorithm, each time a process is created, the system decides whether to create it locally or on a lightly-loaded remote node. To make this decision, a probe message is sent to a small number of randomly chosen nodes. Each node responds with information about its current load.
Reference: [29] <author> Michael J. Feeley, William E. Morgan, Frederic H. Pighin, Anna R. Karlin, Henry M. Levy, and Chandramohan A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In addition, nodes cannot directly access the local memory of a remote node. To read a remote memory location, for example, requires interrupting the remote CPU to process the read request, adding as much as 150 s overhead (on a 266-Mhz DEC Alpha running Digital Unix <ref> [29] </ref>). 2.1.3 The Memory Hierarchy All of these systems, multiprocessors as well as workstation networks, implement a memory hierarchy. At the lowest level of each hierarchy is a processor cache or local memory and at the highest level is remote memory or disk. <p> Chapter 4 THE GLOBAL MEMORY MANAGEMENT ALGORITHM From each according to his abilities, to each according to his needs. Karl Marx, 1875 With this chapter we begin our description of the Global Memory System (GMS) we have developed <ref> [29, 40] </ref>. This chapter describes our global memory management algorithm, which handles page faults using a combined placement-replacement policy that globally manages workstation-network memory. Chapter 5 describes how we maintain the global information used to make policy decisions and includes a comparison with other approaches.
Reference: [30] <author> Edward W. Felten, Richard D. Alpert, Angelos Bilas, Matthias A. Blumrich, Dou-glas W. Clark, Stefanos N. Damianakis, Cezary Dubnick, Liviu Iftode, and Kai Li. </author> <title> Early experience with message-passing on the Shrimp multicomputer. </title> <booktitle> In Proc. of the 23rd International Symposium of Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP <ref> [30] </ref>. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as shown in Figure 2.2. <p> Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers [80, 75], and newer controllers reduce latency even further <ref> [30] </ref>. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead. For remote memory access, then, smaller transfers may ultimately be needed to reduce latency and improve program performance.
Reference: [31] <author> Edward W. Felten and John Zahorjan. </author> <title> Issues in the implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: The remote memory servers were dedicated machines whose large primary memories could be allocated by workstations with heavy paging activity. No client-to-client resource sharing occurred, except through the servers. Felten and Zahorjan generalized this idea to use memory on idle client machines as paging backing store <ref> [31] </ref>. When a machine becomes idle, its kernel activates an otherwise dormant memory server, which registers itself for remote use. Whenever a kernel replaces a VM page, it queries a central registry to locate active memory servers, picking one at random to receive the replacement victim.
Reference: [32] <author> FORE Systems, </author> <title> 1000 Gamma Drive, Pittsburgh PA 15238. TCA-100 TURBOchannel ATM Computer Interface, User's Manual, </title> <year> 1992. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM <ref> [4, 32] </ref>, FDDI [2], and 100-Mb/s Ethernet [52]) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet [11] and Memory Channel [37]) supply throughput in the 1-Gb/s range. <p> ATM Asynchronous-Transfer-Mode networks (ATM) have gained great popularity <ref> [4, 32] </ref>. Link bandwidth is typically between 100 and 155 Mb/s. Messages are divided into small fixed-size cells before injecting them into the network, thus simplifying switching, flow control, and bandwidth guarantee.
Reference: [33] <author> Michael J. Franklin, Michael J. Carey, and Miron Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Subsequent access from that client can be satisfied from either client or server memory; access from another client can be satisfied from server memory. Distributed-filesystem blocks are typically stored in the standard file-buffer cache of client and server machines, whose management is entirely local. Franklin <ref> [33] </ref> points out three key weaknesses of this local-management approach to client caching in the context of distributed object databases, which use the same style of client-server caching. 1. <p> By forwarding the page, the system permits a subsequent access to read the page from memory instead of from disk. Franklin extended the Exodus [14] database to incorporate several features that address these concerns <ref> [33] </ref>. His ideas inspired a similar technique for distributed filesystems called cooperative caching. Cooperative caching was introduced as part of the xFS serverless filesystem [22, 3]. In xFS, a file access can be satisfied from any client cache. <p> Both of these goals were also explored by Franklin et al. in their work on client-server object databases <ref> [33] </ref>. In [22], Dahlin et al. studied several cooperative-cache management algorithms using the simulations of the Sprite filesystem traces [7]. Their simulations showed that N-Chance Forwarding performed best of the alternatives they studied. Algorithm Description Each client uses LRU to independently manage its local cache.
Reference: [34] <author> K. W. Froese and R. B. Bunt. </author> <title> The effect of client caching on file server workloads. </title> <booktitle> In Proceedings of the Twenty-Nineth Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Most virtual memory systems actually use an approximation of LRU, as we describe in Section 6.3.4. Less common are frequency-based schemes such 17 as LFU (Least Frequently Used), which are less popular because access frequency is more difficult to collect and interpret than access time <ref> [55, 82, 34] </ref>.
Reference: [35] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <booktitle> In Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences Vol. </booktitle> <volume> I, </volume> <pages> pages 134-143, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Table 2.1 shows the interconnection-network performance of several multiprocessor systems. For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL <ref> [35, 65] </ref>, Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing
Reference: [36] <author> R. S. Garfinkel and G. L. Nemhauser. </author> <title> Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Formulated in this way, memory management can be regarded as a File Allocation Problem [25, 81], which Leff et al. show can be transformed to an equivalent transportation problem <ref> [36] </ref> and thus efficiently solved [10]. While useful from a theoretical point of view, the assumption of static access probabilities is highly 32 unsatisfactory for understanding memory management in real systems.
Reference: [37] <author> R. B. Gillett. </author> <title> Memory channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM [4, 32], FDDI [2], and 100-Mb/s Ethernet [52]) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet [11] and Memory Channel <ref> [37] </ref>) supply throughput in the 1-Gb/s range. <p> Theoretical peek bandwidth is 1.28 Gb/s for 8-m cables, 550 Mb/s for 25-m cables. Small-message latency is 38 s for standard interfaces and as low as around 8 s for customized interfaces [11]. Memory Channel Memory Channel (MC) <ref> [37] </ref> connects a maximum of 10 to 20 nodes, with 800 Mb/s bandwidth and a raw 5 s hardware latency (20 s total latency using a minimal-overhead software messaging protocol).
Reference: [38] <author> Samuel P. Harbison. </author> <title> Modula-3. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NY, </address> <year> 1992. </year> <month> 153 </month>
Reference-contexts: We traced and analyzed several applications executing in the simulated subpage environment. To see the effect of heavy paging activity, we ran the applications in different memory configurations. The applications we used were the following: Modula-3 is the DEC SRC compiler for the Modula-3 <ref> [38] </ref> programming language. We traced a Modula-3 compilation of smalldb, a library that transparently maintains a copy of in-memory data structures on secondary storage. The trace includes 87 million memory references, and from 773 to 5655 faults, depending on the memory configuration. ld is the unix object file linker.
Reference: [39] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon <ref> [39, 71] </ref>, and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as shown in Figure 2.2.
Reference: [40] <author> H. A. Jamrozik, M. J. Feeley, G. M. Voelker, J. Evans III, A. R. Karlin, H. M. Levy, and M. K. Vernon. </author> <title> Reducing network latency using subpages in a global memory environment. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1996. </year>
Reference-contexts: Chapter 4 THE GLOBAL MEMORY MANAGEMENT ALGORITHM From each according to his abilities, to each according to his needs. Karl Marx, 1875 With this chapter we begin our description of the Global Memory System (GMS) we have developed <ref> [29, 40] </ref>. This chapter describes our global memory management algorithm, which handles page faults using a combined placement-replacement policy that globally manages workstation-network memory. Chapter 5 describes how we maintain the global information used to make policy decisions and includes a comparison with other approaches.
Reference: [41] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Page placement policies for NUMA multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(2) </volume> <pages> 112-129, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: A final issue of particular importance for workstation global memory management is the interaction between replacement and placement on NUMA systems. On a NUMA, replacement is typically handled locally; each node runs an independent replacement algorithm that handles local faults by replacing local pages. LaRowe and Ellis <ref> [41] </ref> proposed several alternative algorithms that improve on local replacement by allowing faults to use remote memory when programs need more memory than is locally available. <p> LaRowe and Ellis proposed a solution for NUMA multiprocessors in which the Clock algorithms running on every node are synchronized; a fault on any node advances the clock on every node <ref> [41] </ref>. We take a similar approach: instead of using a globally synchronized algorithm, however, each node advances its clock independently and clock hands advance even when nodes are not faulting.
Reference: [42] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. </author> <title> Tread-Marks: Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: We analyze both of these algorithms in detail, and compare them to our algorithm for global memory management, in Sections 5.4.1, 5.4.2, and 7.5. Distributed Shared Memory Distributed shared virtual memory (DSM), an active research area for several years <ref> [50, 15, 42, 9] </ref>, is a software-implemented shared-memory abstraction for the distributed memories of a collection of workstations (or a distributed-memory multiprocessor). This abstraction permits a workstation network to be programmed as if it were a shared-memory multiprocessor but with certain performance and functionality limitations.
Reference: [43] <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <institution> Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Each processor is directly connected to a local memory and by a slower network to all of the other memories in the system, the remote memories. Cache-only systems (COMA) are a type of NUMA that implement key memory management operations in hardware <ref> [43, 27] </ref>. COMA hardware is unique in two key ways. First, the unit of data movement is a cache line instead of a page. These cache lines are typically much smaller than a page (the KSR, for example, has 128-byte cache lines and 16-Kbyte pages). <p> The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR <ref> [43, 71] </ref>, CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as
Reference: [44] <author> Nancy P. Kronenberg, Henry M. Levy, and William D. Strecker. VAXclusters: </author> <title> A closely-coupled distributed system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Figure 6.2 shows three nodes involved in the lookup. 84 When a faulted page is not shared, nodes A and B are identical, thereby decreasing the number of network messages needed. This distribution of the page management information is somewhat similar to the handling of distributed locks in VAXclusters <ref> [44] </ref>. As a side-effect of a successful lookup, data structures on each of the three nodes are updated to reflect the changed state of the system. The global-cache-directory is updated with the new location of the page and the page-frame-directories on the requester and the remote node are updated suitably. <p> It is straightforward to extend our implementation to deal with master node failure through an election process to select a new master, as is done in other systems <ref> [44, 62] </ref>. Node deletions are straightforward as well. The master node checks periodically for the liveness of the other nodes. When it detects a crashed node, it redistributes the page-ownership-directory. As with addition, global-cache-directories are also redistributed at that time. 6.1.7 Inter-node Communication Between nodes we use simple non-blocking communication.
Reference: [45] <author> Edward D. Lazowska, John Zahorjan, David R. Cheriton, and Willy Zwaenepoel. </author> <title> File access performance of diskless workstations. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 4(3), </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: A major disadvantage of the small page scheme, relative to subpages, is the reduced TLB coverage and therefore higher TLB miss rate that small pages would incur. Furthermore, previous work <ref> [45] </ref> has shown that although smaller transfers offer the potential for increased locality, this advantage is outweighed by the increased overhead of the multiple requests required. We performed experiments to confirm that this is true for our environment as well.
Reference: [46] <author> Paul J. Leach, Paul H. Levine, Bryan P. Douros, James A. Hamilton, David L. Nelson, and Bernard L. Stumpf. </author> <title> The architecture of an integrated local network. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 1(5) </volume> <pages> 842-857, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Remote-Memory Paging for Virtual Memory Remote paging uses remote memory as backing store, instead of disk, in an otherwise standard virtual-memory paging system. Leach et al. were the first to describe remote paging in the context of the Apollo DOMAIN System <ref> [46] </ref>. Each machine in the network has a paging server that accepts paging requests from remote nodes. This system allowed local users to statically restrict the amount of physical memory available to the paging server.
Reference: [47] <author> Avraham Leff, Joel L. Wolf, and Philip S. Yu. </author> <title> Replication algorithms in a remote caching architecture. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(11), </volume> <month> November </month> <year> 1993. </year> <month> 154 </month>
Reference-contexts: One approach to optimality has been taken by Leff et al. <ref> [48, 47] </ref>. They assume a static access probability for every data page and thereby reduce the problem to that of finding an optimal initial placement of these data pages in workstation memories.
Reference: [48] <author> Avraham Leff, Joel L. Wolf, and Philip S. Yu. </author> <title> Efficient LRU-based buffering in a LAN remote caching architecture. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(2), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: One approach to optimality has been taken by Leff et al. <ref> [48, 47] </ref>. They assume a static access probability for every data page and thereby reduce the problem to that of finding an optimal initial placement of these data pages in workstation memories.
Reference: [49] <author> Daniel Lenoski, James Loudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessey. </author> <title> The DASH prototype: implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH <ref> [49, 66] </ref>, MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local
Reference: [50] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We analyze both of these algorithms in detail, and compare them to our algorithm for global memory management, in Sections 5.4.1, 5.4.2, and 7.5. Distributed Shared Memory Distributed shared virtual memory (DSM), an active research area for several years <ref> [50, 15, 42, 9] </ref>, is a software-implemented shared-memory abstraction for the distributed memories of a collection of workstations (or a distributed-memory multiprocessor). This abstraction permits a workstation network to be programmed as if it were a shared-memory multiprocessor but with certain performance and functionality limitations. <p> Doing so requires an extra message from GCD to PFD when a page transitions from replicated to unique; all other PFD state changes are handled as a by-product of other GMS operations and thus require no additional messages. Li and Hudak <ref> [50] </ref> studied a similar problem for distributed shared memory systems, in which replication information is needed by the system's coherence protocol. That problem differs from ours, however, because coherence protocols are naturally distributed, and thus having local access to replication information was not an issue. <p> A global directory such as the GCD has the potential disadvantage that finding a sharable page requires three network messages, instead of the two needed when location information is stored locally. In addition, directory-update messages are required when pages change locations. A problem similar is studied in <ref> [50] </ref> for distributed shared memory systems, in which pages are located either using a centralized directory, by broadcasting location messages to every node in the network, or by following forwarding pointers to locate the node storing a page's location information.
Reference: [51] <author> E. P. Markatos and G. Dramitinos. </author> <title> Implementation of a reliable remote memory pager. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Whenever a kernel replaces a VM page, it queries a central registry to locate active memory servers, picking one at random to receive the replacement victim. Felten and Zahorjan used a simple queueing model to predict performance. Recently, Markatos and Dramitinos built a remote paging system for Digital Unix <ref> [51] </ref>. In a different environment, Schilit and Duchamp have used remote paging to enhance the performance of mobile computers [62].
Reference: [52] <author> R.M. Metcalfe and D.R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: Memory management is one operating system function that is greatly impacted by this new technology. Modern local-area networks (e.g., ATM [4, 32], FDDI [2], and 100-Mb/s Ethernet <ref> [52] </ref>) supply more than ten times the throughput of earlier networks and cluster networks (e.g., Myrinet [11] and Memory Channel [37]) supply throughput in the 1-Gb/s range. <p> Third, workstation operating systems function independently on each node and manage all system resources locally; in contrast, multiprocessor systems typically manage some resources globally (e.g., processor 12 scheduling) and other resources locally (e.g., memory management). Until recently, workstation networks were built, almost exclusively, using Ethernet <ref> [52] </ref>. Traditionally Ethernet is a shared network with link bandwidth of 10-Mb/s. Modern networks have improved on this older technology in two ways: they provide at least a one order-of-magnitude increase in link throughput and they are usually switched instead of shared.
Reference: [53] <author> Michael N. Nelson. </author> <title> Virtual memory vs. the file system. </title> <type> Technical Report 90/4, </type> <institution> Digital Western Research Laboratory, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The Sprite system explored dynamic allocation policies that grow and shrink the file cache with changing memory needs <ref> [54, 53] </ref>. This research concluded that VM pages are more valuable than file pages and thus they heavily favor allocating memory to the virtual memory system. The file cache only grows when there is excess memory not needed by VM and it shrinks rapidly when VM paging rates increase.
Reference: [54] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The Sprite system explored dynamic allocation policies that grow and shrink the file cache with changing memory needs <ref> [54, 53] </ref>. This research concluded that VM pages are more valuable than file pages and thus they heavily favor allocating memory to the virtual memory system. The file cache only grows when there is excess memory not needed by VM and it shrinks rapidly when VM paging rates increase.
Reference: [55] <author> B. Reed and D. D. E. </author> <title> Long. Analysis of caching algorithms for distributed file systems. </title> <journal> Operating Systems Review, </journal> <volume> 30(3) </volume> <pages> 12-21, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Most virtual memory systems actually use an approximation of LRU, as we describe in Section 6.3.4. Less common are frequency-based schemes such 17 as LFU (Least Frequently Used), which are less popular because access frequency is more difficult to collect and interpret than access time <ref> [55, 82, 34] </ref>.
Reference: [56] <author> S. Reinhardt, M. Hill, J. Larus, A. Lebeck, J. Lewis, and D.Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proc. of the ACM Sigmetrics Conf. on Measurement and Modelling of Computer Systems, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This simply consists of an additional valid bit in the TLB per subpage. 2 1 An alternative technique would be that used by the Wisconsin Wind Tunnel project <ref> [57, 56] </ref> to implement additional state bits for SVM on the CM5; they used ECC bits to cause faults, however, this would still require emulating writes, and the Alpha 250 has imprecise exceptions on data parity errors, making use of parity difficult or impossible.
Reference: [57] <author> Steve K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proc. of the 2nd USENIX Symp. on Micorkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year> <month> 155 </month>
Reference-contexts: This simply consists of an additional valid bit in the TLB per subpage. 2 1 An alternative technique would be that used by the Wisconsin Wind Tunnel project <ref> [57, 56] </ref> to implement additional state bits for SVM on the CM5; they used ECC bits to cause faults, however, this would still require emulating writes, and the Alpha 250 has imprecise exceptions on data parity errors, making use of parity difficult or impossible.
Reference: [58] <author> Ted Romer, Wayne Ohlrich, Anna Karlin, and Brian Bershad. </author> <title> Reducing TLB and memory overhead using online superpage promotion. </title> <booktitle> In Proc. of the 22nd Annual Int. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [73, 72, 58] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [59] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Proceedings of the Summer Usenix Conference, </booktitle> <year> 1985. </year>
Reference-contexts: The virtual memory system manages program data such as executables, stacks, and heaps, storing active portions in memory and inactive portions on disk, if necessary. The filesystem manages a cache of recently accessed file blocks from both local and remote files (e.g., NFS <ref> [59] </ref> or AFS [61]). Workstation memory management involves two main policy issues. First, both the VM system and the file-buffer cache implement a replacement policy. The polices for each subsystem are typically independent; a fault in one subsystem thus replaces a page (or block) managed by that subsystem. <p> Their goal is to permit small memory-starved portable computers to page to the memories of larger servers nearby; pages could migrate from server to server as the portables migrate. 21 Cooperative Caching for Distributed Filesystems A distributed filesystem (e.g., NFS <ref> [59] </ref> and AFS [61]) consists of a server node and multiple client nodes. Servers export files from their local disks and programs on client machines access these remote files as if they were local files.
Reference: [60] <author> P. Sarkar and J. Hartman. </author> <title> Efficient cooperative caching using hints. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The hope is that a series of random forwarding choices results in the page arriving at an idle node, if there is one. An alternative policy, recently proposed by Sarkar and Hartman, uses hints to direct forwarded pages to nodes most likely to have idle memory <ref> [60] </ref>. Hints are collected when a page is forwarded from one node to another; each node informs the other of the age of its oldest page by piggy-backing this information with the messages needed to forward the page. <p> For such a node, it is unlikely that the forwarded page would be young enough to replace any local replicated page, a key source of the overhead in N-Chance. 5.4.2 Cooperative Caching Using Hints Sarkar and Hartman have recently proposed a new algorithm for managing cooperative filesystem caches <ref> [60] </ref>. This algorithm seeks to improve on N-Chance by using some global information to make forwarding decisions, while retaining N-Chance's simplicity and low-cost decision making. Sarkar and Hartman use simulations of the same Sprite workloads used in the N-Chance studies to compare their algorithm to N-Chance and GMS. <p> If this feature holds for other workloads as well, Hints appears to have the additional benefit of simplicity and scalability. Recall that the key impediment to scalability in GMS is the centralized portion of epoch computation. The key question is: do the simulation results presented in <ref> [60] </ref> generalize to other workloads. It is well known, however, that Unix filesystem accesses are idiosyncratic and not representative of VM or database workloads. Note first that the Hints algorithm assumes a reply message for each forwarded page. <p> Sarkar and Hartman proposed this approach for filesystem cooperative-cache management <ref> [60] </ref>. The master copy is always forwarded to global memory (or to a remote cooperative cache) and slave copies are never forwarded. This approach has the advantage that it requires no additional messages to maintain replication information. <p> The key issues are to decide what to cache and how to update caches when pages move. Sarkar and Hartman <ref> [60] </ref> suggest a hint-based variant for filesystem cooperative caching. When a node opens a file it receives a set of hints for blocks of the file listing nodes that might be caching them. These hints are retrieved from the last node to have opened the file.
Reference: [61] <author> M. Satyanarayanan. </author> <title> Scalable, secure, and highly available distributed file access. </title> <journal> IEEE Computer, </journal> <volume> 23(5), </volume> <month> May </month> <year> 1990. </year>
Reference-contexts: The virtual memory system manages program data such as executables, stacks, and heaps, storing active portions in memory and inactive portions on disk, if necessary. The filesystem manages a cache of recently accessed file blocks from both local and remote files (e.g., NFS [59] or AFS <ref> [61] </ref>). Workstation memory management involves two main policy issues. First, both the VM system and the file-buffer cache implement a replacement policy. The polices for each subsystem are typically independent; a fault in one subsystem thus replaces a page (or block) managed by that subsystem. <p> Their goal is to permit small memory-starved portable computers to page to the memories of larger servers nearby; pages could migrate from server to server as the portables migrate. 21 Cooperative Caching for Distributed Filesystems A distributed filesystem (e.g., NFS [59] and AFS <ref> [61] </ref>) consists of a server node and multiple client nodes. Servers export files from their local disks and programs on client machines access these remote files as if they were local files.
Reference: [62] <author> Bill N. Schilit and Dan Duchamp. </author> <title> Adaptive remote paging. </title> <type> Technical Report CUCS-004091, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Felten and Zahorjan used a simple queueing model to predict performance. Recently, Markatos and Dramitinos built a remote paging system for Digital Unix [51]. In a different environment, Schilit and Duchamp have used remote paging to enhance the performance of mobile computers <ref> [62] </ref>. <p> It is straightforward to extend our implementation to deal with master node failure through an election process to select a new master, as is done in other systems <ref> [44, 62] </ref>. Node deletions are straightforward as well. The master node checks periodically for the liveness of the other nodes. When it detects a crashed node, it redistributes the page-ownership-directory. As with addition, global-cache-directories are also redistributed at that time. 6.1.7 Inter-node Communication Between nodes we use simple non-blocking communication.
Reference: [63] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Without mutual trust, the solution is to encrypt the data on its way to or from global memory. This could be done most easily at the network hardware level <ref> [63] </ref>. Our current algorithm is essentially a modified global LRU replacement scheme. As discussed in Section 2.2.1, it is well known that in some cases, such as sequential file access, LRU may not be the best choice.
Reference: [64] <author> A. Silberschatz. </author> <title> Operating Systems Concepts. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, Mass., </address> <year> 1991. </year>
Reference-contexts: In this dissertation, we are primarily concerned with the core data-movement aspect of memory management, which can be summarized by its three constituent subpolicies <ref> [74, 64] </ref>. Three Key Memory Management Subpolicies Fetch The fetch subpolicy determines how and when the system fetches data from a distant memory to a closer memory.
Reference: [65] <institution> Silicon Graphics Inc., 2001 N. Shoreline Blvd, Mountain View, CA. </institution> <type> Power Challenge: Technical Report, </type> <year> 1996. </year>
Reference-contexts: Table 2.1 shows the interconnection-network performance of several multiprocessor systems. For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL <ref> [35, 65] </ref>, Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing
Reference: [66] <author> J. P. Singh, T. Joe, A. Gupta, and J. L. Hennessy. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: For each system, the table shows the system type (NUMA, COMA, or DM-MP), the interconnect type, the peak node-to-node throughput the system can sustain, and the network-communication latency. The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH <ref> [49, 66] </ref>, MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local
Reference: [67] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> One Burlington Woods Drive, Burlington, MA 01803, </address> <year> 1992. </year> <month> 156 </month>
Reference-contexts: We have implemented both in our prototype system and report on their performance in Section 7.2. 6.3.3 Alternative 1: Using TLB Misses to Determine Access Times In order to collect age information mapped pages, we modified the Digital Alpha TLB handler, which is implemented in PALcode <ref> [67] </ref>. Once a minute, we flush the TLB of all entries. 1 Subsequently, when the TLB handler performs a virtual-to-physical translation on a TLB miss, it sets a bit for that physical frame.
Reference: [68] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> One Burlington Woods Drive, Burlington, MA 01803, </address> <year> 1992. </year>
Reference-contexts: We used the prototype to provide measured software management and network overheads for the simulator, and to validate simulated results where possible. The prototype and simulator are described below. 8.2.1 System Prototype Subpage protection is implemented in software by modifying the PALcode <ref> [68] </ref> on the DEC Alpha 250. Our system keeps 32 subpage valid bits for each page, one bit for each 256-byte block; the valid bits indicate which subpages are currently valid (subpages are a multiple of 256 bytes).
Reference: [69] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <type> Technical Report 94/2, </type> <institution> DEC Western Research Lab, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: The remainder of this section provides a detailed analysis using our trace-driven simulator, which was calibrated using prototype measurements. 8.2.2 Trace-Driven Simulator Our simulator models the memory behavior of applications executing in a global memory environment. It takes as input memory reference traces generated from applications instrumented by Atom <ref> [69] </ref>. It then simulates these memory references and models the effect of paging to both local disk and to remote memory. Paging policy is determined by a configurable memory management module; an LRU policy is used by default. <p> We traced a link of Digital Unix V3.2 to generate our ld data. The trace includes 102 million memory references, and from 6807 to 10629 page faults. Atom <ref> [69] </ref> is the instrumentation software we used to generate the traces. To obtain this trace, we instrumented Atom itself (using Atom), then traced it while processing the gzip binary. The trace includes 73 million memory accesses and from 1175 to 5275 page faults.
Reference: [70] <author> Michael Stonebraker. </author> <title> Operating system support for database management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: There are three well-known problems with LRU (and LFU), which are all related to the fact that LRU considers only system-wide page access-time (-frequency) information and thus does not consider characteristics of the programs that are accessing those pages or the files to which they belong <ref> [70] </ref>. 1. Some programs and/or files exhibit better locality than others and thus benefit more from having their pages cached. Replacing the oldest page in the system might be a poor choice if that page belongs to a high-locality program or file and there are other low-locality pages in memory.
Reference: [71] <author> D. Krekel T. Bonniger, R. Esser. </author> <title> CM-5E, KSR2, Paragon XP/S: A comparative description of massively parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 21(2) </volume> <pages> 199-232, </pages> <year> 1995. </year>
Reference-contexts: The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR <ref> [43, 71] </ref>, CM-5 [76, 71], Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as <p> The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 <ref> [76, 71] </ref>, Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as shown in Figure <p> The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 [76, 71], Intel Paragon <ref> [39, 71] </ref>, and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as shown in Figure 2.2.
Reference: [72] <author> Madhusudhan Talluri and Mark D. Hill. </author> <title> Surpassing the TLB performance of super-pages with less operating system support. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [73, 72, 58] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [73] <author> Madhusudhan Talluri, Shing Kong, Mark D. Hill, and David A. Patterson. </author> <title> Tradeoffs in supporting two page sizes. </title> <booktitle> In Proc. of the 19th Annual Int. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [73, 72, 58] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [74] <author> A. S. Tanenbaum. </author> <title> Operating Systems Design and Implementation. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference-contexts: In this dissertation, we are primarily concerned with the core data-movement aspect of memory management, which can be summarized by its three constituent subpolicies <ref> [74, 64] </ref>. Three Key Memory Management Subpolicies Fetch The fetch subpolicy determines how and when the system fetches data from a distant memory to a closer memory.
Reference: [75] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating data and control transfer in distributed systems. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Prog. Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers <ref> [80, 75] </ref>, and newer controllers reduce latency even further [30]. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead.
Reference: [76] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: The table lists the following systems: SGI Challenge XL [35, 65], Stanford DASH [49, 66], MIT Alewife [1], Cray T3D [5, 21], KSR [43, 71], CM-5 <ref> [76, 71] </ref>, Intel Paragon [39, 71], and Princeton SHRIMP [30]. 2.1.2 Workstation Networks A network of workstations (NOW) is similar in many ways to a distributed-memory multiprocessor It consists of a collection of processing nodes, each with local memory, connected to each other by a general-purpose network, as shown in Figure
Reference: [77] <author> R. Turner and H. Levy. </author> <title> Segmented FIFO page replacement. </title> <booktitle> In Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-51, </pages> <month> September </month> <year> 1981. </year> <month> 157 </month>
Reference-contexts: If accessed rapidly enough, all of these pages will be newer than pages belonging to other programs. Such a strategy can result in a program growing to consume all of the memory of a node. Working Set LRU (WS) <ref> [24, 77] </ref> is an approach used by some virtual memory systems that provides a framework for solving these three problems. <p> That is, any page that is not referenced between the time the first and second hands pass over it is deemed to be old and is therefore replaced. FIFO-with-Second-Chance <ref> [77, 26] </ref> is a common variant of Two-Handed Clock, implemented by most systems including Digital Unix. The system maintains three FIFO queues of pages: active, inactive, and free. Only active pages are directly accessible to applications.
Reference: [78] <author> Richard Uhlig, David Nagle, Trevor Mudge, and Stuart Sechrest. </author> <title> Trap-driven simulation with Tapeworm II. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Prog. Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Similar techniques have been used for trace production as well <ref> [78] </ref>. 2 The IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line [17]. 126 Table 8.2: Page-fault latencies for Eager-Fullpage Fetch from remote memory. Latencies are arrival times of subpage and rest of page.
Reference: [79] <author> Uresh Vahalia. </author> <title> UNIX Internals: The New Frontiers. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1996. </year>
Reference-contexts: Before describing our algorithm, we review its basis, the standard 91 Clock-like algorithm found in most systems, including Digital Unix. Standard Two-Handed Clock and FIFO-With-Second-Chance Two-Handed Clock <ref> [6, 79] </ref> maintains a single bit of information for each page, the referenced bit, set by the system when a page is accessed. Physical pages are logically organized into a circular list that is swept by two hands as replacement victims are needed.
Reference: [80] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers <ref> [80, 75] </ref>, and newer controllers reduce latency even further [30]. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead.
Reference: [81] <author> B. W. Wah. </author> <title> File placement on distributed computer systems. </title> <journal> Computer, </journal> <volume> 17(1) </volume> <pages> 315-331, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: This assignment of data to memories does not change over time because the access probabilities are static; as a result, replacement is not needed. Formulated in this way, memory management can be regarded as a File Allocation Problem <ref> [25, 81] </ref>, which Leff et al. show can be transformed to an equivalent transportation problem [36] and thus efficiently solved [10]. While useful from a theoretical point of view, the assumption of static access probabilities is highly 32 unsatisfactory for understanding memory management in real systems.
Reference: [82] <author> D. L. Willick, D. L. Eager, and R. B. Bunt. </author> <title> Disk cache replacement policies for network fileservers. </title> <booktitle> In Proceedings of the 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 2-11, </pages> <month> may </month> <year> 1993. </year>
Reference-contexts: Most virtual memory systems actually use an approximation of LRU, as we describe in Section 6.3.4. Less common are frequency-based schemes such 17 as LFU (Least Frequently Used), which are less popular because access frequency is more difficult to collect and interpret than access time <ref> [55, 82, 34] </ref>.
References-found: 82

