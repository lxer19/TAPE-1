URL: ftp://ftp.cs.washington.edu/tr/1991/09/UW-CSE-91-09-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: etzioni@cs.washington.edu  
Title: A Structural Theory of Explanation-Based Learning  
Author: Oren Etzioni 
Date: May 1992  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Note: To appear in artificial intelligence  
Abstract: greatly from one problem space to another. In fact, seemingly minute modifications to problem space encoding can drastically alter EBL's impact. For example, while prodigy/ebl (a state-of-the-art EBL system) significantly speeds up the prodigy problem solver in the Blocksworld, prodigy/ebl actually slows prodigy down in a representational variant of the Blocksworld constructed by adding a single, carefully chosen, macro-operator to the Blocksworld operator set. Although EBL has been tested experimentally, no theory has been put forth that accounts for such phenomena. This paper presents such a theory. The theory exhibits a correspondence between a graph representation of problem spaces and the proofs used by EBL systems to generate search-control knowledge. The theory relies on this correspondence to account for the variations in EBL's impact. This account is validated by static, a program that extracts EBL-style control knowledge directly from the graph representation, without using training examples. When tested on prodigy/ebl's benchmark tasks, static was up to three times as effective as prodigy/ebl in speeding up prodigy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Neeraj Bhatnagar and Jack Mostow. </author> <title> Adaptive search by explanation-based learning of heuristic censors. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: That is precisely what the static program does. A complete description of static's algorithms appears in [11]. 4.3 Scope of the Structural Thesis Although the structural thesis applies to many existing EBL problem solvers (e.g., <ref> [27, 1, 52] </ref>) its scope is limited for a number of reasons. First, although most commonly-used target concepts (success, failure, etc.) map naturally to PSG subgraphs, other target concepts (e.g., state cycles) do not. Second, PSGs have only been developed for backward-chaining problem solvers.
Reference: [2] <author> Daniel G. Bobrow. Retrospectives: </author> <title> A note from the editor. </title> <journal> Artificial Intelligence, </journal> <volume> 23, </volume> <year> 1984. </year>
Reference: [3] <author> Henrik Bostrom. </author> <title> Generalizing the order of goals as an approach to generalizing number. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Furthermore, many strings are ambiguous. The string abaaba is plausibly generalized to (aba) fl or to aba fl ba. In effect, generalization-to-N imposes an inductive bias on the generalization of recursions, based on its heuristics for detecting and generalizing repeated substrings, which can lead it to overgeneralize (cf. <ref> [3] </ref>). Thus, although generalization-to-N addresses the recursion-depth-specificity of EBL's control knowledge, it will not always generalize a recursion appropriately or adequately. Furthermore, the technique yields rules that expand recursions at run time.
Reference: [4] <author> P. Cheng and J. G. Carbonell. </author> <title> The FERMI system: Inducing iterative macro-operators from experience. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 490-495, </pages> <year> 1986. </year>
Reference-contexts: As a result, when EBL's proofs are recursive, distinct rules are learned at every recursive depth to which the proofs are expanded <ref> [4, 43, 48] </ref>. For example, prodigy/ebl learns distinct Blocksworld control rules for clearing the bottom block of a two-block tower, a three-block tower, and so on. <p> It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation.
Reference: [5] <author> William W. Cohen. </author> <title> Concept Learning Using Explanation Based Generalization as an Abstraction Mechanism. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation. <p> Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., <ref> [5, 47, 52] </ref>) and partial evaluators (e.g., [60, 46, 27]) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. <p> Cohen [6] describes a solution-path caching mechanism that operates in polynomial time and provably improves performance (see also <ref> [5] </ref>). Both Cohen and Tade-palli make useful connections between Valiant's framework and the learning of search control knowledge. Minton's thesis contains a thorough analysis of prodigy/ebl. Minton's focus, however, is on the impact of prodigy/ebl's components on its performance.
Reference: [6] <author> William W. Cohen. </author> <title> Using distribution-free learning theory to analyze chunking. </title> <booktitle> In Pro ceedings of the Eighth Biennial Conference of the Canadian Society for Computational Studies of Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>. <p> Although Tadepalli's model is different from my own, the spirit of the two approaches is similar: we both seek to identify classes of problem spaces in which EBL results in tractable problem solving. Cohen <ref> [6] </ref> describes a solution-path caching mechanism that operates in polynomial time and provably improves performance (see also [5]). Both Cohen and Tade-palli make useful connections between Valiant's framework and the learning of search control knowledge. Minton's thesis contains a thorough analysis of prodigy/ebl.
Reference: [7] <author> R. Davis. </author> <title> Meta-rules: reasoning about control. </title> <journal> AI, </journal> <volume> 15 </volume> <pages> 179-222, </pages> <year> 1980. </year> <note> REFERENCES 41 </note>
Reference-contexts: The model is an idealization of problem solvers such as <ref> [7] </ref>, mrs [18], Soar [25], prodigy [34], theo [37], and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specific meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation.
Reference: [8] <author> G. F. Dejong and R. J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: relationship between EBL's proofs and the length of EBL's control rules, a prerequisite to the analysis in Section 3.3. 3.1 The EBG Framework Mitchell, Keller and Kedar-Cabelli [38] describe a model of EBL, called Explanation-Based Generalization (EBG) that articulates many of the aspects common to various EBL systems (see also <ref> [8] </ref>). Two of the major contributions of the EBG model are the identification of explanations with proofs, giving a precise meaning to the term "explanation," and the clear specification of the inputs and output of EBL shown in Table 3.
Reference: [9] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1976. </year>
Reference-contexts: The weakest precondition of a proof is the (weakest) sufficient condition under which the proof succeeds. Section 3.2, below, defines this notion more precisely (cf. <ref> [9, 29] </ref>). 3 EBL PROBLEM SOLVERS 10 Given: * Target Concept Definition: A concept definition describing the concept to be learned. (It is assumed that this concept definition fails to satisfy the Operationality Criterion.) * Training Example: An example of the target concept. * Domain Theory: A set of rules and
Reference: [10] <author> Oren Etzioni. </author> <title> A Structural Theory of Explanation-Based Learning. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year> <note> Available as technical report CMU-CS-90-185. </note>
Reference-contexts: In addition, generating the large samples typically required by these mechanisms is very costly. Experimental Validation When applied to prodigy, the structural theory helps explain prodigy/ebl's success in Minton's experiments as well as several experimental results reported in <ref> [10] </ref>: 1. static outperforms prodigy/ebl when compared using prodigy/ebl's benchmark tasks. 2. prodigy/ebl's impact degrades significantly in the ABworld relative to the Blocksworld. 3. prodigy/ebl significantly reduces prodigy's Blocksworld problem-solving time over a wide range of problem distributions. 4. prodigy/ebl's impact degrades sharply when learning only from success. <p> See <ref> [10] </ref> for a complete specification. <p> The example illustrates the general correspondence between PSG subgraphs and prodigy/ebl's proofs. This relationship is not surprising if we consider that, by construction, the PSG explicitly represents the failure and success of different problem-solving paths which is precisely the subject matter of EBL's proofs. See <ref> [10] </ref> for a more detailed description of the mapping between PSGs and EBL's proofs. 10 The recursive expansions in prodigy/ebl's recursive proofs are represented as leaf nodes. 5 CHOOSING WHAT PROOFS TO LEARN FROM 19 The correspondence described above facilitates traversing the PSG searching for subgraphs corresponding to nonrecursive proofs (i.e. <p> Even the ad hoc rule-evaluation mechanism used by prodigy/ebl [32] accounts for a significant fraction of prodigy/ebl's learning time <ref> [10, chapter 8] </ref>. Finally, sample-based rule selection is distribution-dependent. If the problem distribution changes, the rule selection algorithm has to be invoked again, on a fresh sample, to ensure that an appropriate rule set is chosen. <p> Additional work is required to precisely identify the classes of problems in which it is effective. Augmenting the heuristic with a variety of example-based and sample-based approaches as suggested in <ref> [10] </ref> and investigated in [19, 42] is a worthwhile direction for future work. 5 CHOOSING WHAT PROOFS TO LEARN FROM 21 5.1.1 Bounded-Depth Recursion Bounded-depth recursive proofs are distinct from nonrecursive proofs. <p> Recursive Heuristic In Practice Experimental inquiries have shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle [26, 55], or on highly skewed problem distributions [47, 56], but is ineffective in many other cases <ref> [10] </ref>, even in the presence of generalization-to-N [52]. <p> This section relied on the structural theory to account for prodigy/ebl's success in Minton's experiments and to informally explain several additional experimental results originally reported in <ref> [10] </ref>. The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance [6, 21, 28, 30, 44, 50].
Reference: [11] <author> Oren Etzioni. </author> <title> Acquiring search-control knowledge via static analysis. </title> <type> Technical Report 92-04-01, </type> <institution> University of Washington, </institution> <year> 1992. </year>
Reference-contexts: In Figure 6, for example, both pick-up (V) and un-stack (V,V2) have (clear V) as a precondition. Consequently, both operators have and-links to that node. As described thus far, recursion would result in infinite PSGs. In fact, the PSG has well-defined termination conditions under which PSGs are provably finite <ref> [11] </ref>. To state these 8 The PSG should not be confused with a state space graph. <p> PSG, for example, (on V1 V) is labeled holds because, to reach it, prodigy subgoals on (holding V) and (clear V) and, if a block is neither clear nor held, then there must be some block on it. 9 An algorithm that derives PSGs from problem space definitions appears in <ref> [11] </ref>. The semantics of PSG nodes can be defined relative to prodigy's problem solving. A literal node appears in the PSG if and only if there is a world state in which the problem 9 The (arm-empty) nodes are labeled holds to keep the PSG's depiction compact. <p> That is precisely what the static program does. A complete description of static's algorithms appears in <ref> [11] </ref>. 4.3 Scope of the Structural Thesis Although the structural thesis applies to many existing EBL problem solvers (e.g., [27, 1, 52]) its scope is limited for a number of reasons. <p> Finally, static derives control rules based on this annotation. See <ref> [11] </ref> for a complete description of static's algorithms. The previous section suggested that, on prodigy/ebl's benchmark tasks, effective control knowledge can be derived from nonrecursive proofs.
Reference: [12] <author> Oren Etzioni. </author> <title> An asymptotic analysis of speedup learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The cost of matching a unique-attribute rule scales only linearly with rule length. 6 Paul Rosenbloom notes that even though the total overhead of t is exponential in s, the ratio ( + t ) t = is constant, so long as t is constant. See <ref> [12] </ref> for additional discussion of this issue. 3 EBL PROBLEM SOLVERS 14 I define the recursive depth of a problem, relative to a given EBL problem solver, to be the maximum of the recursive depths of the proofs EBL has to learn from in order to eliminate backtracking on that problem.
Reference: [13] <author> Oren Etzioni and Ruth Etzioni. </author> <title> Statistical methods for analyzing speedup learning experiments. </title> <note> Submitted for publication., </note> <year> 1992. </year>
Reference-contexts: Average speedup is relatively easy to test experimentally; measuring problem-solving time with and without a set of control rules, on a large, randomly generated sample of problems, indicates whether the set achieves average speedup or not <ref> [13] </ref>. Unfortunately, average speedup is distribution-specific|a rule set may be effective on one problem distribution and ineffective on another. Furthermore, average speedup is a weak notion. Problem solving may remain intractable, due to its exponential nature, despite a sizable average speedup. <p> Segre et al. also argue that the experimenter's choice of time bound can bias the results of the experiment. To address this problem, Etzioni and Etzioni <ref> [13] </ref> develop statistical hypothesis tests designed to analyze speedup learning data that is "truncated" due to the use of time bounds. The tests show that the differences between prodigy+static and prodigy+ebl are statistically significant.
Reference: [14] <author> Oren Etzioni and Steven Minton. </author> <title> Why EBL produces overly-specific knowledge: A critique of the PRODIGY approaches. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Each such proof yields a different control rule, some of which are considerably more useful than others. When analyzing a training example, EBL merely computes the weakest precondition of a particular proof. It is by no means guaranteed to find "the best" control rule, or even an effective one <ref> [14] </ref>. Indeed, EBL frequently derives ineffective control knowledge in practice. Based on the complexity analysis in Section 3, this section presents heuristics that help EBL to overcome this problem by indicating which proofs EBL should learn from. <p> Thus, the recursive heuristic remains the best a priori heuristic we can formulate. The recursive heuristic is particularly important because of the practice of training EBL on relatively simple problems that exhibit shallow recursions. As pointed out in <ref> [14] </ref>, this practice often helps EBL find simpler and more compact explanations of its problem solver's 12 See [27, 46] for exceptions in certain special cases. 5 CHOOSING WHAT PROOFS TO LEARN FROM 23 behavior.
Reference: [15] <author> Oren Etzioni and Tom M. Mitchell. </author> <title> A comparative analysis of chunking and decision analytic control. </title> <booktitle> In The Soar Papers: Research on Integrated Intelligence. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year> <note> To Appear. </note>
Reference: [16] <author> R. Fikes, P. Hart, and N. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3(4), </volume> <year> 1972. </year>
Reference-contexts: A sample prodigy operator appears in Table 1. prodigy's sole problem-solving method is a form of means-ends analysis [40]. Like strips <ref> [16] </ref>, prodigy employs operator preconditions as its differences. <p> In contrast, prodigy's minimal node count is 1400 (since it found shorter solutions to some problems) but it expanded 217,948 which is 155.67 times its minimal node count. 6.1.2 The Stripsworld prodigy's Stripsworld is an extended version of the strips robot-planning problem space <ref> [16] </ref>. The Stripsworld also exhibits nonrecursive failure and goal-clobbering proofs. 13 Recall that prodigy+ebl denotes prodigy guided by prodigy/ebl's rules. 6 EXPERIMENTAL VALIDATION 27 Thus, prodigy's primary challenge in the Stripsworld is finding its way through each prob-lem's room configuration.
Reference: [17] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers And Intractability A guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: To see this note that the problem of subgraph isomorphism, 3 This match cannot be performed via unification (which is linear-time) because unification is not defined for conjunctive expressions. See [22] for a precise definition of unification. 3 EBL PROBLEM SOLVERS 9 which is known to be NP-hard <ref> [17, page 202] </ref>, can be reduced to the problem of matching a conjunctive expression [36, page 184].
Reference: [18] <author> M.R. Genesereth. </author> <title> An overview of meta-level architecture. </title> <booktitle> In Proc. AAAI, </booktitle> <address> Washington, D.C., </address> <month> August </month> <year> 1983. </year> <note> AAAI. </note>
Reference-contexts: The model is an idealization of problem solvers such as [7], mrs <ref> [18] </ref>, Soar [25], prodigy [34], theo [37], and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specific meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation.
Reference: [19] <author> Jonathan Gratch and Gerald Dejong. Composer: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: The heuristics are attractive because they constitute an a priori basis for generating effective control knowledge. A number of researchers have developed post hoc mechanisms for evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. However, as argued in Section 5, these post hoc mechanisms are heuristic as well. In addition, generating the large samples typically required by these mechanisms is very costly. <p> Determining whether one block is above another, while recursive in the problem space, merely requires comparing X-coordinates in the domain theory. 5 CHOOSING WHAT PROOFS TO LEARN FROM 20 evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. The advantage of post hoc approaches is that they "consider" problem distribution, whereas a priori approaches do not. Sample-based approaches have several disadvantages, though. First, the complex interactions between different control rules mean that individual control rules cannot be tested in isolation. <p> When this is the case, the performance of a sample-based mechanism is arbitrarily bad. Large sample sizes ensure that the probability of this event is low but, to provide such guarantees, the mechanisms proposed in <ref> [19, 20, 54] </ref> typically require their problem solver to solve literally thousands of problems. Even the ad hoc rule-evaluation mechanism used by prodigy/ebl [32] accounts for a significant fraction of prodigy/ebl's learning time [10, chapter 8]. Finally, sample-based rule selection is distribution-dependent. <p> Finally, sample-based rule selection is distribution-dependent. If the problem distribution changes, the rule selection algorithm has to be invoked again, on a fresh sample, to ensure that an appropriate rule set is chosen. Thus, as recognized by <ref> [19] </ref>, a priori heuristics for generating effective control rules have distinct advantages. 5.1 The Nonrecursive Heuristic The nonrecursive heuristic can be stated as follows: Learn from nonrecursive proofs. As pointed out earlier, control knowledge represents a tradeoff between reduced problem-space search and increased match cost. <p> Finally, due to the a priori nature of the nonrecursive heuristic, it is easy to define problem distributions where nonrecursive control rules are rarely applicable and thus ineffective. The experiments reported in <ref> [19, 42] </ref> demonstrate this point. Despite the caveats enumerated above, the experiments described in Section 6 demonstrate the value of the nonrecursive heuristic in practice. Additional work is required to precisely identify the classes of problems in which it is effective. <p> Additional work is required to precisely identify the classes of problems in which it is effective. Augmenting the heuristic with a variety of example-based and sample-based approaches as suggested in [10] and investigated in <ref> [19, 42] </ref> is a worthwhile direction for future work. 5 CHOOSING WHAT PROOFS TO LEARN FROM 21 5.1.1 Bounded-Depth Recursion Bounded-depth recursive proofs are distinct from nonrecursive proofs. <p> I describe a number of cases in which this criterion is imperfect, and others have appeared in the literature <ref> [19, 42] </ref>. The following section argues that, though imperfect, the heuristics formulated above are valuable in practice. 6 Experimental Validation This section reports on an array of experimental results that are explained, informally, using the structural theory.
Reference: [20] <author> Russell Greiner. </author> <title> A solution to the ebl utility problem. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: The heuristics are attractive because they constitute an a priori basis for generating effective control knowledge. A number of researchers have developed post hoc mechanisms for evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. However, as argued in Section 5, these post hoc mechanisms are heuristic as well. In addition, generating the large samples typically required by these mechanisms is very costly. <p> Determining whether one block is above another, while recursive in the problem space, merely requires comparing X-coordinates in the domain theory. 5 CHOOSING WHAT PROOFS TO LEARN FROM 20 evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. The advantage of post hoc approaches is that they "consider" problem distribution, whereas a priori approaches do not. Sample-based approaches have several disadvantages, though. First, the complex interactions between different control rules mean that individual control rules cannot be tested in isolation. <p> When this is the case, the performance of a sample-based mechanism is arbitrarily bad. Large sample sizes ensure that the probability of this event is low but, to provide such guarantees, the mechanisms proposed in <ref> [19, 20, 54] </ref> typically require their problem solver to solve literally thousands of problems. Even the ad hoc rule-evaluation mechanism used by prodigy/ebl [32] accounts for a significant fraction of prodigy/ebl's learning time [10, chapter 8]. Finally, sample-based rule selection is distribution-dependent.
Reference: [21] <author> Russell Greiner and Joseph Likuski. </author> <title> Incorporating redundant learned rules: A pre liminary formal analysis of EBL. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: When is control knowledge effective over the entire space? The most widely-used notion of effectiveness in the literature is average speedup. A set of control rules is said to be effective if it speeds up problem solving, on average, on a population of problems (e.g. <ref> [21, 32, 52] </ref>). Average speedup is relatively easy to test experimentally; measuring problem-solving time with and without a set of control rules, on a large, randomly generated sample of problems, indicates whether the set achieves average speedup or not [13]. <p> The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>. <p> For example, Minton points out that a problem space's "solution density and distribution" influences the utility of different learning strategies. Minton himself concedes that "we have not adequately characterized the types of domains for which the learning method produces good results." [32]. Greiner and Likuski <ref> [21] </ref> model EBL as adding redundant rules to a set of inference rules. They report that, in general, finding the optimal inference strategy in a redundant search space is NP-hard.
Reference: [22] <author> Kevin Knight. </author> <title> Unification: A multidisciplinary survey. </title> <journal> Computing Surveys, </journal> <volume> 21(1), </volume> <month> March </month> <year> 1989. </year> <note> REFERENCES 42 </note>
Reference-contexts: To see this note that the problem of subgraph isomorphism, 3 This match cannot be performed via unification (which is linear-time) because unification is not defined for conjunctive expressions. See <ref> [22] </ref> for a precise definition of unification. 3 EBL PROBLEM SOLVERS 9 which is known to be NP-hard [17, page 202], can be reduced to the problem of matching a conjunctive expression [36, page 184].
Reference: [23] <author> R. E. Korf. Macro-operators: </author> <title> A weak method for learning. </title> <journal> Artificial Intelligence, </journal> <volume> 26 </volume> <pages> 35-77, </pages> <year> 1985. </year>
Reference-contexts: Nonrecursively Serializable Subgoals A set of subgoals is said to be serializable if there exists an ordering of the subgoals such that, if the subgoals are achieved in that order, once a subgoal is satisfied it need never be violated in order to achieve the remaining subgoals <ref> [23, page 39] </ref>. A problem is said to be serializable when the subgoals that comprise the problem's goal conjunction are serializable.
Reference: [24] <author> Robert A. Kowalski. </author> <title> A proof procedure using connection graphs. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 572-595, </pages> <year> 1974. </year>
Reference-contexts: An operator has an and-link to a literal if and only if the literal is a precondition to the operator. 4.2 The Correspondence of PSGs to EBL's Proofs The notion of representing programs (or problem spaces) as graphs is well-known in computer science (see, for example, <ref> [24, 58] </ref>). It is interesting to note, however, that prodigy/ebl's proofs correspond to PSG subgraphs. 10 prodigy/ebl's failure proofs, for example, have the following flavor: an operator cannot be executed because one of its preconditions cannot be achieved.
Reference: [25] <author> J. E. Laird, A. Newell, and P. S. Rosenbloom. </author> <title> Soar: An architecture for general intelli gence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: The model is an idealization of problem solvers such as [7], mrs [18], Soar <ref> [25] </ref>, prodigy [34], theo [37], and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specific meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation.
Reference: [26] <author> J. E. Laird, P. S. Rosenbloom, and Newell A. </author> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: As Section 3.3.1 indicates, such disparity leads to exponential overhead due to EBL. 5.2.1 The Recursive Heuristic In Practice Experimental inquiries have shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle <ref> [26, 55] </ref>, or on highly skewed problem distributions [47, 56], but is ineffective in many other cases [10], even in the presence of generalization-to-N [52].
Reference: [27] <author> Stanley Letovsky. </author> <title> Operationality criteria for recursive predicates. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation. <p> That is precisely what the static program does. A complete description of static's algorithms appears in [11]. 4.3 Scope of the Structural Thesis Although the structural thesis applies to many existing EBL problem solvers (e.g., <ref> [27, 1, 52] </ref>) its scope is limited for a number of reasons. First, although most commonly-used target concepts (success, failure, etc.) map naturally to PSG subgraphs, other target concepts (e.g., state cycles) do not. Second, PSGs have only been developed for backward-chaining problem solvers. <p> The recursive heuristic is particularly important because of the practice of training EBL on relatively simple problems that exhibit shallow recursions. As pointed out in [14], this practice often helps EBL find simpler and more compact explanations of its problem solver's 12 See <ref> [27, 46] </ref> for exceptions in certain special cases. 5 CHOOSING WHAT PROOFS TO LEARN FROM 23 behavior. However, in the case of recursive proofs, the practice can lead to a disparity between the shallow recursions in training examples and deep recursions on test cases. <p> Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., [5, 47, 52]) and partial evaluators (e.g., <ref> [60, 46, 27] </ref>) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. <p> Thus, structure of the PSG is a major factor influencing the effectiveness of EBL. The PSG representation changes with the problem space encoding. In practice, problem space encoding is often modified and tweaked repeatedly until EBL generates effective control knowledge. As Letovsky <ref> [27] </ref> put it: "It is not the case that EBG systems can take any, or even most, inefficient encodings of a task and turn out efficient versions; rather it is sometimes the case that there exists some encoding of a task for which an EBG system can do something reasonable." Today,
Reference: [28] <author> Sridhar Mahadevan, B. K. Natarajan, and Prasad Tadepalli. </author> <title> A framework for learning as improving problem-solving performance. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Explanation-Based Learning, </booktitle> <year> 1988. </year>
Reference-contexts: The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>. <p> One analysis [52, 53] independently reaches a conclusion that is closely related to the recursive heuristic, and is discussed at greater length. No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. Mahadevan, Natarajan, and Tadepalli <ref> [28] </ref> present a formal model of learning as improving problem-solving performance based on the framework in [39]. Based on the model, Tadepalli [55] argues that EBL relies on structural constraints (e.g. serial decomposability) and that, like inductive learning, EBL can be analyzed using Valiant's PAC framework [59].
Reference: [29] <author> Steven Minton. </author> <title> EBL and weakest preconditions. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Explanation-Based Learning, </booktitle> <pages> pages 210-214, </pages> <year> 1988. </year>
Reference-contexts: The weakest precondition of a proof is the (weakest) sufficient condition under which the proof succeeds. Section 3.2, below, defines this notion more precisely (cf. <ref> [9, 29] </ref>). 3 EBL PROBLEM SOLVERS 10 Given: * Target Concept Definition: A concept definition describing the concept to be learned. (It is assumed that this concept definition fails to satisfy the Operationality Criterion.) * Training Example: An example of the target concept. * Domain Theory: A set of rules and
Reference: [30] <author> Steven Minton. </author> <title> Learning Effective Search Control Knolwedge: An Explanation-Based Approach. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1988. </year> <note> Available as technical report CMU-CS-88-133. </note>
Reference-contexts: The problem solver does not subgoal on the applicability conditions of control rules. The prodigy problem solver, described below, is an example of a meta-level problem solver. 2.1 The PRODIGY Problem Solver Detailed descriptions of prodigy appear in <ref> [30, 34, 35] </ref>. The bare essentials follow. prodigy is a domain-independent problem solver. Given an initial state and a goal expression, prodigy searches for a sequence of operators that will transform the initial state into a state that matches the goal expression. <p> Although I have argued against learning from recursive proofs, I have also argued that EBL can be effective in recursive problem spaces. Indeed, most of the plans in prodigy/ebl's benchmark problem spaces are recursive, yet prodigy/ebl is quite effective in these problem spaces <ref> [30] </ref>. This paradox is resolved by observing that prodigy/ebl learns from failure and goal interaction in addition to learning from success. As the Blocksworld example in Section 5.1.2 illustrates, proving that a plan will fail can be nonrecursive even when the plan itself is recursive. <p> It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., [5, 47, 52]) and partial evaluators (e.g., [60, 46, 27]) do not use complementary target concepts. Thus, as Minton <ref> [30] </ref> argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. <p> The following section argues that, though imperfect, the heuristics formulated above are valuable in practice. 6 Experimental Validation This section reports on an array of experimental results that are explained, informally, using the structural theory. In particular, the theory explains prodigy/ebl's success in Minton's experiments <ref> [30] </ref> as well as the experimental results listed in Section 1.2. 6.1 Explaining PRODIGY/EBL's Success "It is rare that one sees an AI system evaluated carefully by anyone other than its creator."[2] To demonstrate the explanatory power of the structural theory, this section utilizes it to explain prodigy/ebl's success in Minton's <p> Potentially, therefore, the Stripsworld could present a severe challenge for prodigy/ebl. In Minton's experiments, prodigy/ebl is aided considerably by the fact that each of the problems takes place in one of three simple room configurations (See <ref> [30, page 182] </ref>). As a result, the rules prodigy/ebl learns from recursive proofs, which enable it to look ahead along certain paths, turn out to be reasonably compact and frequently applicable. <p> The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>.
Reference: [31] <author> Steven Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Variable names are capitalized. prodigy's default search strategy is depth-first search. The search is carried out by repeating the following decision cycle <ref> [31] </ref>: 1. Choose a node in the search tree. A node consists of a set of goals and a world state. 2 META-LEVEL PROBLEM SOLVERS 6 2. Choose one of the goals at that node. 3. Choose an operator that can potentially achieve the goal. 4. <p> In either case, a new node is created. Search-control knowledge in prodigy is encoded via control rules, which override prodigy's default behavior by specifying that particular candidates (nodes, goals, operators, or bindings) should be selected, rejected, or preferred over other candidates <ref> [31] </ref>. Alternatives that are selected are the only ones tried; alternatives that are rejected are removed from the selected set. Finally, all other things being equal, preferred alternatives are tried before other ones. prodigy matches control rules against its current state.
Reference: [32] <author> Steven Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 42(2-3), </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The heuristics are attractive because they constitute an a priori basis for generating effective control knowledge. A number of researchers have developed post hoc mechanisms for evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. However, as argued in Section 5, these post hoc mechanisms are heuristic as well. In addition, generating the large samples typically required by these mechanisms is very costly. <p> When is control knowledge effective over the entire space? The most widely-used notion of effectiveness in the literature is average speedup. A set of control rules is said to be effective if it speeds up problem solving, on average, on a population of problems (e.g. <ref> [21, 32, 52] </ref>). Average speedup is relatively easy to test experimentally; measuring problem-solving time with and without a set of control rules, on a large, randomly generated sample of problems, indicates whether the set achieves average speedup or not [13]. <p> Determining whether one block is above another, while recursive in the problem space, merely requires comparing X-coordinates in the domain theory. 5 CHOOSING WHAT PROOFS TO LEARN FROM 20 evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. The advantage of post hoc approaches is that they "consider" problem distribution, whereas a priori approaches do not. Sample-based approaches have several disadvantages, though. First, the complex interactions between different control rules mean that individual control rules cannot be tested in isolation. <p> Large sample sizes ensure that the probability of this event is low but, to provide such guarantees, the mechanisms proposed in [19, 20, 54] typically require their problem solver to solve literally thousands of problems. Even the ad hoc rule-evaluation mechanism used by prodigy/ebl <ref> [32] </ref> accounts for a significant fraction of prodigy/ebl's learning time [10, chapter 8]. Finally, sample-based rule selection is distribution-dependent. If the problem distribution changes, the rule selection algorithm has to be invoked again, on a fresh sample, to ensure that an appropriate rule set is chosen. <p> points out, however, prodigy/ebl is unable to learn the rule "build towers from the bottom up" in its full generality, because the rule requires knowing which blocks are constrained to be below each other in the goal, and prodigy/ebl can only determine this using recursive proofs whose generality is limited <ref> [32, page 385] </ref>. Thus, not all Blocksworld problems are nonrecursively serializable. <p> For example, Minton points out that a problem space's "solution density and distribution" influences the utility of different learning strategies. Minton himself concedes that "we have not adequately characterized the types of domains for which the learning method produces good results." <ref> [32] </ref>. Greiner and Likuski [21] model EBL as adding redundant rules to a set of inference rules. They report that, in general, finding the optimal inference strategy in a redundant search space is NP-hard.
Reference: [33] <author> Steven Minton, Jaime G. Carbonell, Oren Etzioni, Craig A. Knoblock, and Daniel R. Kuokka. </author> <title> Acquiring effective search control rules: Explanation-based learning in the Prodigy system. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <year> 1987. </year>
Reference-contexts: Most of the issues considered in this paper (e.g. recursion and complementary target concepts) do not arise in Greiner and Likuski's model. Subramanian and Feldman [52, 53] extend Greiner and Likuski's model to cover recursive and conjunctive rule sets. Adopting Minton's utility criterion <ref> [33] </ref> they show that, relative to an arbitrary probability distribution and theory, adding a redundant rule m can be justified exactly when p m &gt; C mf =(C mf + C d C ms ) where p m is the probability that m applies, C mf is the average cost of
Reference: [34] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Et zioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem-solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year> <note> Available as technical report CMU-CS-89-103. </note>
Reference-contexts: The model is an idealization of problem solvers such as [7], mrs [18], Soar [25], prodigy <ref> [34] </ref>, theo [37], and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specific meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation. <p> The problem solver does not subgoal on the applicability conditions of control rules. The prodigy problem solver, described below, is an example of a meta-level problem solver. 2.1 The PRODIGY Problem Solver Detailed descriptions of prodigy appear in <ref> [30, 34, 35] </ref>. The bare essentials follow. prodigy is a domain-independent problem solver. Given an initial state and a goal expression, prodigy searches for a sequence of operators that will transform the initial state into a state that matches the goal expression.
Reference: [35] <author> Steven Minton, Craig A. Knoblock, Daniel R. Kuokka, Yolanda Gil, Robert L. Joseph, and Jaime G. Carbonell. </author> <title> Prodigy 2.0: The manual and tutorial. </title> <type> Technical Report CMU-CS-89-146, </type> <institution> Carnegie Mellon University, </institution> <year> 1989. </year> <note> REFERENCES 43 </note>
Reference-contexts: The problem solver does not subgoal on the applicability conditions of control rules. The prodigy problem solver, described below, is an example of a meta-level problem solver. 2.1 The PRODIGY Problem Solver Detailed descriptions of prodigy appear in <ref> [30, 34, 35] </ref>. The bare essentials follow. prodigy is a domain-independent problem solver. Given an initial state and a goal expression, prodigy searches for a sequence of operators that will transform the initial state into a state that matches the goal expression.
Reference: [36] <author> Tom M. Mitchell. </author> <title> Version Spaces: An Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1978. </year> <note> also Stanford CS report STAN-CS-78-711, HPP-79-2. </note>
Reference-contexts: See [22] for a precise definition of unification. 3 EBL PROBLEM SOLVERS 9 which is known to be NP-hard [17, page 202], can be reduced to the problem of matching a conjunctive expression <ref> [36, page 184] </ref>. This observation is important because, although the cost of matching a rule against a state of size s is exponential in the rule's length, match cost is polynomial in s for any rule of bounded length.
Reference: [37] <author> Tom M. Mitchell, John Allen, Prasad Chalasani, John Cheng, Oren Etzioni, Marc Ringuette, and Jeffrey C. Schlimmer. Theo: </author> <title> A framework for self-improving systems. </title> <editor> In K. VanLehn, editor, </editor> <booktitle> Architectures for Intelligence. </booktitle> <publisher> Erlbaum, </publisher> <year> 1991. </year>
Reference-contexts: The model is an idealization of problem solvers such as [7], mrs [18], Soar [25], prodigy [34], theo <ref> [37] </ref>, and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specific meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation.
Reference: [38] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based general ization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: The section presents the EBG framework, which defines standard EBL terminology, and describes the relationship between EBL's proofs and the length of EBL's control rules, a prerequisite to the analysis in Section 3.3. 3.1 The EBG Framework Mitchell, Keller and Kedar-Cabelli <ref> [38] </ref> describe a model of EBL, called Explanation-Based Generalization (EBG) that articulates many of the aspects common to various EBL systems (see also [8]).
Reference: [39] <author> B. K. Natarajan and Prasad Tadepalli. </author> <title> Two new frameworks for learning. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: Still, automatically generating polynomial-time algorithms, for a wide range of problems in P, has distinct advantages over requiring a human programmer to derive the algorithms by hand. Although the polynomial-time criterion is stronger and easier to analyze (cf. <ref> [39] </ref>), it is worst-case and asymptotic. Furthermore, average speedup may be attainable in cases where polynomial-time problem solving is not. <p> No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. Mahadevan, Natarajan, and Tadepalli [28] present a formal model of learning as improving problem-solving performance based on the framework in <ref> [39] </ref>. Based on the model, Tadepalli [55] argues that EBL relies on structural constraints (e.g. serial decomposability) and that, like inductive learning, EBL can be analyzed using Valiant's PAC framework [59].
Reference: [40] <author> Allen Newell and Herbert A. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice Hall, </publisher> <year> 1972. </year>
Reference-contexts: Given an initial state and a goal expression, prodigy searches for a sequence of operators that will transform the initial state into a state that matches the goal expression. A sample prodigy operator appears in Table 1. prodigy's sole problem-solving method is a form of means-ends analysis <ref> [40] </ref>. Like strips [16], prodigy employs operator preconditions as its differences.
Reference: [41] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing, </publisher> <year> 1980. </year>
Reference-contexts: Using the terms introduced above, I now consider each of prodigy/ebl's benchmark problem spaces in turn. 6.1.1 The Blocksworld prodigy's Blocksworld is a standard encoding of the Blocksworld problem space based on the encoding in <ref> [41] </ref>. The Blocksworld exhibits short nonrecursive failure proofs, and 6 EXPERIMENTAL VALIDATION 26 fortuitous recursions for each subgoal in the space. As a result, prodigy/ebl is able to form low-cost operator and bindings choice rules that obviate backtracking in achieving individual subgoals.
Reference: [42] <author> M. Alicia Perez and Oren Etzioni. </author> <title> DYNAMIC: a new role for training problems in EBL. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <note> An expanded version available as technical report CMU-CS-92-124. </note>
Reference-contexts: Finally, due to the a priori nature of the nonrecursive heuristic, it is easy to define problem distributions where nonrecursive control rules are rarely applicable and thus ineffective. The experiments reported in <ref> [19, 42] </ref> demonstrate this point. Despite the caveats enumerated above, the experiments described in Section 6 demonstrate the value of the nonrecursive heuristic in practice. Additional work is required to precisely identify the classes of problems in which it is effective. <p> Additional work is required to precisely identify the classes of problems in which it is effective. Augmenting the heuristic with a variety of example-based and sample-based approaches as suggested in [10] and investigated in <ref> [19, 42] </ref> is a worthwhile direction for future work. 5 CHOOSING WHAT PROOFS TO LEARN FROM 21 5.1.1 Bounded-Depth Recursion Bounded-depth recursive proofs are distinct from nonrecursive proofs. <p> I describe a number of cases in which this criterion is imperfect, and others have appeared in the literature <ref> [19, 42] </ref>. The following section argues that, though imperfect, the heuristics formulated above are valuable in practice. 6 Experimental Validation This section reports on an array of experimental results that are explained, informally, using the structural theory.
Reference: [43] <author> A. E. </author> <title> Prieditis. Discovery of algorithms from weak methods. </title> <booktitle> In Proceedings of the international meeting on advances in learning, </booktitle> <pages> pages 37-52, </pages> <year> 1986. </year>
Reference-contexts: As a result, when EBL's proofs are recursive, distinct rules are learned at every recursive depth to which the proofs are expanded <ref> [4, 43, 48] </ref>. For example, prodigy/ebl learns distinct Blocksworld control rules for clearing the bottom block of a two-block tower, a three-block tower, and so on. <p> It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation.
Reference: [44] <author> Paul Resnick. </author> <title> Generalizing on multiple grounds: Performance learning in model-based troubleshooting. </title> <type> Master's thesis, </type> <institution> M.I.T., </institution> <year> 1988. </year> <note> Available as AI Lab Technical Report 1052. </note>
Reference-contexts: The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>.
Reference: [45] <author> Alberto Segre, Charles Elkan, and Alex Russell. </author> <title> A critical look at experimental evalu ations of EBL. Machine Learning, 1991. forthcoming methodological note. </title>
Reference-contexts: The departure from Minton's cumulative graphs format is motivated by Segre et al.'s argument that, when some problems are unsolved within the CPU time bound, changing the bound can influence the graphical relationship between systems being compared using the cumulative graphs format <ref> [45] </ref>. The graphs used here are specifically designed to address this problem, showing how the relative performance of the systems scales on larger and larger CPU time bounds. Segre et al. also argue that the experimenter's choice of time bound can bias the results of the experiment.
Reference: [46] <author> Peter Sestfot. </author> <title> Automatic call unfolding in a partial evaluator. </title> <editor> In D. Bjorner, A. P. Ershov, and N. D. Jones, editors, </editor> <title> Partial Evaluation and Mixed Computation. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1988. </year> <booktitle> Workshop Proceedings. </booktitle>
Reference-contexts: The recursive heuristic is particularly important because of the practice of training EBL on relatively simple problems that exhibit shallow recursions. As pointed out in [14], this practice often helps EBL find simpler and more compact explanations of its problem solver's 12 See <ref> [27, 46] </ref> for exceptions in certain special cases. 5 CHOOSING WHAT PROOFS TO LEARN FROM 23 behavior. However, in the case of recursive proofs, the practice can lead to a disparity between the shallow recursions in training examples and deep recursions on test cases. <p> Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., [5, 47, 52]) and partial evaluators (e.g., <ref> [60, 46, 27] </ref>) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method.
Reference: [47] <author> Jude W. Shavlik. </author> <title> Acquiring recursive concepts and iterative concepts with explanation based learning. </title> <journal> Machine Learning, </journal> <volume> 5(1), </volume> <year> 1990. </year>
Reference-contexts: It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation. <p> leads to exponential overhead due to EBL. 5.2.1 The Recursive Heuristic In Practice Experimental inquiries have shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle [26, 55], or on highly skewed problem distributions <ref> [47, 56] </ref>, but is ineffective in many other cases [10], even in the presence of generalization-to-N [52]. <p> Determining exactly how skewed the distribution of problems has to be for a problem solver to benefit from recursive EBL proofs depends on the specific search space, problem solver, and matcher involved. [52] estimated this parameter empirically using recursive macro rules learned by BAGGER2 <ref> [47] </ref> in a circuit synthesis domain. <p> Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., <ref> [5, 47, 52] </ref>) and partial evaluators (e.g., [60, 46, 27]) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. <p> This detailed model is motivated by their broader project which seeks to "quan-titatively estimate" the cost of inference. The formal analysis in their paper is supported by experiments using Shavlik's <ref> [47] </ref> circuit synthesis domain theory, providing further confirmation of the value of the recursive heuristic.
Reference: [48] <author> Jude W. Shavlik and G. F. DeJong. </author> <title> Building a computer model of classical mechanics. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 351-355, </pages> <year> 1985. </year>
Reference-contexts: As a result, when EBL's proofs are recursive, distinct rules are learned at every recursive depth to which the proofs are expanded <ref> [4, 43, 48] </ref>. For example, prodigy/ebl learns distinct Blocksworld control rules for clearing the bottom block of a two-block tower, a three-block tower, and so on. <p> It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation.
Reference: [49] <author> Peter Shell and Jaime G. Carbonell. </author> <title> Towards a general framework for composing dis junctive and iterative macro-operators. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <year> 1989. </year> <note> REFERENCES 44 </note>
Reference: [50] <author> Jeff Shrager, Tad Hogg, and Bernardo A. Huberman. </author> <title> A graph-dynamic model of the power law of practice and the problem-solving fan-effect. </title> <journal> Science, </journal> <volume> 242 </volume> <pages> 414-416, </pages> <year> 1988. </year>
Reference-contexts: The experiments, and their analysis, provide a degree of confirmation for the structural theory. 7 Related Work A number of attempts have been made to analyze EBL's performance <ref> [6, 21, 28, 30, 44, 50] </ref>.
Reference: [51] <author> David E. Smith. </author> <title> Controlling Inference. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <year> 1986. </year> <note> Available as technical report STAN-CS-86-1107. </note>
Reference-contexts: They report that, in general, finding the optimal inference strategy in a redundant search space is NP-hard. However, if a single EBL rule is added to a nonconjunctive, nonrecursive, and irredundant rule set, then a simple extension to Smith's algorithm <ref> [51] </ref> can still yield an optimal inference strategy in linear time. Most of the issues considered in this paper (e.g. recursion and complementary target concepts) do not arise in Greiner and Likuski's model. Subramanian and Feldman [52, 53] extend Greiner and Likuski's model to cover recursive and conjunctive rule sets.
Reference: [52] <author> Devika Subramanian and Ronen Feldman. </author> <title> The utility of EBL in recursive domain theories. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: When is control knowledge effective over the entire space? The most widely-used notion of effectiveness in the literature is average speedup. A set of control rules is said to be effective if it speeds up problem solving, on average, on a population of problems (e.g. <ref> [21, 32, 52] </ref>). Average speedup is relatively easy to test experimentally; measuring problem-solving time with and without a set of control rules, on a large, randomly generated sample of problems, indicates whether the set achieves average speedup or not [13]. <p> It follows that, unless 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well. 3.3.2 Generalization-to-N In some cases, the recursion-depth-specificity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref> [4, 5, 27, 43, 48, 47, 52] </ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a finite automaton or a context-free grammar), and construct a new recursion based on this representation. <p> That is precisely what the static program does. A complete description of static's algorithms appears in [11]. 4.3 Scope of the Structural Thesis Although the structural thesis applies to many existing EBL problem solvers (e.g., <ref> [27, 1, 52] </ref>) its scope is limited for a number of reasons. First, although most commonly-used target concepts (success, failure, etc.) map naturally to PSG subgraphs, other target concepts (e.g., state cycles) do not. Second, PSGs have only been developed for backward-chaining problem solvers. <p> shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle [26, 55], or on highly skewed problem distributions [47, 56], but is ineffective in many other cases [10], even in the presence of generalization-to-N <ref> [52] </ref>. Determining exactly how skewed the distribution of problems has to be for a problem solver to benefit from recursive EBL proofs depends on the specific search space, problem solver, and matcher involved. [52] estimated this parameter empirically using recursive macro rules learned by BAGGER2 [47] in a circuit synthesis domain. <p> distributions [47, 56], but is ineffective in many other cases [10], even in the presence of generalization-to-N <ref> [52] </ref>. Determining exactly how skewed the distribution of problems has to be for a problem solver to benefit from recursive EBL proofs depends on the specific search space, problem solver, and matcher involved. [52] estimated this parameter empirically using recursive macro rules learned by BAGGER2 [47] in a circuit synthesis domain. <p> Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., <ref> [5, 47, 52] </ref>) and partial evaluators (e.g., [60, 46, 27]) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. <p> One analysis <ref> [52, 53] </ref> independently reaches a conclusion that is closely related to the recursive heuristic, and is discussed at greater length. No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. <p> Most of the issues considered in this paper (e.g. recursion and complementary target concepts) do not arise in Greiner and Likuski's model. Subramanian and Feldman <ref> [52, 53] </ref> extend Greiner and Likuski's model to cover recursive and conjunctive rule sets.
Reference: [53] <author> Devika Subramanian and Ronen Feldman. </author> <title> The utility of EBL in recursive domain theories. </title> <note> An extended version of the AAAI 1990 paper., </note> <year> 1991. </year>
Reference-contexts: One analysis <ref> [52, 53] </ref> independently reaches a conclusion that is closely related to the recursive heuristic, and is discussed at greater length. No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. <p> Most of the issues considered in this paper (e.g. recursion and complementary target concepts) do not arise in Greiner and Likuski's model. Subramanian and Feldman <ref> [52, 53] </ref> extend Greiner and Likuski's model to cover recursive and conjunctive rule sets.
Reference: [54] <author> Devika Subramanian and Scott Hunter. </author> <title> Measuring utility and the design of provably good ebl algorithms. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Knowledge Assimilation, </booktitle> <address> Menlo Park, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The heuristics are attractive because they constitute an a priori basis for generating effective control knowledge. A number of researchers have developed post hoc mechanisms for evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. However, as argued in Section 5, these post hoc mechanisms are heuristic as well. In addition, generating the large samples typically required by these mechanisms is very costly. <p> Determining whether one block is above another, while recursive in the problem space, merely requires comparing X-coordinates in the domain theory. 5 CHOOSING WHAT PROOFS TO LEARN FROM 20 evaluating control knowledge by measuring its effectiveness on a sample of problems <ref> [19, 20, 32, 54] </ref>. The advantage of post hoc approaches is that they "consider" problem distribution, whereas a priori approaches do not. Sample-based approaches have several disadvantages, though. First, the complex interactions between different control rules mean that individual control rules cannot be tested in isolation. <p> When this is the case, the performance of a sample-based mechanism is arbitrarily bad. Large sample sizes ensure that the probability of this event is low but, to provide such guarantees, the mechanisms proposed in <ref> [19, 20, 54] </ref> typically require their problem solver to solve literally thousands of problems. Even the ad hoc rule-evaluation mechanism used by prodigy/ebl [32] accounts for a significant fraction of prodigy/ebl's learning time [10, chapter 8]. Finally, sample-based rule selection is distribution-dependent.
Reference: [55] <author> Prasad Tadepalli. </author> <title> A formalization of explanation-based macro-operator learning. </title> <booktitle> In Proceedings of the Twelveth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: As Section 3.3.1 indicates, such disparity leads to exponential overhead due to EBL. 5.2.1 The Recursive Heuristic In Practice Experimental inquiries have shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle <ref> [26, 55] </ref>, or on highly skewed problem distributions [47, 56], but is ineffective in many other cases [10], even in the presence of generalization-to-N [52]. <p> No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. Mahadevan, Natarajan, and Tadepalli [28] present a formal model of learning as improving problem-solving performance based on the framework in [39]. Based on the model, Tadepalli <ref> [55] </ref> argues that EBL relies on structural constraints (e.g. serial decomposability) and that, like inductive learning, EBL can be analyzed using Valiant's PAC framework [59].
Reference: [56] <author> M. Tambe, Newell A., and P. Rosenbloom. </author> <title> The problem of expensive chunks and its solution by restricting expressiveness. </title> <journal> Machine Learning Journal, </journal> <volume> 5(3) </volume> <pages> 299-348, </pages> <year> 1990. </year>
Reference-contexts: is exponential in s, the total overhead of matching t is exponential in s. 6 Unique-attribute rules are particularly susceptible to the problem of insufficient pruning because recoding a domain theory into unique attributes, to reduce the per-node match cost of learned rules, can lead EBL to produce highly-specific rules <ref> [56] </ref>). Another problem with recursive proofs is recursion-depth-specificity, the WP of a recursive proof only matches states that support the very same recursive expansion the WP was learned from. <p> To make this observation more precise 5 Roughly, unique attributes (also known as determinate literals) are predicates whose arguments have unique bindings thereby restricting the branching factor in their match tree to one <ref> [56] </ref>. The cost of matching a unique-attribute rule scales only linearly with rule length. 6 Paul Rosenbloom notes that even though the total overhead of t is exponential in s, the ratio ( + t ) t = is constant, so long as t is constant. <p> leads to exponential overhead due to EBL. 5.2.1 The Recursive Heuristic In Practice Experimental inquiries have shown that control knowledge learned from recursive proofs can be effective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle [26, 55], or on highly skewed problem distributions <ref> [47, 56] </ref>, but is ineffective in many other cases [10], even in the presence of generalization-to-N [52].
Reference: [57] <author> Milind Tambe and Paul Rosenbloom. </author> <title> Eliminating expensive chunks by restricting ex pressiveness. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: A ground literal is one that contains no variables. 2 META-LEVEL PROBLEM SOLVERS 7 rules typically reduce the number of nodes searched but increase the cost of expanding each node, measured by the total number of elementary matching operations <ref> [57] </ref>. Consequently, control rules do not necessarily reduce problem-solving time. <p> Consequently, this paper will consider both criteria explicitly. 2.4 The Cost of Matching Control Rules This section shows that the cost of matching a conjunctive logical expression against a problem solver's state, using standard match algorithms, is exponential in the expression's length <ref> [57] </ref>. 3 This observation is invoked repeatedly in the analysis that follows. Consider the tree generated by the matching process. A node in the tree represents an unbound variable in the expression being matched, and a branch in the tree represents a potential binding for the variable.
Reference: [58] <author> Jeffrey D. Ullman. </author> <title> Database and Knowledge-base systems, volume II. </title> <publisher> Computer Science Press, </publisher> <year> 1989. </year>
Reference-contexts: An operator has an and-link to a literal if and only if the literal is a precondition to the operator. 4.2 The Correspondence of PSGs to EBL's Proofs The notion of representing programs (or problem spaces) as graphs is well-known in computer science (see, for example, <ref> [24, 58] </ref>). It is interesting to note, however, that prodigy/ebl's proofs correspond to PSG subgraphs. 10 prodigy/ebl's failure proofs, for example, have the following flavor: an operator cannot be executed because one of its preconditions cannot be achieved.
Reference: [59] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <year> 1984. </year>
Reference-contexts: Based on the model, Tadepalli [55] argues that EBL relies on structural constraints (e.g. serial decomposability) and that, like inductive learning, EBL can be analyzed using Valiant's PAC framework <ref> [59] </ref>. Although Tadepalli's model is different from my own, the spirit of the two approaches is similar: we both seek to identify classes of problem spaces in which EBL results in tractable problem solving.
Reference: [60] <author> Frank van Harmelen and Alan Bundy. </author> <title> Explanation-based generalisation = partial eval uation. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <year> 1988. </year> <note> Research Note. </note>
Reference-contexts: Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful. Most existing EBL systems (e.g., [5, 47, 52]) and partial evaluators (e.g., <ref> [60, 46, 27] </ref>) do not use complementary target concepts. Thus, as Minton [30] argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method.
References-found: 60

