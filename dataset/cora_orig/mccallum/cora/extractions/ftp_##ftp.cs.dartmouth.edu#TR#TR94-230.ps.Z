URL: ftp://ftp.cs.dartmouth.edu/TR/TR94-230.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR94-230/
Root-URL: http://www.cs.dartmouth.edu
Title: A Multiprocessor Extension to the Conventional File System Interface  
Author: Nils Nieuwejaar David Kotz 
Address: College, Hanover, NH 03755-3551  
Affiliation: Department of Computer Science Dartmouth  
Note: Available at  
Pubnum: PCS-TR94-230  
Email: fnils,dfkg@cs.dartmouth.edu  
Date: September 14, 1994  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR94-230.ps.Z  
Abstract: As the I/O needs of parallel scientific applications increase, file systems for multiprocessors are being designed to provide applications with parallel access to multiple disks. Many parallel file systems present applications with a conventional Unix-like interface that allows the application to access multiple disks transparently. By tracing all the activity of a parallel file system in a production, scientific computing environment, we show that many applications exhibit highly regular, but non-consecutive I/O access patterns. Since the conventional interface does not provide an efficient method of describing these patterns, we present an extension which supports strided and nested-strided I/O requests. 
Abstract-found: 1
Intro-found: 1
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: So, while the simple Unix-like interface has worked well in the past, it is clear that it is not well suited to parallel applications, which have more complicated access patterns. One extension to the conventional interface offered by several multiprocessor file systems is a shared file pointer <ref> [Pie89, BGST93] </ref>. This provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174).
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ouster-hout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: A common characteristic of many file system workloads, particularly scientific file system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node.
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <year> 1993. </year>
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This reordering of data transfers can be used to achieve remarkable performance gains [Kot94]. 5 Unconventional interfaces 5.1 nCUBE The file system interface available on the nCUBE is based on a two-step mapping of a file into the compute node memories <ref> [DdR92] </ref>. The first step is to provide a mapping from subfiles stored on multiple disks to an abstract dataset (a traditional one-dimensional I/O stream). The second step is mapping the abstract dataset into the compute node memories.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. Currently available as Dartmouth College technical report PCS-TR94-211. </note>
Reference-contexts: In <ref> [KN94] </ref>, we discuss the results of a tracing study in which all file-related activity on a massively parallel computer was recorded. Unlike previous studies of parallel file systems, we traced information about every I/O request. <p> It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Results in <ref> [KN94] </ref>, however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. So, while the simple Unix-like interface has worked well in the past, it is clear that it is not well suited to parallel applications, which have more complicated access patterns. <p> The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Intel's CFS provides this in addition to several more structured access modes (e.g., round robin access to the file pointer) [Pie89]. However, the tracing study described in <ref> [KN94] </ref> found that CFS's shared file pointers are rarely used in practice and suggests that poor performance and a failure to match the needs of applications are the likely causes. 3 Access Patterns As in [KN94] we define a sequential request to be one that is at a higher file offset <p> However, the tracing study described in <ref> [KN94] </ref> found that CFS's shared file pointers are rarely used in practice and suggests that poor performance and a failure to match the needs of applications are the likely causes. 3 Access Patterns As in [KN94] we define a sequential request to be one that is at a higher file offset than the previous request from the same compute node, and a consecutive request to be a sequential request that begins where the previous request ended. <p> We define an interval to be the distance between the end of one access and the beginning of the next. While the study described in <ref> [KN94] </ref> shows that almost 99% of all files are accessed with fewer than 3 different intervals, that study made no distinction between single-node and multinode files. Looking 2 accesses that were involved in a simple-strided pattern. <p> Since many models of physical events require logically adjacent nodes to share boundary information, this could be an important restriction. This can be seen in the file-sharing results in <ref> [KN94] </ref> which show that most read-only files had at least some bytes that were accessed by multiple processors.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994Sym-posium on Operating Systems Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: we examined could have been reduced from 25,358,601 to 81,103 a reduction of over 99%. 1 Not only would reducing the number of requests lower the aggregate latency costs, but recent work has shown that providing a file system with this level of information can lead to tremendous performance improvements <ref> [Kot94] </ref>. <p> This allows the file system the option of transferring the data from the disk to the I/O node and from the I/O node to the local buffer in the most efficient order rather than strictly sequentially. This reordering of data transfers can be used to achieve remarkable performance gains <ref> [Kot94] </ref>. 5 Unconventional interfaces 5.1 nCUBE The file system interface available on the nCUBE is based on a two-step mapping of a file into the compute node memories [DdR92].
Reference: [LMKQ89] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: This provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in <ref> [LMKQ89] </ref>, page 174). Intel's CFS provides this in addition to several more structured access modes (e.g., round robin access to the file pointer) [Pie89].
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Experience has shown that this simple model of a file is well suited to uniprocessor applications that tend to access files in a simple, sequential fashion [OCH + 85]. It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially <ref> [MK91] </ref>. Results in [KN94], however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. <p> A common characteristic of many file system workloads, particularly scientific file system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node.
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The interface is limited to such operations as open, close, read, write, and seek. Experience has shown that this simple model of a file is well suited to uniprocessor applications that tend to access files in a simple, sequential fashion <ref> [OCH + 85] </ref>. It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Results in [KN94], however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. <p> A common characteristic of many file system workloads, particularly scientific file system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file system workload, we found that while almost 93% of all files were accessed sequentially, consecutive access was primarily limited to those files that were only opened by one compute node.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year> <month> 11 </month>
Reference-contexts: So, while the simple Unix-like interface has worked well in the past, it is clear that it is not well suited to parallel applications, which have more complicated access patterns. One extension to the conventional interface offered by several multiprocessor file systems is a shared file pointer <ref> [Pie89, BGST93] </ref>. This provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). <p> The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Intel's CFS provides this in addition to several more structured access modes (e.g., round robin access to the file pointer) <ref> [Pie89] </ref>.
References-found: 12

