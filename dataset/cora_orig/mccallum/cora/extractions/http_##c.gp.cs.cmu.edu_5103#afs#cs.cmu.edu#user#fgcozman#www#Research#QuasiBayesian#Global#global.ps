URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/fgcozman/www/Research/QuasiBayesian/Global/global.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/fgcozman/www/Research/QuasiBayesian/Global/
Root-URL: http://www.cs.cmu.edu
Title: Robustness Analysis of Bayesian Networks with Global Neighborhoods  
Author: Fabio Cozman CMU-RI-TR - 
Note: This research is supported in part by NASA under Grant NAGW-1175. Fabio Cozman was supported under a scholarship from CNPq, Brazil.  
Date: January 23, 1997  
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: This paper presents algorithms for robustness analysis of Bayesian networks with global neighborhoods. Robust Bayesian inference is the calculation of bounds on posterior values given perturbations in a probabilistic model. We present algorithms for robust inference (including expected utility, expected value and variance bounds) with global perturbations that can be modeled by *-contaminated, constant density ratio, constant density bounded and total variation classes of distributions. c fl1996 Carnegie Mellon University
Abstract-found: 1
Intro-found: 1
Reference: [ Berger & Moreno, 1994 ] <author> Berger, J., and Moreno, E. </author> <year> 1994. </year> <title> Bayesian robustness in bidimensional model: Prior independence. </title> <journal> Journal of Statistical Planning and Inference 40 </journal> <pages> 161-176. </pages>
Reference-contexts: In general, neighborhoods of a Bayesian network do not preserve all the decompositions in the original joint distribution <ref> [ Berger & Moreno, 1994; Walley, 1991 ] </ref> . This fact has led to considerable discussion in the literature [ Chrisman, 1996a; Seidenfeld & Wasserman, 1993 ] .
Reference: [ Berger, 1985 ] <author> Berger, J. O. </author> <year> 1985. </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. This paper focuses on perturbations that are imposed on the global structure of a Bayesian network. <p> The expressions indicate how to obtain upper and lower expectations for u () of this form. 4 *-contaminated global neighborhoods An *-contaminated class is characterized by a distribution p () and a real number * 2 (0; 1) <ref> [ Berger, 1985 ] </ref> : C where q () is an arbitrary distribution.
Reference: [ Berger, 1990 ] <author> Berger, J. O. </author> <year> 1990. </year> <title> Robust bayesian analysis: Sensitivity to the prior. </title> <journal> Journal of Statistical Planning and Inference 25 </journal> <pages> 303-328. </pages>
Reference: [ Breese & Fertig, 1991 ] <author> Breese, J. S., and Fertig, K. W. </author> <year> 1991. </year> <title> Decision making with interval influence diagrams. </title> <editor> In Bonissone, P. P.; Henrion, M.; Kanal, L. N.; and Lemmer, J. F., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 6. </booktitle> <publisher> North-Holland: Elsevier Science. </publisher> <pages> 467-478. </pages>
Reference: [ Cannings & Thompson, 1981 ] <author> Cannings, C., and Thompson, E. A. </author> <year> 1981. </year> <title> Genealogical and Genetic Structure. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: [ Cano, Delgado, & Moral, 1993 ] <author> Cano, J.; Delgado, M.; and Moral, S. </author> <year> 1993. </year> <title> An axiomatic framework for propagating uncertainty in directed acyclic networks. </title> <journal> International Journal of Approximate Reasoning 8 </journal> <pages> 253-280. </pages>
Reference: [ Chrisman, 1996a ] <author> Chrisman, L. </author> <year> 1996a. </year> <title> Independence with lower and upper probabilities. </title> <booktitle> Proc. Twelfth Conference Uncertainty in Artificial Intelligence 169-177. </booktitle>
Reference-contexts: In general, neighborhoods of a Bayesian network do not preserve all the decompositions in the original joint distribution [ Berger & Moreno, 1994; Walley, 1991 ] . This fact has led to considerable discussion in the literature <ref> [ Chrisman, 1996a; Seidenfeld & Wasserman, 1993 ] </ref> .
Reference: [ Chrisman, 1996b ] <author> Chrisman, L. </author> <year> 1996b. </year> <title> Propagation of 2-monotone lower probabilities on an undirected graph. </title> <booktitle> Proc. Twelfth Conference Uncertainty in Artificial Intelligence 178-186. </booktitle>
Reference-contexts: However, this does not preclude the existence of models that are amenable to efficient analysis (for example, Markov-like lower probabilities <ref> [ Chrisman, 1996b ] </ref> ). This paper proposes global models that lead to such efficient exact analysis. Section 2 introduces the theory of inferences which validates our procedures.
Reference: [ Cozman, 1996 ] <author> Cozman, F. </author> <year> 1996. </year> <title> Robust analysis of bayesian networks with finitely generated convex sets of distributions. </title> <type> Technical Report CMU-RI-TR96-41, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: We concentrate on bounds for marginal probabilities, expectations and variances. No efficient exact solutions are available for calculation of posterior bounds in arbitrary Bayesian networks with perturbations modeled by sets of probability distributions (the algorithmic challenge presented by robust Bayesian inference is discussed in a companion technical report <ref> [ Cozman, 1996 ] </ref> ). However, this does not preclude the existence of models that are amenable to efficient analysis (for example, Markov-like lower probabilities [ Chrisman, 1996b ] ). This paper proposes global models that lead to such efficient exact analysis. <p> Motivation for the use of this theory in inferences, and a summary of the theory, is available in a companion technical report <ref> [ Cozman, 1996 ] </ref> . Here we outline the chief ideas behind Quasi-Bayesian theory. Imprecision in probability assessments can be due either to difficulties in eliciting information from experts, or to difficulties in processing or combining data [ Walley, 1991 ] . <p> A general solution for inferences with local neighborhoods in a Bayesian network has been given in a companion technical report, for most of the known neighborhoods used in robust Statistics <ref> [ Cozman, 1996 ] </ref> . Here we depart from this local model and seek global neighborhoods for Bayesian networks, which are appropriate to model global perturbations such as the effect of invalid independence assumptions. We concentrate on neighborhoods (p ( ~ )) for the joint distribution. <p> neighborhoods in the space of all Bayesian networks: C ( Y p i + *q (~x) : The posterior expected value for u (~x) is: E [u] = p (e) where: U (e) = x62e p (e) = x62e An *-contaminated class is a finitely generated convex set of distributions <ref> [ Cozman, 1996 ] </ref> . The vertices of this set are unitary point masses on each one of the possible configurations of the network. The maximum and minimum expected values for u (~x) occur in these vertices, since we are optimizing a linear function over a convex set. <p> We consider calculation of the upper bound; the lower bound can be obtained through similar methods. The constant density bounded class is invariant to marginalization, but not to conditionalization [ Wasser-man & Kadane, 1992 ] . To obtain posterior bounds, we must resort to Lavine's algorithm <ref> [ Cozman, 1996 ] </ref> . The algorithm brackets the value of the posterior upper expectation of u () by successive calculations of a prior upper expectation: E k [u] = max 4 x62e (u (x) k) i i 5 ; where k is a given real number.
Reference: [ Dechter, 1996 ] <author> Dechter, R. </author> <year> 1996. </year> <title> Bucket elimination: A unifying framework for probabilistic inference. </title> <booktitle> Proc. of the Twelfth Conference Uncertainty in Artificial Intelligence 211-219. </booktitle>
Reference: [ DeRobertis & Hartigan, 1981 ] <author> DeRobertis, L., and Hartigan, J. A. </author> <year> 1981. </year> <title> Bayesian inference using intervals of measures. </title> <journal> The Annals of Statistics 9(2) </journal> <pages> 235-244. </pages>
Reference-contexts: the elements of p (x q ; e) which are required in this expression [ Cannings & Thompson, 1981; Dechter, 1996; Zhang & Poole, 1996 ] . 5 Constant density ratio global neighborhoods A density ratio class consists of all probability densities p (A) so that for any event A <ref> [ DeRobertis & Hartigan, 1981 ] </ref> : l;u (p (A)) = fr (A) : l (A) ffr (A) u (A)g ; where l (A) and u (A) are arbitrary positive measures such that l () u () and ff is some positive real number.
Reference: [ Giron & Rios, 1980 ] <author> Giron, F. J., and Rios, S. </author> <year> 1980. </year> <title> Quasi-bayesian behaviour: A more realistic approach to decision making? In Bernardo, </title> <editor> J. M.; DeG-root, J. H.; Lindley, D. V.; and Smith, A. F. M., eds., </editor> <booktitle> Bayesian Statistics. </booktitle> <address> Valencia, Spain: </address> <publisher> University Press. </publisher> <pages> 17-38. </pages>
Reference-contexts: Section 8 extends the analysis to variance bounds. Section 9 discusses the relationship between global neighborhoods and independence relations in a Bayesian network. 2 Quasi-Bayesian theory We use the theory of convex sets of probability distributions, referred to as Quasi-Bayesian theory <ref> [ Giron & Rios, 1980 ] </ref> . Motivation for the use of this theory in inferences, and a summary of the theory, is available in a companion technical report [ Cozman, 1996 ] . Here we outline the chief ideas behind Quasi-Bayesian theory.
Reference: [ Huber, 1980 ] <author> Huber, P. J. </author> <year> 1980. </year> <title> Robust Statistics. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. This paper focuses on perturbations that are imposed on the global structure of a Bayesian network.
Reference: [ Kadane, 1984 ] <author> Kadane, J. B. </author> <year> 1984. </year> <title> Robustness of Bayesian Analyses, volume 4 of Studies in Bayesian econometrics. </title> <address> New York: </address> <publisher> Elsevier Science Pub. Co. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. This paper focuses on perturbations that are imposed on the global structure of a Bayesian network.
Reference: [ Lavine, 1991 ] <author> Lavine, M. </author> <year> 1991. </year> <title> Sensitivity in bayesian statistics, the prior and the likelihood. </title> <journal> Journal of the American Statistical Association 86(414) </journal> <pages> 396-399. </pages>
Reference-contexts: lj The value u (l) , used here and in the next sections, is the lth value of u (X j ) as the N values are ordered from smallest to largest. 6 Constant density bounded global neighborhoods A density bounded class is the set of all distributions such that <ref> [ Lavine, 1991 ] </ref> B where l () and u () are arbitrary non-negative functions such that P P A global neighborhood can be constructed with a sub-class of the density bounded class. Take a base Bayesian network and a positive constant k &gt; 1.
Reference: [ Pearl, 1988 ] <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Consider a set ~x of discrete variables with a finite number of values. A Bayesian network defines a probability distribution through the expression <ref> [ Pearl, 1988 ] </ref> : Y p (x i jpa (x i )); (1) where pa (x i ) is a set of variables, the parents of variable x i . For any function f (), we use the abbreviation f i for f (x i jpa (x i )).
Reference: [ Seidenfeld & Wasserman, 1993 ] <author> Seidenfeld, T., and Wasserman, L. </author> <year> 1993. </year> <title> Dilation for sets of probabilities. </title> <journal> The Annals of Statistics 21(9) </journal> <pages> 1139-1154. </pages>
Reference-contexts: In this case E [u] = p (x q = aje), the posterior probability for x q . The linear programming problem can be solved in closed-form <ref> [ Seidenfeld & Wasserman, 1993 ] </ref> : r (x q = aje) = kp (x q = aje) + p (x q = a c je) r (x q = aje) = p (x q = aje) + kp (x q = a c je) The linear programming problem above can <p> In general, neighborhoods of a Bayesian network do not preserve all the decompositions in the original joint distribution [ Berger & Moreno, 1994; Walley, 1991 ] . This fact has led to considerable discussion in the literature <ref> [ Chrisman, 1996a; Seidenfeld & Wasserman, 1993 ] </ref> .
Reference: [ Shenoy & Shafer, 1990 ] <author> Shenoy, P. P., and Shafer, G. </author> <year> 1990. </year> <title> Axioms for probability and belief-function propagation. </title> <editor> In Shachter, R. D.; Levitt, T. S.; Kanal, L. N.; and Lemmer, J. F., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <publisher> North-Holland: Elsevier Science Publishers. </publisher> <pages> 169-198. </pages>
Reference: [ Tessem, 1992 ] <author> Tessem, B. </author> <year> 1992. </year> <title> Interval probability propagation. </title> <journal> International Journal of Approximate Reasoning 7 </journal> <pages> 95-120. </pages>
Reference: [ Walley, 1991 ] <author> Walley, P. </author> <year> 1991. </year> <title> Statistical Reasoning with Imprecise Probabilities. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Here we outline the chief ideas behind Quasi-Bayesian theory. Imprecision in probability assessments can be due either to difficulties in eliciting information from experts, or to difficulties in processing or combining data <ref> [ Walley, 1991 ] </ref> . This imprecision is modeled by convex sets of distributions, called credal sets. To simplify terminology, we use the term credal set only when it refers to a set of distributions containing more than one element. <p> To produce a convergent algorithm for calculation of lower and upper variances, we can use Walley's vari ance envelope theorem <ref> [ Walley, 1991, Theorem G2 ] </ref> , which demonstrates that V [x q ] = min (E [(x q ) 2 ]) and V [x q ] = min (E [(x q ) 2 ]) . <p> In general, neighborhoods of a Bayesian network do not preserve all the decompositions in the original joint distribution <ref> [ Berger & Moreno, 1994; Walley, 1991 ] </ref> . This fact has led to considerable discussion in the literature [ Chrisman, 1996a; Seidenfeld & Wasserman, 1993 ] .
Reference: [ Wasserman & Kadane, 1992 ] <author> Wasserman, L., and Kadane, J. B. </author> <year> 1992. </year> <title> Computing bounds on expectations. </title> <journal> Journal of the American Statistical Association 87(418) </journal> <pages> 516-522. </pages>
Reference-contexts: Evidence e refers to a set of values for some variables in the network. From a robustness perspective, it is convenient to understand sets of probability distributions as neighborhoods for base probability distributions. Call (p) the neighborhood of distribution p () <ref> [ Wasserman & Kadane, 1992 ] </ref> ; (p) is a convex set of distributions induced by p (). <p> Consider a sample of the posterior distribution p (~xje) with N elements X j , which can be produced through Gibbs sampling techniques [ York, 1992 ] . The following expression converges to the upper expectation of a function u () <ref> [ Wasserman & Kadane, 1992 ] </ref> : max 1 + (1 (j=N ))(k 1) (k 1)Z j + N : where Z 0 = j Z j = lj The value u (l) , used here and in the next sections, is the lth value of u (X j ) as <p> In this case a Monte Carlo sampling procedure can be applied to the problem <ref> [ Wasserman & Kadane, 1992 ] </ref> . Consider a sample of p (~x) with N elements X j . <p> In this case a Monte Carlo sampling procedure can be applied to the problem <ref> [ Wasserman & Kadane, 1992 ] </ref> . Consider a sample of p (~x) with N elements X j .
Reference: [ Wasserman, 1990 ] <author> Wasserman, L. A. </author> <year> 1990. </year> <title> Prior envelopes based on belief functions. </title> <journal> The Annals of Statistics 18(1) </journal> <pages> 454-464. </pages>
Reference-contexts: In this case E [u] = p (x q = aje), the posterior probability for x q . The linear programming problem can be solved in closed-form <ref> [ Wasserman, 1990 ] </ref> : 0 X p e (x q ) A 0 X p e (x q ) A : The linear programming problem above can be intractable if x has too many variables. <p> In this case E [u] = p (x q = aje), the posterior probability for x q . The linear programming problem can be solved in closed-form <ref> [ Wasserman, 1990 ] </ref> : r e (x q = a) = max (0; p e (x q = a) *) : The linear programming problem above can be intractable if x has too many variables.
Reference: [ Wasserman, 1992a ] <author> Wasserman, L. </author> <year> 1992a. </year> <title> Invariance properties of density ratio priors. </title> <journal> The Annals of Statistics 20(4) </journal> <pages> 2177-2182. </pages>
Reference-contexts: This class is invariant to marginalization and application Bayes rule; in fact it is the only class that has these two properties <ref> [ Wasserman, 1992a ] </ref> .
Reference: [ Wasserman, 1992b ] <author> Wasserman, L. </author> <year> 1992b. </year> <title> Recent methodological advances in robust bayesian inference. </title> <editor> In Bernardo, J. M.; Berger, J. O.; Dawid, A. P.; and Smith, A. F. M., eds., </editor> <booktitle> Bayesian Statistics 4. </booktitle> <publisher> Oxford University Press. </publisher> <pages> 483-502. </pages>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. This paper focuses on perturbations that are imposed on the global structure of a Bayesian network.
Reference: [ York, 1992 ] <author> York, J. </author> <year> 1992. </year> <title> Use of the gibbs sampler in expert systems. </title> <booktitle> Artificial Intelligence 56 </booktitle> <pages> 115-130. </pages>
Reference-contexts: In this case a Gibbs sampling procedure can be applied to the problem. Consider a sample of the posterior distribution p (~xje) with N elements X j , which can be produced through Gibbs sampling techniques <ref> [ York, 1992 ] </ref> .
Reference: [ Zhang & Poole, 1996 ] <author> Zhang, N. L., and Poole, D. </author> <year> 1996. </year> <title> Exploiting causal independence in Bayesian network inference. </title> <journal> Journal of Artificial Intelligence Research 301-328. </journal>
References-found: 26

