URL: ftp://ftp.cs.wisc.edu/wwt/hw_sw_sm.ps
Refering-URL: http://www.cs.wisc.edu/~wwt/wwt_papers.html
Root-URL: 
Title: Compiling for Shared-Memory and Message-Passing  
Author: James R. Larus 
Keyword: General Terms: Languages, performance. Additional Key W ords and Phrases: Compilers, parallel programming languages, shared-memory multiprocessors, message-passing multiprocessors, memory systems, cache coherence, and directory protocols.  
Note: 1.0 Introduction hardware. The semantics of most programming languages presume a shared address space in which any portion of a computation can reference any datum. Many parallel computers, how  
Address: 1210 West Dayton St. Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  
Pubnum: Computer 1  
Email: larus@cs.wisc.edu  
Date: November 12, 1993  
Abstract: Many parallel languages presume a shared address space in which any portion of a computation can access any datum. Some parallel computers directly support this abstraction with hardware shared memory . Other computers provide distinct (per processor) address spaces and communication mechanisms on which software can construct a shared address space. Since programmers have difficulty explicitly managing address spaces, there is considerable interest in compiler support for shared address spaces on the widely available message-passing computers. At first glance, it might appear that hardware-implemented shared memory is unquestionably a better base on which to implement a language. This paper argues, however, that compiler implemented shared memory , despite its shortcomings, has the potential to exploit more effectively the resources in a parallel computer. Hardware designers need to find mechanisms to combine the advantages of both approaches in a single system. Categories and Subject Descriptors: B.3.2: [ Memory Structur es]: Design Stylesshared memory ; C.1.2: [ Processor Ar chitectures]: Multiple Data Stream Architectures (Multiprocessors)multiple-instruction-stream, multiple-data-stream (MIMD), parallel processors; D.1.3: [Programming Techniques]: Concurrent programmingparallel programming; D.3.4 [ Programming Languages]: Processorscompilers, optimization, runtime environments. This paper examines the implicationsfor compiler writers and hardware designersof implementing a programs shared address space on computers with and without shared-memory
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve, Vikram S. Adve, Mark D. Hill, and Mary K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298308, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In best-case CISM, a compiler also knows which processors have out-of-date copies and can produce code to update these copies. Updating typically requires less message traffic than invalidation, which is used by directory-systems protocols. Compilers lack of success in software-controlled caching <ref> [1] </ref> suggests that the sophistication of message-passing compilation strategies will be limited by static program analysis. Software-controlled caching requires a compiler to identify interprocessor data dependences and insert code to invalidate an out-of-date cache block before it is accessed. <p> In both cases, compilers must produce conservative code that preserves a program s semantics for all possible executions. Because static program analysis is necessarily imprecise, compilers make pessimistic assumptions. With software-controlled caching, conservative analysis resulted in more memory-system traffic and lower performance than cache hardware <ref> [1] </ref>. 3.8 Cost Modeling An important difference between the two forms of shared memory is a programmers or compiler writers ability to model the cost of a memory reference.
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering - A New Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: T o achieve higher performance, proponents of alternative memory semantics have suggested weakening consistency guarantees so they hold only at defined points in a programs execution <ref> [2] </ref>. Message-passing systems rely on a compiler to ensure that a program will not find itself in a situation in which two processors believe that dif ferent values are current.
Reference: [3] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104114, </pages> <month> June </month> <year> 1990. </year>
Reference: [4] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280289, </pages> <year> 1988. </year>
Reference-contexts: Compiling for Shared-Memory and Message-Passing Computer 6 The primary logical difference between these systems and message-passing computers is that the memory, which is physically distributed in both, is referenced in a single address space in shared-memory computers (see Figure 2). The principal hardware change is the addition of directories <ref> [4] </ref>. The directory on node i maintains information on which nodes have copies of memory locations whose home is node i.
Reference: [5] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 16, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: To ensure a producer-consumer relationship, a compiler must insert synchronization. Techniques exist that combine synchronization with shared memory . One alternative is fine-grained synchronization such as empty-full bits, such as those in the Tera processor <ref> [5] </ref>. This feature complicates the system and memory. In a sense, message passing provides the equivalent of fine-grained synchronization with little overhead when many values can be packed into a message.
Reference: [6] <author> C. Gordon Bell. Multis: </author> <title> A New Class of Multiprocessor Computers. </title> <booktitle> Science, </booktitle> <address> 228:462466, </address> <year> 1985. </year> <title> Compiling for Shared-Memory and Message-Passing Computer 13 </title>
Reference-contexts: The limitations of existing systems reect implementation shortcomings rather than intrinsic limits on shared-memory hardware. These limitations are becoming less severe. For example, directories [4,27] eliminate the need to broadcast and so free shared memory from the processor limitations of bus-based Multis <ref> [6] </ref>. Newer directory systems, such as Dir 1 SW [18,38], reduce the complexity of directories and provide rudimentary facilities to improve performance by enabling software to inform the memory system of upcoming program behavior . MIT Alewife goes further and exposes the underlying message-passing mechanisms [24]. <p> Caches can reduce effective memory access time and communication network bandwidth since interprocessor communication occurs when a block is brought in and ushed from a cache. Cache coherence protocols keep cached copies consistent as processors modify memory locations. Hardware for coherence distinguish classes of parallel computers. Multis <ref> [6] </ref> are bus-based multiprocessors in which all processors watch memory accesses over a shared bus and modify their caches appropriately .
Reference: [7] <author> Zeik Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for Distributed Memory MIMD Computers. </title> <type> Technical Report SCCS-444, </type> <institution> Syracuse University, </institution> <month> March </month> <year> 1993. </year>
Reference: [8] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2:151169, </volume> <year> 1988. </year>
Reference-contexts: The common approach to compiling for these machines is to distribute data (arrays) among processors local memories according to programmer supplied directives, and to partition a computation among processors in a way that reduces communication <ref> [8] </ref>. High-Performance Fortran (HPF), for example, provides a programmer with template, align, and distribute directives to distribute arrays across processors [29]. Although a programmer usually partitions data, the compiler partitions the computation among processors.
Reference: [9] <author> Siddhartha Chatterjee, John R. Gilbert, Robert Schreiber, and Shang-Hua Teng. </author> <title> Automatic Array Alignment in Data-Parallel Programs. </title> <booktitle> In Conference Record of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1628, </pages> <month> January </month> <year> 1993. </year>
Reference: [10] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Thread Abstract Machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 164175, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This buffering requires copying and may require memory allocations as well. Compiler analysis that collects together messages exacerbates the problem by increasing message size. Other approaches, such as Active Messages <ref> [10] </ref>, eliminate buffering in the message-passing substrate but require application-level buf fering to avoid having values change unexpectedly because of an arriving message. HISM systems provide few mechanisms for optimizing remote accesses. One of the most common, nonbinding prefetch, asynchronously moves a remote block into a processor s cache.
Reference: [11] <author> William J. Dally, Andrew Chien, Stuart Fiske, Waldemar Horwat, John Keen, Michael Larivee, Rich Nuth, Scott Wills, Paul Carrick, and Greg Flyer. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <editor> In G. X. Ritter, editor, </editor> <booktitle> Proc. Information Processing 89. </booktitle> <publisher> Elsevier North-Holland, Inc., </publisher> <year> 1989. </year>
Reference: [12] <author> Susan J. Eggers and Randy H. Katz. </author> <title> A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373382, </pages> <year> 1988. </year>
Reference-contexts: The request is short and only returns a cache block. Moreover , this pair of messages incurs a round-trip latency . Increasing cache block size increases the data per message, but may also increase coherence traf fic because of problems such as false sharing <ref> [12] </ref>. The second difficulty is that limited cache size and associativity makes it impossible to transfer and buffer large amounts of data. Moving a lar ge quantity of data into a cache can evict useful data. Best-case CISM alleviates these problems, but new ones arise.
Reference: [13] <author> Dennis Gannon, William Jalby, and K. Gallivan. </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5:587616, </volume> <year> 1988. </year>
Reference: [14] <author> Hans Michael Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Rheinischen Friedrich-Wilhelms-Universitt, </institution> <year> 1989. </year>
Reference: [15] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 6477, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This feature complicates the system and memory. In a sense, message passing provides the equivalent of fine-grained synchronization with little overhead when many values can be packed into a message. Another alternative is Queue on Lock Bit (QOLB) <ref> [15] </ref>, which provides a lock for a cache block, maintains a queue of waiting processors, and ensures that the block is in a processor s cache when it acquires the lock.
Reference: [16] <author> M. Gupta, S. Midkiff, E. Schonberg, P. Sweeney, K.Y. Wang, and M. Burke. </author> <title> Ptran II - A Compiler for High-Performance Fortran. </title> <booktitle> In 4th Workshop on Compilers for Parallel Computers, </booktitle> <address> page ?, Delff, Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference: [17] <author> Mary W. Hall, Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Interprocedural Compilation of Fortran D for MIMD Distributed-Memory Machines. </title> <booktitle> In Proceedings of Supercomputing 92, </booktitle> <pages> pages 522534, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: However, communication optimizations, such as message vectorization and pipelin-ing, are difficult to perform since the compiler is unaware of the communication pattern. Compiling for Shared-Memory and Message-Passing Computer 5 Preliminary measurements suggest that widespread use of runtime resolution is impractical. Hall et al. <ref> [17] </ref> implemented a parallel version of DAXPY in the DGEF A (Gaussian Elimination) routine from the Linpack Benchmarks. On a 32 processor Intel iPSC/860 computer , the runtime resolution version ran 130180 times slower than the sequential code. <p> Most compiler work to-date has focused on dense matrix codes. Even for these programs, the analysis is complex and must be applied interprocedurally since partial information does not result in a gradual performance degradation but prevents parallelization <ref> [17] </ref>. Programs that contain indirect references, dynamic data structures, or pointers cannot be accurately analyzed and so will perform poorly. Worst case HISM behavior arises because of inef fective cache usage, bad data distribution, and false sharing. <p> Worst case HISM behavior arises because of inef fective cache usage, bad data distribution, and false sharing. It is unclear what is the difference between best and worst case behavior, but it is unlikely to approach the factor of 1,000 between best-case and runtime resolution CISM <ref> [17] </ref>. Compiling for Shared-Memory and Message-Passing Computer 12 4.0 Conclusion We can briey summarize the main advantages of compiler implemented shared memory over hardware.
Reference: [18] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):?, </volume> <month> November </month> <year> 1993. </year> <note> Preliminary version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: At the other extreme is Stanford DASH [27], which implements a complex directory protocol entirely in hardware and transfers 32-byte cache blocks. Cooperative shared memory <ref> [18] </ref> is an alternative approach that couples a simple directory protocol (Dir 1 SW) with a programming model and a collection of performance-improving memory system directives .
Reference: [19] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):16121630, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: A capacity miss is a replacement that would not have occurred in an infinite cache and a conict miss is a replacement that would not have occurred in a fully-associative cache of the same size <ref> [19] </ref>. The latter misses are particularly dif ficult to Compiling for Shared-Memory and Message-Passing Computer 9 anticipate since they depend on the relative addresses of the locations accessed after the original reference.
Reference: [20] <author> W. Daniel Hillis and Lewis W. Tucker. </author> <title> The CM-5 Connection Machine: A Scalable Supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11):3140, </volume> <month> November </month> <year> 1993. </year>
Reference: [21] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8):6680, </volume> <month> August </month> <year> 1992. </year>
Reference: [22] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: A cache-coherence protocol uses the directory to serialize conicting updates and to invalidate copies at updates. COMA machines, such as the KSR-1 <ref> [22] </ref>, or ganize all of their memory as a cache. It is unclear how the complex COMA hardware performs or scales [36], so this paper concentrates on the more proven directory technology. This section briey describes cache-coherent memory systems based on directories.
Reference: [23] <author> Charles Koelbel, Piyush Mehrotra, and John Van Rosendale. </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 177186, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Admittedly, these numbers represent a single datapoint from an immature system, but the magnitude of the improvement necessary to come close to breaking even with the sequential code is striking. Inspector executor overheads are much smaller and the speedups for programs that fit the model are close to linear <ref> [23] </ref>. 2.2 Hardware-Implemented Shared Memory The other approach to constructing a shared address space is to support it with hardware. Machines have implemented shared memory in many dif ferent ways.
Reference: [24] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Newer directory systems, such as Dir 1 SW [18,38], reduce the complexity of directories and provide rudimentary facilities to improve performance by enabling software to inform the memory system of upcoming program behavior . MIT Alewife goes further and exposes the underlying message-passing mechanisms <ref> [24] </ref>. At first glance, it appears that a compiler writer would unquestionably prefer HISM. Compilers for shared-memory computers are simpler and provide a more uniform level of performance for a wide range of programs. <p> These advantages cannot be duplicated ef fectively by software on a message-passing computer. These two approaches are, in many ways, complementary. The challenge for shared-memory designers is to provide new primitives and mechanisms that enable compilers to efficiently transfer large quantities of data (for example <ref> [24] </ref>), make better use of the local memory on each processing node, and integrate communication and synchronization in a shared address space. The challenge for message-passing system designers is to provide mechanisms that enable programs to rapidly respond to dynamic memory accesses and to quickly transfer small quantities of data.
Reference: [25] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 6374, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In addition, array privatization, a transformation that enhances parallelism, requires processors to copy data from shared to private arrays on shared-memory machines [30]. In addition, the copying of values from remote locations into contiguous local memory can improve uniprocessor cache performance by reducing both conict and capacity misses <ref> [25] </ref>. 3.6 Synchronization Transmitting values in a message-passing system, because it requires explicit actions by both processors, also transfers synchronization information. A compiler can assume that a value is not consumed until it is produced and sent.
Reference: [26] <author> James R. Larus, Satish Chandra, and David A. Wood. CICO: </author> <title> A Shared-Memory Programming Performance Model. </title> <editor> In Jeanne Ferrante and Tony Hey, editors, </editor> <title> Portability and Performance for Parallel Processors, </title> <publisher> chapter ?, page ? John Wiley & Sons, Ltd., </publisher> <year> 1993. </year> <note> To appear. </note>
Reference: [27] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Cache coherence protocols keep cached copies consistent as processors modify memory locations. Hardware for coherence distinguish classes of parallel computers. Multis [6] are bus-based multiprocessors in which all processors watch memory accesses over a shared bus and modify their caches appropriately . Directory-based computerssuch as Stanford DASH <ref> [27] </ref> and MIT Alewife [3]eliminate the non-scalable bus by having hardware, and sometimes software, maintain a directory that records which processors have copies of a cache block. A cache-coherence protocol uses the directory to serialize conicting updates and to invalidate copies at updates. <p> At one extreme is Lis Shared Virtual Memory [28] which uses a processors virtual memory hardware to detect accesses to nonlocal locations, transfers pages of memory, and maintains the directory entirely in software. At the other extreme is Stanford DASH <ref> [27] </ref>, which implements a complex directory protocol entirely in hardware and transfers 32-byte cache blocks. Cooperative shared memory [18] is an alternative approach that couples a simple directory protocol (Dir 1 SW) with a programming model and a collection of performance-improving memory system directives . <p> Taking into account physical distribution could improve cache performance. For example, on DASH, a cache miss to local memory takes 829 cycles while a remote miss takes 34132 cycles <ref> [27] </ref>. On this machine, data can be distributed by a virtual to physical mapping that exploits the partitioning of physical pages on high-order address bits. However, this approach is limited to page granularity (4KB).
Reference: [28] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: Directory systems differ primarily on the data transfer granularity and the hardware protocol to maintain directory state. At one extreme is Lis Shared Virtual Memory <ref> [28] </ref> which uses a processors virtual memory hardware to detect accesses to nonlocal locations, transfers pages of memory, and maintains the directory entirely in software. At the other extreme is Stanford DASH [27], which implements a complex directory protocol entirely in hardware and transfers 32-byte cache blocks.
Reference: [29] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 1(1):2542, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: High-Performance Fortran (HPF), for example, provides a programmer with template, align, and distribute directives to distribute arrays across processors <ref> [29] </ref>. Although a programmer usually partitions data, the compiler partitions the computation among processors. A common approach is owner computes, in which the processor holding a location (its home processor) computes and assigns the location s new values.
Reference: [30] <author> Dror E. Maydan, Sman P. Amarasinghe, and Monica S. Lam. </author> <title> Array Data-Flow Analysis and its Use in Array Compiling for Shared-Memory and Message-Passing Computer 14 Privatization. </title> <booktitle> In Conference Record of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 215, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Fortran 90 array operations have a read-before-write semantics that can be dif ficult to execute ef fectively on a MIMD processor without copying input or output arrays. In addition, array privatization, a transformation that enhances parallelism, requires processors to copy data from shared to private arrays on shared-memory machines <ref> [30] </ref>. In addition, the copying of values from remote locations into contiguous local memory can improve uniprocessor cache performance by reducing both conict and capacity misses [25]. 3.6 Synchronization Transmitting values in a message-passing system, because it requires explicit actions by both processors, also transfers synchronization information.
Reference: [31] <author> Keshav Pingali and Anne Rogers. </author> <title> Compiling for Locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II142146, </pages> <month> August </month> <year> 1990. </year>
Reference: [32] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Technique for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4):472482, </volume> <month> October </month> <year> 1991. </year>
Reference: [33] <author> Randall Rettberg and Robert Thomas. </author> <title> Contention is no Obstacle to Shared-Memory Multiprocessing. </title> <journal> Communications of the ACM, </journal> <volume> 29(12):12021212, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Machines have implemented shared memory in many dif ferent ways. Nonuniform access machines (NUMA), such as the BBN Buttery <ref> [33] </ref>, partition memory among processors and fetch a remote location at each reference, which results in sharply higher costs for remote accesses. Most shared-memory computers, however , use caches to keep copies of a memory location close to the processors that are actively accessing it.
Reference: [34] <author> Anne Marie Rogers. </author> <title> Compiling for Locality of Reference. </title> <type> Technical Report TR 91-1195, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> March </month> <year> 1991. </year> <type> PhD thesis. </type>
Reference-contexts: When the home processor mapping is unknown at compile time, it must be computed during program execution. Runtime techniques are necessary in common and mundane situations, for example doubly-subscripted array references. T wo techniques for computing this mapping are runtime resolution and inspector-executor systems. With runtime r esolution <ref> [34] </ref>, every processor at reference A [B [i]] computes B [i], examines the resulting value x to determine if it owns A [x], and if so, finds the processor to which to send the value.
Reference: [35] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-Time Scheduling and Execution of Loops on Message Passing Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8:303312, </volume> <year> 1990. </year>
Reference: [36] <author> Jaswinder Pal Singh, Truman Joe, Anoop Gupta, and John L. Hennessy. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessor. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <month> page ?, November </month> <year> 1993. </year>
Reference-contexts: A cache-coherence protocol uses the directory to serialize conicting updates and to invalidate copies at updates. COMA machines, such as the KSR-1 [22], or ganize all of their memory as a cache. It is unclear how the complex COMA hardware performs or scales <ref> [36] </ref>, so this paper concentrates on the more proven directory technology. This section briey describes cache-coherent memory systems based on directories. Processor N-1Processor 0 Directory Memory CacheCPU Network Interface Directory Memory CacheCPU Network Interface Network computer is similar to a message-passing computer . <p> However, this approach is limited to page granularity (4KB). Compilers and programs can partially sidestep the granularity limitation by using the logical to virtual mapping (i.e. array address calculation) to collect logically-related array elements on the same page <ref> [36] </ref>. This type of noncontiguous allocation is possible for statically blocked arrays, which CISM can also distribute and manipulate ef ficiently. Noncontiguous allocation complicates array index calculations and can increase the cost of an array access, but standard compiler optimizations can reduce this problem.
Reference: [37] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3044, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In addition, lar ge improvements can result from changing a programs reference pattern to ensure cached data is reused before being replaced [13,25]. Unlike prefetching, these transformations affect semantics and so require extensive compiler analysis to be applied safely. In addition, the profitability of transformations is difficult to predict <ref> [37] </ref>. Compiling for Shared-Memory and Message-Passing Computer 10 3.5 Memory Renaming Another difference between HISM and CISM is that the former uses the memory system to provide multiple instances of a memory location under the same name. In CISM, local instances are true copies that have different names.
Reference: [38] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A recent study shows that a slightly improved version of Dir 1 SW performs comparably with a more complex protocol similar to the one used in DASH <ref> [38] </ref>. 3.0 Compiler and Hardware Differences This section discusses the implications of constructing a shared address space in these different ways. The two implementations have radically different functional requirements and performance characteristics.
Reference: [39] <author> Hans Zima and Barbara Chapman. </author> <title> Compiling for Distributed-Memory Systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 81(2):264287, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: When an array is partitioned, dif ferent processors may have to reference a location with a dif ferent base address and indices. This renaming is a major hurdle in writing message-passing programs by hand. For compilers, renaming is more than a bookkeeping chore. For example, overlap analysis in SUPERB <ref> [39] </ref> and other compilers identifies array sections obtained from other processors, so they can be kept in a contiguous portion of a local array from which they can be uniformly accessed. Currently, SUPERB only performs this analysis for specific, regular access patterns. Array copying also has advantages.
References-found: 39

