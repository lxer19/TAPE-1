URL: http://ic.arc.nasa.gov/ic/projects/bayes-group/images/tr-fia-90-12-7-01.ps
Refering-URL: http://ic.arc.nasa.gov/ic/projects/bayes-group/autoclass/autoclass-c-program.html
Root-URL: 
Email: Email: &lt;last-name&gt;@ptolemy.arc.nasa.gov  
Title: Bayesian Classification Theory  
Author: Robin Hanson Sterling Software John Stutz Peter Cheeseman 
Address: Moffet Field, CA 94035, USA  
Affiliation: NASA Artificial Intelligence Research Branch NASA Ames Research Center, Mail Stop 244-17  RIACS  
Pubnum: Technical Report FIA-90-12-7-01  
Abstract: The task of inferring a set of classes and class descriptions most likely to explain a given data set can be placed on a firm theoretical foundation using Bayesian statistics. Within this framework, and using various mathematical and algorithmic approximations, the Au-toClass system searches for the most probable classifications, automatically choosing the number of classes and complexity of class descriptions. A simpler version of AutoClass has been applied to many large real data sets, have discovered new independently-verified phenomena, and have been released as a robust software package. Recent extensions allow attributes to be selectively correlated within particular classes, and allow classes to inherit, or share, model parameters though a class hierarchy. In this paper we summarize the mathe matical foundations of Autoclass.
Abstract-found: 1
Intro-found: 1
Reference: [ Aitchison & Brown, 1957 ] <author> J. Aitchison and and J. A. C. Brown. </author> <title> The Lognormal Distribution. </title> <publisher> University Press, </publisher> <address> Cambridge, </address> <year> 1957. </year>
Reference-contexts: For example, someone's weight might be measured as 701 kilograms. For scalar attributes, which can only be positive, like weight, it is best to use the logarithm of that variable <ref> [ Aitchison & Brown, 1957 ] </ref> .
Reference: [ Berger, 1985 ] <author> J. O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> 1985. </note> [ <author> Cheeseman et al., 1988a ] Peter Cheeseman, James Kelly, Matthew Self, John Stutz, Will Taylor, & Don Freeman. </author> <title> Autoclass: a Bayesian Classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: Almost all of the weight is usually in the best few, justifying the neglect of the rest. 2.3 Tradeoffs Bayesian theory offers the advantages of being theoretically well-founded and empirically well-tested <ref> [ Berger, 1985 ] </ref> . It offers a clear procedure whereby one can almost "turn the crank", modulo doing integrals and search, to deal with any new problem. The machinery automatically trades off the complexity of a model against its fit to the evidence. <p> The machinery automatically trades off the complexity of a model against its fit to the evidence. Background knowledge can be included in the input, and the output is a flexible mixture of several different "answers," with a clear and well-founded decision theory <ref> [ Berger, 1985 ] </ref> to help one use that output. Disadvantages include being forced to be explicit about the space of models one is searching in, though this can be good discipline. <p> We choose a prior d (V jS D1 ) = dB (q 1 : : : q L jL) (a) L l l dq l which for a &gt; 0 is a special case of a beta distribution <ref> [ Berger, 1985 ] </ref> ((y) is the Gamma function [ Spiegel, 1968 ] ). This formula is parameterized by a, a "hyperpa-rameter" which can be set to different values to specify different priors. Here we set a = 1=L.
Reference: [ Cheeseman et al., 1988b ] <author> Peter Cheeseman, Matthew Self, James Kelly, John Stutz, Will Taylor, & Don Freeman. </author> <title> Bayesian Classification. </title> <booktitle> In Seventh National Conference on Artificial Intellegence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: This type of classification, related to clustering, is often very useful in exploratory data analysis, where one has few preconceptions about what structures new data may hold. We have previously developed and reported on Au-toClass <ref> [ Cheeseman et al., 1988a; Cheeseman et al., 1988b ] </ref> , an unsupervised classification system based on Bayesian theory. Rather than just partitioning cases, as most clustering techniques do, the Bayesian approach searches in a model space for the "best" class descriptions.
Reference: [ Cheeseman, 1990 ] <author> Peter Cheeseman. </author> <title> On finding the most probable model. </title> <editor> In J. Shrager and P. Langley Eds., </editor> <booktitle> Computational Models of Discovery and Theory Formation, </booktitle> <pages> pages 73-96. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, </address> <year> 1990. </year>
Reference-contexts: Bayesian analysis is not limited to what is traditionally considered "statistical" data, but can be applied to any space of models about how the world might be. For a general discussion of these issues, see <ref> [ Cheeseman, 1990 ] </ref> .
Reference: [ Cox, 1946 ] <author> R. T. Cox. </author> <title> Probability, frequency, and reasonable expectation. </title> <journal> American Journal of Physics, </journal> <volume> 17 </volume> <pages> 1-13, </pages> <year> 1946. </year>
Reference-contexts: In general, a Bayesian agent uses a single real number to describe its degree of belief in each proposition of interest. This assumption, together with some other assumptions about how evidence should affect beliefs, leads to the standard probability axioms. This result was originally proved by Cox <ref> [ Cox, 1946 ] </ref> and has been reformulated for an AI audience [ Heckerman, 1990 ] .
Reference: [ Dempster et al., 1977 ] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Except in very simple problems, the resulting joint dJ (EV T jS) has many local maxima, and so we must now focus on regions R of the V space. To find such local maxima we use the "EM" algorithm <ref> [ Dempster et al., 1977 ] </ref> which is based on the fact that at a maxima the class parameters V c can be estimated from weighted sufficient statistics.
Reference: [ Hanson, Stutz & Cheeseman, 1991 ] <author> R. Hanson, J. Stutz, and P. Cheeseman. </author> <title> Bayesian Clas-sificaiton with correlation and inheritance. </title> <booktitle> In 12th International Joint confercnce on Artificial Intelligence, </booktitle> <pages> pages 692-698, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Parameters associated with "irrelevant" attributes are specified independently at the root. Figure 2 shows how a class tree, this time with S C = S V , can better fit the same data as in Figure 1. See <ref> [ Hanson, Stutz & Cheeseman, 1991 ] </ref> for more about this comparison. The tree of classes has one root class r.
Reference: [ Heckerman, 1990 ] <author> David Heckerman. </author> <title> Probabilistic interpretations for Mycin's certainty factors. </title> <editor> In G. Shafer and J. Pearl, Eds., </editor> <booktitle> Readings in Uncertain Reasoning, </booktitle> <pages> pages 298-312. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, </address> <year> 1990. </year>
Reference-contexts: This assumption, together with some other assumptions about how evidence should affect beliefs, leads to the standard probability axioms. This result was originally proved by Cox [ Cox, 1946 ] and has been reformulated for an AI audience <ref> [ Heckerman, 1990 ] </ref> .
Reference: [ Mardia, Kent, & Bibby, 1979 ] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby. </author> <title> Multavariant Analysis. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: We choose the prior on kk 0 to use an inverse Wishart distribution <ref> [ Mardia, Kent, & Bibby, 1979 ] </ref> K (f kk 0 g j fG kk 0 g ; h) h hK1 1 P K kk 0 G inv 2 2 4 a ( h+1a K Y d kk 0 6 F 1 and F 2 are defined on page 4. 7
Reference: [ Pearl, 1988 ] <editor> Judah Pearl Probabilistic Reasoning in In-tellegent Systems. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Thus real and discretes may not mutually covary. We are away of other models of partial dependence, such as the the trees of Chow and Liu described in <ref> [ Pearl, 1988 ] </ref> , but choose this approach because it includes the limiting cases of full dependence and full independence.
Reference: [ Spiegel, 1968 ] <author> Murray Spiegel. </author> <title> Mathematical Handbook of Formulas and Tables. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: We choose a prior d (V jS D1 ) = dB (q 1 : : : q L jL) (a) L l l dq l which for a &gt; 0 is a special case of a beta distribution [ Berger, 1985 ] ((y) is the Gamma function <ref> [ Spiegel, 1968 ] </ref> ). This formula is parameterized by a, a "hyperpa-rameter" which can be set to different values to specify different priors. Here we set a = 1=L.
Reference: [ Titterington et al., 1985 ] <author> D. M. Titterington, A. F. M. Smith, and U. E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: for discrete attributes). 5 Class Mixtures 5.1 Flat Mixtures - S M The above model spaces S C = S V or S I can be thought of as describing a single class, and so can be extended by considering a space S M of simple mixtures of such classes <ref> [ Titterington et al., 1985 ] </ref> . Figure 1 shows how this model, with S C = S I , can fit a set of artificial real-valued data in five dimensions.
References-found: 12

