URL: http://www.cnl.salk.edu/~lewicki/papers/imagecodes.ps.gz
Refering-URL: http://www.cnl.salk.edu/~lewicki/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lewicki@salk.edu  bruno@redwood.ucdavis.edu  
Title: Inferring sparse, overcomplete image codes using an efficient coding framework  
Author: Michael S. Lewicki Bruno A. Olshausen 
Date: April 10, 1998  
Note: Submitted to J. Opt. Soc. of Am. A: Optics, Image Science, and Vision  
Address: 10010 N. Torrey Pines Rd. La Jolla, CA 92037  1544 Newton Ct., Davis, CA  
Affiliation: Howard Hughes Medical Institute Computational Neurobiology Lab The Salk Institute  Center for Neuroscience University of California, Davis  
Abstract: We apply a general technique for learning overcomplete bases to the problem of finding efficient image codes. The bases learned by the algorithm are localized, oriented, and bandpass, consistent with earlier results obtained using related methods. We show that the learned bases are Gabor-like in structure and that higher degrees of overcompleteness produce greater sampling density in position, orientation, and scale. The efficient coding framework provides a method for comparing different bases objectively by calculating their probability given the observed data or by measuring the entropy of the basis function coefficients. Compared to complete and overcomplete Fourier and wavelet bases, the learned bases have much better coding efficiency. We demonstrate the improvement in the representation of the learned bases by showing superior performance in image denoising and filling-in of missing pixels. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. J. </author> <title> Field. What is the goal of sensory coding. </title> <journal> Neural Computation, </journal> <volume> 6(4) </volume> <pages> 559-601, </pages> <year> 1994. </year>
Reference-contexts: Typically, bases that are chosen for their low-entropy coding properties, such as Gabor functions or wavelets <ref> [1, 2] </ref>, are hand-designed rather than being adapted to the data so as to optimize efficiency. <p> As such, it will only be able to describe second-order statistical structure, as specified by the covariance matrix. Because it is well established that images are not well described by Gaussian distributions <ref> [1, 15, 14] </ref>, we are thus obligated to choose a non-Gaussian prior. The specific form we choose for the prior is to be sparse and factorial. <p> By a sparse prior, we mean that the probability distribution of each coefficient's activity, P (s i ), is highly peaked around zero and with heavy tails. Such a distribution reflects our intuition that natural images should be described in terms of a small number of "events" <ref> [1, 10] </ref>; thus, any given coefficient will rarely be active, and when it does become active it takes on a value along a continuum. We choose here to represent such a distribution using a Laplacian, but other super-Gaussian shapes are also possible. <p> The bases were initialized to random Gaussian blobs with positions that were evenly distributed over the input area. The initial bases were generated by first setting the basis elements to random values between <ref> [1; 1] </ref> and then scaling these values by a two-dimensional Gaussian envelope that had a standard deviation of 0.25 pixels in the complete case and 1 pixel in the 2fi-overcomplete case. This ensured that the initial set of basis functions spanned the input space.
Reference: [2] <author> J. G. Daugman. </author> <title> Entropy reduction and decorrelation in visual coding by oriented neural receptive-fields. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> 36(1) </volume> <pages> 107-114, </pages> <year> 1989. </year>
Reference-contexts: Typically, bases that are chosen for their low-entropy coding properties, such as Gabor functions or wavelets <ref> [1, 2] </ref>, are hand-designed rather than being adapted to the data so as to optimize efficiency.
Reference: [3] <author> M. S. Lewicki and T. J. Sejnowski. </author> <title> Learning nonlinear overcomplete representations for efficient coding. </title> <booktitle> In Advances in Neural and Information Processing Systems, volume 10, </booktitle> <address> San Mateo, 1998. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Typically, bases that are chosen for their low-entropy coding properties, such as Gabor functions or wavelets [1, 2], are hand-designed rather than being adapted to the data so as to optimize efficiency. By applying the methods of Lewicki and Sejnowski <ref> [3, 4] </ref>, we show how a linear basis function expansion can be used to model the probability distribution of natural images, and we show how to adapt the bases to maximize the fit to this distribution, thereby maximizing coding efficiency.
Reference: [4] <author> M. S. Lewicki and T. J. Sejnowski. </author> <title> Learning overcomplete representations. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> submitted. </note>
Reference-contexts: Typically, bases that are chosen for their low-entropy coding properties, such as Gabor functions or wavelets [1, 2], are hand-designed rather than being adapted to the data so as to optimize efficiency. By applying the methods of Lewicki and Sejnowski <ref> [3, 4] </ref>, we show how a linear basis function expansion can be used to model the probability distribution of natural images, and we show how to adapt the bases to maximize the fit to this distribution, thereby maximizing coding efficiency. <p> Pre-multiplying this rule by AA T yields the form given by Lewicki and Sejnowski <ref> [4] </ref>: A / A (zs T + A T AH 1 ) ; (15) where z = d log P (s)=ds (see appendix for details). <p> This ensured that the initial set of basis functions spanned the input space. Similar results were obtained using random initial bases, but convergence was slower. To further speed convergence, we used the modifications of the basic gradient descent procedure described previously <ref> [4] </ref>. For each gradient (equation 15), a stepsize was computed by ffi i = * i =a max , where a max is the element of of the basis matrix, A, with largest absolute value. <p> To estimate how well a particular basis represented a given set of data, we followed two methods described previously <ref> [4] </ref>. The first method is to use log 2 P (xjA) L log 2 ( x ), where x is the precision of the encoding. Shannon's coding theorem states that this will give a lower bound on coding length if the model is correct. <p> We have demonstrated moderate success in learning bases that capture the underlying statistical structure of images and have demonstrated this using quantitative comparison of a number of standard image codes. Overcomplete codes have been shown to yield greater coding efficiency on some test data sets <ref> [4] </ref>. Because the model optimizes the data likelihood, P (xjA), one would expect that increasing the number of parameters would increase the likelihood. This was not, however, the case for the images analyzed here. <p> It will be exciting to apply this framework to pattern domains where good codes remain largely unknown. Appendix Derivation of learning rule Here we give an alternate derivation to the one given by Lewicki and Sejnowski <ref> [4] </ref>. <p> Thus, if the posterior has been properly maximized, e reflects information about the prior. This rule can be written in the form derived by Lewicki and Sejnowski <ref> [4] </ref> by pre-multiplying by AA T AA T A / AA T es T AA T AH 1 (36) where the last step is obtained using A T e = z. <p> ij , which may not be true at a small number of critical points, because the mapping from x to ^s is non-linear (equation 5). 19 Approximating A T AH 1 The expression for the term A T AH 1 in equation 15 can be approximated with the identity matrix <ref> [4] </ref> which works well in many cases, but can break down under some circumstances. The following approximation works under a broader range of conditions.
Reference: [5] <author> A. J. Bell and T. J. Sejnowski. </author> <title> An information maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159, </pages> <year> 1995. </year>
Reference-contexts: Among existing techniques for modeling data using a set of basis functions are principal component analysis (PCA) and more recently independent component analysis (ICA). PCA assumes the data have Gaussian structure and fits an appropriate orthogonal basis, while ICA allows for non-Gaussian distributions and non-orthogonal bases <ref> [5, for example] </ref>. Both of these techniques are limited, however, in that they form a complete or critically sampled basis, i.e. the number of basis vectors is equal to the dimensionality of the input.
Reference: [6] <author> P. J. Huber. </author> <title> Projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 13(2) </volume> <pages> 435-475, </pages> <year> 1985. </year>
Reference-contexts: Both of these techniques are limited, however, in that they form a complete or critically sampled basis, i.e. the number of basis vectors is equal to the dimensionality of the input. A method related to ICA, termed "projection pursuit" <ref> [6, 7] </ref>, seeks to model the probability density by seeking a set of "interesting" projections and is also typically limited to a small set of projection vectors.
Reference: [7] <author> J. H. Friedman. </author> <title> Exploratory projection pursuit. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 82 </volume> <pages> 249-266, </pages> <year> 1987. </year> <month> 20 </month>
Reference-contexts: Both of these techniques are limited, however, in that they form a complete or critically sampled basis, i.e. the number of basis vectors is equal to the dimensionality of the input. A method related to ICA, termed "projection pursuit" <ref> [6, 7] </ref>, seeks to model the probability density by seeking a set of "interesting" projections and is also typically limited to a small set of projection vectors.
Reference: [8] <author> E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and Heeger D. J. </author> <title> Shiftable multiscale transforms. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 38 </volume> <pages> 587-607, </pages> <year> 1992. </year>
Reference-contexts: Codes based on so-called "overcomplete" bases, which allow a greater number of basis vectors than input dimensions, have been proposed because they allow certain advantages in terms of interpolation <ref> [8] </ref> or in achieving sparsity in the representation [9, 10].
Reference: [9] <author> S. Chen, D. L. Donoho, and M. A. Saunders. </author> <title> Atomic decomposition by basis pursuit. </title> <type> Technical report, </type> <institution> Dept. Stat., Stanford Univ., Stanford, </institution> <address> CA, </address> <year> 1996. </year>
Reference-contexts: Codes based on so-called "overcomplete" bases, which allow a greater number of basis vectors than input dimensions, have been proposed because they allow certain advantages in terms of interpolation [8] or in achieving sparsity in the representation <ref> [9, 10] </ref>. One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal [11, 12, 9] or a class of signals such as texture [13, for example]. <p> One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal <ref> [11, 12, 9] </ref> or a class of signals such as texture [13, for example]. <p> This problem is formally equivalent to that of "basis pursuit de-noising" posed by Chen, Donoho, and Saunders <ref> [9] </ref>. In our case though, the ` 1 norm arises from the Laplacian prior. Under this prior (or other "super-Gaussian" priors), finding the most probable basis coefficients essentially selects out a complete basis and sets the coefficients for the remaining vectors to zero.
Reference: [10] <author> B. A. Olshausen and D. J. </author> <title> Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Res., </title> <type> 37(23), </type> <year> 1997. </year>
Reference-contexts: Codes based on so-called "overcomplete" bases, which allow a greater number of basis vectors than input dimensions, have been proposed because they allow certain advantages in terms of interpolation [8] or in achieving sparsity in the representation <ref> [9, 10] </ref>. One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal [11, 12, 9] or a class of signals such as texture [13, for example]. <p> By a sparse prior, we mean that the probability distribution of each coefficient's activity, P (s i ), is highly peaked around zero and with heavy tails. Such a distribution reflects our intuition that natural images should be described in terms of a small number of "events" <ref> [1, 10] </ref>; thus, any given coefficient will rarely be active, and when it does become active it takes on a value along a continuum. We choose here to represent such a distribution using a Laplacian, but other super-Gaussian shapes are also possible. <p> The practical problem that is presented by equation 12 is averaging over the internal states, s, for each image presentation. One possible avenue might be to utilize efficient methods for sampling from the posterior, P (sjx; A). This characterizes in part the approach taken by Olshausen and Field <ref> [10] </ref>, although that algorithm samples the posterior only at its maximum, ignoring the volume and thus requiring an additional adaptive step to scale the basis functions so that the coefficients have the same variance as dictated by the prior. <p> These histograms show that the bases cover a broad range of spatial-frequencies, from 0.075 to 0.3 cy/pixel. These results are again generally consistent with previous observations, although Olshausen and Field <ref> [10] </ref> show a somewhat more bimodal distribution clustered on low and high spatial-frequencies, and Bell and Se-jnowski's [19] bases appear more highly skewed towards the highest spatial-frequencies.
Reference: [11] <author> R. R. Coifman and M. V. Wickerhauser. </author> <title> Entropy-based algorithms for best basis selection. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(2) </volume> <pages> 713-718, </pages> <year> 1992. </year>
Reference-contexts: One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal <ref> [11, 12, 9] </ref> or a class of signals such as texture [13, for example].
Reference: [12] <author> S. G. Mallat and Z. F. Zhang. </author> <title> Matching pursuits with time-frequency dictionaries. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(12) </volume> <pages> 3397-3415, </pages> <year> 1993. </year>
Reference-contexts: One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal <ref> [11, 12, 9] </ref> or a class of signals such as texture [13, for example].
Reference: [13] <author> S. C. Zhu, Y. N. Wu, and D. Mumford. </author> <title> Minimax entropy principle and its application to texture modeling. </title> <journal> Neural Computation, </journal> <volume> 9(8) </volume> <pages> 1627-1660, </pages> <year> 1997. </year>
Reference-contexts: One popular method is to work with an overcomplete "dictionary" of basis functions and select out those bases in the dictionary that yield the lowest entropy description for a particular signal [11, 12, 9] or a class of signals such as texture <ref> [13, for example] </ref>.
Reference: [14] <author> B. A. Olshausen and D. J. </author> <title> Field. Emergence of simple-cell receptive-field properties by learning a sparse code for natural images. </title> <journal> Nature, </journal> <volume> 381 </volume> <pages> 607-609, </pages> <year> 1996. </year>
Reference-contexts: We choose this distribution to be factorial and Laplacian, P (s m ) / exp ( m js m j), which assumes that A decomposes the images into sparse, statistically independent components <ref> [14] </ref>. More will be said about this choice of prior below. <p> As such, it will only be able to describe second-order statistical structure, as specified by the covariance matrix. Because it is well established that images are not well described by Gaussian distributions <ref> [1, 15, 14] </ref>, we are thus obligated to choose a non-Gaussian prior. The specific form we choose for the prior is to be sparse and factorial. <p> This form of the learning rule is more stable, and it was used to learn the basis functions in the examples below. 3 Learning overcomplete representations of natural scenes Here we learn complete and two-times overcomplete representations of natural scenes using the same data set used by Olshausen and Field <ref> [14] </ref>. The bases were initialized to random Gaussian blobs with positions that were evenly distributed over the input area. <p> A convergence tolerance of 0:02 was used for the examples shown here. A sample of the learned basis functions (the odd-numbered) are shown in figure 1, in decreasing order of ` 2 norm. Nearly all of the learned basis functions show a Gabor-like structure as has been found previously <ref> [14, 19] </ref>. The basis functions largest in magnitude also have the lowest peak spatial-frequency tuning. Peak spatial-frequency tuning becomes progressively higher with decreasing magnitude. The checkerboard-like basis functions are smallest in magnitude and could reflect the fact that the image data do not fully span the 144-dimensional space. 6 scenes. <p> Figure 2 shows the result of this analysis. From the characterization of the learned basis functions, a number of properties can be observed which have been reported before using different but related methods <ref> [14, 19] </ref>. Almost all of the basis functions are localized in space, except possibly for ones which have the lowest spatial frequency. The basis functions are also spread out evenly in terms of their spatial position. <p> In addition to allowing for overcomplete bases, there are some differences between our approach and previous approaches that could account for the broader distribution of spatial frequency bandwidths. One is that the method of Olshausen and Field <ref> [14] </ref> assumed a relatively high noise level compared to that assumed here ( 100 versus = 3000). Methods based on independent component analysis [19, 22] assume zero noise, but still produce a bias toward higher spatial frequency bandwidths.
Reference: [15] <author> D. L. Ruderman. </author> <title> The statistics of natural images. </title> <booktitle> Network Computation in Neural Systems, </booktitle> <volume> 5(4) </volume> <pages> 517-548, </pages> <year> 1994. </year>
Reference-contexts: As such, it will only be able to describe second-order statistical structure, as specified by the covariance matrix. Because it is well established that images are not well described by Gaussian distributions <ref> [1, 15, 14] </ref>, we are thus obligated to choose a non-Gaussian prior. The specific form we choose for the prior is to be sparse and factorial.
Reference: [16] <author> H. B. Barlow. </author> <title> Possible principles underlying the transformation of sensory messages. </title> <editor> In W. A. Rosenbluth, editor, </editor> <booktitle> Sensory Communication, </booktitle> <pages> pages 217-234. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1961. </year>
Reference-contexts: the coefficients is chosen to be factorial, P (s) = i P (s i ), in line with Barlow's proposal that an efficient code should try to decompose the image in terms of statistically independent elements, thus reflecting the independent events in the world that gave rise to the image <ref> [16, 17] </ref>. Maximizing L with respect to A will thus find a set of basis functions that best account for the structure in images in terms of sparse, statistically independent events.
Reference: [17] <author> H. B. Barlow. </author> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311, </pages> <year> 1989. </year>
Reference-contexts: the coefficients is chosen to be factorial, P (s) = i P (s i ), in line with Barlow's proposal that an efficient code should try to decompose the image in terms of statistically independent elements, thus reflecting the independent events in the world that gave rise to the image <ref> [16, 17] </ref>. Maximizing L with respect to A will thus find a set of basis functions that best account for the structure in images in terms of sparse, statistically independent events.
Reference: [18] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recip-ies in C: </title> <booktitle> The Art of Scientific Programming (2nd ed.). </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1992. </year>
Reference-contexts: Learning was stopped after 10000 gradient steps, at which point both of the bases learned here were stable. The most probable basis function coefficients, ^s, were obtained using a modified conjugate gradient routine <ref> [18] </ref>. The basic routine was modified to replace the line search with an approximate Newton step. This approach resulted in a substantial speed improvement and produced much better solutions in a fixed amount of time than the standard routine.
Reference: [19] <author> A. J. Bell and T. J. Sejnowski. </author> <title> The 'independent components' of natural scenes are edge filters. </title> <journal> Vision Res., </journal> <volume> 37(23) </volume> <pages> 3327-3338, </pages> <year> 1997. </year>
Reference-contexts: A convergence tolerance of 0:02 was used for the examples shown here. A sample of the learned basis functions (the odd-numbered) are shown in figure 1, in decreasing order of ` 2 norm. Nearly all of the learned basis functions show a Gabor-like structure as has been found previously <ref> [14, 19] </ref>. The basis functions largest in magnitude also have the lowest peak spatial-frequency tuning. Peak spatial-frequency tuning becomes progressively higher with decreasing magnitude. The checkerboard-like basis functions are smallest in magnitude and could reflect the fact that the image data do not fully span the 144-dimensional space. 6 scenes. <p> Figure 2 shows the result of this analysis. From the characterization of the learned basis functions, a number of properties can be observed which have been reported before using different but related methods <ref> [14, 19] </ref>. Almost all of the basis functions are localized in space, except possibly for ones which have the lowest spatial frequency. The basis functions are also spread out evenly in terms of their spatial position. <p> These histograms show that the bases cover a broad range of spatial-frequencies, from 0.075 to 0.3 cy/pixel. These results are again generally consistent with previous observations, although Olshausen and Field [10] show a somewhat more bimodal distribution clustered on low and high spatial-frequencies, and Bell and Se-jnowski's <ref> [19] </ref> bases appear more highly skewed towards the highest spatial-frequencies. <p> One is that the method of Olshausen and Field [14] assumed a relatively high noise level compared to that assumed here ( 100 versus = 3000). Methods based on independent component analysis <ref> [19, 22] </ref> assume zero noise, but still produce a bias toward higher spatial frequency bandwidths. This could be due to the assumption of different distributions for the basis function coefficients. In this paper, we have assumed a Laplacian prior for the coefficients.
Reference: [20] <author> R. L. De Valois, D. G. Albrecht, and L. G. Thorell. </author> <title> Spatial frequency selectivity of cells in macaque visual cortex. </title> <journal> Vision Res., </journal> <volume> 22 </volume> <pages> 545-559, </pages> <year> 1982. </year>
Reference-contexts: The dominance of basis functions at the highest spatial-frequencies is at odds with the observed response properties of neurons in primate visual cortex, where the vast majority of cells reside in the mid to low spatial- frequency range corresponding to the retinal sampling lattice <ref> [20, 21] </ref>. One possibility for this discrepancy is that the highest spatial-frequency cells were greatly undersampled in these experiments, since these cells will be relatively difficult to isolate in comparison to the low spatial-frequency cells.
Reference: [21] <author> A.J. Parker and M. J. Hawken. </author> <title> Two-dimensional spatial structure of receptive fields in monkey striate cortex. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 5 </volume> <pages> 598-605, </pages> <year> 1988. </year>
Reference-contexts: The dominance of basis functions at the highest spatial-frequencies is at odds with the observed response properties of neurons in primate visual cortex, where the vast majority of cells reside in the mid to low spatial- frequency range corresponding to the retinal sampling lattice <ref> [20, 21] </ref>. One possibility for this discrepancy is that the highest spatial-frequency cells were greatly undersampled in these experiments, since these cells will be relatively difficult to isolate in comparison to the low spatial-frequency cells.
Reference: [22] <author> J. H. van Hatteren and A. van der Schaaf. </author> <title> Independent component filters of natural images campared with simple cells in primary visual cortex. </title> <journal> Phil. Trans. Royal Soc. B, </journal> <year> 1998. </year>
Reference-contexts: Alternatively, it has been suggested this discrepancy may be resolved by considering the time domain of the visual signal <ref> [22] </ref>. When basis functions are learned for natural movies, the distribution of spatial-frequency is more spread out, presumably because of the trade-off between tiling velocity and spatial-frequency. <p> One is that the method of Olshausen and Field [14] assumed a relatively high noise level compared to that assumed here ( 100 versus = 3000). Methods based on independent component analysis <ref> [19, 22] </ref> assume zero noise, but still produce a bias toward higher spatial frequency bandwidths. This could be due to the assumption of different distributions for the basis function coefficients. In this paper, we have assumed a Laplacian prior for the coefficients.
Reference: [23] <author> R. Everson and L. Sirovich. </author> <title> Karhunen-Loeve procedure for gappy data. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 12 </volume> <pages> 1657-1664, </pages> <year> 1995. </year>
Reference-contexts: This can be seen by comparing the image patches in row 2, column 1 and row 3, column 4. Everson and Sirovich <ref> [23] </ref> describe a similar method for filling-in missing pixels by using the Karhunen-Loeve transform applied to a specific image class, i.e. faces. In terms of our framework, their procedure corresponds to using a Gaussian prior, and thus it captures only the second-order statistics in the data.
Reference: [24] <author> R. W. Buccigrossi and E. P. Simoncelli. </author> <title> Image compression via joint statistical characterization in the wavelet domain. </title> <type> Technical Report 414, </type> <institution> Univ. Pennsylvania, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Clearly, there is a need for this prior to be more flexible. For over-complete codes, a Laplacian is not sparse enough to reflect the fact that, for each pattern, a subset of the coefficients will be zero. Buccigrossi and Simoncelli <ref> [24] </ref> have observed that coefficients of wavelet representations of images are more sparse than predicted by the Laplace distribution and can be well modeled with a generalized Laplace distribution 16 (log P (s) / jsj p ).
Reference: [25] <author> B. A. Pearlmutter and L. C. </author> <title> Parra. Maximum likelihood blind source separation: A context-senstive generalization of ICA. </title> <booktitle> In Advances in Neural and Information Processing Systems, volume 9, </booktitle> <address> San Mateo, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another possibility for improving the prior is to use a mixture distribu-tion consisting of a delta function at zero and a second function describing the distribution of non-zero coefficients. A more flexible approach proposed recently is to model P (s) with Gaussian mixtures <ref> [25, 26] </ref>. Better approximating the coefficient prior would allow the model to better capture the actual coefficient distribution, but two important problems must be addressed before the benefits of such a model can be realized. The first is finding the most probable coefficient values. <p> This remains a challenge for general models. The approach used here estimated P (xjA) by approximating the coefficient posterior distribution, P (sjx; A), with a Gaussian. One promising approach for more accurate estimates is to model P (s) with a Gaussian mixture <ref> [25, 26] </ref> and again use a Gaussian approximation at the (maximum) posterior mode. A more general approach, suggested by equation 12, is to use Monte Carlo methods [28] to estimate P (xjA) by sampling the coefficient posterior.
Reference: [26] <author> H. Attias. </author> <title> Blind separation of noisy mixtures: An EM algorithm for indepentent factor analysis. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> Submitted. </note>
Reference-contexts: Another possibility for improving the prior is to use a mixture distribu-tion consisting of a delta function at zero and a second function describing the distribution of non-zero coefficients. A more flexible approach proposed recently is to model P (s) with Gaussian mixtures <ref> [25, 26] </ref>. Better approximating the coefficient prior would allow the model to better capture the actual coefficient distribution, but two important problems must be addressed before the benefits of such a model can be realized. The first is finding the most probable coefficient values. <p> This remains a challenge for general models. The approach used here estimated P (xjA) by approximating the coefficient posterior distribution, P (sjx; A), with a Gaussian. One promising approach for more accurate estimates is to model P (s) with a Gaussian mixture <ref> [25, 26] </ref> and again use a Gaussian approximation at the (maximum) posterior mode. A more general approach, suggested by equation 12, is to use Monte Carlo methods [28] to estimate P (xjA) by sampling the coefficient posterior.
Reference: [27] <author> B. D. Rao and K. Kreutz-Delgado. </author> <title> An affine scaling methodology for best basis selection. </title> <type> Technical report, </type> <institution> Center for Information Engineering, Univ. </institution> <address> California San Deigo, </address> <year> 1997. </year>
Reference-contexts: The first is finding the most probable coefficient values. For the cases of positive noise (* &gt; 0) and overcomplete representations, computing the most probable coefficients is not straightforward, although recent work has made progress in finding the most probable coefficients for overcomplete representations with a generalized Laplacian prior <ref> [27] </ref>. A second issue is how to evaluate or approximate the integral required to compute the data probability, P (xjA) (equation 7). This remains a challenge for general models. The approach used here estimated P (xjA) by approximating the coefficient posterior distribution, P (sjx; A), with a Gaussian.
Reference: [28] <author> R. M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <month> 22 </month>
Reference-contexts: One promising approach for more accurate estimates is to model P (s) with a Gaussian mixture [25, 26] and again use a Gaussian approximation at the (maximum) posterior mode. A more general approach, suggested by equation 12, is to use Monte Carlo methods <ref> [28] </ref> to estimate P (xjA) by sampling the coefficient posterior. It should be emphasized though that despite these shortcomings in our particular implementation, the probabilistic framework described here provides a new perspective on the utility of working with overcomplete codes - i.e. better modeling of the underlying probability density.
References-found: 28

