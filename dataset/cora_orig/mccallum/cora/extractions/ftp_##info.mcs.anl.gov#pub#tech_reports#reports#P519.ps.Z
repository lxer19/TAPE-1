URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P519.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Title: Efficient Computation of Gradients and Jacobians by Dynamic Exploitation of Sparsity in Automatic Differentiation  
Author: Christian H. Bischof, Peyvand M. Khademi, Ali Bouaricha, and Alan Carle 
Keyword: Key words. Automatic Differentiation, Sparsity, Partial Separability, Sparse Jacobians, Large-Scale Optimization, MINPACK-2, ADIFOR, SparsLinC.  
Note: Preprint MCS-P519-0595  
Affiliation: Argonne  
Abstract: Automatic differentiation (AD) is a technique that augments computer codes with statements for the computation of derivatives. The computational workhorse of AD-generated codes for first-order derivatives is the linear combination of vectors. For many large-scale problems, the vectors involved in this operation are inherently sparse. If the underlying function is a partially separable one (e.g., if its Hessian is sparse), many of the intermediate gradient vectors computed by AD will also be sparse, even though the final gradient is likely to be dense. For large Jacobians computations, every intermediate derivative vector is usually at least as sparse as the least sparse row of the final Jacobian. In this paper, we show that dynamic exploitation of the sparsity inherent in derivative computation can result in dramatic gains in runtime and memory savings. For a set of gradient problems exhibiting implicit sparsity, we report on the runtime and memory requirements of computing the gradients with the ADIFOR (Automatic DIfferentiation of FORtran) tool, both with and without employing the SparsLinC (Sparse Linear Combinations) library, and show that SparsLinC can reduce runtime and memory costs by orders of magnitude. We also compute sparse Jacobians using the SparsLinC-based approach|in the process, automatically detecting the sparsity structure of the Jacobian|and show that these Jacobian results compare favorably with those of previous techniques that require a priori knowledge of the sparsity structure of the Jacobian. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue. </author> <title> The MINPACK-2 test problem collection. </title> <type> Technical Report ANL/MCS-TM-150, Rev. 1, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In this paper, we report on the runtime and memory performance measures of differentiating large-scale (n 160; 000) optimization codes with ADIFOR when interfaced with SparsLinC, and contrast these with NONSPARSE ADIFOR results (i.e., without SparsLinC). The codes are taken from the MINPACK-2 test set problems <ref> [1] </ref> and the molecular distance geometry class of global minimization functions [24]. In Section 2 we look at sparsity in the context of the computation of VLCs in AD and 3 show how this can be exploited with SparsLinC. <p> A detailed description of the SparsLinC library will be reported elsewhere, and the full Fortran interface specification can be found in [6]. 3 Computing Gradients of Partially Separable Functions The MINPACK-2 Test Problem Collection <ref> [1] </ref> contains a number of unconstrained optimization problems from a variety of application areas. Table 3.1 shows the six MINPACK-2 problems we used in our gradient experiments. <p> The MINPACK-2 problems are identified in Table 4.1. These are a standard set of test problems; the reader is referred to <ref> [1] </ref> for detailed descriptions. The MDG problem is described in More and Wu [24] as follows: Given bond lengths ffi i;j between a subset S of the atom pairs, determine whether there is a molecule that satisfies these bond length constraints.
Reference: [2] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and Andreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15(2) </volume> <pages> 285-294, </pages> <year> 1994. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization <ref> [2, 14] </ref>, computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs. <p> In contrast to partially separable functions where some gradient vectors may be dense, the AD computation of sparse Jacobians, by definition, involves only sparse directional gradient vectors. In Section 4 we briefly review the compressed Jacobian approach, which has previously been used <ref> [17, 2] </ref> as an effective strategy for computing sparse Jacobians in cases where the closure sparsity pattern is known. <p> we present the memory requirements and runtime results of both approaches for the MINPACK-2 and the MDG test problems. 4.1 The Compressed Jacobian Approach The compressed Jacobian approach has been used as an effective strategy for computing sparse Jacobians in conjunction with both finite differencing [17] and automatic differentiation methods <ref> [2] </ref>. The prerequisite for applying the compressed Jacobian approach to a given sparse Jacobian computation is a priori knowledge of the sparsity pattern of the Jacobian. <p> Because of the structural orthogonality property we can uniquely extract all entries of the original Jacobian matrix from the compressed Jacobian <ref> [2] </ref>. If the sparsity pattern of the Jacobian can be determined, graph-coloring techniques can be used to arrive at S and p. These algorithms produce a partitioning of the columns of the Jacobian into p structurally orthogonal groups by coloring the column-intersection graph associated with the Jacobian.
Reference: [3] <author> M.C. Bartholomew-Biggs, L. Bartholomew-Biggs, and B. Christianson. </author> <title> Optimization & automatic differentiation in ADA: Some practical experiences. </title> <journal> Optimization Methods & Software, </journal> <volume> 4(1) </volume> <pages> 47-73, </pages> <year> 1994. </year>
Reference-contexts: A more efficient implementation was introduced by M.C. Bartholomew-Biggs, L. Bartholomew-Biggs, and B. Christianson <ref> [3] </ref>, who used a recurrent recycling scheme for freed up dynamically-allocated memory. These and others works [18] demonstrated the runtime efficiency of the dynamic approach for some small- to medium-sized problems (n 500). <p> The SS and CS sparse representations make use of a bucket storage scheme, where each bucket consists of a (user-configurable) number of elemental data types. Memory is dynamically allocated in units of stores of buckets. In contrast to other sparse implementations <ref> [3, 19] </ref> which allocate memory for each nonzero separately, the bucket scheme allows for greater flexibility and runtime efficiency, at the cost of introducing some storage overhead (not all of the allocated memory is necessarily 7 Table 3.1: MINPACK-2 Unconstrained Optimization Problems Name Description of the Problem EPT Elastic-Plastic Torsion GL2
Reference: [4] <author> Christian Bischof, Ali Bouaricha, Peyvand Khademi, and Jorge More. </author> <title> Computing gradients in large-scale optimization using automatic differentiation. </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Though in some cases the explicit reformulation of the code for such functions is a viable alternative leading to effcient derivative computations of equivalent compressed Jacobians (see Section 4.1 and also <ref> [4] </ref>), in other cases, an algorithmic approach that transparently (i.e., without the need to rewrite code) exploits this "under-the-rug" sparsity may be preferable. <p> We also note that this cost can be further reduced by efficient hand-coding of the gradient or, in the case of partially separable functions, by reformulating the gradient computation as a sparse Jacobian computation <ref> [4] </ref>. The variance between the degree of disparity between NONSPARSE and SPARSE results is due to the differing sparsity characteristics of the MINPACK-2 problems. Figure 3.2 depicts the sparsity characteristics of the MSA problem, for the case of n = 2; 500.
Reference: [5] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: Previous studies have shown that AD is a powerful technique for computing derivatives. Accuracy and runtime efficiency of AD are superior to difference approximations, and the reliability, flexibility and development-time efficacy of AD surpass those of hand-coding approaches. For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) <ref> [5, 7, 8] </ref> has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling [11].
Reference: [6] <author> Christian Bischof, Alan Carle, and Peyvand Khademi. </author> <title> Fortran 77 interface specification to the SparsLinC library. </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Bartholomew-Biggs, L. Bartholomew-Biggs, and B. Christianson [3], who used a recurrent recycling scheme for freed up dynamically-allocated memory. These and others works [18] demonstrated the runtime efficiency of the dynamic approach for some small- to medium-sized problems (n 500). The SparsLinC (Sparse Linear Combinations) library <ref> [6, 8] </ref> is a software package for exploiting sparsity in AD, using the dynamic approach. <p> The computation of the Jacobian using SparsLinC yields the sparsity pattern of the Jacobian as a natural byproduct of the work it does in computing the Jacobian. A detailed description of the SparsLinC library will be reported elsewhere, and the full Fortran interface specification can be found in <ref> [6] </ref>. 3 Computing Gradients of Partially Separable Functions The MINPACK-2 Test Problem Collection [1] contains a number of unconstrained optimization problems from a variety of application areas. Table 3.1 shows the six MINPACK-2 problems we used in our gradient experiments.
Reference: [7] <author> Christian Bischof, Alan Carle, Peyvand Khademi, and Andrew Mauer. </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, 1994. </title> <type> Preprint MCS-P481-1194, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR94491, Center for Research on Parallel Computation, Rice University. </institution> <note> To appear in IEEE Computational Science & Engineering. </note>
Reference-contexts: Previous studies have shown that AD is a powerful technique for computing derivatives. Accuracy and runtime efficiency of AD are superior to difference approximations, and the reliability, flexibility and development-time efficacy of AD surpass those of hand-coding approaches. For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) <ref> [5, 7, 8] </ref> has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling [11].
Reference: [8] <author> Christian Bischof, Alan Carle, Peyvand Khademi, Andrew Mauer, and Paul Hovland. </author> <note> ADIFOR 2.0 user's guide. Technical Memorandum ANL/MCS-TM-192, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year> <note> CRPC Technical Report CRPC-95516-S. </note>
Reference-contexts: Previous studies have shown that AD is a powerful technique for computing derivatives. Accuracy and runtime efficiency of AD are superior to difference approximations, and the reliability, flexibility and development-time efficacy of AD surpass those of hand-coding approaches. For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) <ref> [5, 7, 8] </ref> has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling [11]. <p> Bartholomew-Biggs, L. Bartholomew-Biggs, and B. Christianson [3], who used a recurrent recycling scheme for freed up dynamically-allocated memory. These and others works [18] demonstrated the runtime efficiency of the dynamic approach for some small- to medium-sized problems (n 500). The SparsLinC (Sparse Linear Combinations) library <ref> [6, 8] </ref> is a software package for exploiting sparsity in AD, using the dynamic approach. <p> Here the computations involving a large work array in the original function are unrelated to derivative computation; hence ADIFOR does not generate a corresponding augmented derivative work array (for a full discussion of this issue, see "Variable Nomination" in <ref> [8] </ref>). We also note that the equivalence of the M fF g's and M fG AD16 g's of the EPT, ODC, PJB and SSC problems is due to the similarity of the code structures of these problems.
Reference: [9] <author> Christian Bischof, George Corliss, Larry Green, Andreas Griewank, Kara Haigler, and Perry Newman. </author> <title> Automatic differentiation of advanced CFD codes for multidisciplinary design. </title> <journal> Journal on Computing Systems in Engineering, </journal> <volume> 3(6) </volume> <pages> 625-638, </pages> <year> 1992. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics <ref> [9, 10, 15] </ref>, weather modeling [26], and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs.
Reference: [10] <author> Christian Bischof, Larry Green, Kitty Haigler, and Tim Knauff. </author> <title> Parallel calculation of sensitivity derivatives for aircraft design using automatic differentiation. </title> <booktitle> In Proceedings of the 5th AIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <pages> AIAA 94-4261, pages 73-84. </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics <ref> [9, 10, 15] </ref>, weather modeling [26], and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs. <p> The runtime penalty for this approach is an extraneous function recomputation per strip, which can be effectively amortized for large strip sizes. Since each strip is computed independently, stripmining can also be used for easy parallelization of the gradient computation <ref> [10] </ref>. fl The Unix command `size executable-file' reports the total amount of statically allocated memory (i.e., the memory requirements that can be assessed at link-time such as array sizes, etc.) needed to load and run the executable.
Reference: [11] <author> Christian Bischof, Greg Whiffen, Christine Shoemaker, Alan Carle, and Aaron Ross. </author> <title> Application of automatic differentiation to groundwater transport models. </title> <editor> In Alexander Peters, editor, </editor> <booktitle> Computational Methods in Water Resources X, </booktitle> <pages> pages 173-182, </pages> <address> Dor-drecht, 1994. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling <ref> [11] </ref>. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs.
Reference: [12] <author> Christian H. Bischof and Moe El-Khadiri. </author> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR. </title> <type> Technical Report ANL/MCS-TM-163, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Previous attempts at exploiting partial separability in the context of AD have been based on manual modification of the code for the function in such a way as to transform the derivative computation from a dense gradient problem to a sparse Jacobian problem <ref> [12] </ref>. The transparent exploitation of partial separability in SparsLinC does not require code modification. Instead, it relies on the sparse representation and processing of the VLCs involving the element functions, and on the efficiency of the plus-equals module for the accumulation of the dense gradient vector.
Reference: [13] <author> Christian H. Bischof, William T. Jones, Andrew Mauer, and Jamshid Samareh. </author> <title> Experiences with the application of the ADIC automatic differentiation tool to the CSCMDO 3-D volume grid generation code. </title> <booktitle> In Proceedings of the 34th AIAA Aerospace Sciences Meeting, </booktitle> <pages> pages AIAA 96-0716, </pages> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC <ref> [13] </ref> for C programs. The forward and reverse modes are the two basic modes of AD, and are distinguished by the manner in which the chain rule is applied for propagating derivatives. This basic difference impacts the computational complexity and flexibility of each mode.
Reference: [14] <author> Ali Bouaricha and Jorge More. </author> <title> Impact of partial separability on large-scale optimization. </title> <type> Preprint MCS-P487-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization <ref> [2, 14] </ref>, computational fluid dynamics [9, 10, 15], weather modeling [26], and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs. <p> The detection of the sparsity pattern of Jacobians is of interest in a number of computations (e.g., see <ref> [14] </ref>). As we shall see in Section 4, sparsity detection is a prerequisite for compressing Jacobians using coloring methods. In our Jacobian experiments we use prepackaged MINPACK-2 routines for detecting the sparsity pattern of the Jacobian for each problem.
Reference: [15] <author> Alan Carle, Lawrence Green, Christian Bischof, and Perry Newman. </author> <title> Applications of automatic differentiation in CFD. </title> <booktitle> In Proceedings of the 25th AIAA Fluid Dynamics Conference, </booktitle> <institution> AIAA Paper 94-2197. American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics <ref> [9, 10, 15] </ref>, weather modeling [26], and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs.
Reference: [16] <author> Thomas F. Coleman, Burton S. Garbow, and Jorge J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10(3) </volume> <pages> 329-345, </pages> <year> 1984. </year>
Reference-contexts: These algorithms produce a partitioning of the columns of the Jacobian into p structurally orthogonal groups by coloring the column-intersection graph associated with the Jacobian. In our experiments we employ the graph-coloring software described in <ref> [16] </ref> to obtain S, and then compute C (x) by initializing the ADIFOR-generated derivative code of f (x) with the seed matrix set to S. From the point of view of computational complexity, the consequence of the compressed Jacobian approach is clear.
Reference: [17] <author> Thomas F. Coleman and Jorge J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 187-209, </pages> <year> 1983. </year>
Reference-contexts: The success of the static method depends on finding a minimal number of columns, since this number is a good approximation for the computational complexity of these methods, as a multiplicative factor of the cost to compute the function. Coleman and More <ref> [17] </ref> introduced an algorithm for a terse mapping based on graph-coloring heuristics, and successfully applied this algorithm to large-scale optimization problems. Their approach applies equally well to automatic differentiation as it does to finite differencing, and we make use of it in this paper. <p> In contrast to partially separable functions where some gradient vectors may be dense, the AD computation of sparse Jacobians, by definition, involves only sparse directional gradient vectors. In Section 4 we briefly review the compressed Jacobian approach, which has previously been used <ref> [17, 2] </ref> as an effective strategy for computing sparse Jacobians in cases where the closure sparsity pattern is known. <p> Then, we present the memory requirements and runtime results of both approaches for the MINPACK-2 and the MDG test problems. 4.1 The Compressed Jacobian Approach The compressed Jacobian approach has been used as an effective strategy for computing sparse Jacobians in conjunction with both finite differencing <ref> [17] </ref> and automatic differentiation methods [2]. The prerequisite for applying the compressed Jacobian approach to a given sparse Jacobian computation is a priori knowledge of the sparsity pattern of the Jacobian.
Reference: [18] <author> Laurence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <pages> pages 114-125, </pages> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: A more efficient implementation was introduced by M.C. Bartholomew-Biggs, L. Bartholomew-Biggs, and B. Christianson [3], who used a recurrent recycling scheme for freed up dynamically-allocated memory. These and others works <ref> [18] </ref> demonstrated the runtime efficiency of the dynamic approach for some small- to medium-sized problems (n 500). The SparsLinC (Sparse Linear Combinations) library [6, 8] is a software package for exploiting sparsity in AD, using the dynamic approach.
Reference: [19] <author> Laurence C. W. Dixon, Z. A. Maany, and M. Mohseninia. </author> <title> Automatic differentiation of large sparse systems. </title> <journal> Journal of Economic Dynamics and Control, </journal> <volume> 14(2), </volume> <year> 1990. </year>
Reference-contexts: The dynamic approach, which can also be applied to gradient computations, involves using dynamically allocated memory and sparse data representations for storing and processing only the nonzero information in the derivative objects. An implementation of this approach was introduced by Dixon, Maany, and Mohseninia <ref> [19] </ref>, using the operator overloading capabilities of ADA, and specific to codes written in that language. <p> The SS and CS sparse representations make use of a bucket storage scheme, where each bucket consists of a (user-configurable) number of elemental data types. Memory is dynamically allocated in units of stores of buckets. In contrast to other sparse implementations <ref> [3, 19] </ref> which allocate memory for each nonzero separately, the bucket scheme allows for greater flexibility and runtime efficiency, at the cost of introducing some storage overhead (not all of the allocated memory is necessarily 7 Table 3.1: MINPACK-2 Unconstrained Optimization Problems Name Description of the Problem EPT Elastic-Plastic Torsion GL2
Reference: [20] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <journal> SIAM News, </journal> <volume> 24, No. 3, </volume> <editor> p. </editor> <volume> 20 & No. 4, </volume> <editor> p. </editor> <volume> 8, </volume> <year> 1991. </year>
Reference-contexts: By successive applications of the chain rule to the composition of those elementary operations, derivatives can be computed exactly (up to machine roundoff ) and in a completely mechanical fashion. For a detailed description of AD, see <ref> [20] </ref>. Previous studies have shown that AD is a powerful technique for computing derivatives. Accuracy and runtime efficiency of AD are superior to difference approximations, and the reliability, flexibility and development-time efficacy of AD surpass those of hand-coding approaches.
Reference: [21] <author> Andreas Griewank, David Juedes, and Jean Utke. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Transactions on Mathematical Software, </journal> <month> June </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C <ref> [21] </ref> and ADIC [13] for C programs. The forward and reverse modes are the two basic modes of AD, and are distinguished by the manner in which the chain rule is applied for propagating derivatives. This basic difference impacts the computational complexity and flexibility of each mode.
Reference: [22] <author> Andreas Griewank and Philippe L. Toint. </author> <title> On the unconstrained optimization of partially separable objective functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, </booktitle> <pages> pages 301-312, </pages> <address> London, 1981. </address> <publisher> Academic Press. </publisher>
Reference-contexts: However, it has been shown that if the Hessian of f is sparse, then f is partially separable 2 <ref> [22] </ref>; that is, f can be expressed as f (x) = i=1 where m is the number of element functions f i (x), and each f i (x) is typically a function of just a few of the components of x, implying that each g i (x) = df i dx
Reference: [23] <author> Jim E. Horwedel. GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 243-250. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Other examples of available AD tools are Odyssee [27] and GRESS <ref> [23] </ref> for Fortran, and ADOL-C [21] and ADIC [13] for C programs. The forward and reverse modes are the two basic modes of AD, and are distinguished by the manner in which the chain rule is applied for propagating derivatives.
Reference: [24] <author> J. J. More and Z. Wu. </author> <title> Global continuation for distance geometry problems. </title> <type> Technical Report MCS-P505-0395, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: The codes are taken from the MINPACK-2 test set problems [1] and the molecular distance geometry class of global minimization functions <ref> [24] </ref>. In Section 2 we look at sparsity in the context of the computation of VLCs in AD and 3 show how this can be exploited with SparsLinC. Computational results for the gradient and Jacobian experiments are shown in Sections 3 and 4, respectively. <p> The MINPACK-2 problems are identified in Table 4.1. These are a standard set of test problems; the reader is referred to [1] for detailed descriptions. The MDG problem is described in More and Wu <ref> [24] </ref> as follows: Given bond lengths ffi i;j between a subset S of the atom pairs, determine whether there is a molecule that satisfies these bond length constraints.
Reference: [25] <author> Garry N. Newsam and John D. Ramsdell. </author> <title> Estimation of sparse Jacobian matrices. </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> <volume> 4(3) </volume> <pages> 404-418, </pages> <year> 1983. </year>
Reference-contexts: Their approach applies equally well to automatic differentiation as it does to finite differencing, and we make use of it in this paper. Using an alternative approach, Newsam and Ramsdell <ref> [25] </ref> showed that the sparse Jacobian can be estimated with q + 1 function evaluations, where q is the maximum of the number of nonzero elements in any row of the Jacobian. However, this approach requires the solution of a system of linear equations with a potentially ill-conditioned matrix.
Reference: [26] <author> Seon Ki Park, Kelvin Droegemeier, Christian Bischof, and Tim Knauff. </author> <title> Sensitivity analysis of numerically-simulated convective storms using direct and adjoint methods. </title> <booktitle> In Preprints, 10th Conference on Numerical Weather Prediction, Portland, Oregon, </booktitle> <pages> pages 457-459. </pages> <publisher> American Meterological Society, </publisher> <year> 1994. </year>
Reference-contexts: For example, the AD tool ADIFOR (Automatic DIfferentiation of FORtran) [5, 7, 8] has been used to generate derivative codes for many applications in areas such as large-scale optimization [2, 14], computational fluid dynamics [9, 10, 15], weather modeling <ref> [26] </ref>, and groundwater modeling [11]. Other examples of available AD tools are Odyssee [27] and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs.
Reference: [27] <author> Nicole Rostaing, Stephane Dalmas, and Andre Galligo. </author> <title> Automatic differentiation in Odyssee. </title> <address> Tellus, 45a(5):558-568, </address> <month> October </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Other examples of available AD tools are Odyssee <ref> [27] </ref> and GRESS [23] for Fortran, and ADOL-C [21] and ADIC [13] for C programs. The forward and reverse modes are the two basic modes of AD, and are distinguished by the manner in which the chain rule is applied for propagating derivatives.
References-found: 27

