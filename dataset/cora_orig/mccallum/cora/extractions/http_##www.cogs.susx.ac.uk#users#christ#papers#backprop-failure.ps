URL: http://www.cogs.susx.ac.uk/users/christ/papers/backprop-failure.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Title: Backpropagation Can't Do Parity Generalisation  
Author: Chris Thornton 
Keyword: Keyworks: Learning, Theoretical limitations  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Web: WWW: http://www.cogs.susx.ac.uk  
Date: January 21, 1997  
Abstract: It is accepted that the early connectionist learning methods such as the perceptron algorithm cannot solve parity learning problems. But since the early 1980s, there have been many demonstrations purporting to show that the backpropagation method can do so. However these demonstrations are misleading. Backpropagation in fact reliably fails to solve parity problems when they are posed as genuine, supervised learning problems, i.e., as problems involving generalisation. Thus backpropagation is subject to some of the same limitations as the perceptron method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Learning procedures such as the perceptron learning algorithm are known to be incapable of acquiring parity mappings <ref> [1] </ref>. But even state-of-the-art symbolic methods such as C4.5 [2] and backpropagation [3] generalise poorly from incomplete parity mappings. With 4-bit parity, 16 minimally incomplete training sets (i.e., training sets which contain all but one of the possible cases) can be constructed.
Reference: [2] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning procedures such as the perceptron learning algorithm are known to be incapable of acquiring parity mappings [1]. But even state-of-the-art symbolic methods such as C4.5 <ref> [2] </ref> and backpropagation [3] generalise poorly from incomplete parity mappings. With 4-bit parity, 16 minimally incomplete training sets (i.e., training sets which contain all but one of the possible cases) can be constructed. C4.5 actually generalises incorrectly in all 16 cases; i.e., it always `gets the answer wrong'.
Reference: [3] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). 6 </pages>
Reference-contexts: Learning procedures such as the perceptron learning algorithm are known to be incapable of acquiring parity mappings [1]. But even state-of-the-art symbolic methods such as C4.5 [2] and backpropagation <ref> [3] </ref> generalise poorly from incomplete parity mappings. With 4-bit parity, 16 minimally incomplete training sets (i.e., training sets which contain all but one of the possible cases) can be constructed. C4.5 actually generalises incorrectly in all 16 cases; i.e., it always `gets the answer wrong'. Backpropagation performs no better.
References-found: 3

