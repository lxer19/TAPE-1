URL: ftp://ftp.cs.bris.ac.uk/pub/users/cgc/ECML98/gama.ps.Z
Refering-URL: http://www.cs.bris.ac.uk/~cgc/ECML98-WS/Summary.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: jgama@ncc.up.pt  
Title: Local cascade generalization  
Author: Jo~ao Gama 
Address: Rua Campo Alegre, 823 4150 Porto, Portugal  
Affiliation: LIACC, FEP University of Porto  
Abstract: In a previous work we have presented Cascade Generalization, a new general method for merging classifiers. The basic idea of Cascade Generalization is to sequentially run the set of classifiers, at each step performing an extension of the original data by the insertion of new attributes. The new attributes are derived from the probability class distribution given by a base classifier. This constructive step extends the representational language for the high level classifiers, relaxing their bias. In this paper we extend the previous work by applying Cascade locally. At each iteration of a divide and conquer algorithm, a reconstruction of the instance space occurs by the addition of new attributes. Each new attribute is the probability that an example belongs to a class given by a base classifier. We have implemented three Local Generalization Algorithms. The first merges a linear discriminant with a decision tree, the second merges a naive Bayes with a decision tree, and the third merges a linear discriminant and a naive Bayes with a decision tree. All the algorithms show an increase of performance, when compared with the corresponding single models. Cascade also outperforms other methods for combining classifiers, like Stacked Generalization and competes well against Boosting, with statistically significant confidence levels.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali and M. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> Vol. 24, No. 1, </volume> <year> 1996. </year>
Reference-contexts: Voting is the most common method used to combine classifiers. As pointed out in Ali <ref> [1] </ref>, this strategy is motivated by the Bayesian learning theory which stipulates that in order to maximize the predictive accuracy, instead of using just a single learning model, one should ideally use all models in the hypothesis space. <p> Under an analysis of bias-variance decomposition of the error of the classifier [14], the reduction of 71 the error observed when using Boosting or Bagging is mainly due to the reduction in the variance. Ali <ref> [1] </ref> refers to that "the number of training examples needed by Boosting increases as a function of the accuracy of the learned model. Boosting could not be used to learn many models on the modest training set sizes used in this paper.". <p> Usual learning algorithms produced by the Machine Learning community uses categories when classifying examples. Combining classifiers by means of categorical classes looses the strength of the classifier in its prediction. The use of probability class distributions allows us to explore that information. 5 Empirical Evaluation 5.1 The Algorithms Ali <ref> [1] </ref> and Tumer [21] present empirical and analytical results that show that "the combined error rate depend on the error rate of individual classifiers and the correlation among them". In order to reduce the correlation errors they suggest the use of "radically different types of classifiers".
Reference: [2] <author> L. Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical report 460, </type> <institution> Statistics Department, University of California, </institution> <year> 1996. </year>
Reference-contexts: The smoothed class distributions have influence in the pruning mechanism and in the process of missing values, and is the most relevant difference with C4.5. A decision tree uses a subset of the available attributes in order to classify a query point. Kohavi and Wolpert [14], Breiman <ref> [2, 3] </ref> among other researchers, note that Decision Trees are unstable classifiers. Small variations on the training set could cause large changes in the resulting predictors.
Reference: [3] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group., </booktitle> <year> 1984. </year>
Reference-contexts: Breiman <ref> [3] </ref> proposes Bagging, that produces replications of the training set by sampling with replacement. Each replication of the training set has the same size as the original data, but some examples don't appear in it, while others may appear more than once. <p> The smoothed class distributions have influence in the pruning mechanism and in the process of missing values, and is the most relevant difference with C4.5. A decision tree uses a subset of the available attributes in order to classify a query point. Kohavi and Wolpert [14], Breiman <ref> [2, 3] </ref> among other researchers, note that Decision Trees are unstable classifiers. Small variations on the training set could cause large changes in the resulting predictors.
Reference: [4] <author> C. Brodley. </author> <title> Recursive automatic bias selection for classifier construction. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <year> 1995. </year>
Reference-contexts: The level one data are the outputs of the base classifiers. Another learning process occurs using as input the level one data and as output the final classification. This is a more sophisticated technique of cross validation that could reduce the error due to the bias. Brodley <ref> [4] </ref> presents MCS, a hybrid algorithm that combines in a single tree, nodes that are univariate tests, multivariate tests generated by linear machines and instance based learners.
Reference: [5] <author> C. Brodley and P. Utgoff. </author> <title> Multivariate trees. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <year> 1995. </year>
Reference-contexts: For example, small modifications to C4.5 3 , will allow the construction of a CGT ree whose internal nodes are trees generated by C4.5. 4 Related Work With respect to the final model, there are clear similarities between CGLtree and Mul-tivariate trees <ref> [5] </ref>. Also the final model of CGBtree is related with the recursive naive Bayes presented in [15]. In a previous work [12], we have compared system Ltree, similar to CGLtree, with Oc1 [16] and LMDT [5]. The focus of this paper is on methodologies for combining classifiers. <p> With respect to the final model, there are clear similarities between CGLtree and Mul-tivariate trees <ref> [5] </ref>. Also the final model of CGBtree is related with the recursive naive Bayes presented in [15]. In a previous work [12], we have compared system Ltree, similar to CGLtree, with Oc1 [16] and LMDT [5]. The focus of this paper is on methodologies for combining classifiers. As such, we review other methods that generate and combine multiple models. 4.1 Combining Classifications We can consider two main lines of research.
Reference: [6] <author> Wray Buntine. </author> <title> A theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1990. </year>
Reference-contexts: That is, all nodes in the path contribute to the final classification. Instead of computing class distribution for all paths in the tree at classification time, as it is done, by example, in Buntine <ref> [6] </ref>, Dtree computes a class distribution for all nodes when growing the tree.
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <editor> In A. Prieditis and S. Russel, editors, </editor> <booktitle> Machine Learning Proc of 12th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: The set of rules incorporates knowledge from expert domains. MCS uses a dynamic search control strategy in order to perform an automatic model selection. MCS builds trees, which could apply a different model for different regions of the instance space. Chan and Stolfo <ref> [7] </ref> presents two schemes for classifier combination: arbiter and combiner. Both schemes are based on meta learning, where a meta-classifier is generated from a training data, built based on the predictions of the base classifiers.
Reference: [8] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In KDD 95, </booktitle> <year> 1995. </year>
Reference-contexts: This arbiter, together with an arbitration rule, decides a final classification based on the base predictions. An example of an arbitration rule is "Use the prediction of the arbiter when the base classifiers cannot obtain a majority". Later <ref> [8] </ref>, they have extended this framework using arbiters=combiners in an hierarchical fashion generating arbiter=combiner binary trees. 4.3 Discussion Reported results relative to Boosting or Bagging are quite impressive. Using 10 iterations (this is generating 10 classifiers) Quinlan [19] reports reductions of the error rate between 10% and 19%.
Reference: [9] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning Proc. of 13th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: If the attributes are independent, p (EjCi) can be decomposed into the product p (v 1 jC i ) fl ::: fl p (v k jC i ). Domingos and Pazzani <ref> [9] </ref> show that this procedure as a surprisingly good performance in a wide variety of domains, including many where there are clear dependencies between attributes. The required probabilities are computed from the training set. In the case of nominal attributes we use counts. Continuous attributes were discretized 4 . <p> This has been found to produce better results than assuming a Gaussian distribution. The number of bins that we use is a function of the number of different values observed on the training set: k = min (10; nr: dif f erent values). This heuristic was used in <ref> [9] </ref> and elsewhere with good overall results. Missing values were treated as another possible value for the attribute. In order to classify a query point, a Naive Bayes uses all of the available attributes.
Reference: [10] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <editor> In A. Prieditis and S. Russel, editors, </editor> <booktitle> Machine Learning Proc. of 12th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Missing values were treated as another possible value for the attribute. In order to classify a query point, a Naive Bayes uses all of the available attributes. Langley [15] refers that Naive Bayes relies on an important assumption: that the variability of the 4 See <ref> [10] </ref> for a good discussion on the theme. 72 dataset can be summarized by a single probabilistic description, and that these are suffi-cient to distinguish between classes. From an analysis of Bias-Variance, this implies that Naive Bayes uses a reduced set of models to fit to the data.
Reference: [11] <author> Y. Freund and R. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning Proc of 13th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: From each replication of the training set a classifier is generated. All classifiers are used in order to classify each example on the test set, usually using a uniform vote scheme. The Boosting algorithm from Freund and Schapire <ref> [11] </ref> maintains a weight for each example in the training set that reflects its importance. Adjusting the weights causes the learner to focus on different examples leading to different classifiers. Boosting is an iterative algorithm.
Reference: [12] <author> J. Gama. </author> <title> Probabilistic linear tree. </title> <editor> In D. Fisher, editor, </editor> <booktitle> Machine Learning Proc. of the 14th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Also the final model of CGBtree is related with the recursive naive Bayes presented in [15]. In a previous work <ref> [12] </ref>, we have compared system Ltree, similar to CGLtree, with Oc1 [16] and LMDT [5]. The focus of this paper is on methodologies for combining classifiers. As such, we review other methods that generate and combine multiple models. 4.1 Combining Classifications We can consider two main lines of research.
Reference: [13] <author> J. Gama. </author> <title> Combining classifiers by constructive induction. </title> <editor> In C. Nedellec and C. Rouveirol, editors, </editor> <booktitle> Machine Learning ECML-98. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1998. </year>
Reference-contexts: Instead of looking for methods that fit the data using a single representation language, we present a family of algorithms, under the generic name of Cascade Generalization, whose search space contains models that use different representation languages. Cascade generalization was first presented in <ref> [13] </ref>. It performs an iterative composition of classifiers. At each iteration a classifier is generated. The input space is extended by the addition of new attributes. Those new attributes are obtained in the form of a probability class distribution given, for each example, by the generated base classifier. <p> The language of the final classifier is the language used by the high level generalizer. But it uses terms that are expressions from the language of low level classifiers. In this sense, Cascade Generalization generates a unified theory from the base theories. Here we extend the work presented in <ref> [13] </ref>, by applying Cascade locally. In our implementation, Local Cascade Generalization generates a decision tree. The experimental study shows that this methodology usually improves both accuracy and theory size with statistical significance levels. The next section presents the framework of cascade generalization. <p> The power of this approach comes from the ability to split the hyperspace into subspaces and fit each subspace with different functions. In our previous work <ref> [13] </ref> we have shown that Cascade, significantly improves the performance of this type of learning algorithms. In this paper we explore the applicability of Cascade on the problems and subproblems 68 that a divide and conquer algorithm must solve. <p> In future instances these referees are first consulted to select the most appropriate prediction model and the prediction of the selected model is then returned. 2 This heuristic was suggested at Breiman et all. 3 Two different methods are presented in <ref> [13, 20] </ref>. 70 4.2 Generating different models Several methods for generating multiple models appear in the literature. Breiman [3] proposes Bagging, that produces replications of the training set by sampling with replacement.
Reference: [14] <author> R Kohavi and D. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss function. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning Proc. of 13th International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Quinlan argues that these techniques are mainly applicable for unstable classifiers. Both techniques require that the learning system should not be stable, in order to obtain different classifiers when there are small changes in the training set. Under an analysis of bias-variance decomposition of the error of the classifier <ref> [14] </ref>, the reduction of 71 the error observed when using Boosting or Bagging is mainly due to the reduction in the variance. Ali [1] refers to that "the number of training examples needed by Boosting increases as a function of the accuracy of the learned model. <p> The smoothed class distributions have influence in the pruning mechanism and in the process of missing values, and is the most relevant difference with C4.5. A decision tree uses a subset of the available attributes in order to classify a query point. Kohavi and Wolpert <ref> [14] </ref>, Breiman [2, 3] among other researchers, note that Decision Trees are unstable classifiers. Small variations on the training set could cause large changes in the resulting predictors.
Reference: [15] <author> P. Langley. </author> <title> Induction of recursive bayesian classifiers. </title> <editor> In P.Brazdil, editor, </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-93. </address> <publisher> LNAI 667, Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Also the final model of CGBtree is related with the recursive naive Bayes presented in <ref> [15] </ref>. In a previous work [12], we have compared system Ltree, similar to CGLtree, with Oc1 [16] and LMDT [5]. The focus of this paper is on methodologies for combining classifiers. <p> This heuristic was used in [9] and elsewhere with good overall results. Missing values were treated as another possible value for the attribute. In order to classify a query point, a Naive Bayes uses all of the available attributes. Langley <ref> [15] </ref> refers that Naive Bayes relies on an important assumption: that the variability of the 4 See [10] for a good discussion on the theme. 72 dataset can be summarized by a single probabilistic description, and that these are suffi-cient to distinguish between classes.
Reference: [16] <author> S. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <year> 1994. </year>
Reference-contexts: Also the final model of CGBtree is related with the recursive naive Bayes presented in [15]. In a previous work [12], we have compared system Ltree, similar to CGLtree, with Oc1 <ref> [16] </ref> and LMDT [5]. The focus of this paper is on methodologies for combining classifiers. As such, we review other methods that generate and combine multiple models. 4.1 Combining Classifications We can consider two main lines of research.
Reference: [17] <author> J. Ortega. </author> <title> Exploiting multiple existing models and learning algorithms. </title> <booktitle> In AAAI 96 Workshop in Induction of Multiple Learning Models, </booktitle> <year> 1995. </year>
Reference-contexts: From uniform voting where the opinion of all base classifiers contributes to the final classification with the same strength, to weighted voting, where each base classifier has a weight associated, that could change over the time, and strengthens the classification given by the classifier. Ortega <ref> [17] </ref> presents MAI "Model Applicability Induction" approach for combining predictions from multiple models. The approach consists on learning for each available model a referee that characterize situations in which each of the models is able to make correct predictions.
Reference: [18] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: P (eje n ) is the probability that one example that falls at N ode n goes to N ode n+1 , and P (eje n ; C i ) is the probability that one example from class C i goes from N ode n to 73 N ode n+1 <ref> [18] </ref>. This recursive formulation, allows Dtree efficiently compute the required class distributions on the fly. The smoothed class distributions have influence in the pruning mechanism and in the process of missing values, and is the most relevant difference with C4.5.
Reference: [19] <author> R. Quinlan. Bagging, </author> <title> boosting and c4.5. </title> <booktitle> In Procs. 13th American Association for Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Later [8], they have extended this framework using arbiters=combiners in an hierarchical fashion generating arbiter=combiner binary trees. 4.3 Discussion Reported results relative to Boosting or Bagging are quite impressive. Using 10 iterations (this is generating 10 classifiers) Quinlan <ref> [19] </ref> reports reductions of the error rate between 10% and 19%. Quinlan argues that these techniques are mainly applicable for unstable classifiers. Both techniques require that the learning system should not be stable, in order to obtain different classifiers when there are small changes in the training set.
Reference: [20] <author> K.M. Ting and I.H. Witten. </author> <title> Stacked generalization: when does it work ? In Procs. </title> <booktitle> International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: In future instances these referees are first consulted to select the most appropriate prediction model and the prediction of the selected model is then returned. 2 This heuristic was suggested at Breiman et all. 3 Two different methods are presented in <ref> [13, 20] </ref>. 70 4.2 Generating different models Several methods for generating multiple models appear in the literature. Breiman [3] proposes Bagging, that produces replications of the training set by sampling with replacement. <p> Boosting could not be used to learn many models on the modest training set sizes used in this paper.". Wolpert [22] says that successful implementations of Stacked Generalization is a "black art", for classification tasks and the conditions under which stacking works are still unknown. Recently, Ting and Witten <ref> [20] </ref> have shown that successful stacked generalization requires to use output class distributions rather than class predictions. In their experiments, only the MLR algorithm (a linear discriminant) was suitable for level-1 generalizer. The main point of the proposed method is its ability to merge different models. <p> 32.865 0.65 - 9.062 1.07 + 13.303 1.63 + 24.922 3.71 + 27.731 5.06 2.222 2.87 2.778 3.93 Table 2: Results of (a)Local Cascade Generalization (b)Boosting and Stacked Table 2b presents the results of C5.0 boosting with the default parameter of 10, and Stacked Generalization as it is defined in <ref> [20] </ref>. That is, the level 0 classifiers are C4.5 and Bayes, the level 1 classifier is Discrim. The attributes for the level 1 data, are the probability class distribution, obtained from the level 0 classifiers using a 5 stratified cross validation.
Reference: [21] <author> K. Tumer and J. Ghosh. </author> <title> Classifier combining: analytical results and implications. </title> <booktitle> In AAAI 96 - Workshop in Induction of Multiple Learning Models, </booktitle> <year> 1995. </year>
Reference-contexts: Combining classifiers by means of categorical classes looses the strength of the classifier in its prediction. The use of probability class distributions allows us to explore that information. 5 Empirical Evaluation 5.1 The Algorithms Ali [1] and Tumer <ref> [21] </ref> present empirical and analytical results that show that "the combined error rate depend on the error rate of individual classifiers and the correlation among them". In order to reduce the correlation errors they suggest the use of "radically different types of classifiers".
Reference: [22] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <publisher> In Pergamon Press, </publisher> <editor> editor, </editor> <booktitle> Neural Networks Vol.5, </booktitle> <year> 1992. </year> <month> 77 </month>
Reference-contexts: We can consider two main lines: from one side methods that try to select the most appropriate algorithm for the given task, for instance Schaffer's selection by Cross-Validation, and on the other side, methods that combine predictions of different algorithms, for instance Stacked Generalization <ref> [22] </ref>. This work follows the second research line. Instead of looking for methods that fit the data using a single representation language, we present a family of algorithms, under the generic name of Cascade Generalization, whose search space contains models that use different representation languages. <p> The weight of the misclassified examples is increased. The final classifier aggregates the learned classifier at each iteration by weighted voting. The weight of each classifier is a function of its accuracy. Wolpert <ref> [22] </ref> proposes Stacked Generalization, a technique that uses learning in two levels. A learning algorithm is used to determine how the outputs of the base classifiers should be combined. The original data set constitutes the level zero data. All the base classifiers run at this level. <p> Ali [1] refers to that "the number of training examples needed by Boosting increases as a function of the accuracy of the learned model. Boosting could not be used to learn many models on the modest training set sizes used in this paper.". Wolpert <ref> [22] </ref> says that successful implementations of Stacked Generalization is a "black art", for classification tasks and the conditions under which stacking works are still unknown. Recently, Ting and Witten [20] have shown that successful stacked generalization requires to use output class distributions rather than class predictions.
References-found: 22

