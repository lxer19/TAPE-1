URL: http://www.icsi.berkeley.edu/~amnon/Papers/thesis.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~amnon/
Root-URL: http://www.icsi.berkeley.edu
Abstract-found: 0
Intro-found: 1
Reference: [AES92] <author> N. Alon, P. Erd-os, and J. H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year> <note> This paper describes the Probabilistic Method as developed by Paul Erd-os and its applications in Discrete Mathematics and Theoretical Computer Science. </note>
Reference-contexts: In fact, Erdos and many others after him, used this phenomenon to develop a method, "The probabilistic method", for proving the existence of combinatorial objects with certain properties. The method has been very successful, and the interested reader is referred to <ref> [AES92] </ref>. A similar very successful rule of thumb says that for many reasonable combinatorial problems, if the "natural" randomized construction does not have the required property, this property can not be achieved at all.
Reference: [AGHP92] <author> Alon, Goldreich, Hastad, and Peralta. </author> <title> Simple constructions of almost k-wise independent random variables. Random Structures & Algorithms, </title> <type> 3, </type> <year> 1992. </year>
Reference-contexts: Two explicit constructions are extremely useful when derandomizing algorithms: * k-wise independence A small sample space that is k-wise independent (any k elements are mutually independent) [CG89]. An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) <ref> [NN93, AGHP92] </ref>. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties [Mar, LPS86]. The interested reader is referred to [Wig94] for a survey of pair-wise independence, and [MR95] for a reading on probabilistic algorithms and derandomization. <p> I f1; : : : ; ng, jIj = d), and for any d values b 1 ; : : : ; b d 2 f0; 1g: j [P rob x2S (^ i2I x i = b i ) ] 2 jIj j Theorem: [NN93] (see also <ref> [AGHP92] </ref>) For every integer q, d q and every &gt; 0, there is an explicit set S of q-bits vectors that is d-wise, biased, and of cardinality O ( (d log (q) 1 ) 2 ).
Reference: [AKL + 79] <author> R. Aleliunas, R.M. Karp, R.J. Lipton, L. Lovasz, and C. Rackoff. </author> <title> Random walks, universal sequences and the complexity of maze problems. </title> <booktitle> In Proceedings of the 20th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1979. </year>
Reference-contexts: In fact it might even be suspected that the way the city is built will cause you (with high probability) to stay within your current neighborhood and never go out of it. However, if you know the <ref> [AKL + 79] </ref> theorem you know that no matter what the topology of the city is, with overwhelming probability it will not take too long until this random walk will take you to the police station. <p> A special case of the connectivity problem for general graphs, is the connectivity problem for undirected graph, USTCON. In a beautiful paper, Aleliunas, Karp, Lipton, Lovasz and Rackoff <ref> [AKL + 79] </ref> showed that, with a high probability, a ran 18 dom walk over an undirected graph covers all the graph nodes in polynomial time, thus showing that USTCON can be solved in RL a result we already mentioned in section 1.1.1.
Reference: [AKS83] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> An O(n log n) sorting network. </title> <booktitle> In Proc. 15th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 1-9, </pages> <year> 1983. </year>
Reference-contexts: Using the AKS sorting networks <ref> [AKS83] </ref>, which belong to NC 1 , we get: Corollary 3.1.2 Sort is monotonically reducible to USTCON (poly).
Reference: [AKS87] <author> Ajtai, Komlos, and Szemeredi. </author> <title> Deterministic simulation in LOGSPACE. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1987. </year> <month> 87 </month>
Reference-contexts: We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by [KPS85, CG89, IZ89, CW89] and many others. Using expanders, this can be done using only n + O (k) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification. Theorem: Assume there is an (n + k; m = n; t; m 0 = n; * = 1 n ) extractor.
Reference: [AKSS89] <author> M. Ajtai, J. Komlos, W. Steiger, and E. Szemeredi. </author> <title> Almost sorting in one round. </title> <booktitle> In Advances in Computer Research, </booktitle> <volume> volume 5, </volume> <pages> pages 117-125, </pages> <year> 1989. </year>
Reference-contexts: Corollary 2.5.3 (following <ref> [AKSS89] </ref>, see [WZ93] lemma 6) There are explicit algorithms to find all relations except O (a nlog (n)) among n elements, in one round and using O ( n 2 a 2 polyloglog (n) ) comparisons.
Reference: [ALM + 92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 14-23, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, we have explicit constructions only for t = polylog (R). Zuckerman achieves the hardness result by amplifying the <ref> [ALM + 92] </ref> P CP proof system for N P , and using the F GLSS reduction from SAT to M AX Clique. In the following we write down the P CP amplification we achieve using the extractor of section 2.3, and the hardness result we get. <p> We do not describe what a P CP proof system is, and how the [FGL + 91] reduction works. The interested read is referred to [FGL + 91, BGLR93, Zuc93]. Theorem: <ref> [ALM + 92, AS92] </ref> N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor.
Reference: [Arm] <author> R. Armony. </author> <title> Private Communication. </title>
Reference-contexts: Indeed, several pseudo-random generators for the class RL exist. These pseudorandom generators use the derandomization tools listed in the previous sections: [Nis92] uses hash functions, [INW94] use expanders and <ref> [NZ93, Arm, Zuc96] </ref> use extractors. Still, none of these constructions is optimal, and it is a major open problem to show that pseudo-random generators for RL exist in L. 12 1.4 Our Work In this section we state the new results we achieved.
Reference: [AS92] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs; a new characterization of NP. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 2-13, </pages> <year> 1992. </year>
Reference-contexts: We do not describe what a P CP proof system is, and how the [FGL + 91] reduction works. The interested read is referred to [FGL + 91, BGLR93, Zuc93]. Theorem: <ref> [ALM + 92, AS92] </ref> N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor.
Reference: [BCD + 89] <author> A. Borodin, S.A. Cook, P.W. Dymond, W.L. Ruzzo, and M. Tompa. </author> <title> Two applications of inductive counting for complementation problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(3) </volume> <pages> 559-578, </pages> <year> 1989. </year>
Reference-contexts: Thus, the proofs by Immerman and Szelepcseny [Imm88, Sze88] that NL is closed under complement, came as a great surprise to the scientific community. The same technique, inductive counting, was used by Borodin et al <ref> [BCD + 89] </ref> to 7 But, in fact, what do we have to base our guess on? Do we have the slightest indication that L 6= NL? If we have any indications at all, they show that RL is very close to L, which turns the L 6= NL question into <p> However, this technique failed to solve the problem whether the class SL is closed under complement. As a consequence, an SL hierarchy was built <ref> [Rei82, BCD + 89] </ref>, and turned out to contain many interesting problems, such as 2-colorability [Rei82].
Reference: [BFNW93] <author> Babai, Fortnow, Nisan, and Wigderson. </author> <title> BPP has subexponential time simulations unless EXPTIME has publishable proofs. Computational Complexity, </title> <type> 3, </type> <year> 1993. </year>
Reference-contexts: Building on the work of Shamir [Sha81], Blum and Micali [BM82] and Yao [Yao82], Impagliazzo, Levin and Luby [ILL89] showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert). Nisan and Wigderson <ref> [NW88, BFNW93] </ref> showed that a pseudorandom generator for P that runs in exponential time (in the seed length) exists iff there exists a function solvable in exponential time that is hard for any polynomial size circuit.
Reference: [BGLR93] <author> M. Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 294-304, </pages> <year> 1993. </year> <month> 88 </month>
Reference-contexts: In the following we write down the P CP amplification we achieve using the extractor of section 2.3, and the hardness result we get. We do not describe what a P CP proof system is, and how the [FGL + 91] reduction works. The interested read is referred to <ref> [FGL + 91, BGLR93, Zuc93] </ref>. Theorem: [ALM + 92, AS92] N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor.
Reference: [BM82] <author> M. Blum and S. Micali. </author> <title> How to generate cryptographically strong se-quences of pseudo random bits. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1982. </year>
Reference-contexts: Building on the work of Shamir [Sha81], Blum and Micali <ref> [BM82] </ref> and Yao [Yao82], Impagliazzo, Levin and Luby [ILL89] showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert).
Reference: [BPS92] <author> Y. Ben-Asher, D. Peleg, and A. Schuster. </author> <title> The complexity of reconfigur-ing networks models. </title> <booktitle> In Proc. of the Israel Symposium on the Theory of Computing and Systems, </booktitle> <month> May </month> <year> 1992. </year> <note> To appear Information and Computation. </note>
Reference-contexts: As the symmetric Logspace hierarchy defined in [Rei82] is known to be within L &lt;SL&gt; , this hierarchy collapses to SL. 85 As can easily be seen, the above argument holds for any undirected graph with undirected query edges, which is exactly the definition of SL &lt;SL&gt; given by <ref> [BPS92] </ref>. Thus, SL &lt;SL&gt; = SL, and by induction the SL hierarchy defined in [BPS92] collapses to SL. <p> L &lt;SL&gt; , this hierarchy collapses to SL. 85 As can easily be seen, the above argument holds for any undirected graph with undirected query edges, which is exactly the definition of SL &lt;SL&gt; given by <ref> [BPS92] </ref>. Thus, SL &lt;SL&gt; = SL, and by induction the SL hierarchy defined in [BPS92] collapses to SL.
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: Then we use the randomness just extracted to further extract randomness from X 1 . Lemma 2.2.5 <ref> [CG88, NZ93] </ref> E is an extractor. Proof: For the proof we need the following basic lemma: Lemma 2.2.6 [NZ93] Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> If m 1 = : : : = m k = m we say X is a (k; m; *) CG source. Lemma 2.2.7 <ref> [CG88, NZ93] </ref> Let X = X 1 ffi X 2 : : : ffi X k be a (k; (m 1 ; : : : ; m k ); *) CG source, where m k = (log (n)) and m i1 = c tiny m i .
Reference: [CG89] <author> Chor and Goldreich. </author> <title> On the power of two-point based sampling. </title> <journal> Journal of Complexity, </journal> <volume> 5, </volume> <year> 1989. </year>
Reference-contexts: Once our sample space has polynomial size, we can try all the elements in the sample space, and deterministically find out the result. Two explicit constructions are extremely useful when derandomizing algorithms: * k-wise independence A small sample space that is k-wise independent (any k elements are mutually independent) <ref> [CG89] </ref>. An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) [NN93, AGHP92]. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties [Mar, LPS86]. <p> We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by <ref> [KPS85, CG89, IZ89, CW89] </ref> and many others. Using expanders, this can be done using only n + O (k) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification.
Reference: [CGH + 85] <author> B. Chor, O. Goldreich, J. Hastad, J. Friedman, S. Rudich, and R. Smolensky. </author> <title> The bit extraction problem and t-resilient functions. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: Also, it is not hard to see that there exists a universal family of hash functions jHj of size poly (jN j; jDj) = poly (jN j). Using this family we need to invest n truly random bits. Can we do any better? In <ref> [CGH + 85] </ref> it was shown that if we want pairwise independence, and allow no error, we cannot do much better. However, remember that we don't really need 0 collision error, and we can afford some small collision error. In other words, we only need almost pairwise independence.
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: We also want it to be a black-box simulation, i.e., it is done by calling the original algorithm (possibly several times) and replacing the required random strings with new strings we compute from X. Fact 1.4.3 <ref> [CW89] </ref> Any polynomial time, black-box simulation of RP or BP P , must use a source X with H 1 (X) n fl for some fl &gt; 0. <p> Since H has * 2 collision error, by lemma 2.2.1 (h; h (x)) is quasi-random to within *. 24 Let's look for small families of hash functions with small collision error. Let us start with families of hash function with no collision error. Definition 2.2.2 <ref> [CW89] </ref> H = fh : [N ] 7! [D]g is called a universal family of hash functions, if for any x 1 6= x 2 2 [N ], and for any y 1 ; y 2 2 [D], P rob h2H ( h (x 1 ) = y 1 ^ h <p> We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by <ref> [KPS85, CG89, IZ89, CW89] </ref> and many others. Using expanders, this can be done using only n + O (k) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification. <p> We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by [KPS85, CG89, IZ89, CW89] and many others. Using expanders, this can be done using only n + O (k) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification. Theorem: Assume there is an (n + k; m = n; t; m 0 = n; * = 1 n ) extractor.
Reference: [FGL + 91] <author> U. Feige, S. Goldwasser, L. Lovasz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 2-12, </pages> <year> 1991. </year>
Reference-contexts: In the following we write down the P CP amplification we achieve using the extractor of section 2.3, and the hardness result we get. We do not describe what a P CP proof system is, and how the <ref> [FGL + 91] </ref> reduction works. The interested read is referred to [FGL + 91, BGLR93, Zuc93]. <p> In the following we write down the P CP amplification we achieve using the extractor of section 2.3, and the hardness result we get. We do not describe what a P CP proof system is, and how the [FGL + 91] reduction works. The interested read is referred to <ref> [FGL + 91, BGLR93, Zuc93] </ref>. Theorem: [ALM + 92, AS92] N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor.
Reference: [ILL89] <author> R. Impagliazzo, L. Levin, and M. Luby. </author> <title> Pseudo-random generation from one-way functions. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 12-24, </pages> <year> 1989. </year>
Reference-contexts: Building on the work of Shamir [Sha81], Blum and Micali [BM82] and Yao [Yao82], Impagliazzo, Levin and Luby <ref> [ILL89] </ref> showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert). <p> Many times, in theory and practice, one needs to hash a small set residing in a huge domain to a much smaller domain, with as few collisions as possible. Notice that by definition extractors are good hash functions. The converse is stated in the leftover hash lemma <ref> [ILL89] </ref>: Definition 2.2.1 * Let X be a distribution. <p> : [N ] 7! [D]g is a family of hash functions with collision error ffi, if for any x 1 6= x 2 2 [N ], P rob h2H (h (x 1 ) = h (x 2 )) 1 The following lemma is a variant of the leftover hash lemma <ref> [ILL89] </ref>: Lemma 2.2.1 Let H be a family of hash functions from [N ] to [D] with collision error ffi.
Reference: [Imm88] <author> N. Immerman. </author> <title> Nondeterministic space is closed under complementation. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17, </volume> <year> 1988. </year>
Reference-contexts: If we have to guess if these containments are tight what would be our first (or second) guess? I guess "NO". and as usually happens in complexity theory (and in life in general), pessimism rules until someone shows the contrary 7 . Thus, the proofs by Immerman and Szelepcseny <ref> [Imm88, Sze88] </ref> that NL is closed under complement, came as a great surprise to the scientific community.
Reference: [INW94] <author> Impagliazzo, Nisan, and Wigderson. </author> <title> Pseudorandomness for network algorithms. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1994. </year>
Reference-contexts: Indeed, several pseudo-random generators for the class RL exist. These pseudorandom generators use the derandomization tools listed in the previous sections: [Nis92] uses hash functions, <ref> [INW94] </ref> use expanders and [NZ93, Arm, Zuc96] use extractors. Still, none of these constructions is optimal, and it is a major open problem to show that pseudo-random generators for RL exist in L. 12 1.4 Our Work In this section we state the new results we achieved.
Reference: [IZ89] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 248-253, </pages> <year> 1989. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by <ref> [KPS85, CG89, IZ89, CW89] </ref> and many others. Using expanders, this can be done using only n + O (k) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification. <p> We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by [KPS85, CG89, IZ89, CW89] and many others. Using expanders, this can be done using only n + O (k) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification. Theorem: Assume there is an (n + k; m = n; t; m 0 = n; * = 1 n ) extractor.
Reference: [KPS85] <author> R. Karp, N. Pippernger, and M. Sipser. </author> <title> A time randomness tradeoff. </title> <booktitle> In AMS Conference on Probabilistic Computational Complexity, </booktitle> <year> 1985. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. 70 This problem, known as the "deterministic amplification" problem, was exten-sively studied by <ref> [KPS85, CG89, IZ89, CW89] </ref> and many others. Using expanders, this can be done using only n + O (k) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] noted that the existence of explicit extractors imply stronger amplification.
Reference: [LP82] <author> Lewis and Papadimitriou. </author> <title> Symmetric space-bounded computation. </title> <journal> Theoretical Computer Science, </journal> <volume> 19, </volume> <year> 1982. </year>
Reference-contexts: So STCON is as hard as NL, while USTCON is not harder than RL which looks easier than NL. That's the time for a name for a new Class! Indeed Lewis and Papadimitriou <ref> [LP82] </ref> defined a class SL, Symmetric Logspace, and showed that the following definitions are equivalent: 1. Languages which can be reduced in Logspace via a many-one reduction to USTCON, the undirected st-connectivity problem. 2. Languages which can be recognized by symmetric nondeterministic Turing Machines that run within logarithmic space. See [LP82]. <p> <ref> [LP82] </ref> defined a class SL, Symmetric Logspace, and showed that the following definitions are equivalent: 1. Languages which can be reduced in Logspace via a many-one reduction to USTCON, the undirected st-connectivity problem. 2. Languages which can be recognized by symmetric nondeterministic Turing Machines that run within logarithmic space. See [LP82]. 3. Languages that can be accepted by a uniform family of polynomial size contact schemes (also sometimes called switching networks.) See [Raz91]. In particular, the Aleliunas et al. result shows that SL RL. Adding this to the former inclusions we get: L SL RL NL.
Reference: [LPS86] <author> Lubotzky, Phillips, and Sarnak. </author> <title> Explicit expanders and the ramanujan conjectures. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1986. </year>
Reference-contexts: An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) [NN93, AGHP92]. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties <ref> [Mar, LPS86] </ref>. The interested reader is referred to [Wig94] for a survey of pair-wise independence, and [MR95] for a reading on probabilistic algorithms and derandomization. We can also view extractors as explicit graphs with strong random properties. As such, they can serve as a derandomization tool.
Reference: [Mar] <author> G. A. Margulis. </author> <title> Explicit construction of concentrators. Problems of Information Transmission. </title>
Reference-contexts: An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) [NN93, AGHP92]. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties <ref> [Mar, LPS86] </ref>. The interested reader is referred to [Wig94] for a survey of pair-wise independence, and [MR95] for a reading on probabilistic algorithms and derandomization. We can also view extractors as explicit graphs with strong random properties. As such, they can serve as a derandomization tool.
Reference: [Mes84] <author> R. Meshulam. </author> <title> A geometric construction of a superconcentrator of depth 2. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 215-219, </pages> <year> 1984. </year>
Reference-contexts: Lemma 2.5.4 (following [WZ93]) For every N there is an efficiently constructible depth 2 superconcentrator over N vertices with size O (N 2 polyloglog (N) ). Proof: (following the simple idea in [WZ93]) We are going to use the following lemma: Lemma 2.5.5 <ref> [Mes84] </ref> G = ((A; C; B); E) is a superconcentrator of depth 2 iff for any 1 k n and any sets X A, Y B of size k, j (X) " (Y )j k.
Reference: [MR95] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year> <month> 90 </month>
Reference-contexts: An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) [NN93, AGHP92]. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties [Mar, LPS86]. The interested reader is referred to [Wig94] for a survey of pair-wise independence, and <ref> [MR95] </ref> for a reading on probabilistic algorithms and derandomization. We can also view extractors as explicit graphs with strong random properties. As such, they can serve as a derandomization tool.
Reference: [Nis92] <author> N. Nisan. </author> <title> RL SC. </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 619-623, </pages> <year> 1992. </year>
Reference-contexts: Thus, finding a pseudo-random generator for RL, Random Logspace, would yield the first deterministic Logspace solution to the undirected s; t connectivity problem, and many others. Indeed, several pseudo-random generators for the class RL exist. These pseudorandom generators use the derandomization tools listed in the previous sections: <ref> [Nis92] </ref> uses hash functions, [INW94] use expanders and [NZ93, Arm, Zuc96] use extractors.
Reference: [Nis96] <author> N. Nisan. </author> <title> Refining randomness: Why and how. </title> <booktitle> In Annual Conference on Structure in Complexity Theory, </booktitle> <year> 1996. </year>
Reference-contexts: We can also view extractors as explicit graphs with strong random properties. As such, they can serve as a derandomization tool. Indeed, during the last few years, many derandomization results were found using extractors ( see <ref> [Nis96] </ref> for a list of applications, or section 2.5 for those applications improved by our new extractors). In particular, extractors provide the best deterministic amplification known today (see [Nis96]). <p> Indeed, during the last few years, many derandomization results were found using extractors ( see <ref> [Nis96] </ref> for a list of applications, or section 2.5 for those applications improved by our new extractors). In particular, extractors provide the best deterministic amplification known today (see [Nis96]). <p> Also, all constructions extract much less bits than the min-entropy that exists in the given source. 4 A disperser is a weak extractor. See <ref> [Nis96] </ref>. 15 Extractor for any min-entropy! We devise a new tool for building extractors, which we call "somewhere random mergers". We use this tool to achieve two new extractors. <p> The real parameters appearing in [SZ94] are somewhat better. 2 We did not define what a disperser is. The reader is referred to <ref> [Nis96] </ref> for a survey. 34 2.3 An Extractor For Any Min-Entropy! 2.3.1 An Informal Extractor First we notice that for any source X and most strings x 2 X , there is some splitting point 1 i n that splits x into x 1 ffi x 2 s.t. both P r <p> = d) is O (l (1) ) = n (1) close to a distribution W , with H 1 (W ) = l n ( log (n) ) ( n 1 1 1 n polylog (n) ) (n fl=2 ). 2.5 Applications Extractors have many applications in computer science (see <ref> [Nis96] </ref> for a survey). Here we list only those applications that benefited from our new constructions.
Reference: [NN93] <author> Naor and Naor. </author> <title> Small-bias probability spaces: Efficient constructions and applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22, </volume> <year> 1993. </year>
Reference-contexts: Two explicit constructions are extremely useful when derandomizing algorithms: * k-wise independence A small sample space that is k-wise independent (any k elements are mutually independent) [CG89]. An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) <ref> [NN93, AGHP92] </ref>. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties [Mar, LPS86]. The interested reader is referred to [Wig94] for a survey of pair-wise independence, and [MR95] for a reading on probabilistic algorithms and derandomization. <p> However, remember that we don't really need 0 collision error, and we can afford some small collision error. In other words, we only need almost pairwise independence. Amazingly, Naor and Naor <ref> [NN93] </ref> showed that in this case we can do much better. Definition 2.2.3 [NN93] A set S of n-bit vectors is "d wise biased", if for any d indices I (i.e. <p> However, remember that we don't really need 0 collision error, and we can afford some small collision error. In other words, we only need almost pairwise independence. Amazingly, Naor and Naor <ref> [NN93] </ref> showed that in this case we can do much better. Definition 2.2.3 [NN93] A set S of n-bit vectors is "d wise biased", if for any d indices I (i.e. <p> I f1; : : : ; ng, jIj = d), and for any d values b 1 ; : : : ; b d 2 f0; 1g: j [P rob x2S (^ i2I x i = b i ) ] 2 jIj j Theorem: <ref> [NN93] </ref> (see also [AGHP92]) For every integer q, d q and every &gt; 0, there is an explicit set S of q-bits vectors that is d-wise, biased, and of cardinality O ( (d log (q) 1 ) 2 ).
Reference: [NW88] <author> N. Nisan and A. Wigderson. </author> <title> Hardness vs. randomness. </title> <booktitle> In Proc. 29th IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 2-11, </pages> <year> 1988. </year>
Reference-contexts: Building on the work of Shamir [Sha81], Blum and Micali [BM82] and Yao [Yao82], Impagliazzo, Levin and Luby [ILL89] showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert). Nisan and Wigderson <ref> [NW88, BFNW93] </ref> showed that a pseudorandom generator for P that runs in exponential time (in the seed length) exists iff there exists a function solvable in exponential time that is hard for any polynomial size circuit.
Reference: [NZ93] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in logspace. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: Indeed, several pseudo-random generators for the class RL exist. These pseudorandom generators use the derandomization tools listed in the previous sections: [Nis92] uses hash functions, [INW94] use expanders and <ref> [NZ93, Arm, Zuc96] </ref> use extractors. Still, none of these constructions is optimal, and it is a major open problem to show that pseudo-random generators for RL exist in L. 12 1.4 Our Work In this section we state the new results we achieved. <p> Remark 1.4.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X; Y ) is close to uniform, while we only demand that E (X; Y ) is close to uniform. <p> Remark 1.4.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X; Y ) is close to uniform, while we only demand that E (X; Y ) is close to uniform. <p> As mentioned before, Nisan and Zuckerman use a slightly different definition of extractors. However, slight adaptations to their proof yields the following lower bound: Fact 1.4.1 <ref> [NZ93] </ref> For any m = m (n) and * = *(n), any (n; m; t; t + 1; *)-extractor, must have t = (log (n m) + log ( 1 * )). <p> Remark 2.1.1 This definition is different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X ; Y ) is close to uniform, while we only demand that E (X ; Y ) is close to uniform. <p> Remark 2.1.1 This definition is different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X ; Y ) is close to uniform, while we only demand that E (X ; Y ) is close to uniform. <p> Then we use the randomness just extracted to further extract randomness from X 1 . Lemma 2.2.5 <ref> [CG88, NZ93] </ref> E is an extractor. Proof: For the proof we need the following basic lemma: Lemma 2.2.6 [NZ93] Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> Then we use the randomness just extracted to further extract randomness from X 1 . Lemma 2.2.5 [CG88, NZ93] E is an extractor. Proof: For the proof we need the following basic lemma: Lemma 2.2.6 <ref> [NZ93] </ref> Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> If m 1 = : : : = m k = m we say X is a (k; m; *) CG source. Lemma 2.2.7 <ref> [CG88, NZ93] </ref> Let X = X 1 ffi X 2 : : : ffi X k be a (k; (m 1 ; : : : ; m k ); *) CG source, where m k = (log (n)) and m i1 = c tiny m i . <p> Although there is no one good splitting point, each string has a good splitting point. In the next sections we explain this, and show two methods using this idea to convert an arbitrary source to a CG source. Block Extraction In <ref> [NZ93] </ref> Nisan and Zuckerman showed how to get a CG source from a general random source. Let us say a bit is "surprising" if we expected it to be different. <p> Next, we state the Nisan and Zuckerman lemma somewhat more formally. We do not prove the lemma, and the interested reader is referred to the original paper <ref> [NZ93] </ref>. Let X be a random source over f0; 1g n . <p> Let X be a random source over f0; 1g n . Nisan and Zuckerman construct a function B l (x; y) which gets x 2 X and a short random string y, and returns l bits, s.t.: Lemma 2.2.8 <ref> [NZ93, SZ94] </ref> If H 1 (X) ffin, then B (X; U ) is (ffil) (1) close to a distribution W with H 1 (W ) ( ffil log (ffi 1 ) ) ( ffil log (n) ). <p> Actually, as long as jB 1 j + : : : + jB k j &lt;< m, we can extract another block, which also has high min entropy even conditioned on the history. Certainly we can do that log (n) times. Thus we get: Lemma 2.2.9 <ref> [NZ93, SZ94] </ref> Let X be a distribution on f0; 1g n with H 1 (X) n 1=2+2fl for some fl &gt; 0. Define b i = B l (x; y i ) where l = n 1=2 and 1 i k = O (log (n)). <p> Here we are not interested in the result itself, but rather in the method. Next, we are going to present a simplified version of the SSZ method, in a rather informal way. Following an idea in <ref> [NZ93] </ref>, Srinivasan, Saks and Zhou look at specific strings. They show that for most strings x, there is a good partition of x to log (n) blocks, s.t. for all i the distribution of X i given the history x [1;i1] contains alot of randomness. <p> We observe that a 2-block merger can be obtained from the previously designed extractors of <ref> [NZ93, SZ94] </ref>. Once such a merger is obtained, any number of blocks can be merged in a binary-tree fashion. <p> We show that combining the extractor of Theorem 4 with the <ref> [NZ93] </ref> block extractor, we can extract randomness from sources having n 1 2 +fl min-entropy using less random bits. The idea behind the construction is the following: since the given source X has H 1 (X) n 1 , we can use the [NZ93] block extraction to extract d = O <p> the extractor of Theorem 4 with the <ref> [NZ93] </ref> block extractor, we can extract randomness from sources having n 1 2 +fl min-entropy using less random bits. The idea behind the construction is the following: since the given source X has H 1 (X) n 1 , we can use the [NZ93] block extraction to extract d = O (log (f (n))) blocks that together form a CG source with each block containing some n (1) min-entropy. Then, by investing O (log (n)) bits, we can extract some log (n) 2 (d) = log (n) f (n) random bits.
Reference: [Pip87] <author> N. Pippenger. </author> <title> Sorting and selecting in rounds. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16 </volume> <pages> 1032-1038, </pages> <year> 1987. </year>
Reference-contexts: Here we list only those applications that benefited from our new constructions. Most of the results are achieved by plugging in our new extractor instead of the previous ones. 2.5.1 a-Expanding Graphs Definition 2.5.1 <ref> [Pip87] </ref> An undirected graph is a-expanding if any two disjoint sets of vertices of size at least a are joined by an edge. The obvious lower bound on the degree of an a-expanding graph is Na a . <p> Therefore, there is an edge going from X to Y in G, and therefore G is K-expanding. The maximal degree of a vertex in G is at most T 6d avg = O ( N K T 2 ). <ref> [Pip87, WZ93] </ref> showed that constructing good explicit a-expanding graphs has applications to other problems. <p> The maximal degree of a vertex in G is at most T 6d avg = O ( N K T 2 ). [Pip87, WZ93] showed that constructing good explicit a-expanding graphs has applications to other problems. Plugging in our new extractor we get: Corollary 2.5.2 (following <ref> [Pip87] </ref>, see [WZ93] lemma 5) There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons.
Reference: [Raz91] <author> A. Razborov. </author> <title> Lower bounds for deterministic and nondeterministic branching programs. </title> <booktitle> In Proceedings of the 8th FCT, Lecture Notes in Computer Science, </booktitle> <volume> 529, </volume> <pages> pages 47-60, </pages> <address> New York/Berlin, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Languages which can be recognized by symmetric nondeterministic Turing Machines that run within logarithmic space. See [LP82]. 3. Languages that can be accepted by a uniform family of polynomial size contact schemes (also sometimes called switching networks.) See <ref> [Raz91] </ref>. In particular, the Aleliunas et al. result shows that SL RL. Adding this to the former inclusions we get: L SL RL NL.
Reference: [Rei82] <author> J. H. Reif. </author> <title> Symmetric complementation. </title> <booktitle> In Proc. 14th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 201-214, </pages> <year> 1982. </year>
Reference-contexts: However, this technique failed to solve the problem whether the class SL is closed under complement. As a consequence, an SL hierarchy was built <ref> [Rei82, BCD + 89] </ref>, and turned out to contain many interesting problems, such as 2-colorability [Rei82]. <p> However, this technique failed to solve the problem whether the class SL is closed under complement. As a consequence, an SL hierarchy was built [Rei82, BCD + 89], and turned out to contain many interesting problems, such as 2-colorability <ref> [Rei82] </ref>. In a result co-authored with my advisor Noam Nisan, we develop a new technique and show that SL = coSL, collapsing, in particular, the SL hierarchy. 20 Chapter 2 Explicit Extractors In this chapter we present the currently known techniques for building explicit extractors. <p> In this section we show how to build a spanning forest using USTCON. This basic idea was already noticed by Reif and independently by Cook <ref> [Rei82] </ref>. Given a graph G index the edges from 1 to m. We can view the indices as weights for the edges, and as no two edges have the same weight, we know that there is a unique minimal spanning forest F . <p> It is also clear that this can be done in LogSpace, completing the proof. As the symmetric Logspace hierarchy defined in <ref> [Rei82] </ref> is known to be within L &lt;SL&gt; , this hierarchy collapses to SL. 85 As can easily be seen, the above argument holds for any undirected graph with undirected query edges, which is exactly the definition of SL &lt;SL&gt; given by [BPS92].
Reference: [Sha81] <author> A. Shamir. </author> <title> On the generation of cryptographically strong pseudorandom sequences. </title> <booktitle> In Proceedings of the 8th ICALP, Lecture Notes in 91 Computer Science, </booktitle> <volume> 62, </volume> <pages> pages 544-550, </pages> <address> New York/Berlin, 1981. </address> <publisher> Springer--Verlag. </publisher>
Reference-contexts: Building on the work of Shamir <ref> [Sha81] </ref>, Blum and Micali [BM82] and Yao [Yao82], Impagliazzo, Levin and Luby [ILL89] showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert).
Reference: [Sip88] <author> Sipser. Expanders, </author> <title> randomness, or time versus space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36, </volume> <year> 1988. </year>
Reference-contexts: Using expanders, this can be done using only n + O (k) random bits [AKS87, IZ89, CW89]. Sipser <ref> [Sip88] </ref> noted that the existence of explicit extractors imply stronger amplification. Theorem: Assume there is an (n + k; m = n; t; m 0 = n; * = 1 n ) extractor.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: previously known: m min-entropy t truly random bits m 0 extracted randomness reference (n) O (log 2 n log ( 1 (n) O (log (n) + log ( 1 (n 1=2+fl ) O (log 2 n log ( 1 (n fl ) O (logn) n ffi ; ffi &lt; fl <ref> [SSZ95] </ref> Disperser 4 We see that all constructions require at least n fl min-entropy for some constant fl &gt; 0. Also, all constructions extract much less bits than the min-entropy that exists in the given source. 4 A disperser is a weak extractor. <p> Thus, by using the [SZ94] extractor (see the table above), we get: Corollary 1.4.1 [SZ94] For any fl &gt; 0, BP P can be simulated using sources with H 1 (X ) n 1=2+fl , in n O (log (n)) time. <ref> [SSZ95] </ref> showed that for RP , n fl min-entropy suffices, and the simulation can be done in polynomial time: 17 Corollary 1.4.2 [SSZ95] For any fl &gt; 0, RP can be simulated in polynomial time, using sources with H 1 (X) n fl . <p> 1.4.1 [SZ94] For any fl &gt; 0, BP P can be simulated using sources with H 1 (X ) n 1=2+fl , in n O (log (n)) time. <ref> [SSZ95] </ref> showed that for RP , n fl min-entropy suffices, and the simulation can be done in polynomial time: 17 Corollary 1.4.2 [SSZ95] For any fl &gt; 0, RP can be simulated in polynomial time, using sources with H 1 (X) n fl . <p> Thus, this method seems to work only for sources having at least n 1=2 min-entropy. In section 2.4 we will use new tools to strengthen this method. The SSZ dispersers Srinivasan, Saks and Zhou <ref> [SSZ95] </ref> showed that randomness can be extracted, in a weak sense, from random sources having n fl min-entropy, for any constant fl &gt; 0. Thus, the SSZ method breaks the n 1=2 bound imposed by the block extraction method. <p> Lemma 2.2.11 [Zuc96] Let m (n) = fi (n). For any * there is an explicit (n; m (n); O (log (n) + log ( 1 * )); (n); *)- extractor. Lemma 2.2.12 <ref> [SSZ95] </ref> Let m (n) n fl for some constant fl &gt; 0, then there is some constant ffi &gt; 0 and an explicit (n; m (n); O (log (n)); n ffi ; 1 4 )- disperser. 2 1 The parameters here are simplified. <p> We mentioned that Srinivasan, Saks and Zhou <ref> [SSZ95] </ref> showed a polynomial time, black-box simulation 73 of RP using any random source X having H 1 (X ) n fl , and Srinivasan and Zuck--erman [SZ94] showed an n O (log (n)) time, black-box simulation of BP P using any random source X having H 1 (X ) n
Reference: [SV85] <author> Skyum and Valiant. </author> <title> A complexity theory based on boolean algebra. </title> <journal> Journal of the ACM, </journal> <year> 1985. </year>
Reference-contexts: In subsection 3.1.3 everything is put together. 3.1.1 Projections to U ST CON . We will use only the simplest kind of reductions, i.e. LogSpace uniform projection reductions <ref> [SV85] </ref>. Moreover, we will only be interested in reductions to USTCON. In this subsection we define this kind of reduction and we show some of its basic properties.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: Remark 1.4.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X; Y ) is close to uniform, while we only demand that E (X; Y ) is close to uniform. <p> Remark 1.4.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X; Y ) is close to uniform, while we only demand that E (X; Y ) is close to uniform. <p> Simulating random classes with sources having high min-entropy Our second extractor is motivated by the problem of simulating BP P using only defective sources having high min-entropy. 5 See section 2.5 for the definition of a-expanding graphs. The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 6 This improves the current upper bound of O (log (N ) 1=2+o (1) ) due to [WZ93, SZ94]. 16 In section 1.2.2 we discussed whether randomized algorithms are indeed feasible. <p> The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 6 This improves the current upper bound of O (log (N ) 1=2+o (1) ) due to [WZ93, SZ94]. 16 In section 1.2.2 we discussed whether randomized algorithms are indeed feasible. We saw that crude randomness does exist in nature, and we looked for extractors to extract truly random bits from it. Let us formalize the problem. <p> It is true that we still need to invest t truly random bits, and we do not have a source outputting truly random bits. However, instead of using t truly random bits we can try all 2 t possibilities and decide according to the majority. Thus, by using the <ref> [SZ94] </ref> extractor (see the table above), we get: Corollary 1.4.1 [SZ94] For any fl &gt; 0, BP P can be simulated using sources with H 1 (X ) n 1=2+fl , in n O (log (n)) time. [SSZ95] showed that for RP , n fl min-entropy suffices, and the simulation can <p> However, instead of using t truly random bits we can try all 2 t possibilities and decide according to the majority. Thus, by using the <ref> [SZ94] </ref> extractor (see the table above), we get: Corollary 1.4.1 [SZ94] For any fl &gt; 0, BP P can be simulated using sources with H 1 (X ) n 1=2+fl , in n O (log (n)) time. [SSZ95] showed that for RP , n fl min-entropy suffices, and the simulation can be done in polynomial time: 17 Corollary 1.4.2 [SSZ95] For <p> Remark 2.1.1 This definition is different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X ; Y ) is close to uniform, while we only demand that E (X ; Y ) is close to uniform. <p> Remark 2.1.1 This definition is different from the one in <ref> [NZ93, SZ94] </ref>. In [NZ93, SZ94] E is an extractor if Y ffi E (X ; Y ) is close to uniform, while we only demand that E (X ; Y ) is close to uniform. <p> Srinivasan and Zuckerman used this in a very simple way to show: 25 Lemma 2.2.3 <ref> [SZ94] </ref> There exists an explicit family H of hash functions from [N ] to [D], with * collision error, and poly (log (jN j); 1 * ; jDj) size. Proof: Any function h : [N ] 7! [D] can be represented by writing all its values on [N ]. <p> jP rob h2H (h (x 1 ) = b ^ h (x 2 ) = b) jDj 2 j jDj Therefore, P rob h2H (h (x 1 ) = h (x 2 )) jDj 1 + jDj jDj 2 jDj Thus, combining this with lemma 2.2.1 we get: Lemma 2.2.4 <ref> [SZ94] </ref> There is some constant c &gt; 1 s.t. for any m = (log (n)) there is an explicit (n; m; t = m; m 0 = cm; * = 2 m=5 )- extractor A m . Definition 2.2.4 Denote the constant c in lemma 2.2.4 by c tiny . <p> Let X be a random source over f0; 1g n . Nisan and Zuckerman construct a function B l (x; y) which gets x 2 X and a short random string y, and returns l bits, s.t.: Lemma 2.2.8 <ref> [NZ93, SZ94] </ref> If H 1 (X) ffin, then B (X; U ) is (ffil) (1) close to a distribution W with H 1 (W ) ( ffil log (ffi 1 ) ) ( ffil log (n) ). <p> Actually, as long as jB 1 j + : : : + jB k j &lt;< m, we can extract another block, which also has high min entropy even conditioned on the history. Certainly we can do that log (n) times. Thus we get: Lemma 2.2.9 <ref> [NZ93, SZ94] </ref> Let X be a distribution on f0; 1g n with H 1 (X) n 1=2+2fl for some fl &gt; 0. Define b i = B l (x; y i ) where l = n 1=2 and 1 i k = O (log (n)). <p> Finally, we saw two methods to convert an arbitrary source to a CG source. This results in the following extractors: Lemma 2.2.10 <ref> [SZ94] </ref> 1 Let m (n) n 1=2+fl for some constant fl &gt; 0. For any * there is an explicit (n; m (n); O (log 2 n log ( 1 * )); n ; *)- extractor. Lemma 2.2.11 [Zuc96] Let m (n) = fi (n). <p> The real parameters appearing in <ref> [SZ94] </ref> are somewhat better. 2 We did not define what a disperser is. <p> We observe that a 2-block merger can be obtained from the previously designed extractors of <ref> [NZ93, SZ94] </ref>. Once such a merger is obtained, any number of blocks can be merged in a binary-tree fashion. <p> Then for any m there is an explicit (n; m; O ( m log (n) log ( 1 * ) + log 2 (n) t); m; poly (n) *)- extractor. Proof: The lemma follows from lemma 2.3.2 using lemma 2.3.7. Mergers That Do Not Lose Much. The <ref> [SZ94] </ref> extractor of lemma 2.2.10 works for any source with H 1 (X) n 1=2+fl . Thus, using lemma 2.3.6 by repeatedly using the [SZ94] extractor, we can extract at least n 2 n 1=2+fl quasi-random bits from a source having H 1 (X ) n 2 . <p> Proof: The lemma follows from lemma 2.3.2 using lemma 2.3.7. Mergers That Do Not Lose Much. The <ref> [SZ94] </ref> extractor of lemma 2.2.10 works for any source with H 1 (X) n 1=2+fl . Thus, using lemma 2.3.6 by repeatedly using the [SZ94] extractor, we can extract at least n 2 n 1=2+fl quasi-random bits from a source having H 1 (X ) n 2 . Thus, we have a 2-merger that does not lose much randomness in the merging process. Applying Theorem 3 we get a good n-merger. <p> Notice that Theorem 3 and corollary 2.3.10 take advantage of the simple structure of somewhere random sources, giving us an explicit somewhere random merger that works even for sources with very small min-entropy to which the <ref> [SZ94] </ref> extractor of lemma 2.2.10 does not apply. <p> Although O (2 p 54 polylog (n) log ( 1 * )) is quite a large amount of truly random bits, we can use the <ref> [SZ94] </ref> extractor to extract n 1=3 bits from n 2=3 min-entropy, and then use these n 1=3 &gt;> O (2 log (n) polylog (n)log ( 1 * )) bits to further extract all the remaining min entropy. <p> We mentioned that Srinivasan, Saks and Zhou [SSZ95] showed a polynomial time, black-box simulation 73 of RP using any random source X having H 1 (X ) n fl , and Srinivasan and Zuck--erman <ref> [SZ94] </ref> showed an n O (log (n)) time, black-box simulation of BP P using any random source X having H 1 (X ) n fl , for fl &gt; 1=2.
Reference: [Sze88] <author> Szelepcsenyi. </author> <title> The method of forced enumeration for nondeterministic automata. </title> <journal> Acta Informatica, </journal> <volume> 26, </volume> <year> 1988. </year>
Reference-contexts: If we have to guess if these containments are tight what would be our first (or second) guess? I guess "NO". and as usually happens in complexity theory (and in life in general), pessimism rules until someone shows the contrary 7 . Thus, the proofs by Immerman and Szelepcseny <ref> [Imm88, Sze88] </ref> that NL is closed under complement, came as a great surprise to the scientific community.
Reference: [Wig94] <author> Wigderson. </author> <title> The amazing power of pairwise independence. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1994. </year>
Reference-contexts: An even smaller sample space that is almost k-wise independent (any k elements are almost mutually independent) [NN93, AGHP92]. 9 * Expanders explicit graphs with constant degree and strong expansion prop- erties [Mar, LPS86]. The interested reader is referred to <ref> [Wig94] </ref> for a survey of pair-wise independence, and [MR95] for a reading on probabilistic algorithms and derandomization. We can also view extractors as explicit graphs with strong random properties. As such, they can serve as a derandomization tool.
Reference: [WZ93] <author> A. Wigderson and D. Zuckerman. </author> <title> Expanders that beat the eigenvalue bound: Explicit construction and applications. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: That is, this extractor works for any min-entropy, small or large, and extracts all the randomness present in the given source. These properties turn out to be very important for some applications, most notably the following two corollaries: Corollary: (improving <ref> [WZ93] </ref>) For any N and 1 a N there is an explicitly constructible a-expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) 5 . <p> Another important corollary, that solves a problem similar to the network prob lem presented in section 1.1, is: Corollary: (improving <ref> [WZ93] </ref>) For any N there is an explicitly constructible superconcentrator over N vertices, with linear size and polyloglog (N ) depth 6 . See section 2.5 for more details on these and other applications. <p> Simulating random classes with sources having high min-entropy Our second extractor is motivated by the problem of simulating BP P using only defective sources having high min-entropy. 5 See section 2.5 for the definition of a-expanding graphs. The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 6 This improves the current upper bound of O (log (N ) 1=2+o (1) ) due to [WZ93, SZ94]. 16 In section 1.2.2 we discussed whether randomized algorithms are indeed feasible. <p> The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 6 This improves the current upper bound of O (log (N ) 1=2+o (1) ) due to [WZ93, SZ94]. 16 In section 1.2.2 we discussed whether randomized algorithms are indeed feasible. We saw that crude randomness does exist in nature, and we looked for extractors to extract truly random bits from it. Let us formalize the problem. <p> Thus we need extractors working on sources with very high min-entropy, and losing only a very small fraction of the min-entropy present in the random source. Fortunately, a simple idea due to Wigderson and Zuckerman <ref> [WZ93] </ref> suffices. More of The Same Suppose we have an extractor E that extracts randomness from any source having at least m min-entropy. How much randomness can we extract from sources having M min-entropy when M &gt;> m ? The following algorithm is implicit in [WZ93]: use the same extractor E <p> due to Wigderson and Zuckerman <ref> [WZ93] </ref> suffices. More of The Same Suppose we have an extractor E that extracts randomness from any source having at least m min-entropy. How much randomness can we extract from sources having M min-entropy when M &gt;> m ? The following algorithm is implicit in [WZ93]: use the same extractor E many times over the same string x, each time with a fresh truly random string r i , until you get M m output bits. <p> The obvious lower bound on the degree of an a-expanding graph is Na a . The previous upper bound was O ( N a 2 log (n) 1=2+o (1) )[WZ93; SZ94]. In <ref> [WZ93] </ref>, Wigderson and Zuckerman suggest a simple construction of a-expanding graphs which they improve using a recursive construction. Using the extractor we developed in section 2.3 we can use their simple construction and get: Corollary 2.5.1 (following [WZ93]) For every N and 1 a N , there is an efficiently constructible <p> In <ref> [WZ93] </ref>, Wigderson and Zuckerman suggest a simple construction of a-expanding graphs which they improve using a recursive construction. Using the extractor we developed in section 2.3 we can use their simple construction and get: Corollary 2.5.1 (following [WZ93]) For every N and 1 a N , there is an efficiently constructible a-expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) For completeness, we give the proof: Proof: (based on [WZ93]) Let V be a set with N = 2 n vertices. <p> we can use their simple construction and get: Corollary 2.5.1 (following <ref> [WZ93] </ref>) For every N and 1 a N , there is an efficiently constructible a-expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) For completeness, we give the proof: Proof: (based on [WZ93]) Let V be a set with N = 2 n vertices. <p> Therefore, there is an edge going from X to Y in G, and therefore G is K-expanding. The maximal degree of a vertex in G is at most T 6d avg = O ( N K T 2 ). <ref> [Pip87, WZ93] </ref> showed that constructing good explicit a-expanding graphs has applications to other problems. <p> The maximal degree of a vertex in G is at most T 6d avg = O ( N K T 2 ). [Pip87, WZ93] showed that constructing good explicit a-expanding graphs has applications to other problems. Plugging in our new extractor we get: Corollary 2.5.2 (following [Pip87], see <ref> [WZ93] </ref> lemma 5) There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons. Corollary 2.5.3 (following [AKSS89], see [WZ93] lemma 6) There <p> (following [Pip87], see <ref> [WZ93] </ref> lemma 5) There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons. Corollary 2.5.3 (following [AKSS89], see [WZ93] lemma 6) There are explicit algorithms to find all relations except O (a nlog (n)) among n elements, in one round and using O ( n 2 a 2 polyloglog (n) ) comparisons. <p> Much research was done on finding small explicit superconcentrators of small depth (see <ref> [WZ93] </ref> for references). Again, using our new extractor with the ideas used in previous constructions we manage to reduce the size of a depth 2 supercon-centrator from O (N 2 log (N) 1=2+o (1) ) to O (N 2 polyloglog (N) ). Lemma 2.5.4 (following [WZ93]) For every N there is <p> superconcentrators of small depth (see <ref> [WZ93] </ref> for references). Again, using our new extractor with the ideas used in previous constructions we manage to reduce the size of a depth 2 supercon-centrator from O (N 2 log (N) 1=2+o (1) ) to O (N 2 polyloglog (N) ). Lemma 2.5.4 (following [WZ93]) For every N there is an efficiently constructible depth 2 superconcentrator over N vertices with size O (N 2 polyloglog (N) ). Proof: (following the simple idea in [WZ93]) We are going to use the following lemma: Lemma 2.5.5 [Mes84] G = ((A; C; B); E) is a superconcentrator of <p> Lemma 2.5.4 (following <ref> [WZ93] </ref>) For every N there is an efficiently constructible depth 2 superconcentrator over N vertices with size O (N 2 polyloglog (N) ). Proof: (following the simple idea in [WZ93]) We are going to use the following lemma: Lemma 2.5.5 [Mes84] G = ((A; C; B); E) is a superconcentrator of depth 2 iff for any 1 k n and any sets X A, Y B of size k, j (X) " (Y )j k. <p> Therefore, for any X A and Y C of size 2 m k 2 m+1 , j (X) " (Y ) " B m j 1 3 jB m j k. Hence by lemma 2.5.5, our graph is a superconcentrator. <ref> [WZ93] </ref> showed how to convert a small depth 2 superconcentrator, to a linear-size superconcentrator with small depth. Plugging in the above result into their lemma, we achieve a linear-size superconcentrator of polyloglog (N ) depth. <p> Plugging in the above result into their lemma, we achieve a linear-size superconcentrator of polyloglog (N ) depth. Notice that the best previous linear construction had O (log (n) 1=2+o (1) ) depth, so we achieved an exponential improvement. Corollary 2.5.6 ( Following <ref> [WZ93] </ref>, lemma 10) For every N there is an explicitly constructible superconcentrator over N vertices, with linear size and polyloglog (N ) depth. 2.5.3 Deterministic Amplification Our goal now is to convert a BP P algorithm that uses n random bits and has 1 2 1 error, into one that errs
Reference: [Yao82] <author> A.C. Yao. </author> <title> Theory and application of trapdoor functions. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1982. </year> <month> 92 </month>
Reference-contexts: Building on the work of Shamir [Sha81], Blum and Micali [BM82] and Yao <ref> [Yao82] </ref>, Impagliazzo, Levin and Luby [ILL89] showed that pseudo-random generators for P exist iff there exist 1-way functions (functions that are easy to compute but hard to invert).
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: Theorem: [ALM + 92, AS92] N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor. We get: Lemma 2.5.8 <ref> [Zuc91] </ref> If there is an (r + l; r; t; r; 1 2 )-extractor F , then P CP (r; m; a; 1 3 Actually, it is enough to use a disperser, and extractors can be replaced with dispersers through out all of this subsection.
Reference: [Zuc93] <author> D. Zuckerman. </author> <title> NP-complete problems have a version that's hard to approximate. </title> <booktitle> In Proceedings of the 8th Structures in Complexity Theory, IEEE, </booktitle> <pages> pages 305-312, </pages> <year> 1993. </year>
Reference-contexts: Zuckerman <ref> [Zuc93] </ref> uses extractors to show the hardness of approximating any iterated log of MAX-Clique. In his constructions Zuckerman uses a non-explicit (R; r; t = log (R + 2); r; 1=2) extractor 3 that can be found by choosing a random bipartite graph with the right degree. <p> In the following we write down the P CP amplification we achieve using the extractor of section 2.3, and the hardness result we get. We do not describe what a P CP proof system is, and how the [FGL + 91] reduction works. The interested read is referred to <ref> [FGL + 91, BGLR93, Zuc93] </ref>. Theorem: [ALM + 92, AS92] N P P CP (r = O (log (n)); m = O (1); a = O (1); * = 1 The amplification process we use is exactly as the one for amplifying an RP algorithm with a good extractor. <p> Definition 2.5.4 We denote the size of the largest clique in G by ! = !(G). Corollary 2.5.10 (following <ref> [Zuc93] </ref>) Let k 3 be a constant.
Reference: [Zuc96] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors, and constructive leader election. </title> <booktitle> In Proceedings of the 28th Annual ACM Symposium on the Theory of Computing, ACM, 1996. </booktitle> <volume> 93 94 </volume>
Reference-contexts: Indeed, several pseudo-random generators for the class RL exist. These pseudorandom generators use the derandomization tools listed in the previous sections: [Nis92] uses hash functions, [INW94] use expanders and <ref> [NZ93, Arm, Zuc96] </ref> use extractors. Still, none of these constructions is optimal, and it is a major open problem to show that pseudo-random generators for RL exist in L. 12 1.4 Our Work In this section we state the new results we achieved. <p> This results in the following extractors: Lemma 2.2.10 [SZ94] 1 Let m (n) n 1=2+fl for some constant fl &gt; 0. For any * there is an explicit (n; m (n); O (log 2 n log ( 1 * )); n ; *)- extractor. Lemma 2.2.11 <ref> [Zuc96] </ref> Let m (n) = fi (n). For any * there is an explicit (n; m (n); O (log (n) + log ( 1 * )); (n); *)- extractor.
References-found: 49

