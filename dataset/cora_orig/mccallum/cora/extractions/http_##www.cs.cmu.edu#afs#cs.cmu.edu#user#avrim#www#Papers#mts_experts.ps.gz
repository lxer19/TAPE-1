URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/Papers/mts_experts.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/Papers/pubs.html
Root-URL: 
Email: avrim+@cs.cmu.edu  cburch+@cmu.edu  
Title: On-line Learning and the Metrical Task System Problem analysis showing how two recent algorithms for
Author: Avrim Blum Carl Burch lem. 
Note: An  Supported in part by NSF National Young Investigator grant CCR-9357793. Supported in part by a National Science Foundation Graduate Fellowship.  
Address: Pittsburgh, PA 15213-3891  Pittsburgh, PA 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Abstract: We relate two problems that have been explored in two distinct communities. The first is the problem of combining expert advice, studied extensively in the computational learning theory literature, and in particular the problem of tracking the best expert in the clean decision-theoretic setting. The second is the Metrical Task System (MTS) problem, studied extensively in the On-line Algorithms literature, and in particular, variations on the setting of the uniform metric space. We show that these problems contain several interesting similarities and demonstrate how algorithms designed for each can be used to achieve good bounds and new approaches for solving the other. Specific contributions of this paper include: Finally, we present an experimental comparison of how these algorithms perform on a process migra tion problem. 
Abstract-found: 1
Intro-found: 1
Reference: [BBBT97] <author> Y. Bartal, A. Blum, C. Burch, and A. Tomkins. </author> <title> A polylog(n)-competitive algorithm for metrical task systems. </title> <booktitle> In Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: In contrast, we describe below an algorithm whose com petitive ratio is 1 + O (log (n)=r), resulting in a much better bound for the Experts-DTF problem. 3.3 Odd-Exponent The good performance of Linear suggests using a generalization for more than two experts. Bartal, Blum, Burch, and Tomkins <ref> [BBBT97] </ref> analyze a generalization that can also be applied in an experts setting: Algorithm Odd-Exponent [BBBT97]: Let t be an odd integer, and let L i represent the reduced loss of expert i. Then place p i = n 1 n X r probability on expert i. <p> Bartal, Blum, Burch, and Tomkins <ref> [BBBT97] </ref> analyze a generalization that can also be applied in an experts setting: Algorithm Odd-Exponent [BBBT97]: Let t be an odd integer, and let L i represent the reduced loss of expert i. Then place p i = n 1 n X r probability on expert i. The following theorem shows that this strategy performs well in a uniform task system. Theorem 5 ([BBBT97]) For an <p> If we strictly use reduced loss in Odd-Exponent's probability distribution, we could allocate negative probability to an expert. (Consider the case where one expert has reduced loss of r while the rest are zero.) The analysis of <ref> [BBBT97] </ref>, concerned with theoretical guarantees, skirts the issue by observing that we may assume without loss of generality that experts with zero probability incur zero loss, and furthermore, because of the one-step lookahead in the MTS setting, that an expert never receives a greater loss than that needed to set its <p> Proof. This is a corollary of Theorem 5 by applying The orem 1. The additive term comes from the fact that in the analysis of <ref> [BBBT97] </ref>, the algorithm competes against not the minimum reduced loss, but the average, which is at most r more.
Reference: [BKRS92] <author> A. Blum, H. Karloff, Y. Rabani, and M. Saks. </author> <title> A decomposition theorem and lower bounds for randomized server problems. </title> <booktitle> In Proc IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 197207, </pages> <year> 1992. </year>
Reference-contexts: Then, its total cost is X maxf0; p t i p t+1 t For a refined analysis of the MTS problem, we use the r-unfair competitive ratio considered in <ref> [BKRS92] </ref> and formalized explicitly in [Sei96]. Here the on-line algorithm pays the same amount as before, but OPT pays r times more for movement. That is, the off-line player pays rd i;j + ` j for a task. <p> To emphasize this point, if one reduced loss is r greater than the other, we will say that the higher is pinned by the lower. 3.1 Two experts The following algorithm of <ref> [BKRS92] </ref> achieves an optimal competitive ratio for the r-unfair MTS problem on two states. In the Experts-DTF setting, the algorithm can be viewed as follows: Algorithm Linear [BKRS92]: The algorithm has one parameter r. Let L i represent the reduced loss of expert i with respect to r. <p> r greater than the other, we will say that the higher is pinned by the lower. 3.1 Two experts The following algorithm of <ref> [BKRS92] </ref> achieves an optimal competitive ratio for the r-unfair MTS problem on two states. In the Experts-DTF setting, the algorithm can be viewed as follows: Algorithm Linear [BKRS92]: The algorithm has one parameter r. Let L i represent the reduced loss of expert i with respect to r. The algorithm allocates p 0 = 2 L 1 L 0 probability to expert 0 and the rest to expert 1 (whose probability equation is symmetric).
Reference: [BLS92] <author> A. Borodin, N. Linial, and M. Saks. </author> <title> An optimal online algorithm for metrical task systems. </title> <journal> J of the ACM, </journal> <volume> 39(4):745763, </volume> <year> 1992. </year>
Reference-contexts: This notion of an on-line algorithm having state, with a cost for moving between states, is captured by a problem studied in the On-line Algorithms literature called the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>. In this problem, we imagine the on-line algorithm is controlling a system that can be in one of n states or configurations. <p> Finally, in Section 5 we present an empirical comparison of these algorithms and others for the process migration problem. 2 Definitions and general relations 2.1 The MTS problem and the competitive ratio In the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>, an on-line algorithm controls a system with n states located at points in a space with distance metric d. The algorithm receives, one at a time, a sequence of tasks, each a cost vector specifying the cost of performing the task in each state. <p> Thus the total cost for the partition is at most k X 1 1 1 1 as desired. 3.2 Marking For the MTS problem on a uniform metric space of more than two states, the standard algorithm is the Marking algorithm of Borodin, Linial, and Saks <ref> [BLS92] </ref> and Fiat et al. [FKL + 91]. Algorithm Marking [BLS92]: We maintain a counter for each state. At the beginning of each phase, the counters are reset to 0, and the algorithm occupies a random state. <p> partition is at most k X 1 1 1 1 as desired. 3.2 Marking For the MTS problem on a uniform metric space of more than two states, the standard algorithm is the Marking algorithm of Borodin, Linial, and Saks <ref> [BLS92] </ref> and Fiat et al. [FKL + 91]. Algorithm Marking [BLS92]: We maintain a counter for each state. At the beginning of each phase, the counters are reset to 0, and the algorithm occupies a random state. Given a cost vector `, we increment the ith counter by ` i . <p> This algorithm was designed for the fair setting in which r = 1. For that case, its competitive ratio is 2H n (where H n 2 [ln n; ln n + 1] is the nth harmonic number), which is optimal to constant factors <ref> [BLS92] </ref>. For the un fair setting, however, the competitive ratio does not decrease substantially with r as we would like: the ratio becomes (1 + 1=r)H n . Therefore, the partitioning bound resulting from Theorem 1 is not so good. <p> Recent The algorithm moves to the machine that has in curred the least loss over the last k trials. We implemented Marking, Odd-Exponent (with t = 3), Thresh, and Share. We also tried Work-Function, an MTS algorithm analyzed by Borodin, Linial, and Saks <ref> [BLS92] </ref>. This is a deterministic algorithm that, whenever its state's reduced loss is pinned, moves to the state that is pinning it (for the uniform metric space this corresponds to the state with the least reduced loss).
Reference: [Chu94] <author> T. Chung. </author> <title> Approximate methods for sequential decision making using expert advice. </title> <booktitle> In Proc Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 183189. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: We could imagine modeling the question of when and where such a process should move in the decision-theoretic experts framework of Freund and Schapire [FS95] and Chung <ref> [Chu94] </ref> as follows. <p> This parameter r is known to the on-line and off-line MTS algorithms. 2.2 Tracking experts in the decision-theoretic setting The second setting we consider is the decision-theoretic framework for learning from expert advice (also called the on-line allocation problem) <ref> [FS95, Chu94] </ref>. In this problem the learning algorithm faces a sequence of trials. For trial t, the algorithm chooses a probability distribution p t over a set of n experts.
Reference: [Esk90] <author> M. Eskicioglu. </author> <title> Process migration in distributed systems: A comparitive survey. </title> <type> Technical Report TR 90-3, </type> <institution> University of Alberta, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: In research process migration systems, the time for a process to move is roughly proportional to its size. For a 100-KB process, the time is about a second <ref> [Esk90] </ref>. Our distance corresponds to large but reasonable memory usage. Our simulations compared the performance of nine algorithms, including four simple control algorithms: Uniform The algorithm picks a random machine and stays there.
Reference: [FKL + 91] <author> A. Fiat, R. Karp, M. Luby, L. McGeoch, D. Sleator, and N. Young. </author> <title> Competitive paging algorithms. J of Algorithms, </title> <address> 12:685699, </address> <year> 1991. </year>
Reference-contexts: the total cost for the partition is at most k X 1 1 1 1 as desired. 3.2 Marking For the MTS problem on a uniform metric space of more than two states, the standard algorithm is the Marking algorithm of Borodin, Linial, and Saks [BLS92] and Fiat et al. <ref> [FKL + 91] </ref>. Algorithm Marking [BLS92]: We maintain a counter for each state. At the beginning of each phase, the counters are reset to 0, and the algorithm occupies a random state. Given a cost vector `, we increment the ith counter by ` i .
Reference: [FS95] <author> Y. Freund and R. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 2337, </pages> <year> 1995. </year>
Reference-contexts: We could imagine modeling the question of when and where such a process should move in the decision-theoretic experts framework of Freund and Schapire <ref> [FS95] </ref> and Chung [Chu94] as follows. <p> This parameter r is known to the on-line and off-line MTS algorithms. 2.2 Tracking experts in the decision-theoretic setting The second setting we consider is the decision-theoretic framework for learning from expert advice (also called the on-line allocation problem) <ref> [FS95, Chu94] </ref>. In this problem the learning algorithm faces a sequence of trials. For trial t, the algorithm chooses a probability distribution p t over a set of n experts.
Reference: [HW95] <author> M. Herbster and M. Warmuth. </author> <title> Tracking the best expert. </title> <booktitle> In Proc International Conference on Machine Learning, </booktitle> <pages> pages 286294. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: In the other direction, in Section 4 we present two algorithms for tracking the best expert in the decision-theoretic setting based on established weighted-experts algorithms <ref> [HW95, LW94] </ref> and show that one of them achieves good performance for the MTS problem, whereas the other has an unbounded competitive ratio. <p> We will call this the Experts-DTF setting. This problem is typically analyzed with the goal of performing nearly as well as the best expert on the given sequence of trials. We consider the partitioning bound, a stronger goal (considered by <ref> [HW95] </ref> and [LW94] for specific classes of loss functions) of performing nearly as well as the best sequence of experts. <p> Unfortunately, for the same L coefficient, the k coefficient guaranteed by Corollary 6 is approximately 2e times worse than for an algorithm (presented below) based on Herbster and Warmuth's weight-sharing algorithm <ref> [HW95] </ref>. <p> This situation can repeat indefinitely, causing the algorithm to incur unbounded movement cost with insignificant increase in the off-line optimal cost, giving an unbounded competitive ratio. 4.2 A weight-sharing experts algorithm We now describe a weight-sharing algorithm based on the Variable-share algorithm of Herbster and Warmuth <ref> [HW95] </ref> and prove that it achieves good bounds in both the Experts-DTF and MTS settings. Algorithm Share: The algorithm has two parameters: fi 2 [0; 1] is the usual penalty parameter and ff 2 [0; 1=2] is a sharing parameter.
Reference: [IS95] <author> S. Irani and S. Seiden. </author> <title> Randomized algorithms for metrical task systems. </title> <booktitle> In Intl. Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 159170, </pages> <year> 1995. </year>
Reference-contexts: This notion of an on-line algorithm having state, with a cost for moving between states, is captured by a problem studied in the On-line Algorithms literature called the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>. In this problem, we imagine the on-line algorithm is controlling a system that can be in one of n states or configurations. <p> Finally, in Section 5 we present an empirical comparison of these algorithms and others for the process migration problem. 2 Definitions and general relations 2.1 The MTS problem and the competitive ratio In the Metrical Task System (MTS) problem <ref> [BLS92, IS95] </ref>, an on-line algorithm controls a system with n states located at points in a space with distance metric d. The algorithm receives, one at a time, a sequence of tasks, each a cost vector specifying the cost of performing the task in each state.
Reference: [LW94] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted majority algorithm. Information and Computation, </title> <address> 108(2):212261, </address> <year> 1994. </year>
Reference-contexts: In the other direction, in Section 4 we present two algorithms for tracking the best expert in the decision-theoretic setting based on established weighted-experts algorithms <ref> [HW95, LW94] </ref> and show that one of them achieves good performance for the MTS problem, whereas the other has an unbounded competitive ratio. <p> We will call this the Experts-DTF setting. This problem is typically analyzed with the goal of performing nearly as well as the best expert on the given sequence of trials. We consider the partitioning bound, a stronger goal (considered by [HW95] and <ref> [LW94] </ref> for specific classes of loss functions) of performing nearly as well as the best sequence of experts. <p> We will see that another algorithm, a weight-sharing algorithm based on the ideas of Herbster and Warmuth, succeeds in both settings. 4.1 A threshold experts algorithm The WML algorithm, due to Littlestone and Warmuth, is the following strategy for the on-line discrete prediction problem. Algorithm WML <ref> [LW94] </ref>: The algorithm uses two parameters, fi 2 [0; 1] and ff 2 [0; 1=2], and maintains a weight w i for each expert, initialized to 1.

References-found: 10

