URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94414.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: On the Correctness of Parallel Bisection in Floating Point  
Author: James W. Demmel Inderjit Dhillon Huan Ren 
Note: The author was supported by NSF grants ASC-9005933 and CCR-9196022, and DARPA grant DAAL03 91-C-0047 via a subcontract from the University of Tennessee. The author was supported by DARPA grant DAAL03-91-C-0047 via a subcontract from the University of Tennessee. The author was supported by DARPA grant DAAL03-91-C-0047 via a subcontract from the  
Address: Berkeley, California 94720  Berkeley, California 94720  Berkeley, California 94720  
Affiliation: Computer Science Division and Department of Mathematics University of California  Computer Science Division University of California  Department of Mathematics University of California  University of Tennessee.  
Abstract: Computer Science Division Technical Report UCB//CSD-94-805. University of California, Berkeley, CA 94720. March 30, 1994. Abstract Bisection is an easily parallelizable method for finding the eigenvalues of real symmetric tridiagonal matrices, or more generally symmetric acyclic matrices. It requires a function Count(x) which counts the number of eigenvalues less than x. In exact arithmetic Count(x) is an increasing function of x, but this is not necessarily the case with roundoff. Our first result is that as long as the floating point arithmetic is monotonic, the computed function Count(x) implemented appropriately will also be monotonic; this extends an unpublished 1966 result of Kahan to the larger class of symmetric acyclic matrices. Second, we analyze the impact of nonmonotonicity of Count(x) on the serial and parallel implementations of bisection. We present simple and natural implementations which can fail because of nonmonotonicity; this includes the routine bisect in EISPACK. We also show how to implement bisection correctly despite nonmonotonic-ity; this is important because the fastest known parallel implementation of Count(x) is nonmonotonic even if the floating point is not. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide, Release 1.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year> <pages> 235 pages. </pages>
Reference-contexts: 5.2 most floating point exceptions avoided by tests and branches and [21] IEEE IEEE standard floating point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and [4, 16] sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and <ref> [1] </ref> Best Scaling like sstebz, but prescales for optimal error bounds See section 5.5 Routine and [4, 16] Table 1: Different implementations of FloatingCount () Algorithms Description Where Ser Bisec Serial bisection algorithm that finds all the eigenvalues See section 4.4 of T in a user-specified interval Ser AllEig Serial bisection <p> Thus, in principle j (T ) can be computed nearly as accurately as the inherent uncertainty ~ j permits. Accounting for over/underflow is done as follows. We first discuss the way it is done in Eispack's bisect routine [21], then the superior method in Lapack's sstebz routine <ref> [1, 16] </ref>. The difficulty arises because if d 0 is tiny or zero, the division T 2 ij =d 0 can overflow. In addition, T 2 ij can itself over/underflow. <p> damaged by inserting the line "if jd i j &lt; tol then d i = tol" just before "if d i &lt; 0 then s i = s i + 1" in Algorithm 6; this is done in practice to avoid overflow and division by zero; see Algorithm 5 and <ref> [1, 16] </ref>. However, the proof does not work for the algorithm used to avoid overflow in the subroutine bisect [21].
Reference: [2] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Binary Floating Point Arithmetic, </title> <address> Std 754-1985 edition, </address> <year> 1985. </year> <month> 36 </month>
Reference-contexts: This result is clearly incorrect. In section 4 below, we will see that this can indeed occur using the the Eispack routine bisect (using IEEE floating point standard arithmetic <ref> [2, 3] </ref>, and without over/underflows or other exceptions). The goal of this paper is to explore the impact of nonmonotonicity on the bisection algorithm.
Reference: [3] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Radix Independent Floating Point Arithmetic, </title> <address> Std 854-1987 edition, </address> <year> 1987. </year>
Reference-contexts: This result is clearly incorrect. In section 4 below, we will see that this can indeed occur using the the Eispack routine bisect (using IEEE floating point standard arithmetic <ref> [2, 3] </ref>, and without over/underflows or other exceptions). The goal of this paper is to explore the impact of nonmonotonicity on the bisection algorithm.
Reference: [4] <author> M. Assadullah, J. Demmel, S. Figueroa, A. Greenbaum, and A. McKenney. </author> <title> On finding eigenvalues and singular values by bisection. </title> <note> LAPACK Working Note. in preparation. </note>
Reference-contexts: Let B min i6=j T 2 ij and M i. B !. p These assumptions may be achieved by explicitly scaling the input matrix (multiplying it by an appropriate scalar), and by ignoring small off-diagonal elements T 2 ij &lt; ! and so splitting the matrix into unreduced blocks <ref> [4] </ref>; see section 5.8 for details. By Weyl's Theorem [20], this may introduce a tiny error of amount no more than p computed eigenvalues. 2C. More assumptions on the scaling of the input matrix. <p> used with any of the implementations of Floating 7 Algorithms Description Where bisect algorithm used in the Eispack routine; See section 5.2 most floating point exceptions avoided by tests and branches and [21] IEEE IEEE standard floating point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and <ref> [4, 16] </ref> sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and [1] Best Scaling like sstebz, but prescales for optimal error bounds See section 5.5 Routine and [4, 16] Table 1: Different implementations of FloatingCount () Algorithms Description Where Ser Bisec <p> point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and <ref> [4, 16] </ref> sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and [1] Best Scaling like sstebz, but prescales for optimal error bounds See section 5.5 Routine and [4, 16] Table 1: Different implementations of FloatingCount () Algorithms Description Where Ser Bisec Serial bisection algorithm that finds all the eigenvalues See section 4.4 of T in a user-specified interval Ser AllEig Serial bisection algorithm that finds all the eigenvalues of T See section 4.4 Par AllEig1 Parallel bisection algorithm
Reference: [5] <author> J. Barlow and J. Demmel. </author> <title> Computing accurate eigensystems of scaled diagonally dominant matrices. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 27(3) </volume> <pages> 762-791, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: For example, when using Algorithm 1, t i can range 6 from max i j i j down to O (") max i j i j, or perhaps smaller for special matrices <ref> [5] </ref>. Let U be a non-empty set of indices that correspond to the eigenvalues that the user wants to compute, e.g., U = f1; : : : ; ng if the user wants to compute all the eigenvalues of a matrix of dimension n. <p> If T ii = 0 for all i, then ~ k (1 (C +4)") 1n j k j, i.e. each eigenvalue is changed by an amount small relative to itself. See <ref> [16, 5, 10] </ref> for more such bounds. <p> In other words, the eigenvalues of T 0 and T lie between the eigenvalues of matrices T 1 and T 2 , where the entries of T j approximate those of T with relative accuracy tol. This is a nearly unimprovable backward error bound. Combined with <ref> [5, Theorem 4] </ref>, this easily yields Corollary 5.1 Let T be fl-s.d.d., and suppose tol &lt; (1 fl)=(1 + fl) (see [5] for definitions). Suppose jb i j tol (ja i a i+1 j) 1=2 , and let T 0 = T except for b 0 i = 0. <p> This is a nearly unimprovable backward error bound. Combined with [5, Theorem 4], this easily yields Corollary 5.1 Let T be fl-s.d.d., and suppose tol &lt; (1 fl)=(1 + fl) (see <ref> [5] </ref> for definitions). Suppose jb i j tol (ja i a i+1 j) 1=2 , and let T 0 = T except for b 0 i = 0. <p> Let tol = 10 10 . Then Eispack would set the offdiagonals to zero, returning eigenvalues 10 20 and 1. The true eigenvalues (to about 20 digits) are 10 20 and .75. Note that criterion (5.12) could possibly be used as the stopping criterion in a QR algorithm <ref> [5] </ref> in the hopes of attaing high relative accuracy.
Reference: [6] <author> H. Bernstein and M. Goldstein. </author> <title> Parallel implementation of bisection for the calculation of eigenvalues of a tridiagonal symmetric matrices. </title> <type> Technical report, </type> <institution> Courant Institute, </institution> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: The bisection algorithm offers ample opportunities for parallelism, and many parallel implementations exist <ref> [6, 13, 18, 14] </ref>. We first discuss a natural parallel implementation which gives the false appearance of being correct.
Reference: [7] <author> J. Demmel. </author> <title> Underflow and the reliability of numerical software. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5(4) </volume> <pages> 887-919, </pages> <month> Dec </month> <year> 1984. </year>
Reference-contexts: We still call these two statements an assumption, rather than a theorem, because they are the most convenient building blocks for the ultimate correctness proofs. Assumption 1 1A. Assumptions about Floating Point Arithmetic Models Barring overflow, the usual expression for roundoff is extended to include underflow as follows <ref> [7] </ref>: f l (a b) = (a b)(1 + ffi) + (2.1) where is a binary arithmetic operation, jffij is bounded by machine precision ", jj is bounded by a tiny number !, typically the underflow threshold ! (the smallest normalized number which can safely participate in, or be a result
Reference: [8] <author> J. Demmel and W. Gragg. </author> <title> On computing accurate singular values and eigenvalues of acyclic matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 185 </volume> <pages> 203-218, </pages> <year> 1993. </year>
Reference-contexts: This result was first proven but not published by Kahan in 1966 for symmetric tridiagonal matrices [16]; in this paper we extend this result to symmetric acyclic matrices, a larger class including tridiagonal matrices, arrow matrices, and exponentially many others <ref> [8] </ref>; see section 6. Our third result is to formalize the notion of a correct implementation of bisection, and use this characterization to identify correct and incorrect serial and parallel implementations of bisection. <p> Section 5 reviews the roundoff error analysis of FloatingCount (x), and how to account for over/underflow; this material may also be found in <ref> [16, 8] </ref>. Section 6 illustrates how monotonicity can fail, and proves that a natural serial implementation of FloatingCount (x) must be monotonic if the arithmetic is. Section 7 gives formal proofs for the correctness of the bisection algorithms given in section 4. <p> Section 2.3 list the criteria a bisection algorithm must satisfy to be correct. 2.1 Preliminary Definitions Algorithm 1 was recently extended to the larger class of symmetric acyclic matrices <ref> [8] </ref>, i.e. those matrices whose graphs are acyclic (trees). The undirected graph G (T ) of a symmetric n-by-n matrix T is defined to have n nodes and an edge (i; j), i &lt; j, if and only if T ij 6= 0. <p> It is much harder (and less elegant) to construct and prove correct-ness of similar correct and efficient parallel algorithms that do not use any communication. 5 Roundoff Error Analysis As we mentioned before, Algorithm 1 was recently extended to the symmetric acyclic matrices. In <ref> [8] </ref> the following implementation of Count (x) for acyclic matrices was given. <p> 0 for all children j of i do call TreeCount (j; x; d 0 ; s 0 ) sum = sum + T 2 s = s + s 0 endfor d = (T ii x) sum if d &lt; 0 then s = s + 1 end TreeCount In <ref> [8] </ref> it is also shown that barring over/underflow, the floating point version of Algorithm 2 has the same attractive backward error analysis as the floating point version of Algorithm 1: Let FloatingCount (x; T ) denote the value of Count (x; T ) computed in floating point arithmetic. <p> node in the graph G (T ) and f (n; ") is defined by f (n; ") = (1 + ") n 1: By Assumption 2A (n" :1), we have [22]: f (n; ") 1:06n": (Strictly speaking, the proof of this bound is a slight modification of the one in <ref> [8] </ref>, and requires that d be computed exactly as shown in TreeCount. The analysis in [8] makes no 17 assumption about the order in which the sum for d is evaluated, whereas the bound (5.2) for TreeCount assumes the parentheses in the sum for d are respected. <p> (n; ") = (1 + ") n 1: By Assumption 2A (n" :1), we have [22]: f (n; ") 1:06n": (Strictly speaking, the proof of this bound is a slight modification of the one in <ref> [8] </ref>, and requires that d be computed exactly as shown in TreeCount. The analysis in [8] makes no 17 assumption about the order in which the sum for d is evaluated, whereas the bound (5.2) for TreeCount assumes the parentheses in the sum for d are respected.
Reference: [9] <author> J. Demmel, M. Heath, and H. van der Vorst. </author> <title> Parallel numerical linear algebra. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica, </booktitle> <volume> volume 2. </volume> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: Theorem 6.1, we will show that this cannot happen for the Lapack routine dstebz (sstebz) even for general symmetric acyclic matrices. 4.2 Nonmonotonicity of Parallel Prefix Algorithm We now give another example of a nonmonotonic FloatingCount (x) when Count (x) is implemented using a fast parallel algorithm called parallel prefix <ref> [9] </ref>.
Reference: [10] <author> J. Demmel and W. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(5) </volume> <pages> 873-912, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: If T ii = 0 for all i, then ~ k (1 (C +4)") 1n j k j, i.e. each eigenvalue is changed by an amount small relative to itself. See <ref> [16, 5, 10] </ref> for more such bounds.
Reference: [11] <author> J. Demmel and X. Li. </author> <title> Faster numerical algorithms via exception handling. </title> <editor> In M. J. Ir-win E. Swartzlander and G. Jullien, editors, </editor> <booktitle> Proceedings of the 11th Symposium on Computer Arithmetic, </booktitle> <address> Windsor, Ontario, </address> <month> June 29 - July 2 </month> <year> 1993. </year> <note> IEEE Computer Society Press. to appear in IEEE Trans. Comp.; available as all.ps.Z via anonymous ftp from tr-ftp.cs.berkeley.edu, in directory pub/tech-reports/cs/csd-93-728; software is csd-93-728.shar.Z. </note>
Reference-contexts: By using IEEE arithmetic, we can eliminate all tests in the inner loop, and so make it faster on many architectures <ref> [11] </ref>. To describe the error analysis, we again make Assumptions 1A (ii), 1A (iii) and 2B (ii) as in Section 5.2, and Assumption 2B (i), which is B min i6=j T 2 ij !. Algorithm 4: IEEE routine.
Reference: [12] <author> J. Demmel and H. Ren. </author> <title> The instability and nonmonotonicity of the parallel prefix algorithm. </title> <note> in preparation, </note> <year> 1994. </year>
Reference-contexts: Figure 1 shows the FloatingCount (x) of a 64 fi 64 matrix of norm near 1 with 32 eigenvalues very close to 5 10 8 computed both by the conventional bisection algorithm and the parallel prefix algorithm in the neighborhood of the eigenvalues; see <ref> [19, 12] </ref> for details. 4.3 A Correct Serial Implementation of the Bisection Algorithm As we saw in Section 4.1, the Eispack implementation of the bisection algorithm fails in the face of nonmonotonicity of the function FloatingCount (x).
Reference: [13] <author> I. S. Dhillon and J. W. Demmel. </author> <title> A parallel algorithm for the symmetric tridiagonal eigenproblem and its implementation on the CM-5. </title> <booktitle> In progress, </booktitle> <year> 1993. </year>
Reference-contexts: The bisection algorithm offers ample opportunities for parallelism, and many parallel implementations exist <ref> [6, 13, 18, 14] </ref>. We first discuss a natural parallel implementation which gives the false appearance of being correct. <p> This static partitioning of work has been observed to give good performance on parallel machines like the CM-5 <ref> [13] </ref>, for almost all eigenvalue distributions. In algorithm Par Alleig3, processor i attempts to find eigenvalues i (n=p) + 1 through (i + 1)n=p. <p> Algorithm Ser Bisec may be modified simply to yield a parallel algorithm, where all the enqueuing and dequeing of tasks is done from a global W orklist that is distributed across all the processors. Such an algorithm has been implemented on the CM-5 <ref> [13] </ref> | the work is initially partitioned among the processors (as in algorithms Par AllEig1 and Par AllEig3), and load imbalance is reduced by enqueuing and dequeing tasks from other processors. This algorithm has been observed to give good performance even when the initial partitioning of work is not good.
Reference: [14] <author> Y. Huo and R. Schreiber. </author> <title> Efficient, massively parallel eigenvalue computations. </title> <type> preprint, </type> <year> 1993. </year>
Reference-contexts: The bisection algorithm offers ample opportunities for parallelism, and many parallel implementations exist <ref> [6, 13, 18, 14] </ref>. We first discuss a natural parallel implementation which gives the false appearance of being correct.
Reference: [15] <author> W. Kahan. </author> <title> When to neglect offdiagonal elements of symmetric tridiagonal matrices. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS42, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966. </year>
Reference-contexts: 2 i ) if t &lt; tol 2 then b i = 0 This also guarantees that no eigenvalue will change by more than tol (in fact it guarantees that the square root of the sum of the squares of the changes in all the eigenvalues is bounded by tol) <ref> [15, 20] </ref>. Although it sets b i to zero more often than the simpler test (5.10), it is much more expensive.
Reference: [16] <author> W. Kahan. </author> <title> Accurate eigenvalues of a symmetric tridiagonal matrix. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS41, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966 </year> <month> (revised June </month> <year> 1968). </year> <month> 37 </month>
Reference-contexts: A sufficient condition for floating point to be monotonic is that it be correctly rounded or correctly chopped; thus IEEE floating point 2 arithmetic is monotonic. This result was first proven but not published by Kahan in 1966 for symmetric tridiagonal matrices <ref> [16] </ref>; in this paper we extend this result to symmetric acyclic matrices, a larger class including tridiagonal matrices, arrow matrices, and exponentially many others [8]; see section 6. <p> Section 5 reviews the roundoff error analysis of FloatingCount (x), and how to account for over/underflow; this material may also be found in <ref> [16, 8] </ref>. Section 6 illustrates how monotonicity can fail, and proves that a natural serial implementation of FloatingCount (x) must be monotonic if the arithmetic is. Section 7 gives formal proofs for the correctness of the bisection algorithms given in section 4. <p> used with any of the implementations of Floating 7 Algorithms Description Where bisect algorithm used in the Eispack routine; See section 5.2 most floating point exceptions avoided by tests and branches and [21] IEEE IEEE standard floating point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and <ref> [4, 16] </ref> sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and [1] Best Scaling like sstebz, but prescales for optimal error bounds See section 5.5 Routine and [4, 16] Table 1: Different implementations of FloatingCount () Algorithms Description Where Ser Bisec <p> point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and <ref> [4, 16] </ref> sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and [1] Best Scaling like sstebz, but prescales for optimal error bounds See section 5.5 Routine and [4, 16] Table 1: Different implementations of FloatingCount () Algorithms Description Where Ser Bisec Serial bisection algorithm that finds all the eigenvalues See section 4.4 of T in a user-specified interval Ser AllEig Serial bisection algorithm that finds all the eigenvalues of T See section 4.4 Par AllEig1 Parallel bisection algorithm <p> If T ii = 0 for all i, then ~ k (1 (C +4)") 1n j k j, i.e. each eigenvalue is changed by an amount small relative to itself. See <ref> [16, 5, 10] </ref> for more such bounds. <p> Thus, in principle j (T ) can be computed nearly as accurately as the inherent uncertainty ~ j permits. Accounting for over/underflow is done as follows. We first discuss the way it is done in Eispack's bisect routine [21], then the superior method in Lapack's sstebz routine <ref> [1, 16] </ref>. The difficulty arises because if d 0 is tiny or zero, the division T 2 ij =d 0 can overflow. In addition, T 2 ij can itself over/underflow. <p> Thus we have the following backward error: jT ij T 0 and ii j 2 ^p + &gt; &lt; (2C + 1)! Model 1 C"! Model 2 (2C + 1)! Model 3 : 5.5 Models 1,2 and 3: Best Prescaling Algorithm, Acyclic Matrix Following Kahan <ref> [16] </ref>, let ~ = ! 1=4 1=2 and M = ~ = ! 1=4 1=2 . <p> However, the rounding errors in any existing QR algorithm generally cause far more inaccuracy in the computed eigenvalues than the currently used stopping criteria (which are generally identical to (5.13)). 27 6 Proof of Monotonicity of Count (x) In 1966 Kahan proved but did not publish the following result <ref> [16] </ref>: if the floating point arithmetic is monotonic, then FloatingCount (x) is a monotonically increasing function of x for symmetric tridiagonal matrices. <p> damaged by inserting the line "if jd i j &lt; tol then d i = tol" just before "if d i &lt; 0 then s i = s i + 1" in Algorithm 6; this is done in practice to avoid overflow and division by zero; see Algorithm 5 and <ref> [1, 16] </ref>. However, the proof does not work for the algorithm used to avoid overflow in the subroutine bisect [21].
Reference: [17] <author> W. Kahan. </author> <title> Analysis and refutation of the International Standard ISO/IEC for Lan--guage Compatible Arithmetic. </title> <journal> SIGNUM Newsletter and SIGPLAN Notices, </journal> <year> 1991. </year>
Reference-contexts: This assumption always holds in IEEE arithmetic, and for any model of arithmetic which rounds correctly, i.e., rounds a result to the nearest floating point number. For a detailed treatment of how to compute ff+fi 2 correctly on various machines, see <ref> [17] </ref>.
Reference: [18] <author> S.-S. Lo, B. Phillipe, and A. Sameh. </author> <title> A multiprocessor algorithm for the symmetric eigenproblem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 155-165, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: The bisection algorithm offers ample opportunities for parallelism, and many parallel implementations exist <ref> [6, 13, 18, 14] </ref>. We first discuss a natural parallel implementation which gives the false appearance of being correct.
Reference: [19] <author> R. Mathias. </author> <title> The stability of parallel prefix matrix multiplication with applications to tridiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <note> 1993. submitted. </note>
Reference-contexts: Figure 1 shows the FloatingCount (x) of a 64 fi 64 matrix of norm near 1 with 32 eigenvalues very close to 5 10 8 computed both by the conventional bisection algorithm and the parallel prefix algorithm in the neighborhood of the eigenvalues; see <ref> [19, 12] </ref> for details. 4.3 A Correct Serial Implementation of the Bisection Algorithm As we saw in Section 4.1, the Eispack implementation of the bisection algorithm fails in the face of nonmonotonicity of the function FloatingCount (x).
Reference: [20] <author> B. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: 1 Introduction Let T by an n-by-n real symmetric tridiagonal matrix with diagonals a 1 ; :::; a n and off diagonals b 1 ; :::; b n1 ; we let b 0 0. Let 1 n be T 's eigenvalues. It is well known <ref> [20] </ref> that the function Count (x) defined below returns the number of eigenvalues of T that are less than x (for all but the finite number of x resulting in a divide by zero) : Algorithm 1: Count (x) returns the number of eigenvalues of a real symmetric tridiagonal matrix T <p> B !. p These assumptions may be achieved by explicitly scaling the input matrix (multiplying it by an appropriate scalar), and by ignoring small off-diagonal elements T 2 ij &lt; ! and so splitting the matrix into unreduced blocks [4]; see section 5.8 for details. By Weyl's Theorem <ref> [20] </ref>, this may introduce a tiny error of amount no more than p computed eigenvalues. 2C. More assumptions on the scaling of the input matrix. <p> Suppose the backward error in (5.2) can change eigenvalue k by at most ~ k . For example, Weyl's Theorem <ref> [20] </ref> implies that ~ k kT T 0 k 2 2f (C=2 + 2; ")kT k 2 , i.e. that each eigenvalue is changed by an amount small compared to the largest eigenvalue. <p> Then this backward error can change the eigenvalues k by at most ~ k where ~ k 2ff (") k T k 2 +fi: (5.8) Proof. By Weyl's Theorem <ref> [20] </ref>, ~ k kT T 0 k 2 kjT T 0 jk 2 kff (")jT flj + fiIk 2 ff (")kjT fljk 2 + fi: and kjT fljk 2 = kT flk 2 kT k 2 + kflk 2 2kT k 2 : 23 Algorithms Model 1 Model 2 Model 3 <p> This is useful because setting b i to zero splits T into two independent subproblems which can be solved faster 25 (serially or in parallel). If we are only interested in absolute accuracy, then Weyl's Theorem <ref> [20] </ref> guarantees that the test if jb i j tol then b i = 0 (5.10) will not change any eigenvalue by more than tol. <p> 2 i ) if t &lt; tol 2 then b i = 0 This also guarantees that no eigenvalue will change by more than tol (in fact it guarantees that the square root of the sum of the squares of the changes in all the eigenvalues is bounded by tol) <ref> [15, 20] </ref>. Although it sets b i to zero more often than the simpler test (5.10), it is much more expensive. <p> Then exp ( 1 fl 1+tol ) i exp ( 1 fl 1+tol ) When tol t 1 fl then 1 1 fl 0 i tol 26 Proof of Lemma 5.2. By the Courant Minimax Theorem <ref> [20] </ref> it suffices to construct T 1 and T 2 satisfying condition iii in the Lemma such that for all vectors x x T T 1 x x T T 0 x x T T 2 x and x T T 1 x x T T x x T T 2
Reference: [21] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: Table 4 says that Algorithms Ser Bisec, Ser AllEig, Par AllEig2 and Par AllEig3 will be correct when used with any of the implementations of Floating 7 Algorithms Description Where bisect algorithm used in the Eispack routine; See section 5.2 most floating point exceptions avoided by tests and branches and <ref> [21] </ref> IEEE IEEE standard floating point arithmetic used to accommodate See section 5.3 possible exceptions; tridiagonals only and [4, 16] sstebz algorithm used in the Lapack routine See section 5.4 floating point exceptions avoided by tests and branches and [1] Best Scaling like sstebz, but prescales for optimal error bounds See <p> Thus, in principle j (T ) can be computed nearly as accurately as the inherent uncertainty ~ j permits. Accounting for over/underflow is done as follows. We first discuss the way it is done in Eispack's bisect routine <ref> [21] </ref>, then the superior method in Lapack's sstebz routine [1, 16]. The difficulty arises because if d 0 is tiny or zero, the division T 2 ij =d 0 can overflow. In addition, T 2 ij can itself over/underflow. <p> Note that it will never be applied to tridiagonals with zero diagonal unless b i is exactly zero. This criterion is more stringent than the criterion in the Eispack code bisect <ref> [21] </ref>, which essentially is if jb i j tol (ja i j + ja i+1 j) then b i = 0: (5.13) Note that this is at least about as stringent as the absolute accuracy criterion (5.10) but less stringent than the relative accuracy criterion (5.12). <p> However, the proof does not work for the algorithm used to avoid overflow in the subroutine bisect <ref> [21] </ref>. This is because bisect tests if a computed d i is exactly zero, and increases if it is; this can increase d i (y 0 ) past d i (y) even if inequalities (6.16) are satisfied. The example in section 4.2 shows that monotonicity can indeed fail in practice.
Reference: [22] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: T 0 ii ; (5.2) where " is the machine precision, C is the maximum number of children of any node in the graph G (T ) and f (n; ") is defined by f (n; ") = (1 + ") n 1: By Assumption 2A (n" :1), we have <ref> [22] </ref>: f (n; ") 1:06n": (Strictly speaking, the proof of this bound is a slight modification of the one in [8], and requires that d be computed exactly as shown in TreeCount.
References-found: 22

