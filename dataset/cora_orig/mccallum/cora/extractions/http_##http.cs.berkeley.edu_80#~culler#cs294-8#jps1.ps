URL: http://http.cs.berkeley.edu:80/~culler/cs294-8/jps1.ps
Refering-URL: http://http.cs.berkeley.edu:80/~culler/cs294-8/
Root-URL: http://www.cs.berkeley.edu
Title: TreadMarks: Shared Memory Computing on Networks of Workstations  
Author: Cristiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, Weimin Yu and Willy Zwaenepoel 
Affiliation: Department of Computer Science Rice University  
Abstract: TreadMarks supports parallel computing on networks of workstations by providing the application with a shared memory abstraction. Shared memory facilitates the transition from sequential to parallel programs. After identifying possible sources of parallelism in the code, most of the data structures can be retained without change, and only synchronization needs to be added to achieve a correct shared memory parallel program. Additional transformations may be necessary to optimize performance, but this can be done in an incremental fashion. We discuss the techniques used in TreadMarks to provide efficient shared memory, and our experience with two large applications, mixed integer programming and genetic linkage analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Other synchronization operations can be mapped into acquires and releases as well. A partial order, happened-before-1 , denoted hb1 ! can be defined on releases, acquires, and shared memory accesses in the following way <ref> [1] </ref>: * If a 1 and a 2 are ordinary shared memory accesses, releases, or acquires on the same processor, and a 1 occurs before a 2 in program order, then a 1 hb1 * If a 1 is a release on processor p 1 , and a 2 is a
Reference: [2] <author> S. Ahuja, N. Carreiro, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The programmability and performance differences between these two approaches are not yet well understood. Structured DSM Systems (Linda). Rather than providing the programmer with a shared memory space organized as a linear array of bytes, structured DSM systems offer either a shared space of objects or tuples <ref> [2] </ref>, which are accessed by properly synchronized methods. Besides the advantages from a programming perspective, this approach allows the compiler to infer certain optimizations that can be used to reduce the amount of communication.
Reference: [3] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: It was also shown that an invalidate protocol works better than an update protocol, because of the large amount of data resulting from the update protocol. Entry Consistency (Midway). Entry consistency is another relaxed memory model <ref> [3] </ref>. As in release consistency, consistency actions are taken in conjunction with synchronization operations. Unlike release consistency, however, entry consistency requires that each shared data object be associated with a synchronization object. When a synchronization object is acquired, only the modified data associated with that synchronization object is made consistent.
Reference: [4] <author> J.B. Carter. Munin: </author> <title> Efficient Distributed Shared Memory Using Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1993. </year> <note> Also appeared as Rice Technical Report RICE COMP-TR-211. </note>
Reference-contexts: TreadMarks uses the lazy release consistency algorithm [5] to implement release consistency. Roughly speaking, lazy release consistency enforces consistency at the time of an acquire, in contrast to the earlier implementation of release consistency in Munin <ref> [4] </ref>, sometimes referred to as eager release consistency, which enforced consistency at the time of a release. Figure 7 shows the intuitive argument behind lazy release consistency. Assume that x is replicated at all processors. <p> Although the net effect is somewhat application dependent, release consistent DSMs in general send fewer messages than sequentially consistent DSMs and therefore perform better. In particular, Carter's Ph.D. thesis contains a comparison of seven application programs run either with eager release consistency (Munin) or with sequential consistency <ref> [4] </ref>. Compared to a sequentially consistent DSM, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application. Lazy vs. Eager Release Consistency (Munin). Lazy release consistency causes fewer messages to be sent.
Reference: [5] <author> P. Keleher. </author> <title> Distributed Shared Memory Using Lazy Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1994. </year> <note> Also appeared as Rice Technical Report RICE COMP-TR-240. </note>
Reference-contexts: The naive use of virtual memory protection hardware may lead to poor performance because of discrepancies between the page size of the machine and the granularity of sharing in the application. The system discussed in this paper, TreadMarks <ref> [5] </ref>, provides shared memory as a linear array of bytes. The memory model is a relaxed memory model, namely release consistency. <p> This allows an implementation of release consistency considerable latitude in deciding when and how exactly a shared memory update gets propagated. TreadMarks uses the lazy release consistency algorithm <ref> [5] </ref> to implement release consistency. Roughly speaking, lazy release consistency enforces consistency at the time of an acquire, in contrast to the earlier implementation of release consistency in Munin [4], sometimes referred to as eager release consistency, which enforced consistency at the time of a release. <p> An alternative would be to use an update protocol, in which the acquire message contains the new values of the modified pages. A detailed discussion of the protocols used in TreadMarks is beyond the scope of this paper. We refer the reader to Keleher's thesis <ref> [5] </ref> for more detail. <p> In this section, we briefly describe how communication and memory management by TreadMarks are implemented. For a more detailed discussion of the implementation, we refer the reader to Keleher's Ph.D. thesis <ref> [5] </ref>. TreadMarks implements intermachine communication using the Berkeley sockets interface. Depending on the underlying networking hardware, for example, Ethernet or ATM, TreadMarks uses either UDP/IP or AAL3/4 as the message transport protocol. By default, TreadMarks uses UDP/IP unless the machines are connected by an ATM LAN. <p> The worst case occurs when every other word in the page is changed. In that case, making a diff takes 686 microseconds. 8 Applications A number of applications have been implemented using TreadMarks, and the performance of some benchmarks has been reported earlier <ref> [5] </ref>. Here we describe our experience with two large applications that were recently implemented using TreadMarks. These applications, mixed integer programming and genetic linkage analysis, were parallelized, starting from an existing efficient sequential code, by the authors of the sequential code with some help from the authors of this paper. <p> We share with this approach the programming model, but our implementation avoids expensive cache controller hardware. On the other hand, a hardware implementation can efficiently support applications with finer-grain parallelism. We have some limited experience with comparing the performance of hardware and software shared memory <ref> [5] </ref>. In particular, we compared the performance of four applications, including a slightly older version of ILINK, on an 8-processor SGI 4D/380 hardware shared memory multiprocessor and on TreadMarks running on our 8-processor ATM network of DECStation-5000/240s. <p> In particular, Keleher has compared the performance of ten applications under lazy and eager release consistency, and found that for all but one (3-D Fast Fourier Transform) the lazy implementation performed better <ref> [5] </ref>. It was also shown that an invalidate protocol works better than an update protocol, because of the large amount of data resulting from the update protocol. Entry Consistency (Midway). Entry consistency is another relaxed memory model [3].
Reference: [6] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: TreadMarks and PVM are both explicitly parallel programming methods: the programmer has to divide the computation among different threads and use either synchronization or message passing to control the interactions among the concurrent threads. With implicit parallelism, as in HPF <ref> [6] </ref>, the user writes a single-threaded program, which is then par-allelized by the compiler. In particular, HPF contains data distribution primitives, which may be used by the compiler to drive the parallelization process. This approach is suitable for data-parallel programs, such as Jacobi.
Reference: [7] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Unfortunately, the notion of "the last value written" is not well defined in a distributed system. A more precise notion is sequential consistency, whereby the memory appears to all processes as if they were executing on a single multiprogrammed processor <ref> [7] </ref>. With sequential consistency, the notion of "the last value written" is precisely defined. The simplicity of this model may, however, exact a high price in terms of performance, and therefore much research has been done into relaxed memory models. <p> The consistency model defines how the programmer can expect the memory system to behave. The first DSM system, IVY [9], implemented sequential consistency <ref> [7] </ref>. In this memory model, processes observe shared memory as if they were executing on a multiprogrammed uniprocessor (with a single memory).
Reference: [8] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: An alternative approach to shared memory is to implement it in hardware, using a snooping bus protocol for a small number of processors or using a directory-based protocol for larger number of processors (e.g., <ref> [8] </ref>). We share with this approach the programming model, but our implementation avoids expensive cache controller hardware. On the other hand, a hardware implementation can efficiently support applications with finer-grain parallelism. We have some limited experience with comparing the performance of hardware and software shared memory [5].
Reference: [9] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In this paper, we discuss our experience with parallel computing on networks of workstations using the TreadMarks distributed shared memory (DSM) system. DSM allows processes to assume a globally shared virtual memory even though they execute on nodes that do not physically share memory <ref> [9] </ref>. Figure 1 illustrates a DSM system consisting of N networked workstations, each with its own memory, connected by a network. <p> The consistency model defines how the programmer can expect the memory system to behave. The first DSM system, IVY <ref> [9] </ref>, implemented sequential consistency [7]. In this memory model, processes observe shared memory as if they were executing on a multiprogrammed uniprocessor (with a single memory).
Reference: [10] <author> H. Lu. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1995. </year> <note> Also appeared as Rice Technical Report RICE COMP-TR-250. </note>
Reference-contexts: For a more detailed comparison in programmability and performance between TreadMarks and PVM we refer the reader to Lu's M.S. thesis, which includes nine different applications <ref> [10] </ref>. 22 numbers at the bottom indicate the sequential execution time in seconds for the corresponding data set. 23 Implicit Parallelism (HPF).
Reference: [11] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> IEEE Computer, </journal> <volume> 24(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Most DSM systems choose to replicate data, because this approach gives the best performance for a wide range of application parameters of interest <ref> [11] </ref>. With replicated data, the provision of memory consistency is at the heart of a DSM system: the DSM software must control replication in a manner that provides the abstraction of a single shared memory. The consistency model defines how the programmer can expect the memory system to behave.
Reference: [12] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency:Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year> <note> 0 The theses referred to in this paper can be obtained by anonymous ftp from cs.rice.edu under the directory public/TreadMarks/papers. For information on obtaining the TreadMarks system, send e-mail to treadmarks@ece.rice.edu. 26 </note>
Reference-contexts: We first discuss alternative programming models, and then turn to different implementations of the shared memory programming models. 9.1 Alternative Programming Models Message Passing (PVM). Currently, message passing is the prevailing programming paradigm for distributed memory systems. Parallel Virtual Machine (PVM) <ref> [12] </ref> is a popular software message passing package. It allows a heterogeneous network of computers to appear as a single concurrent computational engine. TreadMarks is currently restricted to a homogeneous set of nodes.
References-found: 12

