URL: ftp://ftp.ai.mit.edu/pub/cbcl/sv-memo.ps.gz
Refering-URL: http://www.ai.mit.edu/people/eosuna/publications.html
Root-URL: 
Title: Support Vector Machines: Training and Applications  
Author: Edgar E. Osuna, Robert Freund and Federico Girosi 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. The pathname for this publication is: ai-publications/1500-1999/AIM-1602.ps.Z Copyright c Massachusetts Institute of Technology, 1996  
Date: 1602 March, 1997  144  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L Paper No.  
Abstract: The Support Vector Machine (SVM) is a new and very promising classification technique developed by Vapnik and his group at AT&T Bell Laboratories [3, 6, 8, 24]. This new learning algorithm can be seen as an alternative training technique for Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers. The main idea behind the technique is to separate the classes with a surface that maximizes the margin between them. An interesting property of this approach is that it is an approximate implementation of the Structural Risk Minimization (SRM) induction principle [23]. The derivation of Support Vector Machines, its relationship with SRM, and its geometrical insight, are discussed in this paper. Since Structural Risk Minimization is an inductive principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing the Mean Square Error over the data set (as Empirical Risk Minimization methods do), training a SVM to obtain the maximum margin classifier requires a different objective function. This objective function is then optimized by solving a large-scale quadratic programming problem with linear and box constraints. The problem is considered challenging, because the quadratic form is completely dense, so the memory needed to store the problem grows with the square of the number of data points. Therefore, training problems arising in some real applications with large data sets are impossible to load into memory, and cannot be solved using standard non-linear constrained optimization algorithms. We present a decomposition algorithm that can be used to train SVM's over large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm. We present previous approaches, as well as results and important details of our implementation of the algorithm using a second-order variant of the Reduced Gradient Method as the solver of the sub-problems. As an application of SVM's, we present preliminary results in Frontal Human Face Detection in images. This application opens many interesting questions and future research opportunities, both in the context of faster and better optimization algorithms, and in the use of SVM's in other pattern classification, recognition, and detection applications. This report describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences and the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology. This research is sponsored by MURI grant N00014-95-1-0600; by a grant from ONR/ARPA under contract N00014-92-J-1879 and by the National Science Foundation under contract ASC-9217041 (this award includes funds from ARPA provided under the HPCC program). Edgar Osuna was supported by Fundacion Gran Mariscal de Ayacucho and Daimler Benz. Additional support is provided by Daimler-Benz, Eastman Kodak Company, Siemens Corporate Research, Inc. and AT&T. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Bazaraa, H. Sherali, and C. Shetty. </author> <title> NonLinear Programming Theory and Algorithms. </title> <editor> J. </editor> <publisher> Wiley, </publisher> <address> New York, 2nd edition, </address> <year> 1993. </year>
Reference-contexts: MINOS 5.4: MINOS 5.4 solves nonlinear problems with linear constraints using Wolfe's Reduced Gradient algorithm in conjunction with Davidson's quasi-Newton method. Details of its implementation are described by Murtagh and Saunders in [17], and MINOS 5.4 User's Guide [18] and Bazaraa et al. <ref> [1] </ref> present an overview with some heuristics and comparisons. Wolfe's Reduced Gradient method depends upon reducing the dimensionality of the problem by representing all variables in terms of an independent set of them.
Reference: [2] <author> D. Bertsekas. </author> <title> Projected newton methods for optimization problems with simple constraints. </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 20(2) </volume> <pages> 221-246, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Let x 2 <ref> [0; 2] </ref>, h a positive number such that h &lt; 1, and let us consider the following feature space: (x) = (1; h 2 sin (x); h 2 cos (x); h sin (2x); h cos (2x); : : : ; h 2 sin (nx); h 2 cos (nx); : : :) <p> Having as a reference the experience obtained with MINOS 5.4, new approaches to a tailored solver through, for example, projected Newton <ref> [2] </ref> or interior point methods [7], should be attempted. At this point it is not clear whether the same type of algorithm is appropriate for all stages of the solution process.
Reference: [3] <author> B.E. Boser, I.M. Guyon, and V.N. Vapnik. </author> <title> A training algorithm for optimal margin classifier. </title> <booktitle> In Proc. 5th ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In this report we address the problem of implementing a new pattern classification technique recently developed by Vladimir Vapnik and his team at AT&T Bell Laboratories <ref> [3, 6, 8, 24] </ref> and known as Support Vector Machine (SVM).
Reference: [4] <author> G. Burel and D. Carel. </author> <title> Detection and localization of faces on digital images. </title> <journal> Pattern Recognition Letters, </journal> <volume> 15 </volume> <pages> 963-967, </pages> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks <ref> [4] </ref> [19], detection of face features and use of geometrical constraints [27], density estimation of the training data [14], labeled graphs [12] and clustering and distribution-based modeling [22][21].
Reference: [5] <author> C. Burges and V. Vapnik. </author> <title> A new method for constructing artificial neural networks. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <address> 101 Crawfords Corner Road Holmdel NJ 07733, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: In particular, section 2 reviews the theory and derivation of SVM's, together with some extensions and geometrical interpretations. Section 3 starts by reviewing the work done by Vapnik et al. <ref> [5] </ref> in solving the training problem for the SVM. Section 3.1.1 gives a brief description and references of the initial approaches we took in order to solve this problem. <p> solving small problems, both because they constitute a natural first step, and also because the decomposition algorithm described in section 3.2 iteratively solves small sub-problems of the type given by (37). 3.1 Previous Work The training of a SVM with small data sets was first approached by Vapnik et al. <ref> [5] </ref> using a constrained conjugate gradient algorithm. Briefly described, conjugate gradient ascent was used to explore the feasible region until the step would move the solution outside of it. When that happened, the largest step along the conjugate direction was taken, while maintaining feasibility. <p> When that happened, the largest step along the conjugate direction was taken, while maintaining feasibility. Every time a variable i reached 0, the corresponding data point was removed (therefore reducing and approximating the solution) and the conjugate gradient process was re-started. The next approach taken by Vapnik et al. <ref> [5] </ref>, was to adapt to this problem the algorithm for bounded large-scale quadratic programs due to More and Toraldo [16]. Originally, this algorithm uses conjugate gradient to explore the face of the feasible region defined by the current iterate, and gradient projection to move to a different face. <p> The Support Vector Machine is a very new technique, and, as far as we know, this paper presents the second problem-solving application to use SVM, after Vapnik et al. <ref> [5, 8, 24] </ref> used it in the character recognition problem, in 1995. Our Face Detection System performs as well as other state-of-the-art systems, and has opened many interesting questions and possible future extensions.
Reference: [6] <author> C.J.C. Burges. </author> <title> Simplified support vector decision rules, </title> <year> 1996. </year>
Reference-contexts: 1 Introduction In this report we address the problem of implementing a new pattern classification technique recently developed by Vladimir Vapnik and his team at AT&T Bell Laboratories <ref> [3, 6, 8, 24] </ref> and known as Support Vector Machine (SVM). <p> This situation causes a lot of redundancy in most cases, and 30 can be solved by relaxing the constraint of using data points to define the decision surface. This is a topic of current research conducted by Burges <ref> [6] </ref> at Bell Laboratories, and it is of great interest for us, because in order to simplify the set of support vectors, one needs to solve highly-nonlinear constrained optimization problems. <p> Since a closed form solution exists for the case of kernel functions that are 2nd. degree polynomials, we are using a simplified SVM <ref> [6] </ref> in our current experimental face detection system that gains an acceleration factor of 20, without degrading the quality of the classifications. 2. Detection of other objects: We are interested in using SVM's to detect other objects in digital images, like cars, airplanes, pedestrians, etc.
Reference: [7] <author> T. Carpenter and D. Shanno. </author> <title> An interior point method for quadratic programs based on conjugate projected gradients. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2(1) </volume> <pages> 5-28, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Having as a reference the experience obtained with MINOS 5.4, new approaches to a tailored solver through, for example, projected Newton [2] or interior point methods <ref> [7] </ref>, should be attempted. At this point it is not clear whether the same type of algorithm is appropriate for all stages of the solution process.
Reference: [8] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 1-25, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction In this report we address the problem of implementing a new pattern classification technique recently developed by Vladimir Vapnik and his team at AT&T Bell Laboratories <ref> [3, 6, 8, 24] </ref> and known as Support Vector Machine (SVM). <p> Other monotonic convex functions of the errors can be defined (see <ref> [8] </ref> for the more general case). Notice that minimizing the first term in (18) amounts to minimizing the VC-dimension of the learning machine, thereby minimizing the second term in the bound (1). <p> The Support Vector Machine is a very new technique, and, as far as we know, this paper presents the second problem-solving application to use SVM, after Vapnik et al. <ref> [5, 8, 24] </ref> used it in the character recognition problem, in 1995. Our Face Detection System performs as well as other state-of-the-art systems, and has opened many interesting questions and possible future extensions.
Reference: [9] <author> F. Girosi. </author> <title> An equivalence between sparse approximation and Support Vector Machines. A.I. </title> <type> Memo 1606, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1997. </year> <note> (available at the URL: http://www.ai.mit.edu/people/girosi/svm.html). </note>
Reference: [10] <author> T. Hastie, A. Buja, and R. Tibshirani. </author> <title> Penalized discriminant analysis. </title> <type> Technical report, </type> <institution> Statistics and Data Analysis Research Department, AT&T Bell Laboratories, </institution> <address> Murray Hill, New Jersey, </address> <month> May </month> <year> 1994. </year>
Reference: [11] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminat adaptative nearest neighbor classification. </title> <type> Technical report, </type> <institution> Department of Statistics and Division of Biostatistics, Stanford University, </institution> <month> December </month> <year> 1994. </year>
Reference: [12] <author> N. Kruger, M. Potzsch, and C. v.d. Malsburg. </author> <title> Determination of face position and pose with learned representation based on labled graphs. </title> <type> Technical Report 96-03, </type> <institution> Ruhr-Universitat, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: This techniques include Neural Networks [4] [19], detection of face features and use of geometrical constraints [27], density estimation of the training data [14], labeled graphs <ref> [12] </ref> and clustering and distribution-based modeling [22][21]. Out of all these previous works, the results of Sung and Poggio [22][21], and Rowley et al. [19] reflect systems with very high detection rates and low false positive detection rates.
Reference: [13] <author> J. Mercer. </author> <title> Functions of positive and negative type and their connection with the theory of integral equations. </title> <journal> Philos. Trans. Roy. Soc. London, </journal> <volume> A 209 </volume> <pages> 415-446, </pages> <year> 1909. </year>
Reference-contexts: will assume that [a; b] d The kernel K defines an integral operator that is known to have a complete system of orthonormal eigenfunctions: Z dy K (x; y) n (y) = n n (x) (42) In 1976, Stewart [20] reported that, according to a theorem of Mercer from 1909 <ref> [13] </ref> the following state ments are equivalent: 1. The function K (x; y) is a positive definite kernel; 2. Z dxdy K (x; y)g (x)g (y) 0 8g 2 C ([a; b] d ) 3. The eigenvalues n in eq. (42) are all positive; 4.
Reference: [14] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <type> Technical Report 326, </type> <institution> MIT Media Laboratory, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: This techniques include Neural Networks [4] [19], detection of face features and use of geometrical constraints [27], density estimation of the training data <ref> [14] </ref>, labeled graphs [12] and clustering and distribution-based modeling [22][21]. Out of all these previous works, the results of Sung and Poggio [22][21], and Rowley et al. [19] reflect systems with very high detection rates and low false positive detection rates.
Reference: [15] <author> E.H. Moore. </author> <title> On properly positive Hermitian matrices. </title> <journal> Bull. Amer. Math. Soc., </journal> <volume> 23(59) </volume> <pages> 66-67, </pages> <year> 1916. </year>
Reference-contexts: In fact, in 1916 Moore <ref> [15] </ref> considers a more general setting for positive definite kernels, and replaces in eq. (41) with any abstract set E. He calls these functions positive Hermitian matrices and shows that to any such K one can associate a RKHS. In table (1) we report some commonly used kernels.
Reference: [16] <author> J. More and G. Toraldo. </author> <title> On the solution of large quadratic programming problems with bound constraints. </title> <journal> SIAM J. Optimization, </journal> <volume> 1(1) </volume> <pages> 93-113, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The next approach taken by Vapnik et al. [5], was to adapt to this problem the algorithm for bounded large-scale quadratic programs due to More and Toraldo <ref> [16] </ref>. Originally, this algorithm uses conjugate gradient to explore the face of the feasible region defined by the current iterate, and gradient projection to move to a different face.
Reference: [17] <author> B. Murtagh and M. Saunders. </author> <title> Large-scale linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 41-72, </pages> <year> 1978. </year>
Reference-contexts: This was enough reason to use it as the formulation when using MINOS 5.4. MINOS 5.4: MINOS 5.4 solves nonlinear problems with linear constraints using Wolfe's Reduced Gradient algorithm in conjunction with Davidson's quasi-Newton method. Details of its implementation are described by Murtagh and Saunders in <ref> [17] </ref>, and MINOS 5.4 User's Guide [18] and Bazaraa et al. [1] present an overview with some heuristics and comparisons. Wolfe's Reduced Gradient method depends upon reducing the dimensionality of the problem by representing all variables in terms of an independent set of them. <p> The variables x s are called superbasic variables, and are intended to be the driving force of the iterates while x N 0 is fixed and x B is adjusted to maintain feasibility <ref> [17] </ref>. <p> MINOS 5.4 implements (56) with certain computational highlights <ref> [17] </ref>: 1. During the algorithm, if it appears that no more improvement can be made with the current partition [B; S; N 0 ], that is, kr S k &lt; ", for a suitably chosen tolerance level ", some of the non-basic variables are added to the superbasics set.
Reference: [18] <author> B. Murtagh and M. Saunders. </author> <note> MINOS 5.4 User's Guide. System Optimization Laboratory,Stanford University, </note> <month> Feb </month> <year> 1995. </year>
Reference-contexts: MINOS 5.4: MINOS 5.4 solves nonlinear problems with linear constraints using Wolfe's Reduced Gradient algorithm in conjunction with Davidson's quasi-Newton method. Details of its implementation are described by Murtagh and Saunders in [17], and MINOS 5.4 User's Guide <ref> [18] </ref> and Bazaraa et al. [1] present an overview with some heuristics and comparisons. Wolfe's Reduced Gradient method depends upon reducing the dimensionality of the problem by representing all variables in terms of an independent set of them.
Reference: [19] <author> H. Rowley, S. Baluja, and T. Kanade. </author> <title> Human face detection in visual scenes. </title> <type> Technical Report CMU-CS-95-158R, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year> <month> 32 </month>
Reference-contexts: This techniques include Neural Networks [4] <ref> [19] </ref>, detection of face features and use of geometrical constraints [27], density estimation of the training data [14], labeled graphs [12] and clustering and distribution-based modeling [22][21]. Out of all these previous works, the results of Sung and Poggio [22][21], and Rowley et al. [19] reflect systems with very high detection <p> This techniques include Neural Networks [4] <ref> [19] </ref>, detection of face features and use of geometrical constraints [27], density estimation of the training data [14], labeled graphs [12] and clustering and distribution-based modeling [22][21]. Out of all these previous works, the results of Sung and Poggio [22][21], and Rowley et al. [19] reflect systems with very high detection rates and low false positive detection rates. Sung and Poggio use clustering and distance metrics to model the distribution of the face and non-face manifold, and a Neural Network to classify a new pattern given the measurements. <p> Use of multiple classifiers: The use of multiple classifiers offers possibilities that can be faster and/or more accurate. Rowley et al. <ref> [19] </ref> have successfully combined the output from different neural networks by means of different schemes of arbitration in the face detection problem. Sung and Poggio [22][21] use a first classifier that is very fast as a way to quickly discard patterns that are clearly non-faces.
Reference: [20] <author> J. Stewart. </author> <title> Positive definite functions and generalizations, an historical survey. </title> <journal> Rocky Mountain J. Math., </journal> <volume> 6 </volume> <pages> 409-434, </pages> <year> 1976. </year>
Reference-contexts: i ; c j 2 R (41) In the following, we will assume that [a; b] d The kernel K defines an integral operator that is known to have a complete system of orthonormal eigenfunctions: Z dy K (x; y) n (y) = n n (x) (42) In 1976, Stewart <ref> [20] </ref> reported that, according to a theorem of Mercer from 1909 [13] the following state ments are equivalent: 1. The function K (x; y) is a positive definite kernel; 2. Z dxdy K (x; y)g (x)g (y) 0 8g 2 C ([a; b] d ) 3.
Reference: [21] <author> K. Sung. </author> <title> Learning and Example Selection for Object and Pattern Detection. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory and Center for Bilogical and Computational Learning, </institution> <month> December </month> <year> 1995. </year>
Reference: [22] <author> K. Sung and T. Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <journal> A.I. </journal> <note> MEMO 1521, C.B.C.L Paper 112, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: Images of landscapes, trees, buildings, rocks, etc., are good sources of false positives due to the many different textured patterns they contain. This bootstrapping step, which was successfully used by Sung and Poggio <ref> [22] </ref> is very important in the context of a face detector that learns from examples because: * Although negative examples are abundant, negative examples that are useful from a learning point of view are very difficult to characterize and define. * By approaching the problem of object detection, and in this
Reference: [23] <author> V. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: If convergence of the minimum of R emp to the minimum of R does not hold, the Empirical Risk Minimization Principle does not allow us to make any inference based on the data set, and it is therefore said to be not consistent. As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension <p> As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension h of the hypothesis space H. <p> The bound (1) suggests that the Empirical Risk Minimization Principle can be replaced by a better induction principle, as we will see in the next section. 3 2.2 Structural Risk Minimization The technique of Structural Risk Minimization developed by Vapnik <ref> [23] </ref> is an attempt to overcome the problem of choosing an appropriate VC-dimension. It is clear from eq. (1) that a small value of the empirical risk does not necessarily imply a small value of the expected risk. <p> It is clear from eq. (1) that a small value of the empirical risk does not necessarily imply a small value of the expected risk. A different induction principle, called the Structural Risk Minimization Principle, has been proposed by Vapnik <ref> [23] </ref>. The principle is based on the observation that, in order to make the expected risk small, both sides in equation (1) should be small. Therefore, both the VC-dimension and the empirical risk should be minimized at the same time. <p> Vapnik shows that, if we assume that all the points x 1 ; : : : ; x ` lie in the unit N -dimensional sphere, the set ff w;b = sign (w x + b) j kwk Ag (7) has a VC-dimension h that satisfies the following bound [24] <ref> [23] </ref>: h minfdA 2 e; N g + 1 (8) If the data points lie inside a sphere of radius R, then (8) becomes h minfdR 2 A 2 e; N g + 1.
Reference: [24] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction In this report we address the problem of implementing a new pattern classification technique recently developed by Vladimir Vapnik and his team at AT&T Bell Laboratories <ref> [3, 6, 8, 24] </ref> and known as Support Vector Machine (SVM). <p> In the next section we discuss this technique in detail, and show how its implementation is related to quadratic programming. 2.3 Support Vector Machines: Mathematical Derivation In this section we describe the mathematical derivation of the Support Vector Machine (SVM) developed by Vapnik <ref> [24] </ref>. <p> If no further constraints are imposed on the pair (w; b) the VC-dimension of the Canonical Hyperplanes is N + 1 <ref> [24] </ref>, that is, the total number of free parameters. In order to be able to apply the Structural Risk Minimization Principle we need to construct sets of hyperplanes of varying VC-dimension, and minimize both the empirical risk (the training classification error) and the VC-dimension at the same time. <p> fact, Vapnik shows that, if we assume that all the points x 1 ; : : : ; x ` lie in the unit N -dimensional sphere, the set ff w;b = sign (w x + b) j kwk Ag (7) has a VC-dimension h that satisfies the following bound <ref> [24] </ref> [23]: h minfdA 2 e; N g + 1 (8) If the data points lie inside a sphere of radius R, then (8) becomes h minfdR 2 A 2 e; N g + 1. <p> The Support Vector Machine is a very new technique, and, as far as we know, this paper presents the second problem-solving application to use SVM, after Vapnik et al. <ref> [5, 8, 24] </ref> used it in the character recognition problem, in 1995. Our Face Detection System performs as well as other state-of-the-art systems, and has opened many interesting questions and possible future extensions. <p> From the object detection point of view, our ultimate goal is to develop a general methodology that extends the results obtained with faces to handle other objects. From a broader point of view, we also consider interesting the use of the function approximation or regression extension that Vapnik <ref> [24] </ref> has done with SVM, in many different areas where Neural Networks are currently used. Finally, another important contribution of this paper is the application of OR-based techniques to domains like Statistical Learning and Artificial Intelligence.
Reference: [25] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequences of events to their probabilities. </title> <journal> Th. Prob. and its Applications, </journal> <volume> 17(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: If convergence of the minimum of R emp to the minimum of R does not hold, the Empirical Risk Minimization Principle does not allow us to make any inference based on the data set, and it is therefore said to be not consistent. As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension <p> As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension h of the hypothesis space H.
Reference: [26] <author> V.N. Vapnik and A. Ya. Chervonenkis. </author> <title> The necessary and sufficient conditions for consistency in the empirical risk minimization method. </title> <journal> Pattern Recognition and Image Analysis, </journal> <volume> 1(3) </volume> <pages> 283-305, </pages> <year> 1991. </year>
Reference-contexts: If convergence of the minimum of R emp to the minimum of R does not hold, the Empirical Risk Minimization Principle does not allow us to make any inference based on the data set, and it is therefore said to be not consistent. As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension <p> As shown by Vapnik and Chervonenkis <ref> [25, 26, 23] </ref> consistency takes place if and only if convergence in probability of R emp to R is replaced by uniform convergence in probability. Vapnik and Chervonenkis [25, 26, 23] showed that necessary and sufficient condition for consistency of the Empirical Risk Minimization Principle is the finiteness of the VC-dimension h of the hypothesis space H.
Reference: [27] <author> G. Yang and T. Huang. </author> <title> Human face detection in a complex background. </title> <journal> Pattern Recognition, </journal> <volume> 27 </volume> <pages> 53-63, </pages> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks [4] [19], detection of face features and use of geometrical constraints <ref> [27] </ref>, density estimation of the training data [14], labeled graphs [12] and clustering and distribution-based modeling [22][21]. Out of all these previous works, the results of Sung and Poggio [22][21], and Rowley et al. [19] reflect systems with very high detection rates and low false positive detection rates.
Reference: [28] <author> W.Y. Young. </author> <title> A note on a class of symmetric functions and on a theorem required in the theory of integral equations. </title> <journal> Philos. Trans. Roy. Soc. London, </journal> <volume> A 209 </volume> <pages> 415-446, </pages> <year> 1909. </year>
Reference: [29] <author> G. Zoutendijk. </author> <title> Methods of Feasible Directions. </title> <editor> D. Van Norstrand, </editor> <address> Princeton, N.J., </address> <year> 1960. </year> <title> 33 later used as negative examples ( class -1 ) in the training process 34 are real support vectors obtained after training the system. Notice the small number of total support vectors and the fact that a higher proportion of them correspond to non-faces. </title> <type> 35 36 37 38 39 40 41 </type>
References-found: 29

