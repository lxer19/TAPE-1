URL: ftp://ftp.idsia.ch/pub/jieyu/nips95.ps.gz
Refering-URL: http://www.idsia.ch/reports.html
Root-URL: 
Email: Email: john@dcs.rhbnc.ac.uk  Email: jieyu@idsia.ch  
Title: Generalisation of A Class of Continuous Neural Networks  
Author: John Shawe-Taylor Jieyu Zhao 
Keyword: Category: Theory: Computational Learning Theory.  
Address: Egham, Surrey TW20 0EX, England  Corso Elvezia 36 CH-6900 Lugano, Switzerland  
Affiliation: Department of Computer Science Royal Holloway, University of London  IDSIA  
Abstract: We propose a way of using boolean circuits to perform real valued computation in a way that naturally extends their boolean functionality. The functionality of multiple fan in threshold gates in this model is shown to mimic that of a hardware implementation of continuous Neural Networks. A Vapnik-Chervonenkis dimension and sample size analysis for the system is performed giving best known sample sizes for a real valued Neural Network. Experimental results confirm the conclusion that the sample sizes required for the networks are significantly smaller that for sigmoidal networks.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Blum, M. Shub and S. Smale, </author> <title> On a theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines. </title> <journal> Bulletin of the American Mathematical Society, </journal> <note> 21 (1) (1989) 1-46. </note>
Reference-contexts: 1 Introduction Recent developments in complexity theory have addressed the question of complexity of computation over the real numbers <ref> [1] </ref>. More recently attempts have been made to introduce some computational cost related to the accuracy of the computations [5]. The model proposed in this paper weakens the computational power still further by relying on classical boolean circuits to perform the computation using a simple encoding of the real values. <p> Definition 1 The real function computed by a boolean circuit C, which computes the boolean function f C : f0; 1g n ! f0; 1g; is the function g C : <ref> [0; 1] </ref> n ! [0; 1]; obtained by coding each input independently as a Bernoulli sequence and interpreting the output as a similar sequence. <p> Definition 1 The real function computed by a boolean circuit C, which computes the boolean function f C : f0; 1g n ! f0; 1g; is the function g C : <ref> [0; 1] </ref> n ! [0; 1]; obtained by coding each input independently as a Bernoulli sequence and interpreting the output as a similar sequence. <p> This representation will therefore only be used for theoretical analysis. There are alternative encodings to the one we have described that can be used for representing real values, for example for x 2 <ref> [1; 1] </ref>, using a stream with probability p = 2x 1 means that multiplication can be performed by an XNOR gate. This representation will be called the bipolar Bernoulli representation. <p> It will therefore have no effect on the VC dimension and sample size analysis performed in Section 4. For this reason we consider only the <ref> [0; 1] </ref> representation for this analysis. 3 Bit Stream Neural Networks In this section we describe a neural network model based on stochastic computing and show that it corresponds to taking AC 0 circuits in the framework considered in Section 2.
Reference: [2] <author> B. Hassibi and D.G. Stork, </author> <title> Second order derivatives for network pruning: Optimal brain surgeon, </title> <booktitle> Advances in Neural Information Processing System, </booktitle> <volume> Vol 5 (1993) 164-171. </volume>
Reference-contexts: The results on MONK-3 problem is extremely good. The results reported by Hassibi and Stork <ref> [2] </ref> using a sophisticated weight pruning technique are only 93.4% correct for the training set and 97.2% correct for the testing set.
Reference: [3] <author> P. Jeavons, D.A. Cohen and J. Shawe-Taylor, </author> <title> Generating Binary Sequences for Stochastic Computing, </title> <journal> IEEE Trans on Information Theory, </journal> <note> 40 (3) (1994) 716-720. </note>
Reference-contexts: Hence, by the discussion above we have for the circuit C consisting of a single AND gate, the function g C is given by g C (x 1 ; x 2 ) = x 1 x 2 . A more interesting example is described in <ref> [3] </ref> where a circuit is given for converting a binary encoded value into a bit stream representing the corresponding value. This circuit is required for generating input and parameter streams for the Bit Stream Neural Networks. <p> Note that the 0:5 valued streams can be generated by taking taps into a large linear feedback shift register (see <ref> [3] </ref> for details). We now give a proposition showing that the definition of real computation given above is well-defined and generalises the Boolean computation performed by the circuit. Proposition 2 The bit stream on the output of a boolean circuit computing a real function is a Bernoulli sequence.
Reference: [4] <author> M. Karpinski and A. MacIntyre, </author> <title> Bounding VC-Dimension for Neural Networks: Progress and Prospects, </title> <booktitle> Proceedings of EuroCOLT'95, </booktitle> <year> 1995, </year> <pages> pp. 337-341, </pages> <booktitle> Springer Lecture Notes in Artificial Intelligence, </booktitle> <pages> 904. </pages>
Reference-contexts: The sample sizes obtained are very similar to those for threshold networks, despite their being derived by very different techniques. They give the best bounds for neural networks involving smooth activation functions, being significantly lower than the bounds obtained recently for sigmoidal networks <ref> [4, 7] </ref>. We subsequently present simulation results showing that Bit Stream Neural Networks based on the technique can be used to solve a standard benchmark problem. <p> The set G will then be the set G = G C = fg C jC 2 Cg; while F G C will be denoted by F C . We now quote some of the results of [7] which uses the techniques of Karpinski and MacIntyre <ref> [4] </ref> to derive sample sizes for classes of smoothly parametrised functions. Proposition 7 [7] Let G be the set of polynomials p of degree at most d with p : R n fi R ` ! R and Hence, there are ` adjustable parameters and the input dimension is n.
Reference: [5] <author> P. Koiran, </author> <title> A Weak Version of the Blum, Shub and Smale Model, </title> <booktitle> ESPRIT Working Group NeuroCOLT Technical Report Series, </booktitle> <address> NC-TR-94-5, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Recent developments in complexity theory have addressed the question of complexity of computation over the real numbers [1]. More recently attempts have been made to introduce some computational cost related to the accuracy of the computations <ref> [5] </ref>. The model proposed in this paper weakens the computational power still further by relying on classical boolean circuits to perform the computation using a simple encoding of the real values.
Reference: [6] <author> J. Shawe-Taylor, </author> <title> Threshold Network Learning in the Presence of Equivalances, </title> <booktitle> Proceedings of NIPS 4, </booktitle> <year> 1991, </year> <pages> pp. 879-886. </pages>
Reference-contexts: Proof : As in the proof of the previous corollary, we need only observe that the functions g C for C 2 C are polynomials of degree at most n + `. Note that the best known sample sizes for threshold networks are given in <ref> [6] </ref>: m m 0 (*; ffi) = *(1 *) 2W ln 6 N ! ffi ; where W is the number of adaptable weights (parameters) and N is the number of computational nodes in the network.
Reference: [7] <author> J. Shawe-Taylor, </author> <title> Sample Sizes for Sigmoidal Networks, </title> <booktitle> to appear in the Proceedings of Eighth Conference on Computational Learning Theory, </booktitle> <address> COLT'95, </address> <year> 1995. </year>
Reference-contexts: The sample sizes obtained are very similar to those for threshold networks, despite their being derived by very different techniques. They give the best bounds for neural networks involving smooth activation functions, being significantly lower than the bounds obtained recently for sigmoidal networks <ref> [4, 7] </ref>. We subsequently present simulation results showing that Bit Stream Neural Networks based on the technique can be used to solve a standard benchmark problem. <p> The set G will then be the set G = G C = fg C jC 2 Cg; while F G C will be denoted by F C . We now quote some of the results of <ref> [7] </ref> which uses the techniques of Karpinski and MacIntyre [4] to derive sample sizes for classes of smoothly parametrised functions. Proposition 7 [7] Let G be the set of polynomials p of degree at most d with p : R n fi R ` ! R and Hence, there are ` <p> We now quote some of the results of <ref> [7] </ref> which uses the techniques of Karpinski and MacIntyre [4] to derive sample sizes for classes of smoothly parametrised functions. Proposition 7 [7] Let G be the set of polynomials p of degree at most d with p : R n fi R ` ! R and Hence, there are ` adjustable parameters and the input dimension is n. <p> In the case considered the number t of input connections is n + `. The result follows from the proposition. Proposition 9 <ref> [7] </ref> Let G be the set of polynomials p of degree at most d with p : R n fi R ` ! R and Hence, there are ` adjustable parameters and the input dimension is n.
Reference: [8] <author> John Shawe-Taylor, Peter Jeavons and Max van Daalen, </author> <title> "Probabilistic Bit Stream Neural Chip : Theory", </title> <journal> Connection Science, </journal> <volume> Vol 3, No 3, </volume> <year> 1991. </year>
Reference-contexts: Using this encoding we also show that AC 0 circuits interpreted in the model correspond to a Neural Network design referred to as Bit Stream Neural Networks, which have been developed for hardware implementation <ref> [8] </ref>. With the perspective afforded by the general approach considered here, we are also able to analyse the Bit Stream Neural Networks (or indeed any other adaptive system based on the technique), giving VC dimension and sample size bounds for PAC learning.
References-found: 8

