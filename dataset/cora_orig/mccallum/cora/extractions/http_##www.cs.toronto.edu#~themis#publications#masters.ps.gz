URL: http://www.cs.toronto.edu/~themis/publications/masters.ps.gz
Refering-URL: http://www.cs.toronto.edu/~themis/publicationsp.html
Root-URL: 
Title: Web Prefetching Using Partial Match Prediction  
Author: by Themistoklis Palpanas 
Degree: A thesis submitted in conformity with the requirements for the degree of Master of Science  
Note: c Copyright by Themistoklis Palpanas 1998  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [Ama98] <editor> Amazon.com. </editor> <address> http://www.amazon.com, March 1998. </address>
Reference-contexts: Nowadays, apart from researchers and educational institutions, commercial organizations, public and private industries use the Web to disseminate information, advertise products, provide support, and deliver services. There already exist companies <ref> [Ama98] </ref> that rely solely on the Web for interacting with their customer base, and it seems that more and more companies will adopt the same commerce paradigm in the near future.
Reference: [Apa98] <institution> Apache HTTP Server Project. </institution> <address> http://www.apache.org, March 1998. </address>
Reference-contexts: status of the served request (i.e., whether it succeeded or not, and why) Response Header Fields: contains further information about the server and the re quested resource Entity: the actual file requested (if any), and some meta-information pertaining to it The site is supported by the Apache HTTP Server v1.2 <ref> [Apa98] </ref> which logs the requests using the Common Log Format (CLF). The CLF is a standardized way of arranging the information in the log file. Each line in the file represents a different request, and is composed of several tokens separated by spaces.
Reference: [AW97] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Internet Web Servers: Workload Characterization and Performance Implications. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 5(5) </volume> <pages> 631-645, </pages> <year> 1997. </year>
Reference-contexts: Web traffic is usually characterized by bursts of numerous, but small requests <ref> [AW97, WAS + 96] </ref>. The frequent need for opening new connections and tearing them down, which imposes a significant overhead, and the slow-start algorithm that TCP implements for congestion avoidance [Jac88], are not well-suited for the short-lived connections required for the Web. <p> Prefetching can only be advantageous when clients navigate in a site with a purpose, i.e., not aimlessly hopping from one page to another, and when the overall access patterns in servers do not vary extremely. Yet, studies <ref> [AW97, DFKM97] </ref> indicate that Web documents tend to change frequently. Still, many of the documents residing in the same site are accessed only once. Moreover, users do not spend much time in each site, and may actually leave immediately after the first access.
Reference: [BAD + 92] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: contribute to these latencies, such as: * server configuration * server load * client configuration 1 * document to be transferred * network characteristics The work in the direction of reducing page retrieval delays originated from relevant research in operating systems, where the aim is to reduce file system latencies <ref> [BAD + 92, KTP + 96] </ref>. Caching is a technique that is already being used in the Web domain. The cache, which is realized in local disk storage, is used for storing Web pages that were recently accessed. <p> Previous work in both communities indicates the need to assist the caching technique with other methods in order to achieve better performance, with the emphasis given to prefetching. 2.1 Operating Systems Context 2.1.1 Caching Caching has been used successfully for many years, yielding significant improvements in I/O latency <ref> [BAD + 92] </ref>. It tries to reduce time latencies induced by the file system by having the requested file pages available in memory. Yet, the benefits from exploiting a cache are limited.
Reference: [BB92] <author> Suzanne Bunton and Gaetano Borrielo. </author> <title> Practical Dictionary Management for Hardware Data Compression. </title> <journal> Communications of the ACM, </journal> <volume> 35(1) </volume> <pages> 95-104, </pages> <year> 1992. </year>
Reference-contexts: Memory Requirements. The extremely large size of the alphabet and its dynamic nature lead to one more concern, the amount of memory required to store the model. This study does not address the memory issue, yet, several techniques in the literature propose schemes for limiting the data structure size <ref> [Sto88, BB92] </ref>. These techniques specify an upper bound M for the amount of memory the model may consume.
Reference: [BBLS91] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Centre, </institution> <year> 1991. </year>
Reference-contexts: The compiler can add instructions in the code for prefetching pages that will soon be accessed, as well as for evicting pages that will no longer be useful, thus, enabling other applications to use the freed space. This approach was tested against the NAS parallel benchmark suite <ref> [BBLS91] </ref>, and resulted in execution speed-ups of twofold and in one case threefold. However, the experiments were performed on a system with multiple disks, which significantly increased the available I/O bandwidth. The effects of varying file system bandwidth were not tested.
Reference: [BCW90] <author> T. C. Bell, J. C. Cleary, and I. H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Research in the compression community has shown that the algorithms employing context modeling (for general purpose compression) tend to achieve superior performance <ref> [BCW90] </ref>. Context models comprise a family of techniques that use the preceding few characters in order to calculate the probability of the next one. As an example, consider the domain of the English language, and the string "start".
Reference: [Bes96] <author> Azer Bestavros. </author> <title> Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic and Service Time in Distributed Information Systems. </title> <booktitle> In International Conference on Data Engineering, </booktitle> <pages> pages 180-189, </pages> <address> New Orleans, LO, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The analysis of the trace indicates that 22% of the resources were accessed more than once, but they accounted for 50% of the requests. From this 50%, 13% were to resources that had been modified since the previous request. Unlike another study <ref> [Bes96] </ref>, which was performed in the educational environment, this trace suggests that the smaller and most frequently referenced resources tend to change more often. It was also observed that pages in the educational domain are modified noticeably less often than pages in other domains. <p> Apart from the computation overhead of this method, there is a space overhead, since the server needs to keep several old versions for each file. The study suggests, though, that these overheads are minimal. 2.2.2 Prefetching Bestavros proposes two server-initiated protocols <ref> [Bes96] </ref>. The first one is a hierarchical data dissemination mechanism that reduces network traffic by propagating Web documents from servers to server proxies that are closer to clients. <p> A cooperation of the client and the server is proposed by Wills and Sommers [WS97]. The trace-driven simulation suggests that depending on the client's browsing habits, client information can assist the server's suggestions for prefetching. The server uses the prediction model presented by Bestavros <ref> [Bes96] </ref>. A recent study attempts to determine bounds in the performance of proxy caching and prefetching [KLM97]. The authors performed a trace-driven simulation with infinite-size cache and complete knowledge of future requests. The external latency between the proxy cache and the server accounts for 77% of the total latency.
Reference: [BLCL + 94] <author> T. Berners-Lee, R. Cailliae, A. Luotonen, H. F. Nielsen, and A. </author> <title> Secret. The world wide web. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 76-82, </pages> <year> 1994. </year> <month> 61 </month>
Reference-contexts: Introduction Web <ref> [BLCL + 94] </ref> usage has proliferated during the last years. It is being used for entertainment, commercial, or educational purposes by millions of people around the world, and for some of them it has become an indispensable tool for their work.
Reference: [BLFF95] <author> T. Berners-Lee, R. Fielding, and H. Frystyk. </author> <title> Hypertext Transfer Protocol - HTTP/1.0. RFC 1945, Internet request for comments, </title> <year> 1995. </year>
Reference-contexts: The issues discussed here provide clarifications and serve as a motivation for the rest of this work. 1.1.1 HTTP/1.0 The original version of the protocol, HTTP/1.0 <ref> [BLFF95] </ref>, is still the most widely employed in the Web. In HTTP, clients are sending requests concerning a specific resource to servers, and servers are responding to these requests. HTTP is layered over the Transmission Control Protocol (TCP) [Pos81], thus, inheriting some of the latter's characteristics.
Reference: [CB98] <author> Mark Crovella and Paul Barford. </author> <title> The Network Effects of Prefetching. </title> <booktitle> In IEEE Infocom, </booktitle> <address> San Francisco, CA, USA, </address> <year> 1998. </year>
Reference-contexts: The study, however, does not comment on the number of single accesses to servers that inherently cannot be prefetched. Crovella and Barford <ref> [CB98] </ref> discuss the effects of prefetching on the network. A trace driven simulation indicates that straightforward approaches to prefetching increase the burstiness of traffic. <p> Yet, a recent study <ref> [CB98] </ref> suggests that prefetching in the Web (with performance levels similar to those presented in this work) not only reduces time latencies, but also has the potential to improve the network traffic properties.
Reference: [CKV93] <author> Kenneth M. Curewitz, P. Krishnan, and Jeffrey Scott Vitter. </author> <title> Practical Prefetch-ing via Data Compression. </title> <booktitle> In ACM SIGMOD International Conference, </booktitle> <pages> pages 257-266, </pages> <address> Washington, DC, USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This is due in part to the overhead prefetching imposes on the system. The dominant factor, though, is the ratio of the read complete time to the total execution time. Curewitz et al. <ref> [CKV93] </ref> claim that data compression techniques can be successfully used for prefetching. An implementation of this idea, based on Prediction by Partial Match (PPM), is presented by Kroeger and Long [KL96]. <p> Therefore, determining a model that can effectively capture the real world is of great importance. Such models, that may be used to drive a prefetching system, can be found in the compression literature. Previous work <ref> [CKV93] </ref> indicates that the idea of applying techniques used in compression algorithms for making predictions is fruitful. <p> Thus, if a compressor can effectively compress a data sequence that means the model it is 17 using accurately describes the data, and can be used for making predictions. The idea of applying prediction techniques used in the compression context for doing prefetching was explored by Curewitz et al. <ref> [CKV93] </ref>. The results indicate that not only is it feasible, but also that the performance is remarkable. The simulation experiments use traces derived from Computer Aided Design applications, and from a set of benchmarks.
Reference: [Con98] <institution> Connectix Corp. </institution> <address> http://www.connectix.com, March 1998. </address>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [CPY96] <author> Ming-Syan Chen, Jong-Soo Park, and Philip S. Yu. </author> <title> Efficient Data Mining for Path Traversal Patterns. </title> <booktitle> In International Conference on Distributed Computing Systems, </booktitle> <pages> pages 385-392, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Note that Web mining is also used for extracting interesting content information from Web pages. However this particular aspect is not the focus of our work. The problem of mining path traversal patterns in the Web is explored by Chen et al. <ref> [CPY96] </ref>. The interesting point in the proposed algorithm is that it differentiates between forward and backward (to objects already visited) traversals. Backward traversals are ignored, since they are considered to merely indicate that the user wants to reach a sibling page. <p> Two problems arise here. The distinction among several users of the same machine (who appear as one entity in the Web server's access log file), and the grouping of their requests in transactions. An interesting suggestion for the latter problem is given by Chen et al. <ref> [CPY96] </ref>. The authors propose the use of maximal forward references, which is a way of filtering out the effect of some backward references. Prediction Engine. The prefetcher can be extended to accommodate the need to make predictions further in the future (i.e., several steps ahead of the user requests).
Reference: [CW84] <author> John G. Cleary and Ian H. Witten. </author> <title> Data Compression Using Adaptive Coding and Partial String Matching. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 32(4) </volume> <pages> 396-402, </pages> <year> 1984. </year>
Reference-contexts: In both situations the performance is rather poor. In order to avoid the aforementioned problems, blending can be used. This is a strategy that allows the predictions of contexts of different lengths to be combined into a single probability. 3.2.1 Prediction by Partial Match Prediction by Partial Match (PPM) <ref> [CW84] </ref> is one of the context models that have been proposed in the literature. A PPM compressor uses multiple high-order Markov models to describe the dataset. That means it keeps track of the probability of a symbol occurring, given that a specific sequence of symbols has already been seen.
Reference: [Dep98] <institution> Department of Computer Science, University of Toronto. </institution> <address> http://www.cs.toronto.edu, March 1998. </address>
Reference-contexts: Each one of those is further discussed in the following paragraphs. 4.1.1 The Log File This is the file logging information about user accesses to the Web server of the Computer Science Department of the University of Toronto <ref> [Dep98] </ref>, as well as the actions that the server took in response to the requests.
Reference: [DFKM97] <author> Fred Douglis, Anja Feldmann, Balachander Krishnamurthy, and Jeffrey C. Mogul. </author> <title> Rate of Change and other Metrics: a Live Study of the World Wide Web. </title> <booktitle> In USENIX Symposium on Internet Technology and Systems, </booktitle> <pages> pages 147-158, </pages> <address> Berkeley, CA, USA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: However, time latencies remain significant despite these improvements. Ideally, we would like all the requested documents to be already in the cache at the time of their request. In this case, the delays would be minimal and rather negligible. Unfortunately, recent studies <ref> [KLM97, DFKM97] </ref> indicate that the benefits from caching are quite limited. The vast number of documents available in the Web, the quick rate of their change, and the diverse needs across users and over time, are the main factors that cause Web caching to perform poorly. <p> Prefetching can only be advantageous when clients navigate in a site with a purpose, i.e., not aimlessly hopping from one page to another, and when the overall access patterns in servers do not vary extremely. Yet, studies <ref> [AW97, DFKM97] </ref> indicate that Web documents tend to change frequently. Still, many of the documents residing in the same site are accessed only once. Moreover, users do not spend much time in each site, and may actually leave immediately after the first access. <p> Indeed, it is only recently that the peculiarities of the Web domain started to become apparent <ref> [WAS + 96, Mar96, DFKM97] </ref>. The characteristics of Web resources are explored by Douglis et al. [DFKM97]. The trace used in the study showed that 69% of the accesses and 60% of the bytes transfered involved images. The same figures for HTML documents are 20% and 21% respectively. <p> Indeed, it is only recently that the peculiarities of the Web domain started to become apparent [WAS + 96, Mar96, DFKM97]. The characteristics of Web resources are explored by Douglis et al. <ref> [DFKM97] </ref>. The trace used in the study showed that 69% of the accesses and 60% of the bytes transfered involved images. The same figures for HTML documents are 20% and 21% respectively.
Reference: [FGM + 97] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. </author> <title> Hypertext Transfer Protocol - HTTP/1.1. RFC 2068, Internet request for comments, </title> <year> 1997. </year>
Reference-contexts: The frequent need for opening new connections and tearing them down, which imposes a significant overhead, and the slow-start algorithm that TCP implements for congestion avoidance [Jac88], are not well-suited for the short-lived connections required for the Web. The HTTP/1.1 protocol <ref> [FGM + 97] </ref> tries to solve the aforementioned problems. It uses long-lived TCP connections (persistent connections ) to serve multiple HTTP requests. This allows a client to make several requests in a short amount of time, using the same connection.
Reference: [Fos98] <author> Cormac Foster. </author> <title> The Price of Speed. http://www.cnet.com/ Content/ Reviews/ Compare/ Accelerator, </title> <month> March </month> <year> 1998. </year>
Reference-contexts: However, the algorithms used are not sophisticated. They merely exploit the idle time of the client's network connection to prefetch all the links of the current page, or the user's popular pages. These products diminish browsing latencies <ref> [Fos98] </ref>, yet, their simplistic approach results in a considerable increase in network traffic. 15 16 Chapter 3 Prediction Model In order to do successful prefetching, a model that describes the users' request patterns is essential. Based on this model, the system can make predictions about future requests.
Reference: [GA96] <author> James Griffioen and Randy Appleton. </author> <title> The Design, Implementation, and Evaluation of a Predictive Caching File System. </title> <type> Technical Report CS-264-96, </type> <institution> Department of Computer Science, University of Kentucky, </institution> <month> June </month> <year> 1996. </year> <month> 62 </month>
Reference-contexts: As a consequence, in contrast to the previous methods, this one makes predictions without using any application specific knowledge at all. Therefore any application can benefit without having to be rewritten, but predictions are in general fewer or less accurate. Griffioen and Appleton <ref> [GA96] </ref> propose a predictive cache which prefetches file pages based on a probability graph of past accesses. <p> The trace-driven simulation reveals that a slight increase in network traffic yields significant reductions on server load, service time, and client cache miss rate. These results however, were obtained by allowing a document to predict the request of its images, thus distorting the figures. Previous work on operating systems <ref> [GA96] </ref> has been the basis of Padmanabhan and Mogul's research [PM96] on prediction of future requests. A trace-driven simulation along with a linear model for the network is used to test the algorithm. The results indicate that the benefits from prefetching are significant, even for slow (i.e., modem) connections.
Reference: [Jac88] <author> Van Jacobson. </author> <title> Congestion Avoidance and Control. </title> <booktitle> In ACM SIGCOMM Confer--ence, </booktitle> <pages> pages 314-329, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Web traffic is usually characterized by bursts of numerous, but small requests [AW97, WAS + 96]. The frequent need for opening new connections and tearing them down, which imposes a significant overhead, and the slow-start algorithm that TCP implements for congestion avoidance <ref> [Jac88] </ref>, are not well-suited for the short-lived connections required for the Web. The HTTP/1.1 protocol [FGM + 97] tries to solve the aforementioned problems. It uses long-lived TCP connections (persistent connections ) to serve multiple HTTP requests.
Reference: [KL96] <author> Thomas M. Kroeger and Darell D. E. </author> <title> Long. Predicting File System Actions from Prior Events. </title> <booktitle> In Winter USENIX Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Curewitz et al. [CKV93] claim that data compression techniques can be successfully used for prefetching. An implementation of this idea, based on Prediction by Partial Match (PPM), is presented by Kroeger and Long <ref> [KL96] </ref>. The advantage of this method is that the prediction of future requests depends on the k previous accesses, thus resulting in better predictions. The major drawback is the algorithm's memory requirements. In the trace-driven simulation, cache-hits increase by an average of 25% over a non-prefetching cache.
Reference: [KLM97] <author> Thomas M. Kroeger, Darrell D. E. Long, and Jeffrey C. Mogul. </author> <title> Exploring the Bounds of Web Latency Reduction from Caching and Prefetching. </title> <booktitle> In USENIX Symposium on Internet Technologies and Systems, </booktitle> <pages> pages 319-328, </pages> <address> San Diego, CA, USA, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: However, time latencies remain significant despite these improvements. Ideally, we would like all the requested documents to be already in the cache at the time of their request. In this case, the delays would be minimal and rather negligible. Unfortunately, recent studies <ref> [KLM97, DFKM97] </ref> indicate that the benefits from caching are quite limited. The vast number of documents available in the Web, the quick rate of their change, and the diverse needs across users and over time, are the main factors that cause Web caching to perform poorly. <p> The trace-driven simulation suggests that depending on the client's browsing habits, client information can assist the server's suggestions for prefetching. The server uses the prediction model presented by Bestavros [Bes96]. A recent study attempts to determine bounds in the performance of proxy caching and prefetching <ref> [KLM97] </ref>. The authors performed a trace-driven simulation with infinite-size cache and complete knowledge of future requests. The external latency between the proxy cache and the server accounts for 77% of the total latency. Caching achieved a reduction of 26%, prefetching 57%, and the combination of both 60%.
Reference: [Knu73] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching, volume 3. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: An order-m prefetcher maintains m+1 Markov predictors 1 which correspond to contexts of length 0 to m. One way of describing this model is by using a trie <ref> [Knu73] </ref>.
Reference: [KTP + 96] <author> Tracy Kimbrel, Andrew Tomkins, R. Hugo Patterson, Brian Bershad, Pei Cao, Edward W. Felten, Garth A. Gibson, Anna R. Karlin, and Kai Li. </author> <title> A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching. </title> <booktitle> In USENIX Association Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 19-34, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: contribute to these latencies, such as: * server configuration * server load * client configuration 1 * document to be transferred * network characteristics The work in the direction of reducing page retrieval delays originated from relevant research in operating systems, where the aim is to reduce file system latencies <ref> [BAD + 92, KTP + 96] </ref>. Caching is a technique that is already being used in the Web domain. The cache, which is realized in local disk storage, is used for storing Web pages that were recently accessed. <p> The effects of varying file system bandwidth were not tested. In addition, the scope of that work is currently limited to numeric applications, so it is expected to be beneficial only in highly restricted settings. A recent study <ref> [KTP + 96] </ref> has tried to explore the effectiveness of combined caching and prefetching techniques using a trace-driven simulation.
Reference: [KW97a] <author> Achim Kraiss and Gerhard Weikum. </author> <title> Vertical Data Migration in Large Near-Line Document Archives Based on Markov-Chain Predictions. </title> <booktitle> In VLDB International Conference, </booktitle> <pages> pages 246-255, </pages> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: The trace-driven simulation shows a substantial reduction in applications latency, despite the significant CPU overhead. However, the trace used was not based on a real workload. The problem of predictive caching emerges also in near-line tertiary storage libraries <ref> [KW97a] </ref>, although in this case time scales are different. The decision to be made is which large multimedia files should be prefetched from slow tertiary storage to fast disks. A continuous-time Markov-chain model is used to predict what the future accesses will be within time t.
Reference: [KW97b] <author> Balachander Krishnamurthy and Craig E. Wills. </author> <title> Study of Piggyback Cache Validation for Proxy Caches in the World Wide Web. </title> <booktitle> In USENIX Symposium on Internet Technology and Systems, </booktitle> <pages> pages 1-12, </pages> <address> Berkeley, CA, USA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: However, no figures are provided for the modification frequency of the pages. Note that a high frequency would result in poorer performance. Additional, more subtle parameters of the proxy cache operation are addressed in other studies. Krishnamurthy and Wills <ref> [KW97b] </ref> explore the cache coherency problem, i.e., assuring the validity of cached resources, and present a new technique, the Piggyback Cache Validation (PCV).
Reference: [LD97] <author> Hui Lei and Dan Duchamp. </author> <title> An Analytical Approach to File Prefetching. </title> <booktitle> In USENIX Annual Technical Conference, </booktitle> <pages> pages 275-288, </pages> <address> Berkeley, CA, USA, </address> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: The simulation shows that on average a 4MB predictive cache has a higher hit rate than a 9 90MB simple Least Recently Used (LRU) cache. Results on time latencies are not reported in this paper. Lei and Duchamp <ref> [LD97] </ref> employ access trees to make predictions. Access trees record all files accessed during one execution of each program. When an application is re-executed, the current activity is compared against the saved access trees. If a similarity is detected, the files in the access tree are prefetched.
Reference: [Lie95] <author> Henry Lieberman. Letizia: </author> <title> An Agent that Assists Web Browsing. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 924-929, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Therefore, considering the path names of Web pages may assist the prefetcher in making more accurate predictions. Moreover, a method for constraining the possible predictions is to consider only those pages which are referenced by the currently requested page. This technique is employed by the Letizia system <ref> [Lie95] </ref>, which is a Web agent for proposing interesting pages to the user. 58 General Enhancements to the Algorithm. The common characteristic here is the adap- tiveness of the system. First, the prediction model should be able to automatically respond to changes in the user access patterns.
Reference: [LRV97] <author> Paolo Lorenzetti, Luigi Rizzo, and Lorenzo Vicisano. </author> <title> Replacement Policies for a Proxy Cache. </title> <address> http://www.iet.unipi.it/ ~luigi/ caching.ps.gz, </address> <year> 1997. </year> <month> 63 </month>
Reference-contexts: Moreover, the majority of the requests are for small documents. Nevertheless, SIZE proves to be the worst policy when byte cache hits are measured since large documents tend not to be cached. In accordance with the aforementioned results, Lorenzetti et al. <ref> [LRV97] </ref> propose a new replacement policy, Lowest Relative Value (LRV). It takes into account the size of the document, the number of past references, and the time of the last reference. Such a cost model enables the algorithm to maximize both the document and byte cache hits.
Reference: [Mar96] <author> Evangelos P. Markatos. </author> <title> Main Memory Caching of Web Documents. </title> <booktitle> In Interna--tional World Wide Web Conference, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Indeed, it is only recently that the peculiarities of the Web domain started to become apparent <ref> [WAS + 96, Mar96, DFKM97] </ref>. The characteristics of Web resources are explored by Douglis et al. [DFKM97]. The trace used in the study showed that 69% of the accesses and 60% of the bytes transfered involved images. The same figures for HTML documents are 20% and 21% respectively. <p> Trace-driven simulations show that LRV performs as well as, or better than the best policy considered so far. The simulations do not account for time latencies. A cache which resides on the server, Main Memory Web Cache, is proposed by Markatos <ref> [Mar96] </ref>. This kind of cache can substantially alleviate the server load by having the majority of the requests serviced from the memory instead of the file system.
Reference: [MDFK97] <author> Jeffrey C. Mogul, Fred Douglis, Anja Feldmann, and Balachander Krishna-murthy. </author> <title> Potential Benefits of Delta Encoding and Data Compression for HTTP. In ACM SIGCOMM Conference, </title> <address> pages 181-194, Cannes, France, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Furthermore, although the algorithm needs to keep track of several parameters, its complexity is not analyzed. The benefits from delta encoding (transmitting only the difference between two versions of the same document) and data compression for the transfer of Web documents are investigated by Mogul et al. <ref> [MDFK97] </ref>. The trace-driven simulation uses an unlimited-size cache and a filtered trace (only status-200 responses, with no images). It demonstrates that the response body-bytes saved are at least 98.5% for half of the delta-eligible responses, or 30% for all responses. Latency is reduced by 12%.
Reference: [MDK96] <author> Todd C. Mowry, Angela K. Demke, and Orran Krieger. </author> <title> Automatic Compiler-Inserted I/O Prefetching for Out-of-Core Applications. </title> <booktitle> In USENIX Association Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 3-17, </pages> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: This poses an additional difficulty in the programming task. One way to overcome the aforementioned problems is by instrumenting the compiler to provide all the necessary prefetching information to the operating system <ref> [MDK96] </ref>. The compiler can add instructions in the code for prefetching pages that will soon be accessed, as well as for evicting pages that will no longer be useful, thus, enabling other applications to use the freed space.
Reference: [MJHS96] <author> Bamshad Mobasher, Namit Jain, Eui-Hong Han, and Jaideep Srivastava. </author> <title> Web Mining: Pattern Discovery from World Wide Web Transactions. </title> <type> Technical Report TR96-050, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Backward traversals are ignored, since they are considered to merely indicate that the user wants to reach a sibling page. It is only in the forward references that useful patterns are expected to be found. Mobasher et al. <ref> [MJHS96] </ref> present a framework for Web Mining, and a system based on that framework, Webminer. Unlike the previous approach, this method does not consider all backward references to be useless, nor all forward references as meaningful. Instead, user references are clustered according to some criteria, which are primarily time constraints.
Reference: [MRG97] <author> Carlos Maltzahn, Kathy J. Richardson, and Dirk Grunwald. </author> <title> Performance Issues of Enterprise Level Web Proxies. </title> <booktitle> In ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 13-23, </pages> <address> New York, NY, USA, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Caching achieved a reduction of 26%, prefetching 57%, and the combination of both 60%. Although these figures are quite different for the second trace used, they exhibit the same trends. The weak association of proxies and latency reduction is confirmed by a real-case study of cache performance <ref> [MRG97] </ref>. The simulation also shows that the available bandwidth for prefetching has little effect on latency reduction for which the dominant factor is how far in advance of the actual need the data can be prefetched.
Reference: [NGBS + 97] <author> Henrik Frystyk Nielsen, Jim Gettys, Anselm Baird-Smith, Eric Prud'hommeaux, H-akon Wium Lie, and Chris Lilley. </author> <title> Network Performance Effects of HTTP/1.1, CSS1, </title> <booktitle> and PNG. In ACM SIGCOMM Conference, </booktitle> <year> 1997. </year>
Reference-contexts: The introduction of the HTTP/1.1 protocol is a major step in this direction. Simulations and working prototypes implementing the new protocol indicate the benefits derived from its usage <ref> [PM94, NGBS + 97] </ref>. However, time latencies remain significant despite these improvements. Ideally, we would like all the requested documents to be already in the cache at the time of their request. In this case, the delays would be minimal and rather negligible.
Reference: [OCH + 85] <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Symposium on Operating Systems, </booktitle> <pages> pages 15-24, </pages> <address> Orcas Island, WA, USA, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: It tries to reduce time latencies induced by the file system by having the requested file pages available in memory. Yet, the benefits from exploiting a cache are limited. Ousterhout et al. present <ref> [OCH + 85] </ref> a trace-driven simulation which indicates that the relative benefit of caching decreases as cache size increases. The experiments show that beyond a certain point response times cannot be substantially reduced any further, no matter how large the cache is.
Reference: [Ous90] <author> John K. Ousterhout. </author> <title> Why Aren't Operating Systems Getting Faster as Fast as Hardware? In Summer USENIX Conference, </title> <address> pages 247-256, </address> <year> 1990. </year>
Reference-contexts: Ousterhout et al. present [OCH + 85] a trace-driven simulation which indicates that the relative benefit of caching decreases as cache size increases. The experiments show that beyond a certain point response times cannot be substantially reduced any further, no matter how large the cache is. Moreover, Ousterhout <ref> [Ous90] </ref> shows that applications are still I/O bound, despite the presence of the cache.
Reference: [Par98] <institution> Parsons Technology Inc. </institution> <address> http://www.parsonstech.com, March 1998. </address> <month> 64 </month>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [Pax94] <author> V. Paxson. </author> <title> Empirically-Derived Analytic Models of Wide-Area TCP Connec--tions. </title> <journal> ACM/IEEE Transactions on Networking, </journal> <volume> 2(8) </volume> <pages> 316-336, </pages> <year> 1994. </year>
Reference-contexts: There already exist companies [Ama98] that rely solely on the Web for interacting with their customer base, and it seems that more and more companies will adopt the same commerce paradigm in the near future. Web traffic is now one of the major components of Internet traffic <ref> [Pax94] </ref>, and accounts for much of the explosive growth that Web is experiencing. Although it is a relatively new domain, it has already attracted the attention of the research community. One of the main directions of research is to reduce the time latencies users experience when navigating through Web sites.
Reference: [Pea98] <institution> PeakSoft Corp. </institution> <address> http://www.peak.com, March 1998. </address>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [PGS93] <author> R. Hugo Patterson, Garth A. Gibson, and M. Satyanarayanan. </author> <title> A Status Report on Research in Transparent Informed Prefetching. </title> <journal> SIGOPS Operating Systems Review, </journal> <volume> 27(2) </volume> <pages> 21-34, </pages> <year> 1993. </year>
Reference-contexts: Prefetching is a concept that helps in that direction. It enables the cache to satisfy more requests by predicting sufficiently ahead of time what these requests will be. A naive method for doing prefetching is to have the application inform the operating system of its future requirements <ref> [PGS93] </ref>. When this method works, it is very efficient since the application knows exactly which files it will need and at which point in the future. However, the problems this approach has to overcome are rather significant. First of all, applications must be modified to use this technique.
Reference: [Pit97] <author> James Pitkow. </author> <booktitle> In Search of Reliable Usage Data on the WWW. In International World Wide Web Conference, </booktitle> <pages> pages 451-463, </pages> <address> Santa Clara, CA, USA, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: It may be the case that more than one user, or even worse a community of users (i.e., when a proxy server is employed), are serviced by the same machine. Ways of addressing the problem of user identification have been proposed <ref> [Pit97] </ref>, yet, they are rather cumbersome, and have not become a common practice. Thus, it is unavoidable for the prediction model to erroneously mix some distinct browsing sessions into a single one. Client-side Caches. Both individual users' caches and proxy caches constitute sources of noise for the prediction model.
Reference: [PM94] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Improving HTTP Latency. </title> <booktitle> In International World Wide Web Conference, </booktitle> <address> Chicago, IL, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The introduction of the HTTP/1.1 protocol is a major step in this direction. Simulations and working prototypes implementing the new protocol indicate the benefits derived from its usage <ref> [PM94, NGBS + 97] </ref>. However, time latencies remain significant despite these improvements. Ideally, we would like all the requested documents to be already in the cache at the time of their request. In this case, the delays would be minimal and rather negligible.
Reference: [PM96] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency. </title> <journal> ACM SIGCOMM Computer Communication Review, </journal> <volume> 27(3) </volume> <pages> 22-36, </pages> <year> 1996. </year>
Reference-contexts: These results however, were obtained by allowing a document to predict the request of its images, thus distorting the figures. Previous work on operating systems [GA96] has been the basis of Padmanabhan and Mogul's research <ref> [PM96] </ref> on prediction of future requests. A trace-driven simulation along with a linear model for the network is used to test the algorithm. The results indicate that the benefits from prefetching are significant, even for slow (i.e., modem) connections.
Reference: [Pos81] <author> J. Postel. </author> <title> Transmission Control Protocol. </title> <type> RFC 793, </type> <institution> Network Information Center, SRI International, </institution> <year> 1981. </year>
Reference-contexts: In HTTP, clients are sending requests concerning a specific resource to servers, and servers are responding to these requests. HTTP is layered over the Transmission Control Protocol (TCP) <ref> [Pos81] </ref>, thus, inheriting some of the latter's characteristics. Figure 1.1 depicts the flow of packets over time, that are exchanged between a client and a server, for a request to be served.
Reference: [Rob98] <institution> RobSoft Inc. </institution> <address> http://www.robsoft.com, March 1998. </address>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [SF98] <author> Myra Spiliopoulou and Lukas C. Faulstich. WUM: </author> <title> A Web Utilization Miner. </title> <booktitle> In International Workshop on the Web and Databases, </booktitle> <address> Valencia, Spain, </address> <month> March </month> <year> 1998. </year>
Reference-contexts: Unlike the previous approach, this method does not consider all backward references to be useless, nor all forward references as meaningful. Instead, user references are clustered according to some criteria, which are primarily time constraints. Recently, a similar system was proposed by Spiliopoulou and Faulstich <ref> [SF98] </ref>. This tool focuses on the discovery of navigation patterns, and can be used to assist in the process of site reorganization. 2.2.4 Commercial Predictive Cache Products The interest in the direction of reducing navigation latencies in the Web is not limited to research studies.
Reference: [SSV97] <author> Peter Scheuermann, Junho Shim, and Radek Vingralek. </author> <title> A Case for Delay-Conscious Caching of Web Documents. </title> <booktitle> In International World Wide Web Conference, </booktitle> <address> Santa Clara, CA, USA, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: The algorithm uses a cost function in order to decide which documents to replace in the cache. The factors determining the cost of a document are the size, the retrieval latency, the frequency of accesses, and the frequency of validation checks for that document. Trace-driven simulations <ref> [SSV97, SSV98] </ref> indicate that the new algorithm achieves a better hit ratio than LRU, while improving the cache consistency level over the current approach (i.e., for documents whose modified status is unknown, validate with the server upon request). However, no results are reported for the byte hit ratio.
Reference: [SSV98] <author> Junho Shim, Peter Scheuermann, and Radek Vingralek. </author> <title> A Unified Algorithm for Cache Replacement and Consistency in Web Proxy Servers. </title> <booktitle> In International Workshop on the Web and Databases, </booktitle> <address> Valencia, Spain, </address> <month> March </month> <year> 1998. </year>
Reference-contexts: Furthermore, it yields the lowest costs in terms of response time, bandwidth used, and number of validation messages. By extending this technique, the server could piggyback resource invalidations back to the cache, which in turn could free some space sooner. The work of Shim et al. <ref> [SSV98] </ref> suggests a unified algorithm for cache replacement and consistency. The algorithm uses a cost function in order to decide which documents to replace in the cache. <p> The algorithm uses a cost function in order to decide which documents to replace in the cache. The factors determining the cost of a document are the size, the retrieval latency, the frequency of accesses, and the frequency of validation checks for that document. Trace-driven simulations <ref> [SSV97, SSV98] </ref> indicate that the new algorithm achieves a better hit ratio than LRU, while improving the cache consistency level over the current approach (i.e., for documents whose modified status is unknown, validate with the server upon request). However, no results are reported for the byte hit ratio.
Reference: [Sto88] <author> James A. Storer. </author> <title> Data Compression: Methods and Theory. </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year> <month> 65 </month>
Reference-contexts: Memory Requirements. The extremely large size of the alphabet and its dynamic nature lead to one more concern, the amount of memory required to store the model. This study does not address the memory issue, yet, several techniques in the literature propose schemes for limiting the data structure size <ref> [Sto88, BB92] </ref>. These techniques specify an upper bound M for the amount of memory the model may consume.
Reference: [WAS + 96] <author> Stephen Williams, Marc Abrams, Charles R. Standridge, Ghaleb Abdulla, and Edward A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In ACM SIGCOMM Conference, </booktitle> <pages> pages 293-305, </pages> <address> New York, NY, USA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Web traffic is usually characterized by bursts of numerous, but small requests <ref> [AW97, WAS + 96] </ref>. The frequent need for opening new connections and tearing them down, which imposes a significant overhead, and the slow-start algorithm that TCP implements for congestion avoidance [Jac88], are not well-suited for the short-lived connections required for the Web. <p> Indeed, it is only recently that the peculiarities of the Web domain started to become apparent <ref> [WAS + 96, Mar96, DFKM97] </ref>. The characteristics of Web resources are explored by Douglis et al. [DFKM97]. The trace used in the study showed that 69% of the accesses and 60% of the bytes transfered involved images. The same figures for HTML documents are 20% and 21% respectively. <p> It was also observed that pages in the educational domain are modified noticeably less often than pages in other domains. The above results manifest the diversity and changing nature of the Web. 10 2.2.1 Caching Williams et al. <ref> [WAS + 96] </ref> investigate the operation of proxy caches, i.e., caches "near" the clients.
Reference: [Web98a] <institution> Web3000 Inc. </institution> <address> http://www.web3000.com, March 1998. </address>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [Web98b] <institution> WebEarly Inc. </institution> <address> http://www.webearly.com, March 1998. </address>
Reference-contexts: Several commercial products, termed as Web accelerators, promise to enhance the performance of browsers <ref> [Con98, Pea98, Web98a, Rob98, Par98, Web98b] </ref>. 14 They reside in the client side, and employ prefetching in order to serve potential future requests from the local cache, and thus, minimize latencies. However, the algorithms used are not sophisticated.
Reference: [WS97] <author> Craig E. Wills and Joel Sommers. </author> <title> Prefetching on the Web Through Merger of Client and Server Profiles. </title> <note> http://www.cs.wpi.edu/ ~cew/ papers/ webprofile.ps, </note> <year> 1997. </year>
Reference-contexts: Prefetching can reduce the average latency by up to 30% with a 25% increase in network traffic. In contrast, a doubling of the bandwidth reduces latency only by 20%. A cooperation of the client and the server is proposed by Wills and Sommers <ref> [WS97] </ref>. The trace-driven simulation suggests that depending on the client's browsing habits, client information can assist the server's suggestions for prefetching. The server uses the prediction model presented by Bestavros [Bes96]. A recent study attempts to determine bounds in the performance of proxy caching and prefetching [KLM97].
References-found: 55

