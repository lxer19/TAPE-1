URL: http://www.cs.ubc.ca/spider/cebly/Papers/aij95abduct.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: email: cebly@cs.ubc.ca, becher@cs.ubc.ca  
Title: Abduction as Belief Revision  
Author: Craig Boutilier and Ver onica Becher 
Address: CANADA, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia Vancouver, British Columbia  
Date: 1995  
Note: To appear, Artificial Intelligence,  
Abstract: We propose a model of abduction based on the revision of the epistemic state of an agent. Explanations must be sufficient to induce belief in the sentence to be explained (for instance, some observation), or ensure its consistency with other beliefs, in a manner that adequately accounts for factual and hypothetical sentences. Our model will generate explanations that nonmonotonically predict an observation, thus generalizing most current accounts, which require some deductive relationship between explanation and observation. It also provides a natural preference ordering on explanations, defined in terms of normality or plausibility. To illustrate the generality of our approach, we reconstruct two of the key paradigms for model-based diagnosis, abductive and consistency-based diagnosis, within our framework. This reconstruction provides an alternative semantics for both and extends these systems to accommodate our predictive explanations and semantic preferences on explanations. It also illustrates how more general information can be incorporated in a principled manner. fl Some parts of this paper appeared in preliminary form as Abduction as Belief Revision: A Model of Preferred Explanations, Proc. of Eleventh National Conf. on Artificial Intelligence (AAAI-93), Washington, DC, pp.642-648 (1993). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ernest W. Adams. </author> <title> The Logic of Conditionals. </title> <address> D.Reidel, Dordrecht, </address> <year> 1975. </year>
Reference-contexts: These default logics are shown to be equivalent to the preferential and rational consequence operations of Lehmann [33, 34]. They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams <ref> [1] </ref> and further developed by Goldszmidt and Pearl [26], and can be given a probabilistic interpretation [7].
Reference: [2] <author> Carlos Alchourr on, Peter G ardenfors, and David Makinson. </author> <title> On the logic of theory change: Partial meet contraction and revision functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 50 </volume> <pages> 510-530, </pages> <year> 1985. </year>
Reference-contexts: We also describe the AGM model of belief revision of Alchourr on, G ardenfors and Makinson <ref> [2] </ref>; and we present the conditional logics required to capture this theory of revision, due to Boutilier [9]. <p> More troublesome is the revision of K by A when K j= :A. Some beliefs in K must be given up before A can be accommodated. The problem lies in determining which part of K to give up. Alchourr on, G ardenfors and Makinson <ref> [2] </ref> have proposed a theory of revision (the AGM theory) based on the following observation: the least entrenched beliefs in K should be given up and A added to this contracted belief set. We use K fl A to denote the belief set resulting when K is revised by A.
Reference: [3] <author> Craig Boutilier. </author> <title> Inaccessible worlds and irrelevance: Preliminary report. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 413-418, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: This ordering is taken to represent the epistemic state of an agent; thus all forms of explanation we describe can be classified as epistemic explanations. Our conditional logic, described in earlier work as a representation of belief revision and default reasoning <ref> [3, 7, 9] </ref>, has the desired nonmonotonicity and induces a natural preference ordering on sentences (hence explanations). In the next section we describe abduction, belief revision, our conditional logics and other necessary logical preliminaries. <p> We can view this as a crude pragmatic theory. Levesque [35] embeds a syntactic notion of simplicity in his semantics for abduction. In our conditional framework one can define conditions under which a proposition is deemed irrelevant to a conditional <ref> [21, 3] </ref>. Explanations can also be strengthened with background information that, while not irrelevant, can be left unstated. For instance, returning to the example given by the factual model in Figure 2, we can see that R explains W , and R ^ :C explains W as well. <p> We note that the representation of Theorist and Brewka models for a given set of defaults does not require that one specify the ordering relation for the model explicitly for each pair of worlds. One may axiomatize the model (relatively) concisely using techniques described in <ref> [3] </ref>. The truth of conditionals determining explanations and preferences can then be tested against this theory. However, we are not suggesting that our conditional framework be used as a computational basis for explanations in simple Theorist-like theories.
Reference: [4] <author> Craig Boutilier. </author> <title> Epistemic entrenchment in autoepistemic logic. </title> <note> Fundamenta Informaticae, 17(1-2):5-30, </note> <year> 1992. </year>
Reference-contexts: model be such that kKk = min (&gt;); that is, the model must have a (unique) minimal cluster formed by kKk. 4 This reflects the intuition that all and only K-worlds are most plausible for an agent with belief set K [9], and corresponds to a form of only knowing <ref> [36, 4] </ref>. The CT4O-model in Figure 1 (a) is a K-revision model for K = Cn (:A; B), while the CO-model in To revise K by A, we construct the revised set K fl A by considering the set min (A) of most plausible A-worlds in M . <p> Thus, we can equate 3 We assume, for simplicity, that such a (limiting) set exists for each ff 2 L CPL , though the following technical developments do not require this [7, 9]. 4 This constraint can be expressed in the object language L B ; see <ref> [9, 4] </ref>. To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 11 the conditional A ) B with the statement B 2 K fl A and interpret our conditional as a certain type of epistemic subjunctive conditional. <p> Taking each such subset (each element of P l (A)) to be a plausible revised state of affairs rather than their union, we can define a weaker notion of revision using the following connective. It reflects the intuition that the consequent C holds within some 5 See <ref> [4] </ref> for a more comprehensive definition of belief and a proof of correspondence to the belief logic weak S5.
Reference: [5] <author> Craig Boutilier. </author> <title> The probability of a possibility: Adding uncertainty to default rules. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in AI, </booktitle> <pages> pages 461-468, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: Preliminary investigations of such defaults, in a conditional setting, may be found in [42, 8]. These may lead to a practical form of explanation, with some basis in rational action. On a related note, our model can be extended with probabilistic information. Boutilier <ref> [5] </ref> shows how the notion of counterfactual probabilities can be grafted onto the conditional logic CO. Probabilistic information can then be used to determine explanations of the type described by G ardenfors, explanations that are almost predictive and to distinguish equally plausible explanations on probabilistic grounds.
Reference: [6] <author> Craig Boutilier. </author> <title> Revision sequences and nested conditionals. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 519-525, </pages> <address> Chambery, FR, </address> <year> 1993. </year>
Reference-contexts: The AGM theory does not provide a method for determining the structure of the resulting epistemic state, even if the original epistemic state and belief set K are completely known (but for a recently developed model that captures such iterated revision, see <ref> [6] </ref>). To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 19 The reductions afforded by Propositions 3.1 and 3.2 are crucial, for they allow an agent to test whether an explanation is valid relative to its current epistemic state (or its current set of simple conditionals).
Reference: [7] <author> Craig Boutilier. </author> <title> Conditional logics of normality: A modal approach. </title> <journal> Artificial Intelligence, </journal> <note> 1994. (in press). To appear, Artificial Intelligence, 1995 REFERENCES 54 </note>
Reference-contexts: This ordering is taken to represent the epistemic state of an agent; thus all forms of explanation we describe can be classified as epistemic explanations. Our conditional logic, described in earlier work as a representation of belief revision and default reasoning <ref> [3, 7, 9] </ref>, has the desired nonmonotonicity and induces a natural preference ordering on sentences (hence explanations). In the next section we describe abduction, belief revision, our conditional logics and other necessary logical preliminaries. <p> To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 8 more consistent with an agent's beliefs than w. We take reflexivity and transitivity to be minimal requirements on , dubbing any such model a CT4O-model. Definition 2.1 <ref> [7] </ref> A CT4O-model is a triple M = hW; ; 'i, where W is a set (of possible worlds), is a reflexive, transitive binary relation on W (the ordering relation), and ' maps P into 2 W ('(A) is the set of worlds where A is true). <p> Validity and satisfiability are defined in a straightforward manner and a sound and complete axiomatization for the logic CT4O is provided in <ref> [7] </ref>. To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 9 A natural restriction on the ordering of plausibility is connectedness; that is, for any pair of worlds w; v, either v w or w v. In other words, all worlds must have comparable degrees of plausibility. <p> In other words, all worlds must have comparable degrees of plausibility. This restriction gives rise to the logic CO (again axiomatized in <ref> [7] </ref>). Definition 2.2 [7] A CO-model is a triple M = hW; ; 'i, where M is a CT4O-model and is totally connected. <p> In other words, all worlds must have comparable degrees of plausibility. This restriction gives rise to the logic CO (again axiomatized in <ref> [7] </ref>). Definition 2.2 [7] A CO-model is a triple M = hW; ; 'i, where M is a CT4O-model and is totally connected. <p> No matter how implausible, each should be somehow ranked and should occur in our models. This property turns out to be crucial in characterizing the AGM theory of belief revision. Definition 2.3 <ref> [7] </ref> Let M = hW; ; 'i be a Kripke model. For all w 2 W , w fl is defined as the map from P into f0; 1g such that w fl (A) = 1 iff w 2 '(A) (w fl is the valuation associated with w). <p> CT4O*-models and CO*-models are (respectively) CT4O-models and CO-models satisfying the To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 10 condition that ff : f maps P into f0; 1gg fw fl : w 2 W g: This restriction is captured axiomatically determining the logics CT4O* and CO* <ref> [7] </ref>. 2.2.2 Modeling Belief Revision Assume we have a fixed (CO- or CT4O-) model M . <p> Indeed, the conditional should be accepted just when an agent, hypothetically revising its beliefs by A, accepts B. Thus, we can equate 3 We assume, for simplicity, that such a (limiting) set exists for each ff 2 L CPL , though the following technical developments do not require this <ref> [7, 9] </ref>. 4 This constraint can be expressed in the object language L B ; see [9, 4]. <p> In this case, the most plausible R-worlds must be different from the most plausible R ^ C-worlds. These conditionals have much the same character as default rules. Recently, a number of conditional logics have been proposed for default reasoning [18, 26, 33, 34]. In particular, Boutilier <ref> [7] </ref> has proposed using the logics CT4O and CO together with the conditional ) for default reasoning. To use the logics for this purpose requires simply that we interpret the ordering relation as ranking worlds according to their degree of normality. <p> They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams [1] and further developed by Goldszmidt and Pearl [26], and can be given a probabilistic interpretation <ref> [7] </ref>. Boutilier [9] also shows how default reasoning based on such a conditional logic can be interpreted as a form of belief revision, hence explaining the equivalence of the conditional logic representation of both processes. G ardenfors and Makinson's [23] notion of expectation inference adopts a similar viewpoint.
Reference: [8] <author> Craig Boutilier. </author> <title> Toward a logic for qualitative decision theory. </title> <booktitle> In Proceedings of the Fourth International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Bonn, </address> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: While A may be unlikely given U , the consequences of developing AIDS are so drastic that one may adopt a default U ) A: one should act as if A given U . Preliminary investigations of such defaults, in a conditional setting, may be found in <ref> [42, 8] </ref>. These may lead to a practical form of explanation, with some basis in rational action. On a related note, our model can be extended with probabilistic information. Boutilier [5] shows how the notion of counterfactual probabilities can be grafted onto the conditional logic CO.
Reference: [9] <author> Craig Boutilier. </author> <title> Unifying default reasoning and belief revision in a modal framework. </title> <journal> Artificial Intelligence, </journal> <note> 1994. (in press). </note>
Reference-contexts: This ordering is taken to represent the epistemic state of an agent; thus all forms of explanation we describe can be classified as epistemic explanations. Our conditional logic, described in earlier work as a representation of belief revision and default reasoning <ref> [3, 7, 9] </ref>, has the desired nonmonotonicity and induces a natural preference ordering on sentences (hence explanations). In the next section we describe abduction, belief revision, our conditional logics and other necessary logical preliminaries. <p> We also describe the AGM model of belief revision of Alchourr on, G ardenfors and Makinson [2]; and we present the conditional logics required to capture this theory of revision, due to Boutilier <ref> [9] </ref>. This will provide the logical apparatus required to describe the process of abduction in terms of belief revision. 2.1 Abduction Abduction is the process of inferring certain facts and/or laws that render some sentence plausible, that explain some phenomenon or observation. <p> The AGM theory provides a set of postulates for contraction as well. This process is related to revision via the Levi and Harper identities: K A = K " K fl A = (K :A ) A 2.2.1 The Logics CO and CO* Boutilier <ref> [9] </ref> presents a family of bimodal logics suitable for representing and reasoning about the revision of a knowledge base. We briefly review the logics and associated possible worlds semantics for revision. We refer to [9] for further details and motivation. <p> " K fl A = (K :A ) A 2.2.1 The Logics CO and CO* Boutilier <ref> [9] </ref> presents a family of bimodal logics suitable for representing and reasoning about the revision of a knowledge base. We briefly review the logics and associated possible worlds semantics for revision. We refer to [9] for further details and motivation. Semantically, the process of revision can be captured by considering a plausibility ordering over possible worlds. We can reason about such structures, as well as AGM revision (and several generalizations of it), using a family of bimodal logics. <p> revision of K, we insist that any such K-revision model be such that kKk = min (&gt;); that is, the model must have a (unique) minimal cluster formed by kKk. 4 This reflects the intuition that all and only K-worlds are most plausible for an agent with belief set K <ref> [9] </ref>, and corresponds to a form of only knowing [36, 4]. <p> Indeed, the conditional should be accepted just when an agent, hypothetically revising its beliefs by A, accepts B. Thus, we can equate 3 We assume, for simplicity, that such a (limiting) set exists for each ff 2 L CPL , though the following technical developments do not require this <ref> [7, 9] </ref>. 4 This constraint can be expressed in the object language L B ; see [9, 4]. <p> Thus, we can equate 3 We assume, for simplicity, that such a (limiting) set exists for each ff 2 L CPL , though the following technical developments do not require this [7, 9]. 4 This constraint can be expressed in the object language L B ; see <ref> [9, 4] </ref>. To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 11 the conditional A ) B with the statement B 2 K fl A and interpret our conditional as a certain type of epistemic subjunctive conditional. <p> For a specific K-revision model we can define the revised belief set K fl K fl Boutilier <ref> [9] </ref> shows that the revision functions determined by CO*-models are exactly those that satisfy the AGM postulates. <p> They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams [1] and further developed by Goldszmidt and Pearl [26], and can be given a probabilistic interpretation [7]. Boutilier <ref> [9] </ref> also shows how default reasoning based on such a conditional logic can be interpreted as a form of belief revision, hence explaining the equivalence of the conditional logic representation of both processes. G ardenfors and Makinson's [23] notion of expectation inference adopts a similar viewpoint. <p> The revised belief set will then correspond precisely to the set of default conclusions the agent would reach by performing conditional default reasoning from this set of facts using its conditional default rules (see <ref> [9] </ref> for details). For this reason, our theory of explanation can be used in one of two ways. We may think of explanations relative to the epistemic state of an agent. This is the viewpoint adopted in Section 3 where we present our theory.
Reference: [10] <author> Craig Boutilier and Veronica Becher. </author> <title> Abduction as belief revision. </title> <type> Technical Report 93-23, </type> <institution> University of British Columbia, Vancouver, </institution> <year> 1993. </year>
Reference-contexts: Although, we believe R, W and :S, one might say that Had the sprinkler been on, the grass (still) would have been wet. This slightly more permissive form of predictive explanation, called counterfactual explanation, is not explored further here (but see <ref> [10] </ref> for further details). 3.1.1 Causal Explanations The notion of explanation described here cannot be given a truly causal interpretation. In the factual model in Figure 2, we suggested that rain explains wet grass. However, it is also the case that wet grass explains rain. <p> However, when asked to explain a 10 When there are several disjoint preferred explanations (e.g., R, S), we may be interested in covering explanations, that capture all of the plausible causes of an observation. We refer to <ref> [10] </ref> for a discussion of this notion. To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 27 root node, no explanation but the trivial explanation seems appropriate.
Reference: [11] <author> Craig Boutilier and Mois es Goldszmidt. </author> <title> Revision by conditional beliefs. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 649-654, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: However, especially in the realm of scientific theory formation, explanations are often causal laws that explain observed correlations. Such explanations require a model of belief revision that allows one to revise a theory with new conditionals. One such model is proposed in <ref> [11] </ref> and may provide a starting point for such investigations. 7 Acknowledgements We would like to thank Mois es Goldszmidt and David Poole for their helpful suggestions and discussion of this topic. Thanks also to the referees whose suggestions helped make parts of this paper clearer.
Reference: [12] <author> Gerhard Brewka. </author> <title> Preferred subtheories: An extended logical framework for default reasoning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1043-1048, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: We also introduce the notion of preferred explanations, showing how the same conditional information used to represent the defeasibility of explanations induces a natural preference ordering. To demonstrate the expressive power of our model, in Section 4 we show how Poole's [43, 44] Theorist framework (without constraints) and Brewka's <ref> [12] </ref> extension of Theorist can be captured in our logics. This reconstruction explains semantically the non-predictive and paraconsistent nature of explanations in Theorist. It also illustrates the correct manner in which to augment Theorist with a notion of predictive explanation and how one should capture semantic preferences on Theorist explanations. <p> Representative of these models is Poole's [43, 44] Theorist framework for explanation and prediction, and Brewka's <ref> [12] </ref> extension of it. In this section, we describe both models, how they can be embedded within our framework, and how the notions we defined in the last section can be used to define natural extensions of the Theorist framework. <p> The fact that university students are a specific subclass of adults suggests that the default rule U :E should be applied instead of A E. Brewka <ref> [12] </ref> has extended the Theorist framework for default prediction by introducing priorities on defaults to handle such a situation.
Reference: [13] <author> Luca Console and Pietro Torasso. </author> <title> A spectrum of logical definitions of model-based diagnosis. </title> <journal> Computational Intelligence, </journal> <volume> 7 </volume> <pages> 133-141, </pages> <year> 1991. </year>
Reference-contexts: Thus, a complete model of correct behavior is required if CB-diagnoses are to be of any use. This is in accordance with the observation of Poole [45] who describes the categories of information required for consistency-based diagnosis and abductive diagnosis. Console and Torasso <ref> [13] </ref> have also addressed this issue. They suggest, as we have elaborated above, that consistency-based diagnosis is appropriate if fault models are lacking, while abductive approaches are more suitable if models of correct behavior are incomplete. <p> While Poole's observation is correct for minimal diagnoses (and Reiter's formulation, in particular), it cannot be extended to the more general case subsequently developed by de Kleer, Mackworth and Reiter. Console and Torasso <ref> [13] </ref> have also explored the distinction between abductive and consistency-based diagnosis and present a definition of explanation (in the style of Reiter) that combines both types.
Reference: [14] <author> Randall Davis. </author> <title> Diagnostic reasoning based on structure and behavior. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 347-410, </pages> <year> 1984. </year>
Reference: [15] <author> Johan de Kleer. </author> <title> Focusing on probable diagnoses. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 842-848, </pages> <address> Anaheim, </address> <year> 1991. </year>
Reference-contexts: An obvious preference criterion on explanations is based on the likelihood of the explanations themselves. An agent should choose the most probable explanation relative to a given context. Such accounts are often found in diagnosis <ref> [46, 15] </ref> and most probable explanations are discussed by Pearl [41]. In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher [52]) and Quine and Ullian [48].
Reference: [16] <author> Johan de Kleer, Alan K. Mackworth, and Raymond Reiter. </author> <title> Characterizing diagnoses. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 324-330, </pages> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: There have been proposals to address these issues in a more qualitative manner using logic-based frameworks also. Peirce (see Rescher [52]) discusses the plausibility of explanations, as do Quine and Ullian [48]. Consistency-based diagnosis <ref> [49, 16] </ref> uses abnormality assumptions to capture the context-dependence of explanations; and preferred explanations are those that minimize abnormalities. Poole's [44] assumption-based framework captures some of these ideas by explicitly introducing a set of default assumptions to account for the nonmonotonicity of explanations. <p> These two abilities have until now been unexplored in this canonical abductive framework. In Section 5, we reconstruct a canonical theory of consistency-based diagnosis due to de Kleer, Mackworth and Reiter <ref> [16, 49] </ref> in our logics. This again suggests extensions of the theory and illustrates the natural similarities and distinctions between consistency-based and abductive diagnosis. <p> The two main paradigms for model-based diagnosis are the abductive approaches, of which Poole's [43, 44] Theorist framework is representative, and consistency-based models such as that of de Kleer, Mackworth and Reiter <ref> [16, 49] </ref>. These will be discussed in detail in Sections 4 and 5. To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 6 2.2 Conditionals and Belief Revision The account of abduction we propose relies heavily on the notion of belief revision. <p> Another approach to model-based diagnosis is consistency-based diagnosis, which is aimed more directly at the diagnostic task, namely to determine why a correctly designed system is not functioning according to its specification. In this section, after presenting the fundamental concepts from Reiter's [49] and de Kleer, Mackworth and Reiter's <ref> [16] </ref> methodology for diagnosis, we show how these canonical consistency-based models can be embedded in our framework for epistemic explanations. This highlights many of the key similarities and differences in the abductive and consistency-based approaches. <p> We also address the role fault models play within our semantics and how diagnoses can be made predictive. To appear, Artificial Intelligence, 1995 5 CONSISTENCY-BASED DIAGNOSIS 41 5.1 A Logical Specification de Kleer, Mackworth and Reiter <ref> [49, 16] </ref> assume that an appropriate model of a system or artifact consists of two parts. The first is a set of components COMP, the parts of a system that one is able to distinguish and that (more or less) independently can fail to function correctly. <p> More precisely, following <ref> [16] </ref>, we have these definitions. 18 Definition 5.1 Let COMP be a set of components.
Reference: [17] <author> Johan de Kleer and Brian C. Williams. </author> <title> Diagnosing multiple faults. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 97-130, </pages> <year> 1987. </year>
Reference-contexts: This is the case in diagnostic applications, for example, where observations to be explained contradict our belief that a system is performing according to specification. The first two of these problems can be addressed using, for example, probabilistic information <ref> [29, 17, 46, 41] </ref>. We might simply require that an explanation render the observation sufficiently probable. Explanations might thus be nonmonotonic in the sense that ff may explain fi, but ff ^ fl may not (e.g., P (fijff) may be sufficiently high while P (fijff ^ fl) may not).
Reference: [18] <author> James P. Delgrande. </author> <title> An approach to default reasoning based on a first-order conditional logic: Revised report. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 63-90, </pages> <year> 1988. </year>
Reference-contexts: In this case, the most plausible R-worlds must be different from the most plausible R ^ C-worlds. These conditionals have much the same character as default rules. Recently, a number of conditional logics have been proposed for default reasoning <ref> [18, 26, 33, 34] </ref>. In particular, Boutilier [7] has proposed using the logics CT4O and CO together with the conditional ) for default reasoning. To use the logics for this purpose requires simply that we interpret the ordering relation as ranking worlds according to their degree of normality.
Reference: [19] <author> Gerhard Friedrich and Wolfgang Nejdl. </author> <title> Choosing observations and actions in model-based diagnosis/repair systems. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 489-498, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: We also hope to explore the issue To appear, Artificial Intelligence, 1995 6 CONCLUDING REMARKS 52 of designing tests to discriminate potential diagnoses, and the trade-off between further testing and repair. This is an issue that has recently attracted much attention <ref> [19, 39] </ref>. The pragmatics of explanation remains an important avenue to pursue. Ways in which to rule out weak or strong explanations, depending on context must be addressed. Another pragmatic concern has to do with the elaboration of explanations. We have assumed that explanations are given relative to background theory.
Reference: [20] <author> Antony Galton. </author> <title> A critique of Yoav Shoham's theory of causal reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 355-359, </pages> <address> Anaheim, </address> <year> 1991. </year>
Reference-contexts: Incorporating such considerations in our model brings to mind Shoham's [55] epistemic account of causality, whereby a causal theory is expressed in terms of the knowledge of an agent, and can be nonmonotonic. Whether or not causality is an epistemic notion (cf. the critique of Galton <ref> [20] </ref>), it is clear that perceived causal relations will have a dramatic impact To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 21 on the conditional beliefs of an agent. Furthermore, it is the epistemic state of an agent with respect to which causal predictions and explanations must be derived.
Reference: [21] <author> Peter G ardenfors. </author> <title> On the logic of relevance. </title> <journal> Synthese, </journal> <volume> 37(3) </volume> <pages> 351-367, </pages> <year> 1978. </year>
Reference-contexts: We can view this as a crude pragmatic theory. Levesque [35] embeds a syntactic notion of simplicity in his semantics for abduction. In our conditional framework one can define conditions under which a proposition is deemed irrelevant to a conditional <ref> [21, 3] </ref>. Explanations can also be strengthened with background information that, while not irrelevant, can be left unstated. For instance, returning to the example given by the factual model in Figure 2, we can see that R explains W , and R ^ :C explains W as well.
Reference: [22] <author> Peter G ardenfors. </author> <title> Knowledge in Flux: Modeling the Dynamics of Epistemic States. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year> <note> To appear, Artificial Intelligence, 1995 REFERENCES 55 </note>
Reference-contexts: To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 5 requires that the explanation make the observation highly probable. Thus, probabilistic explanations still retain the essential predictive power of deductive explanations. Other accounts make less stringent requirements. For instance, G ardenfors <ref> [22] </ref> insists only that the explanation render the observation more probable than it is a priori. A key component of the G ardenfors theory is that the judgements of probability are rendered with respect to the epistemic state of an agent. We return to this in Section 3. <p> We use K fl A to denote the belief set resulting when K is revised by A. The AGM theory logically delimits the scope of acceptable revision functions. To this end, the AGM postulates below are maintained to hold for any reasonable notion of revision <ref> [22] </ref>. (R1) K fl A is a belief set (i.e. deductively closed). (R2) A 2 K fl (R3) K fl A . (R4) If :A 62 K then K + A . <p> A then (K fl A ) B K fl The semantics of AGM revision functions will be described below. An alternative model of revision is based on the notion of epistemic entrenchment <ref> [22] </ref>. Given a belief set K, we can characterize the revision of K by ordering beliefs according to our willingness To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 7 to give them up when necessary. <p> If one of two beliefs must be retracted in order to accommodate some new fact, the least entrenched belief will be relinquished, while the most entrenched persists. G ardenfors <ref> [22] </ref> presents five postulates for such an ordering and shows that these orderings determine exactly the space of revision functions satisfying the AGM postulates. We let B E A denote the fact that A is at least as entrenched as B in theory K. <p> Thus, a model of belief revision seems crucial for explanations of this sort. In order to account for such explanations, one must permit the belief set (or background theory) to be revised in some way that allows consistent explanations of such observations. G ardenfors <ref> [22] </ref> has proposed a model of abduction that relies crucially on the epistemic state of the agent doing the explaining. Our model finds its origins in his account, but there are several crucial differences. First, G ardenfors's model is probabilistic whereas our model is qualitative. <p> To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 17 lesser degree. Unexplained belief in fi places the agent in a state of cognitive dissonance. An explanation relieves this dissonance when it is accepted <ref> [22] </ref>. After this process both explanation and observation are believed. Thus, the abductive process should be understood in terms of hypothetical explanations: when it is realized what could have caused belief in an (unexpected) observation, both observation and explanation are incorporated. <p> Other forms of explanation cannot be captured in our framework, at least in its current formulation. An important type of explanation is of the form addressed by the theory of G ardenfors <ref> [22] </ref>. There an explanation is simply required to render an observation more plausible than it was before the explanation was adopted. As an example, consider possible explanations for Fred's having developed AIDS (A). A possible (even reasonable) explanation is that Fred practiced unsafe sex (U ).
Reference: [23] <author> Peter G ardenfors and David Makinson. </author> <title> Nonmonotonic inference based on expectations. </title> <journal> Artificial Intelligence, </journal> <volume> 65 </volume> <pages> 197-245, </pages> <year> 1994. </year>
Reference-contexts: Boutilier [9] also shows how default reasoning based on such a conditional logic can be interpreted as a form of belief revision, hence explaining the equivalence of the conditional logic representation of both processes. G ardenfors and Makinson's <ref> [23] </ref> notion of expectation inference adopts a similar viewpoint. Roughly, we think of default rules of the form A ) B as inducing various expectations about the normal state of affairs.
Reference: [24] <author> M. R. Genesereth. </author> <title> The use of design descriptions in automated diagnosis. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 411-436, </pages> <year> 1984. </year>
Reference: [25] <author> Matthew L. Ginsberg. </author> <title> Counterfactuals. </title> <journal> Artificial Intelligence, </journal> <volume> 30(1) </volume> <pages> 35-79, </pages> <year> 1986. </year>
Reference-contexts: This counterfactual reading turns out to be quite important in AI, for instance, in diagnostic tasks (see below), planning, and so on <ref> [25] </ref>. For example, if turning on the sprinkler explains the grass being wet and an agent's goal is to wet the grass, then it may well turn on the sprinkler.
Reference: [26] <author> Mois es Goldszmidt and Judea Pearl. </author> <title> On the consistency of defeasible databases. </title> <journal> Artificial Intelligence, </journal> <volume> 52 </volume> <pages> 121-149, </pages> <year> 1991. </year>
Reference-contexts: In this case, the most plausible R-worlds must be different from the most plausible R ^ C-worlds. These conditionals have much the same character as default rules. Recently, a number of conditional logics have been proposed for default reasoning <ref> [18, 26, 33, 34] </ref>. In particular, Boutilier [7] has proposed using the logics CT4O and CO together with the conditional ) for default reasoning. To use the logics for this purpose requires simply that we interpret the ordering relation as ranking worlds according to their degree of normality. <p> They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams [1] and further developed by Goldszmidt and Pearl <ref> [26] </ref>, and can be given a probabilistic interpretation [7]. Boutilier [9] also shows how default reasoning based on such a conditional logic can be interpreted as a form of belief revision, hence explaining the equivalence of the conditional logic representation of both processes.
Reference: [27] <author> Mois es Goldszmidt and Judea Pearl. </author> <title> Rank-based systems: A simple approach to belief revision, belief update, and reasoning about evidence and actions. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 661-672, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Lewis [37] has proposed a counterfactual analysis of causation, whereby a theory of conditionals might be used to determine causal relations between propositions. More recently, and perhaps more compelling, is the use of stratified rankings on conditional theories by Goldszmidt and Pearl <ref> [27] </ref> to represent causation. Incorporating such considerations in our model brings to mind Shoham's [55] epistemic account of causality, whereby a causal theory is expressed in terms of the knowledge of an agent, and can be nonmonotonic. <p> In this regard, an epistemic theory of causal explanation is consistent with Shoham's viewpoint. However, a more sophisticated account of causation is necessary in order to distinguish causal from evidential relations among an agent's beliefs. 8 A more suitable theory should include some account of actions, events, and intervention <ref> [27] </ref>. For instance, if a (possibly hypothetical) mechanism exists for independently wetting the grass (W ) and making it rain (R), this can be exploited to show that W does not cause R, but that R causes W , according to the plausibility judgements of an agent. <p> We are currently investigating how our model might be extended to incorporate causal explanations. Such explanations, especially in diagnostic and planning tasks, are of particular interest. Grafting a representation of causal influences onto our model of explanation, such as that of Goldszmidt and Pearl <ref> [27] </ref>, seems like a promising way in which to (qualitatively) capture causal explanations. Konolige [32] has explored the use of causal theories in diagnosis as a means to obviate the need for fault models.
Reference: [28] <author> Adam Grove. </author> <title> Two modellings for theory change. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 17 </volume> <pages> 157-170, </pages> <year> 1988. </year>
Reference-contexts: We note that the dual of an entrenchment ordering is a plausibility ordering on sentences. A sentence A is more plausible than B just when :A is less entrenched than :B, and means that A would be more readily accepted than B if the opportunity arose. Grove <ref> [28] </ref> studied this relationship and its connection to the AGM theory. Another form of belief change studied within the AGM theory is the process of contraction, or rejecting a belief in a belief set.
Reference: [29] <author> Carl G. Hempel. </author> <booktitle> Philosophy of Natural Science. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1966. </year>
Reference-contexts: This is the case in diagnostic applications, for example, where observations to be explained contradict our belief that a system is performing according to specification. The first two of these problems can be addressed using, for example, probabilistic information <ref> [29, 17, 46, 41] </ref>. We might simply require that an explanation render the observation sufficiently probable. Explanations might thus be nonmonotonic in the sense that ff may explain fi, but ff ^ fl may not (e.g., P (fijff) may be sufficiently high while P (fijff ^ fl) may not). <p> Thus, an explanation renders an observation plausible (in some yet to be determined sense). The most basic and, in some idealized sense, the most compelling form of abduction is represented by Hempel's <ref> [29] </ref> deductive-nomological explanations. Such explanations consist of certain specific facts and universal generalizations (scientific laws) that, taken together, deductively entail a given observation. For example, the observation This thing flies can be explained by the fact This thing is a bird and the law All birds fly. <p> To take an example of Hempel, Jim's close exposure to his brother who has the measles explains Jim catching the measles; but it certainly doesn't imply Jim catching the measles. A number of methods for specifying probabilistic explanations have been proffered. Hempel <ref> [29] </ref> 1 In fact, as we will see in Section 3, the theory is implicit in the epistemic state of our reasoning agent. We will have a few things to say about laws in our framework in the concluding section.
Reference: [30] <author> Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul Martin. </author> <title> Interpretation as abduction. </title> <type> Technical Note 499, </type> <institution> SRI International, </institution> <address> Menlo Park, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Of the many explanations, some may be preferred on grounds other than plausibility. Natural criteria such as simplicity and informativeness are often used to rule out certain explanations in certain contexts [47]. Levesque [35] has proposed criteria for judging the simplicity of explanations. Hobbs et al <ref> [30] </ref> argue that in natural language interpretation most specific explanations are often required, rather than simple explanations. In diagnostic systems, often this problem is circumvented, for explanations are usually drawn from a prespecified set of conjectures [44] (see Sections 4 and 5).
Reference: [31] <author> Kurt Konolige. </author> <title> Abduction versus closure in causal theories. </title> <journal> Artificial Intelligence, </journal> <volume> 53 </volume> <pages> 255-272, </pages> <year> 1992. </year>
Reference-contexts: This indicates that the abductive approach to diagnosis requires information of a form different from that used in the consistency-based approach. This is emphasized by Poole [45]. However, given complete fault models, Theorist diagnoses and consistency-based diagnoses will coincide. Konolige <ref> [31] </ref> has also examined the relationship between the two forms of diagnosis. Without complete information, the Theorist system, in particular the notion of an extension, can still be used to effect consistency-based diagnosis.
Reference: [32] <author> Kurt Konolige. </author> <title> Using default and causal reasoning in diagnosis. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 509-520, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Such explanations, especially in diagnostic and planning tasks, are of particular interest. Grafting a representation of causal influences onto our model of explanation, such as that of Goldszmidt and Pearl [27], seems like a promising way in which to (qualitatively) capture causal explanations. Konolige <ref> [32] </ref> has explored the use of causal theories in diagnosis as a means to obviate the need for fault models. His representation in terms of default causal nets allows both explanations and excuses; but the causal component of his representation remains essentially unanalyzed.
Reference: [33] <author> Sarit Kraus, Daniel Lehmann, and Menachem Magidor. </author> <title> Nonmonotonic reasoning, preferential models and cumulative logics. </title> <journal> Artificial Intelligence, </journal> <volume> 44 </volume> <pages> 167-207, </pages> <year> 1990. </year>
Reference-contexts: In this case, the most plausible R-worlds must be different from the most plausible R ^ C-worlds. These conditionals have much the same character as default rules. Recently, a number of conditional logics have been proposed for default reasoning <ref> [18, 26, 33, 34] </ref>. In particular, Boutilier [7] has proposed using the logics CT4O and CO together with the conditional ) for default reasoning. To use the logics for this purpose requires simply that we interpret the ordering relation as ranking worlds according to their degree of normality. <p> On this interpretation, A ) B means that B holds at the most normal A-worlds; that is, If A then normally B. These default logics are shown to be equivalent to the preferential and rational consequence operations of Lehmann <ref> [33, 34] </ref>. They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams [1] and further developed by Goldszmidt and Pearl [26], and can be given a probabilistic interpretation [7].
Reference: [34] <author> Daniel Lehmann. </author> <booktitle> What does a conditional knowledge base entail? In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 212-222, </pages> <address> Toronto, </address> <year> 1989. </year>
Reference-contexts: In this case, the most plausible R-worlds must be different from the most plausible R ^ C-worlds. These conditionals have much the same character as default rules. Recently, a number of conditional logics have been proposed for default reasoning <ref> [18, 26, 33, 34] </ref>. In particular, Boutilier [7] has proposed using the logics CT4O and CO together with the conditional ) for default reasoning. To use the logics for this purpose requires simply that we interpret the ordering relation as ranking worlds according to their degree of normality. <p> On this interpretation, A ) B means that B holds at the most normal A-worlds; that is, If A then normally B. These default logics are shown to be equivalent to the preferential and rational consequence operations of Lehmann <ref> [33, 34] </ref>. They are also To appear, Artificial Intelligence, 1995 3 EPISTEMIC EXPLANATIONS 13 equivalent to the logic of arbitrarily high probabilities proposed by Adams [1] and further developed by Goldszmidt and Pearl [26], and can be given a probabilistic interpretation [7].
Reference: [35] <author> Hector J. Levesque. </author> <title> A knowledge level account of abduction. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1061-1067, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: In contrast, G ardenfors makes no such requirement, counting as explanations facts that only marginally affect the probability of an observation. However, we share with G ardenfors the idea that explanations may be evaluated with respect to states of belief other than that currently held by an agent. Levesque's <ref> [35] </ref> account of abduction is also based on the notion of an epistemic state. Levesque allows the notion of belief to vary (from the standard deductively-closed notion) within his framework in order to capture different types of explanation (e.g., a syntax-motivated notion of simplest explanation). <p> The semantic conditions we have proposed admit explanations that are intuitively unsatisfying in some circumstances. Of the many explanations, some may be preferred on grounds other than plausibility. Natural criteria such as simplicity and informativeness are often used to rule out certain explanations in certain contexts [47]. Levesque <ref> [35] </ref> has proposed criteria for judging the simplicity of explanations. Hobbs et al [30] argue that in natural language interpretation most specific explanations are often required, rather than simple explanations. <p> Therefore, we do not include such considerations in our semantic account of abduction. Rather, we view these as pragmatic concerns, distinct from the semantic issues involved in predictiveness and plausibility (cf. Levesque <ref> [35] </ref>). Providing an account of the pragmatics of explanations is beyond the scope of this paper; but we briefly review two such issues that arise in our framework: trivial explanations and irrelevant information. 3.4.1 Trivial Explanations A simple theorem of CT4O and CO is fi ) fi. <p> In Poole's Theorist system, for example, explanations are drawn from a prespecified set of conjectures. We can view this as a crude pragmatic theory. Levesque <ref> [35] </ref> embeds a syntactic notion of simplicity in his semantics for abduction. In our conditional framework one can define conditions under which a proposition is deemed irrelevant to a conditional [21, 3]. Explanations can also be strengthened with background information that, while not irrelevant, can be left unstated.
Reference: [36] <author> Hector J. Levesque. </author> <title> All I know: A study in autoepistemic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 263-309, </pages> <year> 1990. </year>
Reference-contexts: model be such that kKk = min (&gt;); that is, the model must have a (unique) minimal cluster formed by kKk. 4 This reflects the intuition that all and only K-worlds are most plausible for an agent with belief set K [9], and corresponds to a form of only knowing <ref> [36, 4] </ref>. The CT4O-model in Figure 1 (a) is a K-revision model for K = Cn (:A; B), while the CO-model in To revise K by A, we construct the revised set K fl A by considering the set min (A) of most plausible A-worlds in M .
Reference: [37] <author> David Lewis. </author> <title> Causation. </title> <journal> Journal of Philosophy, </journal> <volume> 70 </volume> <pages> 556-567, </pages> <year> 1973. </year>
Reference-contexts: The obvious criterion is the following predictive condition: (P) fi 2 K fl which is expressed in the object language as ff ) fi. This captures the intuition that If the explanation were believed, so too would be the observation <ref> [37] </ref>. For hypothetical explanations, this seems sufficient, but for factual explanations (where fi 2 K), this condition is trivialized by the presence of (ES). <p> The connection may be causal (belief in R induces belief in W ) or evidential (belief in W induces belief in R). Ultimately, we would like to be able to distinguish causal from non-causal explanations in this conditional model. Lewis <ref> [37] </ref> has proposed a counterfactual analysis of causation, whereby a theory of conditionals might be used to determine causal relations between propositions. More recently, and perhaps more compelling, is the use of stratified rankings on conditional theories by Goldszmidt and Pearl [27] to represent causation.
Reference: [38] <author> John McCarthy. </author> <title> Epistemological problems in artificial intelligence. </title> <editor> In Ronald J. Brachman and Hector J. Levesque, editors, </editor> <booktitle> Readings in Knowledge Representation, </booktitle> <pages> pages 24-30. </pages> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, </address> <year> 1977. 1985. </year> <note> To appear, Artificial Intelligence, 1995 REFERENCES 56 </note>
Reference-contexts: However, this runs into the qualification problem of default reasoning, the problem of having to know that such conditions are false <ref> [38] </ref>. This view is also untenable when such qualifications cannot be listed, or the phenomenon in question is inherently probabilistic (at least, given our current knowledge).
Reference: [39] <author> Sheila McIlraith and Ray Reiter. </author> <title> On experiments for hypothetical reasoning. </title> <booktitle> In Proc. 2nd International Workshop on Principles of Diagnosis, </booktitle> <pages> pages 1-10, </pages> <address> Milan, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: We also hope to explore the issue To appear, Artificial Intelligence, 1995 6 CONCLUDING REMARKS 52 of designing tests to discriminate potential diagnoses, and the trade-off between further testing and repair. This is an issue that has recently attracted much attention <ref> [19, 39] </ref>. The pragmatics of explanation remains an important avenue to pursue. Ways in which to rule out weak or strong explanations, depending on context must be addressed. Another pragmatic concern has to do with the elaboration of explanations. We have assumed that explanations are given relative to background theory.
Reference: [40] <author> Donald Nute. </author> <title> Topics in Conditional Logic. </title> <address> D.Reidel, Dordrecht, </address> <year> 1980. </year>
Reference-contexts: This is strongly related to the following issue that arises in the study of conditional logics: sentences with the linguistic form (A _ C) ) B are usually intended to represent an assertion with the logical form (A ) B) _ (C ) B) <ref> [40] </ref>. 4 Abductive Models of Diagnosis One of the main approaches to model-based diagnostic reasoning and explanation are the so-called abductive theories. Representative of these models is Poole's [43, 44] Theorist framework for explanation and prediction, and Brewka's [12] extension of it.
Reference: [41] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: This is the case in diagnostic applications, for example, where observations to be explained contradict our belief that a system is performing according to specification. The first two of these problems can be addressed using, for example, probabilistic information <ref> [29, 17, 46, 41] </ref>. We might simply require that an explanation render the observation sufficiently probable. Explanations might thus be nonmonotonic in the sense that ff may explain fi, but ff ^ fl may not (e.g., P (fijff) may be sufficiently high while P (fijff ^ fl) may not). <p> An obvious preference criterion on explanations is based on the likelihood of the explanations themselves. An agent should choose the most probable explanation relative to a given context. Such accounts are often found in diagnosis [46, 15] and most probable explanations are discussed by Pearl <ref> [41] </ref>. In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher [52]) and Quine and Ullian [48].
Reference: [42] <author> Judea Pearl. </author> <title> A calculus of pragmatic obligation. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 12-20, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: While A may be unlikely given U , the consequences of developing AIDS are so drastic that one may adopt a default U ) A: one should act as if A given U . Preliminary investigations of such defaults, in a conditional setting, may be found in <ref> [42, 8] </ref>. These may lead to a practical form of explanation, with some basis in rational action. On a related note, our model can be extended with probabilistic information. Boutilier [5] shows how the notion of counterfactual probabilities can be grafted onto the conditional logic CO.
Reference: [43] <author> David Poole. </author> <title> A logical framework for default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 27-47, </pages> <year> 1988. </year>
Reference-contexts: We also introduce the notion of preferred explanations, showing how the same conditional information used to represent the defeasibility of explanations induces a natural preference ordering. To demonstrate the expressive power of our model, in Section 4 we show how Poole's <ref> [43, 44] </ref> Theorist framework (without constraints) and Brewka's [12] extension of Theorist can be captured in our logics. This reconstruction explains semantically the non-predictive and paraconsistent nature of explanations in Theorist. <p> The goal of model-based diagnosis is to discover an explanation for the aberrant behavior, usually some set of components of the system that, if behaving abnormally, will entail or excuse the actual observation. The two main paradigms for model-based diagnosis are the abductive approaches, of which Poole's <ref> [43, 44] </ref> Theorist framework is representative, and consistency-based models such as that of de Kleer, Mackworth and Reiter [16, 49]. These will be discussed in detail in Sections 4 and 5. <p> Representative of these models is Poole's <ref> [43, 44] </ref> Theorist framework for explanation and prediction, and Brewka's [12] extension of it. In this section, we describe both models, how they can be embedded within our framework, and how the notions we defined in the last section can be used to define natural extensions of the Theorist framework. <p> This also provides an object-level semantic account of Theorist. 4.1 Theorist and Preferred Subtheories Poole <ref> [43, 44] </ref> presents a framework for hypothetical reasoning that supports explanation and default prediction. Theorist is based on default theories, pairs hF ; Di where F and D are sets of sentences. 12 The elements of F are facts, known to be true of the situation under investigation.
Reference: [44] <author> David Poole. </author> <title> Explanation and prediction: An architecture for default and abductive reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 97-110, </pages> <year> 1989. </year>
Reference-contexts: Peirce (see Rescher [52]) discusses the plausibility of explanations, as do Quine and Ullian [48]. Consistency-based diagnosis [49, 16] uses abnormality assumptions to capture the context-dependence of explanations; and preferred explanations are those that minimize abnormalities. Poole's <ref> [44] </ref> assumption-based framework captures some of these ideas by explicitly introducing a set of default assumptions to account for the nonmonotonicity of explanations. In this paper we propose a semantic framework and logical specification of abduction that captures the spirit of probabilistic proposals, but does so in a qualitative fashion. <p> We also introduce the notion of preferred explanations, showing how the same conditional information used to represent the defeasibility of explanations induces a natural preference ordering. To demonstrate the expressive power of our model, in Section 4 we show how Poole's <ref> [43, 44] </ref> Theorist framework (without constraints) and Brewka's [12] extension of Theorist can be captured in our logics. This reconstruction explains semantically the non-predictive and paraconsistent nature of explanations in Theorist. <p> The goal of model-based diagnosis is to discover an explanation for the aberrant behavior, usually some set of components of the system that, if behaving abnormally, will entail or excuse the actual observation. The two main paradigms for model-based diagnosis are the abductive approaches, of which Poole's <ref> [43, 44] </ref> Theorist framework is representative, and consistency-based models such as that of de Kleer, Mackworth and Reiter [16, 49]. These will be discussed in detail in Sections 4 and 5. <p> Hobbs et al [30] argue that in natural language interpretation most specific explanations are often required, rather than simple explanations. In diagnostic systems, often this problem is circumvented, for explanations are usually drawn from a prespecified set of conjectures <ref> [44] </ref> (see Sections 4 and 5). It is clear that the exact form an explanation should take is influenced by the application one has in mind. Therefore, we do not include such considerations in our semantic account of abduction. <p> Representative of these models is Poole's <ref> [43, 44] </ref> Theorist framework for explanation and prediction, and Brewka's [12] extension of it. In this section, we describe both models, how they can be embedded within our framework, and how the notions we defined in the last section can be used to define natural extensions of the Theorist framework. <p> This also provides an object-level semantic account of Theorist. 4.1 Theorist and Preferred Subtheories Poole <ref> [43, 44] </ref> presents a framework for hypothetical reasoning that supports explanation and default prediction. Theorist is based on default theories, pairs hF ; Di where F and D are sets of sentences. 12 The elements of F are facts, known to be true of the situation under investigation. <p> Although nothing crucial depends on this, we assume D to be consistent. Poole also uses a set C of conjectures that may be used in the explanation of observations, but should not be used in default prediction. 13 Definition 4.1 <ref> [44] </ref> An extension of hF ; Di is any set Cn (F [ D) where D is a maximal subset of D such that F [ D is consistent. Intuitively, extensions are formed by assuming as many defaults as possible. <p> To appear, Artificial Intelligence, 1995 4 ABDUCTIVE MODELS OF DIAGNOSIS 29 (skeptical) notion of default prediction is defined by considering what is true at each such normal situation. Definition 4.2 <ref> [44] </ref> Sentence A is predicted by hF ; Di iff A is in each extension of hF ; Di. Conjectures play a key role in abduction, and can be viewed as possible hypotheses that (together with certain defaults) explain a given observation fi. Definition 4.3 [44] C [ D is a <p> Definition 4.2 <ref> [44] </ref> Sentence A is predicted by hF ; Di iff A is in each extension of hF ; Di. Conjectures play a key role in abduction, and can be viewed as possible hypotheses that (together with certain defaults) explain a given observation fi. Definition 4.3 [44] C [ D is a (Theorist) explanation for observation fi (w.r.t. hF ; D; Ci) iff C C, D D, C [ D [ F is consistent and C [ D [ F j= fi.
Reference: [45] <author> David Poole. </author> <title> Normality and faults in logic-based diagnosis. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1304-1310, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: However, without any indication of correct behavior any observation is consistent with the assumption that all components work correctly. Thus, a complete model of correct behavior is required if CB-diagnoses are to be of any use. This is in accordance with the observation of Poole <ref> [45] </ref> who describes the categories of information required for consistency-based diagnosis and abductive diagnosis. Console and Torasso [13] have also addressed this issue. <p> As we have seen, many (if not most) observations cannot be predicted in the consistency-based framework, especially if fault-models are lacking or incomplete. This indicates that the abductive approach to diagnosis requires information of a form different from that used in the consistency-based approach. This is emphasized by Poole <ref> [45] </ref>. However, given complete fault models, Theorist diagnoses and consistency-based diagnoses will coincide. Konolige [31] has also examined the relationship between the two forms of diagnosis. Without complete information, the Theorist system, in particular the notion of an extension, can still be used to effect consistency-based diagnosis. <p> Thus, consistency-based diagnosis can be captured in the Theorist abductive framework without requiring that the form of the system description be altered. SD is simply used as the set of facts F . Poole <ref> [45] </ref> also defines a form of consistency-based diagnosis within Theorist. He shows that AB () is a consistency-based diagnosis iff D () is in some extension of SD [ ffig.
Reference: [46] <author> David Poole. </author> <title> Representing diagnostic knowledge for probabilistic horn abduction. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1129-1135, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: This is the case in diagnostic applications, for example, where observations to be explained contradict our belief that a system is performing according to specification. The first two of these problems can be addressed using, for example, probabilistic information <ref> [29, 17, 46, 41] </ref>. We might simply require that an explanation render the observation sufficiently probable. Explanations might thus be nonmonotonic in the sense that ff may explain fi, but ff ^ fl may not (e.g., P (fijff) may be sufficiently high while P (fijff ^ fl) may not). <p> An obvious preference criterion on explanations is based on the likelihood of the explanations themselves. An agent should choose the most probable explanation relative to a given context. Such accounts are often found in diagnosis <ref> [46, 15] </ref> and most probable explanations are discussed by Pearl [41]. In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher [52]) and Quine and Ullian [48].
Reference: [47] <author> Karl R. </author> <title> Popper. The Logic of Scientific Discovery. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: The semantic conditions we have proposed admit explanations that are intuitively unsatisfying in some circumstances. Of the many explanations, some may be preferred on grounds other than plausibility. Natural criteria such as simplicity and informativeness are often used to rule out certain explanations in certain contexts <ref> [47] </ref>. Levesque [35] has proposed criteria for judging the simplicity of explanations. Hobbs et al [30] argue that in natural language interpretation most specific explanations are often required, rather than simple explanations.
Reference: [48] <author> W.V. Quine and J.S. Ullian. </author> <title> The Web of Belief. Random House, </title> <address> New York, </address> <year> 1970. </year>
Reference-contexts: There have been proposals to address these issues in a more qualitative manner using logic-based frameworks also. Peirce (see Rescher [52]) discusses the plausibility of explanations, as do Quine and Ullian <ref> [48] </ref>. Consistency-based diagnosis [49, 16] uses abnormality assumptions to capture the context-dependence of explanations; and preferred explanations are those that minimize abnormalities. Poole's [44] assumption-based framework captures some of these ideas by explicitly introducing a set of default assumptions to account for the nonmonotonicity of explanations. <p> Such accounts are often found in diagnosis [46, 15] and most probable explanations are discussed by Pearl [41]. In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher [52]) and Quine and Ullian <ref> [48] </ref>. The notion of minimal diagnosis in the consistency-based models of diagnosis [49] is an attempt to qualitatively characterize most probable diagnoses. We will provide a formal framework in which such qualitative judgements of plausibility can be made.
Reference: [49] <author> Raymond Reiter. </author> <title> A theory of diagnosis from first principles. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 57-95, </pages> <year> 1987. </year>
Reference-contexts: There have been proposals to address these issues in a more qualitative manner using logic-based frameworks also. Peirce (see Rescher [52]) discusses the plausibility of explanations, as do Quine and Ullian [48]. Consistency-based diagnosis <ref> [49, 16] </ref> uses abnormality assumptions to capture the context-dependence of explanations; and preferred explanations are those that minimize abnormalities. Poole's [44] assumption-based framework captures some of these ideas by explicitly introducing a set of default assumptions to account for the nonmonotonicity of explanations. <p> These two abilities have until now been unexplored in this canonical abductive framework. In Section 5, we reconstruct a canonical theory of consistency-based diagnosis due to de Kleer, Mackworth and Reiter <ref> [16, 49] </ref> in our logics. This again suggests extensions of the theory and illustrates the natural similarities and distinctions between consistency-based and abductive diagnosis. <p> In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher [52]) and Quine and Ullian [48]. The notion of minimal diagnosis in the consistency-based models of diagnosis <ref> [49] </ref> is an attempt to qualitatively characterize most probable diagnoses. We will provide a formal framework in which such qualitative judgements of plausibility can be made. One of the areas of AI that most frequently appeals to abductive inference is model-based diagnosis. <p> The two main paradigms for model-based diagnosis are the abductive approaches, of which Poole's [43, 44] Theorist framework is representative, and consistency-based models such as that of de Kleer, Mackworth and Reiter <ref> [16, 49] </ref>. These will be discussed in detail in Sections 4 and 5. To appear, Artificial Intelligence, 1995 2 ABDUCTION AND BELIEF REVISION 6 2.2 Conditionals and Belief Revision The account of abduction we propose relies heavily on the notion of belief revision. <p> Another approach to model-based diagnosis is consistency-based diagnosis, which is aimed more directly at the diagnostic task, namely to determine why a correctly designed system is not functioning according to its specification. In this section, after presenting the fundamental concepts from Reiter's <ref> [49] </ref> and de Kleer, Mackworth and Reiter's [16] methodology for diagnosis, we show how these canonical consistency-based models can be embedded in our framework for epistemic explanations. This highlights many of the key similarities and differences in the abductive and consistency-based approaches. <p> We also address the role fault models play within our semantics and how diagnoses can be made predictive. To appear, Artificial Intelligence, 1995 5 CONSISTENCY-BASED DIAGNOSIS 41 5.1 A Logical Specification de Kleer, Mackworth and Reiter <ref> [49, 16] </ref> assume that an appropriate model of a system or artifact consists of two parts. The first is a set of components COMP, the parts of a system that one is able to distinguish and that (more or less) independently can fail to function correctly. <p> To appear, Artificial Intelligence, 1995 5 CONSISTENCY-BASED DIAGNOSIS 42 Definition 5.2 Let COMP . A consistency-based diagnosis (CB-diagnosis for short) for obser vation fi is any D () such that SD [ ffi; D ()g is satisfiable. Reiter's <ref> [49] </ref> Principle of Parsimony suggests that reasonable diagnoses are those that require as few faults as possible to explain the aberrant behavior. A minimal diagnosis is any diagnosis D () such that for no proper subset 0 is D ( 0 ) a diagnosis. <p> For instance, without this fault axiom, the two To appear, Artificial Intelligence, 1995 5 CONSISTENCY-BASED DIAGNOSIS 47 excuses AB (fbulbg) and AB (fplugg) determine diagnoses for the observation dim, as does the larger diagnosis AB (fbulb; plugg). This forms the basis for Reiter's <ref> [49] </ref> characterization of all diagnoses in terms of minimal diagnoses. <p> To appear, Artificial Intelligence, 1995 6 CONCLUDING REMARKS 51 This is important because our definition captures all CB-diagnoses. Poole's definition is based on Reiter's <ref> [49] </ref> definition of diagnosis in terms of minimal sets of abnormal components. It is not hard to see that, in fact, D () is in some extension of SD [ ffig iff D () is a minimal CB-diagnosis.
Reference: [50] <author> Raymond Reiter and Johan de Kleer. </author> <title> Foundations of assumption-based truth maintenance systems: Preliminary report. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> Seattle, </address> <year> 1987. </year>
Reference: [51] <author> Raymond Reiter and Alan K. Mackworth. </author> <title> A logical framework for depiction and image interpretation. </title> <journal> Artificial Intelligence, </journal> <volume> 41 </volume> <pages> 125-155, </pages> <year> 1989. </year>
Reference: [52] <author> Nicholas Rescher. </author> <title> Peirce's Philosophy of Science: Critical Studies in his Theory of Induction and Scientific Method. </title> <institution> University of Notre Dame Press, Notre Dame, </institution> <year> 1978. </year>
Reference-contexts: A tanker truck exploding in front of the yard is much less probable than the sprinkler being turned on. There have been proposals to address these issues in a more qualitative manner using logic-based frameworks also. Peirce (see Rescher <ref> [52] </ref>) discusses the plausibility of explanations, as do Quine and Ullian [48]. Consistency-based diagnosis [49, 16] uses abnormality assumptions to capture the context-dependence of explanations; and preferred explanations are those that minimize abnormalities. <p> Such accounts are often found in diagnosis [46, 15] and most probable explanations are discussed by Pearl [41]. In a more qualitative sense, one might require that adopted explanation (s) be among the most plausible. This view is advocated by Peirce (see Rescher <ref> [52] </ref>) and Quine and Ullian [48]. The notion of minimal diagnosis in the consistency-based models of diagnosis [49] is an attempt to qualitatively characterize most probable diagnoses. We will provide a formal framework in which such qualitative judgements of plausibility can be made.
Reference: [53] <author> Krister Segerberg. </author> <title> Modal logics with linear alternative relations. </title> <journal> Theoria, </journal> <volume> 36 </volume> <pages> 310-322, </pages> <year> 1970. </year>
Reference-contexts: Definition 2.2 [7] A CO-model is a triple M = hW; ; 'i, where M is a CT4O-model and is totally connected. In any reflexive, transitive Kripke frame, a cluster is any maximal mutually accessible set of worlds <ref> [53] </ref>: a set C W is a cluster just when v w for all v; w 2 C and no extension C 0 C has this property.
Reference: [54] <author> Murray Shanahan. </author> <title> Prediction is deduction but explanation is abduction. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1140-1145, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference: [55] <author> Yoav Shoham. </author> <title> Nonmonotonic reasoning and causation. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 213-252, </pages> <year> 1990. </year> <note> To appear, Artificial Intelligence, </note> <year> 1995 </year>
Reference-contexts: More recently, and perhaps more compelling, is the use of stratified rankings on conditional theories by Goldszmidt and Pearl [27] to represent causation. Incorporating such considerations in our model brings to mind Shoham's <ref> [55] </ref> epistemic account of causality, whereby a causal theory is expressed in terms of the knowledge of an agent, and can be nonmonotonic.
References-found: 55

