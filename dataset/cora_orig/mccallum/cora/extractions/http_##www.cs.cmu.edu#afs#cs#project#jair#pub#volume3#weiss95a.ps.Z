URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume3/weiss95a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/weiss95a.html
Root-URL: 
Email: weiss@cs.rutgers.edu  Nitin Indurkhya nitin@cs.usyd.edu.au  
Title: Rule-based Machine Learning Methods for Functional Prediction  
Author: Sholom M. Weiss 
Address: New Brunswick, New Jersey 08903, USA  Sydney Sydney, NSW 2006, AUSTRALIA  
Affiliation: Department of Computer Science, Rutgers University  Department of Computer Science, University of  
Note: Journal of Artificial Intelligence Research 3 (1995) 383-403 Submitted 6/95; published 12/95  
Abstract: We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.
Abstract-found: 1
Intro-found: 1
Reference: <author> Apte, C., Damerau, F., & Weiss, S. </author> <year> (1994). </year> <title> Automated Learning of Decison Rules for Text Categorization. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 12 (3), </volume> <pages> 233-251. </pages>
Reference-contexts: Even a small percentage gain can be quite valuable for the appropriate application <ref> (Apte, Damerau, & Weiss, 1994) </ref> and computational requirements are a secondary factor. We have provided results on several real-world datasets. Mostly, these involve nonlinear relationships. One may wonder how the rule-based method would perform on data with obvious linear relationships.
Reference: <author> Breiman, L. </author> <year> (1993). </year> <title> Stacked regression. </title> <type> Tech. rep., </type> <institution> U. of CA. Berkeley. </institution>
Reference-contexts: Finding an effective global distance measure may not be easy, particularly in the presence of many noisy features. Hence a different technique for combining the two methods is needed. 3. These weights are obtained so as to minimize the least squared error under some constraints <ref> (Breiman, 1993) </ref>.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Tress. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> Ca. </address>
Reference-contexts: Less widely known is that decision trees are also effective in regression. The CART program, developed in the statistical research community, induces both classification and regression trees <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref>. These regression trees are strictly binary trees, a representation which naturally follows from intensive modeling using continuous variables. 1 In terms of performance, regression trees often are competitive in performance to other regression methods (Breiman et al., 1984). <p> These regression trees are strictly binary trees, a representation which naturally follows from intensive modeling using continuous variables. 1 In terms of performance, regression trees often are competitive in performance to other regression methods <ref> (Breiman et al., 1984) </ref>. Regression trees are noted to be particularly strong when there are many higher order dependencies among the input variables (Friedman, 1991). The advantages of the regression tree model are similar to the advantages enjoyed by classification trees over other models. <p> The pruning strategies employed for classification trees are equally valid for regression trees. Like the covering procedures, the only substantial difference is that the error rate is measured in terms of mean absolute distance. One popular method is the weakest-link pruning strategy <ref> (Breiman et al., 1984) </ref>.
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference: <author> Craven, P., & Wahba, G. </author> <year> (1979). </year> <title> Smoothing noisy data with spline functions. estimating the correct degree of smoothing by the method of generalized cross-validation. </title> <journal> Numer. Math., </journal> <volume> 31, </volume> <pages> 317-403. </pages>
Reference: <author> Efron, B. </author> <year> (1988). </year> <title> Computer-intensive methods in statistical regression. </title> <journal> SIAM Review, </journal> <volume> 30 (3), </volume> <pages> 421-449. </pages>
Reference-contexts: With the increasing computational power of computers and with larger volumes of data, interest has grown in pursuing alternative nonlinear regression methods. Nonlinear regression models have been explored by the statistics research community and many new effective methods have emerged <ref> (Efron, 1988) </ref>, including projection pursuit (Friedman & Stuetzle, 1981) and MARS (Friedman, 1991). Methods for nonlinear regression have also been developed outside the mainstream statistics research community. A neural network trained by back-propagation (McClelland & Rumelhart, 1988) is one such model. <p> We have provided results on several real-world datasets. Mostly, these involve nonlinear relationships. One may wonder how the rule-based method would perform on data with obvious linear relationships. In our earlier experiments with data exhibiting linear relationships (for example, the drug study data <ref> (Efron, 1988) </ref>), the rule-based solutions did slightly better than trees. However, the true test is real-world data which, often involve complex non-linear relationships. Comparisons with alternative models can help assess the effectiveness of the new techniques.
Reference: <author> Fayyad, U., & Irani, K. </author> <year> (1992). </year> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pp. </pages> <address> 104-110 San Jose. </address>
Reference-contexts: For function approximation, the expectation is a smooth continuous function, but a decision tree provides discrete regions that are discontinuous at the boundaries. All in all though, regression trees often produce strong results, and for many applications their advantages strongly outweigh their potential disadvantages. 1. A comparative study <ref> (Fayyad & Irani, 1992) </ref> suggests that binary classification trees are somewhat more predictive even for categorical variables. 384 Rule-based Functional Prediction In this paper we describe a new method for inducing regression rules.
Reference: <author> Friedman, J. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19 (1), </volume> <pages> 1-141. </pages>
Reference-contexts: Nonlinear regression models have been explored by the statistics research community and many new effective methods have emerged (Efron, 1988), including projection pursuit (Friedman & Stuetzle, 1981) and MARS <ref> (Friedman, 1991) </ref>. Methods for nonlinear regression have also been developed outside the mainstream statistics research community. A neural network trained by back-propagation (McClelland & Rumelhart, 1988) is one such model. Other models c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. <p> Regression trees are noted to be particularly strong when there are many higher order dependencies among the input variables <ref> (Friedman, 1991) </ref>. The advantages of the regression tree model are similar to the advantages enjoyed by classification trees over other models. Two principal advantages can be cited: (a) dynamic feature selection and (b) explanatory capabilities. Tree induction methods are extremely effective in finding the key attributes in high dimensional applications.
Reference: <author> Friedman, J., & Stuetzle, W. </author> <year> (1981). </year> <title> Projection pursuit regression. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 76, </volume> <pages> 817-823. </pages>
Reference-contexts: With the increasing computational power of computers and with larger volumes of data, interest has grown in pursuing alternative nonlinear regression methods. Nonlinear regression models have been explored by the statistics research community and many new effective methods have emerged (Efron, 1988), including projection pursuit <ref> (Friedman & Stuetzle, 1981) </ref> and MARS (Friedman, 1991). Methods for nonlinear regression have also been developed outside the mainstream statistics research community. A neural network trained by back-propagation (McClelland & Rumelhart, 1988) is one such model. Other models c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.
Reference: <author> Weiss & Indurkhya Girosi, F., & Poggio, T. </author> <year> (1990). </year> <title> Networks and the best approximation property. </title> <journal> Biological Cybernetics, </journal> <volume> 63, </volume> <pages> 169-176. </pages>
Reference: <author> Hartigan, J., & Wong, M. </author> <year> (1979). </year> <title> A k-means clustering algorithm, ALGORITHM AS 136. </title> <journal> Applied Statistics, </journal> <volume> 28 (1). </volume>
Reference-contexts: Classes with identical means should be merged. P-Class is a variation of k-means clustering, a statistical method that minimizes a distance measure <ref> (Hartigan & Wong, 1979) </ref>. Alternative methods that do not depend on distance measures (Lebowitz, 1985) may also be used. Given a fixed number of k classes, this procedure will relatively quickly assign the y i to classes such that the overall distances are minimized.
Reference: <author> Hastie, T., & Tibshirani, R. </author> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: An analogous local optimization technique, called backfitting, has been used in the context of nonlinear statistical regression <ref> (Hastie & Tibshirani, 1990) </ref>. Variations on the selection of the next improvement move could include: 1. First local improvement encountered (such as in backfitting) 2.
Reference: <author> Jacoby, S., Kowalik, J., & Pizzo, J. </author> <year> (1972). </year> <title> Iterative Methods for Non-linear Optimization Problems. </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey. </address>
Reference-contexts: However, rule set optimization can usually be suspended until substantial segments of the covering set have already been pruned. If (1) is used, then either sequentially ordered evaluations (as in backfitting) or stochastic evaluations can be considered. Empirical evidence in the optimization literature supports the superiority of stochastic evaluation <ref> (Jacoby, Kowalik, & Pizzo, 1972) </ref>. Further improvements may be obtained by occasionally making random changes in configuration (Kirpatrick, Gelatt, & Vecchi, 1983). These are general combinatorial optimization techniques that must be substantially reworked to fit a specific problem type. Most are expected to be applied throughout problem solving.
Reference: <author> Kirpatrick, S., Gelatt, C., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <pages> 671. </pages>
Reference-contexts: If (1) is used, then either sequentially ordered evaluations (as in backfitting) or stochastic evaluations can be considered. Empirical evidence in the optimization literature supports the superiority of stochastic evaluation (Jacoby, Kowalik, & Pizzo, 1972). Further improvements may be obtained by occasionally making random changes in configuration <ref> (Kirpatrick, Gelatt, & Vecchi, 1983) </ref>. These are general combinatorial optimization techniques that must be substantially reworked to fit a specific problem type. Most are expected to be applied throughout problem solving.
Reference: <author> LeBlanc, M., & Tibshirani, R. </author> <year> (1993). </year> <title> Combining estimates in regression and classification. </title> <type> Tech. rep., </type> <institution> Department of Statistics, U. of Toronto. </institution>
Reference: <author> Lebowitz, M. </author> <year> (1985). </year> <title> Categorizing numeric information for generalization. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 285-308. </pages>
Reference-contexts: Classes with identical means should be merged. P-Class is a variation of k-means clustering, a statistical method that minimizes a distance measure (Hartigan & Wong, 1979). Alternative methods that do not depend on distance measures <ref> (Lebowitz, 1985) </ref> may also be used. Given a fixed number of k classes, this procedure will relatively quickly assign the y i to classes such that the overall distances are minimized.
Reference: <author> Lin, S., & Kernighan, B. </author> <year> (1973). </year> <title> An efficient heuristic for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 21 (2), </volume> <pages> 498-516. </pages>
Reference-contexts: Local modifications are made until no further improvements are possible. Making local changes to a configuration is a widely-used optimization technique to approximate a global optimum and has been applied quite successfully, for example to find near-optimum solutions to traveling salesman problems <ref> (Lin & Kernighan, 1973) </ref>. An analogous local optimization technique, called backfitting, has been used in the context of nonlinear statistical regression (Hastie & Tibshirani, 1990). Variations on the selection of the next improvement move could include: 1. First local improvement encountered (such as in backfitting) 2.
Reference: <author> McClelland, J., & Rumelhart, D. </author> <year> (1988). </year> <title> Explorations in Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Ma. </address>
Reference-contexts: Methods for nonlinear regression have also been developed outside the mainstream statistics research community. A neural network trained by back-propagation <ref> (McClelland & Rumelhart, 1988) </ref> is one such model. Other models c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Weiss & Indurkhya can be found in numerical analysis (Girosi & Poggio, 1990).
Reference: <author> Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <pages> pp. </pages> <address> 1041-1045 Philadelphia, Pa. </address>
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Decision rules may also offer greater explanatory capabilities than trees because as a tree grows in size, its interpretability diminishes. Among symbolic learning methods, decision tree induction, using recursive partitioning, is highly developed. Many of these methods developed within the machine learning community, such as ID3 decision tree induction <ref> (Quinlan, 1986) </ref>, have been applied exclusively to classification tasks. Less widely known is that decision trees are also effective in regression. The CART program, developed in the statistical research community, induces both classification and regression trees (Breiman, Friedman, Olshen, & Stone, 1984).
Reference: <author> Quinlan, J. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <pages> pp. 236-243. </pages>
Reference-contexts: From this perspective, classification can be thought of as a subcategory of regression. Some machine learning researchers have emphasized this connection by describing regression as "learning how to classify among continuous classes" <ref> (Quinlan, 1993) </ref>. The traditional approach to the problem is classical linear least-squares regression (Scheffe, 1959). Developed and refined over many years, linear regression has proven quite effective for many real-world applications. <p> For a continuous function and even a moderately sized sample, this approximation can lead to increased error. To deal with this limitation, instead of constant-value functions, linear functions can be substituted in a partition <ref> (Quinlan, 1993) </ref>. However, a linear function has the obvious weakness that the true function may be far from linear even in the restricted context of a single region. In general, use of such linearity compromises the highly non-parametric nature of the DNF model. <p> As suggested earlier, partitioning regression methods and k-nearest neighbor regression methods are complementary. Hence one might expect that by suitably combining the two methods, one might obtain better performance. In one recent study <ref> (Quinlan, 1993) </ref>, model trees (i.e., regression trees with linear combinations at the leaf nodes) and nearest neighbor methods were also combined. <p> Results Experiments were conducted to assess the competitiveness of rule-based regression compared to other procedures (including less interpretable ones), as well as to evaluate the performance of the integrated partition and k-nn regression method. Experiments were performed using seven datasets, six of which are described in previous studies <ref> (Quinlan, 1993) </ref>. In addition to these six datasets, new experiments were done on a very large telecommunications application, which is labeled pole. In each of the seven datasets, there was one continuous real-valued response variable. Experimental results are reported in terms of the MAD, as measured using 10-fold cross-validation. <p> For pole, all 48 features were continuous. Descriptions 395 Weiss & Indurkhya Dataset Cases Vars price 159 16 servo 167 19 cpu 209 6 peptide 431 128 housing 506 13 pole 15000 48 Table 1: Dataset Characteristics of the other datasets can be found in the literature <ref> (Quinlan, 1993) </ref>. 4 Table 1 summarizes the key characteristics of the datasets used in this study. Table 2 summarizes the original results reported (Quinlan, 1993). <p> 19 cpu 209 6 peptide 431 128 housing 506 13 pole 15000 48 Table 1: Dataset Characteristics of the other datasets can be found in the literature <ref> (Quinlan, 1993) </ref>. 4 Table 1 summarizes the key characteristics of the datasets used in this study. Table 2 summarizes the original results reported (Quinlan, 1993). These include model-trees (MT), which are regression trees with linear fits at the terminal nodes; neural nets (NNET); 3-nearest neighbors (3-nn); and the combined results of model-trees and 3-nearest neighbors (MT/3-nn). 5 Table 3 summarizes the additional results that we obtained.
Reference: <author> Ripley, B. </author> <year> (1993). </year> <title> Statistical aspects of neural networks. </title> <booktitle> In Proceedings of Seminair Eu-ropeen de Statistique London. </booktitle> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Other models c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Weiss & Indurkhya can be found in numerical analysis (Girosi & Poggio, 1990). An overview of many different regression models, with application to classification, is available in the literature <ref> (Ripley, 1993) </ref>. Most of these methods produce solutions in terms of weighted models. In the real-world, classification problems are more commonly encountered than regression problems. This accounts for the greater attention paid to classification than to regression. But many important problems in the real world are of the regression type.
Reference: <author> Scheffe, H. </author> <year> (1959). </year> <title> The Analysis of Variance. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: From this perspective, classification can be thought of as a subcategory of regression. Some machine learning researchers have emphasized this connection by describing regression as "learning how to classify among continuous classes" (Quinlan, 1993). The traditional approach to the problem is classical linear least-squares regression <ref> (Scheffe, 1959) </ref>. Developed and refined over many years, linear regression has proven quite effective for many real-world applications. Clearly the elegant and computationally simple linear model has its limits, and more complex models may fit the data better.
Reference: <author> Ting, K. </author> <year> (1994). </year> <title> The problem of small disjuncts: Its remedy in decision trees. </title> <booktitle> In Proceedings of the 10th Canadian Conference on Artificial Intelligence, </booktitle> <pages> pp. 91-97. </pages>
Reference: <author> Weiss, S., & Indurkhya, N. </author> <year> (1993a). </year> <title> Optimized Rule Induction. </title> <journal> IEEE Expert, </journal> <volume> 8 (6), </volume> <pages> 61-69. </pages>
Reference-contexts: For example, neural networks are often applied to classification problems. The issue of interpretable solutions has been an important consideration leading to development of "symbolic learning methods." A popular format for interpretable solutions is the disjunctive normal form (DNF) model <ref> (Weiss & Indurkhya, 1993a) </ref>. Decision trees and rules are examples of DNF models. Decision rules are similar in characteristics to decision trees, but they also have some potential advantages: (a) a stronger model (b) often better explanatory capabilities. Unlike trees, DNF rules need not be mutually exclusive. <p> With non-disjoint regions, several rules may be satisfied for a single sample. Some mechanism is needed to resolve the conflicts in k i , the constant values assigned, when multiple rules, R i regions, are invoked. One standard model <ref> (Weiss & Indurkhya, 1993a) </ref> is to order the rules. Such ordered rule-sets have also been referred to as decision lists. <p> Generate a set of Pseudo-classes using the P-class algorithm (Figure 4). 2. Generate a covering rule-set for the transformed classification problem using a rule induction method such as Swap-1 <ref> (Weiss & Indurkhya, 1993a) </ref>. 3. Initialize the current rule set to be the covering rule set and save it. 4. <p> When a rule is induced, its corresponding cases are removed and the remaining cases are considered. When a class has been covered, the next class is considered. An example of such a covering algorithm is that used in Swap-1 <ref> (Weiss & Indurkhya, 1993a) </ref>, and this is the procedure used in this paper. The covering method is identical for classification and regression. <p> It is a variation of the techniques used in Swap-1 <ref> (Weiss & Indurkhya, 1993a) </ref>. The central theme is to hold a model configuration constant and make a single local improvement to that configuration. Local modifications are made until no further improvements are possible.
Reference: <author> Weiss, S., & Indurkhya, N. </author> <year> (1993b). </year> <title> Rule-based regression. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <month> 1072-1078. </month> <title> 402 Rule-based Functional Prediction Weiss, </title> <editor> S., & Indurkhya, N. </editor> <year> (1994). </year> <title> Decision tree pruning: Biased or optimal?. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <pages> pp. 626-632. </pages>
Reference-contexts: For the rule-based method, the parameter m, the number of pseudo-classes, must be determined. This can be found using cross-validation or independent test cases (in our experiments, cross-validation was used). Figure 6 represents a typical plot of the relative error vs. the number of pseudo-classes <ref> (Weiss & Indurkhya, 1993b) </ref>. As the number of partitions increases, results improve until they reach a relative plateau and deteriorate somewhat. Similar complexity plots can be found for other models, for example neural nets (Weiss & Kapouleas, 1989).
Reference: <author> Weiss, S., & Kapouleas, I. </author> <year> (1989). </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 781-787 Detroit, Michigan. </address>
Reference-contexts: Figure 6 represents a typical plot of the relative error vs. the number of pseudo-classes (Weiss & Indurkhya, 1993b). As the number of partitions increases, results improve until they reach a relative plateau and deteriorate somewhat. Similar complexity plots can be found for other models, for example neural nets <ref> (Weiss & Kapouleas, 1989) </ref>. The MARS procedure has several adjustable parameters. 6 For the parameter mi, values tried were 1 (additive modeling), 2, 3, 4 and number of inputs. For df, the default value of 3.0 was tried as well the optimal value estimated by cross-validation.
Reference: <author> Weiss, S., & Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Widmer, G. </author> <year> (1993). </year> <title> Combining knowledge-based and instance-based learning to exploit qualitative knowledge. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 371-385. </pages>
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. 403 </pages>
Reference-contexts: Moreover, by combining 393 Weiss & Indurkhya different models, enhanced results may be achieved. A general approach to combining learning models is a scheme referred to as stacking <ref> (Wolpert, 1992) </ref>. Additional studies have been performed in applying the scheme to regression problems (Breiman, 1993; LeBlanc & Tibshirani, 1993). Using small training samples of simulated data, and linear combinations of regression methods, improved results were reported.
References-found: 31

