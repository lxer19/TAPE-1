URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P644.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Email: vavasis@cs.cornell.edu.  
Title: Accurate Solution of Weighted Least Squares by Iterative Methods  
Author: Elena Y. Bobrovnikova Stephen A. Vavasis 
Note: 14853. Part of this work was done while this author was visiting Lucent Bell Laboratories.  Part of this work was done while this author was visiting  
Address: Ithaca, New York  Ithaca, New York 14853,  
Affiliation: Formerly of the Center for Applied Mathematics, Cornell University,  Department of Computer Science, Cornell University,  Argonne National Laboratory.  
Date: February 6, 1997  
Abstract: We consider the weighted least-squares (WLS) problem with a very ill-conditioned weight matrix. Weighted least-squares problems arise in many applications including linear programming, electrical networks, boundary value problems, and structures. Because of roundoff errors, standard iterative methods for solving a WLS problem with ill-conditioned weights may not give the correct answer. Indeed, the difference between the true and computed solution (forward error) may be large. We propose an iterative algorithm, called MINRES-L, for solving WLS problems. The MINRES-L method is the application of MINRES, a Krylov-space method due to Paige and Saunders, to a certain layered linear system. Using a simplified model of the effects of roundoff error, we prove that MINRES-L gives answers with small forward error. We present computational experiments for some applications. fl This work has been supported in part by an NSF Presidential Young Investigator grant, with matching funds received from AT&T and Xerox Corp. Research supported in part by NSF through grant DMS-9505155 and ONR through grant N00014-96-1-0050. Support was also received from the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Dept. of Energy, under Contract W-31-109-Eng-38 through Argonne National Laboratory. Support was also received from the J. S. Guggenheim Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bjorck. </author> <title> Numerical methods for least squares problems. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: The only two algorithms in literature that are proved to have this property are the NSH algorithm by Vavasis [23] and the complete orthogonal decomposition (COD) algorithm by Hough and Vavasis [12], both of them direct. See Bjorck <ref> [1] </ref> for more information about algorithms for least-squares problems. We would like to have stable iterative methods for this problem because iterative methods can be much more efficient than direct methods for large sparse problems, which is the common setting in applications. <p> a feasible interior point (x; y; s) for this problem computes an update y to y satisfying A T DAy = A T D (s oeX 1 e); (40) where X = diag (x), S = diag (s), D = XS 1 , oe is an algorithm-dependent parameter usually in <ref> [0; 1] </ref>, is the duality gap, and e is the vector of all 1's. 29 See Wright [26]. Since (40) has the form of a WLS problem, we can obtain y using the MINRES-L algorithm. One way to compute s is via s := Ay.
Reference: [2] <author> A. Bjorck, T. Elfving, and Z. Strakos. </author> <title> Stability of conjugate gradient and Lanczos methods for linear least squares problems. </title> <type> Preprint, </type> <year> 1997. </year>
Reference-contexts: This algorithm is commonly referred to as CGNR, which is how we will denote it here. There are several variants of CGNR in the literature; see, e.g., Bjorck, Elfving, and Strakos <ref> [2] </ref>. Note that in most variants one does not form the triple product A T DA when applying CG to (2); instead, one forms matrix-vector products involving matrices A T , D and A. <p> All computations are in IEEE double precision with machine epsilon approximately 2:2 10 16 . Matlab sparse matrix operations were used in all tests. Our implementation of CGNR is based on CGLS1 as in (3.2) of Bjorck, Elfving and Strakos <ref> [2] </ref>. These authors conclude that CGLS1 is a good way to organize CGNR. There are two matrix-vector products per CGLS1 iteration, one with matrix A T D 1=2 and one with D 1=2 A.
Reference: [3] <author> E. Bobrovnikova and S. Vavasis. </author> <title> Iterative methods for weighted least squares. </title> <booktitle> Appears in Proc. Copper Mountain Conference on Iterative Methods distributed to conference participants, </booktitle> <year> 1996. </year>
Reference-contexts: A regularization technique would compute a completely different solution. In our own previous work <ref> [3] </ref>, we proposed an iterative method for (2) based on "correcting" the standard CGNR search directions.
Reference: [4] <author> P. Brown and H. Walker. </author> <title> GMRES on (nearly) singular systems. </title> <type> Technical Report UCRL-JC-115882, </type> <institution> Numerical Mathematics Group, Center for Computational Sciences and Engineering, Lawrence Livermore National Laboratory, </institution> <year> 1994. </year> <note> To appear in SIAM J. Matrix Anal. App. </note>
Reference-contexts: The question arises whether MINRES (in exact arithmetic) will find a solution of (10). MINRES can find a solution only if it lies in the Krylov space, which (because of rank deficiency) is not necessarily full dimensional. This question was answered affirmatively by Theorem 2.4 of Brown and Walker <ref> [4] </ref>. (Their analysis concerns GMRES, but the same result applies to MINRES in exact arithmetic.) Furthermore, their result states that, assuming the initial guess is 0, the computed solution (x; v) will have minimum norm over all possible solutions. <p> Note that MINRES does not necessarily select this v i;j , but because of its minimization property (that is, Theorem 2.4 of Brown and Walker <ref> [4] </ref> described in Section 4), it will select v i;j whose norm is no larger than in the preceding bound. We now can apply (11). The other factor on the right-hand side, namely, kH p k, is easily seen to be bounded by p 2 kAk 2 .
Reference: [5] <author> J. Drkosova, A. Greenbaum, M. Rozloznik, and Z. Strakos. </author> <title> Numerical stability of GMRES. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 309-330, </pages> <year> 1995. </year>
Reference-contexts: As for MINRES, less is known, but a bound like (11) is known to hold for GMRES implemented with Householder transformations <ref> [5] </ref>. GMRES is equivalent to MINRES augmented with a full reorthogonalization process. We are content to assert (11) for MINRES, with evidence coming from our computational experiments. This bound sheds light on why MINRES-L can attain much better accuracy than CGNR.
Reference: [6] <author> A. L. Forsgren. </author> <title> On linear least-squares problems with diagonally dominant weight matrices. </title> <note> Report TRITA-MAT-1995-OS2, Optimization 31 and Systems Theory, </note> <institution> Department of Mathematics, Royal Institute of Technology, </institution> <address> S-100 44 Stockholm, Sweden, </address> <year> 1995. </year>
Reference-contexts: An important property of problem (1) or (2) is the norm bound on the solution, which was obtained independently by Stewart [20], Todd [22] and several other authors. See <ref> [6] </ref> for a more complete bibliography. Here we state this result as in the paper by Stewart. Theorem 1 Let D denote the set of all positive definite m fi m real diagonal matrices. Let A be an mfin real matrix of rank n.
Reference: [7] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations, 3rd edition. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1996. </year>
Reference-contexts: Then kxk O A kbk and kAxk O A kbk for any choice of diagonal positive definite weight matrices D 1 ; : : : ; D p . 3 Previous Work The standard iterative method for least-squares problems, including WLS problems, is conjugate gradient (see Golub and Van Loan <ref> [7] </ref> or Saad [18]) applied to the normal equations (2). This algorithm is commonly referred to as CGNR, which is how we will denote it here. There are several variants of CGNR in the literature; see, e.g., Bjorck, Elfving, and Strakos [2].
Reference: [8] <author> C. Gonzaga and H. Lara. </author> <title> A note on properties of condition numbers. </title> <type> Preprint, </type> <year> 1996. </year>
Reference-contexts: Thus, Corollary 1 yields the bound kvk O A kb 1 A 1 xk = O A ( O A + 1)kbk for this choice of v. (The factor O A + 1 can be improved to O A by using the analysis of Gonzaga and Lara <ref> [8] </ref>.) Combining the x and v contributions means that we have bounded the right-hand side of (18); let us rewrite (18) with the new bound: kr 1 k; kr 2 k 2C* kAk 2 O A ( O A + 2)kbk: (20) Next, we write new equations for r 1 ;
Reference: [9] <author> A. Greenbaum. </author> <title> Estimating the attainable accuracy of recursively computed residual methods. </title> <type> Technical Report TR95-1515, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1995. </year> <note> To appear in SIAM J. Matrix Anal. App. </note>
Reference-contexts: where C is a modest constant, * is machine epsilon, and x is the true solution. (If multiple solutions exist, we take x to be the minimum-norm solution.) As far as we know, this bound has not been rigorously proved, but it is related to a bound proved by Greenbaum <ref> [9] </ref> in the case of conjugate gradient. In particular, Greenbaum's result implies that (11) would hold for CG if we were guaranteed that the recursively updated residual drops to well below machine precision, which always happens in our test cases.
Reference: [10] <author> M. Hanke. </author> <title> Conjugate gradient type methods for ill-posed problems. </title> <publisher> Long-man, Harlow, Essex, </publisher> <year> 1995. </year>
Reference-contexts: Another technique for addressing ill-conditioned linear systems with iterative methods is called "regularization"; a typical regularization technique modifies the ill-conditioned system with additional terms. See Hanke <ref> [10] </ref>. Regularization does not appear to be a good approach for solving (1) because (1) already has a well-defined solution (in particular, Theorem 1 implies that solutions are not highly sensitive to perturbation of the data vector b). A regularization technique would compute a completely different solution.
Reference: [11] <author> P. Hough. </author> <title> Stable computation of search directions for near-degenerate linear programming problems. </title> <type> Unpublished manuscript, </type> <year> 1997. </year>
Reference-contexts: In contrast, the error in all components of s arising from the product Ay is on the order of * ksk (where * is machine-epsilon). A direct method for accurately computing all components of s was proposed by Hough <ref> [11] </ref>, who obtains a bound of the form js i d s i j=s i f (A) * (41) for each i. We will consider methods for extending MINRES-L to accurate computation of s in future work.
Reference: [12] <author> P. Hough and S. Vavasis. </author> <title> Complete orthogonal decomposition for weighted least squares. </title> <type> Technical Report 94TR203, </type> <institution> Advanced Computing Research Institute, Cornell Theory Center, </institution> <year> 1994. </year> <note> To appear in SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: The only two algorithms in literature that are proved to have this property are the NSH algorithm by Vavasis [23] and the complete orthogonal decomposition (COD) algorithm by Hough and Vavasis <ref> [12] </ref>, both of them direct. See Bjorck [1] for more information about algorithms for least-squares problems. We would like to have stable iterative methods for this problem because iterative methods can be much more efficient than direct methods for large sparse problems, which is the common setting in applications. <p> The scaled error that is tabulated and plotted in all cases is defined to be k ^ x xk=kbk. We choose this particular scaling for the error because our goal is to investigate stability bound (5). The true solution x is computed using the COD method <ref> [12] </ref>. Note that the accuracy of CGNR decays as ffi 2 gets smaller, whereas MINRES-L's accuracy stays constant. MINRES-L requires many more flops than CGNR because the system matrix is larger.
Reference: [13] <author> C. Lawson and R. Hanson. </author> <title> Solving Least Squares Problems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, 1974. </address> <publisher> Republished by SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: We mention also that the two-layered WLS and LLS problems were considered in x22 of Lawson and Hanson <ref> [13] </ref>. As noted in the preceding section, the two-layered WLS problem is written in the form (7), in which the diagonal entries of D 1 ; D 2 on the order of 1 and ffi 1 ffi 2 .
Reference: [14] <author> C. Paige and M. Saunders. </author> <title> Solution of sparse indefinite systems of linear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 12 </volume> <pages> 617-629, </pages> <year> 1975. </year>
Reference-contexts: This article presents an iterative algorithm for WLS problems called MINRES-L. MINRES-L consists of applying the MINRES algorithm of Paige and Saunders <ref> [14] </ref> to a certain layered linear system. We prove that MINRES-L satisfies (5). This proof of the forward error bound for MINRES-L is based on a simplified model of how roundoff error affects Krylov space methods. <p> get 2 D 2 A 2 A T A T 1 D 1 A 1 x ! 2 D 2 b 2 1 D 1 b 1 : (10) Our algorithm, which we call MINRES-L (for MINRES "layered"), is the application of the MINRES iteration due to Paige and Saunders <ref> [14] </ref> to (10). Note that (10) is a symmetric linear system. In general, this linear system is rank deficient because if (x; v) is a solution and v 0 satisfies A 1 v 0 = A 1 v, then (x; v 0 ) is also a solution. <p> In our implementation, the CGNR iteration terminates when the scaled computed residual ks k k=kA T Dbk drops below 10 13 . Our implementation of MINRES is based on <ref> [14] </ref>, except Givens rotations were used instead of 2 fi 2 Householder matrices (so that there are some inconsequential sign differences).
Reference: [15] <author> C. Paige and M. Saunders. </author> <title> LSQR: An algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: A different framework for interpreting this difficulty is described in Section 5. Another iterative method for least-squares problems is LSQR due to Paige and Saunders <ref> [15] </ref>. This method shares the same difficulty with CGNR because it works in the same Krylov space. A standard technique for handling ill-conditioning in conjugate gradient is reorthogonalization; see, for example, Paige [16] and Parlett and 6 Scott [17].
Reference: [16] <author> C. C. Paige. </author> <title> Practical use of the symmetric Lanczos process with re-orthogonalization. </title> <journal> BIT, </journal> <volume> 10 </volume> <pages> 183-195, </pages> <year> 1970. </year>
Reference-contexts: Another iterative method for least-squares problems is LSQR due to Paige and Saunders [15]. This method shares the same difficulty with CGNR because it works in the same Krylov space. A standard technique for handling ill-conditioning in conjugate gradient is reorthogonalization; see, for example, Paige <ref> [16] </ref> and Parlett and 6 Scott [17]. Reorthogonalization, however, cannot solve the difficulty with ill-conditioning in (2) because even the act of forming the first Krylov vector A T Db causes a loss of information.
Reference: [17] <author> B. Parlett and D. Scott. </author> <title> The Lanczos algorithm with selective reorthog-onalization. </title> <journal> Math. Comp., </journal> <volume> 33 </volume> <pages> 217-238, </pages> <year> 1979. </year> <month> 32 </month>
Reference-contexts: This method shares the same difficulty with CGNR because it works in the same Krylov space. A standard technique for handling ill-conditioning in conjugate gradient is reorthogonalization; see, for example, Paige [16] and Parlett and 6 Scott <ref> [17] </ref>. Reorthogonalization, however, cannot solve the difficulty with ill-conditioning in (2) because even the act of forming the first Krylov vector A T Db causes a loss of information.
Reference: [18] <author> Y. Saad. </author> <title> Iterative methods for sparse linear systems. </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: A kbk and kAxk O A kbk for any choice of diagonal positive definite weight matrices D 1 ; : : : ; D p . 3 Previous Work The standard iterative method for least-squares problems, including WLS problems, is conjugate gradient (see Golub and Van Loan [7] or Saad <ref> [18] </ref>) applied to the normal equations (2). This algorithm is commonly referred to as CGNR, which is how we will denote it here. There are several variants of CGNR in the literature; see, e.g., Bjorck, Elfving, and Strakos [2].
Reference: [19] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: A generalized minimum residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: The convergence plots are depicted in The excessive number of iterations required by MINRES is apparently caused by a loss of orthogonality in the Lanczos process. To verify this hypothesis, we ran GMRES on the same layered matrix. GMRES <ref> [19] </ref> on a symmetric matrix is equivalent to MINRES with full reorthogonalization. (In exact arithmetic the two algorithms are identical.) We call this algorithm GMRES-L. The same termination tests were used.
Reference: [20] <author> G. W. Stewart. </author> <title> On scaled projections and pseudoinverses. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 112 </volume> <pages> 189-193, </pages> <year> 1989. </year>
Reference-contexts: An important property of problem (1) or (2) is the norm bound on the solution, which was obtained independently by Stewart <ref> [20] </ref>, Todd [22] and several other authors. See [6] for a more complete bibliography. Here we state this result as in the paper by Stewart. Theorem 1 Let D denote the set of all positive definite m fi m real diagonal matrices.
Reference: [21] <author> G. Strang. </author> <title> A framework for equilibrium equations. </title> <journal> SIAM Review, </journal> <volume> 30 </volume> <pages> 283-297, </pages> <year> 1988. </year>
Reference-contexts: The normal equations for (1) have the form A T DAx = A T Db: (2) Weighted least-squares problems arise in several application domains including linear programming, electrical power networks, elliptic boundary value problems and structural analysis, as observed by Strang <ref> [21] </ref>. This article focuses on the case when matrix D is severely ill-conditioned. This happens in certain classes of electrical power networks.
Reference: [22] <author> M. J. Todd. </author> <title> A Dantzig-Wolfe-like variant of Karmarkar's interior-point linear programming algorithm. </title> <journal> Operations Research, </journal> <volume> 38 </volume> <pages> 1006-1018, </pages> <year> 1990. </year>
Reference-contexts: An important property of problem (1) or (2) is the norm bound on the solution, which was obtained independently by Stewart [20], Todd <ref> [22] </ref> and several other authors. See [6] for a more complete bibliography. Here we state this result as in the paper by Stewart. Theorem 1 Let D denote the set of all positive definite m fi m real diagonal matrices. Let A be an mfin real matrix of rank n.
Reference: [23] <author> S. A. Vavasis. </author> <title> Stable numerical algorithms for equilibrium systems. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 </volume> <pages> 1108-1131, </pages> <year> 1994. </year>
Reference-contexts: Since the bounds (3), (4) exist, we can hope that there exist algorithms for (2) that possess the same property, namely, the forward error bound does not depend on D. We will call these algorithms stable, where stability, as defined by Vavasis <ref> [23] </ref>, means that forward error in the computed solution ^ x satisfies kx ^ xk * f (A) kbk; (5) where * is machine precision and f (A) is some function of A not depending on D. <p> The only two algorithms in literature that are proved to have this property are the NSH algorithm by Vavasis <ref> [23] </ref> and the complete orthogonal decomposition (COD) algorithm by Hough and Vavasis [12], both of them direct. See Bjorck [1] for more information about algorithms for least-squares problems. <p> RNAI matrices arise in the analysis of an electrical network with batteries and resistors; see <ref> [23] </ref>. They also arise in network flow problems. In the case of Figure 1, the column corresponding to 22 group of tests. The column corresponding to the top node is deleted. <p> Thus, A is an 18 fi 9 matrix. It is well known that the RNAI matrix for a connected graph always has full rank. RNAI matrices are known to have small values of O A and O A <ref> [23] </ref>. In all these tests, the weight matrix has two layers. We took D 1 = I, D 2 = I, and ffi 1 = 1, while we let ffi 2 vary from experiment to experiment.
Reference: [24] <author> S. A. Vavasis. </author> <title> Stable finite elements for problems with wild coefficients. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 33 </volume> <pages> 890-916, </pages> <year> 1996. </year>
Reference-contexts: The conductivity field c is 1 on the outer part of the domain and is 10 12 on the darker triangles. As discussed in <ref> [24] </ref>, this type of problem gives rise to a weighted least-squares problem in which A encodes information about the geometry and D encodes the ill-conditioned conductivity field. The values of O A and O A for this matrix are not known, although bounds are known for variants of these parameters. <p> The values of O A and O A for this matrix are not known, although bounds are known for variants of these parameters. The particular matrix A is 652 fi 136. The right-hand side vector b was chosen according to the Dirichlet boundary conditions described in <ref> [24] </ref>. The MINRES-L method for this problem gave scaled error of 1:3 10 13 after 382 iterations and 6.5 mflops. To compute the true solution, we used the NSHI method in [24]. <p> The right-hand side vector b was chosen according to the Dirichlet boundary conditions described in <ref> [24] </ref>. The MINRES-L method for this problem gave scaled error of 1:3 10 13 after 382 iterations and 6.5 mflops. To compute the true solution, we used the NSHI method in [24]. In this case, surprisingly, CGNR gave almost as accurate an answer, but the termination test was never activated. (We cut off CGNR after 10n iterations.) The residual of CGNR is quite oscillatory as depicted in Figure 6.
Reference: [25] <author> S. A. Vavasis and Y. Ye. </author> <title> A primal-dual interior point method whose running time depends only on the constraint matrix. </title> <journal> Mathematical Programming, </journal> <volume> 74 </volume> <pages> 79-120, </pages> <year> 1996. </year>
Reference-contexts: These spaces are defined recursively: L 0 = R n , and L l = fminimizers of kD 1=2 Finally, x, the solution to the LLS problem, is the unique element in L p . The layered least-squares problem was first introduced by Vavasis and Ye <ref> [25] </ref> as a technique for accelerating the convergence of interior-point methods. They also established the result mentioned above in this paragraph: the solution to the WLS problem in the limit as ffi l+1 =ffi l ! 0 for all l converges to the solution of the LLS problem.
Reference: [26] <author> S. J. Wright. </author> <title> Primal-Dual Interior-Point Methods. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1997. </year> <month> 33 </month>
Reference-contexts: A T DAy = A T D (s oeX 1 e); (40) where X = diag (x), S = diag (s), D = XS 1 , oe is an algorithm-dependent parameter usually in [0; 1], is the duality gap, and e is the vector of all 1's. 29 See Wright <ref> [26] </ref>. Since (40) has the form of a WLS problem, we can obtain y using the MINRES-L algorithm. One way to compute s is via s := Ay.
References-found: 26

