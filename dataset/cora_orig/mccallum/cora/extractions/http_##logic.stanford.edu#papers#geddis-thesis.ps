URL: http://logic.stanford.edu/papers/geddis-thesis.ps
Refering-URL: http://logic.stanford.edu/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: CACHING AND NON-HORN INFERENCE IN MODEL ELIMINATION THEOREM PROVERS  
Author: Donald F. Geddis 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: June 1995  
Abstract-found: 0
Intro-found: 1
Reference: [AS91] <author> Owen L. Astrachan and Mark E. Stickel. </author> <title> Caching and lemma use in model elimination theorem provers. </title> <type> Technical Note 513, </type> <institution> SRI International, </institution> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: Assuming that the original P (x) had been completely explored, it is also possible to conclude that there are no additional bindings for y that are solutions to P (y). Stickel <ref> [AS91] </ref> has used subgoal caching. 3.6 Generalized Subgoal Caching Just like answer caching, it is possible to generalize the matching criteria for subgoal caching. Assume P (x,y) is a cached subgoal, and the solutions P (1,10), P (1,11), CHAPTER 3. HORN-CLAUSE CACHING 36 P (2,20), and P (2,21) are found. <p> For example, Astrachan and Stickel <ref> [AS91, Section 4.4] </ref> write In non-Horn problems A-literals can contribute to the solution of a goal via the reduction operation. Thus for non-Horn problems a goal cannot be considered in isolation, but must be considered in the context of the A-literals in the chain.
Reference: [Ast92] <author> Owen L. Astrachan. </author> <title> Investigations in Model Elimination Based Theorem Proving. </title> <type> PhD thesis, </type> <institution> Duke University, Durham, North Carolina, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This reinforces the correspondence with the stack data structure that they represent. 2. Literals in the chains are written as the complement of the usual notation. This allows an easier mapping between the chains and the meson-style proof graphs. (The formal description below is adapted from Astrachan's description <ref> [Ast92] </ref>.) 2.4.1 Inference Rules The ME procedure is a set of three inference rules that operate on a structure called a chain. A chain is an ordered list of literals, representing a set of goals to be proved. <p> We do not consider partial caching in this CHAPTER 3. HORN-CLAUSE CACHING 37 thesis; more information on the impact of lemmaizing, and a collection of references, is available in Astrachan's thesis <ref> [Ast92, chapter 7] </ref>. 3.8 Utility Analysis Caching seems like an intuitively appealing augmentation of an inference system. It has the potential for recognizing redundancy in problem solving, and saving arbitrary amounts of time and space thereby.
Reference: [BF94] <author> Peter Baumgartner and Ulrich Furbach. </author> <title> Model elimination without contra-positives. </title> <booktitle> In 12th International Conference on Automated Deduction (CADE-12), </booktitle> <pages> pages 87-101, </pages> <address> Germany, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: While this modified format may be useful for near-Horn theories, it is not feasible for strongly non-Horn theories. Ideally we would like to implement effective caching in a non-Horn goal-directed theorem prover like weak me, without sacrificing completeness. This is the topic of chapter 4. Baumgartner and Furbach <ref> [BF94] </ref> propose a similar scheme called restart model 10 This example is from Plaisted [Pla88]. CHAPTER 4. NON-HORN FAILURE CACHING 84 elimination. Restart model elimination is even more restricted than the simple problem reduction format, while still remaining complete.
Reference: [Bla68] <author> F. Black. </author> <title> A deductive question-answering system. </title> <editor> In M. Minsky, editor, </editor> <booktitle> Semantic Information Processing, </booktitle> <pages> pages 354-402. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1968. </year>
Reference-contexts: Horn clause inference, then, we can simply defer to the work referenced there in order to establish the soundness and completeness properties. 5.4 Related Work 5.4.1 Recursion Control In this section we describe an algorithm that was formally proposed by Smith [SGG86], having earlier been discovered independently by both Black <ref> [Bla68] </ref> and McKay and CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 96 Shapiro [MS81]. Consider the database in table 5.3 that has two ground facts about paths between three cities, and one rule stating that the Path relation is transitive. The goal is to find all paths from city A.
Reference: [Bry90] <author> Fran~cois Bry. </author> <title> Query evaluation in recursive databases: bottom-up and top-down reconciled. </title> <journal> Data & Knowledge Engineering, </journal> <volume> 5(4) </volume> <pages> 289-312, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The resulting algorithm is essentially the same in either case. 5 These are programs where each variable in the head of a rule appears in the body. CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 106 Bry has given a common framework, the Backward Fixing Procedure <ref> [Bry90] </ref>, which unifies the top-down and bottom-up approaches for Horn clause inference. The framework shows the equivalence of particular bottom-up magic set methods 6 and particular top-down memoing approaches 7 . Magic sets arose in the deductive database community. <p> The technique of magic sets (section 5.4.3) adds filtering subgoals to each forward-inference rule, such that only relevant subgoals are derived when processing the rule set bottom-up. As shown by Bry <ref> [Bry90] </ref>, these two approaches result in the same fundamental computations. The postponement caching described in this chapter is a (failed) attempt to combine the failure caching results from chapter 4 with the top-down approach of mem-oing, so that caching can be effective in non-Horn theorem proving.
Reference: [Ged] <author> Donald F. Geddis. dtp: </author> <title> Don's Theorem Prover. </title> <note> Available online as &lt;url:http://logic.stanford.edu/dtp/&gt;. </note>
Reference-contexts: Appendix A Implementation The caching strategies mentioned in this thesis have been implemented in a model elimination-style theorem prover called dtp <ref> [Ged] </ref>. The source code is written in Common Lisp with some CLtL2 [Ste90] extensions (e.g. the LOOP macro). It was developed under Franz Allegro CL 4.2.beta.0 on a Sun Sparc, and occasionally tested on MCL 2.0p2 (Apple Macintosh) and Lucid HP Common Lisp Rev.
Reference: [Ged95] <author> Donald F. Geddis. </author> <title> Caching and Non-Horn Inference in Model Elimination Theorem Provers. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1995. </year> <note> Available online as &lt;url:http://logic.stanford.edu/papers/geddis-thesis.ps&gt;. 135 BIBLIOGRAPHY 136 </note>
Reference-contexts: The development of a new caching scheme for Horn theories (postponement caching). 3. The negative result that, despite the extension of failure caching, the new post ponement caching scheme is not complete for non-Horn theories. This thesis is also available online as a technical report <ref> [Ged95] </ref>. Chapter 2 Inference 2.1 Introduction Automated reasoning for first-order logic is somewhat of an odd field. Because of the well-known semi-decidable nature of logical inference, any system which attempts to determine whether a query follows from a set of sentences is in trouble from the outset.
Reference: [Gen83] <author> Michael R. Genesereth. mrs: </author> <title> A metalevel representation system. </title> <type> Technical Report HPP-83-28, </type> <institution> Knowledge Systems Laboratory, Stanford University, </institution> <year> 1983. </year>
Reference-contexts: It is the search space data structure which allows top-down algorithms to realize that the result of any computation on the other branches can at best yield a solution which is a duplicate of the one 8 The MRS system of Genesereth <ref> [Gen83] </ref> allowed allowed the definition of an entire logical metatheory to describe how to solve particular baselevel subgoals. CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 110 already known. Since bottom-up approaches don't have access to this data structure, they are forced to explore all possible subspaces.
Reference: [Gin93a] <author> Matthew L. Ginsberg. </author> <title> Dynamic backtracking. </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> 1 </volume> <pages> 25-46, </pages> <year> 1993. </year> <note> Available online as &lt;url:ftp://t.uoregon.edu/papers/dynamic.dvi&gt;. JAIR is published online as &lt;url:news:comp.ai.jair.papers&gt; and &lt;url:http://www.cs.washington.edu/research/jair/home.html&gt;. </note>
Reference-contexts: The proof space is shown in figure 2.4. 8 Other candidate algorithms include GSAT, min-conflicts, dependency-directed backtracking, and dynamic backtracking <ref> [Gin93a] </ref>. 9 The example presented only requires database lookup to solve. Note that this is only for clarity of explanation; the extension to inference merely requires solving the individual conjuncts as subgoals rather than looking up their solutions in a database. CHAPTER 2. <p> This is valid because, in order for the overall proof to succeed through this line, the goal G must be established from the 10 The attempt to avoid re-doing such work is the inspiration behind the dynamic backtracking algorithm <ref> [Gin93a] </ref>. 11 Displacement can be generalized to search for an identical subgoal which is the descendent of a sibling of an ancestor, instead of only the sibling itself (as long as the ancestor has only a single possible expansion on the path to the descendent). CHAPTER 2.
Reference: [Gin93b] <author> Matthew L. Ginsberg. </author> <booktitle> Essentials of Artificial Intelligence. </booktitle> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1993. </year>
Reference-contexts: Postponement caching does recursion control by slaving new subgoals to identical parents. The initial goal Outrun (Lion,food) resolves with the first two rules in the database. Assuming we explore the transitive rule first, we now have a conjunctive subgoal: 1 This example is from Ginsberg <ref> [Gin93b, exercise 8.14] </ref>. CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 88 Outrun (Lion,medium) and Outrun (medium,food) To solve the conjunction, work begins with the first conjunct. At this point a standard backward chainer would resolve the subgoal Outrun (Lion,medium) with the same two database sentences above, and the process would loop indefinitely.
Reference: [GN87] <author> Michael R. Genesereth and Nils J. Nilsson. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1987. </year>
Reference-contexts: Any sentence in first order predicate calculus can be converted to a logically equivalent sentence in clausal form 2 . The conversion procedure is described in most automated reasoning texts; see, for example, the descriptions by Loveland [Lov78, section 1.5], Nilsson [Nil80, section 4.2.1], or Genesereth and Nilsson <ref> [GN87, section 4.1] </ref>. The steps involved are: 1. Eliminate implication symbols 2. Reduce the scope of negation symbols 3. Standardize variables apart 4. Eliminate existential quantifiers (by Skolemization) 5. Convert to prenex form 6. Put in conjunctive normal form 7. Eliminate universal quantifiers 8. Eliminate And 9.
Reference: [Kor85] <author> Richard E. Korf. </author> <title> Depth-first iterative deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1) </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: This complexity advantage is occurring without the need for bottom-up inference; all of these searches remain top-down, focussed on the goal. The search strategy used in inference also can cause repetition. For example, iterative deepening <ref> [Kor85] </ref> is a commonly used search strategy. <p> a minimal set of inference rules and without special reasoning for equality, is sound and complete for full first-order inference. 2.5 Refinements In this section we briefly mention some common enhancements to the model elimination inference procedure, and how those enhancements interact with caching strategies. 2.5.1 Iterative Deepening Iterative deepening <ref> [Kor85] </ref> is a common search strategy, where a space is searched with a maximum bound (say, the length of the path from the root to the subgoal). If no answer is found, the depth bound is increased and the search is restarted. <p> This is born out in practice, where inference algorithms with unbounded caches typically show negative utility over the same algorithms with no caching at all. As a first step, the space needs of the underlying blind search mechanism need to be accounted for. Iterative deepening <ref> [Kor85] </ref> is a good solution to this problem: Its space requirements are only linear in the depth of the tree searched (the same as depth-first search), its time is only slightly worse than breadth-first search, and (like breadth-first search) it finds the shallowest solution in the space. <p> complex answers must be propagated in any case, as solutions using reductions occur throughout first-order model elimination proofs, and such solutions indicate nothing about the truth of intermediate subgoals. 4.6 Depth Bounds Depth-bounded search is a common way to control inference, especially via a complete mechanism like depth-first iterative deepening <ref> [Kor85] </ref>. Using a depth bound complicates the process of adding a cache to an inference algorithm.
Reference: [Kor92] <author> Richard E. Korf. </author> <title> Linear-space best-first search: Summary of results. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <pages> pages 533-538, </pages> <address> Menlo Park, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: HORN-CLAUSE CACHING 38 really have time to explore an exponential amount of the inference space, but that doesn't mean it is acceptable to use space linear in the amount of time spent. As Richard Korf writes <ref> [Kor92] </ref> on the subject of blind search techniques, Since best-first search stores all generated nodes in the Open or Closed lists, its space complexity is the same as its time complexity, which is typically exponential.
Reference: [LG88] <author> Doug Lenat and R. V. Guha. </author> <title> The world according to CYC. </title> <type> MCC Technical Report ACA-AI-300-88, </type> <institution> MCC, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Database scenarios generally involve relatively small rule sets and very large sets of ground facts (the EDB relations). AI scenarios often involve large sets of rules with only a few ground facts, such as the CYC commonsense knowledge base <ref> [LG88] </ref>. In the latter case, answering a query may only involve a small fraction of the database rules. Bottom-up approaches require rewriting each rule in the database, whether relevant to the query or not. To be fair, this isn't as big a cost as one might imagine.
Reference: [Llo87] <author> John W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <address> Ger-many, 2nd edition, </address> <year> 1987. </year>
Reference-contexts: The technique is similar to the use of a lifting lemma introduced by Robinson [Rob65], and used, for example, by Lloyd <ref> [Llo87] </ref>. The proofs in the section essentially duplicate those of section 4.4.2, with the additional notion of a binding list to map completions from one part of the space to completions in another part.
Reference: [Lov78] <author> Donald W. Loveland. </author> <title> Automated Theorem Proving: A Logical Basis. </title> <publisher> North-Holland Publishing Company, </publisher> <year> 1978. </year>
Reference-contexts: It seems unfortunate to lose the benefits of caching shown in the previous examples, just because a more expressive language is required for some application. In this thesis, we explore the topic of caching in non-Horn theorem proving (in particular, by augmenting the model elimination inference algorithm <ref> [Lov78, section 3.6] </ref>). An outline of the thesis is as follows: Chapter 2 gives a very brief overview of logic and model elimination. This differs from the standard presentation only in the formatting of chains, as described in section 2.4. <p> The number of re-solvants grew very quickly, and so work proceeded on restrictions of resolution (i.e. removing certain resolvants which were possible in the original formulation) which were still complete. One of these was refined by Loveland <ref> [Lov78] </ref> into a very streamlined backward-chaining algorithm called model elimination. The model elimination procedure focussed on individual literals rather than the clauses of resolution. A very efficient prolog-like implementation by Stickel [Sti89] led to the current enormous popularity of this form of automated inference. <p> Any sentence in first order predicate calculus can be converted to a logically equivalent sentence in clausal form 2 . The conversion procedure is described in most automated reasoning texts; see, for example, the descriptions by Loveland <ref> [Lov78, section 1.5] </ref>, Nilsson [Nil80, section 4.2.1], or Genesereth and Nilsson [GN87, section 4.1]. The steps involved are: 1. Eliminate implication symbols 2. Reduce the scope of negation symbols 3. Standardize variables apart 4. Eliminate existential quantifiers (by Skolemization) 5. Convert to prenex form 6. <p> :P (x) _ :P (y) _ P (F (x,y)) :P (x) _ :P (G (x)) :P (x 2 ) _ Q (x 2 ,G (x 2 )) 2.4 Model Elimination The ideas and examples in this thesis are presented as augmentations to a complete form of the problem reduction framework <ref> [Lov78, chapter 6] </ref> that Loveland calls the meson procedure. A theorem prover using the framework searches an and/or tree of literals, where each subsequent level is created by backward chaining on some parent literal, or else by a resolution between a literal and one of its ancestor literals. <p> A theorem prover using the framework searches an and/or tree of literals, where each subsequent level is created by backward chaining on some parent literal, or else by a resolution between a literal and one of its ancestor literals. Loveland has shown <ref> [Lov78, section 6.2] </ref> that such a framework is equivalent to weak model elimination [Lov78, section 3.6]. As weak me is often more convenient CHAPTER 2. INFERENCE 16 to prove properties about, some of the formal results in this thesis will be about augmentations of the weak me procedure. <p> Loveland has shown [Lov78, section 6.2] that such a framework is equivalent to weak model elimination <ref> [Lov78, section 3.6] </ref>. As weak me is often more convenient CHAPTER 2. INFERENCE 16 to prove properties about, some of the formal results in this thesis will be about augmentations of the weak me procedure. <p> A complete description of model elimination, proofs of completeness, and several variants are given by Loveland <ref> [Lov78] </ref>. The presentation here differs from the standard one in two ways: 1. Chains are written with the most recent literals on the left and the oldest to the right, and right justified in the text. This reinforces the correspondence with the stack data structure that they represent. 2. <p> When we arrive at conjunct F the second time, we attempt to solve F (6,4), and this succeeds. Thus the query is proved, with the bindings w!6, j!2, k!5, and x!4. 2.5.5 Goal Displacement Loveland <ref> [Lov78, section 6.1] </ref> describes a problem reduction format which is similar to weak ME, and a "device of convenience" that can be added called displacement. <p> The subgoal B is completes successfully in the left branch by displacement, because it has a sibling ancestor in the conjunction C and B. B ( D D Table 2.6: Goal displacement Displacement can be effective in the propositional case. For first-order inference, CHAPTER 2. INFERENCE 31 Loveland <ref> [Lov78, section 6.2] </ref> writes [Goal displacement] demands even more care in [first-order] use than for the propositional case. As before, its usefulness is simply in delaying pursuit of a goal. <p> HORN-CLAUSE POSTPONEMENT CACHING 94 5.3 Formal Results Postponement caching for Horn theories is an augmentation of the Horn clause version of Loveland's meson procedure 3 <ref> [Lov78] </ref>. (The Horn version of meson merely eliminates the reduction operation.) Definition 32 (Postponement caching) Begin with the standard meson procedure. In addition, maintain a separate cache table. <p> To convert this to an effective inference engine for non-Horn theories, we must instead describe the top-down behavior of a non-Horn inference engine. There are many such algorithms, but one of the most popular (and the one considered in this thesis) is model elimination <ref> [Lov78] </ref>. With the addition of a single rule of inference, namely a reduction (goal-goal resolution) between a subgoal and an ancestor of that subgoal, a Prolog-like inference engine becomes complete for non-Horn theories. CHAPTER 6. NON-HORN POSTPONEMENT CACHING 128 Stickel [Sti94] has given such a bottom-up rewriting for non-Horn inference.
Reference: [MS81] <author> D. P. McKay and S. Shapiro. </author> <title> Using active connection graphs for reasoning with recursive rules. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence (IJCAI-81), </booktitle> <pages> pages 368-374, </pages> <address> Vancouver, BC, </address> <year> 1981. </year> <note> BIBLIOGRAPHY 137 </note>
Reference-contexts: HORN-CLAUSE POSTPONEMENT CACHING 96 Shapiro <ref> [MS81] </ref>. Consider the database in table 5.3 that has two ground facts about paths between three cities, and one rule stating that the Path relation is transitive. The goal is to find all paths from city A.
Reference: [Nil80] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: Nonetheless, the class of questions that are answerable has been growing steadily. 2.2 First Order Predicate Calculus This section contains a brief background of mathematical logic. For more details see any introductory text on logic or artificial intelligence (for example, Nilsson's <ref> [Nil80, chapter 4] </ref>). 2.2.1 Syntax A symbol is a sequence of alphabetic characters. Each symbol is assigned to exactly one of the following classes: predicate, variable, function, object. <p> Any sentence in first order predicate calculus can be converted to a logically equivalent sentence in clausal form 2 . The conversion procedure is described in most automated reasoning texts; see, for example, the descriptions by Loveland [Lov78, section 1.5], Nilsson <ref> [Nil80, section 4.2.1] </ref>, or Genesereth and Nilsson [GN87, section 4.1]. The steps involved are: 1. Eliminate implication symbols 2. Reduce the scope of negation symbols 3. Standardize variables apart 4. Eliminate existential quantifiers (by Skolemization) 5. Convert to prenex form 6. Put in conjunctive normal form 7. <p> Eliminate implication symbols 2. Reduce the scope of negation symbols 3. Standardize variables apart 4. Eliminate existential quantifiers (by Skolemization) 5. Convert to prenex form 6. Put in conjunctive normal form 7. Eliminate universal quantifiers 8. Eliminate And 9. Rename variables (standardize apart again) An example from Nilsson <ref> [Nil80] </ref> is 8x ( P (x) ) ( 8y ( P (y) ) P (F (x,y)) ) ^ After each of the procedure's steps, the corresponding result is 1. 8x ( :P (x) _ ( 8y ( :P (y) _ P (F (x,y)) ) ^ 2 This is sometimes referred to
Reference: [Pla88] <author> David A. Plaisted. </author> <title> Non-horn clause logic programming without contraposi-tives. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 4(3) </volume> <pages> 287-325, </pages> <year> 1988. </year>
Reference-contexts: Stickel [Sti94] writes: A refinement [Pla90] of the model elimination procedure that uses negative but not positive ancestor goals may make looking up solutions in the cache succeed more frequently, but probably still not often enough. CHAPTER 4. NON-HORN FAILURE CACHING 82 4.7.2 Problem Reduction Format Plaisted <ref> [Pla88] </ref> also suggested an approach completely different from model elimination, the modified problem reduction format. This algorithm is complete for non-Horn theories, without using contrapositives or reductions. Since there are no context-sensitive reduction operations, caching is easy and much like the Horn case. <p> Ideally we would like to implement effective caching in a non-Horn goal-directed theorem prover like weak me, without sacrificing completeness. This is the topic of chapter 4. Baumgartner and Furbach [BF94] propose a similar scheme called restart model 10 This example is from Plaisted <ref> [Pla88] </ref>. CHAPTER 4. NON-HORN FAILURE CACHING 84 elimination. Restart model elimination is even more restricted than the simple problem reduction format, while still remaining complete.
Reference: [Pla90] <author> David A. Plaisted. </author> <title> A sequent style model elimination strategy and a positive refinement. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 6(4) </volume> <pages> 389-402, </pages> <year> 1990. </year>
Reference-contexts: Indeed, this modification to caching is sound, but now the question of utility become paramount. Astrachan and Stickel continue: Examination of "chain dumps" for several non-Horn problems indicates that cache hits would be very rare and this method does not appear viable for non-Horn problems (Plaisted <ref> [Pla90] </ref> noted this as a potential problem with caching using model elimination). Consider a snapshot of the search tree for a particular theorem. For Horn problems, nodes in the search tree (unexpanded and-nodes that correspond to subgoals) can be considered for caching independently of the position at which they occur. <p> The expanded proof space to depth 3 is shown in figure 4.19. A proof of the query without failure caching is shown in figure 4.20. 9 Private communication. CHAPTER 4. NON-HORN FAILURE CACHING 81 4.7 Related Work 4.7.1 Positive Refinement Plaisted's positive refinement <ref> [Pla90] </ref> of model elimination established that only positive subgoals need to be checked for reductions with their ancestors, and the algorithm would remain complete. This refinement is especially attractive for near-Horn problems, because ancestor lists then contain few negative subgoals, and the reduction check is computationally cheap. <p> While the positive refinement makes cache hits more likely, it appears that in many cases they are still not likely enough, and the net result is that the benefit from caching is never realized. Stickel [Sti94] writes: A refinement <ref> [Pla90] </ref> of the model elimination procedure that uses negative but not positive ancestor goals may make looking up solutions in the cache succeed more frequently, but probably still not often enough. CHAPTER 4.
Reference: [Pla94] <author> David A. Plaisted. </author> <title> The search efficiency of theorem proving strategies. </title> <booktitle> In 12th International Conference on Automated Deduction (CADE-12), </booktitle> <pages> pages 57-71, </pages> <address> Germany, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: It is dangerous to generalize experimental evidence from one domain to others, as the results may depend on the domain in some critical and as yet not understood way. Nonetheless, the approach of a bounded size cache seems promising. Plaisted <ref> [Pla94] </ref> also ran extensive experiments with various kinds of theorem provers and caching strategies. Some general observations were: The backward chaining strategies are goal-sensitive, but are mostly inefficient. Forward chaining strategies, though efficient for Horn clauses, are not goal-sensitive.
Reference: [Ram91] <author> R. Ramakrishnan. </author> <title> Magic templates: A spellbinding approach to logic programs. </title> <journal> Journal of Logic Programming, </journal> <volume> 11(3-4):189-216, </volume> <year> 1991. </year>
Reference-contexts: Direct &lt;A,B&gt; &lt;C,B&gt; &lt;B,D&gt; Path-Call &lt;C,end&gt; &lt;B,end&gt; &lt;D,end&gt; Path &lt;C,B&gt; &lt;B,D&gt; &lt;C,D&gt; Finally, a select can be done on the Path relation to return the results for our original query Path (C,end), yielding the solutions Path (C,B) and Path (C,D) The algorithm presented here is essentially the Magic Template algorithm <ref> [Ram91] </ref>, which is a generalization of the Magic Sets algorithm to handle tuples containing variables. Handling variables requires a more complex (and perhaps less efficient) mechanism than relational-algebra processors normally contain.
Reference: [RLK86] <author> J. Rohmer, R. Lescur, and J. M. Kerisit. </author> <title> The Alexander method: A technique for the processing of recursive axioms in deductive databases. </title> <journal> New Generation Computing, </journal> <volume> 4(3), </volume> <year> 1986. </year>
Reference-contexts: The magic set algorithm [Ull89] in deductive databases provides much the same kind of control of infinite recursive spaces, by means of an automatic reformulation. (A similar rewriting of the given rules is the basis for the Alexander method <ref> [RLK86] </ref>.) The standard way to evaluate database queries is to evaluate them as expressions, using the relational operations to combine component relations. This is a bottom-up strategy that calculates new relations by combining old ones.
Reference: [Rob65] <author> J. A. Robinson. </author> <title> A machine-oriented logic based on the rsolution principle. </title> <journal> Journal of the Association for Computing Machinery (JACM), </journal> <volume> 12(1) </volume> <pages> 23-41, </pages> <month> January </month> <year> 1965. </year>
Reference-contexts: In many problems of interest, however, the number of entities in the universe of discourse grows so quickly that a generate-and-test checker of this kind is quickly overwhelmed. A huge leap was made by Robinson <ref> [Rob65] </ref>, who realized that by propagating 10 CHAPTER 2. INFERENCE 11 constraints in the form of variable binding lists, infinite sets of domain objects could be checked in a single step. His unification concept, the key to this summarizing computation, has formed the basis for further automated provers. <p> The technique is similar to the use of a lifting lemma introduced by Robinson <ref> [Rob65] </ref>, and used, for example, by Lloyd [Llo87]. The proofs in the section essentially duplicate those of section 4.4.2, with the additional notion of a binding list to map completions from one part of the space to completions in another part.
Reference: [SGG86] <author> David E. Smith, Michael R. Genesereth, and Matthew L. Ginsberg. </author> <title> Controlling recursive inference. </title> <journal> Artificial Intelligence, </journal> <volume> 30(3) </volume> <pages> 343-389, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: In the case of Horn clause inference, then, we can simply defer to the work referenced there in order to establish the soundness and completeness properties. 5.4 Related Work 5.4.1 Recursion Control In this section we describe an algorithm that was formally proposed by Smith <ref> [SGG86] </ref>, having earlier been discovered independently by both Black [Bla68] and McKay and CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 96 Shapiro [MS81]. Consider the database in table 5.3 that has two ground facts about paths between three cities, and one rule stating that the Path relation is transitive. <p> This choice of control may allow for much better performance on some problems. Further analysis on the relationship of magic sets and postponement caching in the case of non-Horn theories can be found in section 6.3. 5.4.4 Other techniques The material in this section is adapted from Smith's description <ref> [SGG86, section 1.2] </ref>. The primary benefit of the schemes discussed in this chapter is the transformation of some infinite search spaces (typically resulting from recursive rules) into finite spaces. There are other techniques to address the problem of infinite recursion, but all have characteristics that limit their applicability.
Reference: [Spe90] <author> Bruce Spencer. </author> <title> Avoiding duplicate proofs. </title> <editor> In Saumya Debray and Manuel Hermenegildo, editors, </editor> <booktitle> Logic Programming: Proceedings of the 1990 North American Conference, </booktitle> <pages> pages 569-584, </pages> <address> Cambridge, Massachusetts, 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: In addition, restart model elimination includes the negative literals along paths, which Baumgartner and Furbach have found to be valuable information during experiments. 4.7.3 Foothold Format Backward-chaining proof spaces have a large amount of duplication in them: proofs of a given solution occur multiple times in the space. Spencer <ref> [Spe90] </ref> suggests an inference procedure which avoids duplicate proofs during its search. (The following description has been adapted from Spencer's description.) The basic idea behind the foothold format is to avoid redundant proofs that arise when reasoning by cases by breaking up the symmetry of the cases.
Reference: [SS93] <author> A. M. Segre and D. Scharstein. </author> <title> Bounded-overhead caching for definite-clause theorem proving. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 11(1) </volume> <pages> 83-113, </pages> <month> August </month> <year> 1993. </year> <note> BIBLIOGRAPHY 138 </note>
Reference-contexts: The next subgoal is S (2). This succeeds immediately, because it is an instance of a literal in the success cache (S (x)). And thus the overall goal has been proven. Segre has used generalized answer caching in some reported experiments <ref> [SS93] </ref>. 3.5 Subgoal Caching A somewhat more sophisticated scheme is to record the actual generalized subgoals along with the set of known instances. When attempting to prove a subgoal, say P (x), a cache entry is made. <p> With good indexing, the per node overhead time costs can be made logarithmic in the size of the cache, although even this isn't good enough if the cache grows without bound. Fortunately, there is a simple alternative: the size cache can be limited to an arbitrary fixed size. Segre <ref> [SS93] </ref> has reported experimental evidence (on a large number of problems from the TPTP library [SSY93]) that bounded-sized caches show almost the same effectiveness (in terms of ability to prune the space searched) as infinite-sized caches, but with a well-bounded overhead on both total space used and per node time cost.
Reference: [SSY93] <author> C. B. Suttner, G. Sutcliffe, and T. Yemenis. </author> <title> The TPTP Problem Library (TPTP v1.0.0). </title> <type> Technical Report FKI-184-93, </type> <institution> Institut fur Informatik, Tecnische Universitat Munchen, Munich, Germany, </institution> <year> 1993. </year> <note> Available online as &lt;url:http://wwwjessen.informatik.tu-muenchen.de/ suttner/tptp.html&gt;. Contact Geoff Sutcliffe at &lt;url:mailto:geoff@coral.cs.jcu.edu.au&gt;. </note>
Reference-contexts: For one thing, it is the average case which matters, but (like elsewhere in computer science) often only worst case analysis (or none at all) is available. In inference, coming up with useful problem distributions is problematic, making empirical data suspect. There exists a large collection <ref> [SSY93] </ref> of theorem proving problems, but to a large extent these are problems that have been generated to illustrate particular inference systems. It is difficult to say in what sense they represent objectively important problems. <p> Fortunately, there is a simple alternative: the size cache can be limited to an arbitrary fixed size. Segre [SS93] has reported experimental evidence (on a large number of problems from the TPTP library <ref> [SSY93] </ref>) that bounded-sized caches show almost the same effectiveness (in terms of ability to prune the space searched) as infinite-sized caches, but with a well-bounded overhead on both total space used and per node time cost. <p> IMPLEMENTATION 134 for more information. (Of course, some mechanism for viewing the output is also needed: either a postscript previewer like the public domain unix utility ghostview, or else a postscript printer.) A large collection of theorem proving examples is the TPTP (Thousands of Problems for Theorem Provers) collection <ref> [SSY93] </ref>. It is available on the World Wide Web at &lt;URL:http://wwwjessen.informatik.tu-muenchen.de/~suttner/tptp.html&gt;
Reference: [Ste90] <author> Guy L. Steele Jr. </author> <title> Common Lisp: The Language. </title> <publisher> Digital Press, </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: Appendix A Implementation The caching strategies mentioned in this thesis have been implemented in a model elimination-style theorem prover called dtp [Ged]. The source code is written in Common Lisp with some CLtL2 <ref> [Ste90] </ref> extensions (e.g. the LOOP macro). It was developed under Franz Allegro CL 4.2.beta.0 on a Sun Sparc, and occasionally tested on MCL 2.0p2 (Apple Macintosh) and Lucid HP Common Lisp Rev.
Reference: [Sti88] <author> Mark E. Stickel. </author> <title> A prolog technology theorem prover: Implementation by an extended prolog compiler. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 4(4) </volume> <pages> 353-380, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: It is valuable to note that careful choice must be made of what to bound. In pttp <ref> [Sti88] </ref>, Stickel iterates on the number of subgoals used during the proof, with a few 4 Actually, if the goal is purely conjunctive, and if it is propositional, then model elimination is complete even if the negated goal is not added to the theory. CHAPTER 2. INFERENCE 21 minor modifications.
Reference: [Sti89] <author> Mark E. Stickel. </author> <title> A prolog technology theorem prover: A new exposition and implementation in prolog. </title> <type> Technical Note 464, </type> <institution> SRI International, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: One of these was refined by Loveland [Lov78] into a very streamlined backward-chaining algorithm called model elimination. The model elimination procedure focussed on individual literals rather than the clauses of resolution. A very efficient prolog-like implementation by Stickel <ref> [Sti89] </ref> led to the current enormous popularity of this form of automated inference. All of these formulations are still bound by the theoretical restriction: there are some queries for which they will run forever. <p> As weak me is often more convenient CHAPTER 2. INFERENCE 16 to prove properties about, some of the formal results in this thesis will be about augmentations of the weak me procedure. Weak model elimination behaves very much like a version of prolog with a few modifications. pttp <ref> [Sti89] </ref>, is an implementation of this complete version of prolog where * The occurs check is added to the unification routine, so that the unifications are sound. * Successfully resolved ancestors remain on the chain (albeit specially marked), so that they may participate in future reduction operations.
Reference: [Sti94] <author> Mark E. Stickel. </author> <title> Upside-down meta-interpretation of the model elimination theorem-proving procedure for deduction and abduction. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 13(2) </volume> <pages> 189-210, </pages> <year> 1994. </year>
Reference-contexts: It is, perhaps, not surprising that such paths are not often candidates for cache retrieval. . . . Our intuition leads us to believe that caching may still not be viable for any large class of non-Horn problems. Stickel repeats this assessment in a later publication <ref> [Sti94] </ref>: Caching will surely be more complicated and less effective for the full model elimination procedure than for the Prolog subset on which it has CHAPTER 4. NON-HORN FAILURE CACHING 42 been successfully tested. <p> In fact, for Horn theories, no reductions take place at all. While the positive refinement makes cache hits more likely, it appears that in many cases they are still not likely enough, and the net result is that the benefit from caching is never realized. Stickel <ref> [Sti94] </ref> writes: A refinement [Pla90] of the model elimination procedure that uses negative but not positive ancestor goals may make looking up solutions in the cache succeed more frequently, but probably still not often enough. CHAPTER 4. <p> With the addition of a single rule of inference, namely a reduction (goal-goal resolution) between a subgoal and an ancestor of that subgoal, a Prolog-like inference engine becomes complete for non-Horn theories. CHAPTER 6. NON-HORN POSTPONEMENT CACHING 128 Stickel <ref> [Sti94] </ref> has given such a bottom-up rewriting for non-Horn inference. The description here is adapted from his. 2 The metatheoretic predicate Fact has two arguments: a literal and a set of an cestor subgoals sufficient to prove it.
Reference: [Ull88] <author> Jeffrey D. Ullman. </author> <title> Principles of Database and Knowledge-Base Systems, Volume I. </title> <publisher> Computer Science Press, </publisher> <address> 1803 Research Boulevard, Rockville, MD 20850, </address> <year> 1988. </year>
Reference-contexts: When generating the tuples for the next iteration, no new tuples are generated by using only old tuples computed on previous iterations. 4 4 This approach is called semi-nave bottom-up evaluation <ref> [Ull88] </ref>. CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 103 Another source of inefficiency is that this strategy may compute many tuples that are completely irrelevant to the query, which is only used at the final step to determine which of the computed tuples provide an answer via a selection.
Reference: [Ull89] <author> Jeffrey D. Ullman. </author> <title> Principles of Database and Knowledge-Base Systems, Volume II: The New Technologies. </title> <publisher> Computer Science Press, </publisher> <address> 1803 Research Boulevard, Rockville, MD 20850, </address> <year> 1989. </year>
Reference-contexts: In the case of Horn databases, postponement caching is essentially equivalent to OLDT applied to a version of prolog that has sound unification and no cut operation. 5.4.3 Magic Sets This description of magic sets is adapted from a description by Warren [War92, pp. 99-101]. The magic set algorithm <ref> [Ull89] </ref> in deductive databases provides much the same kind of control of infinite recursive spaces, by means of an automatic reformulation. (A similar rewriting of the given rules is the basis for the Alexander method [RLK86].) The standard way to evaluate database queries is to evaluate them as expressions, using the <p> It is also superior to simple top-down approaches, which recompute the solutions to subgoals whenever the recur in the search space. The question remains, though, how magic sets compare to more sophisticated top-down inference engines with caching, such as logic programming memoing or postponement caching. Ullman writes <ref> [Ull89] </ref>: There are a number of reasons why bottom-up calculation is preferable to top-down. 1. . . . Top-down calculation . . . can get trapped in infinite loops and never find the answer. . . . 2. . . .
Reference: [War92] <author> David S. Warren. </author> <title> Memoing for logic programs. </title> <journal> Communications of the ACM, </journal> <volume> 35(3) </volume> <pages> 93-111, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Contrast, for example, the space in figure 5.10, which is the full search space using recursion control, with figure 5.11, which is the smaller full space using postponement caching. 5.4.2 Logic Programming In logic programming, Warren <ref> [War92] </ref> gives an inference algorithm called OLDT. Warren describes prolog as being a non-deterministic language, where the interpreter carries out a depth-first search through the tree of possible alternative executions. In this scheme, OLDT is simply adding memoing to this procedural interpreter. <p> Warren describes prolog as being a non-deterministic language, where the interpreter carries out a depth-first search through the tree of possible alternative executions. In this scheme, OLDT is simply adding memoing to this procedural interpreter. As Warren writes in <ref> [War92, pg. 97] </ref>: Intuitively, we think of a machine that is carrying out a nondeterministic procedure as duplicating itself at a point of choice, and as disappearing when it encounters failure. Thus at any time, we have a set of deterministic CHAPTER 5. HORN-CLAUSE POSTPONEMENT CACHING 100 machines computing away. <p> In the case of Horn databases, postponement caching is essentially equivalent to OLDT applied to a version of prolog that has sound unification and no cut operation. 5.4.3 Magic Sets This description of magic sets is adapted from a description by Warren <ref> [War92, pp. 99-101] </ref>.
References-found: 35

