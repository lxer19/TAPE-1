URL: http://www.cs.mu.oz.au/tr_db/mu_96_41.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Effective Clustering of Records for Fast Query Processing  
Author: Kotagiri Ramamohanarao Evan P. Harris 
Address: Parkville, Victoria 3052 Australia  
Affiliation: Department of Computer Science The University of Melbourne  
Pubnum: Technical Report 96/41  
Abstract: We apply a more realistic cost model than has often been used in the past to the problems of: (1) implementing join algorithms, (2) implementing relational operations which make use of multi-attribute hash files, and (3) creating better multi-attribute hash file organisations using knowledge of the queries which will be asked. This results in a substantial improvement in performance. For some problems a performance improvement of the order of 39 is achieved. We provide preliminary evidence which indicates that even better results are possible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously. In [11, 12] we describe how simulated annealing <ref> [1] </ref> and various heuristic techniques can be used to find good, if not optimal, choice vectors and buffer sizes. This approach can also be used with other join algorithms.
Reference: [2] <author> A. V. Aho and J. D. Ullman. </author> <title> Optimal partial-match retrieval when fields are independently specified. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 168-179, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique <ref> [2, 6, 20, 21, 24] </ref>, although other, more complex, operations have also been suggested [4, 10, 13, 14, 26].
Reference: [3] <author> C. C. Chang, R. C. T. Lee, and M. W. Du. </author> <title> Symbolic Gray code as a perfect multiattribute hashing scheme for partial match queries. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 8(3) </volume> <pages> 235-249, </pages> <month> May </month> <year> 1982. </year> <month> 17 </month>
Reference-contexts: This is an area of continuing research. Other optimisations which can be used, in addition to those we have discussed in previous sections, are the use of Gray codes to interpret the choice vectors <ref> [3, 6] </ref> and the reading of unnecessary pages to improve performance [25]. The idea behind the use of Gray codes is to take the output of using the choice vector, the list of disk page addresses, and apply the Gray code transformation to them.
Reference: [4] <author> C. Y. Chen, C. C. Chang, and R. C. T. Lee. </author> <title> Optimal MMI file systems for orthogonal range queries. </title> <journal> Information Systems, </journal> <volume> 18(1) </volume> <pages> 37-54, </pages> <year> 1993. </year>
Reference-contexts: Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested <ref> [4, 10, 13, 14, 26] </ref>. In this paper, we present cost formulae for implementing relational operations which make use of both an improved buffering scheme and any MAH indexes 2 to reduce the cost of the relational operations. <p> This technique has been applied to the problem of partial-match retrieval for point queries [20, 21] and range queries <ref> [4, 13] </ref>, multiple files [24], and join queries [14]. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. Therefore, the partitioning phase of GRACE hash join, and other similar joins, can be reduced or eliminated. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously.
Reference: [5] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, USA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: These formulae allow us to decide how best to use our main-memory buffer when executing a given operation. We show that this can result in a substantial reduction in the cost of joining relations. Further reductions in the cost of hash-based algorithms, such as hash joins <ref> [5, 17, 22] </ref>, can be achieved by making use of any clustering present in the data file storing the records of the relation. One can take this a step further by designing a file clustering explicitly to reduce the cost of various operations. <p> Subsequent hash-based join algorithms have been concerned with increasing its performance for (often common) boundary conditions. For example, the hybrid hash join <ref> [5] </ref> performs best when the smaller relation is a small multiple of the main-memory buffer size, while bucket size tuning [16] improves the performance in the presence of data skew. The GRACE hash join works in two phases.
Reference: [6] <author> C. Faloutsos. </author> <title> Multiattribute hashing using Gray codes. </title> <booktitle> In Proceedings of the 1986 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 227-238, </pages> <year> 1986. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique <ref> [2, 6, 20, 21, 24] </ref>, although other, more complex, operations have also been suggested [4, 10, 13, 14, 26]. <p> This is an area of continuing research. Other optimisations which can be used, in addition to those we have discussed in previous sections, are the use of Gray codes to interpret the choice vectors <ref> [3, 6] </ref> and the reading of unnecessary pages to improve performance [25]. The idea behind the use of Gray codes is to take the output of using the choice vector, the list of disk page addresses, and apply the Gray code transformation to them. <p> For example, the choice vector 11fl1 requires the reading of pages 13 and 15. By applying the Gray code transformation to these addresses, pages 9 and 10 are required. Faloutsos <ref> [6] </ref> reported that using Gray codes can result in a 0% to 50% improvement in the cost of partial-match retrieval using MAH files. Our preliminary results, using the more accurate cost model of Equation 1, indicates that 0% to 5% is the likely improvement.
Reference: [7] <author> M. Freeston. </author> <title> The BANG file: a new kind of grid file. </title> <booktitle> In Proceedings of the 1987 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 260-269, </pages> <address> San Francisco, California, USA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file <ref> [7] </ref>. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested [4, 10, 13, 14, 26].
Reference: [8] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In Section 4, we discuss preliminary findings which indicate that further improvements over the results in Section 3 should be possible. In the final section we present our conclusions. 2 Join algorithms There are four basic types of join algorithm <ref> [8, 22] </ref>: the nested loop join, the sort-merge join, the hash join, and index-based joins. The nested loop join is 2 typically used when one of the two relations being joined is small, typically at most a small multiple of the main-memory buffer size.
Reference: [9] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: B 1 = B 2 and B 2 = B R = 1. Hagmann's buffers set B 1 = B 2 = (B 1)=2 and B R = 1, and were suggested in <ref> [9] </ref>. In this example, the size of the inner relation is fixed at 781 Mbytes, the result relation at 78 Mbytes, and the amount of main-memory buffer cache at 32 Mbytes. Even for the smallest relations, some improvement is achieved.
Reference: [10] <author> L. Harada, M. Nakano, M. Kitsuregawa, and M. Takagi. </author> <title> Query processing method for multi-attribute clustered relations. </title> <booktitle> In Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 59-70, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested <ref> [4, 10, 13, 14, 26] </ref>. In this paper, we present cost formulae for implementing relational operations which make use of both an improved buffering scheme and any MAH indexes 2 to reduce the cost of the relational operations.
Reference: [11] <author> E. P. Harris. </author> <title> Towards optimal storage design for efficient query processing in relational database systems. </title> <type> Technical Report 94/31, </type> <institution> Department of Computer Science, The University of Melbourne, </institution> <address> Parkville, Victoria 3052, Australia, </address> <month> December </month> <year> 1994. </year> <type> Ph.D. thesis. </type> <note> Revised: </note> <month> May </month> <year> 1995. </year>
Reference-contexts: In this section, we use the join algorithms presented in the last section as our example relational operation implementations, but this approach can be used for implementations of many other relational operations, such as intersection, union and quotient <ref> [11] </ref>. Many forms of clustering can reduce the cost of a join operation, providing the clustering is based on a join attribute. <p> This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously. In <ref> [11, 12] </ref> we describe how simulated annealing [1] and various heuristic techniques can be used to find good, if not optimal, choice vectors and buffer sizes. This approach can also be used with other join algorithms. <p> In [11, 12] we describe how simulated annealing [1] and various heuristic techniques can be used to find good, if not optimal, choice vectors and buffer sizes. This approach can also be used with other join algorithms. For example, in <ref> [11] </ref> we present cost formulae and optimisation techniques for use with the hybrid hash join algorithm. cost which is possible by using MAH files with choice vectors selected by the minimisation algorithms. "Standard files" assumes that the records in each relation are stored in a flat file and are not clustered
Reference: [12] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Towards optimal multi-attribute indexes for relations in a database system. </title> <note> Submitted for publication. </note>
Reference-contexts: This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously. In <ref> [11, 12] </ref> we describe how simulated annealing [1] and various heuristic techniques can be used to find good, if not optimal, choice vectors and buffer sizes. This approach can also be used with other join algorithms. <p> It contains 8 relations and 17 queries on these relations. The relations were assumed to be very large, the largest over 600 Gbytes. With an 8 kbyte page size, the length of that relation's choice vector was 27 bits. More details regarding this distribution can be found in <ref> [12] </ref>. The times used were based on the (now antiquated) Sun SPARCstation 10/30 and Wren 6 disk drive.
Reference: [13] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Optimal dynamic multi-attribute hashing for range queries. </title> <journal> BIT, </journal> <volume> 33(4) </volume> <pages> 561-579, </pages> <year> 1993. </year>
Reference-contexts: Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested <ref> [4, 10, 13, 14, 26] </ref>. In this paper, we present cost formulae for implementing relational operations which make use of both an improved buffering scheme and any MAH indexes 2 to reduce the cost of the relational operations. <p> This technique has been applied to the problem of partial-match retrieval for point queries [20, 21] and range queries <ref> [4, 13] </ref>, multiple files [24], and join queries [14]. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. Therefore, the partitioning phase of GRACE hash join, and other similar joins, can be reduced or eliminated. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously.
Reference: [14] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Using optimized multi-attribute hash indexes for hash joins. </title> <booktitle> In Proceedings of the 5th Australasian Database Conference, </booktitle> <pages> pages 92-111, </pages> <address> Christchurch, New Zealand, Jan-uary 1994. </address> <publisher> Global Publications Services. </publisher>
Reference-contexts: Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested <ref> [4, 10, 13, 14, 26] </ref>. In this paper, we present cost formulae for implementing relational operations which make use of both an improved buffering scheme and any MAH indexes 2 to reduce the cost of the relational operations. <p> This technique has been applied to the problem of partial-match retrieval for point queries [20, 21] and range queries [4, 13], multiple files [24], and join queries <ref> [14] </ref>. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. Therefore, the partitioning phase of GRACE hash join, and other similar joins, can be reduced or eliminated. This substantially reduces the cost of the join. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously.
Reference: [15] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Join algorithm costs revisited. </title> <journal> The VLDB Journal, </journal> <volume> 5(1) </volume> <pages> 64-84, </pages> <year> 1996. </year> <month> 18 </month>
Reference-contexts: The sort-merge join and hash joins are typically used for large relations. In this paper, to determine the effectiveness of the cost model, we concentrate on the nested loop join and a particular hash join, the GRACE hash join [17]. We discuss other join algorithms in <ref> [15] </ref>. 2.1 Nested loop join The nested loop join is the simplest of all the join algorithms. <p> This allows us to choose any value for B 1 , B 2 and B R , providing we do not exceed the amount of available memory. In <ref> [15] </ref> we presented an algorithm which finds values for B 1 , B 2 and B R given V 1 , V 2 , V R and B to minimise Equation 6. <p> We are again subject to the constraint that B 1 + B 2 + B R B , and now have the additional constraint that P B P + B I B . In <ref> [15] </ref> we showed that this latter constraint can be relaxed to P B P = B I and P B P + 2P 1 B by partitioning the relations in-place. We then presented an algorithm which finds values for the remaining variables to minimise the cost of Equation 8. <p> The results in Figure 2 show that using an effective buffer allocation is crucial for achieving the best performance in the implementation of the GRACE hash join algorithm. In <ref> [15] </ref> we describe cost formulae for the sort-merge and hybrid hash joins, compare the join algorithms and the times taken by the various minimisation algorithms.
Reference: [16] <author> M. Kitsuregawa, M. Nakayama, and M. Takagi. </author> <title> The effect of bucket size tuning in the dynamic hybrid GRACE hash join method. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 257-266, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Subsequent hash-based join algorithms have been concerned with increasing its performance for (often common) boundary conditions. For example, the hybrid hash join [5] performs best when the smaller relation is a small multiple of the main-memory buffer size, while bucket size tuning <ref> [16] </ref> improves the performance in the presence of data skew. The GRACE hash join works in two phases.
Reference: [17] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1) </volume> <pages> 66-74, </pages> <year> 1983. </year>
Reference-contexts: These formulae allow us to decide how best to use our main-memory buffer when executing a given operation. We show that this can result in a substantial reduction in the cost of joining relations. Further reductions in the cost of hash-based algorithms, such as hash joins <ref> [5, 17, 22] </ref>, can be achieved by making use of any clustering present in the data file storing the records of the relation. One can take this a step further by designing a file clustering explicitly to reduce the cost of various operations. <p> The sort-merge join and hash joins are typically used for large relations. In this paper, to determine the effectiveness of the cost model, we concentrate on the nested loop join and a particular hash join, the GRACE hash join <ref> [17] </ref>. We discuss other join algorithms in [15]. 2.1 Nested loop join The nested loop join is the simplest of all the join algorithms. <p> The results in Figure 1 show that using an effective buffer allocation is crucial for achieving high performance in the implementation of the nested loop join algorithm. 4 varies. 2.2 Partitioning and the GRACE hash join The GRACE hash join <ref> [17] </ref> was the first of the hash-based join algorithms. Subsequent hash-based join algorithms have been concerned with increasing its performance for (often common) boundary conditions.
Reference: [18] <author> P. A. Larson. </author> <title> Linear hashing with partial expansions. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 224-232, </pages> <address> Montreal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: This hash key is then used to store the record in a file indexed using this key, such as a linear hash file <ref> [18, 19] </ref>. To form the single hash key for a record with n attributes, the following two steps are performed. First, a (possibly different) hash function is applied to the value of each attribute. This results in n hash values, which are treated as bit strings.
Reference: [19] <author> W. Litwin. </author> <title> Linear hashing: a new tool for file and table addressing. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 212-223, </pages> <address> Montreal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: This hash key is then used to store the record in a file indexed using this key, such as a linear hash file <ref> [18, 19] </ref>. To form the single hash key for a record with n attributes, the following two steps are performed. First, a (possibly different) hash function is applied to the value of each attribute. This results in n hash values, which are treated as bit strings.
Reference: [20] <author> J. W. Lloyd. </author> <title> Optimal partial-match retrieval. </title> <journal> BIT, </journal> <volume> 20 </volume> <pages> 406-413, </pages> <year> 1980. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique <ref> [2, 6, 20, 21, 24] </ref>, although other, more complex, operations have also been suggested [4, 10, 13, 14, 26]. <p> This technique has been applied to the problem of partial-match retrieval for point queries <ref> [20, 21] </ref> and range queries [4, 13], multiple files [24], and join queries [14]. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously. <p> The cost reported is the average query cost of all possible queries, each of which was randomly allocated a different probability. The "Old cost model" calculates the bit allocation method using the method in <ref> [20, 21] </ref>. The "New cost model" finds a choice vector using simulated annealing and the costs are calculated using formulae derived from Equation 1. The "New + Gray codes + read unnecessary" adds both Gray codes and reading unnecessary pages to the "New cost model".
Reference: [21] <author> J. W. Lloyd and K. Ramamohanarao. </author> <title> Partial-match retrieval for dynamic files. </title> <journal> BIT, </journal> <volume> 22 </volume> <pages> 150-168, </pages> <year> 1982. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique <ref> [2, 6, 20, 21, 24] </ref>, although other, more complex, operations have also been suggested [4, 10, 13, 14, 26]. <p> This technique has been applied to the problem of partial-match retrieval for point queries <ref> [20, 21] </ref> and range queries [4, 13], multiple files [24], and join queries [14]. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously. <p> The cost reported is the average query cost of all possible queries, each of which was randomly allocated a different probability. The "Old cost model" calculates the bit allocation method using the method in <ref> [20, 21] </ref>. The "New cost model" finds a choice vector using simulated annealing and the costs are calculated using formulae derived from Equation 1. The "New + Gray codes + read unnecessary" adds both Gray codes and reading unnecessary pages to the "New cost model".
Reference: [22] <author> P. Mishra and M. H. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: These formulae allow us to decide how best to use our main-memory buffer when executing a given operation. We show that this can result in a substantial reduction in the cost of joining relations. Further reductions in the cost of hash-based algorithms, such as hash joins <ref> [5, 17, 22] </ref>, can be achieved by making use of any clustering present in the data file storing the records of the relation. One can take this a step further by designing a file clustering explicitly to reduce the cost of various operations. <p> In Section 4, we discuss preliminary findings which indicate that further improvements over the results in Section 3 should be possible. In the final section we present our conclusions. 2 Join algorithms There are four basic types of join algorithm <ref> [8, 22] </ref>: the nested loop join, the sort-merge join, the hash join, and index-based joins. The nested loop join is 2 typically used when one of the two relations being joined is small, typically at most a small multiple of the main-memory buffer size.
Reference: [23] <author> J. Nievergelt, H. Hinterberger, and K. C. Sevcik. </author> <title> The grid file: An adaptable, symmetric multikey file structure. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(1) </volume> <pages> 38-71, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: One can take this a step further by designing a file clustering explicitly to reduce the cost of various operations. Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file <ref> [23] </ref>, multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested [4, 10, 13, 14, 26].
Reference: [24] <author> K. Ramamohanarao, J. Shepherd, and R. Sacks-Davis. </author> <title> Multi-attribute hashing with multiple file copies for high performance partial-match retrieval. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 404-423, </pages> <year> 1990. </year>
Reference-contexts: Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique <ref> [2, 6, 20, 21, 24] </ref>, although other, more complex, operations have also been suggested [4, 10, 13, 14, 26]. <p> This technique has been applied to the problem of partial-match retrieval for point queries [20, 21] and range queries [4, 13], multiple files <ref> [24] </ref>, and join queries [14]. If join attributes contribute bits to the choice vector of a relation, the choice vector can be used to implicitly partition the relation. Therefore, the partitioning phase of GRACE hash join, and other similar joins, can be reduced or eliminated. <p> Equation 11. 3.4 Results The problem of finding optimal choice vectors and values for all of the variables presented in Sections 3.2 and 3.3 is much more difficult that the prob lem presented in Section 2 and the previous problems which try to find 13 optimal choice vectors presented in <ref> [4, 13, 14, 20, 21, 24] </ref>. This is because the problems of finding choice vectors and memory buffer sizes are combined, and the choice vectors of multiple relations must be optimised simultaneously.
Reference: [25] <author> B. Seeger, P. A. Larson, and R. McFadyen. </author> <title> Reading a set of disk pages. </title> <booktitle> In Proceedings of the Nineteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 592-603, </pages> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This is an area of continuing research. Other optimisations which can be used, in addition to those we have discussed in previous sections, are the use of Gray codes to interpret the choice vectors [3, 6] and the reading of unnecessary pages to improve performance <ref> [25] </ref>. The idea behind the use of Gray codes is to take the output of using the choice vector, the list of disk page addresses, and apply the Gray code transformation to them.
Reference: [26] <author> J. A. Thom, K. Ramamohanarao, and L. Naish. </author> <title> A superjoin algorithm for deductive databases. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 189-196, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file [28] and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested <ref> [4, 10, 13, 14, 26] </ref>. In this paper, we present cost formulae for implementing relational operations which make use of both an improved buffering scheme and any MAH indexes 2 to reduce the cost of the relational operations.
Reference: [27] <institution> Transaction Processing Performance Council (TPC), </institution> <address> 777 N. First Street, Suite 600, San Jose, CA 95112-6311, </address> <month> USA. </month> <title> TPC Benchmark D (Decision Support) Standard Specification, Revision 1.0, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: The schema and query distribution used in Figure 5 were based on the TPC-D benchmark <ref> [27] </ref>. It contains 8 relations and 17 queries on these relations. The relations were assumed to be very large, the largest over 600 Gbytes. With an 8 kbyte page size, the length of that relation's choice vector was 27 bits. More details regarding this distribution can be found in [12].
Reference: [28] <author> K.-Y. Whang and R. Krishnamurthy. </author> <title> The multilevel grid file | a dynamic hierarchical multidimensional file structure. </title> <booktitle> In International 19 Symposium on Database Systems for Advanced Applications, pages 449--459, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: One can take this a step further by designing a file clustering explicitly to reduce the cost of various operations. Multi-attribute hashing 1 is one clustering method which is ideal for this task. Other multi-attribute data structures are also appropriate, such as the grid file [23], multi-level grid file <ref> [28] </ref> and BANG file [7]. Partial-match retrieval for point queries is the most obvious application of this technique [2, 6, 20, 21, 24], although other, more complex, operations have also been suggested [4, 10, 13, 14, 26].
References-found: 28

