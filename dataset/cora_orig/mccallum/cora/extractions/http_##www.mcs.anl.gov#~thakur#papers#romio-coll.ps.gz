URL: http://www.mcs.anl.gov/~thakur/papers/romio-coll.ps.gz
Refering-URL: http://www.mcs.anl.gov/romio/
Root-URL: http://www.mcs.anl.gov
Email: luskg@mcs.anl.gov  
Title: Data Sieving and Collective I/O in ROMIO  
Author: Rajeev Thakur William Gropp Ewing Lusk fthakur, gropp, 
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Note: Appeared in Proc. of the 7th Symposium on the Frontiers of Massively Parallel Computation, February 1999, pp. 182189. c 1999 IEEE.  
Abstract: The I/O access patterns of parallel programs often consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, MPI-IO allows users to access a noncontiguous data set with a single I/O function call. This feature provides MPI-IO implementations an opportunity to optimize data access. We describe how our MPI-IO implementation, ROMIO, delivers high performance in the presence of noncontiguous requests. We explain in detail the two key optimizations ROMIO performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how one can implement these optimizations portably on multiple machines and file systems, control their memory requirements, and also achieve high performance. We demonstrate the performance and portability with performance results for three applicationsan astrophysics-application template (DIST3D), the NAS BTIO benchmark, and an unstructured code (UNSTRUC)on five different parallel machines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 2, 7, 9, 10] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably if applications access data by making many small I/O requests.
Reference: [2] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 2, 7, 9, 10] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably if applications access data by making many small I/O requests.
Reference: [3] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 5670, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, 21(5):3138, De-cember 1993. </note>
Reference-contexts: Such optimization is broadly referred to as collective I/O. Collective I/O can be performed at the disk level (disk-directed I/O [5]), at the server level (server-directed I/O [8]), or at the client level (two-phase I/O <ref> [3] </ref>). Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in [11]. 4.1 Two-Phase I/O Two-phase I/O was first proposed in [3] in the context of <p> client level (two-phase I/O <ref> [3] </ref>). Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in [11]. 4.1 Two-Phase I/O Two-phase I/O was first proposed in [3] in the context of accessing distributed arrays from files. Consider the example of reading a two-dimensional array from a file into a (block,block) distribution in memory, as shown in Figure 2. Assume that the array is stored in the file in row-major order.
Reference: [4] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIOA Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: DIST3D, a template representing the I/O access pattern in an astrophysics application (ASTRO3D) from the University of Chicago; 2. the NAS BTIO benchmark <ref> [4] </ref>; and 3. an unstructured code (UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Lab oratories. The I/O in DIST3D consists of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order. The BTIO benchmark [4] simulates <p> benchmark <ref> [4] </ref>; and 3. an unstructured code (UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Lab oratories. The I/O in DIST3D consists of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order. The BTIO benchmark [4] simulates the I/O required by a time-stepping flow solver that periodically writes its solution matrix. The benchmark only performs writes, but we modified it to perform reads also.
Reference: [5] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1):4174, </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: Such optimization is broadly referred to as collective I/O. Collective I/O can be performed at the disk level (disk-directed I/O <ref> [5] </ref>), at the server level (server-directed I/O [8]), or at the client level (two-phase I/O [3]). Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [6] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: Such is the case when parallel applications perform I/O by using the Unix read and write functions, which can access only a single contiguous chunk of data at a time. MPI-IO, the I/O part of the MPI-2 standard <ref> [6] </ref>, is a new interface designed specifically for portable, high-performance parallel I/O. To avoid the above-mentioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call. <p> The process is also assured that concurrent writes from processes other than those involved in this collective-I/O operation will not occur, because MPI-IO's consistency semantics <ref> [6] </ref> do not automatically guarantee consistency for such writes. (In such cases, users must use MPI File sync and ensure that the operations are not concurrent.) 4.2.3 Performance Issues Even if I/O is performed in large contiguous chunks, the performance of the collective-I/O implementation can be significantly affected by the amount
Reference: [7] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10):10751089, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 2, 7, 9, 10] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably if applications access data by making many small I/O requests.
Reference: [8] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Such optimization is broadly referred to as collective I/O. Collective I/O can be performed at the disk level (disk-directed I/O [5]), at the server level (server-directed I/O <ref> [8] </ref>), or at the client level (two-phase I/O [3]). Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [9] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 4959. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 2, 7, 9, 10] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably if applications access data by making many small I/O requests.
Reference: [10] <author> E. Smirni and D. Reed. </author> <title> Lessons from Characterizing the Input/Output Behavior of Parallel Scientific Applications. Performance Evaluation: </title> <journal> An International Journal, </journal> <volume> 33(1):27 44, </volume> <month> June </month> <year> 1998. </year>
Reference-contexts: 1 Introduction Numerous studies of the I/O characteristics of parallel applications have shown that many applications need to access a large number of small, noncontiguous pieces of data from a file <ref> [1, 2, 7, 9, 10] </ref>. For good I/O performance, however, the size of an I/O request must be large (on the order of megabytes). The I/O performance suffers considerably if applications access data by making many small I/O requests.
Reference: [11] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <booktitle> Scientific Programming, </booktitle> <address> 5(4):301317, </address> <month> Winter </month> <year> 1996. </year>
Reference-contexts: Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in <ref> [11] </ref>. 4.1 Two-Phase I/O Two-phase I/O was first proposed in [3] in the context of accessing distributed arrays from files. Consider the example of reading a two-dimensional array from a file into a (block,block) distribution in memory, as shown in Figure 2. <p> The advantage of this method is that by making all file accesses large and contiguous, the I/O time is reduced significantly. The added cost of interprocess communication for redistribution is small compared with the savings in I/O time. The basic two-phase method was extended in <ref> [11] </ref> to access sections of out-of-core arrays. <p> We have described two optimizations our MPI-IO implementation performs that enable it to deliver high performance even if the user's request consists of many small, noncontiguous accesses. Our implementation of these optimizations generalizes the work in <ref> [11, 12] </ref> to handle any noncontiguous access pattern, not just sections of arrays. For the applications we considered, collective I/O performed significantly better than both data sieving and Table 5.
Reference: [12] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> Passion: Optimized I/O for Parallel Applications. </title> <booktitle> Computer, </booktitle> <address> 29(6):7078, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: When a process makes an independent request for noncontiguous data, ROMIO, therefore, does not access each contiguous portion of the data separately. Instead, it uses an optimization called data sieving <ref> [12] </ref>. The basic idea is illustrated in Figure 1. Assume that the user has made a single read request for five noncontiguous pieces of data. <p> We have described two optimizations our MPI-IO implementation performs that enable it to deliver high performance even if the user's request consists of many small, noncontiguous accesses. Our implementation of these optimizations generalizes the work in <ref> [11, 12] </ref> to handle any noncontiguous access pattern, not just sections of arrays. For the applications we considered, collective I/O performed significantly better than both data sieving and Table 5.
Reference: [13] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Abstract-Device Interface for Implementing Portable Parallel-I/O Interfaces. </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 180187. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: ROMIO is designed to be used with any MPI-1 implementationboth portable and vendor-specific implementations. It is currently included as part of three MPI implementations: MPICH, HP MPI, and SGI MPI. A key component of ROMIO that enables such a portable MPI-IO implementation is an internal layer called ADIO <ref> [13] </ref>. ADIO, an abstract-device interface for I/O, consists of a small set of basic functions for parallel I/O. In ROMIO, the MPI-IO interface is implemented portably on top of ADIO, and only ADIO is implemented separately for different file systems. <p> Data sieving and collective I/O are implemented within ADIO functions <ref> [13] </ref>; data sieving is used in ADIO functions that read/write noncontiguous data, and collective I/O is used in ADIO's collective-I/O functions. Both these optimizations ultimately make contiguous I/O requests to the underlying file system, which are implemented by using ADIO's contiguous-I/O functions.
Reference: [14] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> A Case for Using MPI's Derived Datatypes to Improve I/O Performance. </title> <booktitle> In Proceedings of SC98: High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1998. </year>
Reference-contexts: We demonstrate the performance and portability with performance results for three applications on five different parallel machines. We note that ROMIO can perform the optimizations described in this paper only if users provide complete access information in a single function call. In <ref> [14] </ref> we explained how users can do so by using MPI's derived datatypes to create file views and by using MPI-IO's collective-I/O functions whenever possible. In this paper we describe the optimizations in detail and provide extensive performance re sults. The rest of this paper is organized as follows.
References-found: 14

