URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-350.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: Electronic: sandy@media.mit.edu,  
Title: Understanding of Human Action  
Author: Alex Pentland 
Web: http://www-white.media.mit.edu/vismod/demos/ive/  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: The Media Laboratory Massachusetts Institute of Technology,  
Note: Machine  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 350, Sept. 1995 Appeared: 7 th Int'l Forum on Frontier of Telecommunication Technology, Nov. 1995, Tokyo, Japan Abstract The Perceptual Computing Section of the Media Laboratory is working to make computers that understand people, and can work with them in the manner of an attentive human-like assistant. To this end we have built a series of interactive office spaces that are used as real-time experimental testbeds. These spaces are instrumented with cameras and microphones, and perform audio-visual interpretation of human users. Real-time capabilities include 3-D tracking of head, hands, and feet; "holographic" audio; face recognition; and interpretation of face and hand gestures. People in the space can control programs, browse multimedia information, and experience shared virtual environ ments without wires or special goggles.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Azarbayejani, A., and Pentland, A. </author> <title> (1995) "Camera self-calibration from one point correspondence," </title> <type> Technical Report 341, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1995. </year>
Reference-contexts: Pfinder defaultly provides some such interpretation. For instance, Pfinder detects and classifies static hand and body poses. When Pfinder is given a geometric model for the camera <ref> [1] </ref>, it also backprojects the 2-D image information to produce 3-D position estimates using the assumption that the user is standing on a planar floor.
Reference: [2] <author> Azarbayejani, A., and Pentland, A. </author> <title> (1995) "Recursive estimation of motion, structure, and focal length," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 17(6), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: This makes audio difficult to integrate into normal human life, and so we have developed solutions that free the user from these restrictions. In this section we explain the idea of vision-directed audio; for additional detail about the IVE audio input and output systems, see references <ref> [2, 3] </ref>. 6.1 Audio Input Speech recognition applications typically require near-field, i.e. &lt; 1:5m, microphone placement for acceptable performance. Beyond this distance the signal-to-noise ratio of the incoming speech affects the performace significantly. Commercial speech-recognition packages typically break down over a 4 to 6 dB range.
Reference: [3] <author> Casey, M.A., Gardner, W.G., and Basu, S., </author> <title> (1995) "Vision Steered Beam-Forming and Transaural Rendering for the Artificial Life Interactive Video Environment (ALIVE)," </title> <booktitle> 99th Convention of the AES, </booktitle> <address> October 6-9 1995, New York. </address>
Reference-contexts: This makes audio difficult to integrate into normal human life, and so we have developed solutions that free the user from these restrictions. In this section we explain the idea of vision-directed audio; for additional detail about the IVE audio input and output systems, see references <ref> [2, 3] </ref>. 6.1 Audio Input Speech recognition applications typically require near-field, i.e. &lt; 1:5m, microphone placement for acceptable performance. Beyond this distance the signal-to-noise ratio of the incoming speech affects the performace significantly. Commercial speech-recognition packages typically break down over a 4 to 6 dB range.
Reference: [4] <author> Darrell, T., Maes, P., Blumberg, B., and Pentland, A., </author> <title> (1994) "A Novel Environment for Situated Vision and Behavior," </title> <booktitle> IEEE Workshop on Visual Behaviors pp. </booktitle> <pages> 68-72, </pages> <address> Seattle, WA., </address> <month> June 19, </month> <year> 1994. </year>
Reference-contexts: To perform research on these problems we have created a series of experimental testbeds at the Media Laboratory. These testbeds are performance spaces known as Interactive Video Environments (IVE) <ref> [4] </ref>. IVE spaces are instrumented with cameras and microphones, which allow the computer to see, hear, and interpret users' actions. People in an IVE can control programs, browse multimedia information, and experience shared virtual environments without keyboards, special sensors, or special goggles. <p> People in an IVE can control programs, browse multimedia information, and experience shared virtual environments without keyboards, special sensors, or special goggles. Important near-term applications of this technology include practical interfaces to large-screen displays and virtual environments. The IVE system was originally developed by Darrell, Maes, Blumberg and Pentland <ref> [4] </ref> to provide vision input for the ALIVE ("artificial life interactive video environment") system [8]. This system performed real-time person tracking but had no explicit model of the person and required a controlled background. <p> Pfinder's gesture tags and feature positions are used by the artificial life forms to make decisions about how to interact with the user, as illustrated in Figure 9 (a) <ref> [4] </ref>. Pfinder's output can also be used in a much simpler and direct manner. The position of the user and the configuration of the user's appendages can be mapped into a control space, and sounds made by the user are used to change the operating mode. <p> To make a convincing 3-D world, the video must be placed correctly in the 3-D environment, that is, video of the person must be able to occlude, or be occluded by, the graphics. The method of accomplishing this sophisticated compositing operation is described in Darrell et al <ref> [4] </ref>. If you share Pfinder's information about the user between geographically separate locations it is possible to create convincing telepresence without shipping video to the remote site, thus providing very low-bandwidth coding of human action, as in Darrell et al [5].
Reference: [5] <author> Darrell, T., Blumberg, B., Daniel, S., Rhodes, B., Maes, P., and Pentland, A. </author> <title> (1995) "Alive: Dreams and illusions," </title> <booktitle> Visual Proceedings, ACM Siggraph, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Each blob can also have a detailed representation of its shape and appearance, modeled as differences from the underlying blob statistics. The ability to efficiently compute compact representations of people's appearance is useful for low-bandwidth telecommunications, such as our demonstration of a shared virtual envionments at SIGGRAPH '95 <ref> [5] </ref>. The statistics of each blob are recursively updated to combine information contained in the most recent measurements with knowledge contained in the current class statistics and the priors. <p> If you share Pfinder's information about the user between geographically separate locations it is possible to create convincing telepresence without shipping video to the remote site, thus providing very low-bandwidth coding of human action, as in Darrell et al <ref> [5] </ref>. On the remote end, information about the user's head, hand, and feet position is used to drive a video avatar that represents the user in the scene. One such avatar is illustrated in Figure 10.
Reference: [6] <author> Essa, I., Pentland, A., </author> <title> (1994) A Vision System for Observing and Extracting Facial Action Parameters, </title> <booktitle> IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 76-83, </pages> <address> Seattle, WA., </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The method used in conjunction with the IVE project, developed by Essa and Pentland <ref> [6, 7] </ref>, is to generate spatio-temporal motion-energy templates of the whole face for each different expression. For each facial expression, the corresponding template expresses the peak amount and direction of motion that we expect to see at each point on the face. <p> In each image sequence the motion energy was measured, compared to each of the templates, and the expression classified. This experiment generated the confusion matrix shown in Table 1. This table shows just one incorrect classification, giving an overall recognition rate of 98.0%. For additional details see <ref> [6, 7] </ref>. 6 Vision-Driven Audio Audio interpretation of people is as important as visual interpretation. Although much work as been done on speech understanding, virtually all of this work assumes a closely-placed microphone for input and a fixed listening position or headphones for output.
Reference: [7] <author> Essa, I., and Pentland, A. </author> <title> (1995) "Facial Expression Recognition Using a Dynamic Model and Motion Energy," </title> <booktitle> Int'l Conference on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <month> June 20-23 </month> <year> 1995. </year>
Reference-contexts: The method used in conjunction with the IVE project, developed by Essa and Pentland <ref> [6, 7] </ref>, is to generate spatio-temporal motion-energy templates of the whole face for each different expression. For each facial expression, the corresponding template expresses the peak amount and direction of motion that we expect to see at each point on the face. <p> These simple, biologically-plausible motion energy "templates" can be used for expression recognition by comparing the motion-energy observed for a particular face to the "average" template for each expression. Using this simple method, we have been able to demonstrate substantially greater accuracy at expression recognition than has been previously achieved <ref> [7] </ref>. Figures 6 (e) - (i) show the motion-energy templates for several expressions. Each of these templates is unique and therefore can serve as an sufficient feature for categorization of facial expression. <p> In each image sequence the motion energy was measured, compared to each of the templates, and the expression classified. This experiment generated the confusion matrix shown in Table 1. This table shows just one incorrect classification, giving an overall recognition rate of 98.0%. For additional details see <ref> [6, 7] </ref>. 6 Vision-Driven Audio Audio interpretation of people is as important as visual interpretation. Although much work as been done on speech understanding, virtually all of this work assumes a closely-placed microphone for input and a fixed listening position or headphones for output.
Reference: [8] <author> Maes, P., Blumburg, B., Darrell, T., and Pentland, A., </author> <title> (1995) "The ALIVE System: Full-body Interaction with Autonomous Agents." </title> <booktitle> Proceedings of Computer Animation 95, </booktitle> <publisher> IEEE Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: Important near-term applications of this technology include practical interfaces to large-screen displays and virtual environments. The IVE system was originally developed by Darrell, Maes, Blumberg and Pentland [4] to provide vision input for the ALIVE ("artificial life interactive video environment") system <ref> [8] </ref>. This system performed real-time person tracking but had no explicit model of the person and required a controlled background. In the succeeding years we have developed this system in to the more general, and more accurate, system described here. <p> One such application is the Artificial Life IVE (ALIVE) system, a joint research project with Professor Pattie Maes and her students <ref> [8] </ref>. ALIVE utilizes Pfinder's support map polygon to define alpha values for video compositing (placing the user in a scene with some artificial life forms in real-time).
Reference: [9] <author> Moghaddam, B., and Pentland, A., </author> <title> (1995) "Probabalis-tic Visual Learning for Object Detection," </title> <booktitle> Int'l Conference on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <month> June 20-23 </month> <year> 1995. </year>
Reference-contexts: Knowlege of this distribution then allows us to precisely locate faces and face features, and to compare faces along meaningful dimensions. The following gives a brief description of this system; for additional detail see Moghaddam and Pentland <ref> [9] </ref>. 4.1 Face and Feature Detection The standard detection paradigm in image processing is that of normalized correlation or template matching. However this approach is only optimal in the simplistic case of a deterministic signal embedded in white Gaussian noise. <p> Some of the low-order eigentemplates for a human face are shown in Figure 4 (a). In a statistical signal detection framework, the use of eigen-templates has been shown to be orders of magnitude better than standard matched filtering <ref> [9] </ref>. Using this approach we have reformulated the target detection problem from the point of view of a ML estimation problem. In particular, given the visual field, estimate the position (and scale) of the subimage which is most representative of a specific target class . <p> The top three matches in this case are images of the same person taken a month apart and at different scales. The recognition accuracy of this system (defined as the percent correct rank-one matches) is 99% <ref> [9] </ref>. from Figure 3, its reconstruction using a 100-dimensional eigenspace representation (requiring only 85 bytes to encode) and, for comparison, a JPEG compressed image (encoded using 530 bytes). 5 Expression Recognition Once we have accurately located the face and its features, we can now begin to analyze its motion to determine
Reference: [10] <author> Pentland A., Picard, R., Sclaroff, S., </author> <year> (1994) </year> <month> "Photo-book: </month> <title> Tools for Content-Based Manipulation of Image Databases," </title> <booktitle> SPIE Conf. on Storage and Retreval of Image Databases II, </booktitle> <address> San Jose, CA, </address> <month> Feb 6-10, </month> <year> 1994 </year>
Reference-contexts: This result is on based on 12 image sequences of smile, 10 image sequences of surprise, anger, disgust, and raise eyebrow. Success rate for each expression is shown in the bottom row. The overall recognition rate is 98.0%. (a) faces in a database, using the Photobook image database tool <ref> [10] </ref>. ways as the available computational power is capable of, allowing for the tracking of multiple moving sound sources from a single microphone array. 6.2 Audio Output The audio output system can position sounds at arbitrary azimuths and elevations around a listener's head.
Reference: [11] <author> Pentland, A., and Liu, A., </author> <title> (1995) Toward Augmented Control Systems, </title> <booktitle> IEEE Intelligent Vehicle Symposium 95, </booktitle> <address> September 25-26, Detroit, MI. </address>
Reference-contexts: Thad Starner is shown using this system in Figure 8 (a). The accurate classification performance of this system is particularly impressive because in ASL the hand movements are rapid and continuous, and exhibit large coarticulation effect. The second system, developed by Pentland and Liu <ref> [11] </ref>, interpreted people's actions while driving a car. In this system the driver's hand and leg motions were observed while driving in the Nissan Cambridge Basic Research Lab's driving simulator (see Figure 8 (b)). These observations used to classify the driver's action as quickly as was possible.
Reference: [12] <author> Russell, K., Starner, T., and Pentland, A. </author> <title> (1995) "Unencumbered virtual environments," </title> <booktitle> IJCAI-95 Workshop on Entertainment and AI/Alife. </booktitle>
Reference-contexts: This allows the user to control an application with their body directly. This interface has been used to navigate a 3-D virtual game environment as in SURVIVE (Simulated Urban Recreational Violence IVE) <ref> [12] </ref> (illustrated in Figure 9 (b)), and an information landscape / virtual museum [13]. 8.2 Avatars and Telepresence Using Pfinder's estimates of the user's head, hands, and feet position it is possible to create convincing shared virtual spaces.
Reference: [13] <author> Sparacino, F., Wren, C., Pentland, A., and Davenport, G., </author> <year> (1995) </year> <month> "Hyperplex: </month> <title> a world of 3d interactive digital movies," </title> <booktitle> IJCAI-95 Workshop on Entertainment and AI/Alife. </booktitle>
Reference-contexts: This allows the user to control an application with their body directly. This interface has been used to navigate a 3-D virtual game environment as in SURVIVE (Simulated Urban Recreational Violence IVE) [12] (illustrated in Figure 9 (b)), and an information landscape / virtual museum <ref> [13] </ref>. 8.2 Avatars and Telepresence Using Pfinder's estimates of the user's head, hands, and feet position it is possible to create convincing shared virtual spaces.
Reference: [14] <author> Starner, T., and Pentland, A., </author> <title> Visual Recognition of American Sign Language Using Hidden Markov Models, </title> <booktitle> Proc. Int'l Workshop on Automatic Face- and Gesture-Recognition 1995, </booktitle> <address> Zurich, Switzerland, </address> <month> June 26-28, </month> <year> 1995. </year>
Reference-contexts: To date we have used this approach to build two systems for interpreting human action. The first system, developed by Starner and Pentland <ref> [14] </ref>, reads American Sign Language (ASL). This real-time system has allowed us to perform near-perfect classification of a forty-word subset of ASL using only the hand measurements provided by Pfinder (e.g., hand position, orientation, and width/height ratio). Thad Starner is shown using this system in Figure 8 (a).
Reference: [15] <author> Wren, C., Azarbayejani, A., Darrell, T.,and Pentland, A., </author> <year> (1995) </year> <month> "Pfinder: </month> <title> Real-Time Tracking of the Human Body," </title> <booktitle> SPIE Conference on Real-Time Image Processing, </booktitle> <address> Philadelphia, PA., </address> <month> Oct 27, </month> <title> 1995 8 (a) (b) (a) (b) 9 </title>
Reference-contexts: For additional detail see Wren, Darrell, Azarbayejani, and Pentland <ref> [15] </ref>. 3.1 Modeling The Person Pfinder models the human as a connected set of blobs. Each blob has a spatial (x; y) and color (Y; U; V ) Gaussian distribution. <p> At the right are one-standard-deviation ellipses illustrating the statistical blob descriptions formed for the head, hands, feet, shirt, and pants. Note that despite having the hands either in front of the face or the body a correct description is still obtained. For additional detail see reference <ref> [15] </ref>. 3.4 Interface Pfinder provides a modular interface to client applications. Several clients can be serviced in parallel, and clients can attach and detach without affecting the underlying vision routines.
References-found: 15

