URL: http://www.cs.cornell.edu/faculty/home/cardie/papers/94-74c.ps.Z
Refering-URL: http://www.cs.cornell.edu/faculty/home/cardie/cardie.html
Root-URL: http://www.cs.brown.edu/
Title: DOMAIN-SPECIFIC KNOWLEDGE ACQUISITION FOR CONCEPTUAL SENTENCE ANALYSIS  
Degree: A Dissertation Presented by CLAIRE CARDIE Submitted to the Graduate School of the  in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY  
Date: September 1994  
Affiliation: University of Massachusetts Amherst  Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [ Aha and Kibler, 1987 ] <author> Aha, D. and Kibler, D. </author> <title> Learning Representative Exemplars of Concepts: An Initial Case Study. </title> <booktitle> In Proceedings of the Fourth International Conference on Machine Learning, </booktitle> <pages> pages 24-30, </pages> <address> U. C. Irvine, Irvine, CA, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Aha et al., 1991 ] <author> Aha, D., Kibler, D., and Albert, M. </author> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: In addition, MayTag's case representation contains none of the deep or derived features that typically comprise a case in a CBR system. 13 MayTag's emphasis on case-based classification using simple feature vectors and without case adaptation makes it more similar to instance-based machine learning algorithms <ref> [ Aha et al., 1991 ] </ref> . 5.4.1 Recognizing Incomplete Word Definitions As a result of its case-based approach to lexical disambiguation, MayTag allows a word to take on an interpretation different from any it received during the training phase. <p> Intuitively, however, it seems that accurate prediction of each class of missing information may rely on very different subsets of the feature set. Indeed, it has been shown that nearest-neighbor algorithms perform poorly in the presence of irrelevant features <ref> [ Aha et al., 1991, Aha, 1989 ] </ref> . One general method for optimizing our case retrieval algorithm for a particular interpretation task is to include in the k-nn calculations only those features deemed relevant to the task according to some collection of expert knowledge.
Reference: [ Aha, 1989 ] <author> Aha, D. </author> <title> Instance-Based Learning Algorithms. </title> <booktitle> In Proceedings of the Sixth International Conference on Machine Learning, </booktitle> <pages> pages 387-391, </pages> <institution> Cornell University, </institution> <address> Ithaca, NY, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Intuitively, however, it seems that accurate prediction of each class of missing information may rely on very different subsets of the feature set. Indeed, it has been shown that nearest-neighbor algorithms perform poorly in the presence of irrelevant features <ref> [ Aha et al., 1991, Aha, 1989 ] </ref> . One general method for optimizing our case retrieval algorithm for a particular interpretation task is to include in the k-nn calculations only those features deemed relevant to the task according to some collection of expert knowledge.
Reference: [ Almuallim and Dietterich, 1991 ] <author> Almuallim, H. and Dietterich, T. G. </author> <title> Learning With Many Irrelevant Features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552, </pages> <address> Anaheim, CA, 1991. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: However, Almuallim and Dietterich show that the ID3 decision tree system [ Quinlan, 1986 ] is not particularly good at selecting a minimum set of features from an original set containing possibly many irrelevant attributes <ref> [ Almuallim and Dietterich, 1991 ] </ref> . While their results may hold in general, we claim that there is at least one important class of problem for which decision tree algorithms can perform feature specification reasonably well.
Reference: [ Ashley and Rissland, 1988 ] <author> Ashley, K. D. and Rissland, E. L. </author> <title> A cased-based approach to modeling legal expertise. </title> <journal> IEEE Expert, </journal> <volume> 3(3) </volume> <pages> 70-77, </pages> <year> 1988. </year>
Reference-contexts: algorithm (k = 10) with a bias toward cases whose word matches the unknown word. 11 Because MayTag uses the retrieved cases to derive an appropriate syntactic and semantic interpretation for the current word, it is considered an interpretive CBR system. (For examples of other interpretive CBR systmes, see HYPO <ref> [ Ashley and Rissland, 1988, Ashley, 1990 ] </ref> , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET [ Rissland and Skalak, 1991 ] , 9 Partial matches occur when two features have overlapping, but not identical values.
Reference: [ Ashley, 1990 ] <author> Ashley, K. D. </author> <title> Modelling legal argument: Reasoning with cases and hypotheticals. </title> <publisher> MIT Press, Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: As a result, Kenmore can be compared to existing CBR systems along a number of dimensions including: Case Representation: Like HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> , Kenmore uses a simple attribute-value pair case representation rather than a more structured case representation (e.g., the semantic network case representation of GREBE [ Brant ing and Porter, 1991, Branting, 1991 ] ). <p> much simpler than the domain-dependent and knowledge-based case retrieval algorithms typically employed in CBR systems (e.g., CHEF [ Hammond, 1989 ] ). 26 Best Case Selection: Kenmore uses a simple voting scheme to derive a solution from the retrieved cases rather than using more sophisticated case selection methods (e.g., HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> and GREBE [ Branting and Porter, 1991, Branting, 1991 ] ). <p> This makes Kenmore an interpretive CBR system a system that analyzes a new case by comparing and analogizing it with previous interpretation episodes. Interpretive CBR systems exist in a number of domains: (1) The HYPO system, for example, <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE [ Branting and Porter, 1991, Branting, 1991 ] and CABARET [ Rissland and Skalak, 1991 ] analyze problems in the domain of legal reasoning; (3) ANAPRON's [ <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET [ Rissland and Skalak, 1991 ] , and ANAPRON [ Golding and Rosenbloom, 1991 ] . <p> Although some case-based reasoning systems use a richer case representation (e.g., CHEF [ Hammond, 1989 ] , GREBE [ Branting and Porter, 1991, Branting, 1991 ] ), the simple attribute-value pair knowledge representation is a very general representation scheme that can be used in conjunction with case-based algorithms (see HYPO <ref> [ Ashley, 1990 ] </ref> ) as well as the vast majority of symbolic inductive machine learning algorithms. 6 More specifically, MayTag cases consist of 37 attribute-value pairs: 33 features represent the context in which the current word was encountered and 4 represent the syntactic and semantic interpretation of the current word <p> algorithm (k = 10) with a bias toward cases whose word matches the unknown word. 11 Because MayTag uses the retrieved cases to derive an appropriate syntactic and semantic interpretation for the current word, it is considered an interpretive CBR system. (For examples of other interpretive CBR systmes, see HYPO <ref> [ Ashley and Rissland, 1988, Ashley, 1990 ] </ref> , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET [ Rissland and Skalak, 1991 ] , 9 Partial matches occur when two features have overlapping, but not identical values. <p> Most CBR systems rely on indexing schemes specified by the system designers that are based on expert knowledge of a particular domain. The HYPO system <ref> [ Rissland et al., 1984, Ashley, 1990 ] </ref> , for example, retrieves relevant cases in the trade secrets legal reasoning domain based on a predetermined set of dimensions that influence the outcome of cases in this domain.
Reference: [ Ayuso et al., 1987 ] <author> Ayuso, D. M., Shaked, V., and Weischedel, R. M. </author> <title> An environment for acquiring semantic information. </title> <booktitle> In Proceedings of the 25th Annual Meeting of the ACL, </booktitle> <pages> pages 32-40. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1987. </year>
Reference-contexts: Smart interfaces, on the other hand, assist the user in tagging or bracketing a corpus (see Marcus [1991]), but do not generally assist in the acquisition and maintenance of disambiguation heuristics. Notable exceptions are interfaces that help the user define semantic case frames and their associated selectional restrictions <ref> [ Ayuso et al., 1987, Grishman et al., 1986 ] </ref> structures that implicitly perform some lexical and structural disambiguation. Traditionally, this type of knowledge is crafted exclusively by hand. An intelligent interface with some similarities to Kenmore is the Simmons and Yu system [ Simmons and Yu, 1992 ] .
Reference: [ Bareiss et al., 1989 ] <author> Bareiss, E. R., Porter, B. W., and Murray, K. S. </author> <title> Supporting start-to-finish development of knowledge bases. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: Finally, Kenmore has been shown to work 28 with real-world corpora that contain long, complicated sentences rather than the short, highly structured sentences handled in Parse-O-Matic. Another CBR system that is similar to Kenmore in its focus on knowledge acquisition is PROTOS <ref> [ Bareiss, 1989, Bareiss et al., 1989 ] </ref> . PROTOS, however, is not designed specifically for knowledge acquisition in the domain of natural language processing. Instead, it provides a general architecture for knowledge acquisition and has been employed in a number of domains including the diagnosis of hearing disorders.
Reference: [ Bareiss, 1989 ] <author> Bareiss, E. R. </author> <title> Exemplar-based knowledge acquisition: a unified approach to concept representation, classification, and learning. </title> <publisher> Academic Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: Finally, Kenmore has been shown to work 28 with real-world corpora that contain long, complicated sentences rather than the short, highly structured sentences handled in Parse-O-Matic. Another CBR system that is similar to Kenmore in its focus on knowledge acquisition is PROTOS <ref> [ Bareiss, 1989, Bareiss et al., 1989 ] </ref> . PROTOS, however, is not designed specifically for knowledge acquisition in the domain of natural language processing. Instead, it provides a general architecture for knowledge acquisition and has been employed in a number of domains including the diagnosis of hearing disorders.
Reference: [ Berwick, 1983 ] <author> Berwick, Robert. </author> <title> Learning word meanings from examples. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 459-461, </pages> <address> Karlsrsuhe, Germany, </address> <year> 1983. </year>
Reference: [ Birnbaum and Selfridge, 1981 ] <author> Birnbaum, L. and Selfridge, M. </author> <title> Conceptual Analysis of Natural Language. </title> <editor> In Schank, R. and Riesbeck, C., editors, </editor> <booktitle> Inside Computer Understanding. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year> <month> 167 </month>
Reference: [ Bosch and Daelemans, 1993 ] <author> Bosch, A. van den and Daelemans, W. </author> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of European Chapter of ACL, </booktitle> <pages> pages 45-53, </pages> <address> Utrecht, </address> <year> 1993. </year> <note> Also available as ITK Research Report 42. </note>
Reference-contexts: They have found that instance-based approaches to language acquisition work well for low-level language tasks like stress acquisition [ Daelemans et al., 1994 ] and grapheme-to-phoneme conversion 27 <ref> [ Bosch and Daelemans, 1993 ] </ref> and have hypothesized that they will also perform well for higher level tasks. In addition, they argue that language acquisition is a behavior-based, data-driven process rather than one guided by knowledge-based principles and parameter setting.
Reference: [ Bozsahin and Findler, 1992 ] <author> Bozsahin, H. Cem and Findler, V. Nicholas. </author> <title> Memory-Based Hypothesis Formation: Heuristic Learning of Commonsense Causal Relations from Text. </title> <journal> Cognitive Science, </journal> <volume> 16(4) </volume> <pages> 431-454, </pages> <year> 1992. </year>
Reference-contexts: Although EBL has also been used for low-level language tasks like learning phonological representations [ Stetham, 1991 ] and higher level tasks like learning causal relationships <ref> [ Bozsahin and Findler, 1992, Pazzani, 1991 ] </ref> , it has not been used to learn solutions to the lexical and structural disambiguation tasks handled within the Kenmore architecture.
Reference: [ Branting and Porter, 1991 ] <author> Branting, L. K. and Porter, B. W. </author> <title> Rules and Precedents as Complementary Warrants. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 3-9, </pages> <address> Anaheim, CA, 1991. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: typically employed in CBR systems (e.g., CHEF [ Hammond, 1989 ] ). 26 Best Case Selection: Kenmore uses a simple voting scheme to derive a solution from the retrieved cases rather than using more sophisticated case selection methods (e.g., HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] and GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> ). <p> Interpretive CBR systems exist in a number of domains: (1) The HYPO system, for example, [ Ashley, 1990, Rissland and Ashley, 1986 ] creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> and CABARET [ Rissland and Skalak, 1991 ] analyze problems in the domain of legal reasoning; (3) ANAPRON's [ Golding and Rosenbloom, 1991 ] interpretive task is word pronunciation. <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> , CABARET [ Rissland and Skalak, 1991 ] , and ANAPRON [ Golding and Rosenbloom, 1991 ] . <p> Although some case-based reasoning systems use a richer case representation (e.g., CHEF [ Hammond, 1989 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> ), the simple attribute-value pair knowledge representation is a very general representation scheme that can be used in conjunction with case-based algorithms (see HYPO [ Ashley, 1990 ] ) as well as the vast majority of symbolic inductive machine learning algorithms. 6 More <p> word matches the unknown word. 11 Because MayTag uses the retrieved cases to derive an appropriate syntactic and semantic interpretation for the current word, it is considered an interpretive CBR system. (For examples of other interpretive CBR systmes, see HYPO [ Ashley and Rissland, 1988, Ashley, 1990 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> , CABARET [ Rissland and Skalak, 1991 ] , 9 Partial matches occur when two features have overlapping, but not identical values.
Reference: [ Branting, 1991 ] <author> Branting, L. K. </author> <title> Integrating Rules and Precedents for Classification and Explanation: Automating Legal Analysis. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, TX, </institution> <year> 1991. </year>
Reference-contexts: Kenmore can be compared to existing CBR systems along a number of dimensions including: Case Representation: Like HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] , Kenmore uses a simple attribute-value pair case representation rather than a more structured case representation (e.g., the semantic network case representation of GREBE <ref> [ Brant ing and Porter, 1991, Branting, 1991 ] </ref> ). Case Indexing: In handling lexical ambiguities, Kenmore indexes cases using a decision tree subsystem. <p> typically employed in CBR systems (e.g., CHEF [ Hammond, 1989 ] ). 26 Best Case Selection: Kenmore uses a simple voting scheme to derive a solution from the retrieved cases rather than using more sophisticated case selection methods (e.g., HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] and GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> ). <p> Interpretive CBR systems exist in a number of domains: (1) The HYPO system, for example, [ Ashley, 1990, Rissland and Ashley, 1986 ] creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> and CABARET [ Rissland and Skalak, 1991 ] analyze problems in the domain of legal reasoning; (3) ANAPRON's [ Golding and Rosenbloom, 1991 ] interpretive task is word pronunciation. <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> , CABARET [ Rissland and Skalak, 1991 ] , and ANAPRON [ Golding and Rosenbloom, 1991 ] . <p> Although some case-based reasoning systems use a richer case representation (e.g., CHEF [ Hammond, 1989 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> ), the simple attribute-value pair knowledge representation is a very general representation scheme that can be used in conjunction with case-based algorithms (see HYPO [ Ashley, 1990 ] ) as well as the vast majority of symbolic inductive machine learning algorithms. 6 More <p> word matches the unknown word. 11 Because MayTag uses the retrieved cases to derive an appropriate syntactic and semantic interpretation for the current word, it is considered an interpretive CBR system. (For examples of other interpretive CBR systmes, see HYPO [ Ashley and Rissland, 1988, Ashley, 1990 ] , GREBE <ref> [ Branting and Porter, 1991, Branting, 1991 ] </ref> , CABARET [ Rissland and Skalak, 1991 ] , 9 Partial matches occur when two features have overlapping, but not identical values.
Reference: [ Brent, 1990 ] <author> Brent, Michael R. </author> <title> Semantic Classification of Verbs from their Syntactic Contexts: Automated Lexicography with Implications for Child Language Acquisition. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 428-437, </pages> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: However, a few systems have been designed to learn semantic knowledge for sentence analysis or at least knowledge that is on the boundary of syntax and semantics. Brent has focused on the acquisition of semantic knowledge for verbs: (1) distinguishing stative from non-stative verbs 5 <ref> [ Brent, 1990 ] </ref> , and (2) detecting verbs with one of five subcategorization frames [ Brent, 1991 ] . His systems are particularly interesting because they avoid the need for human supervision by searching for a predefined set of unambiguous syntactic contexts.
Reference: [ Brent, 1991 ] <author> Brent, Michael R. </author> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <pages> pages 209-214, </pages> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1991. </year> <note> Association for Computational Linguistics. Also in Computational Linguistics, Vol. 19, No. 2. </note>
Reference-contexts: Brent has focused on the acquisition of semantic knowledge for verbs: (1) distinguishing stative from non-stative verbs 5 [ Brent, 1990 ] , and (2) detecting verbs with one of five subcategorization frames <ref> [ Brent, 1991 ] </ref> . His systems are particularly interesting because they avoid the need for human supervision by searching for a predefined set of unambiguous syntactic contexts.
Reference: [ Bresnan, 1982 ] <author> Bresnan, J. </author> <title> The Mental Representation of Grammatical Relations. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: These constituents are stored in global buffers that track the subject, verb, direct object, indirect object, and prepositional phrases of a sentence. Like lexical-functional grammars <ref> [ Bresnan, 1982 ] </ref> , CIRCUS stores the syntactic predictions associated with each word in the lexicon and retrieves them during sentence analysis.
Reference: [ Brill, 1992 ] <author> Brill, E. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. Association for Computational Linguistics, </booktitle> <year> 1992. </year>
Reference: [ Brill, 1993 ] <author> Brill, E. </author> <title> A Corpus-Based Approach to Language Learning. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: This allows the system to provide a more compelling explanation of its decisions than purely statistical methods. The rule-based part-of-speech tagger presented in Brill [1992] also has this advantage. His system acquires a set of ordered, non-stochastic rules using transformation-based error-driven learning <ref> [ Brill, 1993 ] </ref> .
Reference: [ Brill, 1994 ] <author> Brill, E. </author> <title> Some Advances in Transformation-Based Part of Speech Tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 722-727. </pages> <publisher> AAAI Press / MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Finally, modifications to Brill's original system were required to incorporate lexical preferences and to handle unknown words or words that did not appear during training <ref> [ Brill, 1994 ] </ref> . These situations are handled naturally within the Kenmore framework for knowledge acquisition. Most statistical approaches to lexical ambiguity resolution have focused primarily on the acquisition of syntactic knowledge.
Reference: [ Brown et al., 1991 ] <author> Brown, P., Della Pietra, S., Della Pietra, V., and Mercer, R. </author> <title> Word Sense Disambiguation Using Statistical Methods. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <pages> pages 264-270, </pages> <institution> University of California, Berkeley, CA, 1991. Association for Computational Linguistics. </institution>
Reference: [ Brown et al., 1992 ] <author> Brown, P., Della Pietra, V. J., deSouze, P. V., Lai, J. C., and Mercer, R. L. </author> <title> Class-Based n-gram Models of Natural Language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4), </volume> <year> 1992. </year> <month> 168 </month>
Reference: [ Callan and Utgoff, 1991 ] <author> Callan, J. and Utgoff, P. </author> <title> A Transformational Approach to Constructive Induction. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 122-126, </pages> <institution> Northwestern University, </institution> <address> Chicago, IL, 1991. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [ Carbonell, 1978 ] <author> Carbonell, J. </author> <title> Politics: Automated ideological reasoning. </title> <journal> Cognitive Science, </journal> <volume> 2(1) </volume> <pages> 27-51, </pages> <year> 1978. </year>
Reference-contexts: Other text analyzers relied on detailed, hand-coded knowledge of scripts, plans, goals, and even political beliefs, in addition to hand-coded lexicons (e.g., SAM [ Schank and Riesbeck, 1981 ] , FRUMP [ DeJong, 1979 ] , PAM [ Wilensky, 1978 ] , and POLITICS <ref> [ Carbonell, 1978 ] </ref> ). The Boris system [ Dyer, 1983 ] included all of these knowledge structures (and more) and integrated them directly into the parser.
Reference: [ Cardie and Lehnert, 1991 ] <author> Cardie, C. and Lehnert, W. </author> <title> A Cognitively Plausible Approach to Understanding Complicated Syntax. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 117-124, </pages> <address> Anaheim, CA, 1991. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: CIRCUS [ Lehnert, 1990 ] is a conceptual sentence analyzer that synthesizes three distinct processing techniques to produce a semantic case frame representation of an input sentence. It uses (1) a stack-oriented control for syntactic analysis, (2) a marker-passing design for predictive preference semantics, and (3) lexically-indexed control kernels <ref> [ Cardie and Lehnert, 1991 ] </ref> for handling embedded clauses. 1 Although CIRCUS makes no commitment to a particular style of semantic representation, we use deep semantic case frames of the sort found in conceptual dependency [ Schank, 1975 ] . <p> This is accomplished in CIRCUS with lexically-indexed control kernels (LICKs) <ref> [ Cardie and Lehnert, 1991 ] </ref> . We view the stack of syntactic predictions as a single control kernel whose expectations and binding instructions change in response to specific lexical items as we move through the sentence.
Reference: [ Cardie, 1992a ] <author> Cardie, C. </author> <title> Corpus-Based Acquisition of Relative Pronoun Disambiguation Heuristics. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 216-223, </pages> <institution> University of Delaware, Newark, DE, 1992. Association for Computational Linguistics. </institution>
Reference: [ Cardie, 1992b ] <author> Cardie, C. </author> <title> Learning to Disambiguate Relative Pronouns. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 38-43, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference: [ Cardie, 1992c ] <author> Cardie, C. </author> <title> Using Cognitive Biases to Guide Feature Set Selection. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 743-748, </pages> <note> Indiana Univeristy, Bloomington, IN, 1992. </note> <editor> Lawrence Erlbaum Associates. </editor> <booktitle> Also in Working Notes of the 1992 AAAI Workshop on Constraining Learning with Prior Knowledge, </booktitle> <address> San Jose, CA. </address>
Reference: [ Cardie, 1993a ] <author> Cardie, C. </author> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 798-803, </pages> <address> Washington, DC, 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference: [ Cardie, 1993b ] <author> Cardie, C. </author> <title> Using Decision Trees to Improve Case-Based Learning. </title> <editor> In Utgoff, P., editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32, </pages> <institution> University of Massachusetts, </institution> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Charniak, 1993 ] <author> Charniak, E. </author> <title> Equations for Part-of-Speech Tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 784-789, </pages> <address> Washington, DC, 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: sentence: P robability (wordjtag) fl P robability (tagjpreceding n tags) 2 If n = 2, then the model is a bigram model; if n = 3, then a trigram model, etc. 19 In general, Markov-based tagging algorithms achieve accuracies in the 95% correct range for part-of-speech tagging of unrestricted text <ref> [ Church and Mercer, 1993, Charniak, 1993 ] </ref> . It has been postulated that these methods succeed where traditional rule-based approaches fail because they manage to account for lexical preferences (e.g., bird is generally used as a noun rather than a verb). <p> In general, MayTag's overall performance for part-of-speech prediction is comparable to the performance of statistical part-of-speech taggers, which generally achieve accuracies in the 96% range <ref> [ Charniak, 1993 ] </ref> . 101 As described in Chapter 5, the part-of-speech taxonomy used by MayTag contains 18 parts of speech (see Table 5.4).
Reference: [ Chen et al., 1993 ] <author> Chen, T., Soo, V., and Lin, A. </author> <title> Learning to Parse with Recurrent Neural Networks. </title> <booktitle> In Proceedings of European Conference on Machine Learning Workshop on Machine Learning and Text Analysis, </booktitle> <pages> pages 63-68, </pages> <year> 1993. </year>
Reference: [ Chinchor et al., 1993 ] <author> Chinchor, N., Hirschman, L., and Lewis, D. </author> <title> Evaluating Message Understanding Systems: An Analysis of the Third Message Undestanding Conference (MUC-3). </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 409-449, </pages> <year> 1993. </year> <month> 169 </month>
Reference-contexts: Although current natural language processing systems cannot yet perform in-depth text understanding, they can read an arbitrary text and summarize its major events provided that those events fall within a particular domain of interest (e.g., stories about natural disasters or terrorist events) <ref> [ Chinchor et al., 1993 ] </ref> . This scenario is illustrated in systems for this type of limited summarization task have been knowledge-based natural language systems NLP systems that understand an input text by relying heavily on handcrafted knowledge about the domain and about the world in general.
Reference: [ Church and Hanks, 1990 ] <author> Church, K. and Hanks, P. </author> <title> Word association norms, mutual information, and lexicography. </title> <journal> Computational Linguistics, </journal> <volume> 16, </volume> <year> 1990. </year>
Reference-contexts: His systems are particularly interesting because they avoid the need for human supervision by searching for a predefined set of unambiguous syntactic contexts. Other work on verbs includes a method for inferring predicate argument relations using mutual information statistics <ref> [ Church and Hanks, 1990 ] </ref> . Some progress has also been made in the area of word sense disambiguation. Brown et al. present a statistical method to label word senses of a word with the context in which they appear.
Reference: [ Church and Mercer, 1993 ] <author> Church, K. and Mercer, R. </author> <title> Introduction to the Special Issue on Computational Linguistics Introduction to the Special Issue on Computational Linguistics Using Large Corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19 </volume> <pages> 1-24, </pages> <year> 1993. </year>
Reference-contexts: sentence: P robability (wordjtag) fl P robability (tagjpreceding n tags) 2 If n = 2, then the model is a bigram model; if n = 3, then a trigram model, etc. 19 In general, Markov-based tagging algorithms achieve accuracies in the 95% correct range for part-of-speech tagging of unrestricted text <ref> [ Church and Mercer, 1993, Charniak, 1993 ] </ref> . It has been postulated that these methods succeed where traditional rule-based approaches fail because they manage to account for lexical preferences (e.g., bird is generally used as a noun rather than a verb).
Reference: [ Church et al., 1991 ] <author> Church, K., Gale, W., Hanks, P., and Hindle, D. </author> <title> Using Statistics in Lexical Analysis. </title> <editor> In Zernik, U., editor, </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <address> pages 115-164. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: For example, one corpus that is often used to construct statistical models for natural language acquisition tasks is the 4.5 million-word Penn Treebank [ Marcus et al., 1993 ] ; the smaller Tagged Brown Corpus (one million words) is considered too small for many tagging tasks <ref> [ Church et al., 1991 ] </ref> . However, corpora of even 150 million words have been found to be inadequate for some statistical approaches to ambiguity resolution [ Dagan and Itai, 1991 ] .
Reference: [ Church, 1988 ] <author> Church, K. </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1988. </year>
Reference: [ Cognitive Systems, 1992 ] <institution> Cognitive Systems, Inc. ReMind: </institution> <note> Release Notes Version 1.1. </note> <institution> Boston, </institution> <address> MA, </address> <year> 1992. </year>
Reference-contexts: Given a set of dimensions for the domain, HYPO restricts case retrieval to those cases that address dimensions that are present in the problem case. Similarly, the ReMind case-based reasoning tool <ref> [ Cognitive Systems, 1992 ] </ref> provides two mechanisms for specifying features that will be important during case retrieval. First, the user can assign weights to each feature in the case representation or can specify that some features should be ignored entirely when ReMind's nearest-neighbor case retrieval algorithm is used. <p> The ReMind case-based reasoning shell <ref> [ Cognitive Systems, 1992 ] </ref> also uses decision trees as one of its available case-indexing methods. Like MayTag, the system creates a decision tree from the case base during training by determining which context features correspond to each solution feature.
Reference: [ Correa, 1988 ] <author> Correa, N.. </author> <title> A Binding Rule for Government-Binding Parsing. </title> <booktitle> In Proceedings of COLING-88, </booktitle> <address> Budapest, </address> <year> 1988. </year>
Reference: [ Cottrell, 1986 ] <author> Cottrell, G. </author> <title> A Connectionist Approach to Word Sense Disambiguation. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference: [ Cuetos and Mitchell, 1988 ] <author> Cuetos, F. and Mitchell, D. C. </author> <title> Cross-Linguistic Differences in Parsing: Restrictions on the Use of the Late Closure Strategy in Spanish. </title> <journal> Cognition, </journal> <volume> 30(1) </volume> <pages> 73-105, </pages> <year> 1988. </year>
Reference: [ Cullingford, 1986 ] <author> Cullingford, R. </author> <title> Natural Language Processing. </title> <publisher> Rowman and Littlefield, </publisher> <address> Totowa, NJ, </address> <year> 1986. </year>
Reference: [ Daelemans et al., 1994 ] <author> Daelemans, W., Durieux, G., and Gillis, S. </author> <title> The Acquisition of Stress: A Data-Oriented Approach. </title> <journal> Computational Linguistics, </journal> <volume> 20(3) </volume> <pages> 421-451, </pages> <year> 1994. </year>
Reference-contexts: They have found that instance-based approaches to language acquisition work well for low-level language tasks like stress acquisition <ref> [ Daelemans et al., 1994 ] </ref> and grapheme-to-phoneme conversion 27 [ Bosch and Daelemans, 1993 ] and have hypothesized that they will also perform well for higher level tasks.
Reference: [ Daelemans, to appear ] <author> Daelemans, W. </author> <title> Memory-Based Lexical Acquisition and Processing. </title> <booktitle> In Lecture Notes in Artificial Intelligence, Machine Translation and the Lexicon. to appear. </booktitle>
Reference-contexts: In addition, they argue that language acquisition is a behavior-based, data-driven process rather than one guided by knowledge-based principles and parameter setting. Like the work presented here, the authors view many problems in NLP as categorization problems, to which case-based learning techniques can be applied <ref> [ Daelemans, to appear ] </ref> . A commercial system that uses a case-based architecture to acquire parsing knowledge is Parse-O-Matic [ Goodman, 1991 ] . Parse-O-Matic is a conceptual sentence analyzer that builds semantic case frame representations directly from simple requests.
Reference: [ Dagan and Itai, 1991 ] <author> Dagan, I. and Itai, A. </author> <title> A Statistical Filter for Resolving Pronoun References. </title> <editor> In Feldman, Y. A. and Bruckstein, A., editors, </editor> <booktitle> Artificial Intelligence and Computer Vision, </booktitle> <pages> pages 125-135. </pages> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: However, corpora of even 150 million words have been found to be inadequate for some statistical approaches to ambiguity resolution <ref> [ Dagan and Itai, 1991 ] </ref> .
Reference: [ Daneman and Carpenter, 1980 ] <author> Daneman, M. and Carpenter, P. A. </author> <title> Individual Differences in Working Memory and Reading. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 19 </volume> <pages> 450-466, </pages> <year> 1980. </year>
Reference: [ Daneman and Carpenter, 1983 ] <author> Daneman, M. and Carpenter, P. A. </author> <title> Individual Differences in Integrating Information Between and Within Sentences. </title> <journal> Journal of Experimental Psychology: Learning, Memory,and Cognition, </journal> <volume> 9 </volume> <pages> 561-584, </pages> <year> 1983. </year> <month> 170 </month>
Reference: [ DeJong, 1979 ] <author> DeJong, G. F. </author> <title> Skimming Stories in Real Time: An Experiment in Integrated Understanding. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1979. </year> <note> Also available as Tech Report YALEU/CSD/RR #158. </note>
Reference-contexts: Other text analyzers relied on detailed, hand-coded knowledge of scripts, plans, goals, and even political beliefs, in addition to hand-coded lexicons (e.g., SAM [ Schank and Riesbeck, 1981 ] , FRUMP <ref> [ DeJong, 1979 ] </ref> , PAM [ Wilensky, 1978 ] , and POLITICS [ Carbonell, 1978 ] ). The Boris system [ Dyer, 1983 ] included all of these knowledge structures (and more) and integrated them directly into the parser.
Reference: [ Delannoy et al., 1993 ] <author> Delannoy, J. F., Feng, C., and Matwin, S.and Szpakowicz S. </author> <title> Knowledge Extraction from Text: Machine Learning for Text-to-rule Translation. </title> <booktitle> In Proceedings of European Conference on Machine Learning Workshop on Machine Learning and Text Analysis, </booktitle> <pages> pages 7-13, </pages> <year> 1993. </year>
Reference: [ DeMarcken, 1990 ] <author> DeMarcken, C. </author> <title> Parsing the LOB Corpus. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the ACL. Association for Computational Linguistics, </booktitle> <year> 1990. </year>
Reference: [ Dyer, 1983 ] <author> Dyer, M. </author> <title> In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: The Boris system <ref> [ Dyer, 1983 ] </ref> included all of these knowledge structures (and more) and integrated them directly into the parser. Martin's DMAP system [ Riesbeck and Martin, 1985 ] , which processed texts using a memory-based approach to language understanding, relied on a hand-coded representation of event memory.
Reference: [ Elman, 1990 ] <author> Elman, J. </author> <title> Finding Structure in Time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference: [ Fawcett and Utgoff, 1992 ] <author> Fawcett, T. and Utgoff, P. </author> <title> Automatic feature Generation for Problem Solving Systems. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 144-153, </pages> <institution> University of Aberdeen, </institution> <address> UK, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Fisher and Riloff, 1992 ] <author> Fisher, D. and Riloff, E. </author> <title> Applying Statistical Methods to Small Corpora: Benefitting from a Limited Domain. </title> <booktitle> In Working Notes of AAAI Fall Symposium Series, </booktitle> <pages> pages 47-53, </pages> <address> Cambride, MA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference: [ Frazier and Fodor, 1978 ] <author> Frazier, L. and Fodor, J. D. </author> <title> The Sausage Machine: A New Two-Stage Parsing Model. </title> <journal> Cognition, </journal> <volume> 6 </volume> <pages> 291-325, </pages> <year> 1978. </year>
Reference: [ Gernsbacher et al., 1989 ] <author> Gernsbacher, M. A., Hargreaves, D. J., and Beeman, M. </author> <title> Building and Accessing Clausal Representations: The Advantage of First Mention Versus the Advantage of Clause Recency. </title> <journal> Journal of Memory and Language, </journal> <volume> 28 </volume> <pages> 735-755, </pages> <year> 1989. </year>
Reference-contexts: In particular, it has been shown that the accessibility of the first discourse object, which very often corresponds to the subject of the sentence, remains high even at the end of a sentence <ref> [ Gernsbacher et al., 1989 ] </ref> . This subject accessibility bias is an example of a more general focus of attention bias.
Reference: [ Gibson et al., 1993 ] <author> Gibson, E., Pearlmutter, N., Canseco-Gonzalez, E., and Hickok, G. </author> <title> Cross-linguistic Attachment Preferences: Evidence from English and Spanish. </title> <booktitle> In Sixth Annual CUNY Sentence Processing Conference, </booktitle> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1993. </year> <note> Only abstract in the Sentence Processing Conference proceedings. Full manuscript to appear in journal. </note>
Reference: [ Gibson, 1990 ] <author> Gibson, E. </author> <title> Recency Preferences and Garden-Path Effects. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, 1990. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [ Golding and Rosenbloom, 1991 ] <author> Golding, A. R. and Rosenbloom, P. S. </author> <title> Improving Rule-Based Systems through Case-Based Reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 22-27, </pages> <address> Anaheim, CA, 1991. </address> <publisher> AAAI Press / MIT Press. </publisher> <pages> 171 </pages>
Reference-contexts: [ Ashley, 1990, Rissland and Ashley, 1986 ] creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE [ Branting and Porter, 1991, Branting, 1991 ] and CABARET [ Rissland and Skalak, 1991 ] analyze problems in the domain of legal reasoning; (3) ANAPRON's <ref> [ Golding and Rosenbloom, 1991 ] </ref> interpretive task is word pronunciation. The remainder of this section of the thesis, however, focuses specifically on related work from CBR in the area of knowledge acquisition for NLP systems. <p> Stanfill and Waltz [1986], for example, used a case-based approach in their MBRtalk system that learns how to pronounce English words. More recently, ANAPRON <ref> [ Golding and Rosenbloom, 1991 ] </ref> tackled the problem of surname pronunciation using an architecture that combines rule-based and case-based reasoning. <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET [ Rissland and Skalak, 1991 ] , and ANAPRON <ref> [ Golding and Rosenbloom, 1991 ] </ref> . Very generally, case similarity in MayTag is assessed using a hybrid case retrieval algorithm that combines a k-nearest neighbors matching routine with a decision tree approach for finding relevant features. <p> Although the problems of representing and recognizing open-textured concepts remain largely unsolved, a number of CBR systems have offered promising computational framework for the interpretation of open-textured concepts. In particular, mixed-paradigm reasoners like ANAPRON <ref> [ Golding and Rosenbloom, 1991 ] </ref> and CABARET [ Rissland and 4 We will see shortly that this taxonomy will be a component of the concept description language made available to MayTag's learning algorithm. <p> the subject of the training case is (company-name). 10 More than ten cases will be returned if there are ties. 11 Other values of k were tested, but setting k to ten produced the best results overall given the corpus, taxonomies, and training sets used in the experiments. 70 ANAPRON <ref> [ Golding and Rosenbloom, 1991 ] </ref> . 12 ) MayTag, however, differs from many traditional case-based problem solvers in that it employs no case adaptation phase the retrieved solution is used directly rather than being modified to fit the new situation. <p> In the same way that ANAPRON <ref> [ Golding, 1991, Golding and Rosenbloom, 1991 ] </ref> and CABARET [ Rissland and Skalak, 1991 ] combine case-based and rule-based reasoning, we might use handcrafted rules to represent a small set of safe, clear-cut relative pronoun disambiguation heuristics and then rely on cases to represent the exceptions. 127 of this class
Reference: [ Golding, 1991 ] <author> Golding, A. R. </author> <title> Pronouncing Names by a Combination of Case-Based and Rule-Based Reasoning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: In the same way that ANAPRON <ref> [ Golding, 1991, Golding and Rosenbloom, 1991 ] </ref> and CABARET [ Rissland and Skalak, 1991 ] combine case-based and rule-based reasoning, we might use handcrafted rules to represent a small set of safe, clear-cut relative pronoun disambiguation heuristics and then rely on cases to represent the exceptions. 127 of this class
Reference: [ Goodman, 1991 ] <author> Goodman, M. </author> <title> A Case-Based, </title> <booktitle> Inductive Architecture for Natural Language Processing. In Working Notes of the Machine Learning of Natural Language and Ontology AAAI Spring Symposium, </booktitle> <institution> Stanford University, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Like the work presented here, the authors view many problems in NLP as categorization problems, to which case-based learning techniques can be applied [ Daelemans, to appear ] . A commercial system that uses a case-based architecture to acquire parsing knowledge is Parse-O-Matic <ref> [ Goodman, 1991 ] </ref> . Parse-O-Matic is a conceptual sentence analyzer that builds semantic case frame representations directly from simple requests.
Reference: [ Granger, 1977 ] <author> Granger, R. Foulup: </author> <title> A program that figures out meanings of words from context. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 172-178. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1977. </year>
Reference: [ Grefenstette, 1992 ] <author> Grefenstette, G. SEXTANT: </author> <title> Exploring unexplored contexts for semantic extraction from syntactic analysis. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 324-326, </pages> <institution> University of Delaware, Newark, DE, 1992. Association for Computational Linguistics. </institution>
Reference: [ Grishman et al., 1986 ] <author> Grishman, R., Hirschman, L., and Nhan, N. T. </author> <title> Discovery procedures for sublanguage selectional patterns: Initial experiments. </title> <journal> Computational Linguistics, </journal> <volume> 12 </volume> <pages> 205-215, </pages> <year> 1986. </year>
Reference-contexts: Smart interfaces, on the other hand, assist the user in tagging or bracketing a corpus (see Marcus [1991]), but do not generally assist in the acquisition and maintenance of disambiguation heuristics. Notable exceptions are interfaces that help the user define semantic case frames and their associated selectional restrictions <ref> [ Ayuso et al., 1987, Grishman et al., 1986 ] </ref> structures that implicitly perform some lexical and structural disambiguation. Traditionally, this type of knowledge is crafted exclusively by hand. An intelligent interface with some similarities to Kenmore is the Simmons and Yu system [ Simmons and Yu, 1992 ] .
Reference: [ Grosz and Sidner, 1986 ] <author> Grosz, B. J. and Sidner, C.L. </author> <title> Attention, Intentions, and the Structure of Discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12(3) </volume> <pages> 175-204, </pages> <year> 1986. </year>
Reference-contexts: However, focus is generally considered a discourse problem, that is, a problem that spans multiple sentences <ref> [ Sidner, 1983, Grosz and Sidner, 1986 ] </ref> . Because this work concentrates on the analysis of individual sentences, issues of focus will not be explicitly addressed. 47 The new recipe will produce approximately five dozen cookies.
Reference: [ Hammond, 1989 ] <author> Hammond, K. J. </author> <title> Case-based planning: Viewing planning as a memory task. </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: Cognitive Sys tems, 1992 ] , and CYRUS's E-MOPs [ Kolodner, 1983 ] . (See Chapter 6.) Case Retrieval: Kenmore's case retrieval system is based on a nearest-neighbor similarity metric and, hence, is much simpler than the domain-dependent and knowledge-based case retrieval algorithms typically employed in CBR systems (e.g., CHEF <ref> [ Hammond, 1989 ] </ref> ). 26 Best Case Selection: Kenmore uses a simple voting scheme to derive a solution from the retrieved cases rather than using more sophisticated case selection methods (e.g., HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] and GREBE [ Branting and Porter, 1991, Branting, 1991 ] <p> Although some case-based reasoning systems use a richer case representation (e.g., CHEF <ref> [ Hammond, 1989 ] </ref> , GREBE [ Branting and Porter, 1991, Branting, 1991 ] ), the simple attribute-value pair knowledge representation is a very general representation scheme that can be used in conjunction with case-based algorithms (see HYPO [ Ashley, 1990 ] ) as well as the vast majority of symbolic <p> For example, the CYRUS system [ Kolodner, 1983 ] and many follow-on CBR systems (e.g., CHEF <ref> [ Hammond, 1989 ] </ref> ) use E-MOPs to organize cases hierarchically in a dynamic memory [ Schank, 1982 ] . The resulting discrimination network is then traversed when processing new problem cases. The E-MOPs indexing scheme, however, explicitly encodes generalizations of individual cases as higher level nodes in the network.
Reference: [ Hart, 1961 ] <author> Hart, H. L. A. </author> <title> The Concept of Law. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1961. </year>
Reference: [ Hartigan, 1975 ] <author> Hartigan, J. </author> <title> Clustering Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: In one sense, the system is an intelligent interface for designing natural language processing systems (although the final system is entirely automatic). Because cases are so complex, indexing them for retrieval becomes a problem. Parse-O-Matic relies on an automated, inductive, statistical indexing scheme based on Hartigan's interaction detection algorithm <ref> [ Hartigan, 1975 ] </ref> . In addition, it relies on a set of hand-crafted heuristics that decide which cases and which features of the current case to present to the clustering algorithm. It is the indexing scheme that generalizes the cases/rules for more flexible retrieval.
Reference: [ Hastings et al., 1991 ] <author> Hastings, P., Lytinen, S., and Lindsay, R. </author> <title> Learning Words from Context. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <institution> Northwestern University, Chicago, IL, </institution> <year> 1991. </year>
Reference: [ Hindle and Rooth, 1993 ] <author> Hindle, D. and Rooth, M. </author> <title> Structural Ambiguity and Lexical Relations. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 103-120, </pages> <year> 1993. </year>
Reference: [ Hobbs, 1986 ] <author> Hobbs, J. </author> <title> Resolving Pronoun References. </title> <editor> In Grosz, B., Sparck Jones, K., and Webber, B., editors, </editor> <booktitle> Readings in Natural Language Processing, </booktitle> <pages> pages 339-352. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference: [ Ingria and Stallard, 1989 ] <author> Ingria, R. and Stallard, D. </author> <title> A computational mechanism for pronominal reference. </title> <booktitle> In Proceedings of the 27th Annual Meeting of the ACL, Vancouver, 1989. Association for Computational Linguistics. </booktitle>
Reference: [ Jacobs and Zernik, 1988 ] <author> Jacobs, P. and Zernik, U. </author> <title> Acquiring Lexical Knowledge from Text: A Case Study. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 739-744, </pages> <address> St. Paul, MN, 1988. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 172 </pages>
Reference: [ Jain, 1989 ] <author> Jain, Ajay. </author> <title> A Connectionist Architecture for Sequential Symbolic Domains . Technical Report CMU-CS-89-187, </title> <institution> Carnegie Mellon University, </institution> <year> 1989. </year>
Reference: [ Jelinek, 1985 ] <author> Jelinek, F. </author> <title> Markov Source Modeling of Text Generation. </title> <editor> In Skwirzinski, J., editor, </editor> <booktitle> Impact of Processing Techniques on Communication. </booktitle> <address> Dordrecht, </address> <year> 1985. </year>
Reference: [ Keil and Kelly, 1987 ] <author> Keil, Frank C. and Kelly, Michael H. </author> <title> Developmental changes in category structure. </title> <editor> In Harnad, S., editor, </editor> <title> Categorical Perception: </title> <journal> The Groundwork of Cognition, </journal> <pages> pages 491-510. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference: [ Kimball, 1973 ] <author> Kimball, J. </author> <title> Seven Principles of Surface Structure Parsing in Natural Language. </title> <journal> Cognition, </journal> <volume> 2 </volume> <pages> 15-47, </pages> <year> 1973. </year>
Reference: [ King and Just, 1991 ] <author> King, J. and Just, M. A. </author> <title> Individual Differences in Syntactic Processing: The Role of Working Memory. </title> <journal> Journal of Memory and Language, </journal> <volume> 30 </volume> <pages> 580-602, </pages> <year> 1991. </year>
Reference: [ Kolodner, 1983 ] <author> Kolodner, J. L. </author> <title> Maintaining Organization in a Dynamic Long-Term Memory. </title> <journal> Cognitive Science, </journal> <volume> 7(4) </volume> <pages> 243-280, </pages> <year> 1983. </year>
Reference-contexts: Case Indexing: In handling lexical ambiguities, Kenmore indexes cases using a decision tree subsystem. In particular, this aspect of the system is related to HYPO's dimensions [ Rissland et al., 1984 ] , ReMind's prototypes and inductive indexing [ Cognitive Sys tems, 1992 ] , and CYRUS's E-MOPs <ref> [ Kolodner, 1983 ] </ref> . (See Chapter 6.) Case Retrieval: Kenmore's case retrieval system is based on a nearest-neighbor similarity metric and, hence, is much simpler than the domain-dependent and knowledge-based case retrieval algorithms typically employed in CBR systems (e.g., CHEF [ Hammond, 1989 ] ). 26 Best Case Selection: Kenmore <p> For example, the CYRUS system <ref> [ Kolodner, 1983 ] </ref> and many follow-on CBR systems (e.g., CHEF [ Hammond, 1989 ] ) use E-MOPs to organize cases hierarchically in a dynamic memory [ Schank, 1982 ] . The resulting discrimination network is then traversed when processing new problem cases.
Reference: [ Kolodner, 1993 ] <author> Kolodner, J. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The specific parser used throughout is the CIRCUS conceptual sentence analyzer [ Lehnert, 1990 ] . This is not a requirement of the framework, however, and a variety of sentence analyzers could be used in its place. Finally, the framework requires a case-based reasoning (CBR) module <ref> [ Riesbeck and Schank, 1989, Kolodner, 1993 ] </ref> . Very generally, CBR systems solve problems by first creating a case base of previous problem-solving episodes. Then, when a new problem enters the system, the most similar case is retrieved from the case base and used to solve the novel problem.
Reference: [ Laird, 1987 ] <author> Laird, </author> <title> J.E. Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: As such, it may seem similar to SOAR <ref> [ Laird, 1987, Newell, 1990 ] </ref> a general architecture for building intelligent systems. Both systems, for example, are able to handle seemingly very different problems within a single framework; both systems incorporate a learning component. Nevertheless, the two systems vary tremendously. First, SOAR provides a general problem-solving architecture.
Reference: [ Lakoff, 1987 ] <author> Lakoff, G. Women, </author> <title> Fire, and Dangerous Things: What Categories Reveal about the Mind. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, IL, </address> <year> 1987. </year>
Reference: [ Lange and Dyer, 1989 ] <author> Lange, T. and Dyer, M. </author> <title> High-Level Inferencing in a Connectionist Network. </title> <journal> Connection Science, </journal> <volume> 4(2), </volume> <year> 1989. </year>
Reference: [ Lappin and McCord, 1990 ] <author> Lappin, S and McCord, M. </author> <title> A syntactic filter on pronominal anaphora for slot grammar. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the ACL, </booktitle> <institution> University of Pittsburgh, Pittsburgh, PA, 1990. Association for Computational Linguistics. </institution>
Reference: [ Lebowitz, 1980 ] <author> Lebowitz, M. </author> <title> Generalization and Memory in an Integrated Understanding System. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1980. </year> <note> Also available as Tech Report YALEU/CSD/RR #186. </note>
Reference: [ Lehman et al., 1993 ] <author> Lehman, J., Lewis, R., and Newell, A. </author> <title> Integrating Knowledge Sources in Language Comprehension. </title> <editor> In Rosenbloom, P., Laird, J., and Newell, A., editors, </editor> <booktitle> The SOAR Papers: Research on Integrated Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 1309-1314. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: First, SOAR provides a general problem-solving architecture. Kenmore is not nearly as general an architecture it was designed solely for the acquisition of knowledge needed for natural language processing. Natural language understanding, for instance, is just one of the tasks that can be handled within SOAR <ref> [ Lehman et al., 1993 ] </ref> . Second, all aspects of the SOAR architecture are based on a set of assumptions regarding the cognitive nature of problem solving; cognitive biases and cognitive limitations are built directly into the system architecture.
Reference: [ Lehnert et al., 1990 ] <author> Lehnert, W., Cardie, C., and Riloff, E. </author> <title> Analyzing Research Papers Using Citation Sentences. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 511-518, </pages> <address> Cambridge, MA, July 1990. </address> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 173 </pages>
Reference-contexts: to provide natural language processing capabilities for a variety of projects including summarization of wire service texts [ Lehnert et al., 1993a, Lehnert et al., 1992, Lehnert et al., 1991 ] , text classification [ Riloff and Lehnert, 1992 ] , and the analysis of citation sentences in research papers <ref> [ Lehnert et al., 1990 ] </ref> . Its components will be described in the next three sections. 3.1.1 Syntactic Processing in CIRCUS CIRCUS's stack-oriented syntactic analyzer segments incoming text into constituent phrases.
Reference: [ Lehnert et al., 1991 ] <author> Lehnert, W., Cardie, C., Fisher, D., Riloff, E., and Williams, R. </author> <title> University of Massachusetts: Description of the CIRCUS System as Used in MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 223-233, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This distinction is important in the terrorism domain where the goal is to extract from texts only information concerning eight classes of terrorist events including murders, bombings, attacks, and kidnappings. All other information effectively should be ignored. To be successful in this selective concept extraction task <ref> [ Lehnert et al., 1991 ] </ref> , a sentence analyzer not only needs access to word-concept pairings (e.g., the word killed is linked to the TERRORIST MURDER concept), but must also accurately distinguish legitimate concept activation contexts from bogus ones (e.g., the phrase terrorists killed implies that a TERRORIST MURDER occurred, <p> The remaining entries in the general semantic feature hierarchy, however, might be of use in other domains. The domain-independent features of Table 7.9 were, in fact, also part of the semantic feature taxonomy for the terrorism domain <ref> [ Lehnert et al., 1991 ] </ref> . <p> In addition, the resulting heuristics are prone to errors of omission and may not generalize to new contexts. For example, the UMass/MUC-3 CIRCUS system 4 <ref> [ Lehnert et al., 1991 ] </ref> began with nineteen rules for finding the antecedents of relative pronouns. These rules included both structural and semantic knowledge and were based on approximately 50 instances of relative pronouns. As counter-examples were identified, new rules were added (approximately ten) and existing rules changed.
Reference: [ Lehnert et al., 1992 ] <author> Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., and Soderland, S. </author> <title> University of Massachusetts: Description of the CIRCUS System as Used in MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 282-288, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Lehnert et al., 1993a ] <author> Lehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie, C., Peterson, J., Feng, F., Dolan, C., and Goldman, S. </author> <title> University of Massachusetts/Hughes: Description of the CIRCUS System as Used for TIPSTER Text. </title> <booktitle> In Proceedings, TIPSTER Text Program (Phase I), </booktitle> <pages> pages 241-256, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: all of the results have dropped somewhat; however, chi-square analysis still shows that MayTag's performance is significantly better than the baselines (p = :01). 5.5.2.1 MayTag in the MUC-5/TIPSTER Evaluation MayTag handled the semantic feature tagging task for the UMass/Hughes CIRCUS system that participated in the MUC-5 and TIPSTER evaluations <ref> [ Lehnert et al., 1993a, Lehnert et al., 1993b ] </ref> . <p> Word Random Default MayTag MayTag Definition Selection (open-class (all words) Feature words) p-o-s 34.3 81.5 91.0 95.0 spec-sem 37.3 58.1 74.0 85.5 concept 84.2 91.7 94.3 96.8 system, the OTB tagger <ref> [ Lehnert et al., 1993a ] </ref> supplied parts of speech for each word in the texts and domain-specific concept triggering information was generated by the AutoSlog system [ Riloff, 1993 ] , which was described briefly in Section 2.6.
Reference: [ Lehnert et al., 1993b ] <author> Lehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie, C., Peterson, J., Feng, F., Dolan, C., and Goldman, S. </author> <title> University of Massachusetts/Hughes: Description of the CIRCUS System as Used in MUC-5. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-291, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: all of the results have dropped somewhat; however, chi-square analysis still shows that MayTag's performance is significantly better than the baselines (p = :01). 5.5.2.1 MayTag in the MUC-5/TIPSTER Evaluation MayTag handled the semantic feature tagging task for the UMass/Hughes CIRCUS system that participated in the MUC-5 and TIPSTER evaluations <ref> [ Lehnert et al., 1993a, Lehnert et al., 1993b ] </ref> . <p> In the terrorism domain, for example, whenever killed is used as a verb in the passive voice, the concept 10 In addition, OTB <ref> [ Lehnert et al., 1993b ] </ref> was used for part-of-speech tagging. 11 This is the same case base and experimental design as was used in Section 5.5.2.1 that tested MayTag's ability to predict semantic features for the MUC-5/TIPSTER evaluation.
Reference: [ Lehnert et al., 1994 ] <author> Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., and Soderland, S. </author> <title> Evaluating an Information Extraction System. </title> <journal> Journal of Integrated Computer-Aided Engineering, </journal> <volume> 1(6), </volume> <year> 1994. </year>
Reference-contexts: The general task in the evaluations is one of domain-specific information extraction <ref> [ Lehnert et al., 1994 ] </ref> the NLP system processes a collection of texts, finds all information of interest, and produces a summary of that information in a rigid template format.
Reference: [ Lehnert, 1978 ] <author> Lehnert, W. </author> <title> The Process of Question Answering. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1978. </year>
Reference-contexts: Until fairly recently, natural language processing systems relied exclusively on hand-crafted lexicons and knowledge bases. Early systems for conceptual sentence analysis (e.g., ELI [ Riesbeck and Schank, 1978 ] and QA <ref> [ Lehnert, 1978 ] </ref> ) embedded much of their background knowledge in hand-coded lexicons.
Reference: [ Lehnert, 1990 ] <author> Lehnert, W. </author> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <editor> In Barnden, J. and Pollack, J., editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <pages> pages 135-164. </pages> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ, </address> <year> 1990. </year>
Reference-contexts: For this reason, a human supplies supervision during Kenmore's training phase. Kenmore relies on three major components. First, it requires a corpus of texts a collection of on-line documents. Second, it requires a robust sentence analyzer, or parser. The specific parser used throughout is the CIRCUS conceptual sentence analyzer <ref> [ Lehnert, 1990 ] </ref> . This is not a requirement of the framework, however, and a variety of sentence analyzers could be used in its place. Finally, the framework requires a case-based reasoning (CBR) module [ Riesbeck and Schank, 1989, Kolodner, 1993 ] . <p> We also assume that word meanings will be represented in terms of one or more semantic features from a predefined taxonomy designed for use in the joint ventures domain. 4 Finally, we assume that Kenmore has been instantiated with the CIRCUS <ref> [ Lehnert, 1990 ] </ref> sentence analyzer. To generate word sense disambiguation heuristics for use with the JV corpus, Kenmore first randomly selects a subset of sentences from the corpus and presents them as input to CIRCUS. <p> This thesis, however, emphasizes the acquisition of knowledge for the task of conceptual sentence analysis 7 [ Riesbeck, 1975 ] . This chapter describes CIRCUS <ref> [ Lehnert, 1990 ] </ref> , the conceptual sentence analyzer used throughout 6 Section 9.2 discusses the requirements of the sentence analyzer in more detail. 7 Conceptual sentence analysis focuses on building a representation of the meaning of a sentence rather than a representation of the syntactic structure of a sentence. 15 <p> Despite the general applicability of the framework, however, this work emphasizes the acquisition of knowledge only for the task of conceptual sentence analysis [ Riesbeck, 1975 ] . This chapter first describes the conceptual sentence analyzer called CIRCUS <ref> [ Lehnert, 1990 ] </ref> that will be used in all experiments throughout this work. <p> They not only allow syntactic and semantic processing to happen at the same time, but intertwine the parser's morphological, syntactic, semantic, and pragmatic knowledge in a monolithic and largely procedural representation. CIRCUS <ref> [ Lehnert, 1990 ] </ref> is a conceptual sentence analyzer that synthesizes three distinct processing techniques to produce a semantic case frame representation of an input sentence. <p> The Latin American Terrorism corpus was developed for the MUC-3 and MUC-4 performance evaluations and was described in Chapter 4.3.1. The CIRCUS conceptual sentence analyzer <ref> [ Lehnert, 1990 ] </ref> was described in Chapter 3. It takes as input a single sentence and produces as output a semantic case frame representation of the meaning of the sentence.
Reference: [ Lewis and Gale, 1994 ] <author> Lewis, D. D. and Gale, W. A. </author> <title> A Sequential Algorithm for Training Text Classifiers. </title> <booktitle> In Proceedings of ACM SIGIR, </booktitle> <address> Dublin, Ireland, </address> <year> 1994. </year>
Reference: [ Lytinen and Roberts, 1989 ] <author> Lytinen, S. and Roberts, S. </author> <title> Lexical Acquisition as a By-Product of Natural Language Processing. </title> <booktitle> In Proceedings, IJCAI-89 Workshop on Lexical Acquisition, </booktitle> <year> 1989. </year>
Reference: [ Lytinen, 1984 ] <author> Lytinen, S. </author> <title> The Organization of Knowledge in a Multi-lingual, Integrated Parser. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1984. </year> <note> Also available as Tech Report YALEU/CSD/RR #340. </note>
Reference: [ Marcus et al., 1993 ] <author> Marcus, M., Marcinkiewicz, M., and Santorini, B. </author> <title> Building a Large Annotated Corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year> <month> 174 </month>
Reference-contexts: Unfortunately, statistical methods require the existence of extremely large, often hand-tagged training corpora. For example, one corpus that is often used to construct statistical models for natural language acquisition tasks is the 4.5 million-word Penn Treebank <ref> [ Marcus et al., 1993 ] </ref> ; the smaller Tagged Brown Corpus (one million words) is considered too small for many tagging tasks [ Church et al., 1991 ] .
Reference: [ Marcus, 1991 ] <author> Marcus, Mitchell. </author> <title> The Automatic Acquisition of Linguistic Structure from Large Corpora: An Overview of Work at the University of Pennsylvania. </title> <booktitle> In Working Notes of the Machine Learning of Natural Language and Ontology AAAI Spring Symposium, </booktitle> <institution> Stanford University, </institution> <month> March </month> <year> 1991. </year>
Reference: [ Martin, 1992 ] <author> Martin, J. </author> <title> Computer Understanding of Conventional Metaphoric Language. </title> <journal> Cognitive Science, </journal> <volume> 16(2) </volume> <pages> 233-270, </pages> <year> 1992. </year>
Reference: [ Matwin and Szpakowicz, 1993 ] <author> Matwin, S. and Szpakowicz, S. </author> <title> Text Analysis: </title> <booktitle> How Can Machine Learning Help? In Proceedings of the First Conference of the Pacific Association for Computational Linguistics (PACLING), </booktitle> <pages> pages 33-42, </pages> <address> Vancouver, BC, </address> <year> 1993. </year>
Reference: [ McClelland and Kawamoto, 1986 ] <author> McClelland, J. L. and Kawamoto, A. H. </author> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In McClelland, J. L., Rumelhart, D.E., </editor> <title> and PDP Research Group, the, editors, Parallel distributed processing: Explorations in the microstructure of cognition: Vol. 2. Psychological and biological models. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [ Miikkulainen and Dyer, 1987 ] <author> Miikkulainen, R. and Dyer, M. </author> <title> Building Distributed Representations without Microfeatures. </title> <type> Technical Report TR UCLA-AI-87-17, </type> <institution> UCLA, </institution> <year> 1987. </year>
Reference: [ Miikkulainen and Dyer, 1989 ] <author> Miikkulainen, R. and Dyer, M. </author> <title> A Modular Neural Network Architecture for Sequential Paraphrasing of Script-Based Stories. </title> <type> Technical Report UCLA-AI-89-02, </type> <institution> UCLA, </institution> <year> 1989. </year>
Reference: [ Miller, 1956 ] <author> Miller, G. A. </author> <title> The Magical Number Seven, Plus or Minus Two: Some Limits on our Capacity for Processing Information. </title> <journal> Psychological Review, </journal> <volume> 63(1) </volume> <pages> 81-97, </pages> <year> 1956. </year>
Reference-contexts: representation when this built-in bias is removed by discarding the mr-phrase and mr-syn-type features and disallowing references to them in the antecedent class value. 8.6.2 Incorporating the Restricted Memory Bias Psychological studies have determined that people can remember at most seven plus or minus two items at any one time <ref> [ Miller, 1956 ] </ref> . More recently, Daneman and Carpenter [1983, 1980] show that working memory capacity affects a subject's ability to find the referents of pronouns over varying distances.
Reference: [ Muggleton, 1992 ] <author> Muggleton, S. H. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: In addition, the EBL systems mentioned above generally require extensive background knowledge in the form of commonsense knowledge, detailed semantic hierarchies, or causal theories. The Kenmore framework presumes no such background knowledge. Inductive logic programming (ILP) techniques <ref> [ Muggleton, 1992 ] </ref> have only recently been employed to learn case-role mappings (e.g., Zelle and Mooney [1993]) and to learn rules from parsed text (e.g., Delannoy et al. [1993] and Matwin and Szpakowicz [1993]).
Reference: [ Newell, 1990 ] <author> Newell, A. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: As such, it may seem similar to SOAR <ref> [ Laird, 1987, Newell, 1990 ] </ref> a general architecture for building intelligent systems. Both systems, for example, are able to handle seemingly very different problems within a single framework; both systems incorporate a learning component. Nevertheless, the two systems vary tremendously. First, SOAR provides a general problem-solving architecture.
Reference: [ Newport, 1990 ] <author> Newport, E. </author> <title> Maturational Constraints on Language Learning. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 11-28, </pages> <year> 1990. </year>
Reference-contexts: Moreover, it has been hypothesized that language learning in humans is successful precisely because limits on information processing capacities allow children to ignore much of the linguistic data they receive <ref> [ Newport, 1990 ] </ref> . Some computational language learning systems (e.g., Elman [1990]) actually build a short term memory directly into the architecture of the system. WHirlpool's baseline case representation does not necessarily make use of this restrict memory bias, however.
Reference: [ Nicol, 1988 ] <author> Nicol, J. </author> <title> Coreference Processing During Sentence Comprehension. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [ Pazzani, 1991 ] <author> Pazzani, M. J. </author> <title> A computational theory of learning causal relationships. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 401-424, </pages> <year> 1991. </year>
Reference-contexts: Although EBL has also been used for low-level language tasks like learning phonological representations [ Stetham, 1991 ] and higher level tasks like learning causal relationships <ref> [ Bozsahin and Findler, 1992, Pazzani, 1991 ] </ref> , it has not been used to learn solutions to the lexical and structural disambiguation tasks handled within the Kenmore architecture.
Reference: [ Quinlan, 1983 ] <author> Quinlan, J. R. </author> <title> Learning Efficient Classification Procedures and Their Application to Chess End Games. </title> <editor> In Michalski, R. S., Carbonell, J. G., and Mitchell, T. M., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year> <month> 175 </month>
Reference-contexts: In particular, it is well known that the performance of any machine learning system depends critically on the representation of the training cases it receives as input. Unfortunately, finding a good case representation is a notoriously difficult and time-consuming task and presents another potential knowledge-engineering bottleneck during system development <ref> [ Quinlan, 1983 ] </ref> . As a result, this thesis also presents two automated techniques for improving a baseline case representation. The first approach uses decision trees [ Quinlan, 1986 ] to discard irrelevant features from a case representation. <p> Given that feature set specification is a notoriously time-consuming and knowledge-intensive task <ref> [ Quinlan, 1983 ] </ref> , however, it would be better if the feature set could be chosen systematically and automatically. This is exactly what the decision tree algorithm for feature selection described in Section 6.1 was designed to do.
Reference: [ Quinlan, 1986 ] <author> Quinlan, J. R. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Unfortunately, finding a good case representation is a notoriously difficult and time-consuming task and presents another potential knowledge-engineering bottleneck during system development [ Quinlan, 1983 ] . As a result, this thesis also presents two automated techniques for improving a baseline case representation. The first approach uses decision trees <ref> [ Quinlan, 1986 ] </ref> to discard irrelevant features from a case representation. <p> Using Decision Trees to Discard Irrelevant Features (Chapter 6) This chapter presents and evaluates MayTag's decision tree approach to improving a baseline case representation. The technique uses decision trees <ref> [ Quinlan, 1986 ] </ref> to locate the relevant features in a representation so that the irrelevant ones can be discarded. MayTag Performance Analysis (Chapter 7) This chapter provides a deeper analysis of MayTag's ability to handle lexical ambiguity. <p> However, Almuallim and Dietterich show that the ID3 decision tree system <ref> [ Quinlan, 1986 ] </ref> is not particularly good at selecting a minimum set of features from an original set containing possibly many irrelevant attributes [ Almuallim and Dietterich, 1991 ] .
Reference: [ Quinlan, 1990 ] <author> Quinlan, J. R. </author> <title> Learning Logical Definitions from Relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Although the attribute-value pair representation appears to be adequate for use with the CIRCUS parser, inductive learning algorithms that allow structured case representations (e.g., FOIL <ref> [ Quinlan, 1990 ] </ref> ) would be worth exploring, especially for use with sentence analyzers that create and maintain such structured representations.
Reference: [ Quinlan, 1992 ] <author> Quinlan, J. R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: To avoid this additional knowledge engineering bottleneck, we developed an automated approach for locating the relevant features in a baseline instance representation and have incorporated the approach into the original case retrieval algorithm. 14 The modified algorithm uses the C4.5 decision tree system <ref> [ Quinlan, 1992 ] </ref> and will be described in detail in Chapter 6. (Related work will also be discussed in that chapter.) For the remainder of this chapter, however, it will suffice simply to think of this feature selection algorithm as a black box (Figure 5.7) that takes as input a <p> The decision tree in Figure 6.1, for example, would classify a brown, fuzzy ball, four inches in diameter as a football. Although there are a number of methods for building a decision tree that are consistent with a set of training examples, we use the C4.5 decision tree system <ref> [ Quinlan, 1992 ] </ref> . In building a decision tree, C4.5 employs a metric from information theory (i.e., information gain ratio) to decide which feature to test at each branching point.
Reference: [ Resnik and Hearst, 1993 ] <author> Resnik, P. and Hearst, M. </author> <title> Structural Ambiguity and Conceptual Relations. </title> <booktitle> In Proceedings of the ACL Workshop on Very Large Corpora, </booktitle> <institution> Ohio State, 1993. Association for Computational Linguistics. </institution>
Reference: [ Resnik, 1993 ] <author> Resnik, P. </author> <title> Selection and Information:A Class-Based Approach to Lexical Relationships. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference: [ Riesbeck and Martin, 1985 ] <author> Riesbeck, C. and Martin, C. </author> <title> Direct Memory Access Parsing. </title> <editor> In Riesbeck, C. and Kolodner, J., editors, </editor> <title> Experience, Memory and Reasoning. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1985. </year>
Reference-contexts: The Boris system [ Dyer, 1983 ] included all of these knowledge structures (and more) and integrated them directly into the parser. Martin's DMAP system <ref> [ Riesbeck and Martin, 1985 ] </ref> , which processed texts using a memory-based approach to language understanding, relied on a hand-coded representation of event memory.
Reference: [ Riesbeck and Schank, 1978 ] <author> Riesbeck, C. and Schank, R. </author> <title> comprehension by computer: Expectation-base analysis of sentences in context. </title> <editor> In Levelt, W. J. M. and Flores d'Arcais, G.B., editors, </editor> <booktitle> Studies in the Perception of Language. </booktitle> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Moreover, hand-coded heuristics and lexicons are often incomplete and perform poorly in new domains comprised of specialized vocabularies or a different genre of text. Until fairly recently, natural language processing systems relied exclusively on hand-crafted lexicons and knowledge bases. Early systems for conceptual sentence analysis (e.g., ELI <ref> [ Riesbeck and Schank, 1978 ] </ref> and QA [ Lehnert, 1978 ] ) embedded much of their background knowledge in hand-coded lexicons.
Reference: [ Riesbeck and Schank, 1989 ] <author> Riesbeck, C. and Schank, R. </author> <title> Inside Case-Based Reasoning. </title> <publisher> Erlbaum, </publisher> <address> Northvale, NJ, </address> <year> 1989. </year>
Reference-contexts: The specific parser used throughout is the CIRCUS conceptual sentence analyzer [ Lehnert, 1990 ] . This is not a requirement of the framework, however, and a variety of sentence analyzers could be used in its place. Finally, the framework requires a case-based reasoning (CBR) module <ref> [ Riesbeck and Schank, 1989, Kolodner, 1993 ] </ref> . Very generally, CBR systems solve problems by first creating a case base of previous problem-solving episodes. Then, when a new problem enters the system, the most similar case is retrieved from the case base and used to solve the novel problem.
Reference: [ Riesbeck, 1975 ] <author> Riesbeck, C. </author> <title> Conceptual Analysis. </title> <editor> In Schank, R., editor, </editor> <booktitle> Conceptual Information Processing. </booktitle> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: This thesis, however, emphasizes the acquisition of knowledge for the task of conceptual sentence analysis 7 <ref> [ Riesbeck, 1975 ] </ref> . <p> Despite the general applicability of the framework, however, this work emphasizes the acquisition of knowledge only for the task of conceptual sentence analysis <ref> [ Riesbeck, 1975 ] </ref> . This chapter first describes the conceptual sentence analyzer called CIRCUS [ Lehnert, 1990 ] that will be used in all experiments throughout this work.
Reference: [ Riloff and Lehnert, 1992 ] <author> Riloff, E. and Lehnert, W. </author> <title> Classifying Texts Using Relevancy Signatures. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 329-334, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: The CIRCUS system has been used successfully to provide natural language processing capabilities for a variety of projects including summarization of wire service texts [ Lehnert et al., 1993a, Lehnert et al., 1992, Lehnert et al., 1991 ] , text classification <ref> [ Riloff and Lehnert, 1992 ] </ref> , and the analysis of citation sentences in research papers [ Lehnert et al., 1990 ] . Its components will be described in the next three sections. 3.1.1 Syntactic Processing in CIRCUS CIRCUS's stack-oriented syntactic analyzer segments incoming text into constituent phrases.
Reference: [ Riloff, 1993 ] <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 811-816, </pages> <address> Washington, DC, 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: (all words) Feature words) p-o-s 34.3 81.5 91.0 95.0 spec-sem 37.3 58.1 74.0 85.5 concept 84.2 91.7 94.3 96.8 system, the OTB tagger [ Lehnert et al., 1993a ] supplied parts of speech for each word in the texts and domain-specific concept triggering information was generated by the AutoSlog system <ref> [ Riloff, 1993 ] </ref> , which was described briefly in Section 2.6. In addition, the CIRCUS system included three specialists that recognized date expressions, money expressions, and some location expressions, and converted them into canonical forms. <p> MUC-5 Concept Types Corresponding MayTag Concept jv-entity-person-not-jv tie-up-relationship jv-entity-company-not-jv tie-up-relationship jv-person tie-up-relationship jv-entity-not-jv tie-up-relationship jv-entity-company-jv tie-up-relationship-jv-entity-only jv-entity-jv tie-up-relationship-jv-entity-only industry industry industry-production industry-production industry-sales industry-sales industry-service industry-service facility none ownership none own percent ownership-% revenue none nil nil MUC-5 performance evaluation, the UMass/Hughes system used the AutoSlog <ref> [ Riloff, 1993 ] </ref> system to acquire concept activation knowledge for the JV corpus in the form of concept activation rules. For example, consider the following sentence: IBM recently formed a subsidiary for production of PowerPC chips.
Reference: [ Rissland and Ashley, 1986 ] <author> Rissland, E. L. and Ashley, K. </author> <title> Hypotheticals as Heuristic Device. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 289-297, </pages> <address> Philadelphia, PA, 1986. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As a result, Kenmore can be compared to existing CBR systems along a number of dimensions including: Case Representation: Like HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> , Kenmore uses a simple attribute-value pair case representation rather than a more structured case representation (e.g., the semantic network case representation of GREBE [ Brant ing and Porter, 1991, Branting, 1991 ] ). <p> much simpler than the domain-dependent and knowledge-based case retrieval algorithms typically employed in CBR systems (e.g., CHEF [ Hammond, 1989 ] ). 26 Best Case Selection: Kenmore uses a simple voting scheme to derive a solution from the retrieved cases rather than using more sophisticated case selection methods (e.g., HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> and GREBE [ Branting and Porter, 1991, Branting, 1991 ] ). <p> This makes Kenmore an interpretive CBR system a system that analyzes a new case by comparing and analogizing it with previous interpretation episodes. Interpretive CBR systems exist in a number of domains: (1) The HYPO system, for example, <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE [ Branting and Porter, 1991, Branting, 1991 ] and CABARET [ Rissland and Skalak, 1991 ] analyze problems in the domain of legal reasoning; (3) ANAPRON's [ <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO <ref> [ Ashley, 1990, Rissland and Ashley, 1986 ] </ref> , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET [ Rissland and Skalak, 1991 ] , and ANAPRON [ Golding and Rosenbloom, 1991 ] .
Reference: [ Rissland and Skalak, 1991 ] <author> Rissland, E. L. and Skalak, D. B. CABARET: </author> <title> Rule Interpretation in a Hybrid Architecture. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 34 </volume> <pages> 839-887, </pages> <year> 1991. </year> <month> 176 </month>
Reference-contexts: Interpretive CBR systems exist in a number of domains: (1) The HYPO system, for example, [ Ashley, 1990, Rissland and Ashley, 1986 ] creates legal arguments for a problem case in the domain of trade secret disputes; (2) GREBE [ Branting and Porter, 1991, Branting, 1991 ] and CABARET <ref> [ Rissland and Skalak, 1991 ] </ref> analyze problems in the domain of legal reasoning; (3) ANAPRON's [ Golding and Rosenbloom, 1991 ] interpretive task is word pronunciation. <p> As such, MayTag is one of a number of interpretive CBR systems including HYPO [ Ashley, 1990, Rissland and Ashley, 1986 ] , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET <ref> [ Rissland and Skalak, 1991 ] </ref> , and ANAPRON [ Golding and Rosenbloom, 1991 ] . Very generally, case similarity in MayTag is assessed using a hybrid case retrieval algorithm that combines a k-nearest neighbors matching routine with a decision tree approach for finding relevant features. <p> retrieved cases to derive an appropriate syntactic and semantic interpretation for the current word, it is considered an interpretive CBR system. (For examples of other interpretive CBR systmes, see HYPO [ Ashley and Rissland, 1988, Ashley, 1990 ] , GREBE [ Branting and Porter, 1991, Branting, 1991 ] , CABARET <ref> [ Rissland and Skalak, 1991 ] </ref> , 9 Partial matches occur when two features have overlapping, but not identical values. <p> In the same way that ANAPRON [ Golding, 1991, Golding and Rosenbloom, 1991 ] and CABARET <ref> [ Rissland and Skalak, 1991 ] </ref> combine case-based and rule-based reasoning, we might use handcrafted rules to represent a small set of safe, clear-cut relative pronoun disambiguation heuristics and then rely on cases to represent the exceptions. 127 of this class (93% correct in one set of 50 texts), but did
Reference: [ Rissland et al., 1984 ] <author> Rissland, E. L., Valcarce, E. M., and Ashley, K. D. </author> <title> Explaining and Arguing with Examples. </title> <booktitle> In AAAI-84, Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 288-294, </pages> <address> Austin, TX, </address> <year> 1984. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Case Indexing: In handling lexical ambiguities, Kenmore indexes cases using a decision tree subsystem. In particular, this aspect of the system is related to HYPO's dimensions <ref> [ Rissland et al., 1984 ] </ref> , ReMind's prototypes and inductive indexing [ Cognitive Sys tems, 1992 ] , and CYRUS's E-MOPs [ Kolodner, 1983 ] . (See Chapter 6.) Case Retrieval: Kenmore's case retrieval system is based on a nearest-neighbor similarity metric and, hence, is much simpler than the domain-dependent <p> Most CBR systems rely on indexing schemes specified by the system designers that are based on expert knowledge of a particular domain. The HYPO system <ref> [ Rissland et al., 1984, Ashley, 1990 ] </ref> , for example, retrieves relevant cases in the trade secrets legal reasoning domain based on a predetermined set of dimensions that influence the outcome of cases in this domain.
Reference: [ Rissland, 1990 ] <author> Rissland, E. L. </author> <title> Artificial Intelligence and Law: Stepping Stones to a Model of Legal Reasoning. </title> <journal> The Yale Law Journal, </journal> <volume> 99(8) </volume> <pages> 1957-1981, </pages> <year> 1990. </year>
Reference: [ Rosch and Mervis, 1975 ] <author> Rosch, E. and Mervis, C. </author> <title> Family Resemblances: Studies in the Internal Structure of Categories. </title> <journal> Cognitive Psychology, </journal> <volume> 7 </volume> <pages> 573-605, </pages> <year> 1975. </year>
Reference: [ Rosch, 1973 ] <author> Rosch, E. </author> <title> Natural Categories. </title> <journal> Cognitive Psychology, </journal> <volume> 4 </volume> <pages> 328-350, </pages> <year> 1973. </year>
Reference: [ Samuelsson and Rayner, 1991 ] <author> Samuelsson, C. and Rayner, M. </author> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology. In Working Notes of the Machine Learning of Natural Language and Ontology AAAI Spring Symposium, </booktitle> <pages> pages 143-145, </pages> <institution> Stanford University, </institution> <year> 1991. </year> <title> DFKI:Kaiserslautern,FRG. Check for more recent ref. </title>
Reference: [ Samuelsson, 1991 ] <author> Samuelsson, C. </author> <title> Using Explanation-Based Learning to Speed Up Natural Language Systems. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm, </institution> <year> 1991. </year>
Reference: [ Schank and Riesbeck, 1981 ] <editor> Schank, R. and Riesbeck, C. </editor> <booktitle> Inside Computer Understanding: Five Programs Plus Miniatures. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: Other text analyzers relied on detailed, hand-coded knowledge of scripts, plans, goals, and even political beliefs, in addition to hand-coded lexicons (e.g., SAM <ref> [ Schank and Riesbeck, 1981 ] </ref> , FRUMP [ DeJong, 1979 ] , PAM [ Wilensky, 1978 ] , and POLITICS [ Carbonell, 1978 ] ). The Boris system [ Dyer, 1983 ] included all of these knowledge structures (and more) and integrated them directly into the parser. <p> In the tradition of conceptual analyzers, this component produces no parse tree of the input and employs no global syntactic grammar. It is based on the McEli parser <ref> [ Schank and Riesbeck, 1981 ] </ref> and uses lexically-indexed, local syntactic knowledge to recognize noun phrases, prepositional phrases, and verb phrases. These constituents are stored in global buffers that track the subject, verb, direct object, indirect object, and prepositional phrases of a sentence.
Reference: [ Schank, 1975 ] <author> Schank, R. </author> <title> Conceptual Information Processing. </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1975. </year>
Reference-contexts: analysis, (2) a marker-passing design for predictive preference semantics, and (3) lexically-indexed control kernels [ Cardie and Lehnert, 1991 ] for handling embedded clauses. 1 Although CIRCUS makes no commitment to a particular style of semantic representation, we use deep semantic case frames of the sort found in conceptual dependency <ref> [ Schank, 1975 ] </ref> . In general, CIRCUS recognizes low-level syntactic constituents like noun phrases, verbs, and prepositional phrases, and consults semantic knowledge prior to making any attachment decisions. In addition, CIRCUS explicitly attaches constituents only to the semantic representation of the sentence.
Reference: [ Schank, 1982 ] <author> Schank, R. </author> <title> Dynamic Memory: A theory of learning in computers and people. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: For example, the CYRUS system [ Kolodner, 1983 ] and many follow-on CBR systems (e.g., CHEF [ Hammond, 1989 ] ) use E-MOPs to organize cases hierarchically in a dynamic memory <ref> [ Schank, 1982 ] </ref> . The resulting discrimination network is then traversed when processing new problem cases. The E-MOPs indexing scheme, however, explicitly encodes generalizations of individual cases as higher level nodes in the network.
Reference: [ Selfridge, 1986 ] <author> Selfridge, M. </author> <title> A computer model of child language learning. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 171-216, </pages> <year> 1986. </year>
Reference: [ Sidner, 1983 ] <author> Sidner, C. L. </author> <title> Focusing in the Comprehension of Definite Anaphora. </title> <editor> In Brady, M. and Berwick, R. C., editors, </editor> <booktitle> Computational Models of Discourse, </booktitle> <pages> pages 267-330. </pages> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: However, focus is generally considered a discourse problem, that is, a problem that spans multiple sentences <ref> [ Sidner, 1983, Grosz and Sidner, 1986 ] </ref> . Because this work concentrates on the analysis of individual sentences, issues of focus will not be explicitly addressed. 47 The new recipe will produce approximately five dozen cookies.
Reference: [ Simmons and Yu, 1992 ] <author> Simmons, Robert F. and Yu, Yeong-Ho. </author> <title> The Acquisition and Use of Context-Dependent Grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 391-418, </pages> <year> 1992. </year>
Reference-contexts: Traditionally, this type of knowledge is crafted exclusively by hand. An intelligent interface with some similarities to Kenmore is the Simmons and Yu system <ref> [ Simmons and Yu, 1992 ] </ref> . They present a simple user interface for acquiring context-dependent part-of-speech tags and context-dependent grammars as well as an algorithm for applying the acquired rules.
Reference: [ Skalak and Rissland, 1990 ] <author> Skalak, D. and Rissland, E. </author> <title> Inductive Learning in a Mixed Paradigm Setting. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 840-847, </pages> <address> Boston, MA, 1990. </address> <publisher> AAAI Press / MIT Press. </publisher> <pages> 177 </pages>
Reference: [ St. John, 1992 ] <author> St. John, Mark F. </author> <title> The Story Gestalt: A Model of Knowledge-Intensive Processes in Text Comprehension. </title> <journal> Cognitive Science, </journal> <volume> 16(2) </volume> <pages> 271-306, </pages> <year> 1992. </year>
Reference: [ Stanfill and Waltz, 1986 ] <author> Stanfill, C. and Waltz, D. </author> <title> Toward Memory-based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference: [ Stetham, 1991 ] <author> Stetham, S. </author> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology. In Working Notes of the Machine Learning of Natural Language and Ontology AAAI Spring Symposium, </booktitle> <pages> pages 169-173, </pages> <institution> Stanford University, </institution> <year> 1991. </year> <month> DFKI:Kaiserslautern,FRG. </month>
Reference-contexts: The work presented here, on the other hand, focuses on learning knowledge for natural language systems that process text without the use of formal grammars. Although EBL has also been used for low-level language tasks like learning phonological representations <ref> [ Stetham, 1991 ] </ref> and higher level tasks like learning causal relationships [ Bozsahin and Findler, 1992, Pazzani, 1991 ] , it has not been used to learn solutions to the lexical and structural disambiguation tasks handled within the Kenmore architecture.
Reference: [ Utgoff, 1989 ] <author> Utgoff, P. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 161-186, </pages> <year> 1989. </year>
Reference-contexts: This is a disadvantage if we want to employ the case-based learning algorithm during the training phase to propose definitions for the current word. One solution to this problem is to replace C4.5 with an incremental decision tree algorithm <ref> [ Utgoff, 1989, Utgoff, 1994 ] </ref> . This would allow the system to determine the relevant attributes associated with each word definition feature as training cases arrive without reprocessing all preceding instances.
Reference: [ Utgoff, 1994 ] <author> Utgoff, P. </author> <title> An Improved Algorithm for Incremental Induction of Decision Trees. </title> <editor> In Cohen, W. and Hirsh, H., editors, </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 318-325, </pages> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is a disadvantage if we want to employ the case-based learning algorithm during the training phase to propose definitions for the current word. One solution to this problem is to replace C4.5 with an incremental decision tree algorithm <ref> [ Utgoff, 1989, Utgoff, 1994 ] </ref> . This would allow the system to determine the relevant attributes associated with each word definition feature as training cases arrive without reprocessing all preceding instances.
Reference: [ Waltz and Pollack, 1985 ] <author> Waltz, D. and Pollack, J. </author> <title> Massively parallel parsing: A strongly interactive model of natural language interpretation. </title> <journal> Cognitive Science, </journal> <volume> 9(1) </volume> <pages> 51-74, </pages> <year> 1985. </year>
Reference: [ Weischedel et al., 1993 ] <author> Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., and Palmucci, J. </author> <title> Coping with Ambiguity and Unknown Words through Probabilistic Models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382, </pages> <year> 1993. </year>
Reference-contexts: to knowledge acquisition generally offer no such credit assignment mechanism. 2.3.3 Comprehensive approaches to ambiguity resolution In addition to Brill's transformation-based system for learning solutions to a variety of problems in natural language understanding (see above), BBN's PLUM system offers another statistically-based, comprehensive approach to knowledge acquisition for sentence analysis <ref> [ Weischedel et al., 1993 ] </ref> . Like Kenmore, PLUM handles lexical ambiguity, unknown words, and structural ambiguity for real-world text. It also espouses a hybrid approach to knowledge acquisition and text processing by integrating probabilistic models with knowledge-based parsing methods. Nevertheless, the two systems treat ambiguity resolution very differently.
Reference: [ Wermter and Lehnert, 1989 ] <author> Wermter, S. and Lehnert, W. </author> <title> A hybrid symbolic/connectionist model for noun-phrase understanding. </title> <journal> Connection Science, </journal> <volume> 1(3), </volume> <year> 1989. </year>
Reference: [ Wermter, 1990 ] <author> Wermter, S. </author> <title> Combining Symbolic and Connectionist Techniques for Coordination in Natural Language. </title> <booktitle> In Proceedings of the 14th German Workshop on Artificial Intelligence, </booktitle> <address> Eringerfeld, Germany, </address> <year> 1990. </year>
Reference: [ Wilensky, 1978 ] <author> Wilensky, R. </author> <title> Understanding goal-based stories. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1978. </year> <note> Also available as Tech Report YALEU/CSD/RR #140. </note>
Reference-contexts: Other text analyzers relied on detailed, hand-coded knowledge of scripts, plans, goals, and even political beliefs, in addition to hand-coded lexicons (e.g., SAM [ Schank and Riesbeck, 1981 ] , FRUMP [ DeJong, 1979 ] , PAM <ref> [ Wilensky, 1978 ] </ref> , and POLITICS [ Carbonell, 1978 ] ). The Boris system [ Dyer, 1983 ] included all of these knowledge structures (and more) and integrated them directly into the parser.
Reference: [ Wilensky, 1991 ] <author> Wilensky, R. </author> <title> Extending the Lexicon by Exploiting Subregularities. </title> <type> Technical Report UCB/CSD 91/618, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <year> 1991. </year>
Reference: [ Winograd, 1983 ] <author> Winograd, T., </author> <title> editor. Language as a Cognitive Process, volume 1. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: LICKs, then, embody the basic control mechanism of ATN's [ Woods, 1970 ] but enforce a much stricter set of communication rules. The ATN framework, for example, provides at least three different mechanisms for handling embedded clauses <ref> [ Winograd, 1983 ] </ref> one possibility is to use a global hold register to store a constituent until its gap can be found. None of these mechanisms, however, allows the parent to instantiate more than a single syntactic buffer 8 in the embedded clause with the antecedent.
Reference: [ Wittgenstein, 1953 ] <author> Wittgenstein, L. </author> <title> Philosophical Investigations. </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1953. </year> <month> 178 </month>
Reference: [ Woods, 1970 ] <author> Woods, W. </author> <title> Transition network grammars for natural language analysis. </title> <journal> CACM, </journal> <volume> 13(10), </volume> <year> 1970. </year>
Reference-contexts: Typically, the conceptual representation for an entire subordinate clause is stored in *LB* until it can be incorporated into the representation being constructed by a parent control kernel. LICKs, then, embody the basic control mechanism of ATN's <ref> [ Woods, 1970 ] </ref> but enforce a much stricter set of communication rules.
Reference: [ Yarowsky, 1992 ] <author> Yarowsky, David. </author> <title> Word-Sense Disambiguation Using Statistical Models of Roget's Categories Trained on Large Corpora. </title> <booktitle> In Proceedings, COLING-92, </booktitle> <year> 1992. </year>
Reference: [ Yarowsky, 1994 ] <author> Yarowsky, David. </author> <title> Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French. </title> <booktitle> In Proceedings of the 32th Annual Meeting of the ACL, </booktitle> <year> 1994. </year>
Reference-contexts: Kenmore is a method for acquiring decision lists for lexical 5 Stative verbs are verbs whose actions are assumed to hold at all times after their assertion (e.g., know, believe, love); the actions of non-stative verbs are not always assumed to hold after assertion (e.g., fix, walk). 21 ambiguity resolution <ref> [ Yarowsky, 1994 ] </ref> . Yarowsky's decision lists are ordered lists of single-antecedent rules each entry in the list tests one feature and produces a classification. Unlike Brill's transformation-based rule set, only the first applicable pattern in the list is applied to a novel instance.
Reference: [ Zelle and Mooney, 1993 ] <author> Zelle, J. and Mooney, R. </author> <title> Learning Semantic Grammars with Constructive Inductive Logic Programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 817-822, </pages> <address> Washington, DC, 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference: [ Zelle and Mooney, 1994 ] <author> Zelle, J. and Mooney, R. </author> <title> Learning Semantic Grammars with Constructive Inductive Logic Programming. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 748-753, </pages> <address> Seattle, WA, 1994. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference: [ Zernik, 1991 ] <author> Zernik, U. </author> <title> Introduction. </title> <editor> In Zernik, U., editor, </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <address> pages 1-26. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
References-found: 157

