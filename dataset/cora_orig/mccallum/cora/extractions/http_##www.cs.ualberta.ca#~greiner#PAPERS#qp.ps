URL: http://www.cs.ualberta.ca/~greiner/PAPERS/qp.ps
Refering-URL: http://www.cs.ualberta.ca/~greiner/PAPERS/
Root-URL: 
Email: greiner@learning.siemens.com  
Phone: (609) 734-3627  
Title: Learning Efficient Query Processing Strategies  
Author: Russell Greiner 
Note: Appears in the Proceedings of the Eleventh ACM Symposium on Principles of Database Systems (PODS92), San Diego, June 1992.  
Address: Princeton, NJ 08540-6632  
Affiliation: Siemens Corporate Research  
Abstract: A query processor qp uses the rules in a rule base to reduce a given query to a series of attempted retrievals from a database of facts. The qp's expected cost is the average time it requires to find an answer, averaged over its anticipated set of queries. This cost depends on the qp's strategy, which specifies the order in which it considers the possible rules and retrievals. This paper provides two related learning algorithms, pib and pao, for improving the qp's strategy, i.e., for producing new strategies with lower expected costs. Each algorithm first monitors the qp's operations over a set of queries, observing how often each path of rules leads to a sufficient set of successful retrievals, and then uses these statistics to suggest a new strategy. pib hill-climbs to strategies that are, with high probability, successively better; and pao produces a new strategy that probably is approximately optimal. We describe how to implement both learning systems unobtrusively, discuss their inherent time and space complexities, and use methods from mathematical statistics to prove their correctness. We also discuss additional applications of these approaches to several other database tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [AV88] <author> Serge Abiteboul and Victor Vianu. </author> <title> Procedural and declarative database update languages. </title> <booktitle> In Proc. of 7th Symposium on Principles of Database Systems, </booktitle> <pages> pages 240-50, </pages> <address> Austin, TX, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: The previous sections discussed how they can be used to improve a system that is performing satisficing searches. As such, they can clearly help to reduce the cost of performing both ground queries and and existentially quantified queries (i.e., queries asking for only one answer), which include "non-deterministic queries" <ref> [AV88] </ref>, and "existential queries" [RBK88].
Reference: [BD88] <author> Mark Boddy and Thomas Dean. </author> <title> Solving time dependent planning problems. </title> <type> Technical report, </type> <institution> Brown University, </institution> <year> 1988. </year>
Reference-contexts: These learning algorithms each monitor a qp as it solves a set of queries, collecting certain simple statistics. Each then uses these statistics to modify the qp's strategy, producing a new strategy whose performance will, with provably high probably, be better on subsequent queries. pib is an anytime algorithm <ref> [BD88, DB88] </ref> that uses these statistics to hill-climb (with high probability) to successively better strategies; in particular, given any ffi &gt; 0 and set of possible modifications, it selects and applies a sequence of modifications to a given initial strategy to produce successive strategies that are, with probability greater than 1 <p> j to fi j+1 when in fact C [fi j+1 ] &gt; C [fi j ] | is below ffi: P r [ 9j C [fi j+1 ] &gt; C [fi j ] ] ffi 2 The above theorem justified our view that this pib process is an anytime algorithm <ref> [BD88, DB88] </ref> as, at any time, it provides a strategy (here, the strategy produced at the i th iteration) with the property that the longer we wait, the better the strategy; i.e., j &gt; i means fi j is, with high probability, better than fi i .
Reference: [Bol85] <author> B. Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: 0 ] &gt; 0, and hence that fi 0 is better than fi, whenever we observe that [fi; fi 0 ; fI i g n r 2 1 (2) This equation gives us an a posteriori way of comparing two strategies: first construct the new fi 0 and 5 See <ref> [Bol85, p. 12] </ref>.
Reference: [BR86] <author> F. Bancilhon and R. Ra-makrishnan. </author> <title> An Amateur's Introduction to Recursive Query-Processing Strategies. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-49, </pages> <year> 1986. </year>
Reference-contexts: I am grateful to Manolis Koubarakis for his help in writing a preliminary version of this article, and to Vinay Chaudhri, Igor Jurisica, Alberto Mendelzon and John Mylopoulos, for their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases <ref> [BR86, Ull89, Smi89] </ref>; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! [Ull89], LDL [Zan88], DedGin fl [LV89]).
Reference: [CG91] <author> William Cohen and Russell Greiner. </author> <title> Probabilistic hill climbing. </title> <booktitle> In Proceedings of CLNL-91, </booktitle> <address> Berkeley, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator [MKKC86] or chunk [LNR87, LNR86] to the rule base. See [GJ92] for more details. 8 Third, <ref> [CG91] </ref> describes a related learning algorithm called palo (for "Probably Approximately Locally Optimal"). Like pib, palo uses a set of possible transformations to hill-climb in a situation where the worth of each strategy can only be estimated by sampling. <p> While pib will continue collecting samples and potentially moving to new strategies indefinitely, palo will stop when it reaches an "*-local optimal" | i.e., when it reaches a fi m with the property that 8fi 2 T (fi m ) C [fi] C [fi m ] * See <ref> [CG91, GJ92] </ref> for more details. 4 The pao Algorithm pao's task is to identify a new strategy whose cost is, with high probability, very close to the cost of the optimal strategy.
Reference: [Che52] <author> Herman Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: We know, of course, that this average will tend to the true population mean, = D [fi; fi 0 ], as n ! 1; i.e., = lim n;1 Y n . Chernoff bounds <ref> [Che52] </ref> describe the probable rate of convergence: the probability that "Y n is more than + fi" goes to 0 exponentially fast as n increases; and, for a fixed n, exponentially as fi increases.
Reference: [DB88] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In AAAI-88, </booktitle> <pages> pages 49-54, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These learning algorithms each monitor a qp as it solves a set of queries, collecting certain simple statistics. Each then uses these statistics to modify the qp's strategy, producing a new strategy whose performance will, with provably high probably, be better on subsequent queries. pib is an anytime algorithm <ref> [BD88, DB88] </ref> that uses these statistics to hill-climb (with high probability) to successively better strategies; in particular, given any ffi &gt; 0 and set of possible modifications, it selects and applies a sequence of modifications to a given initial strategy to produce successive strategies that are, with probability greater than 1 <p> j to fi j+1 when in fact C [fi j+1 ] &gt; C [fi j ] | is below ffi: P r [ 9j C [fi j+1 ] &gt; C [fi j ] ] ffi 2 The above theorem justified our view that this pib process is an anytime algorithm <ref> [BD88, DB88] </ref> as, at any time, it provides a strategy (here, the strategy produced at the i th iteration) with the property that the longer we wait, the better the strategy; i.e., j &gt; i means fi j is, with high probability, better than fi i .
Reference: [DeJ88] <author> Gerald DeJong. </author> <booktitle> AAAI workshop on Explanation-Based Learning. Sponsored by AAAI, </booktitle> <year> 1988. </year>
Reference-contexts: This basic observation is the key insight underlying "explanation-based learning" systems <ref> [DeJ88, MCK + 89] </ref>. Each of these systems analyzes the solutions found to certain previous problems, and many use this information to suggest new strategies.
Reference: [Des90] <author> Bipin C. Desai. </author> <title> An Introduction to Database Systems. </title> <publisher> West Publishing Company, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Our pib and pao techniques can apply to yet other tasks that fit into this "satisficing" framework. One obvious additional database application is deciding on the order in which to scan a set of horizontally segmented distributed databases <ref> [Des90, p. 673] </ref>. For example, imagine we have several physical files that each store the same types of facts about people.
Reference: [GJ92] <author> Russell Greiner and Igor Jurisica. </author> <title> EBL systems that (almost) always improve performance. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: We close this subsection with several comments on this framework, and a few extensions. Note 1. The above notation assumes only a finite number of contexts; there are obvious ways of extending this analysis to deal with distributions of an infinite number of contexts, see <ref> [GJ92] </ref>. Note 2. While there can be an infinite number of contexts, they can be partitioned into a large but finite set of equivalence classes, where all members of an equivalence classes have the same cost for each strategy. <p> a | hence, f fl (R p ) = f (R p ) + f (D p ) and f fl (R g ) = f (R g ) + f (D g ). (This definition of f fl () applies when the reduction subgraph under R i is tree; <ref> [GJ92] </ref> deals with the complexities that arise in more general graphs.) Second, the F : : A 7! &lt; + function maps each arc a 2 A to the the total cost of the arcs on the paths other than the path on which this a appears. <p> The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator [MKKC86] or chunk [LNR87, LNR86] to the rule base. See <ref> [GJ92] </ref> for more details. 8 Third, [CG91] describes a related learning algorithm called palo (for "Probably Approximately Locally Optimal"). Like pib, palo uses a set of possible transformations to hill-climb in a situation where the worth of each strategy can only be estimated by sampling. <p> While pib will continue collecting samples and potentially moving to new strategies indefinitely, palo will stop when it reaches an "*-local optimal" | i.e., when it reaches a fi m with the property that 8fi 2 T (fi m ) C [fi] C [fi m ] * See <ref> [CG91, GJ92] </ref> for more details. 4 The pao Algorithm pao's task is to identify a new strategy whose cost is, with high probability, very close to the cost of the optimal strategy.
Reference: [GO91] <author> Russell Greiner and Pekka Orponen. </author> <title> Probably approximately optimal derivation strategies. </title> <editor> In J.A. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference, </booktitle> <address> San Mateo, CA, April 1991. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: b i; hR st R tc D c i; hR td D d ii The expected cost of a strategy is the weighted sum of the costs of its paths, weighted by the probability that we will need to pursue this path, i.e., that none of the prior paths succeeded <ref> [Smi89, GO91] </ref>. (Of course, the cost of a path is the sum of the cost of its arcs.) Note 4. The above definitions are sufficient for the class of simple "disjunctive inference graphs", which consist only of rules whose antecedents each include a single literal. <p> We would also define S to be a set of subsets of N , where the query processor would have to reach each member of some s 2 S for the derivation to succeed. This extension leads to additional complications in specifying strategies; see <ref> [GO91, Appendix A] </ref>. For pedagogical reasons, however, this paper uses only the simple (not hyper) graph notation, in which a simple arc connects a node to but a single child, etc. <p> The only remaining challenge is to determine the number of samples required to be confident that j^p i p i j is small; this requires a few simple applications of Chernoff bounds (Equation 1). The resulting sample complexity appears below: Theorem 2 (adapted from <ref> [GO91] </ref>) Let *; ffi &gt; 0 be given positive constants, and G 2 AOT be any tree shaped inference graph with database retrievals fd i g n i=1 . <p> For example, [Smi89] presents an efficient algorithm OT for the class of simple disjunctive tree shaped inference graphs (see Note 4 and Note 5). Moreover, there are polynomial time ~ G functions that can produce near optimal strategies for some classes G for which G is intractable; cf., <ref> [GO91, Appendix B] </ref>.
Reference: [GO92] <author> Russell Greiner and Pekka Orponen. </author> <title> Probably approximately optimal satisficing strategies. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: at least (1 ffi n , * (9) With probability at least (1 ffi n ) n 1 ffi, Equation 9 holds for each of the n experiments, which, using Lemma 1, means that C [fi pao ] C [fi opt ]+*, as desired. (This proof appears in full in <ref> [GO92] </ref>.) 2 Definition 2 (Reaching an Experiment) Given any tree shaped inference graph G, any distribution P and any strategy fi defined on G, define p (e i ; fi) to be the probability that the strategy fi will reach the experiment e i ; and define p (e i ) <p> Proof Sketch: Given the vectors P and ^ P , let P (i) = h ^p 1 ; : : : ; ^p i ; p i+1 ; : : : ; p n i, and as special cases, P (0) = P and P (n) = ^ P . <ref> [GO92] </ref> proves that @C p [fi] F : [e i ]; using the mean-value theorem, this means that, for any strategy fi, jC P (i) [fi] C P (i1) [fi]j (e i ) fi F : (e i ) fi j^p i p i j jC P [fi] C ^ P
Reference: [Gre91] <author> Russell Greiner. </author> <title> Finding the optimal derivation strategy in a redundant knowledge base. </title> <journal> Artificial Intelligence, </journal> <volume> 50(1) </volume> <pages> 95-116, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, this latter task is NP-hard for general graphs G; see <ref> [Gre91] </ref>. Fortunately, however, G is efficient for some subclasses G. For example, [Smi89] presents an efficient algorithm OT for the class of simple disjunctive tree shaped inference graphs (see Note 4 and Note 5).
Reference: [HC76] <author> Michael Hammer and Arvola Chan. </author> <title> Index Selection in a Self-Adaptive Data Base Management System. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-8, </pages> <year> 1976. </year>
Reference-contexts: Existing relational systems (e.g., system R [SAC + 79]) keep statistics about the database and use them to determine the costs of different access methods. They do not, however, keep statistics about the queries, nor do they use such information to improve the query processing system. <ref> [HC76] </ref> presents a self-adaptive database management system that keeps query, update and domain selectivity statistics and utilizes them to automatically adjust the physical organization of its database. [LN90] uses a similar statistical method (adaptive sampling) to estimate the size of queries.
Reference: [LK82] <author> Averill M. Law and W. David Kelton. </author> <title> Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> Toronto, </address> <year> 1982. </year>
Reference-contexts: We can then determine whether the observed difference is large enough, as a function of the number of samples. (In the simulation vernacular, this corresponds to the "paired-t confidence" <ref> [LK82] </ref>.) We are looking, however, for an efficient a priori way of deciding whether fi 0 is better than fi, without first building this fi 0 . Once again, consider answering queries of form instructor hbi .
Reference: [LN90] <author> R.J. Lipton and J.F. Naughton. </author> <title> Query size estimation by adaptive sampling. </title> <booktitle> In Proceedings of ACM SIGACT/SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 40-46, </pages> <year> 1990. </year>
Reference-contexts: They do not, however, keep statistics about the queries, nor do they use such information to improve the query processing system. [HC76] presents a self-adaptive database management system that keeps query, update and domain selectivity statistics and utilizes them to automatically adjust the physical organization of its database. <ref> [LN90] </ref> uses a similar statistical method (adaptive sampling) to estimate the size of queries. However, none of these systems use statistics to optimize the query processing by modifying the qp's strategy. Section 2 first uses a simple example to motivate and illustrate our approach.
Reference: [LNR86] <author> John E. Laird, Allan Newell, and Paul S. Rosenbloom. </author> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator [MKKC86] or chunk <ref> [LNR87, LNR86] </ref> to the rule base. See [GJ92] for more details. 8 Third, [CG91] describes a related learning algorithm called palo (for "Probably Approximately Locally Optimal").
Reference: [LNR87] <author> John E. Laird, Allan Newell, and Paul S. Rosenbloom. </author> <title> SOAR: An architecture of general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(3), </volume> <year> 1987. </year>
Reference-contexts: The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator [MKKC86] or chunk <ref> [LNR87, LNR86] </ref> to the rule base. See [GJ92] for more details. 8 Third, [CG91] describes a related learning algorithm called palo (for "Probably Approximately Locally Optimal").
Reference: [LV89] <author> A. Lefebvre and L. Vieille. </author> <title> On Deductive Query Evaluation in the DedGin fl System. </title> <booktitle> In Proceedings of the 1st International Conference on Deductive and Object-Oriented Databases, </booktitle> <pages> pages 225-244, </pages> <address> Kyoto, Japan, </address> <year> 1989. </year>
Reference-contexts: their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases [BR86, Ull89, Smi89]; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! [Ull89], LDL [Zan88], DedGin fl <ref> [LV89] </ref>). In general, these systems attempt to produce a qp with a low expected cost, where expected cost measures the average time qp requires to find an answer, averaged over its anticipated set of queries.
Reference: [MCK + 89] <author> Steven Minton, Jaime Carbonell, C.A. Knoblock, D.R. Kuokka, Oren Etzioni, and Y. Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):63-119, </volume> <month> September </month> <year> 1989. </year> <month> 13 </month>
Reference-contexts: This basic observation is the key insight underlying "explanation-based learning" systems <ref> [DeJ88, MCK + 89] </ref>. Each of these systems analyzes the solutions found to certain previous problems, and many use this information to suggest new strategies.
Reference: [MKKC86] <author> Thomas M. Mitchell, Richard M. Keller, and Smadar T. Kedar-Cabelli. </author> <title> Example-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator <ref> [MKKC86] </ref> or chunk [LNR87, LNR86] to the rule base. See [GJ92] for more details. 8 Third, [CG91] describes a related learning algorithm called palo (for "Probably Approximately Locally Optimal").
Reference: [Nil80] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artifical Intelligence. </booktitle> <publisher> Tioga Press, </publisher> <address> Palo Alto, </address> <year> 1980. </year>
Reference-contexts: Theorem 1 continues to hold if we modify pib to perform this tests less frequently, perhaps after processing each successive sequence of k &gt; 1 queries. Second, as mentioned above, the pib system can be viewed as "hill-climbing" using a specific set of possible transformations as operators <ref> [Nil80] </ref>; here each transformation exchanges the order of some particular pair of "sibling" arcs. The general pib system can use (almost) arbitrary sets of transformations to hill-climb; e.g., perhaps using a set of transformations that each add some macro-operator [MKKC86] or chunk [LNR87, LNR86] to the rule base.
Reference: [OG90] <author> Pekka Orponen and Russell Greiner. </author> <title> On the sample complexity of finding good search strategies. </title> <booktitle> In Proceedings of COLT-90, </booktitle> <pages> pages 352-58, </pages> <address> Rochester, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Also, the algorithms presented in this paper can accommodate more complicated f () cost functions, which can allow the cost of traversing an arc to depend on other factors | e.g., the success or failure of that traversal, which other arcs have already been traversed, etc. See <ref> [OG90] </ref>. Note 5. We will later need a few other definitions. <p> In general, determining which strategy is optimal depends on the success probabilities of all of these experiments, not just retrievals. Of course, we need to extend the "inference graphs" defined above 10 to accomodate these internal probabilistic experiments; the "search structures" defined in <ref> [OG90] </ref> are sufficient. <p> by commenting that both 12 of these techniques apply in many other situations as well | in fact, in any situation that involves performing a set of probabilistic experiments until reaching a satisfying configuration of successes and failures where the cost of performing the experiments depends on the order chosen <ref> [OG90] </ref>; Subsection 5.2 lists many other applications that are relevant to database systems.
Reference: [RBK88] <author> R. Ramakrishman, C. Beeri, and R. Krish-namurthy. </author> <title> Optimizing existential datalog queries. </title> <booktitle> In Proc. of 7th Symposium on Principles of Database Systems, </booktitle> <pages> pages 89-102, </pages> <address> Austin, TX, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: As such, they can clearly help to reduce the cost of performing both ground queries and and existentially quantified queries (i.e., queries asking for only one answer), which include "non-deterministic queries" [AV88], and "existential queries" <ref> [RBK88] </ref>.
Reference: [SAC + 79] <author> P.G. Selinger, M.M. Astrahan, D. D. Cham-berlin, R.A. Lorie, and T.G. Price. </author> <title> Access Path Selection in a Relational Data Base Management System. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year>
Reference-contexts: Of course, this is not the first use of statistics in database systems. Existing relational systems (e.g., system R <ref> [SAC + 79] </ref>) keep statistics about the database and use them to determine the costs of different access methods.
Reference: [SK75] <author> H. A. Simon and J. B. Kadane. </author> <title> Optimal problem-solving search: All-or-none solutions. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 235-247, </pages> <year> 1975. </year>
Reference-contexts: That is, our qp is seeking only a single solution; this type of search is called a "satisficing search" <ref> [SK75] </ref>. (Subsection 5.2 underscores the importance of satis-ficing search to database systems by listing a variety of database situations that require this type of process | including the evaluation of ground and functional queries, using negation-as-failure, dealing with horizontally segmented distributed databases, etc.
Reference: [Smi89] <author> David E. Smith. </author> <title> Controlling backward inference. </title> <journal> Artificial Intelligence, </journal> <volume> 39(2) </volume> <pages> 145-208, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: I am grateful to Manolis Koubarakis for his help in writing a preliminary version of this article, and to Vinay Chaudhri, Igor Jurisica, Alberto Mendelzon and John Mylopoulos, for their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases <ref> [BR86, Ull89, Smi89] </ref>; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! [Ull89], LDL [Zan88], DedGin fl [LV89]). <p> Notice that this analysis depends on the success probabilities; information that is usually not known initially (e.g., we usually do not know that 60% of the queries will be instructor (russ), etc.). <ref> [Smi89] </ref> presents one way of approximating their values, based on the (questionable) assumption that these probabilities are correlated with the distribution of facts in the database. <p> b i; hR st R tc D c i; hR td D d ii The expected cost of a strategy is the weighted sum of the costs of its paths, weighted by the probability that we will need to pursue this path, i.e., that none of the prior paths succeeded <ref> [Smi89, GO91] </ref>. (Of course, the cost of a path is the sum of the cost of its arcs.) Note 4. The above definitions are sufficient for the class of simple "disjunctive inference graphs", which consist only of rules whose antecedents each include a single literal. <p> Unfortunately, this latter task is NP-hard for general graphs G; see [Gre91]. Fortunately, however, G is efficient for some subclasses G. For example, <ref> [Smi89] </ref> presents an efficient algorithm OT for the class of simple disjunctive tree shaped inference graphs (see Note 4 and Note 5). Moreover, there are polynomial time ~ G functions that can produce near optimal strategies for some classes G for which G is intractable; cf., [GO91, Appendix B]. <p> can be useful in situations where we know that there can be only k answers to some query; e.g., parent (x,Y) will only yield two bindings for Y for any fixed x, as will senator (x,Y), etc. 5.3 Contributions The view that query processing corresponds to searching through a graph <ref> [Vie89, Smi89] </ref> suggests one form of query optimization: finding an efficient graph search strategy.
Reference: [Ull89] <author> J. Ullman. </author> <title> Principles of Data Base and Knowledge Base Systems, volume 2. </title> <publisher> Ad-dison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: I am grateful to Manolis Koubarakis for his help in writing a preliminary version of this article, and to Vinay Chaudhri, Igor Jurisica, Alberto Mendelzon and John Mylopoulos, for their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases <ref> [BR86, Ull89, Smi89] </ref>; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! [Ull89], LDL [Zan88], DedGin fl [LV89]). <p> Mendelzon and John Mylopoulos, for their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases [BR86, Ull89, Smi89]; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! <ref> [Ull89] </ref>, LDL [Zan88], DedGin fl [LV89]). In general, these systems attempt to produce a qp with a low expected cost, where expected cost measures the average time qp requires to find an answer, averaged over its anticipated set of queries. <p> Inference graphs are similar in functionality to other types of graphs used in the literature to describe evaluation of queries; e.g., rule/goal graphs <ref> [Ull89] </ref>. 2 path will fail (10:15 = 0:85) times the cost of hR g D g i; hence, C [fi 1 ] = (1+1)+(10:15)(1+1) = 3:7. In a similar way, we see that C [fi 2 ] = (1 + 1) + (1 0:6)(1 + 1) = 2:8.
Reference: [Val84] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: For reasons discussed below, we will freely interchange these two descriptions. 2 for "Probably Incrementally Better" and "Probably Approximately Optimal". This systems are related to the work on "Probably Approximately Correct" learning; cf., <ref> [Val84] </ref>. Rule Base R p : instructor (X):- prof (X). R g : instructor (X):- grad (X).
Reference: [Vie89] <author> L. Vieille. </author> <title> Recursive Query Processing: The Power of Logic. </title> <journal> Theoretical Computer Science, </journal> <volume> 68(2), </volume> <year> 1989. </year>
Reference-contexts: can be useful in situations where we know that there can be only k answers to some query; e.g., parent (x,Y) will only yield two bindings for Y for any fixed x, as will senator (x,Y), etc. 5.3 Contributions The view that query processing corresponds to searching through a graph <ref> [Vie89, Smi89] </ref> suggests one form of query optimization: finding an efficient graph search strategy.
Reference: [Zan88] <author> Carlo Zaniolo. </author> <title> Design and Implementation of a Logic Based Language for Data Intensive Applications. </title> <booktitle> In Proceedings of the 5th International Conference on Logic Programming, </booktitle> <year> 1988. </year> <month> 14 </month>
Reference-contexts: John Mylopoulos, for their general comments on this research. process, there has been a great deal of work on query optimization for knowledge bases [BR86, Ull89, Smi89]; this flurry of research activity has produced a great deal of theoretical work and a number of prototype systems (e.g., NAIL! [Ull89], LDL <ref> [Zan88] </ref>, DedGin fl [LV89]). In general, these systems attempt to produce a qp with a low expected cost, where expected cost measures the average time qp requires to find an answer, averaged over its anticipated set of queries.
References-found: 31

