URL: http://www.cs.cmu.edu/~yx/papers/ICCV95_final.ps.Z
Refering-URL: http://www.ius.cs.cmu.edu/afs/cs/usr/yx/www/HomePage.html
Root-URL: 
Title: Hypergeometric Filters For Optical Flow and Affine Matching  
Author: Yalin Xiong Steven A. Shafer 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: This paper proposes new "hypergeometric" filters for the problem of image matching under the translational and affine model. This new set of filters has the following advantages: (1) High-precision registration of two images under the translational and affine model. Because the window effects are eliminated, we are able to achieve superb performance in both translational and affine matching. (2) Affine matching without exhaustive search or image warping. Due to the recursiveness of the filters in the spatial domain, we are able to analytically express the relation between filter outputs and the six affine parameters. This analytical relation enables us to directly compute these affine parameters. (3) Generality. The approach we demonstrate here can be applied to a broad class of matching problems, as long as the transformation between the two image patches can be mathematically represented in the frequency domain. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. </author> <title> Abramowitz and I.A. Stegun. Handbook of Mathemat 1 In these graphs, The scale used for (u 0 ; v 0 ) is larger than those for ru and rv. But the scales for ru and rv are the same. Textured Cube Recovered (u 0 ; v 0 ) ru rv Cube Images ical Functions with Formulas, Graphs, and Mathematical Tables. </title> <type> U.S. </type> <institution> Government Printing Office, </institution> <address> Wash-ington, DC, </address> <year> 1972. </year>
Reference: [2] <author> Edward H. Adelson and James R. Bergen. </author> <title> Spatiotemporal energy models for the perception of motion. </title> <journal> Journal of Optical Society of America, A, </journal> <pages> pages 284-299, </pages> <year> 1984. </year>
Reference: [3] <author> P. Anandan. </author> <title> A computational framework and an algorithm for the measurement of visual motion. </title> <journal> International Journal of Computer Vision, </journal> <pages> pages 283-310, </pages> <year> 1989. </year>
Reference: [4] <author> J. L. Barron, D. J. Fleet, and S. S. Beauchemin. </author> <title> System and experiment: Performance of optical flow techniques. </title> <journal> International Journal of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 43-77, </pages> <year> 1994. </year>
Reference-contexts: Finally, we can formulate an objective function to compute ~u and (~u x ; ~u y ) simultaneously. 6 Experiments 6.1 Experiments on Optical Flow There are many algorithms to compute optical flow from two or more images. Among the published work, Barron et al <ref> [4] </ref> provided a quantitative measure for a few of the optical flow techniques. We will demonstrate the capability of our new technique on the same test images for comparison. While most of the techniques presented in [4] used a sequence of images to reduce error, we will show that our technique <p> Among the published work, Barron et al <ref> [4] </ref> provided a quantitative measure for a few of the optical flow techniques. We will demonstrate the capability of our new technique on the same test images for comparison. While most of the techniques presented in [4] used a sequence of images to reduce error, we will show that our technique achieves better results using just two adjacent frames. No pre-smoothing in the spatial domain or the temporal domain was done, therefore the error due to quantization and noise was much higher. <p> No pre-smoothing in the spatial domain or the temporal domain was done, therefore the error due to quantization and noise was much higher. The error measurement we used was the average angular measure as in <ref> [4] </ref>. All angular errors are in degrees ( ffi ). In our implementation, we set two frequency bounds M and N such that we used all filters h mn (x; y) with 0 m M and N n N . More details of our implementation are available in [9]. <p> In other words, we only used information whose frequency is lower than one-sixth of the Nyquist frequency. Figure 4 also shows the raw (un-thresholded) optical flow computed, the curve of the density versus average error and the results reported in <ref> [4] </ref>, which were computed from the whole sequence. The curve was generated by thresholding the optical flow according to Eq. 22 at different values. <p> We also tested our algorithm on the real image sequences in <ref> [4] </ref>. For all the real image sequences, we took two frames in the middle of each sequence to compute optical flows. <p> The 20th frame Recovered (u 0 ; v 0 ) ru rv We first used the 20th and 21st frames from the diverging tree sequence in <ref> [4] </ref>. The size of H filters was = 7:0, and the frequency bounds were M = N = 15. parameters at every pixel. We can see that the recovered affine parameters are very precise except in areas of no texture.
Reference: [5] <author> David J. Fleet, Allan D. Jepson, and Michael Jenkin. </author> <title> Phase-based disparity measurement. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 53(2) </volume> <pages> 198-210, </pages> <year> 1991. </year>
Reference-contexts: to note that when the Taylor expansion in Eq. 12 is truncated after the first term, Eq. 14 becomes V mn = e j (f 0x u+f 0y v) U mn ; which is exactly what has been used in the phase-based approach, where window effects are ignored as in <ref> [5] </ref>. It can also be shown ([9]) that the phase-based approach using instantaneous frequency is equivalent to truncating after the second term in Eq. 12. Thus the traditional approach with Gabor filters is a degenerate case of the method using hypergeometric filters.
Reference: [6] <author> B. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <journal> International Journal of Computer Vision, </journal> <year> 1981. </year>
Reference: [7] <author> Luc Robert and Martial Hebert. </author> <title> Deriving orientation cues from stereo images. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 377-388, </pages> <year> 1994. </year>
Reference-contexts: Many algorithms such as <ref> [7] </ref> use exhaustive search to find those affine parameters. Generally speaking, the search is six-dimensional, therefore prohibitively expensive. We need a way to compute instead of search for the six affine parameters simultane ously. * General Matching Problem.
Reference: [8] <author> Yalin Xiong and Steven A. Shafer. </author> <title> Depth from focusing and defocusing. </title> <booktitle> In Proc. Computer Vision Patt. Recog., </booktitle> <pages> pages 68-73, </pages> <year> 1993. </year>
Reference-contexts: Generally speaking, the search is six-dimensional, therefore prohibitively expensive. We need a way to compute instead of search for the six affine parameters simultane ously. * General Matching Problem. In computer vision, we also encounter other matching problems, such as in depth from defocus <ref> [8] </ref>. These problems bear obvious similarity in that they all try to match two images, yet the sophisticated algorithms developed for the translational matching usually cannot be effectively extended to other matching problems. To overcome these problems, we introduce the hy-pergeometric filters.
Reference: [9] <author> Yalin Xiong and Steven A. Shafer. </author> <title> Moment and hypergeo-metric filters for high precision computation of focus, stereo, and optical flow. </title> <type> Technical Report CMU-RI-TR-94-28, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <year> 1994. </year> <title> The 9th Frame The Computed Flow Average Angular Error Uncertainty The Seventh Frame Unthresholded Thresholded T e = 0:001 Uncertainty Estimation </title>
Reference-contexts: Two-dimension extension of the H filter is straight forward: h mn (x; y) = h m (x)h n (y): (5) Limited by space, we are going to list the useful properties of the H filters without proof. Interested readers may refer to <ref> [9] </ref>. * The set of hypergeometric filters are orthogonal and complete, i.e. we can losslessly reconstruct the original signal from the coefficients that re sult from convolving the image with H filters. * The hypergeometric filters are recursive in the Fourier domain, i.e. <p> Furthermore, if we assume that the uncertainty of V mn and U mn is caused by image noise and window truncation, we could apply the standard 2 analysis to estimate the error covariance matrix of (u; v) as described in <ref> [9] </ref>. <p> All angular errors are in degrees ( ffi ). In our implementation, we set two frequency bounds M and N such that we used all filters h mn (x; y) with 0 m M and N n N . More details of our implementation are available in <ref> [9] </ref>. Without any post-processing, the approach proposed here will compute a velocity at every pixel. <p> For the NASA coke sequence, we show four images: the middle frame in the sequence, the unthresholded and thresholded optical flows and the uncertainty estimation. More experiments on real and synthetic image sequences could be found in <ref> [9] </ref>. In all experiments we conducted on optical flow, our algorithm has delivered denser and more precise results than traditional approaches. 6.2 Experiments on Affine Matching In the affine matching experiments, we tried to compute six affine matching parameters for every pixel.
References-found: 9

