URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95589.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: IMPACT OF PARTIAL SEPARABILITY ON LARGE-SCALE OPTIMIZATION  
Phone: 60439  
Author: Ali Bouaricha and Jorge J. More 
Note: (Revised version)  This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. De partment of Energy, under Contract W-31-109-Eng-38.  
Date: January 1995  October 1995  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P487-0195  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. Mor e, and G.-L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Preprint MCS-P153-0692, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This collection is representative of large-scale optimization problems arising from applications. Table 3.1 lists each test problem with a short description; see <ref> [1] </ref> for additional information on these problems. The optimization problems in the MINPACK-2 collection arise from the need to minimize a function f of the form f (v) = D where D is some domain in either IR or IR 2 , and is defined by the application.
Reference: [2] <author> B. M. Averick and J. J. Mor e, </author> <title> Evaluation of large-scale optimization problems on vector and parallel architectures, </title> <journal> SIAM J. Optimization, </journal> <volume> 4 (1994), </volume> <pages> pp. 708-721. </pages>
Reference-contexts: We used the vmlm implementation of the limited-memory variable metric algorithm (see Averick and More <ref> [2] </ref>), which is based on the work of Liu and Nocedal [18]. In all of our tests we used n v = 5. Instead of using a termination test, such as krf (x)k t krf (x 0 )k; we terminate after 100 iterations.
Reference: [3] <author> C. Bischof, A. Bouaricha, P. Khademi, and J. J. Mor e, </author> <title> Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P488-0195, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1995. </year>
Reference-contexts: In ELSO we have used the ADIFOR [4, 6] tool and the SparsLinC library [5, 6] because, from a computational viewpoint, they provide all the flexibility and efficiency desired on practical problems. Indeed, Bischof, Bouaricha, Khademi, More <ref> [3] </ref> have shown that the ADIFOR tool can satisfy (2.3) and (2.4) on large-scale variational problems. We now outline the three approaches used by ELSO to compute the gradient of f 0 . <p> A disadvantage, however, is that because of the need to maintain dynamic data structures for sparse vectors, the sparse AD approach usually runs slower than the compressed AD approach. Numerical results <ref> [3] </ref> with ADIFOR and SparsLinC show that the compressed AD approach outperforms the sparse AD approach on various architectures. <p> The sparse AD approach uses the indirect addressing and dynamic memory allocation of the SparsLinC library [5, 6] and thus performs poorly on vector architectures <ref> [3] </ref>. As a result, the performance of the sparse AD approach is far from being practical on the Cray. In the rest of this section we present results only for the compressed and hybrid AD options.
Reference: [4] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 1-29. </pages>
Reference-contexts: These approaches are hand-coded, compressed AD, sparse AD, and hybrid AD. In our work we have been using the ADIFOR (Automatic Differentiation of Fortran) tool <ref> [4, 6] </ref>, and the SparsLinC (Sparse Linear Combination) library [5, 6], but other differentiation tools can be used. We demonstrate ELSO's efficiency by comparing the compressed AD, sparse AD, and hybrid AD options with the hand-coded approach. <p> For additional information on automatic differentiation, see the proceedings edited by Griewank and Corlis [13]; the paper of Iri [15] is of special interest because he discusses the complexity of both the forward and the reverse modes of automatic differentiation. In ELSO we have used the ADIFOR <ref> [4, 6] </ref> tool and the SparsLinC library [5, 6] because, from a computational viewpoint, they provide all the flexibility and efficiency desired on practical problems. Indeed, Bischof, Bouaricha, Khademi, More [3] have shown that the ADIFOR tool can satisfy (2.3) and (2.4) on large-scale variational problems.
Reference: [5] <author> C. Bischof, A. Carle, and P. Khademi, </author> <title> Fortran 77 interface specification to the SparsLinC library, </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year>
Reference-contexts: These approaches are hand-coded, compressed AD, sparse AD, and hybrid AD. In our work we have been using the ADIFOR (Automatic Differentiation of Fortran) tool [4, 6], and the SparsLinC (Sparse Linear Combination) library <ref> [5, 6] </ref>, but other differentiation tools can be used. We demonstrate ELSO's efficiency by comparing the compressed AD, sparse AD, and hybrid AD options with the hand-coded approach. <p> In ELSO we have used the ADIFOR [4, 6] tool and the SparsLinC library <ref> [5, 6] </ref> because, from a computational viewpoint, they provide all the flexibility and efficiency desired on practical problems. Indeed, Bischof, Bouaricha, Khademi, More [3] have shown that the ADIFOR tool can satisfy (2.3) and (2.4) on large-scale variational problems. <p> We also require the sparsity pattern of f 0 (x)V as a by-product of this computation. At present, the SparsLinC library <ref> [5, 6] </ref> is the only tool that addresses this situation, but we expect that others will emerge. 5 The main advantage of the sparse AD approach over the compressed AD approach is that no knowledge of the sparsity pattern is required. <p> The sparse AD approach uses the indirect addressing and dynamic memory allocation of the SparsLinC library <ref> [5, 6] </ref> and thus performs poorly on vector architectures [3]. As a result, the performance of the sparse AD approach is far from being practical on the Cray. In the rest of this section we present results only for the compressed and hybrid AD options.
Reference: [6] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer, </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, </title> <type> Preprint MCS-P381-1194, </type> <institution> Ar-gonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year> <note> Also available as CRPC-TR94491, Center for Research on Parallel Computation, </note> <institution> Rice University. </institution>
Reference-contexts: These approaches are hand-coded, compressed AD, sparse AD, and hybrid AD. In our work we have been using the ADIFOR (Automatic Differentiation of Fortran) tool <ref> [4, 6] </ref>, and the SparsLinC (Sparse Linear Combination) library [5, 6], but other differentiation tools can be used. We demonstrate ELSO's efficiency by comparing the compressed AD, sparse AD, and hybrid AD options with the hand-coded approach. <p> These approaches are hand-coded, compressed AD, sparse AD, and hybrid AD. In our work we have been using the ADIFOR (Automatic Differentiation of Fortran) tool [4, 6], and the SparsLinC (Sparse Linear Combination) library <ref> [5, 6] </ref>, but other differentiation tools can be used. We demonstrate ELSO's efficiency by comparing the compressed AD, sparse AD, and hybrid AD options with the hand-coded approach. <p> For additional information on automatic differentiation, see the proceedings edited by Griewank and Corlis [13]; the paper of Iri [15] is of special interest because he discusses the complexity of both the forward and the reverse modes of automatic differentiation. In ELSO we have used the ADIFOR <ref> [4, 6] </ref> tool and the SparsLinC library [5, 6] because, from a computational viewpoint, they provide all the flexibility and efficiency desired on practical problems. Indeed, Bischof, Bouaricha, Khademi, More [3] have shown that the ADIFOR tool can satisfy (2.3) and (2.4) on large-scale variational problems. <p> In ELSO we have used the ADIFOR [4, 6] tool and the SparsLinC library <ref> [5, 6] </ref> because, from a computational viewpoint, they provide all the flexibility and efficiency desired on practical problems. Indeed, Bischof, Bouaricha, Khademi, More [3] have shown that the ADIFOR tool can satisfy (2.3) and (2.4) on large-scale variational problems. <p> We also require the sparsity pattern of f 0 (x)V as a by-product of this computation. At present, the SparsLinC library <ref> [5, 6] </ref> is the only tool that addresses this situation, but we expect that others will emerge. 5 The main advantage of the sparse AD approach over the compressed AD approach is that no knowledge of the sparsity pattern is required. <p> The sparse AD approach uses the indirect addressing and dynamic memory allocation of the SparsLinC library <ref> [5, 6] </ref> and thus performs poorly on vector architectures [3]. As a result, the performance of the sparse AD approach is far from being practical on the Cray. In the rest of this section we present results only for the compressed and hybrid AD options. <p> The generation of the compressed AD gradients with a fixed upper bound of the innermost loops can be done automatically by setting the appropriate ADIFOR flags <ref> [6] </ref>. The computing time ratios for the strip-mining approach (with loop unrolling) are shown in Tables 4.6 and 4.7. The improvement is dramatic for both the compressed AD and hybrid AD options.
Reference: [7] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Fortran subroutines for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 346-347. </month> <title> [8] , Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 329-345. </pages>
Reference-contexts: In our work we employ the partitioning software described by Coleman, Garbow, and More <ref> [8, 7] </ref>. Given a partitioning of the columns of f 0 (x) into p groups of structurally orthogonal columns, we can determine the Jacobian matrix f 0 (x) by computing the compressed Jacobian matrix f 0 (x)V , where V 2 IR nfip . <p> Since the number of element functions differs for (3.2) and (3.3), the number of groups p determined by the partitioning software <ref> [8, 7] </ref> is likely to be different, and thus the computing times for the compressed Jacobian matrix may depend on p. In our experience the computing time of formulation (3.2) is slightly better than that of (3.3). Therefore, we used formulation (3.2) in the numerical results of Section 4.
Reference: [9] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: For many sparsity patterns, the number of groups p is small and independent of n. For example, if a matrix is banded with bandwidth fi or if it can be permuted to a matrix with bandwidth fi, Coleman and More <ref> [9] </ref> show that p fi. The compressed Jacobian matrix contains all the information of the Jacobian matrix. Given the compressed Jacobian matrix, we can recover f 0 (x) in a sparse data structure.
Reference: [10] <author> A. R. Conn, N. I. M. Gould, and P. L. Toint, LANCELOT, </author> <title> Springer Series in Computational Mathematics, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Algorithms and software that take advantage of partial separability have been developed for various problems (for example, <ref> [14, 19, 20, 17, 21, 22, 10] </ref>), but this software requires that the user provide the gradient of f 0 . An important design goal of ELSO is to avoid this requirement.
Reference: [11] <author> A. Griewank, </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse communication, </title> <booktitle> Optim. Methods Software, 1 (1992), </booktitle> <pages> pp. </pages> <month> 35-54. </month> <title> [12] , Some bounds on the complexity of gradients, Jacobians, and Hessians, in Complexity in Nonlinear Optimization, </title> <editor> P. Pardalos, ed., </editor> <publisher> World Scientific Publishers, </publisher> <year> 1993, </year> <pages> pp. 128-161. </pages>
Reference-contexts: In general, the reverse mode requires O (L ff g) floating-point operations and up to O (L ff g + M ff g) memory, depending on the code. In particular, there is no guarantee that (2.4) is satisfied. Griewank <ref> [11, 12] </ref> has discussed how to improve the performance of the reverse mode, but at present the potential memory demands of the reverse mode are a disadvantage.
Reference: [13] <author> A. Griewank and G. F. Corliss, eds., </author> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: Griewank [11, 12] has discussed how to improve the performance of the reverse mode, but at present the potential memory demands of the reverse mode are a disadvantage. For additional information on automatic differentiation, see the proceedings edited by Griewank and Corlis <ref> [13] </ref>; the paper of Iri [15] is of special interest because he discusses the complexity of both the forward and the reverse modes of automatic differentiation.
Reference: [14] <author> A. Griewank and P. L. Toint, </author> <title> Numerical experiments with partially separable optimization problems, in Numerical Analysis: </title> <booktitle> Proceedings Dundee 1983, </booktitle> <editor> D. F. Griffiths, ed., </editor> <booktitle> Lecture Notes in Mathematics 1066, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Algorithms and software that take advantage of partial separability have been developed for various problems (for example, <ref> [14, 19, 20, 17, 21, 22, 10] </ref>), but this software requires that the user provide the gradient of f 0 . An important design goal of ELSO is to avoid this requirement.
Reference: [15] <author> M. Iri, </author> <title> History of automatic differentiation and rounding error estimation, in Automatic Differentiation of Algorithms, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <year> 1992, </year> <pages> pp. 3-16. </pages>
Reference-contexts: Griewank [11, 12] has discussed how to improve the performance of the reverse mode, but at present the potential memory demands of the reverse mode are a disadvantage. For additional information on automatic differentiation, see the proceedings edited by Griewank and Corlis [13]; the paper of Iri <ref> [15] </ref> is of special interest because he discusses the complexity of both the forward and the reverse modes of automatic differentiation.
Reference: [16] <author> D. Juedes, </author> <title> A taxonomy of automatic differentiation tools, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <publisher> SIAM, </publisher> <year> 1991, </year> <pages> pp. 315-329. </pages>
Reference-contexts: Automatic differentiation tools can be classified roughly according to their use of the forward or the reverse mode of automatic differentiation. See, for example, the survey of Juedes <ref> [16] </ref>. Automatic differentiation tools that use the forward mode generate code for the computation of f 0 (x)V for any V 2 IR nfip .
Reference: [17] <author> M. Lescrenier, </author> <title> Partially separable optimization and parallel computing, </title> <journal> Ann. Oper. Res., </journal> <volume> 14 (1988), </volume> <pages> pp. 213-224. </pages>
Reference-contexts: Algorithms and software that take advantage of partial separability have been developed for various problems (for example, <ref> [14, 19, 20, 17, 21, 22, 10] </ref>), but this software requires that the user provide the gradient of f 0 . An important design goal of ELSO is to avoid this requirement.
Reference: [18] <author> D. C. Liu and J. Nocedal, </author> <title> On the limited memory BFGS method for large scale optimization, </title> <journal> Math. Programming, </journal> <volume> 45 (1989), </volume> <pages> pp. 503-528. </pages>
Reference-contexts: We used the vmlm implementation of the limited-memory variable metric algorithm (see Averick and More [2]), which is based on the work of Liu and Nocedal <ref> [18] </ref>. In all of our tests we used n v = 5. Instead of using a termination test, such as krf (x)k t krf (x 0 )k; we terminate after 100 iterations.
Reference: [19] <author> P. L. Toint, </author> <title> Numerical solution of large sets of algebraic nonlinear equations, </title> <journal> Math. Comp., </journal> <volume> 46 (1986), </volume> <pages> pp. </pages> <month> 175-189. </month> <title> [20] , On large scale nonlinear least squares calculations, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 416-435. </pages>
Reference-contexts: Algorithms and software that take advantage of partial separability have been developed for various problems (for example, <ref> [14, 19, 20, 17, 21, 22, 10] </ref>), but this software requires that the user provide the gradient of f 0 . An important design goal of ELSO is to avoid this requirement.
Reference: [21] <author> P. L. Toint and D. Tuyttens, </author> <title> On large-scale nonlinear network optimization, </title> <journal> Math. Programming, </journal> <volume> 48 (1990), </volume> <pages> pp. 125-159. </pages> <month> 15 [22] , LSNNO: </month> <title> A Fortran subroutine for solving large-scale nonlinear network opti-mization problems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 (1992), </volume> <pages> pp. 308-328. 16 </pages>
Reference-contexts: Algorithms and software that take advantage of partial separability have been developed for various problems (for example, <ref> [14, 19, 20, 17, 21, 22, 10] </ref>), but this software requires that the user provide the gradient of f 0 . An important design goal of ELSO is to avoid this requirement.
References-found: 18

