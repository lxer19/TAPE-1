URL: ftp://ftp.cs.ucsd.edu/pub/sfink/pub/thesis.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~sfink/
Root-URL: http://www.cs.ucsd.edu
Title: A Programming Model for Block-Structured Scientific Calculations on SMP Clusters  
Author: Stephen Jason Fink Professor Scott B. Baden, Chair Professor Larry Carter Professor William G. Griswold 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in the Department of Computer Science and Engineering by  Committee in charge:  Professor Sutanu Sarkar Professor John Weare  
Date: 1998  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair. </author> <title> An efficient parallel algorithm for the 3-d FFT NAS parallel benchmark. </title> <booktitle> In Proc. of SHPCC `94, </booktitle> <pages> pages 129-133, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Perform a set of inverse 1D FFTs along the first dimension of W . 4. Perform a set of inverse 1D FFTs along the second dimension of W . The global matrix transpose step can be very expensive, especially for SMP clusters. To tolerate this overhead, Agarwal et al. <ref> [1] </ref> presented a restructured version of the NAS-FT benchmark to explicitly overlap communication and computation. Their algorithm pipelines the inverse 3D FFT with the other computation in the time-stepping loop. Agarwal et al.'s presentation of the algorithm 118 involves three timesteps in the main loop.
Reference: [2] <author> T. Agerwala, J. L. Martin, J. H. Mirza, D. C. Sadler, D. M. Dias, and M. Snir. </author> <title> SP-2 system architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2), </volume> <year> 1995. </year>
Reference-contexts: The nodes communicate over an interconnection network, either by passing messages or relying on hardware-supported shared-memory. This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 <ref> [2] </ref>, Cray T3E [117], and networks of uniprocessor workstations [8]. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA [123], BSP [134], and LogP [53]. <p> We note that the applications Erlichson et al. considered were not restructured to exploit the multi-tier memory hierarchy. Several distributed memory parallel computers devote specialized hardware on each node to handle communication protocols. Several commercial parallel 78 computers, such as the IBM SP-2 <ref> [2] </ref>, Intel Paragon [111], and Meiko CS-2 [19], augment each node with a general purpose microprocessor to handle message-passing events. This strategy reduces the software overhead of message-passing on the compute processors, and provides the opportunity to overlap communication and computation.
Reference: [3] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> An integrated runtime and compile-time approach for parallelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(7), </volume> <month> Jul. </month> <year> 1995. </year>
Reference-contexts: KeLP's data layout abstractions build on the structural abstraction programming methodology introduced in the LPARX programming system [92]. The data motion abstractions combine ideas ideas from structural abstraction and inspector/executor communication analysis <ref> [3] </ref> with asynchronous execution in the multi-level control flow. With the KeLP programming model, this dissertation contributes a new methodology for programming SMP clusters. KeLP explicitly exposes two levels of parallelism, locality, and data motion to the application programmer. <p> Under structural abstraction, first-class language objects represent the geometric structure of a block-structured parallel code. LPARX introduced structural abstraction objects to represent potentially irregularly block-structured data sets. KeLP inherits this facility from LPARX. Additionally, KeLP combines structural abstraction with an inspector/executor communication model <ref> [3] </ref> to manage data motion more efficiently. KeLP's data 14 motion objects support asynchronous execution, providing a simple mechanism to overlap inter-node communication with other operations. KeLP also extends structural abstraction to coordination of several levels of parallel control. <p> Alternatively, a FloorPlan can represent distribution of work among processors of a single SMP. Yet another example is to use the FloorPlan to represent a section of a distributed array, which proves useful for block dense linear algebra algorithms. The MotionPlan implements a first-class, user-level block communication schedule <ref> [3] </ref>. A MotionPlan is defined by a List of 4-tuples &lt; F; i; T; j &gt;, where 18 F and T are Regions, and i and j are integers. The programmer builds and manipulates MotionPlans one tuple at a time. <p> All Grids in a XArray must have the same dimensionality. 2.2.4 Data Motion KeLP moves data between Grids via the MotionPlan and Mover classes. The MotionPlan implements a block communication schedule, similar to those incorporated in Multiblock PARTI <ref> [3] </ref>. However, whereas Multiblock PARTI supported only a small fixed number of regular communication patterns, KeLP utilizes structural abstraction to support more general block motion patterns. <p> With this facility, the programmer can optimize extant MotionPlans, as described in [64]. Note that a MotionPlan holds only a description of a data motion pattern. Building a MotionPlan does not move any data. That task falls to the Mover. The Mover, a first-class executor <ref> [3] </ref> object, performs the data motion represented by a MotionPlan as a collective operation. The Mover object analyzes a MotionPlan and performs memory-to-memory copies and message-passing to effect the data motion pattern. <p> FIDIL provided general abstractions for representing structured and unstructured domains, and geometric operations over domains. FIDIL was targeted for serial computers and did not provide any notion of parallel control flow or data decomposition. KeLP's data motion model combines structural abstraction with concepts introduced in the Multiblock PARTI <ref> [3] </ref> run time system. Multiblock PARTI supports regular block distributions for dynamic arrays, but does not directly support irregular block decompositions as in systems like KeLP. Multiblock PARTI provides two common communication patterns, one to fill in ghost regions and one that moves data over regular sections. <p> Examining three Grid levels remains an outstanding research issue. 2.5.4 Data Motion KeLP's data motion facilities, the MotionPlan and Mover, provide powerful new abstractions for storing and manipulating communication patterns. As discussed in Section 2.4.1, these classes incorporate concepts demonstrated in LPARX [92] and Multiblock PARTI <ref> [3] </ref>. We have assumed that inter-node communication is very expensive relative to local floating-point computation. The Mover's asynchronous entry points reflect this assumption, giving the programmer the power to overlap communication and computation. This communication interface differs significantly from those in other parallel libraries and languages.
Reference: [4] <author> A. Alexandrov, M. F. Ionescu, K. E. Schauser, and C. Scheiman. LogGP: </author> <title> incorporating long messages into the LogP model- one step closer towards a realistic model for parallel computation. </title> <booktitle> In SPAA `95. 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <month> Jul. </month> <year> 1995. </year>
Reference-contexts: Valiant's Bulk-synchronous Parallel (BSP) model [134] considers a program as a sequence of coarse-grain supersteps, and provides an estimate of communication costs with a bandwidth parameter. The LogP [53] model provides parameters to characterize communication costs on a CTA-like machine, specifically communication latency, bandwidth, and overhead. LogGP <ref> [4] </ref> extends LogP with a parameter to model bandwidth for long messages. None of these models address the two levels of parallelism and locality in SMP clusters.
Reference: [5] <author> B. Alpern and L. Carter. </author> <title> Towards a model for portable parallel performance: Exposing the memory hierarchy. </title> <editor> In T. Hey and J. Ferrante, editors, </editor> <title> Portability and Performance for Parallel Processing. </title> <publisher> John Wiley and Sons, </publisher> <year> 1993. </year>
Reference-contexts: The PMH models a parallel computer as a tree of memory modules, and can represent all levels of the memory hierarchy from secondary storage to functional units. The PMH suggests a coding style where chores run concurrently at different levels of the memory hierarchy <ref> [5] </ref>. The Phase Abstractions [124] programming model describes a parallel program for a CTA in terms of X, Y, and Z levels. The X level corresponds to a serial program on a single processor. The Y level coordinates data decomposition, communication, and synchronization between processors.
Reference: [6] <author> B. Alpern, L. Carter, and J. Ferrante. </author> <title> Modeling parallel computers as memory hierarchies. </title> <editor> In W. K. Giloi, S. Jahnichen, and B. D. Shriver, editors, </editor> <booktitle> Programming Models for Massively Parallel Computers, </booktitle> <pages> pages 116-23. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Introduction 1.1 Motivation Modern high-performance computer systems can potentially deliver unprecedented performance for computationally intensive scientific applications. Unfortunately, for many applications, realizing the hardware's potential remains a formidable task. Most high-performance computer systems present a complex non-uniform memory hierarchy with several levels of parallelism and locality <ref> [6] </ref>. In order to use these systems efficiently, one must judiciously orchestrate parallelism and locality in the application to match the hardware resources. To this end, the programmer or compiler must navigate a complex landscape of performance tradeoffs and implementation details. <p> 157 * Zia Ansari used KeLP to implement PILOT, a visualization system for ir regular block-structured data sets [9]. * Scott Baden, Larry Carter, and Jeanne Ferrante (Department of CSE, UCSD) are defining KeLP N , an extension to the KeLP programming model based on the Parallel Memory Hierarchy model <ref> [6] </ref>. The KeLP N model will support programs that run on more general architectural structures, including out-of-core programs and clusters of clusters of machines.
Reference: [7] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Soren-son. </author> <note> LAPACK Users' Guide, Second Edition. SIAM, </note> <institution> Philadelpha, </institution> <address> PA, </address> <year> 1995. </year>
Reference-contexts: KeLP supports block-structured scientific calculations, in which the underlying data structures have a rectangular geometry. For example, finite difference methods for the solution of partial differential equations carry an underlying structured finite difference mesh. Efficient algorithms for dense linear algebra often exhibit a blocked structure <ref> [7] </ref>. KeLP abstractions help manage the data structures, computational domains, and data motion for these applications. KeLP also supports irregular and dynamic block-structured applications. For example, multiblock methods for PDEs operate over a collection of structured grids with potentially irregular couplings.
Reference: [8] <author> T. E. Anderson, D. E. Culler, and D. Patterson. </author> <title> A case for NOW (networks of workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year> <pages> 158 159 </pages>
Reference-contexts: The nodes communicate over an interconnection network, either by passing messages or relying on hardware-supported shared-memory. This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 [2], Cray T3E [117], and networks of uniprocessor workstations <ref> [8] </ref>. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA [123], BSP [134], and LogP [53].
Reference: [9] <author> Z. Ansari. </author> <title> An extensible parallel visualization tool for the observation of distributed arrays. </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: KeLP's support for irregular block data decompositions simplifies implementation of load balancing algorithms for this environment. * KeLP has been used for instruction in graduate and undergraduate parallel computing classes at UCSD. 157 * Zia Ansari used KeLP to implement PILOT, a visualization system for ir regular block-structured data sets <ref> [9] </ref>. * Scott Baden, Larry Carter, and Jeanne Ferrante (Department of CSE, UCSD) are defining KeLP N , an extension to the KeLP programming model based on the Parallel Memory Hierarchy model [6].
Reference: [10] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Our experiments will examine communication overlap and its interaction with memory traffic on an SMP node. On an SMP node, bandwidth and latency to the shared memory banks will greatly impact a code's performance. Codes that demand heavy memory traffic may saturate the shared memory bus or banks <ref> [10] </ref>, degrading parallel speedup. On the other hand, codes that heavily reuse data in cache may not saturate the memory system, and will achieve greater parallel 67 efficiency. Additionally, the local kernel's memory traffic will affect the efficacy of communication overlap.
Reference: [11] <author> E. Arjomandi, W. O'Farrell, I. Kalas, G. Koblents, F. Ch. Eigler, and G. R. Gao. </author> <title> ABC++: Concurrency by inheritance in C++. </title> <journal> IBM Systems Journal, </journal> <volume> 34(1) </volume> <pages> 120-37, </pages> <year> 1995. </year>
Reference-contexts: HPC++ provides many facilities to help manage communication and synchronization between address spaces. For example, HPC++ provides a global pointer abstraction, parallel versions of C++ Standard Template Library objects, remote function calls, and inter-context collective operations. HPC++ borrows concepts from several previous object-based systems, including CC++ [44], ABC++ <ref> [11] </ref>, and MPC++ [83]. In general, these concurrent object-oriented models that allow multiple concurrent member function invocations could support multi-tier programming. Many systems allow construction of two-level parallel programs, where the first level handles parallelism between objects and the second level handles parallelism within objects.
Reference: [12] <author> K. Arnold and J. Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: The HPC++ project [18] proposes language extensions and library facilities to facilitate parallel C++ implementations. The HPC++ model explicitly targets two levels of parallel programming for SMP clusters. For parallel programming in a single address space (HPC++ context), HPC++ provides lightweight thread objects similar to those of Java <ref> [12] </ref>. HPC++ also supports SPMD programming between multiple contexts. HPC++ provides many facilities to help manage communication and synchronization between address spaces. For example, HPC++ provides a global pointer abstraction, parallel versions of C++ Standard Template Library objects, remote function calls, and inter-context collective operations.
Reference: [13] <author> S. B. Baden, R. Schreiber, K. S. Gatlin, and S. J. Fink. </author> <title> A preliminary evaluation of HPF. </title> <booktitle> In Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Minneapolis, MN, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: With higher-level abstractions, these languages reduce the complexity of parallel programs by hiding many of the low-level implementation details. However, higher-level languages usually match only limited problem domains, and with current technology, some programs do not perform as well as their message-passing counterparts <ref> [13] </ref>. Although current parallel programming languages and libraries represent a wide range of design choices, most share the same underlying model of a parallel computer, which we refer to as the single-tier multicomputer model (Figure 1.1).
Reference: [14] <author> D. A. Bader and J. JaJa. </author> <title> SIMPLE: A methodology for programming high performance algorithms on clusters of symmetric multiprocessors. </title> <note> Preliminary Version, http://www.umiacs.umd.edu/research/EXPAR/papers/3798.html. </note>
Reference-contexts: Our approach differs in that KeLP exposes all partitioning and scheduling decisions to the programmer. Thus, KeLP presents a more complex, explicitly parallel model, but allows the programmer to express a wider class of algorithms and exert more control over the implementation. 39 Bader and JaJa have developed SIMPLE <ref> [14] </ref>, a set of collective communication operations for SMP clusters. SIMPLE provides more general, lower-level primitives than multi-KeLP. For example, SIMPLE provides multi-tier versions of collective communication operations such as barriers, reductions, and gather/scatter. <p> In the Stanford FLASH multiprocessor [77], each node contains a custom programmable protocol processor. The Princeton SHRIMP multicomputer [28] implements a hardwired custom network interface to handle cache coherency protocol between commodity workstations. A few projects implement high-level programming models specifically for SMP clusters. The SIMPLE <ref> [14] </ref> implementation implements collective operations similar to those provided by KeLP. SIMPLE also implements several levels of parallel control by masking out loop iterations. On the Maryland Digital AlphaServer, SIMPLE implements a custom message-passing layer, shown to be more efficient than MPI on this platform.
Reference: [15] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fa-toohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Three of the codes implement finite difference methods: a simple regular Poisson solver, the NAS multigrid benchmark <ref> [15] </ref>, and a multi-block multigrid solver over an irregular domain. A fourth code, the NAS Fast Fourier transform benchmark [15], solves a 3D diffusion equation with FFTs. Finally, we consider two blocked dense linear algebra codes: SUMMA matrix multiplication [135] and ScaLAPACK's right-looking blocked dense LU factorization algorithm [49]. <p> Three of the codes implement finite difference methods: a simple regular Poisson solver, the NAS multigrid benchmark <ref> [15] </ref>, and a multi-block multigrid solver over an irregular domain. A fourth code, the NAS Fast Fourier transform benchmark [15], solves a 3D diffusion equation with FFTs. Finally, we consider two blocked dense linear algebra codes: SUMMA matrix multiplication [135] and ScaLAPACK's right-looking blocked dense LU factorization algorithm [49]. Each code raises distinct issues regarding the KeLP programming model. <p> In total, communication takes 1795 ms. In other words, overlapping communication and computation at a SparcStation node retarded inter-node communication by 43%. 4.2.3 NAS Multigrid Benchmark The NAS-MG multigrid benchmark <ref> [15] </ref> solves Poisson's equation in 3D using a multigrid V-cycle [35]. Each level of the multigrid algorithm involves a relaxation operation, and a residual computation. These two operations involve stencil calculations, and we can parallelize them with techniques similar to those for redblack3D. <p> The CollectiveGroup class deals directly with MPI Communicator objects, which lie below the KeLP level. 4.3.2 NAS-FT Benchmark The NAS Fourier Transform (FT) benchmark solves a 3D diffusion equation for an unknown u (x; y; z; t) <ref> [15] </ref>: du = ff 5 2 u (4.1) After applying a Fourier transform to each side of this equation, the solution is given by the function f (x; y; z; t) = e 4ff 2 jzj 2 t v (x; y; z; t = 0) (4.2) where v (t = 0)
Reference: [16] <author> S. Balay, W. D. Gropp, L. C. McInnes, and B. R. Smith. </author> <title> Efficient management of parallelism in object-oriented numerical software libraries. </title> <editor> In E. Arge, A. M. Bruaset, and H. P. Langtangen, editors, </editor> <booktitle> Modern Software Tools in Scientific Computing. </booktitle> <publisher> Birkhauser Press, </publisher> <year> 1997. </year>
Reference-contexts: P++ supports dynamic run-time array distributions. Like LPARX/KeLP, P++ presents a SPMD programming model, which is appropriate for structured applications with some task parallelism, such as adaptive mesh refinement. The design of the PETSc (Portable Extensible Tools for Scientific Computing) toolkit <ref> [16] </ref> incorporates many of the same ideas as used in KeLP. PETSc provides an object-oriented framework to encapsulate mathematical algorithms, particularly for the numerical solution of partial differential equations. PETSc 43 provides three basic categories of abstract data types: index sets, vectors, and matrices. <p> Finally, interoperability with other programming models remains an outstanding KeLP issue. Merlin et al. have successfully incorporated KeLP with SHPF, a data-parallel HPF-like Fortran dialect [101]. Interaction with other systems, such as PETSc <ref> [16] </ref> and task-oriented systems [74, 66], remains an unresolved issue. Chapter 3 Implementation 3.1 Introduction KeLP aims to provide a convenient programming model to express high performance block-structured scientific calculations on SMP clusters. To realize high performance, the system must satisfy two requirements: 1. <p> The IrregularGrid class computes and carries around a MotionPlan to fill in ghost cells. FD-DSL facilitates data motion, data decomposition, and a few extremely simple arithmetic and norm calculations for finite difference methods. Other libraries, such as POOMA [118], OVERTURE [36], and PETSc <ref> [16] </ref> implement more extensive numerical operations for PDE solvers. KeLP supports the development of more extensive and powerful domain-specific libraries [38]. The design and implementation of such libraries raises many algorithmic and software engineering issues that require expert application-specific knowledge.
Reference: [17] <author> F. Baskett and A. J. Smith. </author> <title> Interference in multiprocessor computer systems with interleaved memory. </title> <journal> CACM, </journal> <volume> 19(6) </volume> <pages> 327-334, </pages> <month> June </month> <year> 1976. </year>
Reference: [18] <author> P. Beckman, D. Gannon, and E. Johnson. </author> <title> Portable parallel programming in HPC++. </title> <booktitle> In Proceedings of the 1996 ICPP Workshop on Challenges for Parallel Processing, </booktitle> <pages> pages 132-9, </pages> <address> Ithaca, NY, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: SIMPLE does not help with data decomposition or overlap of communication and computation. The next Chapter discusses the KeLP 2.0 implementation, which relies on collective operations similar to those provided by SIMPLE. The HPC++ project <ref> [18] </ref> proposes language extensions and library facilities to facilitate parallel C++ implementations. The HPC++ model explicitly targets two levels of parallel programming for SMP clusters. For parallel programming in a single address space (HPC++ context), HPC++ provides lightweight thread objects similar to those of Java [12].
Reference: [19] <author> J. Beecroft, M. Homewook, and M. McLaren. </author> <title> Meiko cs-2 interconnect elan-elite design. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 1627-1638, </pages> <year> 1994. </year>
Reference-contexts: That is, if a Grid copy's data happens to lie contiguously in memory, then the Mover will send or receive the data directly from its original storage locations. An important implementation decision concerns the method used to overlap communication and computation. Some architectures <ref> [19, 111, 128] </ref> provide specialized hardware support to overlap communication and computation. We considered two possibilities: using MPI non-blocking message-passing primitives, and devoting a single smp++ thread solely to communication. The MPI overlapping design is straightforward. <p> Several distributed memory parallel computers devote specialized hardware on each node to handle communication protocols. Several commercial parallel 78 computers, such as the IBM SP-2 [2], Intel Paragon [111], and Meiko CS-2 <ref> [19] </ref>, augment each node with a general purpose microprocessor to handle message-passing events. This strategy reduces the software overhead of message-passing on the compute processors, and provides the opportunity to overlap communication and computation. Several distributed shared memory machines also dedicate hardware for protocol processing.
Reference: [20] <author> M. J. Berger and P. Colella. </author> <title> Local adaptive mesh refinement for shock hydrodynamics. </title> <journal> Journal of Computational Physics, </journal> <volume> 82 </volume> <pages> 64-84, </pages> <year> 1989. </year> <month> 160 </month>
Reference-contexts: Finite difference codes are KeLP's "bread and butter"; the KeLP abstractions closely match the computational structures arising in these codes. Since KeLP supports irregular block data sets and dynamic irregular block data motion patterns, the KeLP abstractions especially suit multi-block and structured adaptive mesh refinement codes <ref> [20, 21] </ref>. In this Section, we evaluate performance of KeLP on three finite difference codes. First, we consider the simple second-order red-black Gauss-Seidel Poisson solver presented earlier in Section 2.3. Next, we examine a KeLP version of the 86 NAS MG multigrid benchmark.
Reference: [21] <author> M. J. Berger and J. Oliger. </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations. </title> <journal> Journal of Computational Physics, </journal> <volume> 53(3) </volume> <pages> 484-512, </pages> <month> Mar. </month> <year> 1984. </year>
Reference-contexts: Finite difference codes are KeLP's "bread and butter"; the KeLP abstractions closely match the computational structures arising in these codes. Since KeLP supports irregular block data sets and dynamic irregular block data motion patterns, the KeLP abstractions especially suit multi-block and structured adaptive mesh refinement codes <ref> [20, 21] </ref>. In this Section, we evaluate performance of KeLP on three finite difference codes. First, we consider the simple second-order red-black Gauss-Seidel Poisson solver presented earlier in Section 2.3. Next, we examine a KeLP version of the 86 NAS MG multigrid benchmark.
Reference: [22] <author> F. Berman and R. Wolski. </author> <title> Scheduling from the perspective of the application. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 100-11, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: mix ing layer using the Euler equations. * Silvia Figueira (Department of CSE, UCSD) used KeLP applications to in vestigate performance in multi-user parallel environments [62]. * Fran Berman, Rich Wolski, and collaborators (Department of CSE, UCSD) have used KeLP to investigate dynamic load balancing policies on heterogeneous workstation clusters <ref> [22, 23, 137] </ref>.
Reference: [23] <author> F. Berman, R. Wolski, S. Figueira, J. Schopf, and G. Shao. </author> <title> Application-level scheduling on distributed heterogeneous networks. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: mix ing layer using the Euler equations. * Silvia Figueira (Department of CSE, UCSD) used KeLP applications to in vestigate performance in multi-user parallel environments [62]. * Fran Berman, Rich Wolski, and collaborators (Department of CSE, UCSD) have used KeLP to investigate dynamic load balancing policies on heterogeneous workstation clusters <ref> [22, 23, 137] </ref>.
Reference: [24] <author> D. P. Bhandarkar. </author> <title> Analysis of memory interference in multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-24(9):897-908, </volume> <month> September </month> <year> 1975. </year>
Reference: [25] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 102-11, </pages> <month> Jul. </month> <year> 1993. </year>
Reference-contexts: Many systems allow construction of two-level parallel programs, where the first level handles parallelism between objects and the second level handles parallelism within objects. Mapping these programs onto multi-tier parallel machines may provide a useful and efficient programming paradigm. The NESL language <ref> [25] </ref> implements nested data-parallelism, a model which supports hierarchical parallelism and data structures through vectors of 40 vectors. NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism.
Reference: [26] <author> G. E. Blelloch, P. B. Gibbons, Y. Matias, and M. Zagha. </author> <title> Accounting for memory bank contention and delay in high-bandwidth multiprocessors. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference: [27] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-16, </pages> <month> Jul. </month> <year> 1995. </year>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement [92, 64, 109, 116]. Many hierarchical programs arise from divide-and-conquer algorithms. Several task-oriented parallel languages <ref> [70, 27, 66, 75] </ref> support fork-join parallelism suitable for divide-and-conquer. 2.4.3 Single-Tier Parallel Languages We now review prominent parallel programming languages that apply to block-structured applications, but do not explicitly address a multi-tier or hierarchical programming style. Data-parallel languages support regular array-based calculations.
Reference: [28] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sand-berg. </author> <title> Virtual memory mapped network interface for the SHRIMP multi-computer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago,IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. <ref> [94, 77, 115, 133, 28] </ref>), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. <p> Several distributed shared memory machines also dedicate hardware for protocol processing. The Wisconsin Typhoon architecture [115] dedicates a general-purpose processor from each SMP node to perform protocol processing. In the Stanford FLASH multiprocessor [77], each node contains a custom programmable protocol processor. The Princeton SHRIMP multicomputer <ref> [28] </ref> implements a hardwired custom network interface to handle cache coherency protocol between commodity workstations. A few projects implement high-level programming models specifically for SMP clusters. The SIMPLE [14] implementation implements collective operations similar to those provided by KeLP.
Reference: [29] <author> F. Bodin, P. Beckman, D. Gannon, J. Botwals, S. Narayana, S. Srinivas, and B. Winnicka. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Journal of Scientific Programming, </journal> <volume> 2, </volume> <year> 1993. </year>
Reference-contexts: KeLP, in turn, gives the programmer much greater control over the implementation decisions. In our opinion, KeLP should not be considered a competitor to data-parallel languages. Rather, KeLP facilities might help implement the underlying run-time system for a data-parallel language. The pC++ language <ref> [29] </ref> extends C++ with a few constructs to support object-oriented data parallel codes. The pC++ collection class implements a distributed set of C++ element objects. pC++ supports an object-parallel control flow model; the program invokes a member function in parallel on elements of a collection.
Reference: [30] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, and B.Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Proc. Supercomputing, </booktitle> <pages> pages 588-597, </pages> <year> 1993. </year> <month> 161 </month>
Reference-contexts: To promote code re-use in these sections, we have implemented a KeLP DSL to support regular BLOCK partitionings. The KeLP DOCK (Decomposition Objects for KeLP) library provides domain-specific abstractions to manage regular HPF-style block data decompositions and partitioning decisions. Following the general strategy introduced in pC++ <ref> [30] </ref>, DOCK provides first-class C++ classes to represent the Fortran-D [67] three stage mapping process adopted in HPF [78]. Unlike pC++ or HPF, DOCK supports two levels of BLOCK data decomposition: either decompositions over multiple SMP nodes, or over the multiple processors at a single SMP node.
Reference: [31] <author> B. L. Bodnar and A. C. Liu. </author> <title> Modeling and performance analysis of single-bus tightly-coupled multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 464-470, </pages> <month> March </month> <year> 1989. </year>
Reference: [32] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> FortranD/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results. </title> <booktitle> In Proc. Supercomputing, </booktitle> <pages> pages 351-360, </pages> <year> 1993. </year>
Reference-contexts: HPF is well-suited for regular, static applications such as jacobi2D on single-tier multi-computers. It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D <ref> [32] </ref>, Kali [100], CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation.
Reference: [33] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. D. Kubiatowicz. </author> <title> Remote queues: exposing message queues for optimization and atomicity. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 42-53, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages <ref> [136, 107, 33] </ref> or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years.
Reference: [34] <author> T. Brewer and G. Astfalk. </author> <title> The evolution of the HP/Convex Exemplar. </title> <booktitle> In Proceedings of the 42nd IEEE Computer Society International Conference (COMPCON 97), </booktitle> <pages> pages 81-6, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: We chose to avoid this more general model due to the difficulty in implementing collective Grids on distributed-memory hardware. A collective Grid, in effect, defines a shared-memory address space across all SMP nodes. Distributed shared-memory (DSM) machines such as the SGI Origin 2000 [93] and the HP/Convex Exemplar <ref> [34] </ref> provide a shared address space. However, clusters of workstation-like SMPs do not provide a shared address space, so a collective Grid implementation would demand a software-based DSM. While software-based DSM systems have been presented (eg. [88, 40]), it is not clear how these systems will perform on SMP clusters.
Reference: [35] <author> W. L. Briggs. </author> <title> A Multigrid Tutorial. </title> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: Many software systems provide specific support for block-structured scientific calculations. Block structures arise in many scientific applications ranging from finite difference methods for partial differential equations <ref> [35] </ref> to blocked al 9 gorithms for numerical linear algebra [48]. In these applications, computational structures arise as uniform rectangular arrays of data, which communicate in potentially irregular patterns. <p> In total, communication takes 1795 ms. In other words, overlapping communication and computation at a SparcStation node retarded inter-node communication by 43%. 4.2.3 NAS Multigrid Benchmark The NAS-MG multigrid benchmark [15] solves Poisson's equation in 3D using a multigrid V-cycle <ref> [35] </ref>. Each level of the multigrid algorithm involves a relaxation operation, and a residual computation. These two operations involve stencil calculations, and we can parallelize them with techniques similar to those for redblack3D. Each level of the multigrid hierarchy defines a grid at a different resolution.
Reference: [36] <author> D. L. Brown, W. D. Henshaw, and D. J. Quinlan. Overture: </author> <title> an object-oriented framework for solving partial differential equations. </title> <booktitle> In Proceedings of 1997 Internation Scientific Computing in Object-Oriented Parallel Environments Conference, </booktitle> <address> Marina del Rey, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: The IrregularGrid class computes and carries around a MotionPlan to fill in ghost cells. FD-DSL facilitates data motion, data decomposition, and a few extremely simple arithmetic and norm calculations for finite difference methods. Other libraries, such as POOMA [118], OVERTURE <ref> [36] </ref>, and PETSc [16] implement more extensive numerical operations for PDE solvers. KeLP supports the development of more extensive and powerful domain-specific libraries [38]. The design and implementation of such libraries raises many algorithmic and software engineering issues that require expert application-specific knowledge.
Reference: [37] <author> E. Bugnion, J. M. Anderson, T. C. Mowry, M. Rosenblum, and M. S. Lam. </author> <title> Compiler-directed page coloring for multiprocessors. </title> <journal> SIGPLAN Notices, </journal> <volume> 31(9) </volume> <pages> 244-55, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: This technique could be used to minimize cache conflicts 49 in limited-associativity caches, having the run-time system transparently interact with the operating system page-coloring policy <ref> [37] </ref>. Finally, interoperability with other programming models remains an outstanding KeLP issue. Merlin et al. have successfully incorporated KeLP with SHPF, a data-parallel HPF-like Fortran dialect [101]. Interaction with other systems, such as PETSc [16] and task-oriented systems [74, 66], remains an unresolved issue.
Reference: [38] <author> E. J. Bylaska, S. R. Kohn, S. B. Baden, A. Edelman, R. Kawai, M. E. Ong, and J. H. Weare. </author> <title> Scalable parallel numerical methods and software tools for material design. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: FD-DSL facilitates data motion, data decomposition, and a few extremely simple arithmetic and norm calculations for finite difference methods. Other libraries, such as POOMA [118], OVERTURE [36], and PETSc [16] implement more extensive numerical operations for PDE solvers. KeLP supports the development of more extensive and powerful domain-specific libraries <ref> [38] </ref>. The design and implementation of such libraries raises many algorithmic and software engineering issues that require expert application-specific knowledge.
Reference: [39] <author> L. E. Cannon. </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, </institution> <year> 1969. </year>
Reference-contexts: Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers <ref> [50, 81, 39, 68, 96] </ref>. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts [135]. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions.
Reference: [40] <author> J. B. Carter. </author> <title> Design of the Munin distributed shared memory system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29(2) </volume> <pages> 219-227, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system <ref> [88, 40, 60] </ref>. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years. <p> However, clusters of workstation-like SMPs do not provide a shared address space, so a collective Grid implementation would demand a software-based DSM. While software-based DSM systems have been presented (eg. <ref> [88, 40] </ref>), it is not clear how these systems will perform on SMP clusters. Examining three Grid levels remains an outstanding research issue. 2.5.4 Data Motion KeLP's data motion facilities, the MotionPlan and Mover, provide powerful new abstractions for storing and manipulating communication patterns.
Reference: [41] <author> N. Casanova and J. Dongarra. NetSolve: </author> <title> a network enabled server for solving computational science problems. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <volume> 11(3) </volume> <pages> 212-23, </pages> <month> Fall </month> <year> 1997. </year> <month> 162 </month>
Reference-contexts: For example, an FFT library should provide startFFT () and finishFFT () calls, which the programmer can use to structure the calling application as needed. While ScaLAPACK does not provide asynchronous operations, the NetSolve interface provides asynchronous access to numerical library routines for distributed systems <ref> [41] </ref>. The results from this dissertation further reinforce the benefits of asynchronous entry points to numerical libraries, 149 even in more tightly coupled cluster environments. We note that the current KeLP model does not explicitly support block-cyclic data layouts.
Reference: [42] <author> L. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multi-cache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-8, </volume> <month> Decem-ber </month> <year> 1978. </year>
Reference-contexts: Cedar connected the multiprocessor nodes to a global shared memory via an omega network. The DASH implementation featured clusters of 4-processor Silicon Graphics 4D/340 multiprocessors, connected by a custom interconnect. The DASH provided consistent shared memory via a full directory cache coherence protocol <ref> [42] </ref>. The Cm* contained ten clusters, each containing of five "computer modules" (processor-memory pairs). Processors could communicate either through the non-uniform shared memory system or by message-passing. 77 Karlsson and Stenstrom [87] examined performance of an SMP cluster with ATM interconnection and distributed virtual shared memory.
Reference: [43] <author> S. Chakrabarti, E. Deprit, E.-J. Im, J. Jones, A. Krishnamurthy, C.-P. Wen, and K. Yelick. Multipol: </author> <title> A distributed data structure library. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> Jul. </month> <year> 1995. </year>
Reference: [44] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Fifth International Workshop of Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: A clever implementation may eliminate the barriers to improve performance. The KeLP 2.0 implementation enforces the procIterator barrier, but eliminates the nodeIterator barrier with inspector/executor communication analysis. Note that KeLP structured loops contrast sharply with unstructured thread programming, where the programmer must explicitly manage synchronization between individual threads. CC++ <ref> [44] </ref> provides a programming model with both types of parallel control constructs. <p> HPC++ provides many facilities to help manage communication and synchronization between address spaces. For example, HPC++ provides a global pointer abstraction, parallel versions of C++ Standard Template Library objects, remote function calls, and inter-context collective operations. HPC++ borrows concepts from several previous object-based systems, including CC++ <ref> [44] </ref>, ABC++ [11], and MPC++ [83]. In general, these concurrent object-oriented models that allow multiple concurrent member function invocations could support multi-tier programming. Many systems allow construction of two-level parallel programs, where the first level handles parallelism between objects and the second level handles parallelism within objects.
Reference: [45] <author> C. Chang, A. Sussman, and J. Saltz. </author> <title> Support for distributed dynamic data structures in C++. </title> <type> Technical Report CS-TR-3266, </type> <institution> University of Maryland, </institution> <year> 1995. </year>
Reference: [46] <author> D. Y. Chang, D. J. Kuck, and D. H. Lawrie. </author> <title> On the effective bandwidth of parallel memories. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(5):480-489, </volume> <month> May </month> <year> 1977. </year>
Reference: [47] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima. </author> <title> Dynamic data distributions in Vienna Fortran. </title> <type> Technical Report 93-92, </type> <institution> ICASE, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: HPF is well-suited for regular, static applications such as jacobi2D on single-tier multi-computers. It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali [100], CM Fortran [132], and Vienna Fortran <ref> [47] </ref>. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation. Like KeLP, ZPL relies on a Region abstraction to describe rectangular subsets of index space.
Reference: [48] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK:a portable linear algebra library for distributed memory computers. design issues and performance. </title> <booktitle> In Applied Parallel Computing. Computation in Physics, Chemistry and Engineering Science. Second International Workshop. PARA `95, </booktitle> <pages> pages 95-106, </pages> <year> 1995. </year>
Reference-contexts: Many software systems provide specific support for block-structured scientific calculations. Block structures arise in many scientific applications ranging from finite difference methods for partial differential equations [35] to blocked al 9 gorithms for numerical linear algebra <ref> [48] </ref>. In these applications, computational structures arise as uniform rectangular arrays of data, which communicate in potentially irregular patterns. Although many software systems support block-structured calculations on distributed-memory and shared-memory single-tier multicomputers, relatively few efforts have targeted multi-tier architectures such as SMP clusters. <p> When using distributed arrays on distributed-memory computers, we must choose a data layout for the array. For the ScaLAPACK LU algorithm, it has been observed that the 2D block-cyclic array layout is usually a good choice <ref> [48] </ref>. This layout has several good properties. First of all, the block-cyclic layout ensures good load balance during the BLAS 3 operations. <p> However, the 1D block-cyclic layout eliminates all inter-node communication during pdgetf2. Thus, the 1D block-cyclic layout may outperform the 2D block-cyclic layout on machines with few nodes and high communication costs <ref> [48] </ref>. Many SMP clusters fall into this category, and on these machines, we should consider the 1D block-cyclic layout. For either 2D or 1D block-cyclic layouts, we must choose a reasonable block size b. <p> We have demonstrated that explicit overlap of communication and computation improves performance on the FFT and dense linear algebra codes. Typically, the programmer will rely on libraries such as ScaLAPACK <ref> [48] </ref> or FFT-Pack [131] to implement these operations. In order to explicitly overlap communication and computation, we propose that standard libraries provide asynchronous entry points for numerical routines. <p> Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models [53, 134] focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation <ref> [135, 65, 48, 55] </ref>. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters. In particular, message-passing parallel performance models neglect the effects of shared memory contention and synchronization on application performance.
Reference: [49] <author> J. Choi, J. J. Dongarra, L. S. Ostrouchov, A. P. Petitet, D. W. Walker, and R. C. Whaley. </author> <title> Design and implementation of the ScaLAPACK LU, QR, and Cholesky factorization routines. </title> <journal> Scientific Programming, </journal> <volume> 5(3) </volume> <pages> 173-84, </pages> <month> Fall </month> <year> 1996. </year>
Reference-contexts: A fourth code, the NAS Fast Fourier transform benchmark [15], solves a 3D diffusion equation with FFTs. Finally, we consider two blocked dense linear algebra codes: SUMMA matrix multiplication [135] and ScaLAPACK's right-looking blocked dense LU factorization algorithm <ref> [49] </ref>. Each code raises distinct issues regarding the KeLP programming model. For each code, we examine algorithmic techniques to improve performance on SMP clusters. In particular, we restructure each algorithm to explicitly overlap communication and computation. <p> This application factors a dense matrix A as A=LU, where L is lower triagonal and U is upper triagonal. For distributed memory machines, we consider the blocked right-looking distributed LU factorization algorithm of ScaLAPACK <ref> [49] </ref>. This algorithm decomposes LU factorization so that local computation invokes BLAS [56]. Most of the computational work falls to BLAS-3 routines, in order to use cache memory efficiently. We present a brief sketch of the algorithm here; see [49] for a full description. proceeds in a blocked fashion. <p> we consider the blocked right-looking distributed LU factorization algorithm of ScaLAPACK <ref> [49] </ref>. This algorithm decomposes LU factorization so that local computation invokes BLAS [56]. Most of the computational work falls to BLAS-3 routines, in order to use cache memory efficiently. We present a brief sketch of the algorithm here; see [49] for a full description. proceeds in a blocked fashion. In each iteration of the outer loop, the algorithm factors the trailing submatrix A, indicated by the union of Regions A 00 ; A 01 ; A10 and A 11 in Figure 4.29.
Reference: [50] <author> J. Choi, J. J. Dongarra, and D. W. Walker. PUMMA: </author> <title> Parallel universal matrix multiplication algorithms on distributed memory concurrent computers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 543-570, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers <ref> [50, 81, 39, 68, 96] </ref>. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts [135]. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions.
Reference: [51] <author> P. E. Crandall, E. V. Sumithasri, J. Leichtl, and M. A. Clement. </author> <title> A taxonomy for dual-level parallelism in cluster computing. </title> <note> Submitted for publication, http://www.eng2.uconn.edu/pc/IJSS.ps. 163 </note>
Reference-contexts: Their taxonomy classifies programming styles along three axes: address space, process scheduling, and heterogeneity of the model. It is not clear how a hybrid model like an XYZ program fits in Gropp and Lusk's taxonomy. Crandall et. al <ref> [51] </ref> report experiences with dual-level parallel programs on an SMP cluster. This work uses threads to control intra-node parallelism, and PVM [130] to manage inter-node parallelism.
Reference: [52] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proc. Supercomputing, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: For built-in distributed array and vector class, pC++ provides a run-time version of HPF array distributions, implementing alignments, templates, and virtual processor sets with C++ classes. KeLP's DOCK library uses a similar strategy, although DOCK does not implement CYCLIC distributions. The Split-C language <ref> [52] </ref> extends C with several constructs for parallel computation. Split-C supports a SPMD programming model with a global address space. The program can access either local memory locations or remote memory locations, using global pointers. <p> This communication interface differs significantly from those in other parallel libraries and languages. MPI [102] allows the expression of communication overlap with non-blocking messages. However, with MPI's interface, the programmer asynchronously starts a single message at a time. Split-C <ref> [52] </ref> provides asynchronous remote memory access, but also at a low-level. In contrast to these approaches, the KeLP MotionPlan represents an arbitrary collective block-structured data motion pattern, potentially involving many 48 messages as part of a single unified pattern.
Reference: [53] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eiken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the Fourth Symposium on Principle and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 [2], Cray T3E [117], and networks of uniprocessor workstations [8]. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA [123], BSP [134], and LogP <ref> [53] </ref>. Research based on the single-tier multicomputer model has resulted in advances in algorithms, compiler technology, programming languages, and run-time libraries which make it possible to achieve reasonably good performance for many scientific applications on distributed-memory parallel computers. <p> One oft-proposed strategy for masking communication costs is to overlap communication and computation. If communication can occur concurrently with computation, the program can keep processors busy with other activities while waiting for communication to complete. Culler et al. have proposed modeling communication delay from two sources: overhead and latency <ref> [53] </ref>. Overhead accounts for the time a processor spends executing instructions on behalf of communication activity. This includes buffer-packing and message-passing protocol activities. Latency describes the transmission delay across the network. A single-processor multicomputer node can overlap latency with local computation, but cannot hide the message overhead. <p> Snyder clearly articulates the two-level approach in the XYZ program levels of the Phase Abstractions programming model [124]. The two-level control flow model matches single-tier architectural models, such as the Candidate Type Architecture [123], BSP [134], and LogP <ref> [53] </ref>. These architectural models do not represent the two levels of locality and parallelism of SMP clusters. In contrast to the two levels of MPI and HPF, the multi-tier KeLP abstractions support three levels of control: a collective level, a node level, and a processor level. <p> Snyder's Candidate Type Architecture (CTA) [123] provides a general machine model for a multicomputer with a central controller. Valiant's Bulk-synchronous Parallel (BSP) model [134] considers a program as a sequence of coarse-grain supersteps, and provides an estimate of communication costs with a bandwidth parameter. The LogP <ref> [53] </ref> model provides parameters to characterize communication costs on a CTA-like machine, specifically communication latency, bandwidth, and overhead. LogGP [4] extends LogP with a parameter to model bandwidth for long messages. None of these models address the two levels of parallelism and locality in SMP clusters. <p> Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models <ref> [53, 134] </ref> focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation [135, 65, 48, 55]. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters.
Reference: [54] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <year> 1994. </year>
Reference: [55] <author> F. Desprez, S. Domas, and B. Tourancheau. </author> <title> Optimization of the ScaLA-PACK LU factorization routine using communication/computation overlap. </title> <booktitle> In Proceedings of the Second International Euro-Par Conference, </booktitle> <pages> pages 3-10, </pages> <address> Lyon, France, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Finally, we consider performance on the full SMP cluster, using multiple processors per node. We will compare three versions of the code: the ScaLAPACK code, a basic multi-tier KeLP version, and a restructured multi-tier version that explicitly overlaps communication and computation. In previous work, Desprez et al. <ref> [55] </ref> present a restructured version of the ScaLAPACK LU algorithm to overlap communication and computation on single-tier multicomputers. They propose two optimizations. First, their algorithm overlaps communication and computation between nodes by starting the broadcast of a column panel before solving on the block row. <p> Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models [53, 134] focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation <ref> [135, 65, 48, 55] </ref>. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters. In particular, message-passing parallel performance models neglect the effects of shared memory contention and synchronization on application performance.
Reference: [56] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: The processor-level control stream executes a serial instruction stream on a single physical processor. In most cases, the processor level will invoke highly tuned serial 19 numeric kernels, such as the BLAS <ref> [56] </ref> or optimized Fortran and C code. The KeLP program starts in the collective level, and descends to the node level and to the processor level through two KeLP constructs: the nodeIterator and procIterator. These iterators interpret the assignment of parallel loop iterations as stored in a Map meta-data object. <p> We now turn our attention to a different problem class: dense linear algebra. Efficient dense linear algorithms operate in block structures in order to make good use of the memory hierarchy <ref> [56] </ref>. Since KeLP provides facilities to manage distributed block structures, we will evaluate whether KeLP abstractions aid in efficient implementations for blocked dense matrix algorithms. Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. <p> This application factors a dense matrix A as A=LU, where L is lower triagonal and U is upper triagonal. For distributed memory machines, we consider the blocked right-looking distributed LU factorization algorithm of ScaLAPACK [49]. This algorithm decomposes LU factorization so that local computation invokes BLAS <ref> [56] </ref>. Most of the computational work falls to BLAS-3 routines, in order to use cache memory efficiently. We present a brief sketch of the algorithm here; see [49] for a full description. proceeds in a blocked fashion.
Reference: [57] <author> A. C. Dusseau, R. H. Arpaci, and D. E. Culler. </author> <title> Effective distributed scheduling of parallel workloads. </title> <journal> Performance Evaluation Review, </journal> <volume> 24(1) </volume> <pages> 25-36, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We consider only dedicated hardware; that is, we focus on the performance of a single code running on reserved hardware, without interference from external users. With this limitation, we need not deal with scheduling decisions and resource contention between multiple independent parallel jobs <ref> [62, 125, 57] </ref>. Block-structured applications encompass those codes which give rise to (possibly multidimensional) rectangular, block data or numerical structures. This class of calculations includes numerical techniques such as finite difference methods for partial differential equations, Fast Fourier Transforms, and blocked algorithms for dense linear algebra.
Reference: [58] <author> R. Eigenmann, J. Hoeflinger, G. Jaxson, and D. Padua. </author> <title> Cedar Fortran and its compiler. </title> <booktitle> In CONPAR 90-VAPP IV. Joint International Conference on Vector and Parallel Parocessing, </booktitle> <pages> pages 288 - 99, </pages> <year> 1990. </year>
Reference-contexts: Although this dissertation focuses on architectural considerations, we also review application-centric techniques that may apply. The Illinois Cedar architecture [133] was a prominent cluster-based NUMA shared memory architecture. To use Cedar for scientific applications, the Cedar Fortran language <ref> [58] </ref> included constructs to express multiple levels of parallelism and locality. Cedar Fortran variables are allocated with global, node, or processor scope. With these constructs, the programmer could implement a two-level array decomposition by hand.
Reference: [59] <author> A. Erlichson, B. Nayfeh, J. P. Singh, and K. Olukotun. </author> <title> The benefits of clustering in shared address space multiprocessors: An applications-driven investigations. </title> <type> Technical Report CSL-TR-94-632, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: This study examined the tradeoffs of dedicating up to five processors on each SMP node to handle protocol processing. The study concludes that dedicated coprocessors improve performance, but not in proportion to the processing resources consumed. In a separate study, Erlichson et al. <ref> [59] </ref> considered the impact of clustering in a shared memory system for various applications. This study concluded that in simulations with realistic hardware assumptions, using SMP nodes in a shared memory multiprocessor does not significantly improve performance compared to uniprocessor nodes on a variety of applications.
Reference: [60] <author> A. Erlichson, N. Nuckolls, G. Chasson, and J. Hennessy. SoftFLASH: </author> <title> Analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-20, </pages> <address> Cambridge, MA, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system <ref> [88, 40, 60] </ref>. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years. <p> Erlichson et al. consider implementation tradeoffs for SoftFLASH, a software-based distributed virtual shared memory system implemented on a cluster of SGI Challenge multiprocessors <ref> [60] </ref>. SoftFLASH, implemented in the Irix operating system, provides shared memory, mutual exclusion, and synchronization facilities for the application programmer. This study examined the tradeoffs of dedicating up to five processors on each SMP node to handle protocol processing.
Reference: [61] <author> B. Falsafi and D. A. Wood. </author> <title> Scheduling communication on an SMP node parallel machine. </title> <booktitle> In Proceedings of the Third International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 128-38, </pages> <address> San Antonio, TX, </address> <month> February </month> <year> 1997. </year> <month> 164 </month>
Reference-contexts: Their work assumes that compute processors suffer much idle time due to stalls for remote read requests over the high-latency ATM switch. Falsafi and Wood <ref> [61] </ref> also compared implementation tradeoffs when using a communication coprocessor on a SMP cluster with distributed virtual shared memory. They report that dedicating a single SMP processor to handle protocol processing improves performance for high-overhead protocols and communication-intensive applications, while the strategy does not help for low-overhead lightweight DSM protocols.
Reference: [62] <author> S. Figueira. </author> <title> Modeling the Effects of Contention on Application Performance in Multi-User Environments. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, San Diego, CA, </address> <year> 1997. </year>
Reference-contexts: We consider only dedicated hardware; that is, we focus on the performance of a single code running on reserved hardware, without interference from external users. With this limitation, we need not deal with scheduling decisions and resource contention between multiple independent parallel jobs <ref> [62, 125, 57] </ref>. Block-structured applications encompass those codes which give rise to (possibly multidimensional) rectangular, block data or numerical structures. This class of calculations includes numerical techniques such as finite difference methods for partial differential equations, Fast Fourier Transforms, and blocked algorithms for dense linear algebra. <p> This program performs direct numerical simulation of a 2D mix ing layer using the Euler equations. * Silvia Figueira (Department of CSE, UCSD) used KeLP applications to in vestigate performance in multi-user parallel environments <ref> [62] </ref>. * Fran Berman, Rich Wolski, and collaborators (Department of CSE, UCSD) have used KeLP to investigate dynamic load balancing policies on heterogeneous workstation clusters [22, 23, 137].
Reference: [63] <author> S. J. Fink, S. B. Baden, and S. R. Kohn. </author> <title> Run-time support for irregular block-structured applications. </title> <note> to appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: Single-tier parallel computers have only one processor per node. A previous version of KeLP (KeLP 1.0) supported only single-tier parallel computers. Previous results have shown that on a single-tier multicomputer, KeLP1.0 performed comparably with hand-coded MPI codes <ref> [63] </ref>. We previously stated as a design goal that KeLP support for multi-tier architectures should not significantly impact performance on single-tier multicomput-ers. We now test this assumption by comparing hand-coded MPI implementations to KeLP 2.0 implementations on the T3E. We present results from the applications described in the previous Sections.
Reference: [64] <author> S. J. Fink, S. B. Baden, and S. R. Kohn. </author> <title> Flexible communication mechanisms for dynamic structured applications. </title> <booktitle> In Proc. 3rd Int'l Workshop IRREGULAR '96, </booktitle> <pages> pages 203-215, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Since the MotionPlan holds information about the program's data dependencies, this information can be useful for making algorithmic decisions at run-time. Additionally, the programmer can modify a MotionPlan, one tuple at a time. With this facility, the programmer can optimize extant MotionPlans, as described in <ref> [64] </ref>. Note that a MotionPlan holds only a description of a data motion pattern. Building a MotionPlan does not move any data. That task falls to the Mover. The Mover, a first-class executor [3] object, performs the data motion represented by a MotionPlan as a collective operation. <p> NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement <ref> [92, 64, 109, 116] </ref>. Many hierarchical programs arise from divide-and-conquer algorithms.
Reference: [65] <author> I. Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Menlo Park, CA, </address> <year> 1995. </year>
Reference-contexts: Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models [53, 134] focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation <ref> [135, 65, 48, 55] </ref>. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters. In particular, message-passing parallel performance models neglect the effects of shared memory contention and synchronization on application performance.
Reference: [66] <author> I. T. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <note> to appear in J. Parallel and Dist. Computing, </note> <year> 1994. </year>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement [92, 64, 109, 116]. Many hierarchical programs arise from divide-and-conquer algorithms. Several task-oriented parallel languages <ref> [70, 27, 66, 75] </ref> support fork-join parallelism suitable for divide-and-conquer. 2.4.3 Single-Tier Parallel Languages We now review prominent parallel programming languages that apply to block-structured applications, but do not explicitly address a multi-tier or hierarchical programming style. Data-parallel languages support regular array-based calculations. <p> Finally, interoperability with other programming models remains an outstanding KeLP issue. Merlin et al. have successfully incorporated KeLP with SHPF, a data-parallel HPF-like Fortran dialect [101]. Interaction with other systems, such as PETSc [16] and task-oriented systems <ref> [74, 66] </ref>, remains an unresolved issue. Chapter 3 Implementation 3.1 Introduction KeLP aims to provide a convenient programming model to express high performance block-structured scientific calculations on SMP clusters. To realize high performance, the system must satisfy two requirements: 1.
Reference: [67] <author> G. Fox, S. Hiranandani, K. Kennedy, C Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, Houston, TX, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: HPF is well-suited for regular, static applications such as jacobi2D on single-tier multi-computers. It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D <ref> [67] </ref>, Fortran 90D [32], Kali [100], CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation. <p> The KeLP DOCK (Decomposition Objects for KeLP) library provides domain-specific abstractions to manage regular HPF-style block data decompositions and partitioning decisions. Following the general strategy introduced in pC++ [30], DOCK provides first-class C++ classes to represent the Fortran-D <ref> [67] </ref> three stage mapping process adopted in HPF [78]. Unlike pC++ or HPF, DOCK supports two levels of BLOCK data decomposition: either decompositions over multiple SMP nodes, or over the multiple processors at a single SMP node.
Reference: [68] <author> G. Fox, S. Otto, and A. Hey. </author> <title> Matrix algorithms on a hypercube i: matrix multiplication. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 17-31, </pages> <year> 1997. </year>
Reference-contexts: Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers <ref> [50, 81, 39, 68, 96] </ref>. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts [135]. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions.
Reference: [69] <author> S. H. Fuller, J. K. Ousterhout, L. Raskin, P. I. Rubinfeld, P. J. Sindhu, and R. J. Swan. Multi-microprocessors: </author> <title> An overview and working example. </title> <journal> Proc. IEEE, </journal> <volume> 66(2) </volume> <pages> 216-28, </pages> <month> Feb. </month> <year> 1978. </year>
Reference-contexts: In contrast, shared memory systems must hide latency of remote references to reduce compute processor stalls. Three prominent cluster-based NUMA shared memory machines were the Illinois Cedar architecture [133], the Stanford DASH [94] and the CMU Cm* <ref> [69] </ref> projects. The Cedar machine consisted of a small number of Alliant FX/8 multiprocessors, each containing up to eight pipelined processors. Cedar connected the multiprocessor nodes to a global shared memory via an omega network.
Reference: [70] <author> A. S. Grimshaw, J. B. Weissman, and T. Strayer. </author> <title> Portable run-time support for dynamic object-oriented parallel processing. </title> <type> Technical Report CS-93-40, </type> <institution> University of Virginia: Department of Computer Science, </institution> <month> Jul. </month> <year> 1993. </year>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement [92, 64, 109, 116]. Many hierarchical programs arise from divide-and-conquer algorithms. Several task-oriented parallel languages <ref> [70, 27, 66, 75] </ref> support fork-join parallelism suitable for divide-and-conquer. 2.4.3 Single-Tier Parallel Languages We now review prominent parallel programming languages that apply to block-structured applications, but do not explicitly address a multi-tier or hierarchical programming style. Data-parallel languages support regular array-based calculations.
Reference: [71] <author> W. G. Griswold, R. Wolski, S. B. Baden, S. J. Fink, and S. R. Kohn. </author> <title> Programming language requirements for the next millenium. </title> <journal> ACM Computing Surveys, </journal> <volume> 28A(4), </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: The DOCK library, which provides a run-time HPF-like data decomposition model, contributed to every application. Domain-specific application libraries provide an ever higher-level interface for the scientific programmer, and show great promise for making parallel computing more accessible to the community at large <ref> [71] </ref>. This Chapter presented results from small, manageable application kernels. The next Chapter surveys some of the real-world applications coded with KeLP. Every KeLP application we know of builds on one or more of the sample applications or Domain-Specific libraries presented in this Chapter.
Reference: [72] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1997. </year> <note> http://www.mcs.anl.gov/mpi/mpich/. </note>
Reference-contexts: Compiler Flags Compiler Flags AlphaServers gcc v2.7.2 -O2 Digital -O5 -tune ev4 Fortran v4.0 -automatic -u -assume noaccuracy sensitive SparcStations gcc v2.7.2 -O2 SunPRO -O4 -u -stackvar f77 SC4.2 T3E Cray C++ -O3 Cray -O3 -dp v3.0.2.1 CF90 v3.0.2.1 Platform Operating System MPI Implementation AlphaServers Digital UNIX 4.0 MPICH 1.1.0 <ref> [72] </ref> SparcStations SunOS 5.6 MPICH 1.1.0 T3E UNICOS 2.0.2.27 Cray Message Passing Toolkit v1.2.0.1 Table 3.2: Software environments for all experiments in the dissertation. mark, ring, simply passes messages around a ring of processors. The second benchmark, exch, organizes the nodes in a logical 1D ring (toroidal) topology.
Reference: [73] <author> W. W. Gropp and E. L. Lusk. </author> <title> A taxonomy of programming models for symmetric multiprocessors and SMP clusters. </title> <booktitle> In Proceedings 1995: Programming models for massively parallel computers, </booktitle> <pages> pages 2-7, </pages> <month> October </month> <year> 1995. </year> <month> 165 </month>
Reference-contexts: However, a straightforward hierarchical extension of the XYZ levels would cover the processor level. In general, it may be worthwhile to consider a recursive XYZ program model based on either nested CTAs or the PMH. Gropp and Lusk have proposed a programming model taxonomy for SMPs and SMP clusters <ref> [73] </ref>. Their taxonomy classifies programming styles along three axes: address space, process scheduling, and heterogeneity of the model. It is not clear how a hybrid model like an XYZ program fits in Gropp and Lusk's taxonomy. Crandall et. al [51] report experiences with dual-level parallel programs on an SMP cluster.
Reference: [74] <author> T. Gross, D. O'Hallaron, and J. Subholk. </author> <title> Task parallelism in a high performance fortran framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali [100], CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx <ref> [74] </ref> combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation. Like KeLP, ZPL relies on a Region abstraction to describe rectangular subsets of index space. <p> Finally, interoperability with other programming models remains an outstanding KeLP issue. Merlin et al. have successfully incorporated KeLP with SHPF, a data-parallel HPF-like Fortran dialect [101]. Interaction with other systems, such as PETSc [16] and task-oriented systems <ref> [74, 66] </ref>, remains an unresolved issue. Chapter 3 Implementation 3.1 Introduction KeLP aims to provide a convenient programming model to express high performance block-structured scientific calculations on SMP clusters. To realize high performance, the system must satisfy two requirements: 1.
Reference: [75] <author> D. Grunwald and S. Vajracharya. </author> <title> The DUDE runtime system: An object-oriented macro-dataflow approach to integrated task and object parallelism. </title> <type> Technical Report CU-CS-779-95, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <year> 1994. </year>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement [92, 64, 109, 116]. Many hierarchical programs arise from divide-and-conquer algorithms. Several task-oriented parallel languages <ref> [70, 27, 66, 75] </ref> support fork-join parallelism suitable for divide-and-conquer. 2.4.3 Single-Tier Parallel Languages We now review prominent parallel programming languages that apply to block-structured applications, but do not explicitly address a multi-tier or hierarchical programming style. Data-parallel languages support regular array-based calculations.
Reference: [76] <author> F. M. Hayes. </author> <title> Design of the AlphaServer multiprocessor sever systems. </title> <journal> Digital Technical Journal, </journal> <volume> 6(3) </volume> <pages> 8-19, </pages> <year> 1994. </year>
Reference-contexts: The first two platforms are SMP clusters. We include results on the Cray T3E to evaluate support for single-tier multicomputers. The Maryland Digital AlphaServer is a cluster of 10 AlphaServer 2100 nodes <ref> [76] </ref>. Each AlphaServer 2100 contains four 275 MHz Alpha 21064A processors. Each processor has an on-chip 16kB direct-mapped data cache and a 16kB direct-mapped instruction cache. Additionally, each processor has a 4MB direct-mapped off-chip unified cache.
Reference: [77] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. Baxter, J. P. Singh, R. Si-moni, K. Gharachorloo, D. Nakahira, M. Horowitz, A. Gupta, M. Rosen-blum, and J. Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-84, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. <ref> [94, 77, 115, 133, 28] </ref>), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. <p> Several distributed shared memory machines also dedicate hardware for protocol processing. The Wisconsin Typhoon architecture [115] dedicates a general-purpose processor from each SMP node to perform protocol processing. In the Stanford FLASH multiprocessor <ref> [77] </ref>, each node contains a custom programmable protocol processor. The Princeton SHRIMP multicomputer [28] implements a hardwired custom network interface to handle cache coherency protocol between commodity workstations. A few projects implement high-level programming models specifically for SMP clusters.
Reference: [78] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Other programming models, such as High Performance Fortran (HPF) <ref> [78] </ref>, provide higher-level abstractions for portable parallel programming. With higher-level abstractions, these languages reduce the complexity of parallel programs by hiding many of the low-level implementation details. <p> The KeLP run-time system manages the tedious low-level implementation details such as message-passing, processes, threads, synchronization, and memory allocation. Many parallel programming models hide even high-level details of parallelism and locality from the programmer. For example, data-parallel languages such as HPF <ref> [78] </ref> perform all inter-processor data motion implicitly, as dictated by the code's data dependencies and the distributed array layouts. With this programming model, the programmer need not explicitly manage data motion, but instead leaves all data motion decisions to the compiler. <p> For example, a SPMD MPI [102] program contains a number of collective operations, such as reductions, barriers and broadcasts, interspersed within a node level program. The node-level instructions form separate threads of control which execute independently. An HPF <ref> [78] </ref> program consists of a single flow of control, the collective level. However, the HPF LOCAL extrinsic environment allows an HPF program to drop into SPMD control flow. In other words, the program breaks into node-level control when it enters an extrinsic environment. <p> Data-parallel languages support regular array-based calculations. The data-parallel approach is typified by High Performance Fortran (HPF) <ref> [78] </ref>, a data-parallel Fortran standard defined by a group from academia and industry. HPF supports loop-based parallelism for F90 codes. The programmer can specify locality with distribution directives, which gives hints to the compiler as to how to distribute arrays across processors. <p> A KeLP DSL will usually specialize the KeLP base classes, adding functionality as needed to support a particular application class. For example, the KeLP FloorPlan can represent any block-structured domain partitioning, including irregular collections of blocks. However, many sections of code require only HPF-style regular BLOCK partitioning s <ref> [78] </ref>. To promote code re-use in these sections, we have implemented a KeLP DSL to support regular BLOCK partitionings. The KeLP DOCK (Decomposition Objects for KeLP) library provides domain-specific abstractions to manage regular HPF-style block data decompositions and partitioning decisions. <p> The KeLP DOCK (Decomposition Objects for KeLP) library provides domain-specific abstractions to manage regular HPF-style block data decompositions and partitioning decisions. Following the general strategy introduced in pC++ [30], DOCK provides first-class C++ classes to represent the Fortran-D [67] three stage mapping process adopted in HPF <ref> [78] </ref>. Unlike pC++ or HPF, DOCK supports two levels of BLOCK data decomposition: either decompositions over multiple SMP nodes, or over the multiple processors at a single SMP node.
Reference: [79] <author> C. H. Hoogendoorn. </author> <title> A general model for memory interference in multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(10):998-1005, </volume> <month> October </month> <year> 1977. </year>
Reference: [80] <author> J. Howe, S. B. Baden, T. Grimmett, and K. Nomura. </author> <title> Modernization of legacy application software. </title> <booktitle> In Proceedings of the Fourth International Workshop on Applied Parallel Computing (PARA98), Umea, </booktitle> <address> Sweden, </address> <month> June </month> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: to the Schrodinger equation for materials. * In collaboration with Keiko Nomura and Tamara Grimmett (Department of Applied and Mechanical Engineering, UCSD), Scott Baden and Jeffrey Howe (Department of Computer Science and Engineering, UCSD) used KeLP to produce a parallel implementation of a legacy Navier-Stokes solver for computational fluid dynamics <ref> [80] </ref>. This KDISTUF code performs direct numerical simulations of incompressible homogeneous sheared and unsheared turbulent flows. * Mary Wheeler (University of Texas at Austin) and collaborators are using KeLP for petroleum reservoir simulations.
Reference: [81] <author> S. Huss-Lederman, E. Jacobson, A. Tsao, and G. Zhang. </author> <title> Matrix multiplication on the intel touchstone DELTA. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 571-594, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers <ref> [50, 81, 39, 68, 96] </ref>. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts [135]. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions.
Reference: [82] <author> IEEE. </author> <title> IEEE Guide to the POSIX Open System Environment. </title> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: In our experience, the thread scheduler often maps several threads onto one physical processor, while leaving another processor idle. Furthermore, the threads migrate often, destroying cache locality. Since the POSIX standard <ref> [82] </ref> provides no user control over the mapping of lightweight threads to physical processors, we could not realize acceptable parallel performance using the lightweight threads. From this point on, we will use the word "thread" to refer to a stream of instructions managed by the KeLP 2.0 implementation.
Reference: [83] <author> Y. Ishikawa. </author> <title> Meta-level achitecture for extendable c++. </title> <type> Technical Report TR-94024, </type> <institution> Tsukuba Research Center, Real World Computing Partnership, </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: For example, HPC++ provides a global pointer abstraction, parallel versions of C++ Standard Template Library objects, remote function calls, and inter-context collective operations. HPC++ borrows concepts from several previous object-based systems, including CC++ [44], ABC++ [11], and MPC++ <ref> [83] </ref>. In general, these concurrent object-oriented models that allow multiple concurrent member function invocations could support multi-tier programming. Many systems allow construction of two-level parallel programs, where the first level handles parallelism between objects and the second level handles parallelism within objects.
Reference: [84] <author> L. K. John and Y.-C. Liu. </author> <title> Performance model for a prioritized multiple-bus multiprocessor system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45(5) </volume> <pages> 580-588, </pages> <month> May </month> <year> 1996. </year> <month> 166 </month>
Reference: [85] <author> L. Kale and S. Krishnan. CHARM++: </author> <title> a portable concurrent object oriented system in C++. </title> <booktitle> In Proceedings of OOPSLA, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference: [86] <author> C. Kamath, R. Ho, and D. P. Manley. DXML: </author> <title> a high-performance scientific subroutine library. </title> <journal> Digital Technical Journal, </journal> <volume> 6(3) </volume> <pages> 44-56, </pages> <year> 1994. </year>
Reference-contexts: We present performance results from the Maryland Digital AlphaServer. In all cases, we use local dgemm kernels from the Digital Extended Math Library (DXML) <ref> [86] </ref>. On this platform, DXML obtains 160 MFLOPS per processor for matrix sizes that do not fit in the L2 cache. We always select problem sizes that do not in in the L2 cache. <p> Based on these results, for the remainder of this Section, we restrict our attention to 1D block-cyclic layouts using a blocking factor of 50. We present results from the Maryland Digital AlphaServer. All local BLAS routines call the Digital Extended Math Library <ref> [86] </ref>. We did not have access to tuned BLAS for the SparcStations, so we do not report LU results on the SparcStations. We implemented the ScaLAPACK algorithm in KeLP. First, we consider performance on single-tier multicomputers, using only one processor per Al-phaServer.
Reference: [87] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of a cluster-based multiprocessor built from ATM switches and bus-based multiprocessor servers. </title> <booktitle> In Proceedings of the Second International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 4-13, </pages> <address> San Jose, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The DASH provided consistent shared memory via a full directory cache coherence protocol [42]. The Cm* contained ten clusters, each containing of five "computer modules" (processor-memory pairs). Processors could communicate either through the non-uniform shared memory system or by message-passing. 77 Karlsson and Stenstrom <ref> [87] </ref> examined performance of an SMP cluster with ATM interconnection and distributed virtual shared memory.
Reference: [88] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of USENIX Winter 1994 Conference, </booktitle> <pages> pages 115-32, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system <ref> [88, 40, 60] </ref>. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years. <p> However, clusters of workstation-like SMPs do not provide a shared address space, so a collective Grid implementation would demand a software-based DSM. While software-based DSM systems have been presented (eg. <ref> [88, 40] </ref>), it is not clear how these systems will perform on SMP clusters. Examining three Grid levels remains an outstanding research issue. 2.5.4 Data Motion KeLP's data motion facilities, the MotionPlan and Mover, provide powerful new abstractions for storing and manipulating communication patterns.
Reference: [89] <author> M. Ketcham. </author> <title> Parallelization of a numerical mixing layer simulation. </title> <type> Master's thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1998. </year> <note> In preparation. </note>
Reference-contexts: This project combines the independent strengths of KeLP and HPF to provide a powerful programming model for multi-block applications. * Michelle Ketcham (Department of Applied and Mechanical Engineering, UCSD) has developed a parallel version of the MIXLAYER code using KeLP <ref> [89] </ref>.
Reference: [90] <author> G. Kiczales, J. Lamping, A. Mendhekar, C. Maeda, C. Lopes, J.-M. Longtier, and J. Irwin. </author> <note> Aspect-oriented programming. Technical Report SPL97-008 P9710042, Xerox PARC, </note> <month> February </month> <year> 1997. </year>
Reference-contexts: The Mover optimization will apply to 80 all codes that link with optimized KeLP library, without any modification of the calling application. Thus, the KeLP abstractions separates the expression of correct programs from optimizations that affect performance. This type of separation of concerns results in easier-to-develop, more maintainable code <ref> [90] </ref>. The microbenchmark results also document some important hardware characteristics of these platforms. First, we note that codes with poor cache locality quickly saturate the memory system, limiting speedup with more than two processors. Secondly, note that interprocessor synchronization can be quite expensive.
Reference: [91] <author> S. Kohn, J. Weare, M. E. Ong, and S. B. Baden. </author> <title> Software abstractions and computational issues in parallel structured adaptive mesh methods for electronic structure calculations. In Proceedings of the Workshop on Structured Adaptive Mesh Refinement Grid Methods, </title> <address> Minneapolis, MN, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Some external efforts include: * Scott Kohn, Elizabeth Ong (Center for Applied Scientific Computing, Lawrence Livermore National Laboratory) John Weare (Department of Chemistry, UCSD), and Scott Baden (Department of Computer Science and Engineering, UCSD) implemented a structured adaptive mesh refinement algorithm to first principles simulations of electronic structures using KeLP <ref> [91] </ref>.
Reference: [92] <author> S. R. Kohn. </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations. </title> <type> PhD thesis, </type> <institution> University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1995. </year>
Reference-contexts: KeLP's parallel control flow objects provide three levels of control, corresponding to three levels of an SMP cluster's memory hierarchy. KeLP's data layout abstractions build on the structural abstraction programming methodology introduced in the LPARX programming system <ref> [92] </ref>. The data motion abstractions combine ideas ideas from structural abstraction and inspector/executor communication analysis [3] with asynchronous execution in the multi-level control flow. With the KeLP programming model, this dissertation contributes a new methodology for programming SMP clusters. <p> These applications entail dynamic and irregular communication between grids that arise at run-time and defy compile-time analysis. To support these potentially complex and dynamic application structures, KeLP builds on structural abstraction, a programming methodology introduced in the the LPARX programming system <ref> [92] </ref>. Under structural abstraction, first-class language objects represent the geometric structure of a block-structured parallel code. LPARX introduced structural abstraction objects to represent potentially irregularly block-structured data sets. KeLP inherits this facility from LPARX. <p> In LPARX, Kohn demonstrated that using the Region calculus, the programmer can express a variety of communication patterns <ref> [92] </ref>. The same concise algorithms apply to the construction of KeLP MotionPlans. For example, many finite difference calculations fill in ghost cells with data from logically overlapping grids. Figure 2.4 shows the KeLP pseudo-code to build a MotionPlan to fill in ghost cells for a finite-difference stencil. <p> F .setRegion (P; north) F .setRegion (P + 1; east) F .setRegion (P + 2; south) F .setRegion (P + 3; west) end dencies for a two-node SMP. b) KeLP code to setup the FloorPlan. 37 2.4 Related Work 2.4.1 Structural Abstraction KeLP descends directly from the LPARX programming system <ref> [92] </ref>. KeLP inherits the Point, Region, Grid, and XArray abstractions directly from LPARX, although the notions of Grid and XArray have evolved. The most important changes in these classes involve the FloorPlan classes, which hold the structure of a KeLP XArray. <p> NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement <ref> [92, 64, 109, 116] </ref>. Many hierarchical programs arise from divide-and-conquer algorithms. <p> The KeLP model introduces new meta-data programming abstractions, hierarchical control flow constructs, storage model, and data motion primitives. We now discuss these contributions of the KeLP model, as well as limitations of the abstractions. 2.5.1 Geometric Abstractions KeLP extends the structural abstraction programming methodology introduced in LPARX <ref> [92] </ref>. KeLP directly inherits the Point, Region, Grid, and XArray from LPARX. KeLP adds to this set the Map, FloorPlan, MotionPlan, and Mover abstractions. These new abstractions add functionality and expressive power to the structural abstraction programming methodology. <p> Examining three Grid levels remains an outstanding research issue. 2.5.4 Data Motion KeLP's data motion facilities, the MotionPlan and Mover, provide powerful new abstractions for storing and manipulating communication patterns. As discussed in Section 2.4.1, these classes incorporate concepts demonstrated in LPARX <ref> [92] </ref> and Multiblock PARTI [3]. We have assumed that inter-node communication is very expensive relative to local floating-point computation. The Mover's asynchronous entry points reflect this assumption, giving the programmer the power to overlap communication and computation. This communication interface differs significantly from those in other parallel libraries and languages. <p> First, we consider the simple second-order red-black Gauss-Seidel Poisson solver presented earlier in Section 2.3. Next, we examine a KeLP version of the 86 NAS MG multigrid benchmark. Finally, we address irregular and multilevel codes with a multigrid solver over an irregular block domain. In previous work, Kohn <ref> [92] </ref> proposed domain-specific library software design for adaptive finite difference codes. The KeLP finite difference codes build on Kohn's work. In the next Section, we describe the design and implementation of domain-specific abstractions for finite difference codes. <p> This facility is useful in certain multigrid algorithms. The central class of the finite difference DSL is the IrregularGrid, based on a facility Kohn specified in <ref> [92] </ref>. The FD-DSL IrregularGrid provides much the same interface as Kohn's, although the underlying implementation differs. The IrregularGrid abstraction represents a finite difference grid, physically partitioned 87 into blocks which are distributed across multiple nodes of a parallel computer.
Reference: [93] <author> J. Laudon and D. Lenoski. </author> <title> System overview of the SGI Origin 200/2000 product line. </title> <booktitle> In Proceedings of the 42nd IEEE Computer Society International Conference (COMPCON 97), </booktitle> <pages> pages 150-6, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: We chose to avoid this more general model due to the difficulty in implementing collective Grids on distributed-memory hardware. A collective Grid, in effect, defines a shared-memory address space across all SMP nodes. Distributed shared-memory (DSM) machines such as the SGI Origin 2000 <ref> [93] </ref> and the HP/Convex Exemplar [34] provide a shared address space. However, clusters of workstation-like SMPs do not provide a shared address space, so a collective Grid implementation would demand a software-based DSM.
Reference: [94] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hen-nessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. <ref> [94, 77, 115, 133, 28] </ref>), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. <p> Our work focuses on carefully orchestrated message-passing codes, restructured to avoid idle time on compute processors. In contrast, shared memory systems must hide latency of remote references to reduce compute processor stalls. Three prominent cluster-based NUMA shared memory machines were the Illinois Cedar architecture [133], the Stanford DASH <ref> [94] </ref> and the CMU Cm* [69] projects. The Cedar machine consisted of a small number of Alliant FX/8 multiprocessors, each containing up to eight pipelined processors. Cedar connected the multiprocessor nodes to a global shared memory via an omega network.
Reference: [95] <author> B.-H. Lim, P. Heidelberger, P. Pattnaik, and M. Snir. </author> <title> Message proxies for efficient, protected communication on smp clusters. </title> <booktitle> In Proceedings of the 167 Third International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 116-27, </pages> <address> San Antonio, TX, February 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Split-C supports automatic data layout for regular problems with spread arrays, which allow the 42 programmer to specify a variety of block and cyclic partitionings. Since the Split-C primitives encourage fine-grain programming, Split-C performance may depend highly on the overhead for short messages [98]. Lim et al. <ref> [95] </ref> examined the performance of various Split-C programs on SMP clusters under several architectural assumptions. <p> Like the KeLP2.0 implementation, pixel processors request inter-node messages via shared-memory mailboxes available to the Cluster Controller. The Proteus programming API presents a uniform message-passing model, which hides the two-level non-uniform memory hierarchy but implements intra-node messaging efficiently in shared memory. Lim et al. present message proxies <ref> [95] </ref>. A message proxy is a kernel-level process running on an idle SMP processor that provides protected access to the network interface. User-level processes communicate with the message proxy through queues in shared memory.
Reference: [96] <author> C. Lin and L. Snyder. </author> <title> A matrix product algorithm and its comparative performance on hypercubes. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 190-4, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Perhaps the best-known blocked dense matrix algorithm is dense matrix multiplication. The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers <ref> [50, 81, 39, 68, 96] </ref>. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts [135]. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions.
Reference: [97] <author> C. Lin and L. Snyder. </author> <title> ZPL:an array sublanguage. </title> <editor> In U. Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proc. Languages and Compilers for Parallel Computing, 6th Int'l Workshop, </booktitle> <pages> pages 96-114. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali [100], CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language <ref> [97] </ref> provides powerful geometric semantics to express data-parallel computation. Like KeLP, ZPL relies on a Region abstraction to describe rectangular subsets of index space. The ZPL programmer uses Regions to specify iteration space masks, which proves useful for some types of boundary conditions.
Reference: [98] <author> R. P. Martin, A. M. Vahdat, D. E. Culler, and T. E. Anderson. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 85-97, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: BW is the peak bandwidth observed for long messages. FLOPS results in parentheses are per-node, others are per-processor. ten causes most of the delay for coarse-grain structured applications <ref> [98] </ref>. This factor will severely limit the utility of communication overlap for single-tier mul ticomputers without a dedicated message coprocessor. Fortunately, the SMP node architecture naturally allows overlap of com munication and computation. <p> Split-C supports automatic data layout for regular problems with spread arrays, which allow the 42 programmer to specify a variety of block and cyclic partitionings. Since the Split-C primitives encourage fine-grain programming, Split-C performance may depend highly on the overhead for short messages <ref> [98] </ref>. Lim et al. [95] examined the performance of various Split-C programs on SMP clusters under several architectural assumptions.
Reference: [99] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel memories with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21(4) </volume> <pages> 339-374, </pages> <month> November </month> <year> 1984. </year>
Reference: [100] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <type> Technical Report 90-69, </type> <institution> ICASE, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: HPF is well-suited for regular, static applications such as jacobi2D on single-tier multi-computers. It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali <ref> [100] </ref>, CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation.
Reference: [101] <author> J. H. Merlin, S. B. Baden, S. J. Fink, and B. M. Chapman. </author> <title> Multiple data parallelism with HPF and KeLP. </title> <booktitle> In Proceedings HPCN '98, </booktitle> <address> Amsterdam, Netherlands, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: Finally, interoperability with other programming models remains an outstanding KeLP issue. Merlin et al. have successfully incorporated KeLP with SHPF, a data-parallel HPF-like Fortran dialect <ref> [101] </ref>. Interaction with other systems, such as PETSc [16] and task-oriented systems [74, 66], remains an unresolved issue. Chapter 3 Implementation 3.1 Introduction KeLP aims to provide a convenient programming model to express high performance block-structured scientific calculations on SMP clusters. <p> software infrastructure for parallel adaptive methods. * John Weare (Department of Chemistry, UCSD), collaborating with Scott Baden, is using KeLP to parallelize a Fourier-based planewave basis code for materials science applications. * John Merlin (Vienna Center for Parallel Computation) has implemented an interface between KeLP and the SHPF data-parallel language <ref> [101] </ref>. This project combines the independent strengths of KeLP and HPF to provide a powerful programming model for multi-block applications. * Michelle Ketcham (Department of Applied and Mechanical Engineering, UCSD) has developed a parallel version of the MIXLAYER code using KeLP [89].
Reference: [102] <author> Message-Passing Interface Standard. </author> <title> MPI: A message-passing interface standard. </title> <institution> University of Tennessee, Knoxville, TN, </institution> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: These complexities hinder efficient parallel implementations, and discourage the use of high-performance parallel computers for scientific computation. To address this problem, much research has considered parallel programming languages and libraries to help the programmer manage locality and parallelism. Notably, the Parallel Virtual Machine (PVM) [130] and Message-Passing Interface (MPI) <ref> [102] </ref> have emerged as standards for portable parallel programming with message-passing. <p> One way to quantify communication costs considers the potential floating-point work that could be done in the time to communicate a message. Table 1.1 shows peak hardware rates and observed message-passing performance for several single-tier multicomputers. The message-passing results were obtained with a simple ping-pong MPI <ref> [102] </ref> program. For all three architectures, a message start takes the same time as several thousand FLOPS at peak speed. <p> For example, a SPMD MPI <ref> [102] </ref> program contains a number of collective operations, such as reductions, barriers and broadcasts, interspersed within a node level program. The node-level instructions form separate threads of control which execute independently. An HPF [78] program consists of a single flow of control, the collective level. <p> We have assumed that inter-node communication is very expensive relative to local floating-point computation. The Mover's asynchronous entry points reflect this assumption, giving the programmer the power to overlap communication and computation. This communication interface differs significantly from those in other parallel libraries and languages. MPI <ref> [102] </ref> allows the expression of communication overlap with non-blocking messages. However, with MPI's interface, the programmer asynchronously starts a single message at a time. Split-C [52] provides asynchronous remote memory access, but also at a low-level.
Reference: [103] <author> Sun Microsystems. </author> <title> The SuperSPARC microprocessor. </title> <type> Technical report, </type> <institution> Sun Microsystems Computer Corporation, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The nodes communicate over a Digital Gigaswitch OC-3 ATM switch, with a peak point-to-point bandwidth of 155Mb/s. 60 The Oregon State SparcStation cluster contains four SparcStation 20 nodes. Each SparcStation contains four 50 MHz SuperSPARC processors <ref> [103] </ref>. Each processor has an on-chip 16kB four-way set associative data cache and a 20 kB five-way set associative instruction cache. Additionally, each processor has a 1MB direct-mapped off-chip unified cache.
Reference: [104] <author> S. S. Muchnick. </author> <title> Advanced Compiler Design and Implementation. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1997. </year>
Reference-contexts: In all applications, we have seen that memory bandwidth limits the efficiency of both local computation and message-passing with communication overlap. The KeLP abstractions do not directly address optimizations such as tiling for the single-processor memory hierarchy <ref> [104] </ref>. The application studies indicate that this area deserves more attention in future research. Chapter 5 Conclusion 5.1 Research Summary This dissertation has presented a programming model to facilitate high-performance implementations of block-structured scientific calculations on SMP clusters.
Reference: [105] <author> T. N. Mudge, H. B. Al-Sadoun, and B. A. Makrucki. </author> <title> Memory-interference model for multiprocessors based on semi-Markov processes. </title> <booktitle> IEE Proceedings, 134, Part E(4):203-214, </booktitle> <month> July </month> <year> 1987. </year>
Reference: [106] <author> S. Murer, J. Feldman, C.-C. Lim, and M.-M. Seidel. </author> <title> pSather:layered ex-tentions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-028, </type> <institution> Computer Science Divison, U.C. Berkeley, </institution> <month> Dec. </month> <year> 1993. </year> <month> 168 </month>
Reference-contexts: With these constructs, the programmer could implement a two-level array decomposition by hand. Cedar Fortran provided both two-level and flat parallel loops, allowing the programmer to program with either one or two levels of loop parallelism. The pSather language is based on a cluster machine model for specifying locality <ref> [106] </ref>. pSather presents a two-level shared address space in the framework of a concurrent object-oriented model. On shared-memory machines, pSather supports a non-blocking asynchronous function call with specialized synchronization mechanisms. On distributed memory machines, asynchronous function calls can be made to remote clusters.
Reference: [107] <author> S. Pakin, V. Karamcheti, and A. A. Chien. </author> <title> Fast messages: efficient portable communication for workstation clusters and MPPs. </title> <journal> IEEE Concurrency, </journal> <volume> 5(2) </volume> <pages> 60-72, </pages> <year> 1997. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages <ref> [136, 107, 33] </ref> or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years.
Reference: [108] <author> Y. Pang. </author> <title> Partitioning of large irregular domain problems for parallel processing. </title> <note> In Preparation. </note>
Reference-contexts: In the following experiments, we solve Poisson's equation over a grid structure covering the geometrical shape of Lake Superior. We obtained grid geometries generated and partitioned across nodes by Yingxin Pang (Department of Computer Science, UCSD) <ref> [108] </ref>, using a heuristic based on work described in [113]. The grid geometries result from laying a quad-tree over the domain, and assigning grids to processors using the recursive spectral bisection partitioning algorithm [112].
Reference: [109] <author> M. Parashar and J. C. Browne. </author> <title> An infrastructure for parallel adaptive mesh refinement techniques. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <year> 1995. </year> <note> Draft. </note>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement <ref> [92, 64, 109, 116] </ref>. Many hierarchical programs arise from divide-and-conquer algorithms. <p> Lim et al. [95] examined the performance of various Split-C programs on SMP clusters under several architectural assumptions. Their results emphasize the need for low-overhead messages for Split-C, and provide a promising software architecture to achieve this by reserving an idle SMP processor. 2.4.4 Libraries Parashar and Browne <ref> [109] </ref> have developed the Hierarchical Dynamic Distributed Array / Distributed Adaptive Grid Hierarchy (HDDA/DAGH) program development infrastructure (PDI), an infrastructure to support structured adaptive mesh refinement calculations. As in structural abstraction, HDDA/DAGH separates the structure of a grid hierarchy from the instantiation of the data.
Reference: [110] <author> R. Parsons and D. Quinlan. </author> <title> Run-time recognition of task parallelism within the P++ parallel array class library. </title> <booktitle> In Proc. Scalable Parallel Libraries Conference, </booktitle> <pages> pages 77-86, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Based on this framework, HDDA/DAGH implements a fixed load-balancing policy based on locality-preserving space- filling curves. P++ is a data-parallel array class library, built on the serial A++ array library <ref> [110] </ref>. P++ performs delayed evaluation of array expressions, allowing the run-time system to recognize array operations that can occur in parallel. P++ supports dynamic run-time array distributions. Like LPARX/KeLP, P++ presents a SPMD programming model, which is appropriate for structured applications with some task parallelism, such as adaptive mesh refinement.
Reference: [111] <author> P. Pierce and G. Regnier. </author> <title> The Paragon implementation of the NX message passing interface. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 184-190, </pages> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: That is, if a Grid copy's data happens to lie contiguously in memory, then the Mover will send or receive the data directly from its original storage locations. An important implementation decision concerns the method used to overlap communication and computation. Some architectures <ref> [19, 111, 128] </ref> provide specialized hardware support to overlap communication and computation. We considered two possibilities: using MPI non-blocking message-passing primitives, and devoting a single smp++ thread solely to communication. The MPI overlapping design is straightforward. <p> We note that the applications Erlichson et al. considered were not restructured to exploit the multi-tier memory hierarchy. Several distributed memory parallel computers devote specialized hardware on each node to handle communication protocols. Several commercial parallel 78 computers, such as the IBM SP-2 [2], Intel Paragon <ref> [111] </ref>, and Meiko CS-2 [19], augment each node with a general purpose microprocessor to handle message-passing events. This strategy reduces the software overhead of message-passing on the compute processors, and provides the opportunity to overlap communication and computation. Several distributed shared memory machines also dedicate hardware for protocol processing.
Reference: [112] <author> A. Pothen, H. Simon, and K.-P. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: The grid geometries result from laying a quad-tree over the domain, and assigning grids to processors using the recursive spectral bisection partitioning algorithm <ref> [112] </ref>. At several phases of the grid generation process, Pang combines adjacent small grids in order to reduce the total number of grids, increase patch granularity, and better differentiate small eigenvalues during the spectral partitioning algorithm. Figure 4.11 shows the resultant grids and partitioning assignments generated by Pang's process.
Reference: [113] <author> J. Rantakokko. </author> <title> A framework for partitioning domains with inhomogeneous workload. </title> <type> Technical Report 8, </type> <institution> Royal Institute of Technology and Uppsala University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: In the following experiments, we solve Poisson's equation over a grid structure covering the geometrical shape of Lake Superior. We obtained grid geometries generated and partitioned across nodes by Yingxin Pang (Department of Computer Science, UCSD) [108], using a heuristic based on work described in <ref> [113] </ref>. The grid geometries result from laying a quad-tree over the domain, and assigning grids to processors using the recursive spectral bisection partitioning algorithm [112].
Reference: [114] <author> C. V. Ravi. </author> <title> On the bandwidth and interference in interleaved memory systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21:899-901, </volume> <month> August </month> <year> 1972. </year>
Reference: [115] <author> S. K. Reinhardt, R. W. Pfile, and D. A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proceedings of the 23rd Annual International Conference on Computer Architecture, </booktitle> <pages> pages 34-43, </pages> <address> Philadelphia,PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. <ref> [94, 77, 115, 133, 28] </ref>), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. <p> This strategy reduces the software overhead of message-passing on the compute processors, and provides the opportunity to overlap communication and computation. Several distributed shared memory machines also dedicate hardware for protocol processing. The Wisconsin Typhoon architecture <ref> [115] </ref> dedicates a general-purpose processor from each SMP node to perform protocol processing. In the Stanford FLASH multiprocessor [77], each node contains a custom programmable protocol processor. The Princeton SHRIMP multicomputer [28] implements a hardwired custom network interface to handle cache coherency protocol between commodity workstations.
Reference: [116] <author> C. Rendleman, V. Becknerand J. Bell, B. Crutchfield, L. Howell, and M. </author> <title> Welcome. Boxlib users guide and manual: A library for managing rectangular domains. edition 1.99. </title> <type> Technical report, </type> <institution> Center for Computational Science and Engineering, Lawrence Livermore National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: NESL is an applicative language, and provides no constructs to control data decomposition or granularity of parallelism. Other systems with hierarchical data structures include systems with support for grid hierarchies in adaptive mesh refinement <ref> [92, 64, 109, 116] </ref>. Many hierarchical programs arise from divide-and-conquer algorithms.
Reference: [117] <author> Cray Research. </author> <title> Scalable power that works: the CRAY T3E series of scalable parallel systems. </title> <journal> Cray Channels, </journal> <volume> 18(1) </volume> <pages> 2-4, </pages> <year> 1996. </year>
Reference-contexts: The nodes communicate over an interconnection network, either by passing messages or relying on hardware-supported shared-memory. This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 [2], Cray T3E <ref> [117] </ref>, and networks of uniprocessor workstations [8]. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA [123], BSP [134], and LogP [53].
Reference: [118] <author> J. V. W. Reynders, J. C. Cummings, M. Tholburn, P. J. Hinker, S. R. Atlas, S. Banerjee, M. Srikant, W. F. Humphrey, S. R. Karmesin, and K. Keahey. POOMA: </author> <title> a framework for scientific simultation on parallel architectures. </title> <booktitle> In 169 Proceedings of the First International Workshop on High-Level Programming Models and Supportive Environments, </booktitle> <address> Honolulu, HI, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The IrregularGrid class computes and carries around a MotionPlan to fill in ghost cells. FD-DSL facilitates data motion, data decomposition, and a few extremely simple arithmetic and norm calculations for finite difference methods. Other libraries, such as POOMA <ref> [118] </ref>, OVERTURE [36], and PETSc [16] implement more extensive numerical operations for PDE solvers. KeLP supports the development of more extensive and powerful domain-specific libraries [38]. The design and implementation of such libraries raises many algorithmic and software engineering issues that require expert application-specific knowledge.
Reference: [119] <author> M. Rosing, R. B. Schnabel, and R. P. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali [100], CM Fortran [132], and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. <ref> [119] </ref>). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation. Like KeLP, ZPL relies on a Region abstraction to describe rectangular subsets of index space.
Reference: [120] <author> A. C. Sawdey, M. T. O'Keefe, and W. B. Jones. </author> <title> A general programming model for developing scalable ocean circulation applications. </title> <booktitle> In Proceedings of the ECMWF Workshop on the Use of Parallel Processors in Meteorology, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Each cluster defines a distinct address space, and each object is assigned to one cluster. Additionally, pSather provides some facilities for allocating distributed objects over multiple clusters, and parallel iteration over coarse-grain distributed objects. Sawdey et al. <ref> [120] </ref> have applied the Fortran-P programming model to grid-based applications on SMP clusters. In Fortran-P, a compiler translates serial grid-based code to explicitly threaded parallel code. Our approach differs in that KeLP exposes all partitioning and scheduling decisions to the programmer. <p> On the Maryland Digital AlphaServer, SIMPLE implements a custom message-passing layer, shown to be more efficient than MPI on this platform. It might be possible to re-implement KeLP, using SIMPLE to provide some needed underlying multi-tier primitives. The Fortran-P <ref> [120] </ref> language has been ported the SGI PowerChallengeAR-RAY. Like KeLP, the underlying Fortran P implementation devotes a single thread on each SMP to handle communication. Unlike KeLP, the Fortran P implementation manages parallelism at each node using dynamic load balancing based on self-similar partitionings of the domain.
Reference: [121] <author> L. Semenzato and P. Hilfinger. </author> <booktitle> Arrays in FIDIL. In Proc. First International Workshop on Arrays, Functional Programming, and Parallel Systems, </booktitle> <month> Jul. </month> <year> 1990. </year>
Reference-contexts: KeLP inherits the Point, Region, Grid, and XArray abstractions directly from LPARX, although the notions of Grid and XArray have evolved. The most important changes in these classes involve the FloorPlan classes, which hold the structure of a KeLP XArray. The FIDIL language <ref> [121] </ref> laid the groundwork for structural abstraction as employed by LPARX and KeLP. FIDIL provided general abstractions for representing structured and unstructured domains, and geometric operations over domains. FIDIL was targeted for serial computers and did not provide any notion of parallel control flow or data decomposition.
Reference: [122] <author> A. S. Sethi and N. Deo. </author> <title> Interference in multiprocessor systems with localized memory access probabilities. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(2):157-163, </volume> <month> February </month> <year> 1979. </year>
Reference: [123] <author> L. Snyder. </author> <title> Type architectures, shared memory, and the corollary of modest potential. </title> <booktitle> Annual Review of Computer Science, </booktitle> <volume> 1 </volume> <pages> 289-317, </pages> <year> 1986. </year>
Reference-contexts: This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 [2], Cray T3E [117], and networks of uniprocessor workstations [8]. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA <ref> [123] </ref>, BSP [134], and LogP [53]. Research based on the single-tier multicomputer model has resulted in advances in algorithms, compiler technology, programming languages, and run-time libraries which make it possible to achieve reasonably good performance for many scientific applications on distributed-memory parallel computers. <p> In other words, the program breaks into node-level control when it enters an extrinsic environment. Snyder clearly articulates the two-level approach in the XYZ program levels of the Phase Abstractions programming model [124]. The two-level control flow model matches single-tier architectural models, such as the Candidate Type Architecture <ref> [123] </ref>, BSP [134], and LogP [53]. These architectural models do not represent the two levels of locality and parallelism of SMP clusters. <p> Many parallel computation models roughly correspond the single-tier multicomputer model, pictured in Figure 1.1. Snyder's Candidate Type Architecture (CTA) <ref> [123] </ref> provides a general machine model for a multicomputer with a central controller. Valiant's Bulk-synchronous Parallel (BSP) model [134] considers a program as a sequence of coarse-grain supersteps, and provides an estimate of communication costs with a bandwidth parameter.
Reference: [124] <author> L. Snyder. </author> <title> Foundations of practical parallel programming languages. </title> <editor> In T. Hey and J. Ferrante, editors, </editor> <title> Portability and Performance of Parallel Processing. </title> <publisher> John Wiley and Sons, </publisher> <year> 1993. </year>
Reference-contexts: However, the HPF LOCAL extrinsic environment allows an HPF program to drop into SPMD control flow. In other words, the program breaks into node-level control when it enters an extrinsic environment. Snyder clearly articulates the two-level approach in the XYZ program levels of the Phase Abstractions programming model <ref> [124] </ref>. The two-level control flow model matches single-tier architectural models, such as the Candidate Type Architecture [123], BSP [134], and LogP [53]. These architectural models do not represent the two levels of locality and parallelism of SMP clusters. <p> The PMH models a parallel computer as a tree of memory modules, and can represent all levels of the memory hierarchy from secondary storage to functional units. The PMH suggests a coding style where chores run concurrently at different levels of the memory hierarchy [5]. The Phase Abstractions <ref> [124] </ref> programming model describes a parallel program for a CTA in terms of X, Y, and Z levels. The X level corresponds to a serial program on a single processor. The Y level coordinates data decomposition, communication, and synchronization between processors.
Reference: [125] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing (in conjunction with IPPS `95), </booktitle> <pages> pages 106-26, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: We consider only dedicated hardware; that is, we focus on the performance of a single code running on reserved hardware, without interference from external users. With this limitation, we need not deal with scheduling decisions and resource contention between multiple independent parallel jobs <ref> [62, 125, 57] </ref>. Block-structured applications encompass those codes which give rise to (possibly multidimensional) rectangular, block data or numerical structures. This class of calculations includes numerical techniques such as finite difference methods for partial differential equations, Fast Fourier Transforms, and blocked algorithms for dense linear algebra.
Reference: [126] <author> A. Sohn and R. Biswas. </author> <title> Communication studies of DMP and SMP machines. </title> <type> Technical Report NAS-97-004, </type> <institution> NAS, </institution> <year> 1997. </year>
Reference-contexts: We chose the message size in the long message regime (at least 1MB), and repeat the microbenchmark repeatedly so the running time exceeds 10 seconds. We evaluate overlap with an overlap efficiency metric similar to one presented by Sohn and Biswas <ref> [126] </ref>. <p> The KeLP programmer should consider barriers and reductions expensive, and should endeavor to structure an application to call these primitives infrequently. Applications with make frequent calls to collective synchronization points will suffer from the high synchronization overheads. 3.6 Related Work Sohn and Biswas <ref> [126] </ref> examined the utility of communication overlap for FFT and bitonic sorting on the IBM SP-2 and SGI PowerCHALLENGEArray. This work introduced the overlap efficiency metric, which we adapted to evaluate overlap efficiency in KeLP.
Reference: [127] <author> A. K. Somani and A. M. Sansano. </author> <title> Minimizing overhead in parallel algorithms through overlapping communication/computation. </title> <type> Technical Report 97-8, </type> <institution> ICASE, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Sohn and Biswas report overlap efficiencies of up 40% on the SP-2, but only up to 10% on the PowerCHALLENGEArray. They did not consider dedicating one SMP processor to handle communication, which would have resulted in much higher overlap performance. The Proteus machine <ref> [127, 128] </ref> is a custom-built hierarchical SMP cluster designed for image processing applications. Each Proteus cluster a number of 76 Intel i860 "pixel processors" which run user code, an Intel i960 Cluster Controller, and shared memory modules. The Cluster Controller handles all inter-node communication.
Reference: [128] <author> A. K. Somani, C. Wittenbrink, R. M. Haralick, L.G. Shapiro, Jenq-Neng Hwang, Chung-Ho Chen, R. Johnson, and K. Cooper. </author> <title> Proteus system architecture and organization. </title> <booktitle> In Proceedings of the Fifth International Parallel Processing Symposium, </booktitle> <pages> pages 287-94, </pages> <month> April </month> <year> 1991. </year> <month> 170 </month>
Reference-contexts: That is, if a Grid copy's data happens to lie contiguously in memory, then the Mover will send or receive the data directly from its original storage locations. An important implementation decision concerns the method used to overlap communication and computation. Some architectures <ref> [19, 111, 128] </ref> provide specialized hardware support to overlap communication and computation. We considered two possibilities: using MPI non-blocking message-passing primitives, and devoting a single smp++ thread solely to communication. The MPI overlapping design is straightforward. <p> Sohn and Biswas report overlap efficiencies of up 40% on the SP-2, but only up to 10% on the PowerCHALLENGEArray. They did not consider dedicating one SMP processor to handle communication, which would have resulted in much higher overlap performance. The Proteus machine <ref> [127, 128] </ref> is a custom-built hierarchical SMP cluster designed for image processing applications. Each Proteus cluster a number of 76 Intel i860 "pixel processors" which run user code, an Intel i960 Cluster Controller, and shared memory modules. The Cluster Controller handles all inter-node communication.
Reference: [129] <author> B. Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: To efficiently manage the shared heap, the implementation must reclaim storage allocated to shared dynamic objects when possible. The KeLP implementation maintains reference counts to dynamic shared objects, which the system reclaims when the reference count reaches 0. The KeLP reference count implementation follows the design presented by Stroustrup <ref> [129] </ref>. Reference counting adds a few extra indirect memory references to access shared objects.
Reference: [130] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: These complexities hinder efficient parallel implementations, and discourage the use of high-performance parallel computers for scientific computation. To address this problem, much research has considered parallel programming languages and libraries to help the programmer manage locality and parallelism. Notably, the Parallel Virtual Machine (PVM) <ref> [130] </ref> and Message-Passing Interface (MPI) [102] have emerged as standards for portable parallel programming with message-passing. <p> It is not clear how a hybrid model like an XYZ program fits in Gropp and Lusk's taxonomy. Crandall et. al [51] report experiences with dual-level parallel programs on an SMP cluster. This work uses threads to control intra-node parallelism, and PVM <ref> [130] </ref> to manage inter-node parallelism. Their results with some simple applications and algorithms motivate further research into multi-tier programming models and environments. 45 2.5 Discussion The KeLP programming abstractions define a new programming model for block-structured scientific calculations on SMP clusters.
Reference: [131] <author> P. N. Swarztrauber. </author> <title> Vectorizing the FFTs. </title> <editor> In G. Rodrigue, editor, </editor> <booktitle> Parallel Computations, </booktitle> <pages> pages 51-83. </pages> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: We have demonstrated that explicit overlap of communication and computation improves performance on the FFT and dense linear algebra codes. Typically, the programmer will rely on libraries such as ScaLAPACK [48] or FFT-Pack <ref> [131] </ref> to implement these operations. In order to explicitly overlap communication and computation, we propose that standard libraries provide asynchronous entry points for numerical routines. For example, an FFT library should provide startFFT () and finishFFT () calls, which the programmer can use to structure the calling application as needed.
Reference: [132] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, </address> <month> Massachesetts. </month> <title> CM Fortran Language Reference Manual, </title> <year> 1991. </year>
Reference-contexts: HPF is well-suited for regular, static applications such as jacobi2D on single-tier multi-computers. It is not yet clear how HPF will perform on SMP clusters. HPF builds on a large body of data-parallel languages, including Fortran D [67], Fortran 90D [32], Kali [100], CM Fortran <ref> [132] </ref>, and Vienna Fortran [47]. There have also been several data-parallel extensions to C (eg. [119]). Fx [74] combines the data-parallel HPF ideas with task-parallel constructs. The ZPL language [97] provides powerful geometric semantics to express data-parallel computation.
Reference: [133] <author> S. W. Turner and A. V. Veidenbaum. </author> <title> Scalability of the Cedar system. </title> <booktitle> In Proc. Supercomputing, </booktitle> <pages> pages 247-254, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. <ref> [94, 77, 115, 133, 28] </ref>), low-overhead messages [136, 107, 33] or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. <p> There are two reasons why hierarchical abstractions 38 are desirable: * The architecture has a hierarchical structure, as in SMP clusters. * The application or algorithm entails hierarchical structures. Although this dissertation focuses on architectural considerations, we also review application-centric techniques that may apply. The Illinois Cedar architecture <ref> [133] </ref> was a prominent cluster-based NUMA shared memory architecture. To use Cedar for scientific applications, the Cedar Fortran language [58] included constructs to express multiple levels of parallelism and locality. Cedar Fortran variables are allocated with global, node, or processor scope. <p> Our work focuses on carefully orchestrated message-passing codes, restructured to avoid idle time on compute processors. In contrast, shared memory systems must hide latency of remote references to reduce compute processor stalls. Three prominent cluster-based NUMA shared memory machines were the Illinois Cedar architecture <ref> [133] </ref>, the Stanford DASH [94] and the CMU Cm* [69] projects. The Cedar machine consisted of a small number of Alliant FX/8 multiprocessors, each containing up to eight pipelined processors. Cedar connected the multiprocessor nodes to a global shared memory via an omega network.
Reference: [134] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: This general model directly reflects the structure of many commercial distributed-memory parallel computers, such as the IBM SP-2 [2], Cray T3E [117], and networks of uniprocessor workstations [8]. Additionally, the single-tier multicomputer arises in various forms in formal parallel computation models, such as CTA [123], BSP <ref> [134] </ref>, and LogP [53]. Research based on the single-tier multicomputer model has resulted in advances in algorithms, compiler technology, programming languages, and run-time libraries which make it possible to achieve reasonably good performance for many scientific applications on distributed-memory parallel computers. <p> Snyder clearly articulates the two-level approach in the XYZ program levels of the Phase Abstractions programming model [124]. The two-level control flow model matches single-tier architectural models, such as the Candidate Type Architecture [123], BSP <ref> [134] </ref>, and LogP [53]. These architectural models do not represent the two levels of locality and parallelism of SMP clusters. In contrast to the two levels of MPI and HPF, the multi-tier KeLP abstractions support three levels of control: a collective level, a node level, and a processor level. <p> Many parallel computation models roughly correspond the single-tier multicomputer model, pictured in Figure 1.1. Snyder's Candidate Type Architecture (CTA) [123] provides a general machine model for a multicomputer with a central controller. Valiant's Bulk-synchronous Parallel (BSP) model <ref> [134] </ref> considers a program as a sequence of coarse-grain supersteps, and provides an estimate of communication costs with a bandwidth parameter. The LogP [53] model provides parameters to characterize communication costs on a CTA-like machine, specifically communication latency, bandwidth, and overhead. <p> Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models <ref> [53, 134] </ref> focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation [135, 65, 48, 55]. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters.
Reference: [135] <author> R. van de Geign and J. Watts. SUMMA: </author> <title> Scalable universal matrix multiplication algorithm. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 9(4) </volume> <pages> 255-74, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: A fourth code, the NAS Fast Fourier transform benchmark [15], solves a 3D diffusion equation with FFTs. Finally, we consider two blocked dense linear algebra codes: SUMMA matrix multiplication <ref> [135] </ref> and ScaLAPACK's right-looking blocked dense LU factorization algorithm [49]. Each code raises distinct issues regarding the KeLP programming model. For each code, we examine algorithmic techniques to improve performance on SMP clusters. In particular, we restructure each algorithm to explicitly overlap communication and computation. <p> The parallel computing literature presents a number of algorithms for computing the product of two distributed matrices on single-tier multicomput-ers [50, 81, 39, 68, 96]. We consider the SUMMA (Scalable Universal Matrix Multiply Algorithm), presented by van de Geijn and Watts <ref> [135] </ref>. SUMMA manipulates large block array sections, and maps naturally into the KeLP and dGrid abstractions. We will consider a basic multi-tier adaptation of SUMMA, and present a new SUMMA variant that explicitly overlaps communication and computation. First, we review the single-tier SUMMA algorithm. <p> We did not have access to tuned BLAS on the SparcStation cluster, so we do not present SUMMA results for the SparcStations. The "MPI" results reported in this Section use the C+MPI SUMMA 128 version made publicly available by van de Geijn and Watts and described in <ref> [135] </ref>. We modified this code to call the DXML dgemm kernel. the number of nodes, assigning each node rougly one million matrix elements. All experiments use a panel size (blocking factor) of 100. On the AlphaServers, the MPI code outperforms the KeLP code by up to 20%. <p> Predicting performance on SMP clusters remains a difficult task. Traditional parallel performance models [53, 134] focus on message-passing costs. 154 Based on these models, researchers have developed analytic models to predict performance for many applications, including those considered in this dissertation <ref> [135, 65, 48, 55] </ref>. However, these analytic techniques do not suffice to accurately predict performance on SMP clusters. In particular, message-passing parallel performance models neglect the effects of shared memory contention and synchronization on application performance.
Reference: [136] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <journal> Computer Architecture News, </journal> <volume> 20(2) </volume> <pages> 256-66, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We assume that SMP nodes communicate via message-passing, and that message-passing incurs high software overheads. We do not assume fast shared-memory access between nodes (eg. [94, 77, 115, 133, 28]), low-overhead messages <ref> [136, 107, 33] </ref> or an efficient software-based distributed-shared memory system [88, 40, 60]. We make no claims regarding the utility, performance, or economic feasibility of these specialized solutions. Indeed, tightly-coupled scalable shared memory systems may well pervade the high-performance computer market in coming years.
Reference: [137] <author> R. Wolski, G. Shao, and F. Berman. </author> <title> Predicting the cost of redistribution in scheduling. </title> <booktitle> In Proceedings of the Eigth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Minneapolis, MN, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: mix ing layer using the Euler equations. * Silvia Figueira (Department of CSE, UCSD) used KeLP applications to in vestigate performance in multi-user parallel environments [62]. * Fran Berman, Rich Wolski, and collaborators (Department of CSE, UCSD) have used KeLP to investigate dynamic load balancing policies on heterogeneous workstation clusters <ref> [22, 23, 137] </ref>.
Reference: [138] <author> P. R. Woodward. </author> <title> Perspectives on supercomputing: Three decades of change. </title> <journal> IEEE Computer, </journal> <volume> 29(10) </volume> <pages> 99-111, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Although many programming models target the single-tier multicomputer model, clusters of symmetric multiprocessors (SMPs) have recently emerged as widespread targets for high-performance scientific computation <ref> [138] </ref>. Figure 1.2 shows a representation of an SMP cluster as a multi-tier multicomputer. Unlike message-passing machines with uniprocessor nodes, SMP clusters present two coarse-grained levels of parallelism.
Reference: [139] <author> D. W. L. Yen, J. H. Patel, and E. S. Davidson. </author> <title> Memory interference in synchronous multiprocessor systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(11):1116-21, </volume> <month> November </month> <year> 1982. </year>
References-found: 139

