URL: http://www.cs.iastate.edu/~honavar/Papers/gdml.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Email: e-mail: honavar@iastate.edu  
Title: Inductive Learning Using Generalized Distance Measures  
Author: Vasant Honavar 
Address: Ames, Iowa 50011. U.S.A.  
Affiliation: Department of Computer Science Iowa State University  
Abstract: 1 This paper briefly reviews the two currently dominant paradigms in machine learning - the connectionist network (CN) models and symbol processing (SP) systems; argues for the centrality of knowledge representation frameworks in learning; examines a range of representations in increasing order of complexity and measures of similarity or distance that are appropriate for each of them; introduces the notion of a generalized distance measure (GDM) and presents a class of GDM-based inductive learning algorithms (GDML). GDML are motivated by the need for an integration of symbol processing (SP) and connectionist network (CN) approaches to machine learning. GDM offer a natural generalization of the notion of distance or measure of mismatch used in a variety of pattern recognition techniques (e.g., k-nearest neighbor classifiers, neural networks using radial basis functions, and so on) to a range of structured representations such strings, trees, pyramids, association nets, conceptual graphs, etc. which include those used in computer vision and syntactic approaches to pattern recognition. GDML are a natural extension of generative or constructive learning algorithms for neural networks that enable an adaptive and parsimonious determination of the network topology as well as the desired weights as a function of learning Potential applications of GDML include tasks such as planning, concept learning, and 2-and-3-dimensional object recognition. GDML offer a basis for a natural integration of SP and CN approaches to the construction of intelligent systems that perceive, learn, and act. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bareiss, E. R. </author> <year> (1988). </year> <title> PROTOS: A Unified Approach to Concept Representation, Classification and Learning. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Texas, Austin, TX. </institution>
Reference-contexts: Such mechanisms have obvious commonalities with processes such as chunking (Laird, Rosenbloom, & Newell, 1986) and exemplar-based knowledge acquisition <ref> (Bareiss, 1988) </ref> used in SP systems. At the same time, the symbol structures constructed through generative learning are grounded in the respective sensory representations. This offers a basis for the integration of SP and CN approaches to learning.
Reference: <author> Carpenter, G. A. & Grossberg, S. </author> <year> (1987). </year> <title> A Massively Parallel for a Self-Organizing Neural Pattern Recognition Machine. Computer Vision, Graphics, </title> <booktitle> and Image Processing 37, </booktitle> <pages> 54-115. </pages>
Reference: <author> Goldfarb, L. </author> <year> (1990). </year> <title> An Evolving Model for Pattern Learning. </title> <booktitle> Pattern Recognition 23, </booktitle> <pages> 595-616. </pages>
Reference: <author> Hinton, G. E. </author> <year> (1990). </year> <title> Connectionist Learning Procedures. </title> <booktitle> Artificial Intelligence 40, </booktitle> <pages> 185-234. </pages>
Reference: <author> Honavar, V. </author> <year> (1992a). </year> <title> Generalized Distance Measures and Inductive Learning. </title> <booktitle> In prepa-ration. </booktitle>
Reference-contexts: Space does not permit a detailed discussion of different designs for GDML here. The reader is refered to (Honavar, 1992b) for a detailed development of GDML designs for different applications and to <ref> (Honavar, 1992a) </ref> for an examination of a GDML design for the parsimonious acquisition of heterarchical representational structures for 3-dimensional object recognition and description. 6 Conclusions Knowledge representation mechanisms play a central role in learning.
Reference: <author> Honavar, V. </author> <year> (1992b). </year> <title> Learning Parsimonious Three-Dimensional Shape Representations Using Generalized Distance Measures. </title> <booktitle> In: NATO Advanced Research Workshop on Mathematical Representations of Shape, </booktitle> <address> Driebergen, Netherlands. </address> <note> To appear. </note>
Reference-contexts: A range of representation-specific mechanisms for accomplishing this can be developed for particular applications such as planning, language learning, etc. Space does not permit a detailed discussion of different designs for GDML here. The reader is refered to <ref> (Honavar, 1992b) </ref> for a detailed development of GDML designs for different applications and to (Honavar, 1992a) for an examination of a GDML design for the parsimonious acquisition of heterarchical representational structures for 3-dimensional object recognition and description. 6 Conclusions Knowledge representation mechanisms play a central role in learning.
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1988). </year> <title> A Network of Neuron-Like Units that Learns to Perceive by Generation as well as Reweighting of its Links. </title> <booktitle> Proceedings of 1988 Connectionist Models Summer School, </booktitle> <editor> Touretzky, D., Hinton, G and Sejnowski T. (Eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1989). </year> <title> Brain-Structured Connectionist Networks that Perceive and Learn, </title> <booktitle> Connection Science 1, </booktitle> <pages> pp. 139-159. </pages>
Reference: <author> Honavar, V. & Uhr, L. </author> <year> (1990). </year> <title> Successive Refinement of Multi-Resolution Representations of the Environment in Connectionist Networks. </title> <booktitle> In: Proceedings of the Second Conference on Neural Networks and Parallel-Distributed Processing, </booktitle> <institution> Indiana University-Purdue University. </institution>
Reference: <author> Honavar, V. & Uhr, L. </author> <title> (1992) Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <journal> Information Sciences. </journal> <note> In press. </note>
Reference-contexts: more recently, in computational learning theory, most of the contemporary work in neural network models has tended to downplay and perhaps even ignore the impact of representation on the complexity and feasibility of learning in specific domains. 3 Generative Learning | An Example of Dynamical Generation of Representations Generative learning <ref> (Honavar & Uhr, 1992) </ref> provides mechanisms for rapid construction of internal representations through extraction, abstraction, and generalization of potentially useful input patterns (see below). Such mechanisms have obvious commonalities with processes such as chunking (Laird, Rosenbloom, & Newell, 1986) and exemplar-based knowledge acquisition (Bareiss, 1988) used in SP systems. <p> Such a system can implement a variety of alternative designs for choosing a pattern to be encoded by a generated node for deciding when to generate a new node, etc. <ref> (see Honavar & Uhr, 1992) </ref> for details. Central to any such learning scheme is a measure of similarity (or conversely, distance) of an input pattern to a previously seen pattern that has, in some form, been encoded by the system. <p> A number of related potentially useful strategies for increasing the information-content or utility of the patterns extracted and encoded by generative learning have been discussed in <ref> (Honavar & Uhr, 1992) </ref>. Generation is supplemented by modification of response properties of existing nodes in the network whenever patterns that are sufficiently similar to a currently encoded pattern are encountered. <p> For instance, a system for the recognition and description of 3-dimensional objects might employ an ordered hierarchy of representations (sets, relational descriptions, etc). that encode the necessary information from its environment at increasing amounts of detail <ref> (Honavar, 1992) </ref>. 5 Inductive Learning Using a Generalized Distance Measure - GDML Our notion of generalized distance measure is motivated by similar ideas introduced in the context of transformation systems by Goldfarb (1986). Consider a single uniform representational framework R (e.g., a space of vectors, strings, trees, graphs). <p> The extension of generative learning structures of the sort outlined above and discussed in detail in <ref> (Honavar & Uhr, 1992) </ref> to a range of representations discussed here is straightforward once the appropriate generalized distance measures are defined and the computational structures (or algorithms) for calculating them are specified.
Reference: <author> Kibler, D. & Langley, P. </author> <year> (1988). </year> <title> Machine Learning as an Experimental Science. </title> <booktitle> In: Proceedings of the Third European Session on Machine Learning. </booktitle> <address> London, UK: </address> <publisher> Pitman. </publisher>
Reference-contexts: The complexity of the task or the concept to be learned is inextricably linked to the representation employed. The following example taken from <ref> (Kibler & Langley, 1988) </ref> illustrates this point extremely well: Consider the task of learning the concept of a sine wave. If the time-domain representation of the waveform is chosen, the complexity of the function to be learned measured by the number of peaks in the waveform is infinite.
Reference: <author> Laird, J. E., Rosenbloom, P. S. </author> & <title> Newell (1986). Chunking in SOAR: The Anatomy of a General Learning Mechanism. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 11-46. </pages>
Reference-contexts: Such mechanisms have obvious commonalities with processes such as chunking <ref> (Laird, Rosenbloom, & Newell, 1986) </ref> and exemplar-based knowledge acquisition (Bareiss, 1988) used in SP systems. At the same time, the symbol structures constructed through generative learning are grounded in the respective sensory representations. This offers a basis for the integration of SP and CN approaches to learning.
Reference: <author> Levenshtein, A. </author> <year> (1966). </year> <title> Binary Codes Capable of Correcting Deletions, </title> <journal> Insertions, and Reversals. Soviet Physics - Doklady 10, </journal> <pages> 703-710. </pages>
Reference-contexts: In general however, we need to be able to compare strings of unequal lengths, defined over an arbitrary alphabet . Thus we need a more flexible notion of similarity or distance between two strings, e.g., Levenshtein distance which was originally developed for error correcting codes <ref> (Levenshtein, 1966) </ref>. It is based on the following intuitive notion of distance: The distance from a string ff to another string fi can be measured by the minimum cost of transforming ff into fi (this is also refered to as the edit distance between ff and fi).
Reference: <author> Lu, S. Y. </author> <year> (1979). </year> <title> A Tree to Tree Distance and its Application to Cluster Analysis. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 1, </journal> <pages> 219-229. </pages>
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Several simple, elegant and mathematically well- understood weight modification algorithms exist for such networks with a single layer of modifiable weights. However, such single-layer networks have limited representational power and hence are incapable of learning complex mappings <ref> (Minsky & Papert, 1969) </ref>. Multi-layer architectures can potentially overcome this limitation given an appropriate set of learning processes (Nilsson, 1965) and indeed multi-layer networks that learn using the generalized delta rule (Rumelhart, Hinton, & Williams, 1986) demonstrate this fact.
Reference: <author> Natarajan, B. K. </author> <year> (1992). </year> <title> Machine Learning: A Theoretical Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recent results in computational learning theory <ref> (Natarajan, 1992) </ref> indicate that the choice of knowledge representation influences the the computational complexity (measured in terms of time and memory requirements) and hence the practical feasibility of learning: e.g., Boolean concepts expressed in k-term DNF (disjunctions of at most k pure conjuncts, each with an unlimited number of literals) most
Reference: <author> Rumelhart, D. E., Hinton, G. E. & Williams, J. R. </author> <title> (1986) Learning Internal Representations by Error Propagation. </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> Rumelhart, D. E. and McClelland, J. L. (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: However, such single-layer networks have limited representational power and hence are incapable of learning complex mappings (Minsky & Papert, 1969). Multi-layer architectures can potentially overcome this limitation given an appropriate set of learning processes (Nilsson, 1965) and indeed multi-layer networks that learn using the generalized delta rule <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> demonstrate this fact. However, most of the work on CN learning algorithms has tended to downplay the role of representation (see below).
Reference: <author> Sanfeliu, A. & Fu, K. S. </author> <year> (1983). </year> <title> A Distance Measure for Attributed Relational Graphs for Pattern Recognition. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 13, </journal> <pages> 353-362. </pages>
Reference: <author> Selkow, S. M. </author> <year> (1977). </year> <title> The Tree-to-tree Editing Problem. </title> <journal> Information Processing Letters 6, </journal> <pages> 184-186. </pages>
Reference-contexts: Such representations are well-suited for learning tasks in natural language, planning and scheduling applications. 4.4 Trees Methods for computing the distance between trees with nodes having labels belonging to a specified alphabet X exploit the fact that a tree can be viewed as a recursive string of subtrees <ref> (Selkow, 1977) </ref>.
Reference: <author> Shapiro, L. G. & Haralick, R. M. </author> <year> (1985). </year> <title> A Metric for Comparing Relational Descriptions. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 7, </journal> <pages> 90-94. </pages>
Reference: <author> Tsai, W. H. & Fu, K. S. </author> <year> (1979). </year> <title> Error-Correcting Isomorphisms of Attributed Relational Graphs for Pattern Analysis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 9, </journal> <pages> 757-768. </pages>
Reference-contexts: Such representations have been demonstrated to be quite efficient and effective in tasks such as 2-dimensional object recognition. 4.5 Graphs A measure of distance between graphs can be defined analogous to those for strings and trees <ref> (Tsai & Fu, 1979) </ref>, as the minimum cost of transforming one graph into another. However, the complexity of algorithms for computing distance thus defined is exponential in the size of the graphs. The problem of developing practically useful yet computationally tractable measures of similarity between graphs remains largely open.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
Reference-contexts: and hence the practical feasibility of learning: e.g., Boolean concepts expressed in k-term DNF (disjunctions of at most k pure conjuncts, each with an unlimited number of literals) most likely are not feasibly learnable whereas concepts expressed in k-CNF (conjunctive normal form with at most k literals per conjunct) are <ref> (Valiant, 1984) </ref>. The complexity of the task or the concept to be learned is inextricably linked to the representation employed. The following example taken from (Kibler & Langley, 1988) illustrates this point extremely well: Consider the task of learning the concept of a sine wave.
Reference: <author> Wagner, R. A. & Fisher, M. J. </author> <year> (1974). </year> <title> The String to String Correction Problem. </title> <journal> Journal of the ACM 21, </journal> <pages> 168-173. </pages>
Reference-contexts: That is, ? (ff; fi) = min C (ff; fi) The edit distance between two strings of length m and n respectively can be computed by a O (mn) dynamic programming algorithm <ref> (Wagner & Fisher, 1974) </ref>. Extension of this approach to handle strings which contain variables as well as constant letters of the alphabet is in progress.
References-found: 23

