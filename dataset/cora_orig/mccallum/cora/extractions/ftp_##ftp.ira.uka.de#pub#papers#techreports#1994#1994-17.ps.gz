URL: ftp://ftp.ira.uka.de/pub/papers/techreports/1994/1994-17.ps.gz
Refering-URL: http://wwwipd.ira.uka.de/~prechelt/
Root-URL: 
Email: email: (lukowicz j heinze j prechelt j  
Title: Experimental Evaluation in Computer Science: A Quantitative Study "The fundamental principle of science, the definition
Author: Paul Lukowicz, Ernst A. Heinz, Lutz Prechelt, Walter F. Tichy experiment." Richard P. Donald E. Knuth 
Note: To appear in "Journal of Systems and Software",  The survey includes complete volumes of several refereed computer science journals, a conference, and 50 titles drawn at random from all articles published by ACM in 1993. The journals Optical Engineering (OE) and Neural Computation  OE and NC is only 15% and 12%, respectively. Conversely, the fraction of papers that devote one fifth or more of their space to experimental validation is almost 70% for OE and NC, while it is a mere 30% for the CS random sample and 20% for software engineering.  
Date: 17/94  January(?) 1995 August 25, 1994  
Address: Germany  
Affiliation: Department of Informatics University of Karlsruhe,  
Web: tichy) ira.uka.de  
Pubnum: Technical Report  
Abstract: A survey of over 400 recent research articles suggests that computer scientists publish relatively few papers with experimentally validated results. The low ratio of validated results appears to be a serious weakness in computer science research. This weakness should be rectified for the long-term health of the field. Contents 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David H. Bailey. </author> <title> Twelve ways to fool the mass es when giving performance results on parallel computers. </title> <booktitle> Supercomputing Review, </booktitle> <pages> pages 54-55, </pages> <month> August </month> <year> 1991. </year> <note> Also in: </note> <institution> Supercomputer, </institution> <month> September </month> <year> 1991, </year> <pages> 4-7. </pages>
Reference-contexts: He states that most experimental efforts "fall short of science on several levels" and continues "It is symptomatic of the situation in OR and computer science one cannot publish reports that an algorithm does not perform well in computational tests." In a similar spirit, Bailey <ref> [1] </ref> presents a list of common experimental flaws in the field of computer performance evaluation, suggesting that many of these errors are committed intentionally | to "fool the masses".
Reference: [2] <author> Doug Baldwin and Johannes Koomen. </author> <title> Using sci entific experiments in early computer science lab oratories. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 24(1) </volume> <pages> 102-106, </pages> <year> 1992. </year>
Reference-contexts: Several articles describe the role of experimental research in branches of CS, e.g. machine learning [11], algorithms [8], or software engineering [7]. 2 The latter article is quite critical of software engi-neering research and states: "There are far too few examples of moderately effective research." Bald-win and Koomen <ref> [2] </ref> discuss practicing experimental computer science during CS education.
Reference: [3] <author> Peter J. </author> <title> Denning. </title> <journal> What is experimental com puter science? Communications of the ACM, </journal> <volume> 23(10) </volume> <pages> 543-544, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: Today, the situation is perceived as largely unchanged: In 1994, the authors of reference [13] conclude that experimental CS is still underfunded, and that researchers in the area often face difficult career paths at universities. In 1980, Denning <ref> [3] </ref> gives a definition of what experimental CS is: "Measuring an apparatus in order to test a hypothesis." Denning notes that standards in the natural sciences describe how to carry out such work properly, but that CS is rarely performing well by these standards.
Reference: [4] <author> Peter J. Denning. </author> <title> Performance analysis: Exper imental computer science at its best. </title> <journal> Communi cations of the ACM, </journal> <volume> 24(11) </volume> <pages> 725-727, </pages> <month> November </month> <year> 1981. </year>
Reference-contexts: He concludes that "If we do not live up to the traditional standards of science, there will come a time when no one takes us seriously". In later articles, Denning cites the field of performance evaluation as a positive example of experimental CS research <ref> [4, 5] </ref>.
Reference: [5] <author> Peter J. Denning. </author> <title> Performance evaluation: exper imental computer science at its best. </title> <journal> ACM Per formance Evaluation Review (SIGMETRICS), </journal> <volume> 10(3) </volume> <pages> 106-109, </pages> <year> 1981. </year>
Reference-contexts: He concludes that "If we do not live up to the traditional standards of science, there will come a time when no one takes us seriously". In later articles, Denning cites the field of performance evaluation as a positive example of experimental CS research <ref> [4, 5] </ref>.
Reference: [6] <author> Jerome A. Feldman and William R. Sutherland. </author> <title> Rejuvenating experimental computer science | A report to the national science foundation and others. </title> <journal> Communications of the ACM, </journal> <volume> 22(9):497 502, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Early surveys <ref> [6, 12] </ref> published in 1979 describe the state of experimental CS with respect to the poor support it received.
Reference: [7] <author> Norman Fenton, Shari Lawrence Pfleeger, and Robert L. Glass. </author> <title> Science and substance: A chal lenge to software engineers. </title> <journal> IEEE Software, </journal> <pages> pages 86-95, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In later articles, Denning cites the field of performance evaluation as a positive example of experimental CS research [4, 5]. Several articles describe the role of experimental research in branches of CS, e.g. machine learning [11], algorithms [8], or software engineering <ref> [7] </ref>. 2 The latter article is quite critical of software engi-neering research and states: "There are far too few examples of moderately effective research." Bald-win and Koomen [2] discuss practicing experimental computer science during CS education.
Reference: [8] <author> John N. Hooker. </author> <title> Needed: An empirical science of algorithms. </title> <journal> Operations Research, </journal> <volume> 42(2) </volume> <pages> 201-212, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: In later articles, Denning cites the field of performance evaluation as a positive example of experimental CS research [4, 5]. Several articles describe the role of experimental research in branches of CS, e.g. machine learning [11], algorithms <ref> [8] </ref>, or software engineering [7]. 2 The latter article is quite critical of software engi-neering research and states: "There are far too few examples of moderately effective research." Bald-win and Koomen [2] discuss practicing experimental computer science during CS education. <p> In 1990, Iyer [10] writes that "experimental CS is a relatively new, yet fast developing area" and finds: "It is indeed encouraging to see that there is substantial research going on in this important area." Four years later, however, Hooker <ref> [8] </ref> notes that experimental research is dramatically underdeveloped in algorithms research.
Reference: [9] <author> STN International. INSPEC: </author> <title> Information ser vice for physics and engineering communities. </title> <publisher> c/o Chemical Abstracts Service, </publisher> <address> P.O. Box 3012, Columbus, Ohio, </address> <year> 1994. </year> <title> Made by IEE, </title> <address> Sta tion House, 70 Nightingale Road, Hitchin, Herts SG5 1RJ, Great Britain. </address>
Reference-contexts: Moreover, we drew a random sample of 74 titles from the set of all works published by ACM in 1993, using the INSPEC database <ref> [9] </ref>. From this sample, we excluded 24 articles that were either inappropriate (because they were not peer-reviewed research papers) or not available in our library. See Appendix A for details. The resulting set contains 50 papers, of which 30 are refereed conference contributions.
Reference: [10] <author> Ravi K. Iyer. </author> <title> Experimental computer science. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(2) </volume> <pages> 109-110, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: In 1990, Iyer <ref> [10] </ref> writes that "experimental CS is a relatively new, yet fast developing area" and finds: "It is indeed encouraging to see that there is substantial research going on in this important area." Four years later, however, Hooker [8] notes that experimental research is dramatically underdeveloped in algorithms research.
Reference: [11] <author> Pat Langley. </author> <title> Machine learning as an experimen tal science. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 5-8, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In later articles, Denning cites the field of performance evaluation as a positive example of experimental CS research [4, 5]. Several articles describe the role of experimental research in branches of CS, e.g. machine learning <ref> [11] </ref>, algorithms [8], or software engineering [7]. 2 The latter article is quite critical of software engi-neering research and states: "There are far too few examples of moderately effective research." Bald-win and Koomen [2] discuss practicing experimental computer science during CS education.
Reference: [12] <author> Daniel D. McCracken, Peter J. Denning, and David H. Brandin. </author> <title> An ACM executive committee position on the crisis in experimental computer science. </title> <journal> Communications of the ACM, </journal> <volume> 22(9):503 504, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Early surveys <ref> [6, 12] </ref> published in 1979 describe the state of experimental CS with respect to the poor support it received.
Reference: [13] <author> Computer Science and Telecommunications Board. </author> <title> Academic careers for experimental com puter scientists and engineers. </title> <journal> Communications of the ACM, </journal> <volume> 37(4) </volume> <pages> 87-90, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Early surveys [6, 12] published in 1979 describe the state of experimental CS with respect to the poor support it received. Today, the situation is perceived as largely unchanged: In 1994, the authors of reference <ref> [13] </ref> conclude that experimental CS is still underfunded, and that researchers in the area often face difficult career paths at universities.
References-found: 13

