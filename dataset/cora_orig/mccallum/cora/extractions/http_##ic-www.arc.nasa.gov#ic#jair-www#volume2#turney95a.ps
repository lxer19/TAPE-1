URL: http://ic-www.arc.nasa.gov/ic/jair-www/volume2/turney95a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/turney95a.html
Root-URL: 
Email: TURNEY@AI.IIT.NRC.CA  
Title: Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm  
Author: Peter D. Turney 
Address: Canada, Ottawa, Ontario, Canada, K1A 0R6.  
Affiliation: Knowledge Systems Laboratory, Institute for Information Technology National Research Council  
Note: Journal of Artificial Intelligence Research 2 (1995) 369-409 Submitted 10/94; published 3/95 1995 National Research Council Canada. All rights reserved. Published by permission.  
Abstract: This paper introduces ICET, a new algorithm for costsensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for costsensitive classification EG2, CS-ID3, and IDX and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICETs search in bias space and discovers a way to improve the search.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D., & Littman, M. </author> <year> (1991). </year> <title> Interactions between learning and evolution. </title> <booktitle> In Proceedings of the Second Conference on Artificial Life, </booktitle> <editor> C. Langton, C. Taylor, D. Farmer, and S. Rasmussen, editors. </editor> <address> California: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Aha, D.W., Kibler, D., & Albert, M.K. </author> <year> (1991). </year> <title> Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: We filled in the missing values, using a simple single nearest neighbor algorithm <ref> (Aha et al., 1991) </ref>. The missing values were filled in using the whole dataset, before the dataset was split into training and testing sets. For the nearest neighbor algorithm, the data were normalized so that the mini Table 19: Test costs for the Hepatitis Prognosis dataset.
Reference: <author> Aha, D.W., & Bankert, R.L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. Case-Based Reasoning: Papers fr om the 1994 W ork-shop, edited by D.W . Aha, </title> <type> T echnical Report WS-94-07, </type> <pages> pp. 106-1 12. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: For a given decision tree, the total cost of tests will be the same for all cases. In situations of this type, the problem of minimizing cost simplifies to the problem of choosing the best subset of the set of available tests <ref> (Aha and Bankert, 1994) </ref>. The sequential order of the tests is no longer important for reducing cost. Let us consider a simple example to illustrate the method. T able 1 shows the test costs for four tests. T wo of the tests are immediate and two are delayed. <p> The Baldwin effect may explain much of the success of ICET. 4. Experiments This section describes experiments that were performed on five datasets, taken from the Irv-ine collection <ref> (Murphy & Aha, 1994) </ref>. The five datasets are described in detail in Appendix A. All five datasets involve medical problems. The test costs are based on information from the Ontario Ministry of Health (1992). The main purpose of the experiments is to gain insight into the behavior of ICET. <p> These are just a few of the pragmatic constraints that are faced in real-world classification problems. Appendix A. Five Medical Datasets This appendix presents the test costs for five medical datasets, taken from the Irvine collection <ref> (Murphy & Aha, 1994) </ref>. The costs are based on information from the Ontario Ministry of Health (1992). Although none of the medical data were gathered in Ontario, it is reasonable to assume that other areas have similar relative test costs.
Reference: <author> Anderson, R.W. </author> <title> (in press). Learning and evolution: A quantitative genetics approach. </title> <journal> Journal of Theoretical Biology. </journal>
Reference: <author> Baldwin, J.M. </author> <title> (1896). A new factor in evolution. </title> <journal> American Naturalist, </journal> <volume> 30, </volume> <pages> 441-451. </pages>
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and r egression trees. </title> <address> California: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: If we have a simple cost matrix, an algorithm such as CAR T <ref> (Breiman et al., 1984) </ref> that is sensitive to misclassification error cost has no advantage over an algorithm such as C4.5 (Quinlan, 1992) that maximizes accuracy (assuming other differences between these two algorithms are negligible).

Reference: <author> Frey, P.W., & Slate, D.J., </author> <year> (1991). </year> <title> Letter recognition using Holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 161-182. </pages>
Reference: <author> Friedman, J.H., & Stuetzle, </author> <title> W . (1981). Projection pursuit regression. </title> <journal> Journal of the American Statistics Association, </journal> <volume> 76, </volume> <pages> 817-823. </pages>
Reference: <author> Gordon, D.F., & Perlis, D. </author> <year> (1989). </year> <title> Explicitly biased generalization. </title> <journal> Computational Intelligence, </journal> <volume> 5, </volume> <pages> 67-81. </pages>
Reference: <author> Grefenstette, J.J. </author> <year> (1986). </year> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 16, </volume> <pages> 122-128. </pages>
Reference-contexts: Benefit is treated as negative cost. This paper introduces a new algorithm for costsensitive classification, called ICET (Inexpensive Classification with Expensive T ests pronounced iced tea). ICET uses a genetic algorithm <ref> (Grefenstette, 1986) </ref> to evolve a population of biases for a decision tree induction algorithm (a modified version of C4.5, Quinlan, 1992). <p> The genetic algorithm evolves a population of biases for the decision tree induction algorithm. The genetic algorithm we use is GENESIS <ref> (Grefenstette, 1986) </ref>. 7 The decision tree induction algorithm is C4.5 (Quinlan, 1992), modified to use ICF . That is, the decision tree induction algorithm is EG2, implemented as described in Section 3.2. ICET uses a two-tiered search strategy . <p> Figure 2 gives a sketch of the ICET algorithm. GENESIS has several parameters that can be used to alter its performance. The parameters we used are listed in T able 4. These are essentially the default parameter settings <ref> (Grefenstette, 1986) </ref>. W e used a population size of 50 individuals and 1,000 trials, which results in 20 generations. An individual in the population consists of a string of numbers, where n is the number of attributes (tests) in the given dataset. <p> These numbers are adjacent, yet the Hamming distance from 0111 to 1000 is lar ge. In a Gray code, adjacent numbers are represented with binary codes that have small Hamming distances. This tends to improve the performance of a genetic algorithm <ref> (Grefenstette, 1986) </ref>. GENESIS genetic algorithm population of biases EG2 classifier EG2 classifier EG2 classifier data decision tree decision tree decision tree fitness function fittest decision tree n 2+ C i w C i 1 TURNEY 380 age cost is measured as described in Section 2.2.
Reference: <author> Grefenstette, J.J., Ramsey, C.L., & Schultz, A.C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 355-381. </pages>
Reference: <author> Hermans, J., Habbema, J.D.F., & Van der Bur ght, A.T. </author> <year> (1974). </year> <title> Cases of doubt in allocation problems, k populations. </title> <journal> Bulletin of the International Statistics Institute, </journal> <volume> 45, </volume> <pages> 523-529. </pages>
Reference: <author> Hinton, G.E., & Nowlan, S.J. </author> <year> (1987). </year> <title> How learning can guide evolution. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 495-502. </pages>
Reference: <author> Karakoulas, G. </author> <title> (in preparation). A Q-learning approach to cost-effective classification. </title> <type> Technical Report, </type> <institution> Knowledge Systems Laboratory , National Research Council Canada. </institution> <note> Also submitted to the Twelfth International Conference on Machine Learning, ML-95. </note>
Reference: <author> Knoll, U., Nakhaeizadeh, G., & Tausend, B. </author> <year> (1994). </year> <title> Costsensitive pruning of decision trees. </title> <booktitle> Proceedings of the Eight Eur opean Confer ence on Machine Learning, ECML-94, </booktitle> <pages> pp. 383-386. </pages> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Koza, J.R. </author> <year> (1992). </year> <title> Genetic Pr ogramming: On the pr ogramming of computers by means of natural selection. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lirov, Y., & Yue, O.-C. </author> <year> (1991). </year> <title> Automated network troubleshooting knowledge acquisition. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 1, </volume> <pages> 121-132. </pages>
Reference-contexts: The problem of costsensitive classification arises frequently . It is a problem in medical diagnosis (Nez, 1988, 1991), robotics (T an & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (V erdenius, 1991), communication network troubleshooting <ref> (Lirov & Yue, 1991) </ref>, machinery diagnosis (where the main cost is skilled labor), automated testing of electronic equipment (where the main cost is time), and many other areas. <p> However, searching for the optimal solution is computationally infeasible (Pearl, 1988). W e attempted to take a decision theoretic COSTSENSITIVE CLASSIFICATION: EMPIRICAL EVALUATION 397 approach to this problem by implementing the AO* algorithm (Pearl, 1984) and designing a heuristic evaluation function to speed up the AO* search <ref> (Lirov & Y ue, 1991) </ref>. W e were unable to make this approach execute fast enough to be practical. We also attempted to apply genetic programming (Koza, 1993) to the problem of cost-sensitive classification.
Reference: <author> Maynard Smith, J. </author> <year> (1987). </year> <title> When learning guides evolution. </title> <journal> Nature, </journal> <volume> 329, </volume> <pages> 761-762. </pages>
Reference: <author> Morgan, C.L. </author> <title> (1896). On modification and variation. </title> <journal> Science, </journal> <volume> 4, </volume> <pages> 733-740. </pages>
Reference: <author> Murphy, P.M., & Aha, </author> <month> D.W . </month> <year> (1994). </year> <institution> UCI Repository of Machine Learning Databases . University of California at Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: The Baldwin effect may explain much of the success of ICET. 4. Experiments This section describes experiments that were performed on five datasets, taken from the Irv-ine collection <ref> (Murphy & Aha, 1994) </ref>. The five datasets are described in detail in Appendix A. All five datasets involve medical problems. The test costs are based on information from the Ontario Ministry of Health (1992). The main purpose of the experiments is to gain insight into the behavior of ICET. <p> These are just a few of the pragmatic constraints that are faced in real-world classification problems. Appendix A. Five Medical Datasets This appendix presents the test costs for five medical datasets, taken from the Irvine collection <ref> (Murphy & Aha, 1994) </ref>. The costs are based on information from the Ontario Ministry of Health (1992). Although none of the medical data were gathered in Ontario, it is reasonable to assume that other areas have similar relative test costs.
Reference: <author> Norton, S.W. </author> <year> (1989). </year> <title> Generating better decision trees. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, IJCAI-89, </booktitle> <pages> pp. 800-805. </pages> <address> Detroit, Mich-igan. </address>
Reference-contexts: There are several machine learning algorithms that consider the costs of tests, such as EG2 (N ez, 1988, 1991), CS-ID3 (T an & Schlimmer , 1989, 1990; T an, 1993), and IDX <ref> (Norton, 1989) </ref>. There are also several algorithms that consider the costs of classification errors (Breiman et al. , 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, in press; Knoll et al., 1994). <p> Section 3 introduces the five algorithms that we examine here, C4.5 (Quinlan, 1992), EG2 (Nez, 1991), CS-ID3 (T an & Schlimmer , 1989, 1990; T an, 1993), IDX <ref> (Norton, 1989) </ref>, and ICET . The five algorithms are evaluated empirically on five real-world medical datasets. The datasets are discussed in detail in Appendix A. Section 4 presents three sets of experiments. <p> An example of a criterion that cannot be represented as a cost is stability (Turney, in press). 3. Algorithms This section discusses the algorithms used in this paper: C4.5 (Quinlan, 1992), EG2 (Nez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX <ref> (Norton, 1989) </ref>, and ICET. 5. We will occasionally say simple cost matrix or complex cost matrix. <p> CS-ID3 uses a lazy evaluation strategy . It only constructs the part of the decision tree that classifies the current case. We did not implement this aspect of CS-ID3, since it was not relevant for the experiments reported here. 3.4 IDX IDX <ref> (Norton, 1989) </ref> is a TDIDT algorithm that selects the attribute that maximizes the following heuristic function: (5) We implemented IDX by modifying C4.5 so that it selects the attribute that maximizes (5).
Reference: <author> Nez, M. </author> <year> (1988). </year> <title> Economic induction: A case study . Proceedings of the Thir d Eur opean Working Session on Learning, </title> <booktitle> EWSL-88, </booktitle> <pages> pp. 139-145. </pages> <address> California: </address> <publisher> Morgan Kaufmann. </publisher> <address> TURNEY 408 Nez, M. </address> <year> (1991). </year> <title> The use of background knowledge in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 231-250. </pages> <institution> Ontario Ministry of Health (1992). Schedule of benefits: Physician services under the health insurance act, </institution> <month> October 1, </month> <year> 1992. </year> <institution> Ontario: Ministry of Health. </institution>
Reference-contexts: The problem of costsensitive classification arises frequently . It is a problem in medical diagnosis <ref> (Nez, 1988, 1991) </ref>, robotics (T an & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (V erdenius, 1991), communication network troubleshooting (Lirov & Yue, 1991), machinery diagnosis (where the main cost is skilled labor), automated testing of electronic equipment (where the main cost is time), and many other areas.
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. </author> <year> (1994). </year> <title> Reducing misclas-sification costs: Knowledge-intensive approaches to learning from noisy data. </title> <booktitle> Proceedings of the Eleventh International Confer ence on Machine Learning, ML-94, </booktitle> <pages> pp. 217-225. </pages> <address> New Brunswick, New Jersey. </address>
Reference: <author> Pearl, J. </author> <year> (1984). </year> <title> Heuristics: Intelligent search strategies for computer problem solving. </title> <publisher> Mas-sachusetts: Addison-Wesley. </publisher>
Reference-contexts: However, searching for the optimal solution is computationally infeasible (Pearl, 1988). W e attempted to take a decision theoretic COSTSENSITIVE CLASSIFICATION: EMPIRICAL EVALUATION 397 approach to this problem by implementing the AO* algorithm <ref> (Pearl, 1984) </ref> and designing a heuristic evaluation function to speed up the AO* search (Lirov & Y ue, 1991). W e were unable to make this approach execute fast enough to be practical. We also attempted to apply genetic programming (Koza, 1993) to the problem of cost-sensitive classification.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference. </title> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The method can be applied to any standard classification decision tree, regardless of how the tree is generated. We end with a discussion of the relation between cost and accuracy. 2.1 Decision Trees and Cost-Sensitive Classification The decision trees used in decision theory <ref> (Pearl, 1988) </ref> are somewhat dif ferent from the classification decision trees that are typically used in machine learning (Quinlan, 1992). When we refer to decision trees in this paper , we mean the standard classification decision trees of machine learning. <p> However , searching for an optimal tree is infeasible <ref> (Pearl, 1988) </ref>. ICET was designed to find a good (but not necessarily optimal) tree, where good is defined as better than the competition (i.e., IDX, CS-ID3, and EG2). 1. Not all papers are like this. Decision tree induction algorithms such as C4.5 (Quinlan, 1992) automatically select relevant tests. <p> However, the work described here is the first application of genetic algorithms to the problem of costsensitive classification. We mentioned in Section 2.1 that decision theory may be used to define the optimal solution to the problem of costsensitive classification. However, searching for the optimal solution is computationally infeasible <ref> (Pearl, 1988) </ref>. W e attempted to take a decision theoretic COSTSENSITIVE CLASSIFICATION: EMPIRICAL EVALUATION 397 approach to this problem by implementing the AO* algorithm (Pearl, 1984) and designing a heuristic evaluation function to speed up the AO* search (Lirov & Y ue, 1991).
Reference: <author> Pipitone, F., De Jong, K.A., & Spears, W .M. </author> <year> (1991). </year> <title> An artificial intelligence approach to analog systems diagnosis. In Testing and Diagnosis of Analog Cir cuits and Systems , Ruey-wen Liu, </title> <editor> editor. </editor> <address> New York: </address> <publisher> Van Nostrand-Reinhold. </publisher>
Reference-contexts: None of these algorithms consider the cost of tests. Therefore they all focus on complex classification cost matrices, since, when tests have no cost and the classification error matrix is simple, the problem reduces to maximizing accuracy. The FIS system <ref> (Pipitone et al., 1991) </ref> attempts to find a decision tree that minimizes the average total cost of the tests required to achieve a certain level of accuracy . This approach could be implemented in ICET by altering the fitness function. The main distinction between FIS (Pipitone et al., 1991) and ICET <p> The FIS system <ref> (Pipitone et al., 1991) </ref> attempts to find a decision tree that minimizes the average total cost of the tests required to achieve a certain level of accuracy . This approach could be implemented in ICET by altering the fitness function. The main distinction between FIS (Pipitone et al., 1991) and ICET is that FIS does not learn from data. The information gain of a test is estimated using a qualitative causal model, instead of training cases. Qualitative causal models are elicited from domain experts, using a special knowledge acquisition tool.
Reference: <author> Provost, F.J. </author> <year> (1994). </year> <title> Goal-directed inductive learning: Trading off accuracy for reduced error cost. </title> <booktitle> AAAI Spring Symposium on Goal-Driven Learning. </booktitle>
Reference: <author> Provost, F.J., & Buchanan, B.G. </author> <title> (in press). Inductive policy: The pragmatics of bias selection. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Quinlan, J.R. </author> <year> (1992). </year> <title> C4.5: Programs for machine learning. </title> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Benefit is treated as negative cost. This paper introduces a new algorithm for costsensitive classification, called ICET (Inexpensive Classification with Expensive T ests pronounced iced tea). ICET uses a genetic algorithm (Grefenstette, 1986) to evolve a population of biases for a decision tree induction algorithm <ref> (a modified version of C4.5, Quinlan, 1992) </ref>. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. <p> Section 2 discusses why a decision tree is the natural form of knowledge representation for classification with expensive tests and how we measure the average cost of classification of a decision tree. Section 3 introduces the five algorithms that we examine here, C4.5 <ref> (Quinlan, 1992) </ref>, EG2 (Nez, 1991), CS-ID3 (T an & Schlimmer , 1989, 1990; T an, 1993), IDX (Norton, 1989), and ICET . The five algorithms are evaluated empirically on five real-world medical datasets. The datasets are discussed in detail in Appendix A. Section 4 presents three sets of experiments. <p> We end with a discussion of the relation between cost and accuracy. 2.1 Decision Trees and Cost-Sensitive Classification The decision trees used in decision theory (Pearl, 1988) are somewhat dif ferent from the classification decision trees that are typically used in machine learning <ref> (Quinlan, 1992) </ref>. When we refer to decision trees in this paper , we mean the standard classification decision trees of machine learning. The claims we make here about classification decision trees also apply to decision theoretical decision trees, with some modification. <p> ICET was designed to find a good (but not necessarily optimal) tree, where good is defined as better than the competition (i.e., IDX, CS-ID3, and EG2). 1. Not all papers are like this. Decision tree induction algorithms such as C4.5 <ref> (Quinlan, 1992) </ref> automatically select relevant tests. Aha and Bankert (1994), among others, have used sequential test selection procedures in conjunction with a supervised learning algorithm. <p> If we have a simple cost matrix, an algorithm such as CAR T (Breiman et al., 1984) that is sensitive to misclassification error cost has no advantage over an algorithm such as C4.5 <ref> (Quinlan, 1992) </ref> that maximizes accuracy (assuming other differences between these two algorithms are negligible). Most of the experiments in this paper use a simple cost matrix (the only exception is Section 4.2.3). <p> This does not mean that all criteria can be represented as costs. An example of a criterion that cannot be represented as a cost is stability (Turney, in press). 3. Algorithms This section discusses the algorithms used in this paper: C4.5 <ref> (Quinlan, 1992) </ref>, EG2 (Nez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX (Norton, 1989), and ICET. 5. We will occasionally say simple cost matrix or complex cost matrix. <p> We will occasionally say simple cost matrix or complex cost matrix. This should not cause confusion, since test costs are not represented with a matrix. 100 1 p ( ) score A accuracy B cost= COSTSENSITIVE CLASSIFICATION: EMPIRICAL EVALUATION 377 C4.5 <ref> (Quinlan, 1992) </ref> builds a decision tree using the standard TDIDT (top-down induction of decision trees) approach, recursively partitioning the data into smaller subsets, based on the value of an attribute. At each step in the construction of the decision tree, C4.5 selects the attribute that maximizes the information gain ratio. <p> At each step in the construction of the decision tree, C4.5 selects the attribute that maximizes the information gain ratio. The induced decision tree is pruned using pessimistic error estimation <ref> (Quinlan, 1992) </ref>. There are several parameters that can be adjusted to alter the behavior of C4.5. In our experiments with C4.5, we used the default settings for all parameters. We used the C4.5 source code that is distributed with (Quinlan, 1992). <p> The induced decision tree is pruned using pessimistic error estimation <ref> (Quinlan, 1992) </ref>. There are several parameters that can be adjusted to alter the behavior of C4.5. In our experiments with C4.5, we used the default settings for all parameters. We used the C4.5 source code that is distributed with (Quinlan, 1992). EG2 (Nez, 1991) is a TDIDT algorithm that uses the Information Cost Function (ICF) (Nez, 1991) for selection of attributes. ICF selects attributes based on both their information gain and their cost. <p> The genetic algorithm evolves a population of biases for the decision tree induction algorithm. The genetic algorithm we use is GENESIS (Grefenstette, 1986). 7 The decision tree induction algorithm is C4.5 <ref> (Quinlan, 1992) </ref>, modified to use ICF . That is, the decision tree induction algorithm is EG2, implemented as described in Section 3.2. ICET uses a two-tiered search strategy .
Reference: <author> Ragavan, H., & Rendell, L. </author> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, ML-93, </booktitle> <pages> pp. 252-259. </pages> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rymon, R. </author> <year> (1993). </year> <title> An SEtree based characterization of the induction problem. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, ML-93, </booktitle> <pages> pp. 268-275. </pages> <address> Cali-fornia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Selecting a classification method by cross-validation. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 135-143. </pages>
Reference: <editor> Schaffer, J.D., Whitley , D., & Eshelman, L.J. </editor> <year> (1992). </year> <title> Combinations of genetic algorithms and neural networks: A survey of the state of the art. In Combinations of Genetic Algorithms and Neural Networks , D. </title> <editor> Whitley and J.D. Schaf fer, editors. </editor> <address> California: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Based on past trials, GENESIS can find the lies that yield the best performance from EG2. In ICET, learning (local search in EG2) and evolution (in GENESIS) interact. A common form of hybrid genetic algorithm uses local search to improve the individuals in a population <ref> (Schaffer et al., 1992) </ref>. The improvements are then coded into the strings that represent the individuals. This is a form of Lamarckian evolution. In ICET, the improvements due to EG2 are not coded into the strings. However, the improvements can accelerate evolution by altering the fitness landscape.
Reference: <author> Seshu, R. </author> <year> (1989). </year> <title> Solving the parity problem. </title> <booktitle> Proceedings of the Fourth Eur opean Working Session on Learning, EWSL-89, </booktitle> <pages> pp. 263-271. </pages> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spears, W.M. </author> <year> (1992). </year> <title> Crossover or mutation? Foundations of Genetic Algorithms 2, </title> <booktitle> FOGA-92, edited by D. Whitley. </booktitle> <address> California: </address> <publisher> Morgan Kaufmann. </publisher> <editor> COSTSENSITIVE CLASSIFICATION: </editor> <title> EMPIRICAL EVALUATION 409 Sutton, R.S. (1992). Introduction: The challenge of reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 225-227. </pages>
Reference-contexts: Since crossover has a randomizing effect, similar to mutation, we must also increase the mutation rate, to compensate for the loss of crossover (Wilson, 1987; Spears, 1992). It is very difficult to analytically calculate the increase in mutation rate that is required to compensate for the loss of crossover <ref> (Spears, 1992) </ref>. Therefore we experimentally tested three different mutation settings. 14 The results are summarized in Table 12. When the crossover rate was set to zero, the best mutation rate was 0.10.
Reference: <author> Tan, M., & Schlimmer , J. </author> <year> (1989). </year> <title> Costsensitive concept learning of sensor use in approach and recognition. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, ML-89, </booktitle> <pages> pp. 392-395. </pages> <address> Ithaca, New York. </address>
Reference: <author> Tan, M., & Schlimmer , J. </author> <year> (1990). </year> <title> CSL: A costsensitive learning system for sensing and grasping objects. </title> <booktitle> IEEE International Confer ence on Robotics and Automation . Cincin-nati, </booktitle> <publisher> Ohio. </publisher>
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Costsensitive learning of classification knowledge and its applications in robotics. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 7-33. </pages>
Reference: <author> Tcheng, D., Lambert, B., Lu, S., Rendell, L. </author> <year> (1989). </year> <title> Building robust learning systems by combining induction and optimization. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, IJCAI-89, </booktitle> <pages> pp. 806-812. </pages> <address> Detroit, </address> <publisher> Michigan. </publisher> <editor> Turney, P .D. (in press). </editor> <title> T echnical note: Bias and the quantification of stability . Machine Learning. Verdenius, F . (1991). A method for inductive cost optimization. </title> <booktitle> Proceedings of the Fifth European Working Session on Learning, </booktitle> <pages> EWSL-91 , pp. 179-191. </pages> <address> New Y ork: </address> <publisher> Springer - Verlag. </publisher>
Reference: <author> Waddington, C.H. </author> <year> (1942). </year> <title> Canalization of development and the inheritance of acquired characters. </title> <journal> Nature, </journal> <volume> 150, </volume> <pages> 563-565. </pages>
Reference: <author> Whitley, D., Dominic, S., Das, R., & Anderson, C.W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 259-284. </pages>
Reference-contexts: However , it might be possible to substantially improve the performance of ICET by tuning some of these parameters. A recent trend in genetic algorithm research is to let the genetic algorithm adjust some of its own parameters, such as mutation rate and crossover rate <ref> (Whitleyet al., 1993) </ref>. Another possibility is to stop breeding when the fitness levels stop improving, instead of stopping after a fixed number of generations. Provost and Buchanan (in press) use a goodness measure as a stopping condition for the bias space search. 6.
Reference: <author> Whitley, D., & Gruau, F. </author> <year> (1993). </year> <title> Adding learning to the cellular development of neural networks: Evolution and the Baldwin effect. </title> <journal> Evolutionary Computation, </journal> <volume> 1, </volume> <pages> 213-233. </pages>
Reference-contexts: However , it might be possible to substantially improve the performance of ICET by tuning some of these parameters. A recent trend in genetic algorithm research is to let the genetic algorithm adjust some of its own parameters, such as mutation rate and crossover rate <ref> (Whitleyet al., 1993) </ref>. Another possibility is to stop breeding when the fitness levels stop improving, instead of stopping after a fixed number of generations. Provost and Buchanan (in press) use a goodness measure as a stopping condition for the bias space search. 6.
Reference: <author> Whitley, D., Gordon, S., & Mathias, K. </author> <year> (1994). </year> <title> Lamarckian evolution, the Baldwin ef fect and function optimization. Parallel Problem Solving from Nature PPSN III. </title> <editor> Y. Davi-dor, H.P. Schwefel, and R. Manner, </editor> <booktitle> editors, </booktitle> <pages> pp. 6-15. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Wilson, S.W. </author> <year> (1987). </year> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 199-228. </pages>
References-found: 44

